{"title": [{"text": "BinLin: A Simple Method of Dependency Tree Linearization", "labels": [], "entities": []}], "abstractContent": [{"text": "Surface Realization Shared Task 2018 is a workshop on generating sentences from lemmatized sets of dependency triples.", "labels": [], "entities": [{"text": "Surface Realization Shared Task", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.836411327123642}]}, {"text": "This paper describes the results of our participation in the challenge.", "labels": [], "entities": []}, {"text": "We develop a data-driven pipeline system which first orders the lemmas and then conjugates the words to finish the surface realization process.", "labels": [], "entities": []}, {"text": "Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language generation (NLG) is the task of generating natural language utterances from textual inputs or structured data representations.", "labels": [], "entities": [{"text": "Natural Language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.773009642958641}]}, {"text": "For many years one of the research foci in the NLG community has been Surface Realization (SR) -the process of transforming a sentence plan into a linearly-ordered, grammatical string of morphologically inflected words).", "labels": [], "entities": [{"text": "Surface Realization (SR) -the process of transforming a sentence plan into a linearly-ordered, grammatical string of morphologically inflected words", "start_pos": 70, "end_pos": 218, "type": "Description", "confidence": 0.7814514481503031}]}, {"text": "The SR Shared Task is aimed at developing a common input representation that could be used by a variety of NLG systems to generate realizations from).", "labels": [], "entities": []}, {"text": "In the case of the Surface Realization Shared Task 2018 () there are two different representations the contestants can use, depending on the track they participate in: Shallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information as found in the Universal Dependencies (UD) annotations (version 2.0).", "labels": [], "entities": [{"text": "Surface Realization Shared Task", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.8150792121887207}]}, {"text": "1 1 http://universaldependencies.org/ Deep Track: same as above, but having functional words and morphological features removed.", "labels": [], "entities": []}, {"text": "We participated in the shallow track, and therefore our task was to generate a sentence by ordering the lemmas and inflecting them to the correct surface forms.", "labels": [], "entities": []}, {"text": "The outputs of the participating systems are assessed using both automatic and manual evaluation.", "labels": [], "entities": []}, {"text": "The former is performed by computing BLEU (), NIST),) scores and normalized string edit distance (EDIST) between the reference sentence and a system output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9982151985168457}, {"text": "normalized string edit distance (EDIST)", "start_pos": 65, "end_pos": 104, "type": "METRIC", "confidence": 0.7330209485122136}]}, {"text": "Manual evaluation is based on preference judgments: third-year undergraduate students from Cambridge, Oxford and Edinburgh rate pairs of candidate outputs (including the target sentence), scoring them for Clarity, Fluency and Meaning Similarity.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 205, "end_pos": 212, "type": "METRIC", "confidence": 0.9484301805496216}, {"text": "Fluency", "start_pos": 214, "end_pos": 221, "type": "METRIC", "confidence": 0.9469147324562073}]}, {"text": "The data used for the task is the UD treebanks distributed in the 10-column CoNLL-U format.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.7502598166465759}]}, {"text": "The data is available for Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish.", "labels": [], "entities": []}, {"text": "According to the requirements of the Shallow Track, the information on word order was removed by randomized scrambling of the token sequence; the words were also replaced by their lemmas.", "labels": [], "entities": [{"text": "Shallow Track", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.908985823392868}]}, {"text": "Our contribution is a simple method of dependency tree linearization which orders a bag of lemmas based on the available syntactic information.", "labels": [], "entities": []}, {"text": "The major limitation of the method is its input order sensitivity; solving this problem is reserved for future work.", "labels": [], "entities": []}, {"text": "Our paper has the following structure.", "labels": [], "entities": []}, {"text": "Section 2 describes related work done in the past.", "labels": [], "entities": []}, {"text": "Section 3 presents the results of the exploratory data analysis conducted prior to system development.", "labels": [], "entities": []}, {"text": "The details of our system architecture are specified in Section 4 which is followed by the description of the experimental setup and evaluation (Section 5).", "labels": [], "entities": []}, {"text": "Section 6 mentions the limitations of the proposed surface realization method and outlines future work directions.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7636099457740784}]}], "datasetContent": [{"text": "Training data was filtered to exclude outliers according to the results of the data analysis (Section 3).", "labels": [], "entities": []}, {"text": "The system components were trained separately ten times with different random seeds.", "labels": [], "entities": []}, {"text": "In this section, we report mean scores and standard deviation for each model evaluated on the development data and averaged across the random seed values.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 43, "end_pos": 61, "type": "METRIC", "confidence": 0.9402304291725159}]}, {"text": "The evaluation of the proposed approach was done both independently for each of the single components and as a whole in the pipeline mode.", "labels": [], "entities": []}, {"text": "All the results are computed on the tokenized data instances.", "labels": [], "entities": []}, {"text": "We start with the evaluation of the morphological inflection generator, and report the exact string match accuracy for each of the tested approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.5682547688484192}]}, {"text": "Two simple baselines were developed for the experiment: given a lemma, LEMMA copies the lemma itself as a prediction of the surface form, MAJOR outputs the most frequent surface form if the lemma is not an OOV item, or the lemma itself, otherwise.", "labels": [], "entities": [{"text": "LEMMA", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9726364612579346}]}, {"text": "Lemma-form frequencies were computed on the training data.", "labels": [], "entities": [{"text": "Lemma-form", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9669846296310425}]}, {"text": "For the baselines, we report caseinsensitive scores only; the results can be easily extrapolated to the case-sensitive scenario.", "labels": [], "entities": []}, {"text": "As expected, the baselines are outperformed by all data-driven methods examined.", "labels": [], "entities": []}, {"text": "Strong performance of the majority baseline for English and Dutch data can be attributed to the simpler morphology of the languages.", "labels": [], "entities": []}, {"text": "The best results are achieved by the model of Aharoni and Goldberg (2017) (MORPHRNNHARD), which outperforms all other methods across all languages.", "labels": [], "entities": [{"text": "MORPHRNNHARD", "start_pos": 75, "end_pos": 87, "type": "METRIC", "confidence": 0.9856386780738831}]}, {"text": "Despite the fact that the approach has a bias towards languages with concatenative morphology (due to the assumption of the monotonic alignment between the input and output character sequences), it also performs well on Arabic.", "labels": [], "entities": []}, {"text": "This model was chosen for our further pipeline experiments.", "labels": [], "entities": []}, {"text": "Bad sample complexity of the soft attention model (MORPHRNNSOFT) explains its inferior performance compared to the hard attention model.", "labels": [], "entities": [{"text": "MORPHRNNSOFT", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.8906453251838684}]}, {"text": "MORPHRNNSOFT model seems to be highly sensitive to the different values of hyperparameters; its performance has the highest standard deviation among all models, which is most likely due to the same sample complexity issue.", "labels": [], "entities": [{"text": "MORPHRNNSOFT", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6653077006340027}]}, {"text": "Interestingly enough, on English, French, Italian and Dutch data the multi-layer perceptron architecture (MORPHMLP) achieves better results.", "labels": [], "entities": []}, {"text": "The latter has a considerably simpler, but less flexible structure, which prohibits the usage of such networks for languages with rich morphology -the number of parameters needed to account for various forms and morphological features grows rapidly until the model can no longer fit into the memory.", "labels": [], "entities": []}, {"text": "This also highlights the importance of cross-lingual evaluation of morphological analyzers and generators.", "labels": [], "entities": []}, {"text": "In order to better understand the most common errors made by each of the approaches (excluding the baselines), we examined the predictions of the models on the English development set.", "labels": [], "entities": [{"text": "English development set", "start_pos": 160, "end_pos": 183, "type": "DATASET", "confidence": 0.9044893582661947}]}, {"text": "We filtered out incorrect predictions of capitalization of.", "labels": [], "entities": []}, {"text": "Unlike character-based models, MORPHMLP treats each surface form as anatomic unit and is therefore prone to errors caused by the data sparsity issues, failing to predict correct forms for unseen lemmas or unseen grammar patterns (wrong lemma error type).", "labels": [], "entities": []}, {"text": "If the model correctly identifies the base form and still makes a mistake, in half of the cases it is an incorrect prediction of verb tenses, singular/plural noun forms or indefinite English articles (wrong form).", "labels": [], "entities": []}, {"text": "The latter cases are caused by the fact that our model does not use any information about the next token when predicting the form of the current lemma.", "labels": [], "entities": []}, {"text": "This limitation is inherent to the pipeline architecture we employed and can be accounted for in a joint morphology/syntax modeling scenario.", "labels": [], "entities": []}, {"text": "Finally, there are also cases where a model predicts an alternative surface form which does not match the ground truth, but is grammatically correct (alt.", "labels": [], "entities": []}, {"text": "Strictly speaking, the latter cases are not errors, but for simplicity we will treat them as such in this section.", "labels": [], "entities": []}, {"text": "MORPHRNNSOFT model predicts fewer wrong morphological variants, but suffers from another problem -hallucinating non-existing surface forms: \"singed\" instead of \"sung\", \"dened\" instead of \"denied\", \"siaseme\" vs. \"siamese\".", "labels": [], "entities": [{"text": "MORPHRNNSOFT", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.6391798853874207}]}, {"text": "This is not surprising, given the sequential nature of the model; usually this happens in cases with flat probability distributions over a number of possible characters following the already predicted character sequence.", "labels": [], "entities": []}, {"text": "A large portion of such errors includes incorrect spellings of proper nouns (proper noun err): \"Jersualm\" vs. \"Jerusalem\", \"Mconal\" instead of \"McDonal\".", "labels": [], "entities": []}, {"text": "Finally, one prominent group of errors is that of incorrect digit sequences.", "labels": [], "entities": []}, {"text": "MORPHMLP does not make these mistakes, because it uses a heuristic: OOV lemmas are copied verbatim as predictions of the surface forms.", "labels": [], "entities": [{"text": "MORPHMLP", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9372649192810059}]}, {"text": "The majority of erroneous cases for MOR-PHRNNHARD model constitute the group of alternative forms.", "labels": [], "entities": [{"text": "MOR-PHRNNHARD", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.5116137266159058}]}, {"text": "Compared to other models, there are considerably fewer cases of predicting non-existent forms (\"allergys\", \"goining\").", "labels": [], "entities": []}, {"text": "The wrong form error type is mainly represented by incorrect predictions of verb forms: \"sing\" instead of \"sung\", \"got\" instead of \"gotten\", \"are\" instead of \"'m\", etc.", "labels": [], "entities": []}, {"text": "The results of the error analysis suggest that there is still a large room for improvement of the morphological inflection generation component.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6142157514890035}]}, {"text": "A principled approach to handling unseen tokens and away to constrain the predictions to well-formed outputs would be interesting directions to investigate further.", "labels": [], "entities": []}, {"text": "The syntactic component has been evaluated by computing system-level BLEU, NIST and edit distance scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9877781271934509}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8142099380493164}, {"text": "edit distance scores", "start_pos": 84, "end_pos": 104, "type": "METRIC", "confidence": 0.6901870767275492}]}, {"text": "Following the official evaluation protocol, output texts were normalized prior to computing metrics by lower-casing all tokens.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, surface realization systems have not been evaluated on all the data used in the shared task.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7142801880836487}]}, {"text": "A simple baseline (RAND) which outputs a random permutation of the sentence tokens performs poorly across all languages.", "labels": [], "entities": []}, {"text": "Compared to it, the 74.88% of the development data sentences ordered correctly by our method seem to indicate a good performance.", "labels": [], "entities": []}, {"text": "To get an idea of where our approach breaks, we sampled a few erroneous predictions and examined them manually.", "labels": [], "entities": []}, {"text": "Generally speaking, the syntactic ordering procedure works well on the deeper tree levels, but as we move up, it gets harder to account for the many descendants anode has.", "labels": [], "entities": [{"text": "syntactic ordering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7009382396936417}]}, {"text": "An example of this error mode is given in.", "labels": [], "entities": []}, {"text": "We tried to improve the prediction capabilities of the system by incorporating feature representations of the leftmost and the rightmost descendant nodes and conditioning the model on the previous pre-  Every move Google makes brings this particular future closer.", "labels": [], "entities": []}, {"text": "Every move Google makes closer brings this particular future.", "labels": [], "entities": []}, {"text": "Figure 5: A common error our syntactic ordering component makes.", "labels": [], "entities": []}, {"text": "The node in the rectangle is current head, the node in the oval indicates its child for which the position prediction was incorrect.", "labels": [], "entities": []}, {"text": "The upper sentence is the gold ordering, the one below is predicted by our system.", "labels": [], "entities": []}, {"text": "dictions, but this did not yield any improvements.", "labels": [], "entities": []}, {"text": "Further investigation with regard to this issue is reserved for future work.", "labels": [], "entities": []}, {"text": "shows the metric evaluation results of the pipeline on the development and test data provided by the organizers (Dev-SR and Test-SR), as well as the development data from the original UD dataset, which was used in our preliminary experiments (Dev-UD).", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 184, "end_pos": 194, "type": "DATASET", "confidence": 0.756420910358429}]}, {"text": "Given the large gap between the system performance on Dev-SR and Dev-UD, we manually inspected the predictions and observed that the Dev-SR outputs were less grammatical than those made for the Dev-UD data.", "labels": [], "entities": [{"text": "Dev-UD data", "start_pos": 194, "end_pos": 205, "type": "DATASET", "confidence": 0.9180554449558258}]}, {"text": "We investigated the issue and discovered that the morphological component worked as expected, but the syntactic ordering module was flawed.", "labels": [], "entities": []}, {"text": "The proposed method's performance varies depending on the children nodes' order returned by the BFS procedure (line 4 of Algorithm 1).", "labels": [], "entities": [{"text": "BFS", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.7644497752189636}]}, {"text": "shows an example where our right 's folks . That ,: An example sentence which poses a challenge to our system: \"That 's right , folks .\" system fails.", "labels": [], "entities": []}, {"text": "It is easier to determine the order of node's children starting with content words and then inserting punctuation signs; if it is the other way round, ordering tokens becomes harder.", "labels": [], "entities": []}, {"text": "As mentioned in Section 3, we have used the original UD training and development data which contains token information in the natural order of token occurrence in the sentences.", "labels": [], "entities": []}, {"text": "However, in the shared task data the word order information was removed by randomized scrambling of the tokens, which made it harder for the syntactic linearizer to make predictions on Dev-SR and Test-SR.", "labels": [], "entities": []}, {"text": "Unfortunately, we did not anticipate that this will have such a great influence on the prediction capabilities of the proposed approach.", "labels": [], "entities": [{"text": "prediction", "start_pos": 87, "end_pos": 97, "type": "TASK", "confidence": 0.9387468695640564}]}, {"text": "We plan to investigate ways of improving it in future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-lingual data analysis.", "labels": [], "entities": [{"text": "Cross-lingual data analysis", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6478105187416077}]}, {"text": " Table 2: Evaluation of the morphological inflection system component on the original UD development  set using the percentage of exact string matches as a metric. For the neural architectures, we report both  case-sensitive and case-insensitive mean scores and standard deviation (averaged across ten random seed  values).", "labels": [], "entities": [{"text": "UD development  set", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.8735429048538208}, {"text": "standard deviation", "start_pos": 262, "end_pos": 280, "type": "METRIC", "confidence": 0.9416039884090424}]}, {"text": " Table 3: Major error types made by each of the tested morphological component models.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of the syntactic ordering component on the original UD development set. We report  mean scores and standard deviation for the SYNMLP model; the scores were averaged over ten models  trained with different random seeds. RAND is the random baseline. The scores are case-insensitive.", "labels": [], "entities": [{"text": "UD development set", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.7519023617108663}]}, {"text": " Table 5: Final metric evaluation results of the system pipeline. Dev-UD denotes the development set of  the original UD dataset. Dev-SR and Test-SR is the data provided by the organizers (with scrambled  lemmas).", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.7325553447008133}]}]}