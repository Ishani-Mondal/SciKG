{"title": [{"text": "Learning to Write with Cooperative Discriminators", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory.", "labels": [], "entities": []}, {"text": "We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discrimina-tors that can guide abase RNN generator towards more globally coherent generations.", "labels": [], "entities": []}, {"text": "More concretely, discriminators each specialize in a different principle of communication, such as Grice's maxims, and are collectively combined with the base RNN generator through a composite decoding objective.", "labels": [], "entities": []}, {"text": "Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models based on Recurrent Neural Networks (RNNs) have brought substantial advancements across a wide range of language tasks (Jozefowicz et al., 2016;.", "labels": [], "entities": []}, {"text": "However, when used for longform text generation, RNNs often lead to degenerate text that is repetitive, self-contradictory, and overly generic, as shown in.", "labels": [], "entities": [{"text": "longform text generation", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.733916163444519}]}, {"text": "We propose a unified learning framework that can address several challenges of long-form text generation by composing a committee of discriminators each specializing in a different principle of communication.", "labels": [], "entities": [{"text": "text generation", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7526963949203491}]}, {"text": "Starting with an RNN language model, our framework learns to construct a more powerful generator by training a number of discriminative models that can collectively address limitations of the base RNN generator, and Context: The two guards thudded into the door on the other side and began pounding on it, shouting furiously.", "labels": [], "entities": []}, {"text": "Scious raised the large bunch of keys then placed them in a large pocket in his coat.", "labels": [], "entities": []}, {"text": "\"Come, we have to go,\" he whispered and moved up the corridor, the guards still hammering on the door.", "labels": [], "entities": []}, {"text": "The table next to the room they had been in was bare.", "labels": [], "entities": []}], "datasetContent": [{"text": "We pose the evaluation of our model as the task of generating an appropriate continuation given an initial context.", "labels": [], "entities": []}, {"text": "In our open-ended generation setting the continuation is not required to be a specific length, so we require our models and baselines to generate 5-sentence continuations, consistent with the way the discriminator and seq2seq baseline datasets are constructed.", "labels": [], "entities": []}, {"text": "Previous work has reported that automatic mea-sures such as BLEU () and Meteor do not lead to meaningful evaluation when used for long or creative text generation where there can be high variance among acceptable generation outputs ( For open-ended generation tasks such as our own, human evaluation has been found to be the only reliable measure ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9940088987350464}]}, {"text": "For human evaluation, two possible endings are presented to a human, who assesses the text according to several criteria, which are closely inspired by Grice's Maxims: repetition, contradiction, relevance and clarity.", "labels": [], "entities": [{"text": "repetition", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9374589920043945}, {"text": "clarity", "start_pos": 209, "end_pos": 216, "type": "METRIC", "confidence": 0.9786188006401062}]}, {"text": "See supplementary material for examples of the evaluation forms we used.", "labels": [], "entities": []}, {"text": "For each criterion, the two continuations are compared using a 5-point Likert scale, to which we assign numerical values of \u22122 to 2.", "labels": [], "entities": []}, {"text": "The scale measures whether one generation is strongly or somewhat preferred above the other, or whether they are equal.", "labels": [], "entities": []}, {"text": "Finally, the human is asked to make a judgement about overall quality: which ending is better, or are they of equal qual-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for automatic evaluation metrics for all systems and domains, using the original con- tinuation as the reference. The metrics are: Length -Average total length per example; Trigrams -%  unique trigrams per example; Vocab -% unique words per example.", "labels": [], "entities": [{"text": "Length -Average total length", "start_pos": 149, "end_pos": 177, "type": "METRIC", "confidence": 0.8799881219863892}]}, {"text": " Table 2: Results of crowd-sourced evaluation on different aspects of the generation quality as well as  overall quality judgments. For each sub-criteria we report the average of comparative scores on a scale  from -2 to 2. For the overall quality evaluation decisions are aggregated over 3 annotators per example.", "labels": [], "entities": []}, {"text": " Table 4: Crowd-sourced ablation evaluation of generations on TripAdvisor. Each ablation uses only one  discriminative communication model, and is compared to ADAPTIVELM.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9737982749938965}]}]}