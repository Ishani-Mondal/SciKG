{"title": [{"text": "Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification", "labels": [], "entities": [{"text": "Large Scale e-Commerce Text Classification", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.5829047679901123}]}], "abstractContent": [{"text": "The lack of high-quality labeled training data has been one of the critical challenges facing many industrial machine learning tasks.", "labels": [], "entities": []}, {"text": "To tackle this challenge, in this paper , we propose a semi-supervised learning method to utilize unlabeled data and user feedback signals to improve the performance of ML models.", "labels": [], "entities": [{"text": "ML", "start_pos": 169, "end_pos": 171, "type": "TASK", "confidence": 0.9720670580863953}]}, {"text": "The method employs a primary model M ain and an auxiliary evaluation model Eval, where M ain and Eval models are trained iteratively by automatically generating labeled data from unlabeled data and/or users feedback signals.", "labels": [], "entities": []}, {"text": "The proposed approach is applied to different text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8531208038330078}]}, {"text": "We report results on both the publicly available Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.920815646648407}]}, {"text": "Answers dataset and our e-commerce product classification dataset.", "labels": [], "entities": [{"text": "Answers dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8773391842842102}, {"text": "e-commerce product classification", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.689165989557902}]}, {"text": "The experimental results show that the proposed method reduces the classification error rate by 4% and up to 15% across various experimental setups and datasets.", "labels": [], "entities": [{"text": "classification error rate", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.821259061495463}]}, {"text": "A detailed comparison with other semi-supervised learning approaches is also presented later in the paper.", "labels": [], "entities": []}, {"text": "The results from various text classification tasks demonstrate that our method outperforms those developed in previous related studies.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.8351941704750061}]}], "introductionContent": [{"text": "There are many ways to improve the performance of a machine learning model.", "labels": [], "entities": []}, {"text": "Improving the training data is one such method.", "labels": [], "entities": []}, {"text": "Obtaining highquality training data, such as human labeled data, is usually expensive and time-consuming.", "labels": [], "entities": []}, {"text": "Many machine learning systems use unlabeled data or a mixture of labeled and unlabeled data for training because it is cheaper and easier to collect enormous amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "Industrydeployed machine learning systems that serve millions of users generate vast amounts of unlabeled data and noisy user feedback signals everyday.", "labels": [], "entities": []}, {"text": "Those data and signals are very important and can be utilized in the training of real-world machine learning systems.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew semi-supervised learning method with a feedback loop to leverage vast amounts of unlabeled data and feedback signals.", "labels": [], "entities": []}, {"text": "In particular, we train two machine learning models iteratively.", "labels": [], "entities": []}, {"text": "The main model, which is represented as M ain, performs the main task at runtime.", "labels": [], "entities": [{"text": "M ain", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.7070844173431396}]}, {"text": "The auxiliary model, which is represented as Eval, works offline and it estimates the correctness of the M ain models output.", "labels": [], "entities": [{"text": "Eval", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.7014881372451782}]}, {"text": "The information available to the auxiliary model Eval is much richer than the run-time model M ain.", "labels": [], "entities": [{"text": "Eval", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9177599549293518}]}, {"text": "Extra data, such as user feed-back data and session context information, can be used when training the auxiliary model.", "labels": [], "entities": []}, {"text": "The idea is to control the false positive rate of Eval to produce high-quality, automatically labeled data from unlabeled data.", "labels": [], "entities": [{"text": "false positive rate", "start_pos": 27, "end_pos": 46, "type": "METRIC", "confidence": 0.7533836762110392}, {"text": "Eval", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.8185539245605469}]}, {"text": "The entire process runs iteratively and the performance of both models is improved in an iterative manner.", "labels": [], "entities": []}, {"text": "The assumption of run-time M ain model has much fewer available information is due to the business logic flow and/or UX design constraints which limit the run-time M ain model to collect richer features in some industry setups.", "labels": [], "entities": []}, {"text": "In this paper, we use text classification experiments to illustrate the proposed approach.", "labels": [], "entities": [{"text": "text classification", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.7521611154079437}]}, {"text": "However, this semi-supervised learning approach can also be applied to other machine learning tasks, such as machine translation and search relevance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8287921845912933}]}, {"text": "Different semi-supervised learning approaches have been previously proposed to leverage unlabeled data, including,, and.", "labels": [], "entities": []}, {"text": "Experimental results on the public Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.8979492783546448}]}, {"text": "Answers dataset and on anew public e-commerce dataset for product classification demonstrate the advantages and potential of the proposed framework compared with previous work.", "labels": [], "entities": [{"text": "product classification", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7290749847888947}]}, {"text": "The contributions of this work are as follows: \u2022 A new semi-supervised learning method with the introducing of an auxiliary evaluation component, and \u2022 A scalable, cost-effective and efficient way to convert vast amounts of unlabeled data into high quality labeled data for supervised training purposes.", "labels": [], "entities": []}, {"text": "In section 2, we present the details of the proposed semi-supervised learning approach in the context of text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.8639915982882181}]}, {"text": "In section 3, the theoretical analysis of the proposed method is provided.", "labels": [], "entities": []}, {"text": "Next, we give an overview of related works and highlight their differences compared to our approach.", "labels": [], "entities": []}, {"text": "Section 5 defines various experimental setups and presents the results for two different datasets.", "labels": [], "entities": []}, {"text": "Finally, we present the papers conclusions in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The auxiliary evaluation model Eval is a binary classifier that predicts whether the automatic label is corrector not.", "labels": [], "entities": []}, {"text": "It is trained using gradient boosting 2 . If the false positive rate of the eval-  Given:  To illustrate the effectiveness of the proposed semi-supervised learning method, we evaluate it with different text classification tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.7713436186313629}]}, {"text": "We compare the new method with a few other benchmark semi-supervised approaches using the public Yahoo!", "labels": [], "entities": []}, {"text": "Answers topic classification dataset ; and our e-commerce product categorization dataset.", "labels": [], "entities": [{"text": "Answers topic classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7921769618988037}]}, {"text": "Answers topic classification dataset contains 10 classes.", "labels": [], "entities": [{"text": "Answers topic classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7683696150779724}]}, {"text": "Each class contains 140K training samples and 6K testing samples.", "labels": [], "entities": []}, {"text": "In this dataset, the total number of training instances is 1.4M and total number of test instances is 60K . We shuffle and split the original 1.4 M labeled training data into two sets.", "labels": [], "entities": []}, {"text": "The first set contains 100K instances with labels and is used as the initial labeled dataset L.", "labels": [], "entities": []}, {"text": "The second set contains 1.3 M instances and the labels are deleted to form the unlabeled dataset U . The 60K test samples are untouched as the blind test set T .  The proposed method is derived to tackle large scale text classification problems that occur in the e-Commerce industry, where the challenge is that we significantly lacked high-quality labeled data for these problems.", "labels": [], "entities": [{"text": "text classification", "start_pos": 216, "end_pos": 235, "type": "TASK", "confidence": 0.722953587770462}]}, {"text": "For example, the e-commerce product categorization dataset contains product titles and 600 different categories for the product titles.", "labels": [], "entities": []}, {"text": "This dataset contains four different parts: the product category description data for 600 categories, a 6K observation manually labeled initial training dataset L, a 28K observation manually labeled blind test set T , and a 3.5 million observation unlabeled dataset U that included rich user feedback session data F . The main task here is to predict the product category as soon as the online user enters the product title.", "labels": [], "entities": []}, {"text": "For example, the user might enter a product title, such as green coach bag to describe a product.", "labels": [], "entities": []}, {"text": "The system should classify this input title into the most relevant product category, such as \"women's purse & bag\".", "labels": [], "entities": []}, {"text": "The 3.5 million unlabeled user behavior dataset contains a seller chosen category and a category suggested by a machine learning model.", "labels": [], "entities": []}, {"text": "We consider these data to be unlabeled since the seller chosen category has a greater than 30% error rate according to our evaluations.", "labels": [], "entities": [{"text": "error rate", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9824954867362976}]}, {"text": "The reason for this high error rate is due to the fact that the users are not familiar with the category tree or they just intentionally select the wrong category to increase the chance of selling their product.", "labels": [], "entities": [{"text": "error rate", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9681608378887177}]}, {"text": "Note that for the maintask system that runs online, only the product title information is available to main-task model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracy and error reduction rate [%]  on the Yahoo! Answers dataset. The method pro- posed in this study is printed in bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9813637733459473}, {"text": "error reduction rate", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.9234657883644104}, {"text": "Yahoo! Answers dataset", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.9477130323648453}]}, {"text": " Table 2: EMAEC gain in error reduction rate [%]  compared to the co-training baseline on the e- commerce dataset", "labels": [], "entities": [{"text": "EMAEC", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.6652692556381226}, {"text": "error reduction rate", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.9469196399052938}, {"text": "e- commerce dataset", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.6784977540373802}]}]}