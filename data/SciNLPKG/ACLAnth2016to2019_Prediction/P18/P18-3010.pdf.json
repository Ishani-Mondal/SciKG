{"title": [{"text": "SuperNMT: Neural Machine Translation with Semantic Supersenses and Syntactic Supertags", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6270393828550974}]}], "abstractContent": [{"text": "In this paper we incorporate semantic su-persensetags and syntactic supertag features into EN-FR and EN-DE factored NMT systems.", "labels": [], "entities": []}, {"text": "In experiments on various test sets, we observe that such features (and particularly when combined) help the NMT model training to converge faster and improve the model quality according to the BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.998971700668335}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) models have recently become the state-of-the art in the field of Machine Translation ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7714325288931528}, {"text": "Machine Translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.8718286454677582}]}, {"text": "Compared to Statistical Machine Translation (SMT), the previous state-of-the-art, NMT performs particularly well when it comes to word-reorderings and translations involving morphologically rich languages (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.8362173438072205}]}, {"text": "Although NMT seems to partially 'learn' or generalize some patterns related to syntax from the raw, sentence-aligned parallel data, more complex phenomena (e.g. prepositional-phrase attachment) remain problematic ().", "labels": [], "entities": [{"text": "prepositional-phrase attachment", "start_pos": 161, "end_pos": 192, "type": "TASK", "confidence": 0.7260038256645203}]}, {"text": "More recent work showed that explicitly () or implicitly () modeling extra syntactic information into an NMT system on the source (and/or target) side could lead to improvements in translation quality.", "labels": [], "entities": []}, {"text": "When integrating linguistic information into an MT system, following the central role assigned to syntax by many linguists, the focus has been mainly on the integration of syntactic features.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9820578098297119}]}, {"text": "Although there has been some work on semantic features for SMT (), so far, no work has been done on enriching NMT systems with more general semantic features at the word-level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9933168888092041}]}, {"text": "This might be explained by the fact that NMT models already have means of learning semantic similarities through word-embeddings, where words are represented in a common vector space ().", "labels": [], "entities": []}, {"text": "However, making some level of semantics more explicitly available at the word level can provide the translation system with a higher level of abstraction beneficial to learn more complex constructions.", "labels": [], "entities": []}, {"text": "Furthermore, a combination of both syntactic and semantic features would provide the NMT system with away of learning semantico-syntactic patterns.", "labels": [], "entities": []}, {"text": "To apply semantic abstractions at the word-level that enable a characterisation beyond that what can be superficially derived, coarse-grained semantic classes can be used.", "labels": [], "entities": []}, {"text": "Inspired by Named Entity Recognition which provides such abstractions fora limited set of words, supersense-tagging uses an inventory of more general semantic classes for domain-independent settings.", "labels": [], "entities": []}, {"text": "We investigate the effect of integrating supersense features (26 for nouns, 15 for verbs) into an NMT system.", "labels": [], "entities": []}, {"text": "To obtain these features, we used the AMALGrAM 2.0 tool () which analyses the input sentence for Multi-Word Expressions as well as noun and verb supersenses.", "labels": [], "entities": []}, {"text": "The features are integrated using the framework of , replicating the tags for every subword unit obtained by byte-pair encoding (BPE).", "labels": [], "entities": []}, {"text": "We further experiment with a combination of semantic supersenses and syntactic supertag features (CCG syntactic categories) using EasySRL () and less complex features such as POS-tags, assuming that supersense-tags have the potential to be useful especially in combination with syntactic information.", "labels": [], "entities": [{"text": "EasySRL", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.8719749450683594}]}, {"text": "The remainder of this paper is structured as follows: First, in Section 2, the related work is discussed.", "labels": [], "entities": []}, {"text": "Next, Section 3 presents the semantic and syntactic features used.", "labels": [], "entities": []}, {"text": "The experimental set-up is described in Section 4 followed by the results in Section 5.", "labels": [], "entities": []}, {"text": "Finally, We conclude and present some of the ideas for future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Best BLEU scores for Baseline (BPE), Syntac-", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9987265467643738}, {"text": "Baseline (BPE)", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.4958789274096489}]}, {"text": " Table 2: Best BLEU scores for Baseline (BPE), Syntac-", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9986175298690796}, {"text": "Baseline (BPE)", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.49966711550951004}]}]}