{"title": [{"text": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling", "labels": [], "entities": [{"text": "Predicting Predicates and Arguments", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.8507112860679626}, {"text": "Neural Semantic Role Labeling", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.5951745435595512}]}], "abstractContent": [{"text": "Recent BIO-tagging-based neural semantic role labeling models are very high performing , but assume gold predicates as part of the input and cannot incorporate span-level features.", "labels": [], "entities": [{"text": "BIO-tagging-based neural semantic role labeling", "start_pos": 7, "end_pos": 54, "type": "TASK", "confidence": 0.6521015942096711}]}, {"text": "We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them.", "labels": [], "entities": []}, {"text": "The model makes independent decisions about what relationship , if any, holds between every possible word-span pair, and learns contextu-alized span representations that provide rich, shared input features for each decision.", "labels": [], "entities": []}, {"text": "Experiments demonstrate that this approach sets anew state of the art on PropBank SRL without gold predicates.", "labels": [], "entities": [{"text": "PropBank SRL", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.9206998348236084}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) captures predicateargument relations, such as \"who did what to whom.\"", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8506307005882263}]}, {"text": "Recent high-performing SRL models are BIO-taggers, labeling argument spans fora single predicate at a time (as shown in).", "labels": [], "entities": [{"text": "SRL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9824233055114746}]}, {"text": "They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment.", "labels": [], "entities": []}, {"text": "We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass.", "labels": [], "entities": [{"text": "predicting all the predicates", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7977994829416275}]}, {"text": "Our model builds on a recent coreference resolution model ( , by making central use of learned, contextualized span representations.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8287537395954132}]}, {"text": "We use these representations to predict SRL graphs directly over text spans.", "labels": [], "entities": [{"text": "SRL graphs", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9032533764839172}]}, {"text": "Each edge is identified by independently predicting which role, if any, holds between every possible pair of text spans, while using aggressive beam Code and models: https://github.com/luheng/lsgn pruning for efficiency.", "labels": [], "entities": []}, {"text": "The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes).", "labels": [], "entities": [{"text": "SRL roles", "start_pos": 49, "end_pos": 58, "type": "TASK", "confidence": 0.8885816037654877}]}, {"text": "Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (: it can model overlapping spans across different predicates in the same output structure (see).", "labels": [], "entities": []}, {"text": "The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given.", "labels": [], "entities": [{"text": "SRL", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9002313613891602}]}, {"text": "In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 122, "end_pos": 130, "type": "DATASET", "confidence": 0.9779585599899292}]}, {"text": "It also reinforces the strong performance of similar span embedding methods for coreference ( , suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing, relation extraction (, and QA-SRL ().", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.7225983440876007}, {"text": "relation extraction", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.863163411617279}]}], "datasetContent": [{"text": "We ELMo embeddings To further improve performance, we also add ELMo word representations () to the BiLSTM input (in the +ELMo rows).", "labels": [], "entities": []}, {"text": "Since the contextualized representations ELMo provides can be applied to most previous neural systems, the improvement is orthogonal to our contribution.", "labels": [], "entities": []}, {"text": "In   Effectiveness of beam pruning shows the predicate and argument spans kept in the beam, sorted with their unary scores.", "labels": [], "entities": []}, {"text": "Our model efficiently prunes unlikely argument spans and predicates, significantly reduces the number of edges it needs to consider.", "labels": [], "entities": []}, {"text": "shows the recall of predicate words on the CoNLL 2012 development set.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9978439807891846}, {"text": "CoNLL 2012 development set", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.971295177936554}]}, {"text": "By retaining \u03bb p = 0.4 predicates per word, we are able to keep over 99.7% argument-bearing predicates.", "labels": [], "entities": []}, {"text": "Compared to having a part-of-speech tagger (POS:X in), our joint beam pruning allowing the model to have a soft trade-off between efficiency and recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9976886510848999}]}, {"text": "4 Long-distance dependencies shows the performance breakdown by binned distance between arguments to the given predicates.", "labels": [], "entities": []}, {"text": "Our model is better at accurately predicting arguments that are farther away from the predicates, even  compared to an ensemble model ( ) that has a higher overall F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 164, "end_pos": 166, "type": "METRIC", "confidence": 0.9983386993408203}]}, {"text": "This is very likely due to architectural differences; in a BIO tagger, predicate information passes through many LSTM timesteps before reaching a long-distance argument, whereas our architecture enables direct connections between all predicates-arguments pairs.", "labels": [], "entities": []}, {"text": "Agreement with syntax As mentioned in , their BIO-based SRL system has good agreement with gold syntactic span boundaries (94.3%) but falls short of previous syntaxbased systems shows its viola- tions of global structural constraints 5 compared to previous systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.6434774994850159}]}, {"text": "Our model made more constraint violations compared to previous systems.", "labels": [], "entities": []}, {"text": "For example, our model predicts duplicate core arguments 6 (shown in the U column in) more often than previous work.", "labels": [], "entities": []}, {"text": "This is due to the fact that our model uses independent classifiers to label each predicate-argument pair, making it difficult for them to implicitly track the decisions made for several arguments with the same predicate.", "labels": [], "entities": []}, {"text": "The Ours+decode row in shows SRL performance after enforcing the U-constraint using dynamic programming) at decoding time.", "labels": [], "entities": [{"text": "SRL", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9179668426513672}]}, {"text": "Constrained decoding attest time is effective at eliminating all the core-role inconsistencies (shown in the U-column), but did not bring significant gain on the end result (shown 5 described a list of global constraints for SRL systems, e.g., there can beat most one core argument of each type for each predicate.", "labels": [], "entities": [{"text": "SRL", "start_pos": 225, "end_pos": 228, "type": "TASK", "confidence": 0.9748817682266235}]}, {"text": "Arguments with labels ARG0,ARG1,.", "labels": [], "entities": [{"text": "ARG0,ARG1", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.6924673914909363}]}], "tableCaptions": [{"text": " Table 1: End-to-end SRL results for CoNLL 2005 and CoNLL 2012, compared to previous systems.  CoNLL 05 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).", "labels": [], "entities": [{"text": "SRL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8889718055725098}, {"text": "CoNLL 2005", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.9430692195892334}, {"text": "CoNLL 2012", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.8417391777038574}, {"text": "CoNLL 05", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.8783111274242401}, {"text": "WSJ", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.7062743306159973}]}, {"text": " Table 2: Experiment results with gold predicates.", "labels": [], "entities": []}, {"text": " Table 3: Comparison on the CoNLL 05 devel- opment set against previous systems in terms  of unlabeled agreement with gold constituency  (Syn%) and each type of SRL-constraints viola- tions (Unique core roles, Continuation roles and  Reference roles).", "labels": [], "entities": [{"text": "CoNLL 05 devel- opment", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.8498465180397033}]}]}