{"title": [{"text": "Neural Argument Generation Augmented with Externally Retrieved Evidence", "labels": [], "entities": [{"text": "Neural Argument Generation Augmented", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.876492902636528}]}], "abstractContent": [{"text": "High quality arguments are essential elements for human reasoning and decision-making processes.", "labels": [], "entities": []}, {"text": "However, effective argument construction is a challenging task for both human and machines.", "labels": [], "entities": [{"text": "argument construction", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8144250810146332}]}, {"text": "In this work, we study a novel task on automatically generating arguments of a different stance fora given statement.", "labels": [], "entities": []}, {"text": "We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia.", "labels": [], "entities": []}, {"text": "Our model first generates a set of talking point phrases as intermediate representation , followed by a separate decoder producing the final argument based on both input and the keyphrases.", "labels": [], "entities": []}, {"text": "Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than a popular sequence-to-sequence generation model according to both automatic evaluation and human assessments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Generating high quality arguments plays a crucial role in decision-making and reasoning processes.", "labels": [], "entities": []}, {"text": "A multitude of arguments and counter-arguments are constructed on a daily basis, both online and offline, to persuade and inform us on a wide range of issues.", "labels": [], "entities": []}, {"text": "For instance, debates are often conducted in legislative bodies to secure enough votes for bills to pass.", "labels": [], "entities": []}, {"text": "In another example, online deliberation has become a popular way of soliciting public opinions on new policies' pros and cons).", "labels": [], "entities": []}, {"text": "Nonetheless, constructing persuasive arguments is a daunting task, for both human and computers.", "labels": [], "entities": []}, {"text": "We believe that developing effective argument generation models will enable abroad range of compelling applications, including debate coaching, improving students' essay writing skills, and pro- viding context of controversial issues from different perspectives.", "labels": [], "entities": [{"text": "argument generation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8555223047733307}, {"text": "debate coaching", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.9694950580596924}, {"text": "essay writing", "start_pos": 164, "end_pos": 177, "type": "TASK", "confidence": 0.7014176100492477}]}, {"text": "As a consequence, there exists a pressing need for automating the argument construction process.", "labels": [], "entities": [{"text": "argument construction process", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.7812926371892294}]}, {"text": "To date, progress made in argument generation has been limited to retrieval-based methodsarguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (.", "labels": [], "entities": [{"text": "argument generation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7738728821277618}]}, {"text": "Although sentence ordering algorithms are developed for information structuring (, existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output.", "labels": [], "entities": [{"text": "sentence ordering", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.7571283578872681}, {"text": "information structuring", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.748496949672699}]}, {"text": "In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure.", "labels": [], "entities": [{"text": "argument generation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7875531017780304}]}, {"text": "One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of domains.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.8007415235042572}]}, {"text": "Especially, most previous NLG systems rely on tem-plates that are either constructed by rules, or acquired from a domain-specific corpus to enhance grammaticality and coherence.", "labels": [], "entities": []}, {"text": "This makes them unwieldy to be adapted for new domains.", "labels": [], "entities": []}, {"text": "In this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance.", "labels": [], "entities": []}, {"text": "To address the above challenges, we present a neural network-based argument generation framework augmented with externally retrieved evidence.", "labels": [], "entities": []}, {"text": "Our model is inspired by the observation that when humans construct arguments, they often collect references from external sources, e.g., Wikipedia or research papers, and then write their own arguments by synthesizing talking points from the references.", "labels": [], "entities": []}, {"text": "displays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who argue against the motion that \"government should be allowed to view private emails\".", "labels": [], "entities": [{"text": "Reddit subcommunity /r/ChangeMyView 1", "start_pos": 40, "end_pos": 77, "type": "DATASET", "confidence": 0.6893979821886335}]}, {"text": "Both replies leverage information drawn from Wikipedia, such as \"political corruption\" and \"Fourth Amendment on protections of personal privacy\".", "labels": [], "entities": []}, {"text": "Concretely, our neural argument generation model adopts the popular encoder-decoderbased sequence-to-sequence (seq2seq) framework (), which has achieved significant success in various text generation tasks (.", "labels": [], "entities": [{"text": "neural argument generation", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.6999575098355612}, {"text": "text generation", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.7553053498268127}]}, {"text": "Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia 2 . Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of \"talking points\", and the other then generates an argument based on both input and keyphrases.", "labels": [], "entities": [{"text": "English Wikipedia 2", "start_pos": 120, "end_pos": 139, "type": "DATASET", "confidence": 0.9120237827301025}]}, {"text": "Automatic evaluation based on BLEU) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models () that are also trained with retrieved evidence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9947302341461182}]}, {"text": "We further design a novel evaluation procedure to measure whether the arguments are on-topic by predicting their relevance to the given statement based on a separately trained relevance estimation model.", "labels": [], "entities": []}, {"text": "Results suggest that our model generated arguments are more likely to be predicted as on-topic, compared to other seq2seq-based generations models.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 highlights the roadmap of our system.", "labels": [], "entities": []}, {"text": "The dataset used for our study is introduced in Section 3.", "labels": [], "entities": []}, {"text": "The model formulation and retrieval methods are detailed in Sections 4 and 5.", "labels": [], "entities": [{"text": "model formulation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7149415016174316}]}, {"text": "We then describe the experimental setup and results in Sections 6 and 7, followed by further analysis and future directions in Section 8.", "labels": [], "entities": []}, {"text": "Related work is discussed in Section 9.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 10.", "labels": [], "entities": []}], "datasetContent": [{"text": "Encoding the full set of evidence by our current decoder takes a huge amount of time.", "labels": [], "entities": []}, {"text": "We there propose a sampling strategy to allow the encoder to finish encoding within reasonable time by considering only a subset of the evidence: For each sentence in the statement, up to three evidence sentences are randomly sampled from the retrieved set; then the sampled sentences are concatenated.", "labels": [], "entities": []}, {"text": "This procedure is repeated three times per statement, where a statement is an user argument for training data and an OP for test set.", "labels": [], "entities": [{"text": "OP", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9684398770332336}]}, {"text": "In our experiments, we remove duplicates samples and the ones without any retrieved evidence sentence.", "labels": [], "entities": []}, {"text": "Finally, we breakdown the augmented data into a training set of 224,553 examples (9,737 unique OPs), 13,911 for validation (640 OPs), and 30,417 retained for test (1,892 OPs).", "labels": [], "entities": []}, {"text": "For automatic evaluation, we use BLEU (), an n-gram precision-based metric (up to bigrams are considered), and ME-TEOR (), measuring unigram recall and precision by considering paraphrases, synonyms, and stemming.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9987720847129822}, {"text": "precision-based metric", "start_pos": 52, "end_pos": 74, "type": "METRIC", "confidence": 0.9565818309783936}, {"text": "ME-TEOR", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9662395715713501}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9287986159324646}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9911671280860901}]}, {"text": "Human arguments are used as the gold-standard.", "labels": [], "entities": []}, {"text": "Because each OP maybe paired with more than one highquality arguments, we compute BLEU and ME-TEOR scores for the system argument compared against all arguments, and report the best.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9990315437316895}, {"text": "ME-TEOR", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.993625283241272}]}, {"text": "We do not use multiple reference evaluation because: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval.", "labels": [], "entities": [{"text": "argument generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7298519909381866}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9977987408638}, {"text": "METEOR", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9604279398918152}]}, {"text": "The best performing model is highlighted in bold per metric.", "labels": [], "entities": []}, {"text": "Our separate decoder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization testing, p < 0.0001.", "labels": [], "entities": []}, {"text": "the arguments are often constructed from different angles and cover distinct aspects of the issue.", "labels": [], "entities": []}, {"text": "For models that generate more than one arguments based on different sets of sampled evidence, the one with the highest score is considered.", "labels": [], "entities": []}, {"text": "As can be seen from, our models produce better BLEU scores than almost all the comparisons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9993252754211426}]}, {"text": "Especially, our models with separate decoder yield significantly higher BLEU and ME-TEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9990491271018982}, {"text": "ME-TEOR", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9830923676490784}]}, {"text": "Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9932818412780762}, {"text": "RETRIEVAL", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9511284828186035}]}, {"text": "Moreover, utilizing attention over both input and the generated keyphrases further boosts our models' performance.", "labels": [], "entities": []}, {"text": "Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9993313550949097}]}, {"text": "The reason could be that arguments generated based on system retrieval contain less topic-specific words and more generic argumentative phrases.", "labels": [], "entities": []}, {"text": "Since the later is often observed inhuman written arguments, it may lead to higher precision and thus better BLEU scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9994425177574158}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9992771744728088}]}, {"text": "We also study the effect of our reranking-based decoder by varying the reranking step size (p) and the number of top words expanded to beam hypotheses deterministically (k).", "labels": [], "entities": []}, {"text": "From the results in, we find that reranking with a smaller step size, e.g., Beams are reranked at every 5, 10, and 20 steps (p).", "labels": [], "entities": []}, {"text": "For each step size, we also show the effect of varying k, where top-k words are selected deterministically for beam expansion, with 10 \u2212 k randomly sampled over multinomial distribution after removing the k words.", "labels": [], "entities": [{"text": "beam expansion", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.8204250037670135}]}, {"text": "Reranking with smaller step size yields better results.", "labels": [], "entities": []}, {"text": "p = 5, can generally lead to better METEOR scores.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9922497272491455}]}, {"text": "Although varying the number of top words for beam expansion does not yield significant difference, we do observe more diverse beams from the system output if more candidate words are selected stochastically (i.e. with a smaller k).", "labels": [], "entities": [{"text": "beam expansion", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8109086155891418}]}, {"text": "During our pilot study, we observe that generic arguments, such as \"I don't agree with you\" or \"this is not true\", are prevalent among generations by seq2seq models.", "labels": [], "entities": []}, {"text": "We believe that good arguments should include content that addresses the given topic.", "labels": [], "entities": []}, {"text": "Therefore, we design a novel evaluation method to measure whether the generated arguments contain topic-relevant information.", "labels": [], "entities": []}, {"text": "To achieve the goal, we first train a topicrelevance estimation model inspired by the latent semantic model in.", "labels": [], "entities": []}, {"text": "A pair of OP and argument, each represented as the average of word embeddings, are separately fed into a twolayer transformation model.", "labels": [], "entities": []}, {"text": "A dot-product is computed over the two projected low-dimensional vectors, and then a sigmoid function outputs the relevance score.", "labels": [], "entities": []}, {"text": "For model learning, we further divide our current training data into training, developing, and test sets.", "labels": [], "entities": []}, {"text": "For each OP and argument pair, we first randomly sample 100 arguments from other threads, and then pick the top 5 dissimilar ones, measured by Jaccard distance, as negative training samples.", "labels": [], "entities": []}, {"text": "This model achieves a Mean Reciprocal Rank (MRR) score of 0.95 on the test set.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR) score", "start_pos": 22, "end_pos": 54, "type": "METRIC", "confidence": 0.9724054421697345}]}, {"text": "Descriptions about model formulation and related training  details are included in the supplementary material.", "labels": [], "entities": [{"text": "model formulation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.758898913860321}]}, {"text": "We then take this trained model to evaluate the relevance between OP and the corresponding system arguments.", "labels": [], "entities": []}, {"text": "Each system argument is treated as positive sample; we then select five negative samples from arguments generated for other OPs whose evidence sentences most similar to that of the positive sample.", "labels": [], "entities": []}, {"text": "Intuitively, if an argument contains more topic relevant information, then the relevance estimation model will output a higher score for it; otherwise, the argument will receive a lower similarity score, and thus cannot be easily distinguished from negative samples.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 186, "end_pos": 202, "type": "METRIC", "confidence": 0.9482224881649017}]}, {"text": "Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in.", "labels": [], "entities": [{"text": "MRR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.45218268036842346}, {"text": "Precision at 1 (P@1)", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.9468237236142159}]}, {"text": "The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model.", "labels": [], "entities": []}, {"text": "Moreover, we manually pick 29 commonly used generic responses (e.g., \"I don't think so\") and count their frequency in system outputs.", "labels": [], "entities": []}, {"text": "For the seq2seq model, more than 75% of its outputs contain at least one generic argument, compared to 16.2% by our separate decoder model with attention over keyphrases.", "labels": [], "entities": []}, {"text": "This further implies that our model generates more topic-relevant content.", "labels": [], "entities": []}, {"text": "We also hire three trained human judges who are fluent English speakers to rate system arguments for the following three aspects on a scale of 1", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for evidence sentence retrieval from", "labels": [], "entities": [{"text": "evidence sentence retrieval", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.8122944633165995}]}, {"text": " Table 2. Importantly, this strategy al- lows model training to make rapid progress dur- ing early stages. Training each of our full models  takes about 4 days on a Quadro P5000 GPU card  with a batch size of 32. The model converges after  about 10 epochs in total with pre-training initial- ization, which is described below.", "labels": [], "entities": []}, {"text": " Table 2: Truncation size (i.e., number of tokens in-", "labels": [], "entities": [{"text": "Truncation size", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9409258961677551}]}, {"text": " Table 3: Results on argument generation by BLEU", "labels": [], "entities": [{"text": "argument generation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7510987222194672}, {"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9803603291511536}]}, {"text": " Table 4: Evaluation on topic relevance-models that", "labels": [], "entities": []}]}