{"title": [{"text": "Identifying Key Sentences for Precision Oncology Using Semi-Supervised Learning", "labels": [], "entities": [{"text": "Identifying Key Sentences", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9276619950930277}, {"text": "Precision Oncology", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9193929135799408}]}], "abstractContent": [{"text": "We present a machine learning pipeline that identifies key sentences in abstracts of oncological articles to aid evidence-based medicine.", "labels": [], "entities": []}, {"text": "This problem is characterized by the lack of gold standard data-sets, data imbalance and thematic differences between available silver standard corpora.", "labels": [], "entities": []}, {"text": "Additionally, available training and target data differs with regard to their domain (professional summaries vs. sentences in abstracts).", "labels": [], "entities": []}, {"text": "This makes supervised machine learning inapplicable.", "labels": [], "entities": []}, {"text": "We propose the use of two semi-supervised machine learning approaches: To mitigate difficulties arising from heterogeneous data sources, overcome data imbalance and create reliable training data we propose using transductive learning from positive and unlabelled data (PU Learning).", "labels": [], "entities": []}, {"text": "For obtaining a realistic classification model, we propose the use of abstracts summarised in relevant sentences as un-labelled examples through Self-Training.", "labels": [], "entities": []}, {"text": "The best model achieves 84% accuracy and 0.84 F1 score on our dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.99891197681427}, {"text": "F1 score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9744488298892975}]}], "introductionContent": [{"text": "The ever-growing amount of biomedical literature accessible online is a valuable source of information for clinical decisions.", "labels": [], "entities": []}, {"text": "The PubMed database, for instance, lists approximately 30 million articles' abstracts.", "labels": [], "entities": [{"text": "PubMed database", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9685004949569702}]}, {"text": "As a consequence, machine learning (ML) based text mining (TM) is increasingly employed to support evidence-based medicine by finding, condensing and analysing relevant information).", "labels": [], "entities": [{"text": "machine learning (ML) based text mining (TM)", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.7052355408668518}]}, {"text": "Practitioners in this field search for clinically relevant articles and findings, and are typically not interested in the bulk of search results which are devoted to basic research.", "labels": [], "entities": []}, {"text": "However, defining clinical relevance in a given abstract is not a trivial task.", "labels": [], "entities": [{"text": "defining clinical relevance", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7401907046635946}]}, {"text": "On top, although abstracts provide a very brief summary of their corresponding articles' content, practitioners determine abstracts' clinical relevance based on only a few key sentences.", "labels": [], "entities": []}, {"text": "To optimally support such users, it is thus necessary to first retrieve only clinically relevant articles and next to identify the sentences in those articles which express their clinical relevance.", "labels": [], "entities": []}, {"text": "Any survival benefit of dMMR was lost in N2 tumors.", "labels": [], "entities": []}, {"text": "Mutations in BRAF(V600E) (HR, 1.37; 95% CI, 1.08 to 1.70; P = .009) or KRAS (HR, 1.44; 95% CI, 1.21 to 1.70; P \u00a1 .001) were independently associated with worse DFS.", "labels": [], "entities": [{"text": "BRAF", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.6676046252250671}, {"text": "KRAS", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.7344111800193787}]}, {"text": "The observed MMR by tumor site interaction was validated in an independent cohort of stage III colon cancers (P(interaction) = .037).", "labels": [], "entities": [{"text": "MMR", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9883731007575989}, {"text": "P(interaction)", "start_pos": 110, "end_pos": 124, "type": "METRIC", "confidence": 0.9561286717653275}]}, {"text": "Example 1: Snippet of highlighted clinically relevant (or key; yellow background color) and irrelevant (no background color) sentences in a precision oncology setting.", "labels": [], "entities": []}, {"text": "Source document with PMID 24019539.", "labels": [], "entities": [{"text": "PMID", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9853200316429138}]}, {"text": "In this work, we present an ML pipeline to identify key (clinically relevant) sentences, in a precision oncology setting, in abstracts of oncological articles to aid evidence-based medicine.", "labels": [], "entities": [{"text": "ML", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9666078090667725}]}, {"text": "This setting is implied throughout the text when referring to clinical relevance or key (clinically relevant) sentences.", "labels": [], "entities": []}, {"text": "An example of relevant and irrelevant sentences is shown in Example 1.", "labels": [], "entities": []}, {"text": "For solving this problem no gold standard corpora is available.", "labels": [], "entities": []}, {"text": "Additionally, clinical relevance has only a vague definition and is a subjective measure.", "labels": [], "entities": []}, {"text": "As manually labelling text is expensive, semisupervised learning offers the possibility to utilize related annotated corpora.", "labels": [], "entities": []}, {"text": "We focus on SelfTraining (, which mostly relies on supervised classifiers trained on labelled data and use of unlabelled examples to improve the decision boundary.", "labels": [], "entities": []}, {"text": "Several corpora can be used to mitigate the issues arising from the lack of gold standard data set and data imbalance.", "labels": [], "entities": [{"text": "gold standard data set", "start_pos": 76, "end_pos": 98, "type": "DATASET", "confidence": 0.7041670978069305}]}, {"text": "These corpora implicitly define characteristics of key sentences, but cannot be considered as gold standards.", "labels": [], "entities": []}, {"text": "In the following, we call them \"silver standard\" corpora -collections of sentences close to the intended semantic but with large amounts of noise.", "labels": [], "entities": []}, {"text": "Specifically, we employ Clinical Interpretations of Variants in Cancer (CIViC) ( for implicit notion of clinical relevance and positive data points, i.e. sentences or abstracts which have clinical relevance.", "labels": [], "entities": [{"text": "Clinical Interpretations of Variants in Cancer (CIViC)", "start_pos": 24, "end_pos": 78, "type": "TASK", "confidence": 0.6480844020843506}]}, {"text": "Unfortunately, negative data points, i.e. sentences or abstracts which do not have clinical relevance, are not present in this data set.", "labels": [], "entities": []}, {"text": "PubMed abstracts, referenced by CIViC, are used as unlabelled data.", "labels": [], "entities": [{"text": "PubMed abstracts", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9279962182044983}, {"text": "CIViC", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9585336446762085}]}, {"text": "Since we consider all sentences in CIViC to be positive examples and the corresponding abstracts are initially unlabelled, additional data for negative examples is required.", "labels": [], "entities": []}, {"text": "We utilize the Hallmarks of Cancer Corpus (HoC) ( as an auxiliary source of noisy labelled data.", "labels": [], "entities": [{"text": "Hallmarks of Cancer Corpus (HoC)", "start_pos": 15, "end_pos": 47, "type": "DATASET", "confidence": 0.74075015102114}]}, {"text": "To expand on our set of labelled data points we propose transductive learning from positive and unlabelled data (PU Learning) to identify noise within HoC, with CIViC as a guide set for determining the relevance of sentences from HoC.", "labels": [], "entities": []}, {"text": "This gives us additional, both positive and negative data points, used as an initialization for Self-Training.", "labels": [], "entities": []}, {"text": "The pipeline is available at https://github.com/nachne/semisuper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Removing noise from HoC n and HoC p . Results for different PU Learning techniques, averaged  over 10 runs, on 20% reserved test sets P test \u2282 P and U test \u2282 U . To generate HoC  n as required for  tolerant mode, Roc-SVM (highlighted in bold) was used in the previous step.", "labels": [], "entities": []}, {"text": " Table 3: Performance of different semi-supervised approaches trained on P , N , and U after tolerant  noise filtering. Results averaged over 10 runs with randomised 20% validation sets from P and N ;  min-df threshold = 0.002, 25% of most relevant features selected with \u03c7 2 . The model we consider most  suitable for identifying key sentences is highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 4: Supervised classifiers trained on P and N after strict noise filtering. Results averaged over 10  runs with randomised 20% reserved test sets; min-df threshold = 0.002, 25% of most relevant features  selected with \u03c7 2 .", "labels": [], "entities": []}]}