{"title": [{"text": "Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search", "labels": [], "entities": [{"text": "Illustrative Language Understanding", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6081580718358358}, {"text": "Image Search", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.7175319194793701}]}], "abstractContent": [{"text": "We introduce Picturebook, a large-scale lookup operation to ground language via 'snapshots' of our physical world accessed through image search.", "labels": [], "entities": []}, {"text": "For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding.", "labels": [], "entities": []}, {"text": "We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations.", "labels": [], "entities": []}, {"text": "We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words.", "labels": [], "entities": []}, {"text": "We experiment and report results across a wide range of tasks: word similarity , natural language inference, semantic relatedness, sentiment/topic classification , image-sentence ranking and machine translation.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7258496880531311}, {"text": "sentiment/topic classification", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.6251367181539536}, {"text": "image-sentence ranking", "start_pos": 164, "end_pos": 186, "type": "TASK", "confidence": 0.7035434395074844}, {"text": "machine translation", "start_pos": 191, "end_pos": 210, "type": "TASK", "confidence": 0.7802141606807709}]}, {"text": "We also show that gate activations corresponding to Picturebook em-beddings are highly correlated to human judgments of concreteness ratings.", "labels": [], "entities": [{"text": "Picturebook", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.9411060214042664}]}], "introductionContent": [{"text": "Constructing grounded representations of natural language is a promising step towards achieving human-like language learning.", "labels": [], "entities": []}, {"text": "In recent years, a large amount of research has focused on integrating vision and language to obtain visually grounded word and sentence representations.", "labels": [], "entities": []}, {"text": "One source of grounding, which has been utilized in existing work, is image search engines.", "labels": [], "entities": [{"text": "image search engines", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8573789397875468}]}, {"text": "Search engines allow us to obtain correspondences between language and images that are far less restricted than existing multimodal datasets which typically have restricted vocabularies.", "labels": [], "entities": []}, {"text": "While true natural language understanding may require fully *Both authors contributed equally to this work.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.7980334162712097}]}, {"text": "embodied cognition, search engines allow us to get a form of quasi-grounding from high-coverage 'snapshots' of our physical world provided by the interaction of millions of users.", "labels": [], "entities": []}, {"text": "One place to incorporate grounding is in the lookup table that maps tokens to vectors.", "labels": [], "entities": []}, {"text": "The dominant approach to learning distributed word representations is through indexing a learned matrix.", "labels": [], "entities": []}, {"text": "While immensely successful, this lookup operation is typically learned through co-occurrence objectives or a task-dependent reward signal.", "labels": [], "entities": []}, {"text": "Avery different way to obtain word embeddings is to aggregate features obtained by using the word as a query for an image search engine.", "labels": [], "entities": []}, {"text": "This involves retrieving the top-k images from a search engine, running those through a convolutional network and aggregating the results.", "labels": [], "entities": []}, {"text": "These word embeddings are grounded via the retrieved images.", "labels": [], "entities": []}, {"text": "While several authors have considered this approach, it has been largely limited to a few thousand queries and only a small number of tasks.", "labels": [], "entities": []}, {"text": "In this paper we introduce Picturebook embeddings produced by image search using words as queries.", "labels": [], "entities": []}, {"text": "Picturebook embeddings are obtained through a convolutional network trained with a semantic ranking objective on a proprietary image dataset with over 100+ million images ().", "labels": [], "entities": []}, {"text": "Using Google image search, a Picturebook embedding fora word is obtained by concatenating the k-feature vectors of our convolutional network on the top-k retrieved search results.", "labels": [], "entities": []}, {"text": "The main contributions of our work are as follows: \u2022 We obtain Picturebook embeddings for the 2.2 million words that occur in the Glove vocabulary () , allowing each word to have a Glove embedding and a parallel grounded word representation.", "labels": [], "entities": []}, {"text": "This collection of word representations that we visually ground via image search is 2-3 orders of magnitude larger than prior work.", "labels": [], "entities": []}, {"text": "\u2022 We introduce a multimodal gating mechanism to selectively choose between Glove and Picturebook embeddings in a task-dependent way.", "labels": [], "entities": []}, {"text": "We apply our approach to over a dozen datasets and several different tasks: word similarity, sentence relatedness, natural language inference, topic/sentiment classification, image sentence ranking and Machine Translation (MT).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7130312621593475}, {"text": "topic/sentiment classification", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.642391674220562}, {"text": "image sentence ranking", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.7014999985694885}, {"text": "Machine Translation (MT)", "start_pos": 202, "end_pos": 226, "type": "TASK", "confidence": 0.8180272459983826}]}, {"text": "\u2022 We introduce Inverse Picturebook to perform the inverse lookup operation.", "labels": [], "entities": []}, {"text": "Given a Picturebook embedding, we find the closest words which would generate the embedding.", "labels": [], "entities": []}, {"text": "This is useful for generative modelling tasks.", "labels": [], "entities": [{"text": "generative modelling tasks", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.9628549218177795}]}, {"text": "\u2022 We perform an extensive analysis of our gating mechanism, showing that the gate activations for Picturebook embeddings are highly correlated with human judgments of concreteness.", "labels": [], "entities": []}, {"text": "We also show that Picturebook gate activations are negatively correlated with image dispersion ( , indicating that our model selectively chooses between word embeddings based on their abstraction level.", "labels": [], "entities": [{"text": "image dispersion", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.6619852036237717}]}, {"text": "\u2022 We highlight the importance of the convolutional network used to extract embeddings.", "labels": [], "entities": []}, {"text": "In particular, networks trained with semantic labels result in better embeddings than those trained with visual labels, even when evaluating similarity on concrete words.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the effectiveness of our embeddings, we perform both quantitative and qualitative evaluation across a wide range of natural language processing tasks.", "labels": [], "entities": []}, {"text": "Hyperparameter details of each experiment are included in the appendix.", "labels": [], "entities": []}, {"text": "Since the use of Picturebook embeddings adds extra parameters to our models, we include a baseline for each experiment (either based on Glove or learned embeddings) that we extensively tune.", "labels": [], "entities": []}, {"text": "In most experiments, we end up with baselines that are stronger than what has previously been reported.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Nearest neighbours of words. Results are retrieved over the 100K most frequent words.", "labels": [], "entities": []}, {"text": " Table 3: SimLex-999 results (Spearman's \u21e2). Best results overall are bolded. Best results per section  are underlined. Bracketed numbers signify the number of images used. Some rows are copied across  sections for ease of reading.", "labels": [], "entities": [{"text": "Bracketed", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9803668260574341}]}, {"text": " Table 4: Classification accuracies are reported for SNLI and MulitNLI. For SICK we report Pearson,  Spearman and MSE. Higher is better for all metrics except MSE. Best results overall per column are  bolded. Best results per section are underlined.", "labels": [], "entities": [{"text": "Classification accuracies", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8705112934112549}, {"text": "SNLI", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8328359127044678}, {"text": "MulitNLI", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.855593740940094}, {"text": "Pearson", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9502400755882263}, {"text": "Spearman", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.7733858227729797}, {"text": "MSE", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.5736203193664551}, {"text": "MSE", "start_pos": 159, "end_pos": 162, "type": "DATASET", "confidence": 0.866965651512146}]}, {"text": " Table 5: Test accuracy [%] on topic and sentiment classification datasets. Best results per dataset are  bolded, best results per section are underlined. We compare directly against other bag of ngram baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9573594927787781}, {"text": "sentiment classification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7534455060958862}]}, {"text": " Table 6: COCO test-set results for image-sentence retrieval experiments. Our models use VSE++. R@K  is Recall@K (high is good). Med r is the median rank (low is good).", "labels": [], "entities": [{"text": "image-sentence retrieval", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7312971651554108}, {"text": "Recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9825353622436523}]}, {"text": " Table 7: Machine Translation results on the Multi30k English ! German task. We note that our models  do not use BPE, and we perform better in BLEU relative to METEOR.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.801538497209549}, {"text": "BPE", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9808388948440552}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9989736080169678}, {"text": "METEOR", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.7727341651916504}]}, {"text": " Table 8: Machine Translation results on the Multi30k English ! French task.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.759032666683197}, {"text": "Multi30k English ! French task", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.6283867120742798}]}, {"text": " Table 9: Machine Translation results on the IWSLT 2014 German-English task.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8130359351634979}, {"text": "IWSLT 2014 German-English task", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.7819986343383789}]}, {"text": " Table 10: Correlations (rounded, x100) of mean Picturebook gate activations to human judgements of  concreteness ratings (ccorr) and image dispersion (disp) within the specified most frequent words.", "labels": [], "entities": [{"text": "concreteness ratings (ccorr)", "start_pos": 101, "end_pos": 129, "type": "METRIC", "confidence": 0.6449741244316101}]}]}