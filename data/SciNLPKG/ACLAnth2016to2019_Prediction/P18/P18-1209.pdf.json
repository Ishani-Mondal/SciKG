{"title": [{"text": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors", "labels": [], "entities": []}], "abstractContent": [{"text": "Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is mul-timodal fusion.", "labels": [], "entities": [{"text": "mul-timodal fusion", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7963500618934631}]}, {"text": "The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation.", "labels": [], "entities": []}, {"text": "Previous research in this field has exploited the ex-pressiveness of tensors for multimodal representation.", "labels": [], "entities": []}, {"text": "However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor.", "labels": [], "entities": []}, {"text": "In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency.", "labels": [], "entities": [{"text": "Multimodal Fusion", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7344190776348114}, {"text": "multimodal fusion", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7805333137512207}]}, {"text": "We evaluate our model on three different tasks: mul-timodal sentiment analysis, speaker trait analysis, and emotion recognition.", "labels": [], "entities": [{"text": "mul-timodal sentiment analysis", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.741991659005483}, {"text": "speaker trait analysis", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7600181897481283}, {"text": "emotion recognition", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7285217791795731}]}, {"text": "Our model achieves competitive results on all these tasks while drastically reducing computational complexity.", "labels": [], "entities": []}, {"text": "Additional experiments also show that our model can perform robustly fora wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal research has shown great progress in a variety of tasks as an emerging research field of artificial intelligence.", "labels": [], "entities": []}, {"text": "Tasks such as speech recognition (, emotion recognition,,,, sentiment analysis,) * equal contributions as well as speaker trait analysis and media description () have seen a great boost in performance with developments in multimodal research.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7958607077598572}, {"text": "emotion recognition", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7184180021286011}, {"text": "sentiment analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8295441269874573}, {"text": "speaker trait analysis", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.698572059472402}, {"text": "media description", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7005148977041245}]}, {"text": "However, a core research challenge yet to be solved in this domain is multimodal fusion.", "labels": [], "entities": [{"text": "multimodal fusion", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7286912202835083}]}, {"text": "The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions.", "labels": [], "entities": []}, {"text": "In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity.", "labels": [], "entities": []}, {"text": "Some of the recent attempts (,  at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance.", "labels": [], "entities": [{"text": "multimodal fusion", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7916679382324219}, {"text": "multimodal representation", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.7230963110923767}]}, {"text": "Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations.", "labels": [], "entities": []}, {"text": "This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset.", "labels": [], "entities": []}, {"text": "In this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance.", "labels": [], "entities": [{"text": "Multimodal Fusion", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.7341605126857758}, {"text": "multimodal fusion", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.7633733451366425}]}, {"text": "The overall architecture is shown in.", "labels": [], "entities": []}, {"text": "We evaluated our approach with experiments on three multimodal tasks using public datasets and compare its performance with state-of-the-art models.", "labels": [], "entities": []}, {"text": "We also study how different low-rank settings impact the performance of our model and show that our model performs robustly within a wide range of rank settings.", "labels": [], "entities": []}, {"text": "Finally, we perform an analysis of the impact of our method on the number of parameters and run-time with comparison to other fusion methods.", "labels": [], "entities": []}, {"text": "Through theoretical analysis, we show that our model can scale linearly in the number of modalities, and our experiments also show a corresponding speedup in training when compared with: Overview of our Low-rank Multimodal Fusion model structure: LMF first obtains the unimodal representation z a , z v , z l by passing the unimodal inputs x a , xv , x l into three sub-embedding networks f v , fa , fl respectively.", "labels": [], "entities": []}, {"text": "LMF produces the multimodal output representation by performing low-rank multimodal fusion with modality-specific factors.", "labels": [], "entities": []}, {"text": "The multimodal representation can be then used for generating prediction tasks.", "labels": [], "entities": [{"text": "generating prediction tasks", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7253833214441935}]}, {"text": "The main contributions of our paper are as follows: \u2022 We propose the Low-rank Multimodal Fusion method for multimodal fusion that can scale linearly in the number of modalities.", "labels": [], "entities": [{"text": "Multimodal Fusion", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7465733289718628}, {"text": "multimodal fusion", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.7960953712463379}]}, {"text": "\u2022 We show that our model compares to state-ofthe-art models in performance on three multimodal tasks evaluated on public datasets.", "labels": [], "entities": []}, {"text": "\u2022 We show that our model is computationally efficient and has fewer parameters in comparison to previous tensor-based methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare LMF with previous state-of-the-art baselines, and we use the Tensor Fusion Networks (TFN) (Zadeh et al., 2017) as a baseline for tensorbased approaches, which has the most similar structure with us except that it explicitly forms the large multi-dimensional tensor for fusion across different modalities.", "labels": [], "entities": []}, {"text": "We design our experiments to better understand the characteristics of LMF.", "labels": [], "entities": []}, {"text": "Our goal is to answer the following four research questions: (1) Impact of Multimodal Low-rank Fusion: Direct comparison between our proposed LMF model and the previous TFN model.", "labels": [], "entities": [{"text": "Multimodal Low-rank Fusion", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.5927290320396423}]}, {"text": "The results of these experiments are presented in Section 5.", "labels": [], "entities": []}, {"text": "We perform our experiments on the following multimodal datasets, CMU-MOSI ( To evaluate model generalization, all datasets are split into training, validation, and test sets such that the splits are speaker independent, i.e., no identical speakers from the training set are present in the test sets.", "labels": [], "entities": []}, {"text": "illustrates the data splits for all datasets in detail.", "labels": [], "entities": []}, {"text": "Multiple evaluation tasks are performed during our evaluation: multi-class classification and regression.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7604351937770844}]}, {"text": "The multi-class classification task is applied to all three multimodal datasets, and the regression task is applied to the CMU-MOSI and the POM dataset.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7316067814826965}, {"text": "CMU-MOSI", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.9443860054016113}, {"text": "POM dataset", "start_pos": 140, "end_pos": 151, "type": "DATASET", "confidence": 0.8705938756465912}]}, {"text": "For binary classification and multiclass classification, we report F1 score and accuracy Acc\u2212k where k denotes the number of classes.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6734922528266907}, {"text": "multiclass classification", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.8056760430335999}, {"text": "F1 score", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9771531820297241}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9990500807762146}, {"text": "Acc", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.844370424747467}]}, {"text": "Specifically, Acc\u22122 stands for the binary classification.", "labels": [], "entities": [{"text": "Acc\u22122", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9604818224906921}]}, {"text": "For regression, we report Mean Absolute Error (MAE) and Pearson correlation.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 26, "end_pos": 51, "type": "METRIC", "confidence": 0.9452736775080363}, {"text": "Pearson correlation", "start_pos": 56, "end_pos": 75, "type": "METRIC", "confidence": 0.8945366442203522}]}, {"text": "Higher values denote better performance for all metrics except for MAE.", "labels": [], "entities": [{"text": "MAE", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.6469548344612122}]}], "tableCaptions": [{"text": " Table 1: The speaker independent data splits for  training, validation, and test sets.", "labels": [], "entities": []}, {"text": " Table 2: Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality  trait recognition on POM. Best results are highlighted in bold.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.972342312335968}, {"text": "CMU-MOSI", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9748551249504089}, {"text": "emotion recognition", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.6860771030187607}, {"text": "IEMOCAP", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.5269660949707031}, {"text": "personality  trait recognition", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.5906625886758169}]}, {"text": " Table 3: Comparison of the training and testing  speeds between TFN and LMF. The second and  the third columns indicate the number of data point  inferences per second (IPS) during training and  testing time respectively. Both models are imple- mented in the same framework with equivalent run- ning environment.", "labels": [], "entities": [{"text": "number of data point  inferences per second (IPS)", "start_pos": 125, "end_pos": 174, "type": "METRIC", "confidence": 0.7506165087223053}]}]}