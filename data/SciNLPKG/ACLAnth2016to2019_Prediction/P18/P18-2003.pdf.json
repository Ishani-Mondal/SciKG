{"title": [{"text": "Deep RNNs Encode Soft Hierarchical Syntax", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision.", "labels": [], "entities": []}, {"text": "We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (par-ent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it.", "labels": [], "entities": []}, {"text": "These predictions are made from representations produced at different depths in networks that are pre-trained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language model-ing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8647308945655823}, {"text": "semantic role labeling", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.686002274354299}, {"text": "machine translation", "start_pos": 183, "end_pos": 202, "type": "TASK", "confidence": 0.8018276393413544}]}, {"text": "In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges.", "labels": [], "entities": []}, {"text": "This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (.", "labels": [], "entities": []}, {"text": "However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful fora wide variety of NLP tasks, even in the neural setting (.", "labels": [], "entities": []}, {"text": "In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision.", "labels": [], "entities": []}, {"text": "We consider a set of feature prediction tasks drawn from different depths of syntactic parse trees; given a word-level representation, we attempt to predict the POS tag and the parent, grandparent, and great-grandparent constituent labels of that word.", "labels": [], "entities": [{"text": "feature prediction", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7646081447601318}]}, {"text": "We evaluate how well a simple feedforward classifier can detect these syntax features from the word representations produced by the RNN layers from deep NLP models trained on the tasks of dependency parsing, semantic role labeling, machine translation, and language modeling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.790396124124527}, {"text": "semantic role labeling", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.6185989777247111}, {"text": "machine translation", "start_pos": 232, "end_pos": 251, "type": "TASK", "confidence": 0.8272021114826202}, {"text": "language modeling", "start_pos": 257, "end_pos": 274, "type": "TASK", "confidence": 0.7438661158084869}]}, {"text": "We also evaluate whether a similar classifier can predict if a dependency arc exists between two words in a sentence, given their representations.", "labels": [], "entities": []}, {"text": "We find that, across all four types of supervision, the representations learned by these models encode syntax beyond the explicit information they encounter during training; this is seen in both the word-level tasks and the dependency arc prediction task.", "labels": [], "entities": [{"text": "dependency arc prediction task", "start_pos": 224, "end_pos": 254, "type": "TASK", "confidence": 0.7643183916807175}]}, {"text": "Furthermore, we also observe that features associated with different levels of syntax tree correlate with word representations produced by RNNs at different depths.", "labels": [], "entities": []}, {"text": "Largely speaking, we see that deeper layers in each model capture notions of syntax that are higher-level and more abstract, in the sense that higher-level constituents cover a larger span of the underlying sentence.", "labels": [], "entities": []}, {"text": "These findings suggest that models trained on NLP tasks are able to induce syntax even when direct syntactic supervision is unavailable.", "labels": [], "entities": []}, {"text": "Furthermore, the models are able to differentiate this induced syntax into a soft hierarchy across different layers of the model, perhaps shedding some light on why deep RNNs are so useful for NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "We predict each syntactic property with a simple feed-forward network with a single 300-dimensional hidden layer activated by a ReLU: where i is the word index and l is the layer index within a model.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.7581527233123779}]}, {"text": "To ensure that the classifiers are not trained on the same data as the RNNs, we train the classifier for each layer l separately using the development set of CoNLL-2012 and evaluate on the test set ().", "labels": [], "entities": [{"text": "CoNLL-2012", "start_pos": 158, "end_pos": 168, "type": "DATASET", "confidence": 0.8671634793281555}]}, {"text": "In addition, we compare performance with word-level baselines.", "labels": [], "entities": []}, {"text": "We report the per-word majority class baseline; at the POS level, for example, \"cat\" will be classified as a noun and \"walks\" as a verb.", "labels": [], "entities": []}, {"text": "This baseline outperforms the pre-trained GloVe () embeddings on every task.", "labels": [], "entities": []}, {"text": "We also consider a contextual baseline, in which we concatenate each word's embedding with the average of its context's embeddings; however, this baseline also performed worse that the reported one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The training data, recurrent architecture, and hyperparameters of each model.", "labels": [], "entities": []}, {"text": " Table 2: Results of the dependency arc prediction task. L0-L4 denote the different layers of the model.  DP refers to the RNN trained with dependency parsing supervision.", "labels": [], "entities": [{"text": "dependency arc prediction task", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.841468334197998}, {"text": "dependency parsing", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.7373655140399933}]}]}