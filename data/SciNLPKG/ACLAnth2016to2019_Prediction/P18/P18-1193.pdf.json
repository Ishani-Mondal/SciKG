{"title": [{"text": "Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation", "labels": [], "entities": [{"text": "Situated Mapping of Sequential Instructions", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7813782215118408}]}], "abstractContent": [{"text": "We propose a learning approach for mapping context-dependent sequential instructions to actions.", "labels": [], "entities": []}, {"text": "We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world.", "labels": [], "entities": []}, {"text": "To train from start and goal states without access to demonstrations , we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization.", "labels": [], "entities": [{"text": "SESTRA", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.7711293697357178}]}, {"text": "We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.", "labels": [], "entities": [{"text": "absolute", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9253119826316833}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8890419006347656}]}], "introductionContent": [{"text": "An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals.", "labels": [], "entities": []}, {"text": "For example, consider the environment and instructions in, in which a user describes moving chemicals between beakers and mixing chemicals together.", "labels": [], "entities": []}, {"text": "To execute the second instruction, the agent needs to resolve sixth beaker and last one to objects in the environment.", "labels": [], "entities": []}, {"text": "The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown.", "labels": [], "entities": []}, {"text": "In this paper, we describe a model and learning approach to map sequences of instructions to actions.", "labels": [], "entities": []}, {"text": "Our model considers previous utterances and the world state to select actions, learns to combine simple actions to achieve complex goals, and can be trained using", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation Following, we evaluate task completion accuracy using exact match between the final state and the annotated goal state.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9748591184616089}]}, {"text": "We report accuracy for complete interactions (5utts), the first three utterances of each interaction (3utts), and single instructions (Inst).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995242357254028}, {"text": "Inst", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9777318835258484}]}, {"text": "For single instructions, execution starts from the annotated start state of the instruction.", "labels": [], "entities": []}, {"text": "Systems We report performance of ablations and two baseline systems: POLICYGRADIENT: policy gradient with cumulative episodic reward without a baseline, and CONTEXTUALBANDIT: the contextual bandit approach of.", "labels": [], "entities": [{"text": "POLICYGRADIENT", "start_pos": 69, "end_pos": 83, "type": "METRIC", "confidence": 0.9881647825241089}, {"text": "CONTEXTUALBANDIT", "start_pos": 157, "end_pos": 173, "type": "METRIC", "confidence": 0.8505136966705322}]}, {"text": "Both systems use the reward with the shaping term and our model.", "labels": [], "entities": []}, {"text": "We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using contextaction demonstration pairs.", "labels": [], "entities": []}, {"text": "Only the supervised approach uses the heuristically generated labels.", "labels": [], "entities": []}, {"text": "Although the results are not comparable, we also report the performance of previous approaches to SCONE.", "labels": [], "entities": [{"text": "SCONE", "start_pos": 98, "end_pos": 103, "type": "TASK", "confidence": 0.8881054520606995}]}, {"text": "All three approaches generate logical representations based on lambda calculus.", "labels": [], "entities": []}, {"text": "In contrast to our approach, this requires an ontology of hand built symbols and rules to evaluate the logical forms.", "labels": [], "entities": []}, {"text": "uses supervised learning with annotated logical forms.", "labels": [], "entities": []}, {"text": "Training Details For test results, we run each experiment five times and report results for the model with best validation interaction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.7830337882041931}]}, {"text": "For ablations, we do the same with three experiments.", "labels": [], "entities": []}, {"text": "We use a batch size of 20.", "labels": [], "entities": []}, {"text": "We stop training using a validation set sampled from the training data.", "labels": [], "entities": []}, {"text": "We hold the validation set constant for each domain for all experiments.", "labels": [], "entities": []}, {"text": "We use patience over the average reward, and select the best model using interaction-level (5utts) validation accuracy.", "labels": [], "entities": [{"text": "patience", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9687263369560242}]}, {"text": "We tune \u03bb, \u03b4, and Mon the development set.", "labels": [], "entities": []}, {"text": "The selected values and other implementation details are described in the Supplementary Material.", "labels": [], "entities": [{"text": "Supplementary Material", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.7530636489391327}]}, {"text": "Our approach significantly outperforms POLICYGRADIENT and CON-TEXTUALBANDIT, both of which suffer due to biases learned early during learning, hindering later exploration.", "labels": [], "entities": [{"text": "POLICYGRADIENT", "start_pos": 39, "end_pos": 53, "type": "METRIC", "confidence": 0.8507272601127625}]}, {"text": "This problem does not appear in TANGRAMS, where no action type is dominant at the beginning of executions, and all methods perform well.", "labels": [], "entities": [{"text": "TANGRAMS", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.5999657511711121}]}, {"text": "POLICYGRADIENT completely fails to learn ALCHEMY and SCENE due to observing only negative total rewards early during learning.", "labels": [], "entities": [{"text": "POLICYGRADIENT", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8318416476249695}, {"text": "ALCHEMY", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9104356169700623}, {"text": "SCENE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9236882925033569}]}], "tableCaptions": [{"text": " Table 1: Data statistics for ALCHEMY (ALC), SCENE  (SCE), and TANGRAMS (TAN).", "labels": [], "entities": [{"text": "ALCHEMY", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.8246409296989441}, {"text": "TANGRAMS (TAN)", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.8808038681745529}]}, {"text": " Table 2: Counts of discourse phenomena in SCONE  from 30 randomly selected development interactions  for each domain. We count occurrences of coreference  between instructions (e.g., he leaves in SCENE) and el- lipsis (e.g., then, drain 2 units in ALCHEMY), when the  last explicit mention of the referent was 1, 2, 3, or 4  turns in the past. We also report the average number of  multi-turn references per interaction (Refs/Ex).", "labels": [], "entities": [{"text": "ALCHEMY", "start_pos": 249, "end_pos": 256, "type": "METRIC", "confidence": 0.834354043006897}]}, {"text": " Table 3: Test accuracies for single instructions (Inst), first-three instructions (3utts), and full interactions (5utts).", "labels": [], "entities": []}, {"text": " Table 4: Development results, including model ablations. We also report mean \u00b5 and standard deviation \u03c3 for all  metrics for our approach across five experiments. We bold the best performing variations of our model.", "labels": [], "entities": [{"text": "standard deviation \u03c3", "start_pos": 84, "end_pos": 104, "type": "METRIC", "confidence": 0.9117154280344645}]}, {"text": " Table 5: Common error counts in the three domains.", "labels": [], "entities": [{"text": "Common error counts", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7740200161933899}]}]}