{"title": [{"text": "Exploring Semantic Properties of Sentence Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural vector representations are ubiquitous throughout all subfields of NLP.", "labels": [], "entities": []}, {"text": "While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties.", "labels": [], "entities": []}, {"text": "We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural vector representations have become ubiquitous in all subfields of natural language processing.", "labels": [], "entities": [{"text": "Neural vector representations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7916859984397888}]}, {"text": "For the case of word vectors, important properties of the representations have been studied, including their linear substructures (, the linear superposition of word senses (, and the nexus to pointwise mutual information scores between co-occurring words ().", "labels": [], "entities": []}, {"text": "However, thus far, only little is known about the properties of sentence embeddings.", "labels": [], "entities": []}, {"text": "Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector.", "labels": [], "entities": []}, {"text": "A number of such sentence embedding methods have been proposed in recent years (.", "labels": [], "entities": []}, {"text": "Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments of semantic relatedness, typically with respect to the SemEval Semantic Textual Similarity competitions.", "labels": [], "entities": [{"text": "Sentence embeddings", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8499139845371246}, {"text": "SemEval Semantic Textual Similarity", "start_pos": 172, "end_pos": 207, "type": "TASK", "confidence": 0.8507231324911118}]}, {"text": "The SICK dataset () was created to better benchmark the effectiveness of different models across abroad range of challenging lexical, syntactic, and semantic phenomena, in terms of both similarities and the ability to be predictive of entailment.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7316802442073822}]}, {"text": "However, even on SICK, oftentimes very shallow methods prove effective at obtaining fairly competitive results (.", "labels": [], "entities": [{"text": "SICK", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8637246489524841}]}, {"text": "to what extent different embedding methods are predictive of i) the occurrence of words in the original sentence, ii) the order of words in the original sentence, and iii) the length of the original sentence (.", "labels": [], "entities": []}, {"text": "inspected neural machine translation systems with regard to their ability to acquire morphology, while investigated to what extent they learn source side syntax.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.748216966787974}]}, {"text": "argue that the latent representations of advanced neural reading comprehension architectures encode information about predication.", "labels": [], "entities": []}, {"text": "Finally, sentence embeddings have also often been investigated in classification tasks such as sentiment polarity or question type classification (.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8543720841407776}, {"text": "question type classification", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.7429320017496744}]}, {"text": "Concurrently with our research, investigated to what extent one can learn to classify specific syntactic and semantic properties of sentences using large amounts of training data (100,000 instances) for each property.", "labels": [], "entities": []}, {"text": "Overall, still, remarkably little is known about what specific semantic properties are directly reflected by such embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we specifically focus on a few select aspects of sentence semantics and inspect to what extent prominent sentence embedding methods are able to capture them.", "labels": [], "entities": []}, {"text": "Our framework generates triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now proceed to describe our experimental evaluation based on this paradigm.", "labels": [], "entities": []}, {"text": "Using the aforementioned triplet generation methods, we create the evaluation datasets listed in Table 1, drawing on source sentences from SICK, Penn Treebank WSJ and MSR Paraphase corpus.", "labels": [], "entities": [{"text": "triplet generation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7572986781597137}, {"text": "Penn Treebank WSJ", "start_pos": 145, "end_pos": 162, "type": "DATASET", "confidence": 0.9762517015139262}, {"text": "MSR Paraphase corpus", "start_pos": 167, "end_pos": 187, "type": "DATASET", "confidence": 0.8665473659833273}]}, {"text": "Although the process to modify the sentences is automatic, we rely on human annotators to double-check the results for grammaticality and semantics.", "labels": [], "entities": []}, {"text": "This is particularly important for synonym substitution, for which we relied on WordNet).", "labels": [], "entities": [{"text": "synonym substitution", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9725910723209381}, {"text": "WordNet", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.9757874011993408}]}, {"text": "Unfortunately, not all synonyms are suitable as replacements in a given context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generated Evaluation Datasets  Dataset  # of Sentences", "labels": [], "entities": [{"text": "Generated Evaluation Datasets  Dataset  # of Sentences", "start_pos": 10, "end_pos": 64, "type": "DATASET", "confidence": 0.7603496398244586}]}, {"text": " Table 2: Evaluation of Negation Detection", "labels": [], "entities": [{"text": "Evaluation of Negation Detection", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6946733817458153}]}, {"text": " Table 3: Evaluation of Negation Variants", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of Clause Relatedness", "labels": [], "entities": [{"text": "Evaluation of Clause Relatedness", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7164531350135803}]}, {"text": " Table 5: Evaluation of Argument Sensitivity", "labels": [], "entities": []}, {"text": " Table 6: Evaluation of Fixed Point Reorder", "labels": [], "entities": [{"text": "Fixed Point Reorder", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.644858161608378}]}]}