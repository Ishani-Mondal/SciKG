{"title": [{"text": "Neural Factor Graph Models for Cross-lingual Morphological Tagging", "labels": [], "entities": [{"text": "Cross-lingual Morphological Tagging", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6869039237499237}]}], "abstractContent": [{"text": "Morphological analysis involves predicting the syntactic traits of a word (e.g. {POS: Noun, Case: Acc, Gender: Fem}).", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.925724595785141}]}, {"text": "Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict-often false-assumption that tag sets exactly overlap between the HRL and LRL.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7169845998287201}]}, {"text": "In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption.", "labels": [], "entities": [{"text": "cross-lingual morphological tagging", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.6861916581789652}, {"text": "information sharing between languages", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.7878628596663475}]}, {"text": "The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pair-wise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data.", "labels": [], "entities": []}, {"text": "Experiments on four languages from the Universal Dependencies Treebank (Nivre et al., 2017) demonstrate superior tagging accuracies over existing cross-lingual approaches.", "labels": [], "entities": [{"text": "Universal Dependencies Treebank", "start_pos": 39, "end_pos": 70, "type": "DATASET", "confidence": 0.6517993907133738}]}], "introductionContent": [{"text": "Morphological analysis,, inter alia) is the task of predicting fine-grained annotations about the syntactic properties of tokens in a language such as part-of-speech, case, or tense.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8142397999763489}, {"text": "predicting fine-grained annotations about the syntactic properties of tokens in a language such as part-of-speech, case, or tense", "start_pos": 52, "end_pos": 181, "type": "Description", "confidence": 0.7347347512841225}]}, {"text": "For instance, in, the given Portuguese sentence is labeled with the respective morphological tags such as Gender and its label value Masculine.", "labels": [], "entities": [{"text": "Gender", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9526078701019287}, {"text": "Masculine", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.7955777645111084}]}, {"text": "The accuracy of morphological analyzers is paramount, because their results are often a first step in the NLP pipeline for tasks such as translation ( and parsing (, and errors in the upstream analysis may cascade to the downstream tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9990887641906738}, {"text": "translation", "start_pos": 137, "end_pos": 148, "type": "TASK", "confidence": 0.9774128794670105}, {"text": "parsing", "start_pos": 155, "end_pos": 162, "type": "TASK", "confidence": 0.9349180459976196}]}, {"text": "One difficulty, however, in creating these taggers is that only a limited amount of annotated data is available fora majority of the world's languages to learn these morphological taggers.", "labels": [], "entities": []}, {"text": "Fortunately, recent efforts in morphological annotation follow a standard annotation schema for these morphological tags across languages, and now the Universal Dependencies Treebank ( has tags according to this schema in 60 languages.", "labels": [], "entities": [{"text": "Universal Dependencies Treebank", "start_pos": 151, "end_pos": 182, "type": "DATASET", "confidence": 0.8562025427818298}]}, {"text": "have recently shown that combining this shared schema with cross-lingual training on a related high-resource language (HRL) gives improved performance on tagging accuracy for low-resource languages (LRLs).", "labels": [], "entities": [{"text": "tagging", "start_pos": 154, "end_pos": 161, "type": "TASK", "confidence": 0.9591949582099915}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.7996553182601929}]}, {"text": "The output space of this model consists of tag sets such as {POS: Adj, Gender: Masc, Number: Sing}, which are predicted fora token at each time step.", "labels": [], "entities": []}, {"text": "However, this model relies heavily on the fact that the entire space of tag sets for the LRL must match those of the HRL, which is often not the case, either due to linguistic divergence or small differences in the annotation schemes between the two languages.", "labels": [], "entities": []}, {"text": "For instance, in Figure 1 \"refrescante\" is assigned a gender in the Portuguese UD treebank, but not in the In this paper, we propose a method that instead of predicting full tag sets, makes predictions over single tags separately but ties together each decision by modeling variable dependencies between tags overtime steps (e.g. capturing the fact that nouns frequently occur after determiners) and pairwise dependencies between all tags at a single time step (e.g. capturing the fact that infinitive verb forms don't have tense).", "labels": [], "entities": [{"text": "Portuguese UD treebank", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.7990080316861471}]}, {"text": "The specific model is shown in, consisting of a factorial conditional random field (FCRF; ) with neural network potentials calculated by long short-term memory (LSTM;) at every variable node ( \u00a73).", "labels": [], "entities": []}, {"text": "Learning and inference in the model is made In particular, the latter is common because many UD resources were created by full or semi-automatic conversion from treebanks with less comprehensive annotation schemes than UD.", "labels": [], "entities": []}, {"text": "Our model can generate label values for these tags too, which could possibly aid the enhancement of UD annotations, although we do not examine this directly in our work.", "labels": [], "entities": []}, {"text": "tractable through belief propagation over the possible tag combinations, allowing the model to consider an exponential label space in polynomial time.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6922301054000854}]}, {"text": "This model has several advantages: \u2022 The model is able to generate tag sets unseen in training data, and share information between similar tag sets, alleviating the main disadvantage of previous work cited above.", "labels": [], "entities": []}, {"text": "\u2022 Our model is empirically strong, as validated in our main experimental results: it consistently outperforms previous work in cross-lingual low-resource scenarios in experiments.", "labels": [], "entities": []}, {"text": "\u2022 Our model is more interpretable, as we can probe the model parameters to understand which variable dependencies are more likely to occur in a language, as we demonstrate in our analysis.", "labels": [], "entities": []}, {"text": "In the following sections, we describe the model and these results in more detail.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset sizes. tgt size = 100 or 1,000  LRL sentences are added to HRL Train", "labels": [], "entities": [{"text": "HRL Train", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.8640232384204865}]}, {"text": " Table 2: Tag Set Sizes with tgt size=100", "labels": [], "entities": []}, {"text": " Table 1. In order to simulate low- resource settings, we follow the experimental pro- cedure from", "labels": [], "entities": []}, {"text": " Table 3: Token-wise accuracy and F1 scores on mono-lingual experiments", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9869201183319092}, {"text": "F1 scores", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9804314970970154}]}, {"text": " Table 4: Token-wise accuracy and F1 scores on cross-lingual experiments", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9860760569572449}, {"text": "F1 scores", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9800719916820526}]}, {"text": " Table 5: Ablation Experiments (tgt size=1000)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.992007851600647}]}]}