{"title": [{"text": "Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities TE XT TRA NSC RI P TI ON", "labels": [], "entities": [{"text": "Scalable multimodal sentiment classification", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.5719827637076378}, {"text": "TE XT TRA NSC RI P TI", "start_pos": 119, "end_pos": 140, "type": "METRIC", "confidence": 0.8386367900030953}]}], "abstractContent": [{"text": "In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment.", "labels": [], "entities": []}, {"text": "The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.9043590724468231}]}, {"text": "Mul-timodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers.", "labels": [], "entities": [{"text": "Mul-timodal fusion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6361955404281616}]}, {"text": "In the detection of sentiment in these videos, acoustic and video features provide clarity to otherwise ambiguous transcripts.", "labels": [], "entities": [{"text": "detection of sentiment", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8335511485735575}]}, {"text": "In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze spoken sentences for sentiment.", "labels": [], "entities": []}, {"text": "We discard traditional transcription features in order to minimize human intervention and to maximize the deployabil-ity of our model on at-scale real-world data.", "labels": [], "entities": []}, {"text": "We select high-level features for our model that have been successful in non-affect domains in order to test their gen-eralizability in the sentiment detection domain.", "labels": [], "entities": [{"text": "sentiment detection domain", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.9209706981976827}]}, {"text": "We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, obtaining an F 1 score of 0.8049 on the validation set and an F 1 score of 0.6325 on the held-out challenge test set.", "labels": [], "entities": [{"text": "CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset", "start_pos": 50, "end_pos": 124, "type": "DATASET", "confidence": 0.6920904137871482}, {"text": "F 1 score", "start_pos": 139, "end_pos": 148, "type": "METRIC", "confidence": 0.9909172654151917}, {"text": "F 1 score", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9920647740364075}]}, {"text": "Figure 1: A blindspot in multimodal sentiment analysis is the inclusion of human-transcriptions of spoken sentiment, which limits model applicability.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7605546613534292}]}, {"text": "We address this by using only prosodic and visual features for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.9717713892459869}]}], "introductionContent": [{"text": "Multimodal fusion models in the spoken-word domain incorporate features outside of text-based natural language processing (NLP) to increase model performance.", "labels": [], "entities": []}, {"text": "These models benefit from the full scope of person-person interaction, which provides both context and clarification for speech that is ambiguous as text alone.", "labels": [], "entities": []}, {"text": "The addition of multimodal data has been shown to increase model performance across abroad set of spoken-word fields, such as sarcasm (, question () and sentiment ( ) detection.", "labels": [], "entities": [{"text": "sentiment ( ) detection", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.6882725358009338}]}, {"text": "Each of these examples contains speech that can be difficult to infer from transcribed text-instead, the speaker's intent is clarified to listeners via intonations or expressions.", "labels": [], "entities": []}, {"text": "It follows that machine learning models trained to include domain knowledge from these modalities would likewise be able to correctly interpret complex communication.", "labels": [], "entities": []}, {"text": "Multimodal sentiment analysis (MSA) is one example of ambiguous speech that has been shown to benefit from additional modalities (.", "labels": [], "entities": [{"text": "Multimodal sentiment analysis (MSA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8789393703142802}]}, {"text": "MSA is the identification of the explicit or implicit attitude of a thought or sentence toward a situation or event.", "labels": [], "entities": [{"text": "identification of the explicit or implicit attitude of a thought or sentence toward a situation or event", "start_pos": 11, "end_pos": 115, "type": "TASK", "confidence": 0.6862141875659719}]}, {"text": "In recent years, the online community has been shown to frequently express sentiment orally in videos or recordings uploaded to sites like Youtube or Facebook.", "labels": [], "entities": []}, {"text": "These spoken-word opinion pieces have been collected and annotated into large high-quality multimodal sentiment datasets (.", "labels": [], "entities": []}, {"text": "Recently, the largest annotated sentiment dataset to date, CMU-MOSEI, was released ().", "labels": [], "entities": []}, {"text": "This dataset contains over 23,500 spoken sentence videos, totaling 65 hours, 53 minutes, and 36 seconds.", "labels": [], "entities": []}, {"text": "This large quantity of data comes from real-world expressions of sentiment, offering a unique opportunity to train and test model performance and generalization on a large dataset.", "labels": [], "entities": []}, {"text": "Additionally, released a software development kit (SDK) for training and testing models on the CMU-MOSEI dataset, with future work focusing on addition of other multimodal datasets.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.9821695387363434}]}, {"text": "These releases culminated in a challenge focused on human multimodal language with the opportunity to train a model and evaluate it on a held-out challenge test set.", "labels": [], "entities": []}, {"text": "As is common in sentiment datasets, the MO-SEI dataset includes features from human transcriptions of speech (.", "labels": [], "entities": [{"text": "MO-SEI dataset", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.852041631937027}]}, {"text": "Ideally, models trained to annotate sentiment will operate on real-world data with as few barriers to deployment as possible in order to maximize efficiency and continuity.", "labels": [], "entities": [{"text": "continuity", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.9787938594818115}]}, {"text": "The use of human transcripts represents one of these barriers-it greatly limits the scalability of models in the real-world due to the time and cost in transcription and the inequality in quality between human and computer transcripts).", "labels": [], "entities": []}, {"text": "The goal of this work is to build a model that broadly generalizes to unseen data using only scalable audio and visual features, reducing the need for transcription of human speech.", "labels": [], "entities": []}, {"text": "In order to achieve this, we implement a model pipeline which has been successfully deployed in domains of sensitive and affectively impactful video analysis (.", "labels": [], "entities": []}, {"text": "From this pipeline, we select simple high-level video features and a generalized subset of audio features extracted using openSMILE.", "labels": [], "entities": []}, {"text": "We further test the generalizability of this pipeline by evaluating its applicability to the MSA domain.", "labels": [], "entities": []}, {"text": "Additionally, this pipeline automatically extracts interpretable features that highlight model attention.", "labels": [], "entities": []}, {"text": "These features can be easily mapped back to videos, as shown by, which allows easy interpretation of model performance.", "labels": [], "entities": []}, {"text": "Although recent work in MSA has begun exploring applicability of deep learning features, these models mostly achieve high performance numbers in specific scenarios but have poor generalizability and interpretability.", "labels": [], "entities": [{"text": "MSA", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9348928928375244}]}, {"text": "In the next section, we examine related work on multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.7661324540774027}]}, {"text": "Section 3 explains the model pipeline and evaluation procedure.", "labels": [], "entities": []}, {"text": "Section 4 presents our model results on the CMU-MOSEI validation set and the grand challenge held-out test set.", "labels": [], "entities": [{"text": "CMU-MOSEI validation set", "start_pos": 44, "end_pos": 68, "type": "DATASET", "confidence": 0.9615335861841837}]}, {"text": "Finally, in Section 5 we discuss our results, our model's limitations, and propose future work to improve our model.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model output presents predictions as binary positive or negative classes as well as a confidence metric for each video sentence.", "labels": [], "entities": []}, {"text": "We evaluated our model's performance on basic classification of sentiment using precision, recall and F 1 -scores.", "labels": [], "entities": [{"text": "classification of sentiment", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8239737351735433}, {"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9996587038040161}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.999424934387207}, {"text": "F 1 -scores", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9864023327827454}]}, {"text": "We selected these metrics because they are known to report accurate performance representation on imbalanced classes.", "labels": [], "entities": []}, {"text": "Since these metrics are defined for two-classes, we binarize the ground truth scores values by thresholding values > 0 as positive and the remaining as negative.", "labels": [], "entities": []}, {"text": "Although we trained the SVM classifier for binary predictions, the confidence scores obtained from the classifier for each sample are continuous and can be used to perform regression.", "labels": [], "entities": []}, {"text": "Since sentiment scores in the dataset scale between, we scaled our confidence scores to match the expected distribution of sentiment using a linear transformation function.", "labels": [], "entities": []}, {"text": "These were the predictions that we submitted to the ACL2018 Grand Challenge.", "labels": [], "entities": [{"text": "ACL2018 Grand Challenge", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.871601402759552}]}, {"text": "We also performed a regression between the ground truth scores and scores obtained by our methods on the validation set, and reported the Mean Absolute Error (MAE) for these experiments alongside our classification results.", "labels": [], "entities": [{"text": "Mean Absolute Error (MAE)", "start_pos": 138, "end_pos": 163, "type": "METRIC", "confidence": 0.9501023292541504}]}, {"text": "The CMU-MOSEI dataset ( used to train and test our model provides a largescale breakdown of sentiment analysis.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.971299558877945}, {"text": "sentiment analysis", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9237283170223236}]}, {"text": "However, the dataset follows typical practices for multimodal sentiment datasets, which make it difficult to train a fully automatic model.", "labels": [], "entities": []}, {"text": "We identify practices which would increase automation.", "labels": [], "entities": []}, {"text": "First, the data is pre-segmented at the sentence level, resulting in no sentenceless data.", "labels": [], "entities": []}, {"text": "For a model to be employed in the real-world, it needs to be aware of sentenceless data as well as imperfect sentence boundaries.", "labels": [], "entities": []}, {"text": "For example, human often segment speech at the sentence or category level (, however, machine learning algorithms have yet to perfect this practice.", "labels": [], "entities": []}, {"text": "Previous work has found that NLP models are prone to complete failure when presented with excess words or information, even when those words are unrelated to the task (.", "labels": [], "entities": []}, {"text": "Ideally, models in the real-world will be robust to such noise.", "labels": [], "entities": []}, {"text": "Second, our model does not use human transcription in order to avoid limitations in real-world applicability.", "labels": [], "entities": []}, {"text": "However, text is a modality that improves MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9414733052253723}]}, {"text": "Rather than releasing text transcriptions for model building, we propose future datasets release automatic speech recognition transcriptions.", "labels": [], "entities": [{"text": "speech recognition transcriptions", "start_pos": 107, "end_pos": 140, "type": "TASK", "confidence": 0.7964965105056763}]}, {"text": "This would further model automation by incorporating scalable transcription practices, as is becoming more common in other domains (.", "labels": [], "entities": []}, {"text": "Additionally, recent work suggests the gap between human transcription and ASR will soon be negated by advances in the speech recognition domain, furthering the argument that human transcription is no longer necessary for building models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9856605529785156}, {"text": "speech recognition domain", "start_pos": 119, "end_pos": 144, "type": "TASK", "confidence": 0.8042195638020834}]}, {"text": "By including the full range of data and switching from human to ASR transcription, we believe that sentiment models can be trained, evaluated, and employed at-scale on real-world data.", "labels": [], "entities": [{"text": "ASR transcription", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8807391226291656}]}, {"text": "Work on automating multimodal sentiment analysis should focus on model performance using tractable methods of data collection; as exemplified by other domains intended to work with realworld data, with human level transcriptions of data reported as a comparison metric.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.7202361424763998}]}], "tableCaptions": [{"text": " Table 1: Performance of individual modality and multimodal fusion for sentiment analysis on the vali- dation set of CMU-MOSEI. MAE is the Mean Absolute Error.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9148848056793213}, {"text": "CMU-MOSEI", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.6896809935569763}, {"text": "MAE", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9935333728790283}, {"text": "Mean Absolute Error", "start_pos": 139, "end_pos": 158, "type": "METRIC", "confidence": 0.8899391492207845}]}, {"text": " Table 3: Confusion matrix of classification results  from the methods on the validation set of CMU- MOSEI.", "labels": [], "entities": [{"text": "validation set of CMU- MOSEI", "start_pos": 78, "end_pos": 106, "type": "DATASET", "confidence": 0.7696630557378134}]}]}