{"title": [{"text": "Semantically Equivalent Adversarial Rules for Debugging NLP Models", "labels": [], "entities": [{"text": "Debugging NLP", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.8694290220737457}]}], "abstractContent": [{"text": "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically.", "labels": [], "entities": []}, {"text": "To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs)-semantic-preserving perturbations that induce changes in the model's predictions.", "labels": [], "entities": []}, {"text": "We generalize these adversaries into semantically equivalent adversarial rules (SEARs)-simple, universal replacement rules that induce adversaries on many instances.", "labels": [], "entities": []}, {"text": "We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.9586467444896698}]}, {"text": "Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts.", "labels": [], "entities": [{"text": "SEARs", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.8611355423927307}]}, {"text": "SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.", "labels": [], "entities": [{"text": "SEARs", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.425859659910202}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9967464208602905}]}], "introductionContent": [{"text": "With increasing complexity of models for tasks like classification (, machine comprehension (, and visual question answering (, models are becoming increasingly challenging to debug, and to determine whether they are ready for deployment.", "labels": [], "entities": [{"text": "question answering", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7329182326793671}]}, {"text": "In particular, these complex models are prone to brittleness: different ways of phrasing the same sentence can often cause the model to  output different predictions.", "labels": [], "entities": []}, {"text": "While held-out accuracy is often useful, it is not sufficient: practitioners consistently overestimate their model's generalization () since test data is usually gathered in the same manner as training and validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9975740313529968}]}, {"text": "When deployed, these seemingly accurate models encounter sentences that are written very differently than the ones in the training data, thus making them prone to mistakes, and fragile with respect to distracting additions ().", "labels": [], "entities": []}, {"text": "These problems are exacerbated by the variability in language, and by cost and noise in annotations, making such bugs challenging to detect and fix.", "labels": [], "entities": []}, {"text": "A particularly challenging issue is oversensitivity (: a class of bugs where models output different predictions for very similar inputs.", "labels": [], "entities": []}, {"text": "These bugs are prevalent in image classifi- Figure 2: Semantically Equivalent Adversarial Rules: For the task of question answering, the proposed approach identifies transformation rules for questions in (a) that result in paraphrases of the queries, but lead to incorrect answers (#Flips is the number of times this happens in the validation data).", "labels": [], "entities": [{"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7892626225948334}, {"text": "Flips", "start_pos": 283, "end_pos": 288, "type": "METRIC", "confidence": 0.9946224689483643}]}, {"text": "We show examples of rephrased questions that result in incorrect answers for the two rules in (b) and (c).", "labels": [], "entities": []}, {"text": "cation (), a domain where one can measure the magnitude of perturbations, and many small-magnitude changes are imperceptible to the human eye.", "labels": [], "entities": []}, {"text": "For text, however, a single word addition can change semantics (e.g. adding \"not\"), or have no semantic impact for the task at hand.", "labels": [], "entities": []}, {"text": "Inspired by adversarial examples for images, we introduce semantically equivalent adversaries (SEAs) -text inputs that are perturbed in semantics-preserving ways, but induce changes in a black box model's predictions (example in).", "labels": [], "entities": []}, {"text": "Producing such adversarial examples systematically can significantly aid in debugging ML models, as it allows users to detect problems that happen in the real world, instead of oversensitivity only to malicious attacks such as intentionally scrambling, misspelling, or removing words (.", "labels": [], "entities": []}, {"text": "While SEAs describe local brittleness (i.e. are specific to particular predictions), we are also interested in bugs that affect the model more globally.", "labels": [], "entities": []}, {"text": "We represent these via simple replacement rules that induce SEAs on multiple predictions, such as in, where a simple contraction of \"is\"after Wh pronouns (what, who, whom) (2b) makes 70 (1%) of the previously correct predictions of the model \"flip\" (i.e. become incorrect).", "labels": [], "entities": []}, {"text": "Perhaps more surprisingly, adding a simple \"?\" induces mistakes in 3% of examples.", "labels": [], "entities": []}, {"text": "We call such rules semantically equivalent adversarial rules (SEARs).", "labels": [], "entities": []}, {"text": "In this paper, we present SEAs and SEARs, designed to unveil local and global oversensitivity bugs in NLP models.", "labels": [], "entities": [{"text": "SEARs", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9399768114089966}]}, {"text": "We first present an approach to generate semantically equivalent adversaries, based on paraphrase generation techniques (, that is model-agnostic (i.e. works for any black box model).", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7396397590637207}]}, {"text": "Next, we generalize SEAs into semantically equivalent rules, and outline the properties for optimal rule sets: semantic equivalence, high adversary count, and non-redundancy.", "labels": [], "entities": []}, {"text": "We frame the problem of finding such a set as a submodular optimization problem, leading to an accurate yet efficient algorithm.", "labels": [], "entities": []}, {"text": "Including the human into the loop, we demonstrate via user studies that SEARs help users uncover important bugs on a variety of state-of-the-art models for different tasks (sentiment classification, visual question answering).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.9547922611236572}, {"text": "visual question answering", "start_pos": 199, "end_pos": 224, "type": "TASK", "confidence": 0.6392895479997}]}, {"text": "Our experiments indicate that SEAs and SEARs make humans significantly better at detecting impactful bugs -SEARs uncover bugs that cause 3 to 4 times more mistakes than human-generated rules, in much less time.", "labels": [], "entities": []}, {"text": "Finally, we show that SEARs are actionable, enabling the human to close the loop by fixing the discovered bugs using a data augmentation procedure.", "labels": [], "entities": [{"text": "SEARs", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.49469509720802307}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 6: Fixing bugs using SEARs: Effect of re- training models using SEARs, both on original  validation and on sensitivity dataset. Retraining  significantly reduces the number of bugs, with sta- tistically insignificant changes to accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.9969210624694824}]}]}