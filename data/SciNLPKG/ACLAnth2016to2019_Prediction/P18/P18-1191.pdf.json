{"title": [], "abstractContent": [{"text": "We present anew large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser.", "labels": [], "entities": [{"text": "Question-Answer driven Semantic Role Labeling (QA-SRL) annotations", "start_pos": 38, "end_pos": 104, "type": "TASK", "confidence": 0.6998799476358626}]}, {"text": "Our corpus , QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with anew crowd-sourcing scheme that we show has high precision and good recall at modest cost.", "labels": [], "entities": [{"text": "QA-SRL Bank 2.0", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.8799431522687277}, {"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.998181939125061}, {"text": "recall", "start_pos": 210, "end_pos": 216, "type": "METRIC", "confidence": 0.9977431297302246}]}, {"text": "We also present neural models for two QA-SRL subtasks: detecting argument spans fora predicate and generating questions to label the semantic relationship.", "labels": [], "entities": []}, {"text": "The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9213321208953857}, {"text": "span-level", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9250743985176086}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.6497566103935242}, {"text": "QA-SRL prediction task", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.8151053786277771}]}, {"text": "They can also, as we show, be used to gather additional annotations at low cost.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning semantic parsers to predict the predicateargument structures of a sentence is along standing, open challenge ().", "labels": [], "entities": []}, {"text": "Such systems are typically trained from datasets that are difficult to gather, 1 but recent research has explored training nonexperts to provide this style of semantic supervision (.", "labels": [], "entities": []}, {"text": "In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large * Much of this work was done while these authors were at the Allen Institute for Artificial Intelligence.", "labels": [], "entities": []}, {"text": "The PropBank ( and FrameNet) annotation guides are 89 and 119 pages, respectively.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.9379233121871948}]}, {"text": "In 1950 Alan M. Turing published \"Computing machinery and intelligence\" in Mind, in which he proposed that machines could be tested for intelligence using questions and answers.", "labels": [], "entities": []}, {"text": "What did someone propose? that machines could be tested for intelligent using questions and answers 6 When did someone propose something?", "labels": [], "entities": []}, {"text": "In tested 7 What can be tested?", "labels": [], "entities": []}, {"text": "machines 8 What can something be tested for?", "labels": [], "entities": []}, {"text": "intelligence 9 How can something be tested?", "labels": [], "entities": []}, {"text": "using questions and answers using 10 What was being used?", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the crowdsourced validation step to do a final human evaluation of our models.", "labels": [], "entities": []}, {"text": "We test 3 parsers: the span-based span detection model paired with each of the local and sequential question generation models trained on the initial dataset, and our final model (span-based span detection and sequential question generation) trained with the expanded data.", "labels": [], "entities": [{"text": "span-based span detection", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.5689257681369781}, {"text": "span-based span detection", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.6774223148822784}, {"text": "sequential question generation", "start_pos": 210, "end_pos": 240, "type": "TASK", "confidence": 0.6836580634117126}]}, {"text": "Methodology On the 5,205 sentence densely annotated subset of dev and test, we generate QA-SRL labels with all of the models using a span detection threshold of \u03c4 = 0.2 and combine the questions with the existing data.", "labels": [], "entities": []}, {"text": "We filter out questions that fail the autocomplete grammaticality check (counting them invalid) and pass the data into the validation step, annotating each question to a total of 6 validator judgments.", "labels": [], "entities": []}, {"text": "We then compute question and span accuracy as follows: A question is considered correct if 5 out of 6 annotators consider it valid, and a span is considered correct if its generated question is correct and the span is among those selected for the question by validators.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9243486523628235}]}, {"text": "We rank all questions and spans by the threshold at which they are generated, which allows us to compute accuracy at different levels of recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9978579878807068}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9959625601768494}]}, {"text": "As expected, the sequence-based question generation models are much more accurate than the local model; this is largely because the local model generated many questions that failed the grammaticality check.", "labels": [], "entities": [{"text": "sequence-based question generation", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.6481676797072092}]}, {"text": "Furthermore, training with our expanded data results in more questions and spans generated at the same threshold.", "labels": [], "entities": []}, {"text": "If we choose a threshold value which gives a similar number of questions per sentence as were labeled in the original data annotation (2 questions / verb), question and span accuracy are 82.64% and 77.61%, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.6313537359237671}]}, {"text": "shows the output of our best system on 3 randomly selected sentences from our development set (one from each domain).", "labels": [], "entities": []}, {"text": "The model was overall highly accurate-only one question and 3 spans are considered incorrect, and each mistake is nearly correct, 6 even when the sentence contains a negation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example QA-SRL questions, decomposed into their slot-based representation. See He et al.  (2015) for the full details. All slots draw from a small, deterministic set of options, including verb tense  (present, pastparticiple, etc.) Here we have replaced the verb-tense slot with its conjugated form.", "labels": [], "entities": []}, {"text": " Table 2: Statistics for the dataset with questions  written by workers across three domains.", "labels": [], "entities": []}, {"text": " Table 3: Precision and recall of our annotation  pipeline on a merged and validated subset of 100  verbs. The unfiltered number represents relaxing  the restriction that none of 2 validators marked the  question as invalid.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9872236251831055}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9983375072479248}]}, {"text": " Table 4: Results for Span Detection on the dense  development dataset. Span detection results are  given with the cutoff threshold \u03c4 at 0.5, and at  the value which maximizes F-score. The top chart  lists precision, recall and F-score with exact span  match, while the bottom reports matches where  the intersection over union (IOU) is \u2265 0.5.", "labels": [], "entities": [{"text": "Span Detection", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.935659259557724}, {"text": "Span detection", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8216978311538696}, {"text": "F-score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.9965167045593262}, {"text": "precision", "start_pos": 206, "end_pos": 215, "type": "METRIC", "confidence": 0.9993283748626709}, {"text": "recall", "start_pos": 217, "end_pos": 223, "type": "METRIC", "confidence": 0.9989044666290283}, {"text": "F-score", "start_pos": 228, "end_pos": 235, "type": "METRIC", "confidence": 0.9956623911857605}, {"text": "exact span  match", "start_pos": 241, "end_pos": 258, "type": "METRIC", "confidence": 0.9065396388371786}, {"text": "intersection over union (IOU)", "start_pos": 304, "end_pos": 333, "type": "METRIC", "confidence": 0.6658919453620911}]}, {"text": " Table 5: Question Generation results on the dense  development set. EM -Exact Match accuracy, PM  -Partial Match Accuracy, SA -Slot-level accuracy", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6977186799049377}, {"text": "EM -Exact Match accuracy", "start_pos": 69, "end_pos": 93, "type": "METRIC", "confidence": 0.83744136095047}, {"text": "PM  -Partial Match Accuracy", "start_pos": 95, "end_pos": 122, "type": "METRIC", "confidence": 0.9262911438941955}, {"text": "SA -Slot-level accuracy", "start_pos": 124, "end_pos": 147, "type": "METRIC", "confidence": 0.8972346484661102}]}, {"text": " Table 6: Joint span detection and question gener- ation results on the dense development set, using  exact-match for both spans and questions.", "labels": [], "entities": [{"text": "Joint span detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7029627958933512}, {"text": "exact-match", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9753364324569702}]}, {"text": " Table 7: Results on the expanded development set  comparing the full model trained on the original  data, and with the expanded data.", "labels": [], "entities": []}]}