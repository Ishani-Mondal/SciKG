{"title": [{"text": "Chat Discrimination for Intelligent Conversational Agents with a Hybrid CNN-LMTGRU Network", "labels": [], "entities": [{"text": "Chat Discrimination", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7588611841201782}]}], "abstractContent": [{"text": "Recently, intelligent dialog systems and smart assistants have attracted the attention of many, and development of novel dialogue agents have become a research challenge.", "labels": [], "entities": []}, {"text": "Intelligent agents that can handle both domain-specific task-oriented and open-domain chitchat dialogs are one of the major requirements in the current systems.", "labels": [], "entities": []}, {"text": "In order to address this issue and to realize such smart hybrid dialogue systems , we develop a model to discriminate user utterance between task-oriented and chitchat conversations.", "labels": [], "entities": []}, {"text": "We introduce a hybrid of convolutional neural network (CNN) and a lateral multiple timescale gated recurrent units (LMTGRU) that can represent multiple temporal scale dependencies for the discrimination task.", "labels": [], "entities": []}, {"text": "With the help of the combined slow and fast units of the LMTGRU, our model effectively determines whether a user will have a chitchat conversation or a task-specific conversation with the system.", "labels": [], "entities": []}, {"text": "We also show that the LMTGRU structure helps the model to perform well on longer text inputs.", "labels": [], "entities": []}, {"text": "We address the lack of dataset by constructing a dataset using Twitter and Maluuba Frames data.", "labels": [], "entities": [{"text": "Maluuba Frames data", "start_pos": 75, "end_pos": 94, "type": "DATASET", "confidence": 0.8299639225006104}]}, {"text": "The results of the experiments demonstrate that the proposed hybrid network outperforms the conventional models on the chat discrimination task as well as performed comparable to the baselines on various benchmark datasets.", "labels": [], "entities": [{"text": "chat discrimination task", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.8016146024068197}]}], "introductionContent": [{"text": "Dialogue systems can be classified as domainspecific task-oriented and open-domain chit-chat dialog systems).", "labels": [], "entities": []}, {"text": "The task-oriented dialog systems help users complete tasks in specific domains.", "labels": [], "entities": []}, {"text": "The chit-chat dialog systems enable users to have an open-ended chat conversations with the system.", "labels": [], "entities": []}, {"text": "While most of the functionalities offered by the two types of systems are complementary to each other, there have been very little efforts made to combine these two type of systems.", "labels": [], "entities": []}, {"text": "Therefore, the potential of chat agents have been limited.", "labels": [], "entities": []}, {"text": "Recently, intelligent assistants have become popular with the integration of such systems in smartphones and home appliances.", "labels": [], "entities": []}, {"text": "These intelligent assistants typically perform various tasks including weather forecast alerts, alarm settings, web search, and soon.", "labels": [], "entities": [{"text": "soon", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9092640280723572}]}, {"text": "Moreover, such assistants need to have the ability to perform chitchat conversation with the users.", "labels": [], "entities": []}, {"text": "This has led to the need for the development of novel and hybrid multi-domain task-oriented agents and opendomain chit-chat agents.", "labels": [], "entities": []}, {"text": "In order to develop such hybrid agents, we have to determine whether a user will have a chit-chat with the system or the user is looking fora task completion.", "labels": [], "entities": []}, {"text": "For example, if a user says \"Hi, how are you doing?\", then the user can be considered to have a chat with the system.", "labels": [], "entities": []}, {"text": "Alternatively, if the user says \"I want a flight to Los Angeles,\" then the user is looking fora completion of a specific task.", "labels": [], "entities": []}, {"text": "We address this task as a binary classification problem and call this task as chat discrimination.", "labels": [], "entities": [{"text": "chat discrimination", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7020218968391418}]}, {"text": "Chat discrimination has not been sufficiently investigated in recent times.", "labels": [], "entities": [{"text": "Chat discrimination", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9155997335910797}]}, {"text": "This is mainly because there are not enough studies to develop hybrids of task-oriented and chit-chat agents.", "labels": [], "entities": []}, {"text": "Although task-oriented and chit-chat agents have long research histories, they do not require chat discrimination.", "labels": [], "entities": []}, {"text": "We usually assume that the users of taskoriented agents will have task-oriented conversations with the systems and the users of chit-chat agents will always have non task-specific conversations with the systems.", "labels": [], "entities": []}, {"text": "Ina recent study, researchers in) have tried chat detection using conventional classifiers with the help of a newly created dataset in Japanese language.", "labels": [], "entities": [{"text": "chat detection", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8262406885623932}]}, {"text": "But this dataset has not been released for further research or comparison.", "labels": [], "entities": []}, {"text": "In this work, we develop a hybrid network for chat discrimination by combining a convolutional neural network (CNN) and a gated recurrent unit (GRU).", "labels": [], "entities": [{"text": "chat discrimination", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.7765119969844818}]}, {"text": "CNNs have been proven to be suitable for text classification problems.", "labels": [], "entities": [{"text": "text classification", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8587061166763306}]}, {"text": "Moreover, the temporal hierarchy concept with multiple timescale gated recurrent unit (MTGRU) ( ) has also been proven to perform well in language modeling ) and summarization  tasks.", "labels": [], "entities": [{"text": "multiple timescale gated recurrent unit (MTGRU)", "start_pos": 46, "end_pos": 93, "type": "METRIC", "confidence": 0.6482469663023949}, {"text": "summarization  tasks", "start_pos": 162, "end_pos": 182, "type": "TASK", "confidence": 0.928327888250351}]}, {"text": "The MTGRU is known to handle long term dependency better with the help of the varying timescales to represent multiple compositionalities of language.", "labels": [], "entities": [{"text": "MTGRU", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.686103105545044}]}, {"text": "The temporal hierarchy approach has also been shown to eliminate the need for complex structures and normalization techniques (, and thereby increasing the computational efficiency of the model.", "labels": [], "entities": []}, {"text": "For our classification model, we develop a lateral multiple timescale structure.", "labels": [], "entities": []}, {"text": "Our proposed lateral multiple timescale gated recurrent unit (LMTGRU) is significantly different from the conventional hierarchical MTGRU structure.", "labels": [], "entities": []}, {"text": "The conventional MTGRU is most effective for handling long term dependencies in very long text inputs for applications such as summarization but performs comparable to vanilla GRU with shorter text inputs.", "labels": [], "entities": [{"text": "MTGRU", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.6291183829307556}, {"text": "summarization", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.9859824776649475}]}, {"text": "Unlike the hierarchical architecture, the lateral connections in an LMTGRU will enable encoding of rich features that have different temporal dependencies from the input utterances in order to help classify the information correctly.", "labels": [], "entities": []}, {"text": "LMTGRU follows a lateral (branch or root) architecture where the slow and fast units are directly connected to the inputs and the final output of the units are combined to form the final representation.", "labels": [], "entities": [{"text": "LMTGRU", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7805003523826599}]}, {"text": "This structure enables all the layers with different timescales to capture relevant features directly from the inputs unlike hierarchical multilayer structures.", "labels": [], "entities": []}, {"text": "Since the data consist of utterances as input, and the input to the RNN is represented as higher order features from the CNN, LMTGRU proves to be more suitable for this task.", "labels": [], "entities": []}, {"text": "Our major contributions are as follows: \u2022 We introduce a hybrid CNN-LMTGRU structure to build rich features from input texts to classify utterances correctly.", "labels": [], "entities": []}, {"text": "\u2022 The LMTGRU architecture enables our model to perform well on longer text sequences with the help of the slow layer as well as maintain comparable performance on shorter sequences.", "labels": [], "entities": []}, {"text": "\u2022 To address the lack of dataset, we create a dataset using Twitter data (Microsoft Research Social Media Conversation Corpus) () for chit-chat conversations and Maluuba Frames data) for task-oriented conversations.", "labels": [], "entities": [{"text": "Microsoft Research Social Media Conversation Corpus)", "start_pos": 74, "end_pos": 126, "type": "DATASET", "confidence": 0.8633534908294678}]}, {"text": "\u2022 In order to demonstrate that the proposed model performs well on other text classification tasks and to compare it to the existing baselines, we report the performance on various sentence classification benchmark datasets.", "labels": [], "entities": [{"text": "text classification", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7655175626277924}]}, {"text": "The results of our experiments demonstrate that the proposed model performs well on the benchmark datasets as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "Chat discrimination task requires a chat dataset like the one shown in.", "labels": [], "entities": [{"text": "Chat discrimination", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8363248407840729}]}, {"text": "We address the lack of such a dataset by using the Microsoft Research Social Media Conversation Corpus 1 and Maluuba Frames 2 datasets.", "labels": [], "entities": [{"text": "Microsoft Research Social Media Conversation Corpus 1 and Maluuba Frames 2 datasets", "start_pos": 51, "end_pos": 134, "type": "DATASET", "confidence": 0.8513369609912237}]}, {"text": "Microsoft Research Social Media Conversation Corpus is a collection of conversational snippets extracted from Twitter logs.", "labels": [], "entities": [{"text": "Microsoft Research Social Media Conversation Corpus", "start_pos": 0, "end_pos": 51, "type": "DATASET", "confidence": 0.9266210993131002}]}, {"text": "The advantage of using this dataset is that it has been evaluated by crowd sourced annotators measuring quality of the response.", "labels": [], "entities": []}, {"text": "These data are suitable for detecting open-domain non-task oriented chats.", "labels": [], "entities": [{"text": "detecting open-domain non-task oriented chats", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7900786161422729}]}, {"text": "On the other hand, we use the Maluuba Frames dataset for the domain task-specific conversations.", "labels": [], "entities": [{"text": "Maluuba Frames dataset", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.9224034349123637}]}, {"text": "This corpus is for the travel agent domain where the users can inquire the agent and ask for booking of hotels and flights.", "labels": [], "entities": []}, {"text": "The dialogs were recorded using 12 participants over a period of 20 days.", "labels": [], "entities": []}, {"text": "We process the data to utilize only the user utterances in our chat discrimination dataset.", "labels": [], "entities": []}, {"text": "Finally, we have 20,532 utterances with 10,266 in each class.", "labels": [], "entities": []}, {"text": "We divide the data into 10% for validation, 10% for test, and the remaining for train.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the proposed method and compare it to the conventional models using our chat discrimination dataset.", "labels": [], "entities": []}, {"text": "In order to demonstrate that the proposed model performs well on other text classification datasets and to compare it to the existing baselines, we report the performance on various sentence classification benchmark datasets as well.", "labels": [], "entities": [{"text": "text classification", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.696513220667839}]}, {"text": "We trained the proposed CNN-LMTGRU model in an end-to-end fashion, where we do not use any pre-trained word embedding.", "labels": [], "entities": []}, {"text": "An embedding of size 300 was used for the model and was trained with the model.", "labels": [], "entities": []}, {"text": "We used 128 filters of sizes {3, 4, 5} for the CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7986786365509033}]}, {"text": "We used 300 units of MTGRU where half of the units are fast and the remaining are slow units to construct the LMTGRU structure.", "labels": [], "entities": [{"text": "MTGRU", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.7687932848930359}]}, {"text": "The \u03c4 for the fast units and the slow units were initialized to 1.0 and 1.25, respectively.", "labels": [], "entities": []}, {"text": "We follow  to initialize the timescale parameter.", "labels": [], "entities": []}, {"text": "In order to control the \u03c4 during training, we set the lower bound to 1.0 using clip by value.", "labels": [], "entities": []}, {"text": "This is done as the fastest layer should have a \u03c4 of 1.0, however there is no upper bound for the slow layers.", "labels": [], "entities": []}, {"text": "After training, the final \u03c4 values are 1.16 and 1.37 for the fast and the slow layers, respectively.", "labels": [], "entities": []}, {"text": "The learning rate to update the \u03c4 , which is different from the global learning rate, is set to 0.00001 in order to avoid large changes in the timescale.", "labels": [], "entities": []}, {"text": "We used the RMSprop Optimizer (Tieleman and Hinton, 2012) to perform stochastic gradient descent with the decay set to 0.9 and the global learning rate to 0.001.", "labels": [], "entities": [{"text": "global learning rate", "start_pos": 131, "end_pos": 151, "type": "METRIC", "confidence": 0.6909205218156179}]}, {"text": "For regularization we employ dropout of 0.5 on the final CNN output as well as in the LMTGRU layers to avoid overfitting.", "labels": [], "entities": [{"text": "regularization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.959725022315979}]}, {"text": "We utilized the validation performance for early stopping of the training for better generalization.", "labels": [], "entities": [{"text": "generalization", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.965158224105835}]}, {"text": "Following, we test our model on various benchmarks.", "labels": [], "entities": []}, {"text": "Summary statistics of the datasets are given below.", "labels": [], "entities": []}, {"text": "\u2022 MR: Movie reviews with one sentence per review.", "labels": [], "entities": [{"text": "MR", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9925077557563782}]}, {"text": "This binary classification task involves detecting positive/negative reviews (Pang and).", "labels": [], "entities": [{"text": "Pang", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.849384069442749}]}, {"text": "The average sequence length is 20 and the dataset size is 10, 662.", "labels": [], "entities": []}, {"text": "\u2022 SST-1: This is the Stanford Sentiment Treebank is an extension of MR with multiple labels (very positive, positive, neutral, negative, very negative) ().", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9286278486251831}]}, {"text": "The average sequence length is 18 and the dataset size is 11, 855.", "labels": [], "entities": []}, {"text": "\u2022 SST-2: This is similar to SST-1 but with binary labels.", "labels": [], "entities": []}, {"text": "The average sequence length is 19 and the dataset size is 9, 613.", "labels": [], "entities": []}, {"text": "\u2022 Subj: Subjectivity dataset consists of sentences with binary labels (subjective or objective).", "labels": [], "entities": []}, {"text": "The average sequence length is 23 and the dataset size is 10, 000 ().", "labels": [], "entities": []}, {"text": "\u2022 TREC: The TREC task is a classification task to classify 6 types of question (questions about person, location, numeric information, etc.).", "labels": [], "entities": [{"text": "TREC", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9558926224708557}, {"text": "TREC", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.8475391864776611}]}, {"text": "The average sequence length is 10 and the dataset size is 5, 952 ().", "labels": [], "entities": []}, {"text": "\u2022 CR: Customer reviews of various products with positive/negative labels.", "labels": [], "entities": [{"text": "CR", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9907242655754089}]}, {"text": "The average sequence length is 19 and the dataset size is 3, 775 (Hu and).", "labels": [], "entities": [{"text": "Hu", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.8732900619506836}]}, {"text": "\u2022 MPQA: Opinion polarity detection is a subtask of the MPQA dataset with 2 classes.", "labels": [], "entities": [{"text": "Opinion polarity detection", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.7736108799775442}, {"text": "MPQA dataset", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9664451777935028}]}, {"text": "The average sequence length is 3 and the dataset size is 10, 606).", "labels": [], "entities": []}, {"text": "For the evaluation on the benchmark datasets, we implemented a CNN-LMTGRU model that is identical to the one described in Section 5.1.", "labels": [], "entities": []}, {"text": "The data for train, validation, and test for the benchmark datasets follow the previous works).", "labels": [], "entities": []}, {"text": "The performance curve of the hybrid models is shown in, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Chat classification results on the test set.", "labels": [], "entities": [{"text": "Chat classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8674764335155487}]}, {"text": " Table 3: Results of our CNN-LMTGRU model against other methods on various sentence classification  benchmark datasets.", "labels": [], "entities": [{"text": "sentence classification  benchmark", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.7832952539126078}]}]}