{"title": [{"text": "Dynamic and Static Topic Model for Analyzing Time-Series Document Collections", "labels": [], "entities": []}], "abstractContent": [{"text": "For extracting meaningful topics from texts, their structures should be considered properly.", "labels": [], "entities": [{"text": "extracting meaningful topics from texts", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.841102135181427}]}, {"text": "In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past, and are also related to each other at each time.", "labels": [], "entities": []}, {"text": "To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time.", "labels": [], "entities": []}, {"text": "We show the results of experiments on collections of scientific papers , in which the proposed method out-performed conventional models.", "labels": [], "entities": []}, {"text": "Moreover , we show an example of extracted topic structures, which we found helpful for analyzing research activities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models such as latent Dirichlet allocation (LDA) () have been utilized for analyzing a wide variety of datasets such as document collections, images, and genes.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.5331518799066544}]}, {"text": "Although vanilla LDA has been favored partly due to its simplicity, one of its limitations is that the output is not necessarily very understandable because the priors on the topics are independent.", "labels": [], "entities": []}, {"text": "Consequently, there has been a lot of research aimed at improving probabilistic topic models by utilizing the inherent structures of datasets in their modeling (see, e.g., ;; see Section 2 for other models).", "labels": [], "entities": []}, {"text": "In this work, we aimed to leverage the dynamic and static structures of topics for improving the modeling capability and the understandability of topic models.", "labels": [], "entities": []}, {"text": "These two types of structures, which we instantiate below, are essential in many types of datasets, and in fact, each of them has been considered separately in several previous studies.", "labels": [], "entities": []}, {"text": "In this paper, we propose a topic model that is aware of both of these structures, namely dynamic and static topic model (DSTM).", "labels": [], "entities": []}, {"text": "The underlying motivation of DSTM is twofold.", "labels": [], "entities": [{"text": "DSTM", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.8910356760025024}]}, {"text": "First, a collection of documents often has dynamic structures; i.e., topics evolve along time influencing each other.", "labels": [], "entities": []}, {"text": "For example, topics in papers are related to topics in past papers.", "labels": [], "entities": []}, {"text": "We may want to extract such dynamic structures of topics from collections of scientific papers for summarizing research activities.", "labels": [], "entities": []}, {"text": "Second, there are also static structures of topics such as correlation and hierarchy.", "labels": [], "entities": []}, {"text": "For instance, in a collection of news articles, the \"sports\" topic must have the \"baseball\" topic and the \"football\" topic as its subtopic.", "labels": [], "entities": []}, {"text": "This kind of static structure of topics helps us understand the relationship among them.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review related work.", "labels": [], "entities": []}, {"text": "In Section 3, the generative model and the inference/learning procedures of DSTM are presented.", "labels": [], "entities": [{"text": "generative", "start_pos": 18, "end_pos": 28, "type": "TASK", "confidence": 0.9677029848098755}, {"text": "DSTM", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8340829014778137}]}, {"text": "In Section 4, the results of the experiments are shown.", "labels": [], "entities": []}, {"text": "This paper is concluded in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used two datasets comprising technical papers: NIPS (Perrone et al., 2016) and Drone (.", "labels": [], "entities": [{"text": "NIPS (Perrone et al., 2016)", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.8713715523481369}, {"text": "Drone", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.7763190865516663}]}, {"text": "NIPS is a collection of the papers that appeared in NIPS conferences.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9493157863616943}]}, {"text": "Drone is a collection of abstracts of papers on unmanned aerial vehicles (UAVs) and was collected from related conferences and journals for surveying recent developments in UAVs.", "labels": [], "entities": [{"text": "Drone", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9320566654205322}]}, {"text": "The characteristics of those datasets are summarized in.", "labels": [], "entities": []}, {"text": "See Supplementary B for the details of data preprocessing.: Means (and standard deviations) of PPLs averaged overall epochs for each dataset with different values of K and S.", "labels": [], "entities": []}, {"text": "The proposed method, DSTM, achieved the smallest PPL.: Part of the topic structure extracted from Drone dataset using the proposed method.", "labels": [], "entities": [{"text": "Drone dataset", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.9444437325000763}]}, {"text": "The solid arrows denote the temporal evolution of \"planning\" topics.", "labels": [], "entities": []}, {"text": "The dotted arrows mean that \"planning\" topics are related to \"hardware\", \"control\", and \"mapping\" topics via some supertopics (filled circles).", "labels": [], "entities": []}, {"text": "First, we evaluate the performance of the proposed method quantitatively using perplexity (PPL): For each epoch, we used 90% of tokens in each document for training and calculated the PPL using the remaining 10% of tokens.", "labels": [], "entities": []}, {"text": "We randomly created 10 train-test pairs and evaluated the means of the PPLs over those random trials.", "labels": [], "entities": []}, {"text": "We compared the performance of DSTM to three baselines: LDA (, PAM (, and the proposed model without the static structure, which we term DRTM.", "labels": [], "entities": [{"text": "LDA", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.7978349924087524}, {"text": "PAM", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.8250054717063904}, {"text": "DRTM", "start_pos": 137, "end_pos": 141, "type": "DATASET", "confidence": 0.9414399266242981}]}, {"text": "See Supplementary C on their hyperparameter setting.", "labels": [], "entities": []}, {"text": "The means of the PPLs averaged overall epochs for each dataset with different values K are shown in.", "labels": [], "entities": []}, {"text": "In both datasets with every setting of K, the proposed model, DSTM, achieved the smallest PPL, which implies its effectiveness for modeling a collection of technical papers.", "labels": [], "entities": [{"text": "PPL", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9431999325752258}]}, {"text": "For clarity, we conducted paired t-tests between the perplexities of the proposed method and those of the baselines.", "labels": [], "entities": []}, {"text": "On the differences between DSTM and DRTM, the p-values were 4.2 \u21e5 10 \ud97b\udf592 (K = 30), 7.9 \u21e5 10 \ud97b\udf595 (K = 40), and 6.4 \u21e5 10 \ud97b\udf597 (K = 50) for the NIPS dataset, and 1.3 \u21e5 10 \ud97b\udf594 (K = 15), 8.8 \u21e5 10 \ud97b\udf595 (K = 20), and 4.9 \u21e5 10 \ud97b\udf596 (K = 25) for the Drone dataset, respectively.", "labels": [], "entities": [{"text": "DRTM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8529510498046875}, {"text": "NIPS dataset", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9842469096183777}, {"text": "Drone dataset", "start_pos": 232, "end_pos": 245, "type": "DATASET", "confidence": 0.9895141124725342}]}, {"text": "It is also noteworthy that DRTM shows more significant improvement relative to LDA than PAM does.", "labels": [], "entities": [{"text": "DRTM", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.5387368202209473}]}, {"text": "This suggests that the dynamic structure with multiple-topic dependencies is essential for datasets of this kind.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of the datasets.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.5163990259170532}]}]}