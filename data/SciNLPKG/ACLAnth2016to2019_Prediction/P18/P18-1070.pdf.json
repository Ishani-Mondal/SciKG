{"title": [{"text": "STRUCTVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.6566932201385498}]}], "abstractContent": [{"text": "Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8271439969539642}, {"text": "transducing natural language (NL) utterances into formal meaning representations (MRs)", "start_pos": 32, "end_pos": 118, "type": "TASK", "confidence": 0.831762603351048}]}, {"text": "Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models.", "labels": [], "entities": [{"text": "Annotating NL utterances", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8376354177792867}]}, {"text": "We introduce STRUCTVAE, a vari-ational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances.", "labels": [], "entities": [{"text": "STRUCTVAE", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.6652356386184692}, {"text": "semi-supervised semantic parsing", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.704054057598114}]}, {"text": "STRUCTVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables.", "labels": [], "entities": [{"text": "STRUCTVAE", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.861266016960144}]}, {"text": "Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, STRUCTVAE outperforms strong supervised models.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7472535371780396}, {"text": "ATIS domain", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9408904016017914}, {"text": "Python code generation", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.540546049674352}]}], "introductionContent": [{"text": "Semantic parsing tackles the task of mapping natural language (NL) utterances into structured formal meaning representations (MRs).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.854485809803009}, {"text": "mapping natural language (NL) utterances into structured formal meaning representations (MRs)", "start_pos": 37, "end_pos": 130, "type": "TASK", "confidence": 0.757237684726715}]}, {"text": "This includes parsing to general-purpose logical forms such as \u03bb-calculus and the abstract meaning representation (AMR,;), as well as parsing to computerexecutable programs to solve problems such as question answering, or generation of domainspecific (e.g., SQL) or general purpose programming languages (e.g., Python) ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.7497029602527618}]}, {"text": "1 Code available at http://pcyin.me/struct vae While these models have along history (), recent advances are largely attributed to the success of neural network models (.", "labels": [], "entities": []}, {"text": "However, these models are also extremely data hungry: optimization of such models requires large amounts of training data of parallel NL utterances and manually annotated MRs, the creation of which can be expensive, cumbersome, and time-consuming.", "labels": [], "entities": []}, {"text": "Therefore, the limited availability of parallel data has become the bottleneck of existing, purely supervised-based models.", "labels": [], "entities": []}, {"text": "These data requirements can be alleviated with weakly-supervised learning, where the denotations (e.g., answers in question answering) of MRs (e.g., logical form queries) are used as indirect supervision (;;, inter alia), or dataaugmentation techniques that automatically generate pseudo-parallel corpora using hand-crafted or induced grammars.", "labels": [], "entities": [{"text": "question answering) of MRs", "start_pos": 115, "end_pos": 141, "type": "TASK", "confidence": 0.8405184268951416}]}, {"text": "In this work, we focus on semi-supervised learning, aiming to learn from both limited amounts of parallel NL-MR corpora, and unlabeled but readily-available NL utterances.", "labels": [], "entities": []}, {"text": "We draw inspiration from recent success in applying variational auto-encoding (VAE) models in semisupervised sequence-to-sequence learning , and propose STRUCTVAE -a principled deep generative approach for semi-supervised learning with tree-structured latent variables.", "labels": [], "entities": []}, {"text": "STRUCT-VAE is based on a generative story where the surface NL utterances are generated from treestructured latent MRs following the standard VAE architecture: (1) an off-the-shelf semantic parser functions as the inference model, parsing an observed NL utterance into latent meaning representations ( \u00a7 3.2); (2) a reconstruction model decodes the latent MR into the original observed utterance.", "labels": [], "entities": [{"text": "STRUCT-VAE", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.7215225696563721}]}, {"text": "This formulation enables our model to perform both standard supervised learning by optimizing the inference model (i.e., the parser) using parallel corpora, and unsupervised learning by maximizing the variational lower bound of the likelihood of the unlabeled utterances.", "labels": [], "entities": []}, {"text": "In addition to these contributions to semisupervised semantic parsing, STRUCTVAE contributes to generative model research as a whole, providing a recipe for training VAEs with structured latent variables.", "labels": [], "entities": [{"text": "semisupervised semantic parsing", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6295561989148458}, {"text": "STRUCTVAE", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.6956346035003662}, {"text": "generative model research", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.9188633759816488}]}, {"text": "Such a structural latent space is contrast to existing VAE research using flat representations, such as continuous distributed representations (, discrete symbols , or hybrids of the two (.", "labels": [], "entities": [{"text": "VAE", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.7003295421600342}]}, {"text": "We apply STRUCTVAE to semantic parsing on the ATIS domain and Python code generation.", "labels": [], "entities": [{"text": "STRUCTVAE", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.5938546657562256}, {"text": "semantic parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7793949544429779}, {"text": "ATIS domain", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9543983638286591}, {"text": "Python code generation", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.6808957755565643}]}, {"text": "As an auxiliary contribution, we implement a transition-based semantic parser, which uses Abstract Syntax Trees (ASTs, \u00a7 3.2) as intermediate MRs and achieves strong results on the two tasks.", "labels": [], "entities": []}, {"text": "We then apply this parser as the inference model for semi-supervised learning, and show that with extra unlabeled data, STRUCTVAE outperforms its supervised counterpart.", "labels": [], "entities": []}, {"text": "We also demonstrate that STRUCTVAE is compatible with different structured latent representations, applying it to a simple sequence-to-sequence parser which uses \u03bb-calculus logical forms as MRs.", "labels": [], "entities": [{"text": "STRUCTVAE", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.4038928747177124}]}], "datasetContent": [{"text": "In our semi-supervised semantic parsing experiments, it is of interest how STRUCTVAE could further improve upon a supervised parser with extra unlabeled data.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7148172408342361}]}, {"text": "We evaluate on two datasets: Semantic Parsing We use the ATIS dataset, a collection of 5,410 telephone inquiries of flight booking (e.g., \"Show me flights from ci0 to ci1\").", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.6938861757516861}, {"text": "ATIS dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9314599335193634}]}, {"text": "The target MRs are defined using \u03bb-calculus logical forms (e.g., \"lambda $0 e (and (flight $0) (from $ci0) (to $ci1))\").", "labels": [], "entities": []}, {"text": "We use the pre-processed dataset released by, where entities (e.g., cities) are canonicalized using typed slots (e.g., ci0).", "labels": [], "entities": []}, {"text": "To predict \u03bb-calculus logical forms using our transition-based parser, we use the ASDL grammar defined by to convert between logical forms and ASTs (see Appendix C for details). and use the astor package to convert ASDL ASTs into Python source code.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance on ATIS w.r.t. the size of labeled train- ing data L.  \u2020 Existing neural network-based methods", "labels": [], "entities": []}, {"text": " Table 2: Performance on DJANGO w.r.t. the size of labeled  training data L", "labels": [], "entities": [{"text": "DJANGO", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.7065966129302979}]}, {"text": " Table 4: Performance of the STRUCTVAE-SEQ on ATIS  w.r.t. the size of labeled training data L", "labels": [], "entities": [{"text": "STRUCTVAE-SEQ", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.7701225280761719}, {"text": "ATIS  w.r.t.", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8192094564437866}]}, {"text": " Table 5: Comparison of STRUCTVAE with different base- line functions b(x), italic  \u2020 : semi-supervised learning with  the MLP baseline is worse than supervised results.", "labels": [], "entities": []}]}