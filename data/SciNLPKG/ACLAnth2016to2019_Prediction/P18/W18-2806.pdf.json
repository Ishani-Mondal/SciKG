{"title": [], "abstractContent": [{"text": "We present a novel methodology involving mappings between different modes of semantic representations.", "labels": [], "entities": []}, {"text": "We propose dis-tributional semantic models as a mechanism for representing the kind of world knowledge inherent in the system of abstract symbols characteristic of a sophisticated community of language users.", "labels": [], "entities": []}, {"text": "Then, motivated by insight from ecological psychology , we describe a model approximating affordances, by which we mean a language learner's direct perception of opportunities for action in an environment.", "labels": [], "entities": []}, {"text": "We present a preliminary experiment involving mapping between these two rep-resentational modalities, and propose that our methodology can become the basis fora cognitively inspired model of grounded language learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational approaches to grounded language learning have typically involved mapping from perceptual to linguistic modalities through the application of complex information processing operations., for instance, use hidden Markov models to translate from object tracks to natural language descriptions of event observed in video clips.", "labels": [], "entities": []}, {"text": "Likewise the ImageNet database has provided a platform for the productive application of deep neural network architectures for mapping between images and natural language labels (.", "labels": [], "entities": []}, {"text": "Significantly with regard to the ideas outlined here, describe a methodology for training an agent to construct novel sequences of actions based on analogies with previously learned strategies; the mechanism for learning a vocabulary of basic actions consists of a combination of convolutional and LSTM layers within a neural network.", "labels": [], "entities": []}, {"text": "Work of this nature highlights the state of the art in modelling technologies, and as an information engineering approach to meaningful tasks such as question answering and image labelling a significant contribution is made.", "labels": [], "entities": [{"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.9033806622028351}, {"text": "image labelling", "start_pos": 173, "end_pos": 188, "type": "TASK", "confidence": 0.6857019513845444}]}, {"text": "This is arguably done, however, at the expense of presenting interpretable or indeed plausible models of the way that environmentally embedded agents use relatively scant exposure to a language speaking community in order to develop a lexicon that is rich and productive.", "labels": [], "entities": []}, {"text": "In this regard, the conventional computational stance on grounded language learning embraces a view of the relationship between language and the world as a symbol grounding problem, by which abstract symbols susceptible to formal operations are somehow associated with perceptions and propositions: the hard work is done by a complex and philosophically opaque process of transforming signals into symbols, with the sense that computation byway of deep nets in some sense stands in for an inscrutable mind-brain gestalt.", "labels": [], "entities": []}, {"text": "As an alternative to this approach, R \u02db aczaszek- propose a symbol ungrounding problem: by this account, language begins as a semiotic structure with the representational scheme of a nascent language learner iconically and indexically aligned to embodied and embedded experiences of the world.", "labels": [], "entities": []}, {"text": "This alignment is understood in terms of notion of affordances, which we take to mean the direct perception of opportunities for action in an environment.", "labels": [], "entities": []}, {"text": "The connection of language to opportunities for taking action on objects (and indeed the perception of language itself as an affordance for communication) creates a framework for understanding how abstract symbols begin as grounded complexes of multi-modal interactions with a language teacher and then gradually emerge as con-straints on the way that a cognitive agent behaves in an environment).", "labels": [], "entities": []}, {"text": "The strength of thinking of perception in general and language in particular in terms of affordances is that this moves away from the problem of the computational load associated with the spontaneous construction of contextually productive representational structures.", "labels": [], "entities": []}, {"text": "For Clark (1997), affordances play a role in the an action-oriented model of cognition revolving around light-weight, environmentally situated representations, while proposes affordances as a mechanism for resolving the issue of the mental gymnastics inherent in a computational cognitive model.", "labels": [], "entities": []}, {"text": "These approaches, which seek to place mind in the context of environmental embodiment and embeddedness, prefigure recent attempts to introduce affordances as a component of a cognitively oriented theory of language in which words can be mapped to denotations oriented towards action on objects in situations, and utterances themselves become opportunities for communication.", "labels": [], "entities": []}, {"text": "Despite these valuable theoretical contributions, affordances have proved resistant to empirical modelling, not least because it is difficult to come up with a tractable scheme for representing a cognitive feature that is specifically conceived as an antidote to representational approaches to cognition.", "labels": [], "entities": []}, {"text": "Our present objective is to begin to map away towards the computational simulation of the role of affordances in language acquisition through interaction with an established linguistic community.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.6932004690170288}]}, {"text": "In order to do this, we'll extract both statistical and syntactic information from a large-scale corpus to model two different modes of semantic representation, one geared towards the kind of world-knowledge inherent in the evolution of language on the time-scale of a community of language users, the other designed to reflect the way that an agent might encounter language grounded in the affordances of denoted objects.", "labels": [], "entities": []}, {"text": "In as much as we will be combining established distributional semantic techniques with likewise established syntactic analysis, this work can be broadly positioned in the context of other recent models., for instance, compound co-occurrence and syntactical information in order to generate word-embeddings enhanced for compositional tasks.", "labels": [], "entities": []}, {"text": "Vuli\u00b4cVuli\u00b4c (2017) likewise uses information about dependency relationships to map word embeddings from multiple languages into a shared vector space, achieving impressive results on cross-lingual versions of word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 210, "end_pos": 231, "type": "TASK", "confidence": 0.7481490075588226}]}, {"text": "An important caveat regarding our own research, however, is that we are using syntactical information as a kind of stand-in fora simulation of the way that an agent might encounter words aligned with events involving objects: in the end, we would actually like to seethe methodology outlined here as groundwork towards a model of language acquisition which specifically does not fallback on the kind of rich linguistic knowledge inherent in either vector space models or dependency parsers.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 330, "end_pos": 350, "type": "TASK", "confidence": 0.7297556698322296}]}], "datasetContent": [{"text": "Beginning with the framework described above, we first examine the degree to which our representations capture properties associated with the denotations of some basic nouns.", "labels": [], "entities": []}, {"text": "In order to establish a small-scale vocabulary of objects, we turn to the tables of words exemplifying types of objects described by in her seminal work on conceptual prototypes.", "labels": [], "entities": []}, {"text": "We choose the five words that were reported as most prototypical of five conceptual categories, as determined by a survey of a large number of respondents.", "labels": [], "entities": []}, {"text": "The categories are VEHICLES, CLOTHING, TOOLS, FURNITURE, and FRUIT.", "labels": [], "entities": [{"text": "VEHICLES", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.947806179523468}, {"text": "CLOTHING", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9546947479248047}, {"text": "TOOLS", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9441425204277039}, {"text": "FURNITURE", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9963339567184448}, {"text": "FRUIT", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9731312990188599}]}, {"text": "Our objective for this preliminary work will be to establish representations of these object types in both a distributional semantic vector space and a probabilistic affordance space.", "labels": [], "entities": []}, {"text": "In order to explore the effectiveness of the conceptual spaces generated by the representational techniques described above, we first extract the word-vectors corresponding to our vocabulary from the word2vec distributional semantic model and perform k-means clustering on these, specifying a total of five target classes.", "labels": [], "entities": []}, {"text": "Results are reported in the WORD-VECTORS column in Table 1.", "labels": [], "entities": [{"text": "WORD-VECTORS", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.5519680380821228}]}, {"text": "While these clusters do not correspond exactly with human judgement, they do align somewhat with the expected delineations between object classes.", "labels": [], "entities": []}, {"text": "The large cluster containing a mix of furniture and tools is characterised by words like saw and ruler which are presumably affected by a high degree of word sense ambiguity.", "labels": [], "entities": []}, {"text": "Next we explore the space of affordances.", "labels": [], "entities": []}, {"text": "The representations in this space are, as described above, construed as matrices of probabilities.", "labels": [], "entities": []}, {"text": "Specifically, we take the top 20 most likely words: Clusters of word representations in distributional semantic and probabilistic affordance spaces.", "labels": [], "entities": []}, {"text": "Word-vectors are clustered based on k-means clustering, and affordance representations are clustered based on a k-medoid algorithm, with the most cost-effective medoids indicated in bold.", "labels": [], "entities": []}, {"text": "for each grammatical class and generate probabilities for each word in each of these classes for each of our 25 object-words.", "labels": [], "entities": []}, {"text": "In order to calculate the distance between two affordance representations, we take the Hellinger distance between two aligned probability distributions.", "labels": [], "entities": []}, {"text": "This operation, which we take as a good quantification of the relationship between two distributions, results in a matrix of three dimensional vectors, each element corresponding to a grammatical class.", "labels": [], "entities": []}, {"text": "So, for two vocabulary words a and band a grammatical class c, the element of a vector representing the relationship between those two words can be described as follows, where h is the label for one of the top 20 words occurring in that grammatical class: 3) We treat the set of three values corresponding to each target-to-target relationship as a distance vector, and so consider the distance between those two words to be simply the norm of that vector.", "labels": [], "entities": []}, {"text": "With a distance matrix thus established, we use a k-medoids algorithm of our own design to cluster the affordance representations.", "labels": [], "entities": []}, {"text": "We apply this measure because we are working from a matrix of distances, rather than from an explicit vector space; we might also consider, for instance, multidimensional scaling to project these representations into a vector space, but we consider the kmedoid approach to be appropriate for our present purposes.", "labels": [], "entities": []}, {"text": "Results are reported in the right column of, with optimal medoids highlighted.", "labels": [], "entities": []}, {"text": "As with the clustering of word-vectors, the results here do not correspond perfectly with human judgements.", "labels": [], "entities": []}, {"text": "We don't see this as necessarily being a problem, though: it would be strange, in fact, to expect a developing cognitive agent to categorically classify each object based on affordanceoriented interactions with an environment.", "labels": [], "entities": []}, {"text": "So, for instance, fruits are compounded with some furniture and some tools in a single category orbiting the highly ambiguous term orange.", "labels": [], "entities": []}, {"text": "The crucial question is how we can effectively map between word-vectors, which we take to represent a kind of encyclopaedic knowledge of the world, and the affordances which are proposed as at least a rough model of the way that words are encountered by an early language learner.", "labels": [], "entities": []}, {"text": "In order to explore this issue further, we construct a rudimentary neural network, mapping the 200 elements of each of our word-vectors onto the sets of probability distributions corresponding to affordances byway of a single dense softmax layer.", "labels": [], "entities": []}, {"text": "This operation is in effect quite similar to a multiclass logistic regression, except that here we are attempting to learn to approximate an actual probability distribution rather than to simply reward the assignment of the highest score to a particular class.", "labels": [], "entities": []}, {"text": "Formally, we map from a word-vector \u2212 \u2192 v w to a probability distribution p(x n |w) associated with a word x n observed participating with vocabulary word was a member of grammatical class X by learning a weight matrix M , expressed herein terms of dot products with each row \u2212 \u2212 \u2192 m x k associated with members (x 1 ...x |X| ) of class X: A separate weight matrix is learned for each of the three grammatical classes associate with the objects that we seek to model.", "labels": [], "entities": []}, {"text": "As a basic test of the generality of this network, we perform a five-fold cross-validation, holding one term from each class out of the network construction process for each fold.: Accuracy rates for mapping from distributional semantic word-vectors to affordance matrices, from a word to the same word and from a word to another word of the same class.", "labels": [], "entities": []}, {"text": "vector is considered to map to the point in the space of affordance matrices that is closest based on Hellinger as technique described above.", "labels": [], "entities": []}, {"text": "This experiment is designed to test the ability of this simple model to map between two different modes of semantic representation, one based on a largescale analysis of the way that words occur in the context of a complex, developed vocabulary, the other utilising syntax to simulate the small-scale encounter of words as mapping to opportunities for action on corresponding denotations.", "labels": [], "entities": []}, {"text": "While the network generally fails to make exact word-to-word mappings, it is notable that it does, more often than not, manage to map a word-vector to an affordance representation corresponding to another word of at least the correct class.", "labels": [], "entities": []}, {"text": "We suggest that this indicates there is some basic categorical information in word-vector representations that can be aligned with data about the way that objects are predictably encountered in the world.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy rates for mapping from distributional semantic word-vectors to affordance matrices,  from a word to the same word and from a word to another word of the same class.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9899317026138306}]}]}