{"title": [{"text": "A Sequence Learning Method for Domain-Specific Entity Linking", "labels": [], "entities": [{"text": "Domain-Specific Entity Linking", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.6710496346155802}]}], "abstractContent": [{"text": "Recent collective Entity Linking studies usually promote global coherence of all the mapped entities in the same document by using semantic embeddings and graph-based approaches.", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7384936213493347}]}, {"text": "Although graph-based approaches are shown to achieve remarkable results, they are computationally expensive for general datasets.", "labels": [], "entities": []}, {"text": "Also, semantic embeddings only indicate relatedness between entity pairs without considering sequences.", "labels": [], "entities": []}, {"text": "In this paper, we address these problems by introducing a twofold neu-ral model.", "labels": [], "entities": []}, {"text": "First, we match easy mention-entity pairs and using the domain information of this pair to filter candidate entities of closer mentions.", "labels": [], "entities": []}, {"text": "Second, we resolve more ambiguous pairs using bidirectional Long Short-Term Memory and CRF models for the entity disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.7170297205448151}]}, {"text": "Our proposed system outperforms state-of-the-art systems on the generated domain-specific evaluation dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity Linking is the task of matching ambiguous mentions in a text to the corresponding entities in the given knowledge base.", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7516656517982483}, {"text": "matching ambiguous mentions in a text", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.8118168910344442}]}, {"text": "The output of the entity linking is a crucial step for many tasks, including relation extraction (, link prediction () and knowledge graph completion (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8227904140949249}, {"text": "link prediction", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7740911841392517}, {"text": "knowledge graph completion", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.672351727883021}]}, {"text": "The main challenge is to disambiguate candidate entities for the given mentions.", "labels": [], "entities": []}, {"text": "For instance, it requires to resolve the mention Wicker Park in the following text \"Wicker Park is a 2004 American psychological drama mystery film directed by Paul McGuigan and starring Josh Hartnett...\" to the referent entity Wicker Park (film) 1 in DBpedia.", "labels": [], "entities": [{"text": "Wicker Park is a 2004 American psychological drama mystery film", "start_pos": 84, "end_pos": 147, "type": "TASK", "confidence": 0.5303683519363404}, {"text": "DBpedia", "start_pos": 252, "end_pos": 259, "type": "DATASET", "confidence": 0.7400199174880981}]}, {"text": "But the mention Wicker Park has three different candidate entities as indicated in the Wikipedia disambiguation page of this mention.", "labels": [], "entities": [{"text": "Wicker Park", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9006534814834595}]}, {"text": "The key step for entity disambiguation is the similarity computation between mention-entity and entity-entity pairs.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7102681249380112}]}, {"text": "Early studies focused on modeling the similarity between local context that computes the similarity between mention context and relevant candidate entities (.", "labels": [], "entities": []}, {"text": "Recent state-of-the-art methods consider global coherence that is the relatedness between all candidate entities in the same document).", "labels": [], "entities": []}, {"text": "These methods depend on well-defined link structures as seen in Wikipedia to compute global coherence.", "labels": [], "entities": []}, {"text": "After the emergence of word embeddings (, it facilitates to produce more generalized coherence computations without using hand-crafted features.", "labels": [], "entities": []}, {"text": "Hence, the dependency of well-defined knowledge bases has decreased and knowledge base agnostic approaches become revealed (.", "labels": [], "entities": []}, {"text": "Most recent deep learning approaches have been presented as away to support better generalization for the similarity measurement of context, mention and entity (.", "labels": [], "entities": []}, {"text": "Also, mentions and entities are combined into the same continuous vector space for the entity disambiguation (.", "labels": [], "entities": []}, {"text": "From a different perspective, the entity disambiguation should be transformed into as a sequence learning task to capture more generalized semantics between candidate entities and also mentions.", "labels": [], "entities": []}, {"text": "In this paper, we generate RDF embeddings ( as the input of a sequence learning model using bidirectional Long Short-Term Memory (LSTM)).", "labels": [], "entities": []}, {"text": "Then, we perform Conditional Random Field (CRF) to match the best mention-entity pairs.", "labels": [], "entities": [{"text": "Conditional Random Field (CRF)", "start_pos": 17, "end_pos": 47, "type": "METRIC", "confidence": 0.7796221077442169}]}, {"text": "LSTM networks are not suitable for large entity vocabularies since English DBpedia contains more than 5M entities.", "labels": [], "entities": [{"text": "English DBpedia", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.8154944777488708}]}, {"text": "To reduce the size of these vocabularies, our study employs the two-fold method.", "labels": [], "entities": []}, {"text": "First, we match easy mention-entity pairs in which each mention contains only one candidate entity.", "labels": [], "entities": []}, {"text": "Similar to AIDAlight study we identify the domain of the given text and the size of candidate entities are reduced to reasonable dimensions for the detected domain.", "labels": [], "entities": []}, {"text": "The contributions of our study can be summarized as below: \u2022 Our study proposes a novel algorithm that first disambiguates easy mention-entity pairs fora specific domain.", "labels": [], "entities": []}, {"text": "Thereafter, it applies CRF model to link more ambiguous entities.", "labels": [], "entities": []}, {"text": "\u2022 Our study provides a sequence learning model like a translation task in which a sequence of mentions will be translated into a sequence of referent entities in the domainspecific knowledge base.", "labels": [], "entities": []}, {"text": "Our method employs one of prominent Named Entity Recognition approaches ( to perform a domain-specific Entity Linking.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6619127293427786}, {"text": "Entity Linking", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7205518782138824}]}, {"text": "We aim to model the topical coherence of the mention-entity pairs in terms of a sequence labeling task.", "labels": [], "entities": []}, {"text": "We conduct the experimental setup using the well-known evaluation framework called GERBIL) to compare our study with the state-of-the-art Entity Linking systems.", "labels": [], "entities": [{"text": "GERBIL", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.5577589869499207}]}, {"text": "The rest of this paper is organized as follows: In Section 2, it gives an overview of related work.", "labels": [], "entities": []}, {"text": "In Section 3, the sequence learning method is proposed fora specific domain.", "labels": [], "entities": [{"text": "sequence learning", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7705003023147583}]}, {"text": "Section 4 presents the experiments are for the selected approaches on the prepared evaluation dataset.", "labels": [], "entities": []}, {"text": "We conclude our study and highlight the research questions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Manually annotated texts tend to be biased because people usually select familiar terms for the entity annotation.", "labels": [], "entities": []}, {"text": "Also, this annotation process is sometimes noisy for unpopular terms.", "labels": [], "entities": []}, {"text": "Therefore, Wikipedia should be chosen because it is curated by crowdsourcing and involves structured annotation process.", "labels": [], "entities": []}, {"text": "MSNBC, IITB () and Wikilinks ( proposes experimental datasets for general entity annotation tasks.", "labels": [], "entities": [{"text": "MSNBC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9850350618362427}, {"text": "IITB", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9106314182281494}, {"text": "general entity annotation tasks", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7238283157348633}]}, {"text": "Wikilinks provides a large-scale labeled corpus automatically constructed via links to Wikipedia.", "labels": [], "entities": [{"text": "Wikilinks", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.955176591873169}]}, {"text": "Wikilinks presents an automated method to identify a collection of massive amounts of entity mentions and is based on crawling anchor links in Wikipedia pages and exploiting anchor text as mentions.", "labels": [], "entities": [{"text": "Wikilinks", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8642749190330505}]}, {"text": "However, Wikipedia can also be employed for the level of ambiguity adjustments in order to use disambiguation pages and this is not directly indicated in Wikilinks.", "labels": [], "entities": []}, {"text": "Ambiguity is the ratio between ambiguous and unique entities and provides more realistic environment to entity annotators (.", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9775298833847046}]}, {"text": "To adjust ambiguity and generate annotated texts for specific domains, we use a recent study) which extracts the latest Wikipedia dump in English 6 for specific domains.", "labels": [], "entities": []}, {"text": "To do it, they use Wikipedia category pages and DBpedia \"dct:subject\" 7 property.", "labels": [], "entities": []}, {"text": "Also, they provide an ambiguous environment in which, Wikipedia disambiguation pages are used for the selected domains.", "labels": [], "entities": []}, {"text": "As an example, the mention Wicker Park has a Wikipedia disambiguation page 8 and it can be used to increase ambiguity in the movie domain.", "labels": [], "entities": []}, {"text": "The movie evaluation dataset involves 123 annotated texts in English.", "labels": [], "entities": [{"text": "movie evaluation dataset", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.6454100012779236}]}, {"text": "For each text, the average number of entities is 4.99 and there are 614 entities in total.", "labels": [], "entities": []}, {"text": "Entities such as movies, directors, and starring are extracted from infoboxes of Wikipedia articles and mapped with referent entities by DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 137, "end_pos": 144, "type": "DATASET", "confidence": 0.9515081644058228}]}, {"text": "Disambiguation pages of these entities are extracted in other domains such as music and location to increase the ratio of ambiguity in the evaluation dataset for the movie domain.", "labels": [], "entities": []}, {"text": "The ambiguity ratio of the evaluation dataset is 48.79% computed as the division of all ambiguous entities to the total number of unique entities extracted for the movie domain.", "labels": [], "entities": []}, {"text": "Therefore, a more realistic ambiguous dataset can be generated to evaluate Entity Linking systems.", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.8882929980754852}]}], "tableCaptions": [{"text": " Table 1: Evaluation scores of Entity Linking (EL) systems in GERBIL.", "labels": [], "entities": [{"text": "GERBIL", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.8812693357467651}]}]}