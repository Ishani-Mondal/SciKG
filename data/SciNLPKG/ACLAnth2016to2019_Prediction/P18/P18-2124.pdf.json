{"title": [{"text": "Know What You Don't Know: Unanswerable Questions for SQuAD", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.5957050919532776}]}], "abstractContent": [{"text": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context.", "labels": [], "entities": []}, {"text": "Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify.", "labels": [], "entities": []}, {"text": "To address these weaknesses, we present SQUADRUN, anew dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 90, "end_pos": 133, "type": "DATASET", "confidence": 0.8504527977534703}]}, {"text": "To do well on SQUADRUN, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.", "labels": [], "entities": []}, {"text": "SQUADRUN is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQUADRUN.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7328948577245077}, {"text": "F1", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9959717392921448}, {"text": "F1", "start_pos": 150, "end_pos": 152, "type": "METRIC", "confidence": 0.9987083673477173}]}, {"text": "We release SQUADRUN to the community as the successor to SQuAD.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (.", "labels": [], "entities": [{"text": "Machine reading comprehension", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8373177250226339}, {"text": "natural language understanding", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.649289627869924}]}, {"text": "In turn, these datasets have spurred a diverse array of model architecture improvements (; Hu * The first two authors contributed equally to this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, we survey existing reading comprehension datasets with these criteria in mind.", "labels": [], "entities": []}, {"text": "We use the term \"negative example\" to refer to a context passage paired with an unanswerable question.", "labels": [], "entities": []}, {"text": "In extractive reading comprehension datasets, a system must extract the correct answer to a question from a context document or paragraph.", "labels": [], "entities": []}, {"text": "The Zero-shot Relation Extraction dataset ( contains negative examples generated with distant supervision.", "labels": [], "entities": [{"text": "Zero-shot Relation Extraction dataset", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.6908816322684288}]}, {"text": "found that 65% of these negative examples do not have a plausible answer, making them easy to identify.", "labels": [], "entities": []}, {"text": "Other distant supervision strategies can also create negative examples.", "labels": [], "entities": []}, {"text": "TriviaQA () retrieves context documents from the web or Wikipedia for each question.", "labels": [], "entities": []}, {"text": "Some documents do not contain the correct answer, yielding negative examples; however, these are excluded from the final dataset.", "labels": [], "entities": []}, {"text": "generate negative examples for SQuAD by pairing existing questions with other paragraphs from the same article based on TF-IDF overlap; we refer to these as TFIDF examples.", "labels": [], "entities": []}, {"text": "In general, distant supervision does not ensure the existence of a plausible answer in the retrieved context, and might also add noise, as the context might contain a paraphrase of the correct answer.", "labels": [], "entities": []}, {"text": "Moreover, when retrieving from a small set of possible contexts, as in Clark and Gardner (2017), we find that the retrieved paragraphs are often not very relevant to the question, making these negative examples easy to identify.", "labels": [], "entities": []}, {"text": "The NewsQA data collection process also yields unanswerable questions, because crowdworkers write questions given only a summary of an article, not the full text (.", "labels": [], "entities": [{"text": "NewsQA data collection", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8758601148923238}]}, {"text": "Only 9.5% of their questions are unanswerable, making this strategy hard to scale.", "labels": [], "entities": []}, {"text": "Of this fraction, we found that some are misannotated as unanswerable, and others are out-of-scope (e.g., summarization questions).", "labels": [], "entities": [{"text": "summarization", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.9752667546272278}]}, {"text": "also exclude negative examples from their final dataset.", "labels": [], "entities": []}, {"text": "Jia and Liang (2017) propose a rule-based procedure for editing SQuAD questions to make them unanswerable.", "labels": [], "entities": []}, {"text": "Their questions are not very diverse: they only replace entities and numbers with similar words, and replace nouns and adjectives with WordNet antonyms.", "labels": [], "entities": []}, {"text": "We refer to these unanswerable questions as RULEBASED questions.", "labels": [], "entities": [{"text": "RULEBASED", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9482657313346863}]}, {"text": "Sentence selection datasets test whether a system can rank sentences that answer a question higher  Finally, some datasets, like MCTest (Richardson et al., 2013) and RACE (, pose multiple choice questions, which can have a \"none of the above\" option.", "labels": [], "entities": [{"text": "MCTest (Richardson et al., 2013)", "start_pos": 129, "end_pos": 161, "type": "DATASET", "confidence": 0.8593852818012238}]}, {"text": "In practice, multiple choice options are often unavailable, making these datasets less suited for training user-facing systems.", "labels": [], "entities": []}, {"text": "Multiple choice questions also tend to be quite different from extractive ones, with more emphasis on fill-in-the-blank, interpretation, and summarization ().", "labels": [], "entities": [{"text": "interpretation", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.937343955039978}, {"text": "summarization", "start_pos": 141, "end_pos": 154, "type": "TASK", "confidence": 0.9491528272628784}]}, {"text": "We now describe our new dataset, which we constructed to satisfy both the relevance and plausible answer desiderata from Section 2.", "labels": [], "entities": []}, {"text": "We employed crowdworkers on the Daemo crowdsourcing platform () to write unanswerable questions.", "labels": [], "entities": [{"text": "Daemo crowdsourcing platform", "start_pos": 32, "end_pos": 60, "type": "DATASET", "confidence": 0.9441757003466288}]}, {"text": "Each task consisted of an entire article from the original SQuAD dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.85764479637146}]}, {"text": "For each paragraph in the article, workers were asked to pose up to five questions that were impossible to answer based on the paragraph alone, while referencing entities in the paragraph and ensuring that a plausible answer is present.", "labels": [], "entities": []}, {"text": "As inspiration, we also showed questions from SQuAD for each paragraph; this further encouraged unanswerable questions to look similar to answerable ones.", "labels": [], "entities": []}, {"text": "Workers were asked to spend 7 minutes per paragraph, and were paid $10.50 per hour.", "labels": [], "entities": []}, {"text": "Screenshots of our interface are shown in Appendix A.1.", "labels": [], "entities": [{"text": "Appendix A.1", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.698379248380661}]}, {"text": "We removed questions from workers who wrote 25 or fewer questions on that article; this filter helped remove noise from workers who had trouble understanding the task, and therefore quit before completing the whole article.", "labels": [], "entities": []}, {"text": "We applied this filter to both our new data and the existing answerable questions in SQuAD.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.8033527731895447}]}, {"text": "To generate train, development, and test splits, we used the same partition of articles as SQuAD, and combined the existing SQuAD data with our new data for each split.", "labels": [], "entities": []}, {"text": "For the SQUADRUN development and test sets, we removed articles for which we did not col-  lect unanswerable questions.", "labels": [], "entities": [{"text": "SQUADRUN development and test sets", "start_pos": 8, "end_pos": 42, "type": "DATASET", "confidence": 0.7502846240997314}]}, {"text": "This resulted in a roughly one-to-one ratio of answerable to unanswerable questions in these splits, whereas the train data has roughly twice as many answerable questions as unanswerable ones.", "labels": [], "entities": []}, {"text": "summarizes overall statistics of SQUADRUN.", "labels": [], "entities": [{"text": "SQUADRUN", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.5325357913970947}]}], "tableCaptions": [{"text": " Table 2: Dataset statistics of SQUADRUN, compared  to the original SQuAD dataset.", "labels": [], "entities": [{"text": "SQUADRUN", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.7898517847061157}, {"text": "SQuAD dataset", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.9329198896884918}]}, {"text": " Table 3: Exact Match (EM) and F1 scores on SQUADRUN and SQuAD. The gap between humans and the best  tested model is much larger on SQUADRUN, suggesting there is a great deal of room for model improvement.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9427135109901428}, {"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.999357283115387}, {"text": "SQUADRUN", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.8553560972213745}, {"text": "SQuAD", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.753911554813385}]}, {"text": " Table 4: Exact Match (EM) and F1 scores on the SQUADRUN development set, compared with SQuAD with  two types of automatically generated negative examples. SQUADRUN is more challenging for current models.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9447230577468873}, {"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9991324543952942}, {"text": "SQUADRUN development set", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.8388779560724894}]}]}