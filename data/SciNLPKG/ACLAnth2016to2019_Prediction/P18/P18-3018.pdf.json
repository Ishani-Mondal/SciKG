{"title": [{"text": "Trick Me If You Can: Adversarial Writing of Trivia Challenge Questions", "labels": [], "entities": [{"text": "Adversarial Writing of Trivia Challenge", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.8453403234481811}]}], "abstractContent": [{"text": "Modern question answering systems have been touted as approaching human performance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8758944272994995}]}, {"text": "However, existing question answering datasets are imperfect tests.", "labels": [], "entities": [{"text": "question answering", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7719208598136902}]}, {"text": "Questions are written with humans in mind, not computers, and often do not properly expose model limitations.", "labels": [], "entities": []}, {"text": "To address this, we develop an adversarial writing setting, where humans interact with trained models and try to break them.", "labels": [], "entities": []}, {"text": "This annotation process yields a challenge set, which despite being easy for trivia players to answer, systematically stumps automated question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7097635120153427}]}, {"text": "Diagnosing model errors on the evaluation data provides actionable insights to explore in developing robust and generalizable question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7563858926296234}]}], "introductionContent": [{"text": "Proponents of modern machine learning systems have claimed human parity on difficult tasks such as question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8104175627231598}]}, {"text": "1 Datasets such as SQuAD and TriviaQA () have certainly advanced the state of the art, but are they providing the right examples to measure how well machines can answer questions?", "labels": [], "entities": []}, {"text": "Many of the existing question answering datasets are written and evaluated with humans in mind, not computers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7871351838111877}]}, {"text": "Though the way computers solve NLP tasks is fundamentally different than humans.", "labels": [], "entities": []}, {"text": "They train on hundreds of thousands of questions, rather than looking at small groups of them in isolation.", "labels": [], "entities": []}, {"text": "This allows models to pickup on superficial patterns that may occur in data crawled from the internet () or from biases in the crowd-sourced annotation process ().", "labels": [], "entities": []}, {"text": "Additionally, because existing test sets do not provide specific diagnostic information for improving models, it can be difficult to get proper insight into a system's capabilities or its limitations.", "labels": [], "entities": []}, {"text": "Unfortunately, when rigorous evaluations are not performed, strikingly simple model limitations can be overlooked.", "labels": [], "entities": []}, {"text": "To address this lacuna, we ask trivia enthusiasts-who write new questions for scholastic and open circuit tournaments-to create examples that specifically challenge Question Answering (QA) systems.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.8345344364643097}]}, {"text": "We develop a user interface (Section 2) that allows question writers to adversarially craft these questions.", "labels": [], "entities": []}, {"text": "This interface provides a model's predictions and its evidence from the training data to facilitate a model-driven annotation process.", "labels": [], "entities": []}, {"text": "Humans find the resulting challenge questions easier than regular questions (Section 3), but strong QA models struggle (Section 4).", "labels": [], "entities": []}, {"text": "Unlike many existing QA test sets, our questions highlight specific phenomena that humans can capture but machines cannot (Section 5).", "labels": [], "entities": [{"text": "QA test sets", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.7748341759045919}]}, {"text": "We release our QA challenge set to better evaluate models and systematically improve them.", "labels": [], "entities": [{"text": "QA challenge set", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.7109212179978689}]}], "datasetContent": [{"text": "In this section, we evaluate numerous QA systems on the challenge questions.", "labels": [], "entities": []}, {"text": "We consider a diverse set of models: ones based on recurrent networks, feed-forward networks, and IR systems to properly explore the difficulty of the examples.", "labels": [], "entities": []}, {"text": "We consider two neural models: a recurrent neural network (RNN) and Deep Averaging Network (.", "labels": [], "entities": []}, {"text": "The two models treat the problem as text classification and predict which of the answer entities the question is about.", "labels": [], "entities": [{"text": "text classification", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7358513176441193}]}, {"text": "The RNN is a bidirectional GRU () and the DAN uses fully connected layers with a word vector average as input.", "labels": [], "entities": []}, {"text": "To train the systems, we collect the data used at the 2017 NIPS Human-Computer Question Answering competition.", "labels": [], "entities": [{"text": "NIPS Human-Computer Question Answering competition", "start_pos": 59, "end_pos": 109, "type": "TASK", "confidence": 0.7586346864700317}]}, {"text": "The dataset consists of about 70,000 questions with 13,500 answer options.", "labels": [], "entities": []}, {"text": "We split the data into validation and test sets to provide baseline evaluations for the models.", "labels": [], "entities": []}, {"text": "We also report results on the baseline system (IR) shown to users during the writing process.", "labels": [], "entities": []}, {"text": "For evaluation, we report the accuracy as a function of the question position (to capture the incremental nature of the game).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994329810142517}]}, {"text": "The accuracy varies as the words are fed in (mostly improving, but occasionally degrading).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993152618408203}]}, {"text": "The buzz position of all models significantly degrades on the challenge set.", "labels": [], "entities": []}, {"text": "We compare the accuracy on the original test set (Test Questions) to the challenge questions in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994590878486633}]}, {"text": "For both the challenge and original test data, the questions begin with abstract clues that are difficult to answer (accuracy at or below 10%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9992461204528809}]}, {"text": "However, during the crucial middle portions of the questions (after revealing 25% to 75%), where buzzes in Quiz Bowl matches most frequently occur, the accuracy on original test questions rises significantly quicker than the challenge ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9990336894989014}]}, {"text": "For both questions, the accuracy rises towards the end as the \"give-away\" clues arrive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9995924830436707}]}, {"text": "Despite users never observing the output of a neural system, the two neural models decreased more in absolute accuracy than the IR system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.880777895450592}]}, {"text": "The DAN model had the largest absolute accuracy decrease (from 54.1% to 32.4% on the full question), likely because a vector average isn't capable of capturing the difficult wording of the challenge questions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8645114898681641}]}, {"text": "The human results are displayed on the left of and show a different trend.", "labels": [], "entities": []}, {"text": "For both question types, human accuracy rises very quickly after about 50% of the question has been seen.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.990772008895874}]}, {"text": "We suspect this occurs because the \"give-aways\", which often contain commonsense or simple knowledge clues, are easy for humans but quite difficult for computers.", "labels": [], "entities": []}, {"text": "The reverse is true for the early clues.", "labels": [], "entities": []}, {"text": "They contain quotes and entities that models can retrieve but humans struggle to remember.", "labels": [], "entities": []}], "tableCaptions": []}