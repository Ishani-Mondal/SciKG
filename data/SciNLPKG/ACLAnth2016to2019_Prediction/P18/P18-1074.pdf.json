{"title": [{"text": "A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling", "labels": [], "entities": [{"text": "Low-resource Sequence Labeling", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.642230232556661}]}], "abstractContent": [{"text": "We propose a multilingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling.", "labels": [], "entities": []}, {"text": "In this new architecture , we combine various transfer models using two layers of parameter sharing.", "labels": [], "entities": []}, {"text": "On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7186285108327866}]}, {"text": "On the second level, we adopt different parameter sharing strategies for different transfer schemes.", "labels": [], "entities": []}, {"text": "This architecture proves to be particularly effective for low-resource settings , when there are less than 200 training sentences for the target task.", "labels": [], "entities": []}, {"text": "Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.", "labels": [], "entities": [{"text": "Name Tagging", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.7842394113540649}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.8949497938156128}]}], "introductionContent": [{"text": "When we use supervised learning to solve Natural Language Processing (NLP) problems, we typically train an individual model for each task with task-specific labeled data.", "labels": [], "entities": []}, {"text": "However, our target task maybe intrinsically linked to other tasks.", "labels": [], "entities": []}, {"text": "For example, Part-of-speech (POS) tagging and Name Tagging can both be considered as sequence labeling; Machine Translation (MT) and Abstractive Text Summarization both require the ability to understand the source text and generate natural language sentences.", "labels": [], "entities": [{"text": "Part-of-speech (POS) tagging", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6291703641414642}, {"text": "Name Tagging", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8090130388736725}, {"text": "sequence labeling", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7147921621799469}, {"text": "Machine Translation (MT)", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.8493104457855225}, {"text": "Abstractive Text Summarization", "start_pos": 133, "end_pos": 163, "type": "TASK", "confidence": 0.6063972314198812}]}, {"text": "Therefore, it is valuable to transfer knowledge from related tasks to the target task.", "labels": [], "entities": []}, {"text": "Multi-task Learning (MTL) is one of * * Part of this work was done when the first author was on an internship at Facebook.", "labels": [], "entities": [{"text": "Multi-task Learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7798945903778076}]}, {"text": "The code of our model is available at https://github.", "labels": [], "entities": []}, {"text": "com/limteng-rpi/mlmt the most effective solutions for knowledge transfer across tasks.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7078091502189636}]}, {"text": "In the context of neural network architectures, we usually perform MTL by sharing parameters across models.", "labels": [], "entities": [{"text": "MTL", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9835007190704346}]}, {"text": "Previous studies have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing.", "labels": [], "entities": [{"text": "MTL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9399416446685791}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9892365336418152}, {"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.8717687726020813}]}, {"text": "However, most of these previous efforts focused on tasks and languages which have sufficient labeled data but hit a performance ceiling on each task alone.", "labels": [], "entities": []}, {"text": "Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.7544895708560944}]}, {"text": "According to Ethnologue 2 , there are 7, 099 living languages in the world.", "labels": [], "entities": []}, {"text": "It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements.", "labels": [], "entities": []}, {"text": "Furthermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource languages.", "labels": [], "entities": [{"text": "disaster response and recovery", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7843039035797119}]}, {"text": "Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from related high-resource languages and tasks.", "labels": [], "entities": []}, {"text": "In (, the authors simulated a low-resource setting for English and Spanish by downsampling the training data for the target task.", "labels": [], "entities": []}, {"text": "However, for most low-resource languages, the data sparsity problem also lies in related tasks and languages.", "labels": [], "entities": []}, {"text": "Under such circumstances, a single transfer model can only bring limited improvement.", "labels": [], "entities": []}, {"text": "To tackle this issue, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing.", "labels": [], "entities": []}, {"text": "In the first level, we share character embeddings, character-level convolutional neural networks, and word-level long-short term memory layer across all models.", "labels": [], "entities": []}, {"text": "These components serve as a basis to connect multiple models and transfer universal knowledge among them.", "labels": [], "entities": []}, {"text": "In the second level, we adopt different sharing strategies for different transfer schemes.", "labels": [], "entities": []}, {"text": "For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge (e.g., I-PER 3 should not be assigned to the first word in a sentence).", "labels": [], "entities": [{"text": "Name Tagging tasks", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.8512305617332458}]}, {"text": "To illustrate our idea, we take sequence labeling as a case study.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7037652730941772}]}, {"text": "In the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., POS tag) to each token in a sentence.", "labels": [], "entities": []}, {"text": "It underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and chunking.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.7466491758823395}, {"text": "Name Tagging", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.7917108833789825}]}, {"text": "Experiments show that our model can effectively transfer various types of knowledge from different auxiliary tasks and obtains up to 50.5% absolute F-score gains on Name Tagging compared to the mono-lingual single-task baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.9238044619560242}, {"text": "Name Tagging", "start_pos": 165, "end_pos": 177, "type": "TASK", "confidence": 0.8050600588321686}]}, {"text": "Additionally, our approach does not rely on a large amount of auxiliary task data to achieve the improvement.", "labels": [], "entities": []}, {"text": "Using merely 1% auxiliary data, we already obtain up to 9.7% absolute gains in Fscore.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.94267737865448}]}], "datasetContent": [{"text": "We use 50-dimensional pre-trained word embeddings and 50-dimensional randomly initialized character embeddings.", "labels": [], "entities": []}, {"text": "We train word embeddings using the word2vec package . English, Span-ish, and Dutch embeddings are trained on corresponding Wikipedia articles  We optimize parameters using Stochastic Gradient Descent with momentum, gradient clipping and exponential learning rate decay.", "labels": [], "entities": [{"text": "exponential learning rate decay", "start_pos": 237, "end_pos": 268, "type": "METRIC", "confidence": 0.7893731147050858}]}, {"text": "At step t, the learning rate \u03b1 t is updated using \u03b1 t = \u03b1 0 * \u03c1 t/T , where \u03b1 0 is the initial learning rate, \u03c1 is the decay rate, and T is the decay step.", "labels": [], "entities": []}, {"text": "To reduce overfitting, we apply Dropout () to the output of the LSTM layer.", "labels": [], "entities": []}, {"text": "We conduct hyper-parameter optimization by exploring the space of parameters shown in Table 2 using random search.", "labels": [], "entities": [{"text": "hyper-parameter optimization", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.8210528790950775}]}, {"text": "Due to time constraints, we only perform parameter sweeping on the Dutch Name Tagging task with 200 training examples.", "labels": [], "entities": [{"text": "parameter sweeping", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7279041707515717}, {"text": "Dutch Name Tagging task", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8007703423500061}]}, {"text": "We select the set of parameters that achieves the best performance on the development set and apply it to all models.: Hyper-parameter search space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Name Tagging data set statistics: #token  and #name (between parentheses).", "labels": [], "entities": [{"text": "Name Tagging", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.8121117949485779}]}, {"text": " Table 3: Comparison with state-of-the-art models.", "labels": [], "entities": []}, {"text": " Table 6: Performance comparison between mod- els with different components (C: character em- bedding; L: shared LSTM; S: language-specific  layer; H: highway networks; D: dropout).", "labels": [], "entities": []}]}