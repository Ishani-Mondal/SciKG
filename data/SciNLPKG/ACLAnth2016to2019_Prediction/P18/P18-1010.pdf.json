{"title": [{"text": "Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking", "labels": [], "entities": [{"text": "Fine-grained Entity Typing", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.6594910025596619}, {"text": "Linking", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.7726046442985535}]}], "abstractContent": [{"text": "Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies.", "labels": [], "entities": []}, {"text": "Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies.", "labels": [], "entities": []}, {"text": "This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing , and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.7815489172935486}, {"text": "entity typing", "start_pos": 205, "end_pos": 218, "type": "TASK", "confidence": 0.6968573033809662}, {"text": "FIGER dataset", "start_pos": 303, "end_pos": 316, "type": "DATASET", "confidence": 0.9484346210956573}]}, {"text": "We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMen-tions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and Type-Net, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types.", "labels": [], "entities": []}, {"text": "In experiments on all three datasets we show substantial gains from hierarchy-aware training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Identifying and understanding entities is a central component in knowledge base construction and essential for enhancing downstream tasks such as relation extraction *equal contribution Data and code for experiments: https://github.", "labels": [], "entities": [{"text": "knowledge base construction", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.6510898371537527}, {"text": "relation extraction", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.8202483355998993}]}, {"text": "com/MurtyShikhar/Hierarchical-Typing), question answering () and search ().", "labels": [], "entities": [{"text": "MurtyShikhar", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8879877328872681}, {"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8561607003211975}]}, {"text": "This has led to considerable research in automatically identifying entities in text, predicting their types, and linking them to existing structured knowledge sources.", "labels": [], "entities": [{"text": "predicting their types", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.873309055964152}]}, {"text": "Current state-of-the-art models encode a textual mention with a neural network and classify the mention as being an instance of a fine grained type or entity in a knowledge base.", "labels": [], "entities": []}, {"text": "Although in many cases the types and their entities are arranged in a hierarchical ontology, most approaches ignore this structure, and previous attempts to incorporate hierarchical information yielded little improvement in performance (.", "labels": [], "entities": []}, {"text": "Additionally, existing benchmark entity typing datasets only consider small label sets arranged in very shallow hierarchies.", "labels": [], "entities": []}, {"text": "For example, FIGER, the de facto standard fine grained entity type dataset, contains only 113 types in a hierarchy only two levels deep.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.4806938171386719}]}, {"text": "In this paper we investigate models that explicitly integrate hierarchical information into the embedding space of entities and types, using a hierarchy-aware loss on top of a deep neural network classifier over textual mentions.", "labels": [], "entities": []}, {"text": "By using this additional information, we learn a richer, more robust representation, gaining statistical efficiency when predicting similar concepts and aiding the classification of rarer types.", "labels": [], "entities": [{"text": "classification of rarer types", "start_pos": 164, "end_pos": 193, "type": "TASK", "confidence": 0.8067988753318787}]}, {"text": "We first validate our methods on the narrow, shallow type system of FIGER, out-performing state-of-the-art methods not incorporating hand-crafted features and matching those that do.", "labels": [], "entities": [{"text": "FIGER", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.767905056476593}]}, {"text": "To evaluate on richer datasets and stimulate further research into hierarchical entity/typing prediction with larger and deeper ontologies, we introduce two new human annotated datasets.", "labels": [], "entities": [{"text": "hierarchical entity/typing prediction", "start_pos": 67, "end_pos": 104, "type": "TASK", "confidence": 0.6284919798374176}]}, {"text": "The first is MedMentions, a collection of PubMed ab-stracts in which 246k concept mentions have been annotated with links to the Unified Medical Language System (UMLS) ontology, an order of magnitude more annotations than comparable datasets.", "labels": [], "entities": []}, {"text": "UMLS contains over 3.5 million concepts in a hierarchy having average depth 14.4.", "labels": [], "entities": [{"text": "UMLS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9056562781333923}]}, {"text": "Interestingly, UMLS does not distinguish between types and entities (an approach we heartily endorse), and the technical details of linking to such a massive ontology lead us to refer to our MedMentions experiments as entity linking.", "labels": [], "entities": []}, {"text": "Second, we present TypeNet, a curated mapping from the Freebase type system into the WordNet hierarchy.", "labels": [], "entities": [{"text": "WordNet hierarchy", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.9168741703033447}]}, {"text": "TypeNet contains over 1900 types with an average depth of 7.8.", "labels": [], "entities": [{"text": "TypeNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7597857117652893}]}, {"text": "In experimental results, we show improvements with a hierarchically-aware training loss on each of the three datasets.", "labels": [], "entities": []}, {"text": "In entity-linking MedMentions to UMLS, we observe a 6% relative increase inaccuracy over the base model.", "labels": [], "entities": []}, {"text": "In experiments on entity-typing from Wikipedia into TypeNet, we show that incorporating the hierarchy of types and including a hierarchical loss provides a dramatic 29% relative increase in MAP.", "labels": [], "entities": []}, {"text": "Our models even provide benefits for shallow hierarchies allowing us to match the state-of-art results of on the FIGER (GOLD) dataset without requiring hand-crafted features.", "labels": [], "entities": [{"text": "FIGER (GOLD) dataset", "start_pos": 113, "end_pos": 133, "type": "DATASET", "confidence": 0.7028952956199646}]}, {"text": "We will publicly release the TypeNet and MedMentions datasets to the community to encourage further research in truly fine-grained, hierarchical entity-typing and linking.", "labels": [], "entities": [{"text": "MedMentions datasets", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.8950404226779938}]}], "datasetContent": [{"text": "We perform three sets of experiments: mentionlevel entity typing on the benchmark dataset FIGER, entity-level typing using Wikipedia and TypeNet, and entity linking using MedMentions.", "labels": [], "entities": [{"text": "mentionlevel entity typing", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.562043696641922}, {"text": "benchmark dataset FIGER", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.7021490931510925}, {"text": "entity linking", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.7322687655687332}]}], "tableCaptions": [{"text": " Table 1: Statistics from various biological entity  linking data sets from scientific articles. NCBI  Disease (Do\u02d8 gan et al., 2014) focuses exclusively  on disease entities. BCV-CDR (Li et al., 2016)  contains both chemicals and diseases. BCII-GN  and NLM (Wei et al., 2015) both contain genes.", "labels": [], "entities": [{"text": "NCBI  Disease (Do\u02d8 gan et al., 2014)", "start_pos": 97, "end_pos": 133, "type": "DATASET", "confidence": 0.9417481584982439}]}, {"text": " Table 3: Statistics from various type sets. Type- Net is the largest type hierarchy with a gold map- ping to KB entities. *The entire WordNet could be  added to TypeNet increasing the total size to 17k  types.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9567869901657104}]}, {"text": " Table 4: Stats for the final TypeNet dataset. child- of, parent-of, and equivalence links are from Free- base types \u2192 WordNet synsets.", "labels": [], "entities": [{"text": "TypeNet dataset", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7335936576128006}]}, {"text": " Table 5: Accuracy and Macro/Micro F1 on FIGER  (GOLD).  \u2020 is an LSTM model.  \u2021 is an attentive  LSTM along with additional hand crafted features.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9985083937644958}, {"text": "F1", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.7589558959007263}, {"text": "FIGER", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9594812393188477}, {"text": "GOLD", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.4250573515892029}]}, {"text": " Table 6: MAP of entity-level typing in Wikipedia  data using TypeNet. The second column shows  results using 5% of the total data. The last column  shows results using the full set of 344,246 entities.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8011364936828613}, {"text": "Wikipedia  data", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.8578924238681793}]}, {"text": " Table 7: Accuracy on entity linking in MedMen- tions. Maximum recall is 81.82% because we use  an imperfect alias table to generate candidates.  Normalized scores consider only mentions which  contain the gold entity in the candidate set. Men- tion tfidf is csim from Section 4.3.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9825749397277832}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9865854978561401}]}]}