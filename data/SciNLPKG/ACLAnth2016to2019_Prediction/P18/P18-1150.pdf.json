{"title": [{"text": "A Graph-to-Sequence Model for AMR-to-Text Generation", "labels": [], "entities": [{"text": "AMR-to-Text Generation", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9072646498680115}]}], "abstractContent": [{"text": "The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.9889655709266663}]}, {"text": "The current state-of-the-art method uses a sequence-to-sequence model, leverag-ing LSTM for encoding a linearized AMR structure.", "labels": [], "entities": []}, {"text": "Although it is able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences.", "labels": [], "entities": []}, {"text": "We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics.", "labels": [], "entities": []}, {"text": "On a standard benchmark, our model shows superior results to existing methods in the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8560246030489603}]}, {"text": "shows an AMR graph in which the nodes (such as \"describe-01\" and \"person\") represent the concepts, and edges (such as \":ARG0\" and \":name\") represent the relations between concepts they connect.", "labels": [], "entities": []}, {"text": "AMR has been proven helpful on other NLP tasks, such as machine translation (, question answering (, summarization () and event detection (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8438624441623688}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8662056624889374}, {"text": "summarization", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.9762817025184631}, {"text": "event detection", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7857288420200348}]}, {"text": "The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.9899202883243561}]}, {"text": "The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts.", "labels": [], "entities": [{"text": "AMR graphs from texts", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.8542878329753876}]}, {"text": "The translation from AMR nodes to text phrases can be far from literal.", "labels": [], "entities": []}, {"text": "For example, shown in, \"Ryan\" is represented as \"(p / person :name (n / name :op1 \"Ryan\"))\", and \"description of\" is represented as \"(d / describe-01 :ARG1 )\".", "labels": [], "entities": [{"text": "ARG1", "start_pos": 151, "end_pos": 155, "type": "DATASET", "confidence": 0.5827378034591675}]}, {"text": "While initial work used statistical approaches (, recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model), which has achieved the state-of-the-art results on AMR-to-text generation (.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.9657633304595947}]}, {"text": "One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large.", "labels": [], "entities": []}, {"text": "In particular, closely-related nodes, such as parents, children and siblings can be faraway after serialization.", "labels": [], "entities": []}, {"text": "It can be difficult fora linear recurrent neural network to automatically induce their original connections from bracketed string forms.", "labels": [], "entities": []}, {"text": "To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structures directly.", "labels": [], "entities": []}, {"text": "To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states.", "labels": [], "entities": []}, {"text": "Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM) is used to avoid gradient diminishing and bursting in the recurrent process.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9306415915489197}]}, {"text": "The decoder is an attention-based LSTM model with a copy mechanism (, which helps copy sparse tokens (such as numbers and named entities) from the input.", "labels": [], "entities": []}, {"text": "Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9985141158103943}, {"text": "AMR-to-text generation", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.972369372844696}]}, {"text": "Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art () trained on the same dataset.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9835457503795624}]}, {"text": "When using gigaword sentences as additional training data, our model is consistently better than using the same amount of gigaword data, showing the effectiveness of our model on large-scale training set.", "labels": [], "entities": []}, {"text": "We release our code and models at https: //github.com/freesunshine0316/ neural-graph-to-seq-mp.", "labels": [], "entities": []}], "datasetContent": [{"text": "As shown in, we compare our model with a set of baselines on the AMR devset to demonstrate how the graph encoder and the copy mechanism can be useful when training instances are not sufficient.", "labels": [], "entities": [{"text": "AMR devset", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.9523904919624329}]}, {"text": "Seq2seq is the sequence-to-sequence baseline described in Section 2.", "labels": [], "entities": []}, {"text": "Seq2seq+copy extends Seq2seq with the copy mechanism, and Seq2seq+charLSTM+copy further extends Seq2seq+copy with character LSTM.", "labels": [], "entities": []}, {"text": "Graph2seq is our graph-to-sequence model, Graph2seq+copy extends Graph2seq with the copy mechanism, and Graph2seq+charLSTM+copy further extends Graph2seq+copy with the character LSTM.", "labels": [], "entities": []}, {"text": "We also try Graph2seq+Anon, which applies our graph-to-sequence model on the anonymized data from.", "labels": [], "entities": []}, {"text": "The graph encoder As can be seen from Table 1, the performance of Graph2seq is 1.6 BLEU points higher than Seq2seq, which shows that our graph encoder is effective when applied alone.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9988070726394653}]}, {"text": "Adding the copy mechanism (Graph2seq+copy vs Seq2seq+copy), the gap becomes 2.3.", "labels": [], "entities": []}, {"text": "This shows that the graph encoder learns better node representations compared to the sequence encoder, which allows attention and copying to function better.", "labels": [], "entities": []}, {"text": "Applying the graph encoder together with the copy mechanism gives again of 3.4 BLEU points over the baseline (Graph2seq+copy vs Seq2seq).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9986457228660583}]}, {"text": "The graph encoder is consistently better than the sequence encoder no matter whether character LSTMs are used.", "labels": [], "entities": []}, {"text": "We also list the encoding part of decoding times on the devset, as the decoders of the seq2seq and the graph2seq models are similar, so the time differences reflect efficiencies of the encoders.", "labels": [], "entities": []}, {"text": "Our graph encoder gives consistently better efficiency compared with the sequence encoder, showing the advantage of parallelization.", "labels": [], "entities": []}, {"text": "The copy mechanism shows that the copy mechanism is effective on both the graph-to-sequence and the sequence-to-sequence models.", "labels": [], "entities": []}, {"text": "Anonymization gives comparable overall performance gains on our graph-to-sequence model as the copy mechanism (comparing Graph2seq+Anon with Graph2seq+copy).", "labels": [], "entities": []}, {"text": "However, the copy mechanism has several advantages over anonymization as discussed in Section 3.5.", "labels": [], "entities": []}, {"text": "Character LSTM Character LSTM helps to increase the performances of both systems by roughly 0.6 BLEU points.", "labels": [], "entities": [{"text": "Character LSTM Character LSTM", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.6778588742017746}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.999293327331543}]}, {"text": "This is largely because it further alleviates the data sparsity problem by handling unseen words, which may share common substrings with in-vocabulary words.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: DEV BLEU scores and decoding times.", "labels": [], "entities": [{"text": "DEV", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6742352247238159}, {"text": "BLEU scores", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9028806686401367}]}, {"text": " Table 2: TEST results. \"(200K)\", \"(2M)\" and  \"(20M)\" represent training with the corresponding  number of additional sentences from Gigaword.", "labels": [], "entities": [{"text": "TEST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9288837313652039}, {"text": "Gigaword", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.9331182241439819}]}]}