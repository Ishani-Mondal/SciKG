{"title": [{"text": "Recognizing Emotions in Video Using Multimodal DNN Feature Fusion", "labels": [], "entities": [{"text": "Recognizing Emotions", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8999126553535461}]}], "abstractContent": [{"text": "We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language.", "labels": [], "entities": [{"text": "Computational Modeling of Human Multimodal Language", "start_pos": 181, "end_pos": 232, "type": "TASK", "confidence": 0.7059514572223028}]}, {"text": "Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs).", "labels": [], "entities": []}, {"text": "We show that our fusion approach outperforms unimodal predictors.", "labels": [], "entities": []}, {"text": "Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment.", "labels": [], "entities": []}, {"text": "This leads to an overall binary accuracy of 90%, overall 4-class accuracy of 89.2% and an overall mean-absolute-error (MAE) of 0.12.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8781980276107788}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9156980514526367}, {"text": "mean-absolute-error (MAE)", "start_pos": 98, "end_pos": 123, "type": "METRIC", "confidence": 0.9316898286342621}]}, {"text": "Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities.", "labels": [], "entities": []}, {"text": "The presented multimodal approach creates a simple and robust baseline on this new Grand Challenge dataset.", "labels": [], "entities": [{"text": "Grand Challenge dataset", "start_pos": 83, "end_pos": 106, "type": "DATASET", "confidence": 0.8999083042144775}]}, {"text": "Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic emotion detection is a longstanding and challenging problem in the field of artificial intelligence and machine learning.", "labels": [], "entities": [{"text": "Automatic emotion detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8069320917129517}]}, {"text": "One reason why emotion analysis is so difficult is due to the fact that emotions are somewhat subjective, which affects how emotions are perceived and subsequently labeled by human annotators.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.7555330693721771}]}, {"text": "To compound this even further, the expressed emotions may change, in particular for video data.", "labels": [], "entities": []}, {"text": "In addition, multiple emotions can be expressed simultaneously and also as a sequence overtime.", "labels": [], "entities": []}, {"text": "Emotions provide a type of para-linguistic information that is crucial for many applications in artificial intelligence including: affective speech generation, bio-medical diagnostics, machine translation and human-computer interaction.", "labels": [], "entities": [{"text": "affective speech generation", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.7504850625991821}, {"text": "machine translation", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.8143902122974396}]}, {"text": "Multimodal machine learning has been recently attracting interest, with the abundance of multimedia data available on the internet making it easy for researchers to integrate data of multiple modalities.", "labels": [], "entities": [{"text": "Multimodal machine learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7061484456062317}]}, {"text": "It is a dynamic research field which aims to integrate and model multiple sources of input, usually acoustic, visual and text.", "labels": [], "entities": []}, {"text": "In order to produce major advances in emotion analysis, there must be adequate techniques for combining and analyzing complex signals.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7906809449195862}]}, {"text": "While this notion is applicable across many fields and tasks, in this work we focus on emotion analysis from video data -a very active research area that is beaming with interesting results and methodologies (.", "labels": [], "entities": [{"text": "emotion analysis", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7633194625377655}]}, {"text": "A survey by motivates some of the uses of multimodal analysis, together with five main components: \u2022 Representation: Representing and summarizing multimodal data \u2022 Translation: Mapping data from one modality to another \u2022 Alignment: Identifying relationships between modalities: for example, transcribed text of a video \u2022 Fusion: Joining information for different modalities in order to perform a prediction \u2022 Co-learning: Exchanging knowledge between modalities Our work touches on representation, alignment, and co-learning issues, but it is mostly focused on fusion.", "labels": [], "entities": [{"text": "multimodal analysis", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7326474487781525}]}, {"text": "Specifically, we are interested in finding away to predict emotions from video data by fusing together three modalities: verbal content, acoustic features and sequences of images.", "labels": [], "entities": []}, {"text": "In this work we provide the experimental framework for developing a system for 6-class (multi-label) emotion classification and regression for the First Workshop and Grand Challenge on Computational Modeling of Human Multimodal Language at Association for Computational Linguistics (ACL) 2018.", "labels": [], "entities": [{"text": "multi-label) emotion classification", "start_pos": 88, "end_pos": 123, "type": "TASK", "confidence": 0.6965510100126266}, {"text": "Computational Modeling of Human Multimodal Language at Association for Computational Linguistics (ACL)", "start_pos": 185, "end_pos": 287, "type": "TASK", "confidence": 0.8257485798427037}]}, {"text": "This paper is organized as follows: in Section 2, we present some relevant work on multimodal emotion recognition.", "labels": [], "entities": [{"text": "multimodal emotion recognition", "start_pos": 83, "end_pos": 113, "type": "TASK", "confidence": 0.6800012489159902}]}, {"text": "In Section 3 we provide an overview of the CMU-MOSEI dataset and a description of our task.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.965820699930191}]}, {"text": "In Section 4, we present our methodology and multimodal fusion technique.", "labels": [], "entities": []}, {"text": "In Section 5, we show our experiments and results.", "labels": [], "entities": []}, {"text": "In Section 6 we show some analysis of our experiments and in Section 7 we finally discuss and make suggestions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the results of our experiments on a random prediction baseline, followed by unimodal, bimodal and trimodal inputlevel feature fusion.", "labels": [], "entities": []}, {"text": "We used the outcome of these experiments to evaluate and compare each model perfomance.", "labels": [], "entities": []}, {"text": "Finally, we provide the results for the Grand Challenge from our best-performing system: the trimodal BLSTM.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.45184412598609924}]}], "tableCaptions": [{"text": " Table 1: Baseline MAE based on randomized pre- dictions of quantity of labels, label category, and  intensity.", "labels": [], "entities": [{"text": "Baseline MAE", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.3194204419851303}]}, {"text": " Table 2: Unimodal prediction results, overall  mean-absolute error (MAE) for each DNN.", "labels": [], "entities": [{"text": "Unimodal prediction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6700692921876907}, {"text": "overall  mean-absolute error (MAE)", "start_pos": 39, "end_pos": 73, "type": "METRIC", "confidence": 0.8959880868593851}]}, {"text": " Table 3: Bimodal prediction results, overall mean- absolute error (MAE) for each DNN and ablation.", "labels": [], "entities": [{"text": "Bimodal prediction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.839043527841568}, {"text": "overall mean- absolute error (MAE)", "start_pos": 38, "end_pos": 72, "type": "METRIC", "confidence": 0.9141766428947449}]}, {"text": " Table 5: Official Grand Challenge system results  for our early-fusion trimodal BLSTM.", "labels": [], "entities": []}]}