{"title": [{"text": "A Deep Relevance Model for Zero-Shot Document Filtering", "labels": [], "entities": [{"text": "Zero-Shot Document Filtering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7131411035855612}]}], "abstractContent": [{"text": "In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand.", "labels": [], "entities": []}, {"text": "As a fundamental task, information filtering therefore becomes a critical necessity.", "labels": [], "entities": [{"text": "information filtering", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.9073306322097778}]}, {"text": "In this paper , we propose a novel deep relevance model for zero-shot document filtering, named DAZER.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.5859252115090688}, {"text": "DAZER", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.5671712756156921}]}, {"text": "DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category.", "labels": [], "entities": []}, {"text": "With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space.", "labels": [], "entities": []}, {"text": "The relevance signals are extracted through a gated convolutional process.", "labels": [], "entities": []}, {"text": "The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner.", "labels": [], "entities": []}, {"text": "Experiments on two document collections of two different tasks (i.e., topic catego-rization and sentiment analysis) demonstrate that DAZER significantly outper-forms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.9030040204524994}, {"text": "DAZER", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.65386962890625}]}], "introductionContent": [{"text": "Filtering irrelevant information and organizing relevant information into meaningful topical categories is indispensable and ubiquitous.", "labels": [], "entities": []}, {"text": "For example, a data analyst tracking an emerging event would like to retrieve the documents relevant to a specific topic (category) from a large document collection in a short response time.", "labels": [], "entities": []}, {"text": "In the era of big data, the potentially possible categories covered by documents would be limitless.", "labels": [], "entities": []}, {"text": "It is unrealistic to manually identify a lot of positive examples for each possible category.", "labels": [], "entities": []}, {"text": "However, new information needs indeed emerge everywhere in many real-world scenarios.", "labels": [], "entities": []}, {"text": "Recent studies on dataless text classification show promising results on reducing labeling effort (.", "labels": [], "entities": [{"text": "dataless text classification", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.5894056757291158}]}, {"text": "Without any labeled document, a dataless classifier performs text classification by using a small set of relevant words for each category (called \"seed words\").", "labels": [], "entities": [{"text": "text classification", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.700612485408783}]}, {"text": "However, existing dataless classifiers do not consider document filtering.", "labels": [], "entities": [{"text": "document filtering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7413047254085541}]}, {"text": "We need to provide the seed words for each category covered by the document collection, which is often infeasible in the real world.", "labels": [], "entities": []}, {"text": "To this end, we are particularly interested in the task of zero-shot document filtering.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6285671889781952}]}, {"text": "Here, zeroshot means that the instances of the targeted categories are unseen during the training phase.", "labels": [], "entities": []}, {"text": "To facilitate zero-shot filtering, we take a small set of seed words to represent a category of interest.", "labels": [], "entities": [{"text": "zero-shot filtering", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7239062488079071}]}, {"text": "This is extremely useful when the information need (i.e., the categories of interest) is dynamic and the text collection is large and temporally updated (e.g., the possible categories are hard to know).", "labels": [], "entities": []}, {"text": "Specifically, we propose a novel deep relevance model for zero-shot document filtering, named DAZER.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.5805928011735281}]}, {"text": "In DAZER, we use the word embeddings learnt from an external large text corpus to represent each word.", "labels": [], "entities": [{"text": "DAZER", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.5687360167503357}]}, {"text": "A category can then be well represented also in the embedding space (called category embedding) through some composition with the word embeddings of the provided seed words.", "labels": [], "entities": []}, {"text": "Given a small number of seed words provided fora category as input, DAZER is devised to produce a score indicating the relevance between a document and the category.", "labels": [], "entities": [{"text": "DAZER", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.8843614459037781}]}, {"text": "It is intuitive to connect zero-shot document filtering with the task of ad-hoc retrieval.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.608831524848938}]}, {"text": "Indeed, by treating the seed words of each category as a query, the zero-shot document filtering is equivalent to ranking documents based on their relevance to the query.", "labels": [], "entities": []}, {"text": "The relevance ranking is a core task in information retrieval, and has been studied for many years.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.847056120634079}]}, {"text": "Although they share the same formulation, these two tasks diverge fundamentally.", "labels": [], "entities": []}, {"text": "For ad-hoc retrieval, a user constructs a query with a specific information need.", "labels": [], "entities": []}, {"text": "The relevant documents are assumed to contain these query words.", "labels": [], "entities": []}, {"text": "This is confirmed by the existing works that exact keyword match is still the most important signal of relevance in ad-hoc retrieval.", "labels": [], "entities": [{"text": "exact keyword match", "start_pos": 45, "end_pos": 64, "type": "METRIC", "confidence": 0.6791697343190511}]}, {"text": "For document filtering, the seed words fora category are expected to convey the conceptual meaning of the latter.", "labels": [], "entities": [{"text": "document filtering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7823764085769653}]}, {"text": "It is impossible to list all the words to fully cover the relevant documents of a category.", "labels": [], "entities": []}, {"text": "Therefore, it is essential to capture the conceptual relevance for zero-shot document filtering.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.5953301390012106}]}, {"text": "The classical retrieval models simply estimate the relevance based on the query keyword matching, which is far from capturing the conceptual relevance.", "labels": [], "entities": []}, {"text": "The existing deep relevance models for ad-hoc retrieval utilize the statistics of the hard/soft-match signals in terms of cosine similarity between two word embeddings (.", "labels": [], "entities": []}, {"text": "However, the scalar information like cosine similarity between two embedding vectors is too coarse or limited to reflect the conceptual relevance.", "labels": [], "entities": []}, {"text": "On the contrary, we believe that the embedding features could provide rich knowledge towards the conceptual relevance.", "labels": [], "entities": []}, {"text": "A key challenge is to endow DAZER a strong generalization ability to also successfully extract the relevance signals for unseen categories.", "labels": [], "entities": []}, {"text": "To achieve this purpose, we extract the relevance signals based on the hidden feature interactions between the category and each word in the embedding space.", "labels": [], "entities": []}, {"text": "Specifically, two element-wise operations are utilized in DAZER: element-wise subtraction and element-wise product.", "labels": [], "entities": []}, {"text": "Since these two kinds of interactions represent the relative information encoded in hidden embedding space, we expect that the relevance signal extraction process could generalize well to unseen categories.", "labels": [], "entities": [{"text": "relevance signal extraction", "start_pos": 127, "end_pos": 154, "type": "TASK", "confidence": 0.785548726717631}]}, {"text": "Firstly, DAZER utilizes a gated convolutional operation with k-max pooling to extract the relevance signals.", "labels": [], "entities": []}, {"text": "Then, DAZER abstracts higherlevel relevance features through a multi-layer perceptron, which can be considered as a relevance aggregation procedure.", "labels": [], "entities": []}, {"text": "At last, DAZER calculates an overall score indicating the relevance between a document and the category.", "labels": [], "entities": [{"text": "DAZER", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8360678553581238}]}, {"text": "Without further constraints, it is possible for DAZER to encode the bias towards the category-specific features seen during the training (i.e., model overfitting).", "labels": [], "entities": []}, {"text": "Therefore, we further introduce an adversarial learning over the output of the relevance aggregation procedure.", "labels": [], "entities": []}, {"text": "The purpose is to ensure that the higher-level relevance features contain no category-dependent information, leading to a better zero-shot filtering performance.", "labels": [], "entities": [{"text": "zero-shot filtering", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.6228360533714294}]}, {"text": "To the best of our knowledge, DAZER is the first deep model to conduct zero-shot document filtering.", "labels": [], "entities": [{"text": "DAZER", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.7221653461456299}, {"text": "zero-shot document filtering", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.5836480359236399}]}, {"text": "We conduct extensive experiments on two real-world document collections from two different domains (i.e., 20-Newsgroup for topic categorization, and Movie Review for sentiment analysis).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.9101322889328003}]}, {"text": "Our experimetnal results suggest that DAZER achieves promising filtering performance and performs significantly better than the existing alternative solutions, including state-of-the-art deep relevance ranking methods.", "labels": [], "entities": []}, {"text": "illustrates the network structure of the proposed DAZER model.", "labels": [], "entities": []}, {"text": "It consists of two main components: relevance signal extraction and relevance aggregation.", "labels": [], "entities": [{"text": "relevance signal extraction", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7726432085037231}, {"text": "relevance aggregation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7984093427658081}]}, {"text": "In the following, we present each component in detail.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct experiments on two real-world document collections to evaluate the effectiveness of the proposed DAZER 1 .  Evaluation Protocol.", "labels": [], "entities": [{"text": "DAZER 1 .  Evaluation Protocol", "start_pos": 125, "end_pos": 155, "type": "DATASET", "confidence": 0.5934348821640014}]}, {"text": "With the specified unseen categories, we take all the training documents of the other categories to train a model.", "labels": [], "entities": []}, {"text": "Then, all documents in the test set are used for evaluation.", "labels": [], "entities": []}, {"text": "For each unseen category, the task is to rank the documents of that category higher than the others.", "labels": [], "entities": []}, {"text": "Here, we choose to report mean average precision (MAP) for performance evaluation.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 26, "end_pos": 54, "type": "METRIC", "confidence": 0.9428742627302805}]}, {"text": "MAP is a widely used metric to evaluate the ranking quality.", "labels": [], "entities": [{"text": "MAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.477445513010025}]}, {"text": "The higher the relevant documents are ranked, the larger the MAP value is, which means a better filtering performance.", "labels": [], "entities": [{"text": "MAP", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9430263638496399}]}, {"text": "For all neural networks based models, the training documents from one randomly sampled training category work as the validation set for early stop.", "labels": [], "entities": []}, {"text": "We report the averaged results over 5 runs for all the methods (excluding SSVM and BM25).", "labels": [], "entities": [{"text": "SSVM", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.812467098236084}, {"text": "BM25", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.6870582103729248}]}, {"text": "The statistical significance is conducted by applying the student t-test.", "labels": [], "entities": [{"text": "significance", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.5741000175476074}]}, {"text": "For 20NG dataset, we directly use the seed words 7 manually compiled in).", "labels": [], "entities": [{"text": "20NG dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8672310709953308}]}, {"text": "These seed words are selected from the category descriptions and widely used in the works of dataless text classification ().", "labels": [], "entities": [{"text": "dataless text classification", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6927166779836019}]}, {"text": "For Movie Review, following the seed word selection process (i.e., assisted by standard LDA) proposed in (, we manually select the seed words for each sentiment label.", "labels": [], "entities": [{"text": "Movie Review", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.6828035116195679}]}, {"text": "lists the seed words selected for each sentiment label for Movie Review dataset.", "labels": [], "entities": [{"text": "Movie Review dataset", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.9371985197067261}]}, {"text": "There are on average 5.2 and 4.6 seed words for each category over 20NG and Movie Review respectively.", "labels": [], "entities": [{"text": "20NG", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.901996374130249}, {"text": "Movie Review", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.8512763381004333}]}, {"text": "It is worthwhile to highlight that no category information is exploited within the seed word selection process.", "labels": [], "entities": [{"text": "seed word selection", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.6561721861362457}]}, {"text": "For DAZER, the number of convolution filters ism = 50 and k = 3 is used for k-max pooling.", "labels": [], "entities": []}, {"text": "The dimension size for relevance aggregation isl a = 75.", "labels": [], "entities": [{"text": "relevance aggregation", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7868143618106842}]}, {"text": "The local window size l is set to be 2.", "labels": [], "entities": []}, {"text": "The learning rate is 0.00001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9801985323429108}]}, {"text": "The models are trained with a batch size of 16 and \u03bb \u0398 = 0.0001, \u03bb = 0.1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of the 7 methods for zero-shot document filtering in terms of MAP. The best and  second best results are highlighted in boldface and underlined respectively, on each task.  \u2020 indicates that  the difference to the best result is statistically significant at 0.05 level. Avg: averaged MAP over all tasks.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6472347378730774}, {"text": "Avg", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.9987509250640869}]}, {"text": " Table 5: Impact of different settings for DAZER on 20NG. The best results are highlighted in boldface.  -e dif f  c,w : no element-wise subtraction; -e prod  c,w : no element-wise product; -Gate: no category-specific gate  mechanism; -Adv: no adversarial learning.", "labels": [], "entities": [{"text": "20NG", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8504123091697693}]}, {"text": " Table 6: Impact of different settings for DAZER on Movie Review. The best results are highlighted in  boldface. -e dif f  c,w : no element-wise subtraction; -e prod  c,w : no element-wise product; -Gate: no category- specific gate mechanism; -Adv: no adversarial learning.", "labels": [], "entities": []}, {"text": " Table 7: Performance of the 7 methods for zero-shot document filtering in terms of MAP. The words ap- pearing in the category name are used as the seed words. The best and second best results are highlighted  in boldface and underlined respectively, on each task.", "labels": [], "entities": [{"text": "zero-shot document filtering", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6232720911502838}]}]}