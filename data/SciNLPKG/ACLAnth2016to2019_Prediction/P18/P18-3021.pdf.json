{"title": [{"text": "Automatic Spelling Correction for Resource-Scarce Languages using Deep Learning", "labels": [], "entities": [{"text": "Automatic Spelling Correction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6414430340131124}]}], "abstractContent": [{"text": "Spelling correction is a well-known task in Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9845920503139496}, {"text": "Natural Language Processing (NLP)", "start_pos": 44, "end_pos": 77, "type": "TASK", "confidence": 0.7130826016267141}]}, {"text": "Automatic spelling correction is important for many NLP applications like web search engines, text summarization, sentiment analysis etc.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7853922247886658}, {"text": "text summarization", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7907387316226959}, {"text": "sentiment analysis", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.9572608768939972}]}, {"text": "Most approaches use parallel data of noisy and correct word mappings from different sources as training data for automatic spelling correction.", "labels": [], "entities": [{"text": "automatic spelling correction", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.7428754369417826}]}, {"text": "Indic languages are resource-scarce and do not have such parallel data due to low volume of queries and non-existence of such prior implementations.", "labels": [], "entities": []}, {"text": "In this paper, we show how to build an automatic spelling corrector for resource-scarce languages.", "labels": [], "entities": [{"text": "spelling corrector", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7560648024082184}]}, {"text": "We propose a sequence-to-sequence deep learning model which trains end-to-end.", "labels": [], "entities": []}, {"text": "We perform experiments on synthetic datasets created for In-dic languages, Hindi and Telugu, by incorporating the spelling mistakes committed at character level.", "labels": [], "entities": []}, {"text": "A comparative evaluation shows that our model is competitive with the existing spell checking and correction techniques for Indic languages.", "labels": [], "entities": [{"text": "spell checking and correction", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8166340738534927}]}], "introductionContent": [{"text": "Spelling correction is important for many of the potential NLP applications such as text summarization, sentiment analysis, machine translation (.", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9598952829837799}, {"text": "text summarization", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7970745861530304}, {"text": "sentiment analysis", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.9681158363819122}, {"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.8441283106803894}]}, {"text": "Automatic spelling correction is crucial in search engines as spelling mistakes are very common in user-generated text.", "labels": [], "entities": [{"text": "Automatic spelling correction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6559703846772512}]}, {"text": "Many websites have a feature of automatically giving correct suggestions to the misspelled user queries in the form of Did you mean?", "labels": [], "entities": []}, {"text": "Providing suggestions makes it convenient for users to accept a proposed correction without retyping or correcting the query manually.", "labels": [], "entities": []}, {"text": "This task is approached by collecting similar intent queries from user logs ().", "labels": [], "entities": []}, {"text": "The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session.", "labels": [], "entities": []}, {"text": "Example query pairs are (house lone, house loan), (ello world, hello world), (mobilephone, mobile phone).", "labels": [], "entities": []}, {"text": "Thus, large amounts of data is collected and models are trained using techniques like Machine Learning, Statistical Machine Translation etc.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 104, "end_pos": 135, "type": "TASK", "confidence": 0.7335455616315206}]}, {"text": "The task of spelling correction is challenging for resource-scarce languages.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9859877824783325}]}, {"text": "In this paper, we consider Indic languages, Hindi and Telugu, because of their resource scarcity.", "labels": [], "entities": []}, {"text": "Due to lesser query share, we do not find the same level of parallel alteration data from logs.", "labels": [], "entities": []}, {"text": "We also do not have many language resources such as Parts of Speech (POS) Taggers, Parsers etc.", "labels": [], "entities": [{"text": "Parts of Speech (POS) Taggers", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.5418432312352317}]}, {"text": "to linguistically analyze and understand these queries.", "labels": [], "entities": []}, {"text": "Due to lack of relevant data, we create synthetic dataset using highly probable spelling errors and real world errors in Hindi and Telugu given by language experts.", "labels": [], "entities": []}, {"text": "Similarly, synthetic dataset can be created for any resource-scarce language incorporating the real world errors.", "labels": [], "entities": []}, {"text": "Deep Learning techniques have shown enormous success in sequence to sequence mapping tasks).", "labels": [], "entities": [{"text": "sequence to sequence mapping tasks", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.6543349087238312}]}, {"text": "Most of the existing spell-checkers for Indic languages are implemented using rule-based techniques (.", "labels": [], "entities": []}, {"text": "In this paper, we approach the spelling correction problem for Indic languages with Deep learning.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9171622693538666}]}, {"text": "This model can be employed for any resource-scarce language.", "labels": [], "entities": []}, {"text": "We propose a character based Sequence-to-sequence text Correction Model for Indic Languages (SCMIL) which trains end-to-end.", "labels": [], "entities": []}, {"text": "Our main contributions in this paper are summarized as follows: \u2022 We propose a character based recurrent sequence-to-sequence architecture with a Long Short Term Memory (LSTM) encoder and a LSTM decoder for spelling correction of Indic languages.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.8898428678512573}]}, {"text": "\u2022 We create synthetic datasets 1 of noisy and correct word mappings for Hindi and Telugu by collecting highly probable spelling errors and inducing noise in clean corpus.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate the performance of SCMIL by comparing with various approaches such as Statistical Machine Translation (SMT), rulebased methods, and various deep learning models, for this task.", "labels": [], "entities": [{"text": "SCMIL", "start_pos": 33, "end_pos": 38, "type": "TASK", "confidence": 0.969480574131012}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.7906987766424814}]}], "datasetContent": [{"text": "We performed experiments with SCMIL and other models using synthetic datasets which we created for the Indic languages: Hindi and Telugu.", "labels": [], "entities": []}, {"text": "Hindi is the most prominent Indian language and the third most spoken language in the world.", "labels": [], "entities": []}, {"text": "Telugu is the most widely spoken Dravidian language in the world and third most spoken native language in India.", "labels": [], "entities": []}, {"text": "Due to lack of data with error patterns in Indic languages, we have built a synthetic dataset that SCMIL is trained on.", "labels": [], "entities": []}, {"text": "Initially, we create data lists for Hindi and Telugu.", "labels": [], "entities": []}, {"text": "For this, we have extracted a corpus of most frequent Hindi words 2 and most frequent Telugu words . We have also extracted Hindi movie names and Telugu movie names of the movies released between the years 1930 and 2018 from Wikipedia which constitute phrases in the data lists.", "labels": [], "entities": []}, {"text": "Thus, the Hindi and Telugu data lists consist of words and phrases consisting maximum of five words.", "labels": [], "entities": [{"text": "Hindi and Telugu data lists", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.6759006381034851}]}, {"text": "For each data instance in the data list, multiple noisy words are generated by introducing error.", "labels": [], "entities": []}, {"text": "The type of errors include insertion, deletion, substitution of one character, and word fusing.", "labels": [], "entities": []}, {"text": "Spaces between words are randomly dropped in phrases to simulate the word fusing problem.", "labels": [], "entities": []}, {"text": "The list of errors for Hindi and Telugu is created by collecting the highly committed spelling errors users make in each of these languages.", "labels": [], "entities": []}, {"text": "We created this error list from linguistic resources and with help from language experts.", "labels": [], "entities": []}, {"text": "The language experts analyzed Hindi and Telugu usage and listed the most probable errors.", "labels": [], "entities": []}, {"text": "These errors are based on observations on real data and lexicon of Hindi and Telugu.", "labels": [], "entities": []}, {"text": "Thus, the synthetic datasets are made as close as possible to real world user-generated data.", "labels": [], "entities": []}, {"text": "shows the example of generation of noisy words corresponding to a correct word considering a Hindi word.", "labels": [], "entities": []}, {"text": "Thus, the pairs of noisy word and original word constitute the parallel data for training.", "labels": [], "entities": []}, {"text": "gives the details about size of the synthetic datasets for Hindi and Telugu.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the synthetic datasets for Hindi and Telugu.", "labels": [], "entities": []}]}