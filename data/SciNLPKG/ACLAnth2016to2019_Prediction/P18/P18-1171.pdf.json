{"title": [{"text": "Sequence-to-sequence Models for Cache Transition Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs.", "labels": [], "entities": [{"text": "AMR semantic graphs", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7846434712409973}]}, {"text": "We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system.", "labels": [], "entities": []}, {"text": "To address the sparsity issue of neu-ral AMR parsing, we feed feature embed-dings from the transition state to provide relevant local information for each de-coder state.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.8797570466995239}]}, {"text": "We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus.", "labels": [], "entities": []}, {"text": "We evaluate our neural transition model on the AMR parsing task, and our parser out-performs other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.", "labels": [], "entities": [{"text": "AMR parsing task", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.9265312552452087}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8844737013181051}]}, {"text": "shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts.", "labels": [], "entities": []}, {"text": "AMR has been used in various applications such as text summarization (), sentence compression (, and event extraction (.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7669276297092438}, {"text": "sentence compression", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8425231575965881}, {"text": "event extraction", "start_pos": 101, "end_pos": 117, "type": "TASK", "confidence": 0.7398080974817276}]}, {"text": "The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq The task of AMR graph parsing is to map natural language strings to AMR semantic graphs.", "labels": [], "entities": [{"text": "AMR graph parsing", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.9211920102437338}]}, {"text": "Different parsers have been developed to tackle this problem (.", "labels": [], "entities": []}, {"text": "On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 157, "end_pos": 168, "type": "TASK", "confidence": 0.9506617188453674}]}, {"text": "propose a linearization approach that encodes labeled graphs as sequences.", "labels": [], "entities": []}, {"text": "To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models.", "labels": [], "entities": []}, {"text": "use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate.", "labels": [], "entities": []}, {"text": "However, the final performance still falls behind the best-performing models.", "labels": [], "entities": []}, {"text": "The best performing AMR parsers model graph structures directly.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8822088837623596}]}, {"text": "One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system of , which is currently the top performing system.", "labels": [], "entities": []}, {"text": "This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of, who use stack LSTMs to capture action history information in the transition state of the transition system.", "labels": [], "entities": []}, {"text": "apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8853654265403748}]}, {"text": "Instead of linearizing the target AMR graph to a sequence structure, propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs.", "labels": [], "entities": []}, {"text": "They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer.", "labels": [], "entities": []}, {"text": "propose a special transition framework called a cache transition system to generate the set of semantic graphs.", "labels": [], "entities": []}, {"text": "They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer.", "labels": [], "entities": []}, {"text": "apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8975091278553009}]}, {"text": "In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9765663146972656}]}, {"text": "We want to take advantage of the sequence-to-sequence model to encode wholesentence context information and the history action sequence, while using the transition system to constrain the possible output.", "labels": [], "entities": []}, {"text": "The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 144, "end_pos": 155, "type": "TASK", "confidence": 0.8922301232814789}]}, {"text": "More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9721699357032776}]}, {"text": "We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.8809672296047211}]}, {"text": "We extend the hard attention model of, which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right.", "labels": [], "entities": []}, {"text": "When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order.", "labels": [], "entities": []}, {"text": "On the decoder side, we augment the prediction of output action with embedding features from the current transition state.", "labels": [], "entities": []}, {"text": "Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence models for AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.9663332104682922}]}], "datasetContent": [{"text": "We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing  ).", "labels": [], "entities": [{"text": "SemEval 2016 task 8", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.8171576112508774}, {"text": "meaning representation parsing", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.6864674687385559}]}, {"text": "We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment.", "labels": [], "entities": []}, {"text": "We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs.", "labels": [], "entities": []}, {"text": "For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template.", "labels": [], "entities": [{"text": "Illinois Named Entity Tagger", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.8846077919006348}]}, {"text": "We further categorize the dataset with the categories we have defined.", "labels": [], "entities": []}, {"text": "After categorization, we use Stanford CoreNLP ( ) to get the POS tags and dependencies of the categorized dataset.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9216377437114716}]}, {"text": "We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases.", "labels": [], "entities": []}, {"text": "We use a cache size of 5 in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance breakdown of each transi- tion phase.", "labels": [], "entities": []}, {"text": " Table 2: Impact of various components for the  sequence-to-sequence model (dev).", "labels": [], "entities": []}, {"text": " Table 3: Impact of cache size for the sequence- to-sequence model, hard attention (dev).", "labels": [], "entities": [{"text": "hard attention (dev)", "start_pos": 68, "end_pos": 88, "type": "METRIC", "confidence": 0.8737142443656921}]}, {"text": " Table 4:  Comparison to other AMR parsers.  *Model has been trained on the previous release  of the corpus (LDC2014T12).", "labels": [], "entities": []}]}