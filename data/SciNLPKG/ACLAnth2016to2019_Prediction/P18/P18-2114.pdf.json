{"title": [{"text": "Split and Rephrase: Better Evaluation and a Stronger Baseline", "labels": [], "entities": [{"text": "Split and Rephrase", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7262706657250723}]}], "abstractContent": [{"text": "Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP.", "labels": [], "entities": [{"text": "Splitting and rephrasing a complex sentence", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8207945923010508}]}, {"text": "We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets.", "labels": [], "entities": []}, {"text": "To aid this, we present anew train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9947640895843506}]}], "introductionContent": [{"text": "Processing long, complex sentences is challenging.", "labels": [], "entities": []}, {"text": "This is true either for humans in various circumstances or in NLP tasks like parsing) and machine translation ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9451034665107727}, {"text": "machine translation", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.7672197818756104}]}, {"text": "An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing.", "labels": [], "entities": []}, {"text": "A recent work by introduced a dataset, evaluation method and baseline systems for the task, naming it \"Split-andRephrase\".", "labels": [], "entities": []}, {"text": "The dataset includes 1,066,115 instances mapping a single complex sentence to a sequence of sentences that express the same meaning, together with RDF triples that describe their semantics.", "labels": [], "entities": []}, {"text": "They considered two system setups: a text-to-text setup that does not use the accompanying RDF information, and a semantics-augmented setup that does.", "labels": [], "entities": []}, {"text": "They report a BLEU score of 48.9 for their best text-to-text system, and of 78.7 for the best RDF-aware one.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9995367527008057}]}, {"text": "We focus on the text-totext setup, which we find to be more challenging and more natural.", "labels": [], "entities": []}, {"text": "We begin with vanilla SEQ2SEQ models with attention ( and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of and approaching their best RDF-aware method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9987347722053528}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9953429698944092}]}, {"text": "However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated-the same fact is mentioned in multiple output sentences; and (3) some facts are missingmentioned in the input but omitted in the output.", "labels": [], "entities": []}, {"text": "The model learned to memorize entity-fact pairs instead of learning to split and rephrase.", "labels": [], "entities": []}, {"text": "Indeed, feeding the model with examples containing entities alone without any facts about them causes it to output perfectly phrased but unsupported facts.", "labels": [], "entities": []}, {"text": "Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which-coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of distinct simple sentences-helps to explain the high BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 317, "end_pos": 327, "type": "METRIC", "confidence": 0.9819360971450806}]}, {"text": "To aid further research on the task, we propose a more challenging split of the data.", "labels": [], "entities": []}, {"text": "We also establish a stronger baseline by extending the SEQ2SEQ approach with a copy mechanism, which was shown to be helpful in similar tasks ( In parallel to our work, an updated version of the dataset was released (v1.0), which is larger and features a train/test split protocol which is similar to our proposal.", "labels": [], "entities": []}, {"text": "We report results on this dataset as well.", "labels": [], "entities": []}, {"text": "The code and data to reproduce our results are available on Github.", "labels": [], "entities": [{"text": "Github", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9264425039291382}]}, {"text": "We encourage future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task Definition In the split-and-rephrase task we are given a complex sentence C, and need to produce a sequence of simple sentences T 1 , ..., T n , n \u2265 2, such that the output sentences convey all and only the information in C.", "labels": [], "entities": []}, {"text": "As additional supervision, the split-and-rephrase dataset associates each sentence with a set of RDF triples that describe the information in the sentence.", "labels": [], "entities": []}, {"text": "Note that the number of simple sentences to generate is not given as part of the input.", "labels": [], "entities": []}, {"text": "Experimental Details We focus on the task of splitting a complex sentence into several simple ones without access to the corresponding RDF triples in either train or test time.", "labels": [], "entities": []}, {"text": "For evaluation we follow and compute the averaged individual multi-reference BLEU score for each prediction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9831188321113586}]}, {"text": "We split each prediction to 1 https://github.com/biu-nlp/ sprp-acl2018 2 Note that this differs from \"normal\" multi-reference BLEU (as implemented in multi-bleu.pl) since the number of references differs among the instances in the test-    Analysis We begin analyzing the results by manually inspecting the model's predictions on the validation set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9715676307678223}]}, {"text": "This reveals three common kinds of mistakes as demonstrated in: unsupported facts, repetitions, and missing facts.", "labels": [], "entities": [{"text": "repetitions", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.9068323373794556}]}, {"text": "All the unsupported facts seem to be related to entities mentioned in the source sentence.", "labels": [], "entities": []}, {"text": "Inspecting the attention weights) reveals a worrying trend: throughout the prediction, the model focuses heavily on the first word in of the first entity (\"A wizard of Mars\") while paying little attention to other cues like \"hardcover\", \"Diane\" and references of a specific complex sentence, and then average these numbers.", "labels": [], "entities": []}, {"text": "\"the ISBN number\".", "labels": [], "entities": [{"text": "ISBN number", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9396258294582367}]}, {"text": "This explains the abundance of \"hallucinated\" unsupported facts: rather than learning to split and rephrase, the model learned to identify entities, and spit out a list of facts it had memorized about them.", "labels": [], "entities": []}, {"text": "To validate this assumption, we count the number of predicted sentences which appeared as-is in the training data.", "labels": [], "entities": []}, {"text": "We find that 1645 out of the 1693 (97.16%) predicted sentences appear verbatim in the training set.", "labels": [], "entities": []}, {"text": "gives more detailed statistics on the WEBSPLIT dataset.", "labels": [], "entities": [{"text": "WEBSPLIT dataset", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.9512888789176941}]}, {"text": "To further illustrate the model's recognize-andspit strategy, we compose inputs containing an entity string which is duplicated three times, as shown in the bottom two rows of.", "labels": [], "entities": []}, {"text": "As expected, the model predicted perfectly phrased and correct facts about the given entities, although these facts are clearly not supported by the input.", "labels": [], "entities": []}, {"text": "Models with larger capacities may have greater representation power, but also a stronger tendency to memorize the training data.", "labels": [], "entities": []}, {"text": "We therefore perform experiments with copy-enhanced models of varying LSTM widths (128, 256 and 512).", "labels": [], "entities": []}, {"text": "We train the models using the negative log likelihood of p(w) as the objective.", "labels": [], "entities": [{"text": "negative log likelihood of p(w)", "start_pos": 30, "end_pos": 61, "type": "METRIC", "confidence": 0.8485919311642647}]}, {"text": "Other than the copy mechanism, we keep the settings identical to those in Section 2.", "labels": [], "entities": []}, {"text": "We train models on the original split, our proposed data split and the v1.0 split.", "labels": [], "entities": []}, {"text": "On the original data-split, the COPY512 model outperforms all baselines, improving over the previous best by 8.68 BLEU points.", "labels": [], "entities": [{"text": "COPY512", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.7290949821472168}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9984564781188965}]}, {"text": "On the new data-split, as expected, the performance degrades for all models, as they are required to generalize to sentences not seen during training.", "labels": [], "entities": []}, {"text": "The copy-augmented models perform better than the baselines in this case as well, with a larger relative gap which can be explained by the lower lexical overlap between the train and the test sets in the new split.", "labels": [], "entities": []}, {"text": "On the v1.0 split the results are similar to those on our split,: Predictions from the COPY512 model, trained on the new data split.", "labels": [], "entities": [{"text": "COPY512 model", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.9167081117630005}]}, {"text": "in spite of it being larger, indicating that merely adding data will not solve the task.", "labels": [], "entities": []}, {"text": "Analysis We inspect the models' predictions for the first 20 complex sentences of the original and new validation sets in.", "labels": [], "entities": []}, {"text": "We mark each simple sentence as being \"correct\" if it contains all and only relevant information, \"unsupported\" if it contains facts not present in the source, and \"repeated\" if it repeats information from a previous sentence.", "labels": [], "entities": []}, {"text": "We also count missing facts.", "labels": [], "entities": []}, {"text": "shows the attention weights of the COPY512 model for the same sentence in.", "labels": [], "entities": []}, {"text": "Reassuringly, the attention is now distributed more evenly over the input symbols.", "labels": [], "entities": []}, {"text": "On the new splits, all models perform catastrophically.", "labels": [], "entities": []}, {"text": "shows outputs from the COPY512 model when trained on the new split.", "labels": [], "entities": [{"text": "COPY512", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9206308722496033}]}, {"text": "On the original split, while SEQ2SEQ128 mainly suffers from missing information, perhaps due to insufficient memorization capacity, SEQ2SEQ512 generated the most unsupported sentences, due to overfitting or memorization.", "labels": [], "entities": []}, {"text": "The overall number of issues is clearly reduced in the copy-augmented models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the WEBSPLIT dataset.", "labels": [], "entities": [{"text": "WEBSPLIT dataset", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9226934611797333}]}, {"text": " Table 2: BLEU scores, simple sentences per  complex sentence (#S/C) and tokens per simple  sentence (#T/S), as computed over the test set.  SOURCE are the complex sentences and REFER- ENCE are the reference rephrasings from the test  set. Models marked with * use the semantic RDF  triples.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991494417190552}, {"text": "REFER- ENCE", "start_pos": 178, "end_pos": 189, "type": "METRIC", "confidence": 0.9531436363855997}]}, {"text": " Table 4: Statistics for the RDF-based data split", "labels": [], "entities": [{"text": "RDF-based data", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8368691504001617}]}, {"text": " Table 5: Results over the test sets of the original,  our proposed split and the v1.0 split", "labels": [], "entities": []}, {"text": " Table 7: Results of the manual analysis, showing  the number of simple sentences with unsupported  facts (unsup.), repeated facts, missing facts and  correct facts, for 20 complex sentences from the  original and new validation sets.", "labels": [], "entities": []}]}