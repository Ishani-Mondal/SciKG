{"title": [], "abstractContent": [{"text": "We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993).", "labels": [], "entities": [{"text": "open-domain neural semantic parser", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.7382604777812958}, {"text": "Discourse Representation Theory (DRT; Kamp and Reyle 1993)", "start_pos": 114, "end_pos": 172, "type": "TASK", "confidence": 0.8256910660050132}]}, {"text": "We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction , condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables).", "labels": [], "entities": [{"text": "DRS structure prediction", "start_pos": 188, "end_pos": 212, "type": "TASK", "confidence": 0.6904486417770386}, {"text": "condition prediction", "start_pos": 215, "end_pos": 235, "type": "TASK", "confidence": 0.7179487347602844}, {"text": "referent prediction", "start_pos": 274, "end_pos": 293, "type": "TASK", "confidence": 0.7321678400039673}]}, {"text": "Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.", "labels": [], "entities": [{"text": "Groningen Meaning Bank (GMB)", "start_pos": 28, "end_pos": 56, "type": "DATASET", "confidence": 0.8858562111854553}]}], "introductionContent": [{"text": "Semantic parsing is the task of mapping natural language to machine interpretable meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8217295408248901}]}, {"text": "A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL;) to dependency-based compositional semantics (\u03bb-DCS;), lambda calculus (), abstract meaning representations (, and minimal recursion semantics ().", "labels": [], "entities": []}, {"text": "Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations ().", "labels": [], "entities": []}, {"text": "The successful application of encoder-decoder models) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7369992136955261}]}, {"text": "The fact that meaning representations do not naturally conform to a linear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding Most previous work focuses on building semantic parsers for question answering tasks, such as querying a database to retrieve an answer), or conversing with a flight booking system (.", "labels": [], "entities": []}, {"text": "As a result, parsers trained on query-based datasets work on restricted domains (e.g., restaurants, meetings;), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic constructions.", "labels": [], "entities": []}, {"text": "In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning representations in the style of Discourse Representation Theory (DRT;.", "labels": [], "entities": [{"text": "open-domain semantic parsing", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6291075746218363}, {"text": "Discourse Representation Theory (DRT", "start_pos": 155, "end_pos": 191, "type": "TASK", "confidence": 0.7624425172805787}]}, {"text": "DRT is a popular theory of meaning representation designed to account fora variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions within and across sentences.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.745182454586029}, {"text": "interpretation of pronouns and temporal expressions within and across sentences", "start_pos": 122, "end_pos": 201, "type": "TASK", "confidence": 0.7071906805038453}]}, {"text": "Advantageously, it supports meaning representations for entire texts rather than isolated sentences which in turn can be translated into firstorder logic.", "labels": [], "entities": []}, {"text": "The Groningen Meaning Bank (GMB; ) provides a large collection of English texts annotated with Discourse Representation Structures (see for an example).", "labels": [], "entities": [{"text": "Groningen Meaning Bank (GMB; )", "start_pos": 4, "end_pos": 34, "type": "DATASET", "confidence": 0.798299959727696}]}, {"text": "GMB integrates various levels of semantic annotation (e.g., anaphora, named entities, thematic roles, rhetorical relations) into a unified formalism providing expressive meaning representations for open-domain texts.", "labels": [], "entities": [{"text": "GMB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8554961681365967}]}, {"text": "We treat DRT parsing as a structure prediction problem.", "labels": [], "entities": [{"text": "DRT parsing", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.8242830634117126}, {"text": "structure prediction", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7243517190217972}]}, {"text": "We develop a method to transform DRSs to tree-based representations which can be further linearized to bracketed string format.", "labels": [], "entities": []}, {"text": "We examine a series of encoder-decoder models) differing in the way tree- statement(x 1 ), say(e 1 ), Cause(e 1 , x 1 ), Topic(e 1 ,\u03c0 1 ) Figure 1: DRT meaning representation for the sentence The statement says each of the dead men wore magazine vests and carried two hand grenades.", "labels": [], "entities": []}, {"text": "structured logical forms are generated and show that a structure-aware decoder is paramount to open-domain semantic parsing.", "labels": [], "entities": [{"text": "open-domain semantic parsing", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6329695284366608}]}, {"text": "Our proposed model decomposes the decoding process into three stages.", "labels": [], "entities": []}, {"text": "The first stage predicts the structure of the meaning representation omitting details such as predicates or variable names.", "labels": [], "entities": []}, {"text": "The second stage fills in missing predicates and relations (e.g., thing, Agent) conditioning on the natural language input and the previously predicted structure.", "labels": [], "entities": []}, {"text": "Finally, the third stage predicts variable names based on the input and the information generated so far.", "labels": [], "entities": []}, {"text": "Decomposing decoding into these three steps reduces the complexity of generating logical forms since the model does not have to predict deeply nested structures, their variables, and predicates all at once.", "labels": [], "entities": []}, {"text": "Moreover, the model is able to take advantage of the GMB annotations more efficiently, e.g., examples with similar structures can be effectively used in the first stage despite being very different in their lexical make-up.", "labels": [], "entities": []}, {"text": "Finally, a piecemeal mode of generation yields more accurate predictions; since the output of every decoding step serves as input to the next one, the model is able to refine its predictions taking progressively more global context into account.", "labels": [], "entities": []}, {"text": "Experimental results on the GMB show that our three-stage decoder outperforms a vanilla encoder-decoder model and a related variant which takes shallow structure into account, by a wide margin.", "labels": [], "entities": [{"text": "GMB", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.9773388504981995}]}, {"text": "Our contributions in this work are three-fold: an open-domain semantic parser which yields discourse representation structures; a novel end-toend neural model equipped with a structured decoder which decomposes the parsing process into three stages; a DRS-to-tree conversion method which transforms DRSs to tree-based representations allowing for the application of structured decoders as well as sequential modeling.", "labels": [], "entities": [{"text": "DRS-to-tree conversion", "start_pos": 252, "end_pos": 274, "type": "TASK", "confidence": 0.7214012742042542}]}, {"text": "We release our code 1 and tree formatted version of the GMB in the hope of driving further research in opendomain semantic parsing.", "labels": [], "entities": [{"text": "GMB", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9375752806663513}, {"text": "opendomain semantic parsing", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6623904208342234}]}], "datasetContent": [{"text": "Settings Our experiments were carried out on the GMB following the tree conversion process discussed in Section 3.", "labels": [], "entities": [{"text": "GMB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.88479083776474}]}, {"text": "We adopted the training, development, and testing partitions recommended in . We compared the three models introduced in Section 4, namely the baseline sequence decoder, the shallow structured decoder and the deep structure decoder.", "labels": [], "entities": []}, {"text": "We used the same empirical hyper-parameters for all three models.", "labels": [], "entities": []}, {"text": "The dimensions of word and lemma embeddings were 64 and 32, respectively.", "labels": [], "entities": []}, {"text": "The dimensions of hidden vectors were 256 for the encoder and 128 for the decoder.", "labels": [], "entities": []}, {"text": "The encoder used two hidden layers, whereas the decoder only one.", "labels": [], "entities": []}, {"text": "The dropout rate was 0.1.", "labels": [], "entities": []}, {"text": "Pre-trained word embeddings (100 dimensions) were generated with Word2Vec trained on the AFP portion of the English Gigaword corpus.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9117076992988586}, {"text": "AFP portion of the English Gigaword corpus", "start_pos": 89, "end_pos": 131, "type": "DATASET", "confidence": 0.8589930534362793}]}, {"text": "3 Evaluation Due to the complex nature of our structured prediction task, we cannot expect model output to exactly match the gold standard.", "labels": [], "entities": []}, {"text": "For instance, the numbering of the referents maybe different, but nevertheless valid, or the order of the children of a tree node (e.g., \"DRS(india(x 1 ) say(e 1 ))\" and \"DRS(say(e 1 ) india(x 1 ))\" are the same).", "labels": [], "entities": []}, {"text": "We thus use F 1 instead of exact match accuracy.", "labels": [], "entities": [{"text": "F 1", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9921779930591583}, {"text": "exact match", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.8011311292648315}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.5798201560974121}]}, {"text": "Specifically, we report D-match 4 a metric designed to evaluate scoped meaning representations and released as part of the distribution of the Parallel Meaning Bank corpus (.", "labels": [], "entities": [{"text": "Parallel Meaning Bank corpus", "start_pos": 143, "end_pos": 171, "type": "DATASET", "confidence": 0.6376595720648766}]}, {"text": "D-match is based on Smatch 5 , a metric used to evaluate AMR graphs ; it calculates F 1 on discourse representation graphs (DRGs), i.e., triples of nodes, arcs, and their referents, applying multiple restarts to obtain a good referent (node) mapping between graphs.", "labels": [], "entities": [{"text": "AMR graphs", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.8786487281322479}, {"text": "F 1", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9910092651844025}]}, {"text": "We converted DRSs (predicted and goldstandard) into DRGs following the top-down procedure described in Algorithm 1. 6 ISCONDI-TION returns true if the child is a condition (e.g., india(x 1 )), where three arcs are created, one is connected to a parent node and the other two are connected to arg1 and arg2, respectively (lines 7-12).", "labels": [], "entities": []}, {"text": "ISQUANTIFIER returns true if the child is a quantifier (e.g., \u03c0 1 , \u00ac and 2) and three arcs are created; one is connected to the parent node, one to the referent that is created if and only", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the GMB (avg denotes the  average number of tokens per sentence).", "labels": [], "entities": [{"text": "GMB", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.7847430109977722}]}, {"text": " Table 3: GMB development set.", "labels": [], "entities": [{"text": "GMB development set", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.9584578275680542}]}, {"text": " Table 4: GMB test set.", "labels": [], "entities": [{"text": "GMB test set", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9745891491572062}]}]}