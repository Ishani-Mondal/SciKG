{"title": [{"text": "Universal Language Model Fine-tuning for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7751901745796204}]}], "abstractContent": [{"text": "Inductive transfer learning has greatly im-pacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch.", "labels": [], "entities": [{"text": "Inductive transfer learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7833579579989115}, {"text": "computer vision", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7158017456531525}]}, {"text": "We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model.", "labels": [], "entities": []}, {"text": "Our method significantly outper-forms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.8055002093315125}, {"text": "error", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9267701506614685}]}, {"text": "Furthermore , with only 100 labeled examples, it matches the performance of training from scratch on 100\u00d7 more data.", "labels": [], "entities": []}, {"text": "We open-source our pretrained models and code 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Inductive transfer learning has had a large impact on computer vision (CV).", "labels": [], "entities": [{"text": "Inductive transfer learning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8343594471613566}, {"text": "computer vision (CV)", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.7959327757358551}]}, {"text": "Applied CV models (including object detection, classification, and segmentation) are rarely trained from scratch, but instead are fine-tuned from models that have been pretrained on ImageNet, MS-COCO, and other datasets.", "labels": [], "entities": [{"text": "object detection, classification", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.7299699783325195}]}, {"text": "Text classification is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection (), emergency response, and commercial document classification, such as for legal discovery).", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.770229697227478}, {"text": "spam, fraud, and bot detection", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.5656342804431915}, {"text": "emergency response", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7556762993335724}, {"text": "document classification", "start_pos": 184, "end_pos": 207, "type": "TASK", "confidence": 0.741761326789856}, {"text": "legal discovery", "start_pos": 221, "end_pos": 236, "type": "TASK", "confidence": 0.7045852243900299}]}, {"text": "While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge.", "labels": [], "entities": []}, {"text": "Research in NLP focused mostly on transductive transfer (.", "labels": [], "entities": [{"text": "transductive transfer", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.821052759885788}]}, {"text": "For inductive transfer, fine-tuning pretrained word embeddings (, a simple transfer technique that only targets a model's first layer, has had a large impact in practice and is used inmost state-of-the-art models.", "labels": [], "entities": [{"text": "inductive transfer", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7442251741886139}]}, {"text": "Recent approaches that concatenate embeddings derived from other tasks with the input at different layers () still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness.", "labels": [], "entities": []}, {"text": "In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models.", "labels": [], "entities": []}, {"text": "However, inductive transfer via finetuning has been unsuccessful for NLP (.", "labels": [], "entities": []}, {"text": "first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability.", "labels": [], "entities": []}, {"text": "We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption.", "labels": [], "entities": [{"text": "LM fine-tuning", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8876211941242218}]}, {"text": "LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier.", "labels": [], "entities": [{"text": "forgetting", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9904735684394836}]}, {"text": "Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods.", "labels": [], "entities": []}, {"text": "We propose anew method, Universal Language Model Fine-tuning (ULMFiT) that addresses these issues and enables robust inductive transfer learning for any NLP task, akin to fine-tuning ImageNet models: The same 3-layer LSTM architecturewith the same hyperparameters and no additions other than tuned dropout hyperparametersoutperforms highly engineered models and trans-fer learning approaches on six widely studied text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 414, "end_pos": 439, "type": "TASK", "confidence": 0.8009228308995565}]}, {"text": "On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10\u00d7 and-given 50k unlabeled examples-with 100\u00d7 more data.", "labels": [], "entities": [{"text": "ULMFiT", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.5937918424606323}]}, {"text": "Contributions Our contributions are the following: 1) We propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning for any task for NLP.", "labels": [], "entities": []}, {"text": "2) We propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning.", "labels": [], "entities": []}, {"text": "3) We significantly outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets.", "labels": [], "entities": [{"text": "error", "start_pos": 112, "end_pos": 117, "type": "METRIC", "confidence": 0.9867414832115173}]}, {"text": "4) We show that our method enables extremely sample-efficient transfer learning and perform an extensive ablation analysis.", "labels": [], "entities": []}, {"text": "5) We make the pretrained models and our code available to enable wider adoption.", "labels": [], "entities": []}], "datasetContent": [{"text": "Type  While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important realworld applications.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7684031426906586}, {"text": "text classification tasks", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.8417547543843588}]}, {"text": "Topic classification For topic classification, we evaluate on the large-scale AG news and DBpedia ontology datasets created by .  Pre-processing We use the same pre-processing as in earlier work.", "labels": [], "entities": [{"text": "Topic classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7896015346050262}, {"text": "topic classification", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.8233902752399445}, {"text": "AG news and DBpedia ontology datasets", "start_pos": 78, "end_pos": 115, "type": "DATASET", "confidence": 0.8394385774930319}]}, {"text": "In addition, to allow the language model to capture aspects that might be relevant for classification, we add special tokens for upper-case words, elongation, and repetition.", "labels": [], "entities": []}, {"text": "Hyperparameters We are interested in a model that performs robustly across a diverse set of tasks.", "labels": [], "entities": []}, {"text": "To this end, if not mentioned otherwise, we use the same set of hyperparameters across tasks, which we tune on the IMDb validation set.", "labels": [], "entities": [{"text": "IMDb validation set", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9002957741419474}]}, {"text": "We use the AWD-LSTM language model () with an embedding size of 400, 3 layers, 1150 hidden activations per layer, and a BPTT batch size of 70.", "labels": [], "entities": [{"text": "BPTT batch size", "start_pos": 120, "end_pos": 135, "type": "METRIC", "confidence": 0.7491308649381002}]}, {"text": "We apply dropout of 0.4 to layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix.", "labels": [], "entities": []}, {"text": "The classifier has a hidden layer of size 50.", "labels": [], "entities": []}, {"text": "We use Adam with \u03b2 1 = 0.7 instead of the default \u03b2 1 = 0.9 and \u03b2 2 = 0.99, similar to.", "labels": [], "entities": []}, {"text": "We use a batch size of 64, abase learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively, and tune the number of epochs on the validation set of each task . We otherwise use the same practices used in ().", "labels": [], "entities": []}, {"text": "Baselines and comparison models For each task, we compare against the current state-of-theart.", "labels": [], "entities": []}, {"text": "For the IMDb and TREC-6 datasets, we compare against CoVe (), a stateof-the-art transfer learning method for NLP.", "labels": [], "entities": [{"text": "IMDb", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.9231663942337036}, {"text": "TREC-6 datasets", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.882363498210907}]}, {"text": "For the AG, Yelp, and DBpedia datasets, we compare against the state-of-the-art text categorization method by Johnson and Zhang (2017).", "labels": [], "entities": [{"text": "DBpedia datasets", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8431364595890045}]}], "tableCaptions": [{"text": " Table 1: Text classification datasets and tasks with  number of classes and training examples.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7513884603977203}]}, {"text": " Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).", "labels": [], "entities": [{"text": "Test error rates", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8308629194895426}]}, {"text": " Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).", "labels": [], "entities": [{"text": "Test error rates", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8329844077428182}, {"text": "text classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.709360733628273}]}, {"text": " Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.", "labels": [], "entities": [{"text": "Validation error rates", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.7280786832173666}]}, {"text": " Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.", "labels": [], "entities": [{"text": "Validation error rates", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.6853179534276327}]}, {"text": " Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.", "labels": [], "entities": []}]}