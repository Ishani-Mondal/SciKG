{"title": [{"text": "Automatic Question Generation using Relative Pronouns and Adverbs", "labels": [], "entities": [{"text": "Automatic Question Generation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5522049268086752}]}], "abstractContent": [{"text": "This paper presents a system that automatically generates multiple, natural language questions using relative pronouns and relative adverbs from complex English sentences.", "labels": [], "entities": []}, {"text": "Our system is syntax-based, runs on dependency parse information of a single-sentence input, and achieves high accuracy in terms of syntactic correctness, semantic adequacy, fluency and uniqueness.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9987226128578186}]}, {"text": "One of the key advantages of our system, in comparison with other rule-based approaches, is that we nearly eliminate the chances of getting a wrong wh-word in the generated question, by fetching the requisite wh-word from the input sentence itself.", "labels": [], "entities": []}, {"text": "Depending upon the input, we generate both factoid and descriptive type questions.", "labels": [], "entities": []}, {"text": "To the best of our information , the exploitation of wh-pronouns and wh-adverbs to generate questions is novel in the Automatic Question Generation task.", "labels": [], "entities": [{"text": "Automatic Question Generation", "start_pos": 118, "end_pos": 147, "type": "TASK", "confidence": 0.7002337574958801}]}], "introductionContent": [{"text": "Asking questions from learners is said to facilitate interest and learning, to recognize problem learning areas) to assess vocabulary () and reading comprehension); (), to provide writing support (, to support inquiry needs (, etc.", "labels": [], "entities": []}, {"text": "Manual generation of questions from a text for creating practice exercises, tests, quizzes, etc. has consumed labor and time of academicians and instructors since forever, and with the invent of a large body of educational material available online, there is a growing need to make this task scalable.", "labels": [], "entities": [{"text": "Manual generation of questions from a text", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8522743497576032}]}, {"text": "Along with that, in the recent times, there is an increased demand to create Intelligent tutoring systems that use computerassisted instructional material or self-help practice exercises to aid learning as well as objectively check learner's aptitude and accomplishments.", "labels": [], "entities": []}, {"text": "Inevitably, the task of Automatic Question Generation (QG) caught the attention of NLP researchers from across the globe.", "labels": [], "entities": [{"text": "Automatic Question Generation (QG)", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.7537944714228312}]}, {"text": "Automatic QG has been defined as \"the task of automatically generating questions from various inputs like raw text, database or semantic representation\" (.", "labels": [], "entities": [{"text": "Automatic QG", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.601209282875061}]}, {"text": "Apart from its direct application in the educational domain, in general, the core NLP areas like Question Answering, Dialogue Generation, Information Retrieval, Summarization, etc.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8376297652721405}, {"text": "Dialogue Generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8248257040977478}, {"text": "Information Retrieval", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.80757737159729}, {"text": "Summarization", "start_pos": 161, "end_pos": 174, "type": "TASK", "confidence": 0.9673795104026794}]}, {"text": "also benefit from large scale automatic Question Generation.", "labels": [], "entities": [{"text": "Question Generation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7300175875425339}]}], "datasetContent": [{"text": "There is no standard way to evaluate the output of a QG system.", "labels": [], "entities": []}, {"text": "In the current paper, we go with manual evaluation, where 4 independent human evaluators, all non-native English speakers but proficient in English, give scores to questions generated from the system.", "labels": [], "entities": []}, {"text": "The scoring schema is similar to one used by) albeit with some modifications.", "labels": [], "entities": []}, {"text": "To judge syntactic correctness, the evaluators give a score of 3 when the questions are syntactically well-formed and natural, 2 when they have a few syntactic errors and 1 when they are syntactically unacceptable.", "labels": [], "entities": [{"text": "syntactic correctness", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.8323855400085449}]}, {"text": "Similarly, for semantic correctness, the raters give a score of 3 when the questions are semantically correct, 2 when they have a weird meaning and 1 when they are semantically unacceptable.", "labels": [], "entities": [{"text": "semantic correctness", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8100634217262268}]}, {"text": "Unlike), we test the fluency and semantic relatedness separately.", "labels": [], "entities": []}, {"text": "The former tells us how natural the question reads.", "labels": [], "entities": []}, {"text": "A question with many embedded clauses and adjuncts is syntactically acceptable, but disturbs the intended purpose of the question and, hence, should be avoided.", "labels": [], "entities": []}, {"text": "For example, a question like Who is that girl who works at Google which has its main office in America which is a big country? is syntactically and semantically fine, but isn't as fluent as the question Who is that girl who works at Google?", "labels": [], "entities": []}, {"text": "which is basically the same question but is more fluent.", "labels": [], "entities": []}, {"text": "The evaluators give a score of 1 for questions that aren't fluent and 2 to the ones that are.", "labels": [], "entities": []}, {"text": "Lastly, evaluators rate the questions for how unique they are.", "labels": [], "entities": []}, {"text": "Adding this criteria is important because questions generated for academic purposes need to cover different aspects of the sentence.", "labels": [], "entities": []}, {"text": "This is why if the generated questions are more or less alike, the evaluators give them a low score on distribution or variety.", "labels": [], "entities": []}, {"text": "For a well distributed output, the score is 2 and fora less distributed one, it is 1.", "labels": [], "entities": []}, {"text": "The evaluators give a score of 0 when there is no output fora given sentence.", "labels": [], "entities": []}, {"text": "The scores obtained separately for syntactic correctness, semantic adequacy, fluency and distribution are used to compare the performance of the two systems.", "labels": [], "entities": [{"text": "syntactic correctness", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6968195885419846}]}, {"text": "We take sentences from the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.8786346614360809}]}, {"text": "Out of a total of 25773127 sentences, 3889289 sen-tences have one or more relative pronoun or relative adverb in them.", "labels": [], "entities": []}, {"text": "This means that sentences with relative clauses form roughly 20% of the corpus.", "labels": [], "entities": []}, {"text": "To conduct manual evaluation, we take 300 sentences from the set of sentences with relative clauses, and run ours and Heilman's system on them.", "labels": [], "entities": []}, {"text": "We give the questions generated per sentence for both the systems to 4 independent human evaluators who rate the questions on syntactic correctness, semantic adequacy, fluency and distribution.", "labels": [], "entities": []}], "tableCaptions": []}