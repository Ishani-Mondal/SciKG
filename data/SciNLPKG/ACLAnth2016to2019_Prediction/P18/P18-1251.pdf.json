{"title": [{"text": "Composing Finite State Transducers on GPUs", "labels": [], "entities": [{"text": "GPUs", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.7626845240592957}]}], "abstractContent": [{"text": "Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition.", "labels": [], "entities": [{"text": "Weighted finite state transducers (FSTs)", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6082456495080676}, {"text": "part-of-speech tagging", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7281861901283264}, {"text": "speech recognition", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7919732630252838}]}, {"text": "There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation , and we also discuss the optimizations used to achieve the best performance on this architecture.", "labels": [], "entities": [{"text": "FST composition", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.9135961532592773}]}, {"text": "We show that our approach obtains speedups of up to 6\u00d7 over our serial implementation and 4.5\u00d7 over OpenFST.", "labels": [], "entities": [{"text": "OpenFST", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9341223239898682}]}], "introductionContent": [{"text": "Finite-state transducers (FSTs) and their algorithms are widely used in speech and language processing for problems such as grapheme-to-phoneme conversion, morphological analysis, part-of-speech tagging, chunking, named entity recognition, and others (.", "labels": [], "entities": [{"text": "Finite-state transducers (FSTs)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7539486408233642}, {"text": "grapheme-to-phoneme conversion", "start_pos": 124, "end_pos": 154, "type": "TASK", "confidence": 0.7697959244251251}, {"text": "morphological analysis", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.769009530544281}, {"text": "part-of-speech tagging", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.7584567964076996}, {"text": "named entity recognition", "start_pos": 214, "end_pos": 238, "type": "TASK", "confidence": 0.6169662574927012}]}, {"text": "Hidden Markov models (, conditional random fields () and connectionist temporal classification () can also bethought of as finite-state transducers.", "labels": [], "entities": [{"text": "connectionist temporal classification", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7719854911168417}]}, {"text": "Composition is one of the most important operations on FSTs, because it allows complex FSTs to be built up from many simpler building blocks, but it is also one of the most expensive.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.9312898516654968}]}, {"text": "Much work has been done on speeding up composition on a single CPU processor.", "labels": [], "entities": []}, {"text": "Methods such as on-the-fly composition, shared data structures, and composition filters have been used to improve time and space efficiency.", "labels": [], "entities": []}, {"text": "There has also been some successful work on speeding up composition using multiple CPU cores.", "labels": [], "entities": []}, {"text": "This is a challenge because many of the algorithms used in NLP do not parallelize in a straightforward way and previous work using multi-core implementations do not handle the reduction of identical edges generated during the composition.", "labels": [], "entities": []}, {"text": "The problem becomes more acute on the graphics processing units (GPUs) architecture, which have thousands of cores but limited memory available.", "labels": [], "entities": []}, {"text": "Another problem with the composition algorithm is that techniques used on previous work (such as composition filters and methods to expand or gather transitions using dictionaries or hash tables) do not translate well to the GPU architecture given the hardware limitations and communication overheads.", "labels": [], "entities": []}, {"text": "In this paper, we parallelize the FST composition task across multiple GPU cores.", "labels": [], "entities": [{"text": "FST composition task", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.962216834227244}]}, {"text": "To our knowledge, this is the first successful attempt to do so.", "labels": [], "entities": []}, {"text": "Our approach treats the composed FST as a sparse graph and uses some techniques from the work of; to explore the graph and generate the composed edges during the search.", "labels": [], "entities": []}, {"text": "We obtain a speedup of 4.5\u00d7 against OpenFST's implementation and 6\u00d7 against our own serial implementation.", "labels": [], "entities": [{"text": "OpenFST", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9155224561691284}]}], "datasetContent": [{"text": "We tested the performance of our implementation by constructing several FSTs of varying sizes and comparing our implementation against other baselines.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.7819781303405762}]}], "tableCaptions": [{"text": " Table 1: FSTs used in our experiments. Key: Data  = language pair used to generate the transduc-", "labels": [], "entities": [{"text": "FSTs", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.5137146711349487}]}, {"text": " Table 2: This table shows how the total running time of our GPU implementation compares against  all other methods. Times (in seconds) are for composing two transducers using English as the shared  input/output vocabulary and German as the source language of the first transducer (de-en,en-*). Ratios  are relative to our parallel algorithm on the GeForce GTX 1080 Ti.", "labels": [], "entities": [{"text": "GeForce GTX 1080 Ti", "start_pos": 349, "end_pos": 368, "type": "DATASET", "confidence": 0.9316877275705338}]}, {"text": " Table 3: This table shows how the total running time of our GPU implementation compares against all  other methods. Times (in seconds) are for composing two transducers and performing edge reduction  using English as the shared input/output vocabulary and German as the source language of the first  transducer (de-en,en-*). Ratios are relative to our parallel algorithm on the GeForce GTX 1080 Ti.", "labels": [], "entities": [{"text": "edge reduction", "start_pos": 185, "end_pos": 199, "type": "TASK", "confidence": 0.6730361431837082}, {"text": "GeForce GTX 1080 Ti", "start_pos": 379, "end_pos": 398, "type": "DATASET", "confidence": 0.9082353115081787}]}]}