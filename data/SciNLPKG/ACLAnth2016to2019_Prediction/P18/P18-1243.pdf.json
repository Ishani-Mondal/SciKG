{"title": [{"text": "Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game", "labels": [], "entities": [{"text": "Interactive Language Acquisition", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6378431419531504}]}], "abstractContent": [{"text": "Building intelligent agents that can communicate with and learn from humans in natural language is of great value.", "labels": [], "entities": []}, {"text": "Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting.", "labels": [], "entities": [{"text": "Supervised language learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6124015152454376}]}, {"text": "We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game.", "labels": [], "entities": []}, {"text": "The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion.", "labels": [], "entities": []}, {"text": "Results compared with other methods verified the effectiveness of the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language is one of the most natural forms of communication for human and is typically viewed as fundamental to human intelligence; therefore it is crucial for an intelligent agent to be able to use language to communicate with human as well.", "labels": [], "entities": []}, {"text": "While supervised training with deep neural networks has led to encouraging progress in language learning, it suffers from the problem of capturing mainly the statistics of training data, and from alack of adaptiveness to new scenarios and being flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting.", "labels": [], "entities": [{"text": "language learning", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7863301932811737}]}, {"text": "Moreover, supervised training of deep neural network models needs a large number of training samples while many interesting applications require rapid learning from a small amount of data, which poses an even greater challenge to the supervised setting.", "labels": [], "entities": []}, {"text": "In contrast, humans learn in away very different from the supervised setting).", "labels": [], "entities": []}, {"text": "First, humans act upon the world and learn from the consequences of their actions.", "labels": [], "entities": []}, {"text": "While for mechanical actions such as movement, the consequences mainly follow geometrical and mechanical principles, for language, humans act by speaking, and the consequence is typically a response in the form of verbal and other behavioral feedback (e.g., nodding) from the conversation partner (i.e., teacher).", "labels": [], "entities": []}, {"text": "These types of feedback typically contain informative signals on how to improve language skills in subsequent conversations and play an important role in humans' language acquisition process.", "labels": [], "entities": []}, {"text": "Second, humans have shown a celebrated ability to learn new concepts from small amount of data (.", "labels": [], "entities": []}, {"text": "From even just one example, children seem to be able to make inferences and draw plausible boundaries between concepts, demonstrating the ability of one-shot learning).", "labels": [], "entities": []}, {"text": "The language acquisition process and the oneshot learning ability of human beings are both impressive as a manifestation of human intelligence, and are inspiring for designing novel settings and algorithms for computational language learning.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7490382492542267}, {"text": "computational language learning", "start_pos": 210, "end_pos": 241, "type": "TASK", "confidence": 0.637083907922109}]}, {"text": "In this paper, we leverage conversation as both an interactive environment for language learning) and a natural interface for acquiring new knowledge ().", "labels": [], "entities": []}, {"text": "We propose an approach for interactive language acquisition with one-shot concept learning ability.", "labels": [], "entities": [{"text": "interactive language acquisition", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.6257066130638123}]}, {"text": "The proposed approach allows an agent to learn grounded language from scratch, acquire the trans-ferable skill of actively seeking and memorizing information about novel objects, and develop the one-shot learning ability, purely through conversational interaction with a teacher.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments with comparison to baseline approaches.", "labels": [], "entities": []}, {"text": "We first experiment with a wordlevel task in which the teacher and the learner communicate a single word each time.", "labels": [], "entities": []}, {"text": "We then investigate the impact of image variations on concept learning.", "labels": [], "entities": [{"text": "concept learning", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7402804791927338}]}, {"text": "We further perform evaluation on the more challenging sentence-level task in which the teacher and the agent communicate in the form of sentences with varying lengths.", "labels": [], "entities": []}, {"text": "To evaluate the performance in learning a transferable ability, rather than the ability of fitting a particular dataset, we use an Animal dataset for training and test the trained models on a Fruit dataset).", "labels": [], "entities": []}, {"text": "More details on the datasets are provided in Appendix A.1.", "labels": [], "entities": [{"text": "Appendix A.1", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8474078476428986}]}, {"text": "Each session consists of two randomly sampled classes, and the maximum number of interaction steps is six.", "labels": [], "entities": []}, {"text": "The following methods are compared: \u2022 Reinforce: a baseline model with the same network structure as the proposed model and trained using RL only, i.e. minimizing L R \u03b8 ; \u2022 Imitation: a recurrent encoder decoder () model with the same structure as ours and trained via imitation (minimizing L I \u03b8 ); \u2022 Imitation+Gaussian-RL: a joint imitation and reinforcement method using a Gaussian policy () in the latent space of the control vector ct ( ).", "labels": [], "entities": []}, {"text": "The policy is changed by modifying the control vector ct the action policy depends upon.", "labels": [], "entities": []}, {"text": "The training algorithm is implemented with the deep learning platform The whole network is trained from scratch in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "The network is randomly initialized without any pre-training and is trained with decayed Adagrad ().", "labels": [], "entities": []}, {"text": "We use a batch size of 16, a learning rate of 1\u00d710 \u22125 and a weight decay rate of 1.6 \u00d7 10 \u22123 . We also exploit experience replay (.", "labels": [], "entities": [{"text": "weight decay rate", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.9082443316777548}]}, {"text": "The reward discount factor \u03b3 is 0.99, the word embedding dimension dis 1024 and the dictionary size k is 80.", "labels": [], "entities": [{"text": "reward discount factor \u03b3", "start_pos": 4, "end_pos": 28, "type": "METRIC", "confidence": 0.9001568704843521}]}, {"text": "The visual image size is 32 \u00d7 32, the maximum length of generated sentence is 6 and the memory size is 10.", "labels": [], "entities": []}, {"text": "Word embedding vectors are initialized as random vectors and remain fixed during training.", "labels": [], "entities": []}, {"text": "A sampling operation is used for sentence generation during training for exploration while a max operation is used during testing both for Proposed and for Reinforce baseline.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8059896230697632}]}, {"text": "The max operation is used in both training and testing for Imitation and Imitation+Gaussian-RL baselines.", "labels": [], "entities": [{"text": "Imitation", "start_pos": 59, "end_pos": 68, "type": "TASK", "confidence": 0.581466555595398}]}], "tableCaptions": []}