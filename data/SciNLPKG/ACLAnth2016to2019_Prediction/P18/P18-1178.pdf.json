{"title": [{"text": "Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification", "labels": [], "entities": [{"text": "Multi-Passage Machine Reading Comprehension", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6777882501482964}, {"text": "Cross-Passage Answer Verification", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.698228269815445}]}], "abstractContent": [{"text": "Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine.", "labels": [], "entities": [{"text": "Machine reading comprehension (MRC) on real web", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8729695147938199}]}, {"text": "Compared with MRC on a single passage, multi-passage MRC is more challenging , since we are likely to get multiple confusing answer candidates from different passages.", "labels": [], "entities": [{"text": "MRC", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9412588477134705}, {"text": "MRC", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.69929438829422}]}, {"text": "To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations.", "labels": [], "entities": []}, {"text": "Specifically , we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification.", "labels": [], "entities": []}, {"text": "The experimental results show that our method outperforms the base-line by a large margin and achieves the state-of-the-art performance on the En-glish MS-MARCO dataset and the Chi-nese DuReader dataset, both of which are designed for MRC in real-world settings.", "labels": [], "entities": [{"text": "En-glish MS-MARCO dataset", "start_pos": 143, "end_pos": 168, "type": "DATASET", "confidence": 0.6704542537530264}, {"text": "Chi-nese DuReader dataset", "start_pos": 177, "end_pos": 202, "type": "DATASET", "confidence": 0.6298949917157491}, {"text": "MRC", "start_pos": 235, "end_pos": 238, "type": "TASK", "confidence": 0.9809412360191345}]}], "introductionContent": [{"text": "Machine reading comprehension (MRC), empowering computers with the ability to acquire knowledge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent.", "labels": [], "entities": [{"text": "Machine reading comprehension (MRC)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8119681974252065}]}, {"text": "Recent years have seen rapid growth in the MRC community.", "labels": [], "entities": [{"text": "MRC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9852043390274048}]}, {"text": "With the release of various datasets, the MRC task has evolved from the early cloze-style test to answer extraction from a single passage ( * This work was done while the first author was doing internship at 2016) and to the latest more complex question answering on web data.", "labels": [], "entities": [{"text": "MRC task", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.8970518112182617}, {"text": "answer extraction from a single passage", "start_pos": 98, "end_pos": 137, "type": "TASK", "confidence": 0.8834524154663086}, {"text": "question answering", "start_pos": 245, "end_pos": 263, "type": "TASK", "confidence": 0.7194422036409378}]}, {"text": "Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage (.", "labels": [], "entities": [{"text": "MRC tasks", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.9274373352527618}, {"text": "answer extraction", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.8939544558525085}]}, {"text": "A significant milestone is that several MRC models have exceeded the performance of human annotators on the SQuAD dataset 1 ().", "labels": [], "entities": [{"text": "MRC", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9752312898635864}, {"text": "SQuAD dataset 1", "start_pos": 108, "end_pos": 123, "type": "DATASET", "confidence": 0.8925792574882507}]}, {"text": "However, this success on single Wikipedia passage is still not adequate, considering the ultimate goal of reading the whole web.", "labels": [], "entities": []}, {"text": "Therefore, several latest datasets) attempt to design the MRC tasks in more realistic settings by involving search engines.", "labels": [], "entities": [{"text": "MRC tasks", "start_pos": 58, "end_pos": 67, "type": "TASK", "confidence": 0.9295003712177277}]}, {"text": "For each question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer.", "labels": [], "entities": []}, {"text": "One of the intrinsic challenges for such multipassage MRC is that since all the passages are question-related but usually independently written, it's probable that multiple confusing answer candidates (correct or incorrect) exist.", "labels": [], "entities": [{"text": "multipassage MRC", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.6311380863189697}]}, {"text": "shows an example from MS-MARCO.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.9503917694091797}]}, {"text": "We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect.", "labels": [], "entities": []}, {"text": "As is shown by, these confusing answer candidates could be quite difficult for MRC models to distinguish.", "labels": [], "entities": [{"text": "MRC", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9723798036575317}]}, {"text": "Therefore, special consideration is required for such multi-passage MRC problem.", "labels": [], "entities": [{"text": "MRC", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.8330352306365967}]}, {"text": "In this paper, we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the cor-Question: What is the difference between a mixed and pure culture?", "labels": [], "entities": []}, {"text": "Passages: A culture is a society's total way of living and a society is a group that live in a defined territory and participate in common culture.", "labels": [], "entities": []}, {"text": "While the answer given is in essence true, societies originally form for the express purpose to enhance . .", "labels": [], "entities": []}, {"text": "There has been resurgence in the economic system known as capitalism during the past two decades.", "labels": [], "entities": []}, {"text": "4. The mixed economy is a balance between socialism and capitalism.", "labels": [], "entities": []}, {"text": "As a result, some institutions are owned and maintained by . .", "labels": [], "entities": []}, {"text": "A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies.", "labels": [], "entities": []}, {"text": "Culture on the other hand, is the lifestyle that the people in the country . .", "labels": [], "entities": []}, {"text": "[4] Best Answer: A pure culture comprises a single species or strains.", "labels": [], "entities": [{"text": "Answer", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.8739780187606812}]}, {"text": "A mixed culture is taken from a source and may contain multiple strains or species.", "labels": [], "entities": []}, {"text": "A contaminated culture contains organisms that derived from someplace . .", "labels": [], "entities": []}, {"text": "It will beat that time when we can truly obtain a pure culture.", "labels": [], "entities": []}, {"text": "A pure culture is a culture consisting of only one strain.", "labels": [], "entities": []}, {"text": "You can obtain a pure culture by picking out a small portion of the mixed culture . .", "labels": [], "entities": []}, {"text": "A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies.", "labels": [], "entities": []}, {"text": "A pure culture is a culture consisting of only one strain.", "labels": [], "entities": []}, {"text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Reference Answer: A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies.: An example from MS-MARCO.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 194, "end_pos": 202, "type": "DATASET", "confidence": 0.9657386541366577}]}, {"text": "The text in bold is the predicted answer candidate from each passage according to the boundary model.", "labels": [], "entities": []}, {"text": "The candidate from is chosen as the final answer by this model, while the correct answer is from and can be verified by the answers from,,.", "labels": [], "entities": []}, {"text": "rect answers could occur more frequently in those passages and usually share some commonalities, while incorrect answers are usually different from one another.", "labels": [], "entities": []}, {"text": "The example in demonstrates this phenomenon.", "labels": [], "entities": []}, {"text": "We can see that the answer candidates extracted from the last four passages are all valid answers to the question and they are semantically similar to each other, while the answer candidates from the other two passages are incorrect and there is no supportive information from other passages.", "labels": [], "entities": []}, {"text": "As human beings usually compare the answer candidates from different sources to deduce the final answer, we hope that MRC model can also benefit from the cross-passage answer verification process.", "labels": [], "entities": [{"text": "MRC", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.928198516368866}]}, {"text": "The overall framework of our model is demonstrated in , which consists of three modules.", "labels": [], "entities": []}, {"text": "First, we follow the boundary-based MRC models ( to find an answer candidate for each passage by identifying the start and end position of the answer.", "labels": [], "entities": [{"text": "MRC", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.8206794857978821}]}, {"text": "Second, we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective.", "labels": [], "entities": []}, {"text": "Third, we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations.", "labels": [], "entities": [{"text": "answer verification", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.9385021030902863}]}, {"text": "We hope that the answer candidates can collect supportive information from each other according to their semantic similarities and further decide whether each candidate is corrector not.", "labels": [], "entities": []}, {"text": "Therefore, the final answer is determined by three factors: the boundary, the content and the crosspassage answer verification.", "labels": [], "entities": []}, {"text": "The three steps are modeled using different modules, which can be jointly trained in our end-to-end framework.", "labels": [], "entities": []}, {"text": "We conduct extensive experiments on the MS-MARCO ( and) datasets.", "labels": [], "entities": [{"text": "MS-MARCO ( and) datasets", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.739875853061676}]}, {"text": "The results show that our answer verification MRC model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on both datasets.", "labels": [], "entities": [{"text": "answer verification MRC", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.5956411858399709}]}, {"text": "gives an overview of our multi-passage MRC model which is mainly composed of three modules including answer boundary prediction, answer content modeling and answer verification.", "labels": [], "entities": [{"text": "MRC", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7433233261108398}, {"text": "answer boundary prediction", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.8016297618548075}, {"text": "answer content modeling", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.7958069642384847}, {"text": "answer verification", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.8369855880737305}]}, {"text": "First of all, we need to model the question and passages.", "labels": [], "entities": []}, {"text": "Following, we compute the question-aware representation for each passage (Section 2.1).", "labels": [], "entities": []}, {"text": "Based on this representation, we employ a Pointer Network ( to predict the start and end position of the answer in the module of answer boundary prediction (Section 2.2).", "labels": [], "entities": [{"text": "answer boundary prediction", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6255570352077484}]}, {"text": "At the same time, with the answer content model (Section 2.3), we estimate whether each word should be included in the answer and thus obtain the answer representations.", "labels": [], "entities": []}, {"text": "Next, in the answer verification module (Section 2.4), each answer candidate can attend to the other answer candidates to collect supportive information and we compute one score for each candidate   to indicate whether it is corrector not according to the verification.", "labels": [], "entities": []}, {"text": "The final answer is determined by not only the boundary but also the answer content and its verification score (Section 2.5).", "labels": [], "entities": []}], "datasetContent": [{"text": "To verify the effectiveness of our model on multipassage machine reading comprehension, we conduct experiments on the MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) datasets.", "labels": [], "entities": [{"text": "multipassage machine reading comprehension", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.7663343399763107}, {"text": "MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) datasets", "start_pos": 118, "end_pos": 188, "type": "DATASET", "confidence": 0.8643614252408346}]}, {"text": "Our method achieves the state-of-the-art performance on both datasets.", "labels": [], "entities": []}, {"text": "We choose the MS-MARCO and DuReader datasets to test our method, since both of them are  One prerequisite for answer verification is that there should be multiple correct answers so that they can verify each other.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9325615763664246}, {"text": "DuReader datasets", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.938065767288208}, {"text": "answer verification", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8334461748600006}]}, {"text": "Both the MS-MARCO and DuReader datasets require the human annotators to generate multiple answers if possible.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.9514778256416321}, {"text": "DuReader datasets", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9518918395042419}]}, {"text": "Table 2 shows the proportion of questions that have multiple answers.", "labels": [], "entities": []}, {"text": "However, the same answer that occurs many times is treated as one single answer here.", "labels": [], "entities": []}, {"text": "Therefore, we also report the proportion of questions that have multiple answer spans to match with the human-generated answers.", "labels": [], "entities": []}, {"text": "A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference answer.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9846256971359253}]}, {"text": "From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader.", "labels": [], "entities": [{"text": "MS-MARCO", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.9201650023460388}, {"text": "DuReader", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.9485533237457275}]}, {"text": "These answers will provide strong signals for answer verification if we can leverage them properly.", "labels": [], "entities": [{"text": "answer verification", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9316867291927338}]}], "tableCaptions": [{"text": " Table 2: Percentage of questions that have multi- ple valid answers or answer spans", "labels": [], "entities": []}, {"text": " Table 3: Performance of our method and competing models on the MS-MARCO test set", "labels": [], "entities": [{"text": "MS-MARCO test set", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9425282677014669}]}, {"text": " Table 4: Performance on the DuReader test set", "labels": [], "entities": [{"text": "DuReader test set", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9880266785621643}]}, {"text": " Table 6: Scores predicted by our model for the answer candidates shown in", "labels": [], "entities": []}, {"text": " Table 1. Although the  candidate", "labels": [], "entities": []}]}