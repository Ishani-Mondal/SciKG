{"title": [{"text": "Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings", "labels": [], "entities": [{"text": "Attention Focusing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8939705789089203}, {"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7226861119270325}]}], "abstractContent": [{"text": "In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6853140791257223}]}, {"text": "Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.6296646098295847}]}, {"text": "Source and target words are at the two ends of along information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases.", "labels": [], "entities": []}, {"text": "This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language.", "labels": [], "entities": []}, {"text": "In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings.", "labels": [], "entities": []}, {"text": "We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others.", "labels": [], "entities": []}, {"text": "Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly * Corresponding author \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u2026 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u2026 \ud97b\udf59 \ud97b\udf59 \u2026 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u2026 source target \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 Figure 1: Schematic representation of seq2seq NMT, where x 1 ,.", "labels": [], "entities": []}, {"text": ".. , x T and h 1 ,.", "labels": [], "entities": []}, {"text": ".. , h T represent source-side word embeddings and hidden states respectively, ct represents a source-side context vector, st a target-side decoder RNN hidden state, and y ta predicted word.", "labels": [], "entities": []}, {"text": "Seeking to shorten the distance between source and target word embed-dings, in what we term bridging, is the key insight for the advances presented in this paper.", "labels": [], "entities": []}, {"text": "improve quality of both sentence translation , in general, and alignment and translation of individual source words with target words, in particular.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7243658751249313}]}], "introductionContent": [{"text": "Neural machine translation (NMT) is an endto-end approach to machine translation that has achieved competitive results vis-a-vis statistical machine translation (SMT) on various language pairs (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7900661627451578}, {"text": "machine translation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7518688142299652}, {"text": "statistical machine translation (SMT)", "start_pos": 129, "end_pos": 166, "type": "TASK", "confidence": 0.7994815607865652}]}, {"text": "In NMT, the sequence-to-sequence (seq2seq) model learns word embeddings for both source and target words synchronously.", "labels": [], "entities": []}, {"text": "However, as illustrated in, source and target word embeddings are at the two ends of along information processing procedure.", "labels": [], "entities": []}, {"text": "The individual associations between them will gradually become loose due to the separation of source-side hidden states (represented by h 1 , . .", "labels": [], "entities": []}, {"text": ", h T in) and a target- Figure 2: Examples of NMT output with incorrect alignments of source and target words that cannot be the translation of each other in any possible context.", "labels": [], "entities": []}, {"text": "side hidden state (represented by st in.", "labels": [], "entities": []}, {"text": "As a result, in the absence of a more tight interaction between source and target word pairs, the seq2seq model in NMT produces tentative translations that contain incorrect alignments of source words with target counterparts that are non-admissible equivalents in any possible translation context.", "labels": [], "entities": []}, {"text": "Differently from SMT, in NMT an attention model is adopted to help align output with input words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9823966026306152}]}, {"text": "The attention model is based on the estimation of a probability distribution overall input words for each target word.", "labels": [], "entities": []}, {"text": "Word alignments with attention weights can then be easily deduced from such distributions and support the translation.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6862005740404129}, {"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9602693915367126}]}, {"text": "Nevertheless, sometimes one finds translations by NMT that contain surprisingly wrong word alignments, that would unlikely occur in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9813451766967773}]}, {"text": "For instance, shows two Chineseto-English translation examples by NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.9768897294998169}]}, {"text": "In the top example, the NMT seq2seq model incorrectly aligns the target side end of sentence mark eos to /late with a high attention weight (0.80 in this example) due to the failure of appropriately capturing the similarity, or the lack of it, between the source word /late and the target eos.", "labels": [], "entities": []}, {"text": "It is also worth noting that, as /this and /month end up not being translated in this example, inappropriate alignment of target side eos is likely the responsible factor for under translation in NMT as the decoding process ends once a target eos is generated.", "labels": [], "entities": []}, {"text": "Statistics on our development data show that as much as 50% of target side eos do not properly align to source side eos.", "labels": [], "entities": []}, {"text": "The second example in shows another case where source words are translated into target items that are not their possible translations in that or in any other context.", "labels": [], "entities": []}, {"text": "In particular, /winter olympics is incorrectly translated into a target comma \",\" and /honors into have.", "labels": [], "entities": []}, {"text": "In this paper, to address the problem illustrated above, we seek to shorten the distance within the seq2seq NMT information processing procedure between source and target word embeddings.", "labels": [], "entities": []}, {"text": "This is a method we term as bridging, and can be conceived as strengthening the focus of the attention mechanism into more translation-plausible source and target word alignments.", "labels": [], "entities": []}, {"text": "In doing so, we hope that the seq2seq model is able to learn more appropriate word alignments between source and target words.", "labels": [], "entities": [{"text": "word alignments between source and target words", "start_pos": 78, "end_pos": 125, "type": "TASK", "confidence": 0.7683727400643485}]}, {"text": "We propose three simple yet effective strategies to bridge between word embeddings.", "labels": [], "entities": []}, {"text": "The inspiring insight in all these three models is to move source word embeddings closer to target word embeddings along the seq2seq NMT information processing procedure.", "labels": [], "entities": []}, {"text": "We categorize these strategies in terms of how close the source and target word embeddings are along that procedure, schematically depicted in.", "labels": [], "entities": []}, {"text": "(1) Source-side bridging model: Our first strategy for bridging, which we call source-side bridging, is to move source word embeddings just one step closer to the target end.", "labels": [], "entities": []}, {"text": "Each source word embedding is concatenated with the respective source hidden state at the same position so that the attention model can more closely benefit from source word embeddings to produce word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 196, "end_pos": 211, "type": "TASK", "confidence": 0.693323090672493}]}, {"text": "(2) Target-side bridging model: Ina second more bold strategy, we seek to incorporate relevant source word embeddings more closely into the prediction of the next target hidden state.", "labels": [], "entities": []}, {"text": "In particular, the most appropriate source words are selected according to their attention weights and they are made to more closely interact with target hidden states.: Architecture of target-side bridging model.", "labels": [], "entities": []}], "datasetContent": [{"text": "As we have presented above three different methods to bridge between source and target word embeddings, in the present section we report on a series of experiments on Chinese to English translation that are undertaken to assess the effectiveness of those bridging methods.", "labels": [], "entities": [{"text": "Chinese to English translation", "start_pos": 167, "end_pos": 197, "type": "TASK", "confidence": 0.6469733789563179}]}, {"text": "We resorted to Chinese-English bilingual corpora that contain 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.", "labels": [], "entities": []}, {"text": "We chose the NIST06 dataset as our development set, and the NIST02, NIST03, NIST04, NIST08 datasets as our test sets.", "labels": [], "entities": [{"text": "NIST06 dataset", "start_pos": 13, "end_pos": 27, "type": "DATASET", "confidence": 0.9897474646568298}, {"text": "NIST02", "start_pos": 60, "end_pos": 66, "type": "DATASET", "confidence": 0.9846494793891907}, {"text": "NIST03", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9419611692428589}, {"text": "NIST04", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.9520553350448608}, {"text": "NIST08 datasets", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.968835324048996}]}, {"text": "We used the case-insensitive 4-gram NIST BLEU score as our evaluation metric () and the script 'mteval-v11b.pl' to compute BLEU scores.", "labels": [], "entities": [{"text": "NIST", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6400441527366638}, {"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.948224663734436}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9909332990646362}]}, {"text": "We also report TER scores on our dataset ().", "labels": [], "entities": [{"text": "TER scores", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.9756167531013489}]}, {"text": "For the efficient training of the neural networks, we limited the source (Chinese) and target (English) vocabularies to the most frequent 30k words, covering approximately 97.7% and 99.3% of the two corpora respectively.", "labels": [], "entities": []}, {"text": "All the out-ofvocabulary words were mapped to the special token UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8821239471435547}]}, {"text": "The dimension of word embedding was 620 and the size of the hidden layer was 1000.", "labels": [], "entities": []}, {"text": "All other settings were the same as in.", "labels": [], "entities": []}, {"text": "The maximum length of sentences that we used to train the NMT model in our experiments was set to 50, for both the Chinese and English sides.", "labels": [], "entities": [{"text": "NMT model", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.8956020772457123}]}, {"text": "Additionally, during decoding, we used the beam-search algorithm and set the beam size to 10.", "labels": [], "entities": []}, {"text": "The model parameters were selected according to the maximum BLEU points on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9991150498390198}]}, {"text": "We compared our proposed models against the following two systems: \u2022 cdec (: this is an open source hierarchical phrase-based SMT system with default configuration and a 4-gram language model trained on the target side of the training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.881583571434021}]}, {"text": "\u2022 RNNSearch*: this is an attention-based NMT system, taken from the dl4mt tutorial with slight changes.", "labels": [], "entities": []}, {"text": "It improves the attention model by feeding the lastly generated word.", "labels": [], "entities": []}, {"text": "For the activation function f of an RNN, we use the gated recurrent unit (GRU) (.", "labels": [], "entities": [{"text": "recurrent unit (GRU)", "start_pos": 58, "end_pos": 78, "type": "METRIC", "confidence": 0.6953853070735931}]}, {"text": "Dropout was applied only on the output layer and the dropout (Hinton et al., 2012) rate was set to 0.5.", "labels": [], "entities": [{"text": "Hinton et al., 2012)", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.7878191669782003}]}, {"text": "We used the stochastic gradient descent algorithm with mini-batch and Adadelta to train the NMT models.", "labels": [], "entities": []}, {"text": "The minibatch was set to 80 sentences and decay rates \u03c1 and \u03b5 of Adadelta were set to 0.95 and 10 \u22126 . For our NMT system with the direct bridging model, we use a simple pre-training strategy to train our model.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.8812968134880066}]}, {"text": "We first train a regular attentionbased NMT model, then use this trained model to initialize the parameters of the NMT system equipped with the direct bridging model and randomly initialize the additional parameters of the direct bridging model in this NMT system.", "labels": [], "entities": []}, {"text": "The reason of using pre-training strategy is that the embedding loss requires well-trained word alignment as a starting point.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.7005117237567902}]}, {"text": "displays the translation performance measured in terms of BLEU and TER scores.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9483233094215393}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.999291181564331}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9936188459396362}]}, {"text": "Clearly, everyone of the three NMT models we proposed, with some bridging method, improve the translation accuracy overall test sets in comparison to the SMT (cdec) and NMT (RNNSearch*) baseline systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.7918896079063416}]}], "tableCaptions": [{"text": " Table 1: BLEU and TER scores on the NIST Chinese-English translation tasks. The BLEU scores are  case-insensitive. Avg means the average scores on all test sets. \" \u2021\": statistically better than RNNSearch*  (p < 0.01). Higher BLEU (or lower TER) scores indicate better translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983556866645813}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9967995882034302}, {"text": "NIST Chinese-English translation tasks", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.7448148876428604}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9978349804878235}, {"text": "Avg", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9953383207321167}, {"text": "BLEU", "start_pos": 226, "end_pos": 230, "type": "METRIC", "confidence": 0.9975051283836365}]}, {"text": " Table 3: Confusion matrix for translation by POS, in percentage. To cope with fine-grained differences  among verbs (e.g., VV, VC and VE in Chinese, and VB, VBD, VBP, etc. in English), we merge all  verbs into V. Similarly, we merged all nouns into N. CD stands for Cardinal numbers, JJ for adjectives or  modifiers, AD for adverbs. These POS tags exist in both Chinese and English. For the sake of simplicity,  for each target POS tag, we present only the two source POS tags that are more frequently aligned with  it.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9624513983726501}]}, {"text": " Table 4.  The data in this", "labels": [], "entities": []}, {"text": " Table 5: Ratios of over translation (ROT) on test  sets. NN stands for nouns excluding proper nouns  and temporal nouns, NR for proper nouns, DT for  determiners, and CD for cardinal numbers.", "labels": [], "entities": [{"text": "over translation (ROT)", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6023732662200928}]}, {"text": " Table 6: 1-gram BLEU scores averaged on test  sets, supporting the assessment of under transla- tion. A larger score indicates less under transla- tion.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9950882792472839}]}]}