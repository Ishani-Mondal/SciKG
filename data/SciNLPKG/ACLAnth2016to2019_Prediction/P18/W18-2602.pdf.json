{"title": [{"text": "Systematic Error Analysis of the Stanford Question Answering Dataset", "labels": [], "entities": [{"text": "Systematic Error Analysis", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.739611824353536}, {"text": "Stanford Question Answering Dataset", "start_pos": 33, "end_pos": 68, "type": "DATASET", "confidence": 0.821650430560112}]}], "abstractContent": [{"text": "We analyzed the outputs of multiple question answering (QA) models applied to the Stanford Question Answering Dataset (SQuAD) to identify the core challenges for QA systems on this data set.", "labels": [], "entities": [{"text": "multiple question answering (QA)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7894678314526876}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 82, "end_pos": 125, "type": "DATASET", "confidence": 0.8671900970595223}]}, {"text": "Through an iterative process, challenging aspects were hypothesized through qualitative analysis of the common error cases.", "labels": [], "entities": []}, {"text": "A classifier was then constructed to predict whether SQuAD test examples were likely to be difficult for systems to answer based on features associated with the hypothesized aspects.", "labels": [], "entities": []}, {"text": "The classifier's performance was used to accept or reject each aspect as an indicator of difficulty.", "labels": [], "entities": []}, {"text": "With this approach , we ensured that our hypotheses were systematically tested and not simply accepted based on our pre-existing biases.", "labels": [], "entities": []}, {"text": "Our explanations are not accepted based on human evaluation of individual examples.", "labels": [], "entities": []}, {"text": "This process also enabled us to identify the primary QA strategy learned by the models, i.e., systems determined the acceptable answer type fora question and then selected the acceptable answer span of that type containing the highest density of words present in the question within its local vicinity in the passage.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the introduction of the Stanford Question Answering Dataset (SQuAD,, research groups have directed significant efforts towards achieving a high position on the SQuAD leaderboard.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD", "start_pos": 30, "end_pos": 72, "type": "DATASET", "confidence": 0.8140938182671865}]}, {"text": "This competition has re-sulted in many new models for question answering using machine reading comprehension.", "labels": [], "entities": [{"text": "question answering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.875589907169342}]}, {"text": "Within SQuAD, a single test example consists of three components: a question, a text passage and an answer.", "labels": [], "entities": []}, {"text": "The answer is a span extracted from the passage answering the question.", "labels": [], "entities": []}, {"text": "Questions were created by human annotators, who were shown a passage and asked to produce question and answer pairs.", "labels": [], "entities": []}, {"text": "In performing the question answering task, the best performing systems employed complex attention flow mechanisms for matching questions to substrings of the text passage.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8588736454645792}]}, {"text": "These models, while varied, all belong to the same general family of neural network architectures.", "labels": [], "entities": []}, {"text": "In this work, we conducted a systematic error analysis on the development set of SQuAD to explain the common failures and successes of some of these models; the results can be expected to generalize to the entire family.", "labels": [], "entities": []}, {"text": "Our goal was to explain the models' failures and successes using well defined features automatically extracted from examples.", "labels": [], "entities": []}, {"text": "We wanted to use simple features, such as word identity, over complex features.", "labels": [], "entities": []}, {"text": "We wanted to avoid explanations based on the human strategy used to answer a question, or complex features that cannot be extracted automatically, such as reasoning, commonsense or external knowledge.", "labels": [], "entities": []}, {"text": "Finally, we wanted to isolate a passages' readability from the strategy required to answer questions.", "labels": [], "entities": []}, {"text": "Our methodology used classifiers to predict the difficulty of questions.", "labels": [], "entities": []}, {"text": "The classifier performance was used to confirm or refute the validity of a hypothesized challenge using its true and false positive rates over the entire development set.", "labels": [], "entities": []}, {"text": "Systematic testing across all system failures and successes can reduce the risk of confirmation bias inherent to random spot checks.", "labels": [], "entities": []}, {"text": "A key difference with previous error analysis on SQuAD is that we looked for successes present-ing the same challenges observed in failures.", "labels": [], "entities": []}, {"text": "This confirms that the same explanations are not applicable to the successes.", "labels": [], "entities": []}, {"text": "While many system errors could be explained in term of human challenges, features related to those challenges were usually independent of failures and successes.", "labels": [], "entities": []}, {"text": "This can easily be missed by random spot checks relying on human evaluation.", "labels": [], "entities": []}, {"text": "From our evaluations, we identified a reading strategy that matched the observed failures and successes.", "labels": [], "entities": []}, {"text": "We believe that this methodology is more robust than the common ad-hoc approaches purely based on human evaluations over a small random sample.", "labels": [], "entities": []}, {"text": "The reading strategy we identified indicates that SQuAD is surprisingly well suited for neural network based models.", "labels": [], "entities": []}, {"text": "While it remains a valuable resource, this now limits its suitability for further improvement of QA models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Models used for error analysis. The sin- gle and ensemble version of each was used", "labels": [], "entities": [{"text": "error analysis", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.6562918424606323}]}, {"text": " Table 2: Distribution of question as a function of  the number of models predicting a partial match  (PM). Also includes the two main classes, EASY  (all models predicted exact matches, EMs), and  HARD (no prediction is a match.)", "labels": [], "entities": [{"text": "EASY", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9946516156196594}, {"text": "HARD", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9964320659637451}]}, {"text": " Table 5: Example of acceptability. The systems'  answer, in italic, is the only date in the passage.  The human answer is underlined.", "labels": [], "entities": []}, {"text": " Table 6: Distribution of questions by NE type and  difficulty class.", "labels": [], "entities": []}, {"text": " Table 7: Area under the curve (AUC) per group of  features", "labels": [], "entities": [{"text": "Area under the curve (AUC)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7643887656075614}]}]}