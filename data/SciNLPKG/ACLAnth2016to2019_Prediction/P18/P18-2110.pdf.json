{"title": [], "abstractContent": [{"text": "Many corpora span broad periods of time.", "labels": [], "entities": []}, {"text": "Language processing models trained during onetime period may notwork well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer).", "labels": [], "entities": []}, {"text": "This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (inter-vals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years).", "labels": [], "entities": []}, {"text": "We show experimentally that classification performance varies overtime, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language, and therefore data derived from language, changes overtime.", "labels": [], "entities": []}, {"text": "Word senses can shift overlong periods of time, and written language can change rapidly in online platforms (.", "labels": [], "entities": []}, {"text": "However, little is known about how shifts in text overtime affect the performance of language processing systems.", "labels": [], "entities": [{"text": "text overtime", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.687454953789711}]}, {"text": "This paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time.", "labels": [], "entities": [{"text": "document classification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7615862190723419}]}, {"text": "We consider both long-term variations in text overtime and seasonal variations which change throughout a year but repeat across years.", "labels": [], "entities": [{"text": "text overtime", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.6767160892486572}]}, {"text": "Our empirical study considers corpora containing formal text spanning decades as well as usergenerated content spanning only a few years.", "labels": [], "entities": []}, {"text": "After describing the datasets and experiment design, this paper has two main sections, respectively addressing the following research questions: 1.", "labels": [], "entities": []}, {"text": "In what ways does document classification depend on the timestamps of the documents?", "labels": [], "entities": [{"text": "document classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.6926876157522202}]}, {"text": "2. Can document classifiers be adapted to perform better in time-varying corpora?", "labels": [], "entities": []}, {"text": "To address question 1, we train and test on data from different time periods, to understand how performance varies with time.", "labels": [], "entities": []}, {"text": "To address question 2, we apply a domain adaptation approach, treating time intervals as domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7143837660551071}]}, {"text": "We show that inmost cases this approach can lead to improvements in classification performance, even on future time intervals.", "labels": [], "entities": [{"text": "classification", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.9511975049972534}]}], "datasetContent": [{"text": "Time intervals (non-seasonal) Time intervals (seasonal) Size Reviews (music) Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec 653K Reviews (hotels) Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec 78.6K Reviews (restaurants) Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec 1.16M News (economy) Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec 6.29K n/a 35.8K Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec 9.83K: Descriptions of corpora spanning multiple time intervals.", "labels": [], "entities": []}, {"text": "Size is the number of documents.", "labels": [], "entities": []}, {"text": "Our study experiments with six corpora: \u2022 Reviews: Three corpora containing reviews labeled with sentiment: music reviews from Amazon (, and hotel reviews and restaurant reviews from Yelp.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 183, "end_pos": 187, "type": "DATASET", "confidence": 0.9593870639801025}]}, {"text": "We discarded reviews that had fewer than 10 tokens or a helpfulness/usefulness score of zero.", "labels": [], "entities": []}, {"text": "The reviews with neutral scores were removed.", "labels": [], "entities": []}, {"text": "\u2022 Politics: Sentences from the American party platforms of Republicans and Democrats from 1948 to 2016, available every four years.", "labels": [], "entities": []}, {"text": "2 \u2022 News: Newspaper articles from 1950-2014, labeled with whether the article is relevant to the US economy.", "labels": [], "entities": []}, {"text": "3 \u2022 Twitter: Tweets labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (.", "labels": [], "entities": []}, {"text": "Our experiments require documents to be grouped into time intervals.", "labels": [], "entities": []}, {"text": "shows the intervals for each corpus.", "labels": [], "entities": []}, {"text": "Documents that fall outside of these time intervals were removed.", "labels": [], "entities": []}, {"text": "We grouped documents into two types of intervals: \u2022 Seasonal: Time intervals within a year (e.g., January through March) that maybe repeated across years.", "labels": [], "entities": []}, {"text": "\u2022 Non-seasonal: Time intervals that do not repeat (e.g., 1997-1999).", "labels": [], "entities": []}, {"text": "For each dataset, we performed binary classification, implemented in sklearn ().", "labels": [], "entities": [{"text": "binary classification", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6929856240749359}]}, {"text": "We built logistic regression classifiers with TF-IDF weighted n-gram features (n \u2208 {1, 2, 3}), removing features that appeared in less than 2 documents.", "labels": [], "entities": []}, {"text": "Except when otherwise specified, we held out a random 10% of documents as validation data for each dataset.", "labels": [], "entities": []}, {"text": "We used Elastic Net (combined 1 and 2 ) regularization (, and tuned the regularization parameters to maximize performance on the validation data.", "labels": [], "entities": []}, {"text": "We evaluated the performance using weighted F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9962116479873657}]}], "tableCaptions": [{"text": " Table 3: F1 scores when testing on the final time interval after training on all previous intervals.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9679456055164337}]}]}