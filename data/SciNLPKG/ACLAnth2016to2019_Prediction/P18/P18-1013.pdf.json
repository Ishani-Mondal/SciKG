{"title": [{"text": "A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss", "labels": [], "entities": [{"text": "Extractive and Abstractive Summarization", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.6529631391167641}]}], "abstractContent": [{"text": "We propose a unified model combining the strength of extractive and abstractive sum-marization.", "labels": [], "entities": []}, {"text": "On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.9778703451156616}]}, {"text": "On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph.", "labels": [], "entities": []}, {"text": "In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated.", "labels": [], "entities": []}, {"text": "Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions.", "labels": [], "entities": []}, {"text": "By end-to-end training our model with the inconsistency loss and original losses of extractive and ab-stractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9925325512886047}, {"text": "CNN/Daily Mail dataset", "start_pos": 228, "end_pos": 250, "type": "DATASET", "confidence": 0.9397076487541198}]}], "introductionContent": [{"text": "Text summarization is the task of automatically condensing apiece of text to a shorter version while maintaining the important points.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7488906681537628}]}, {"text": "The ability to condense text information can aid many applications such as creating news digests, presenting search results, and generating reports.", "labels": [], "entities": []}, {"text": "There are mainly two types of approaches: extractive and abstractive.", "labels": [], "entities": []}, {"text": "Extractive approaches assemble summaries directly from the source text typically selecting one whole sentence at a time.", "labels": [], "entities": []}, {"text": "In contrast, abstractive approaches can generate novel words and phrases not copied from the source text.", "labels": [], "entities": []}, {"text": "Original Article: McDonald's says......", "labels": [], "entities": [{"text": "McDonald's", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9246625304222107}]}, {"text": "The company says it expects the new 'Artisan Grilled Chicken' to be in its more than 14,300 U.S. stores by the end of next week, in products including anew sandwich, as well as existing sandwiches, wraps and salads.", "labels": [], "entities": []}, {"text": "It says the biggest change is the removal of sodium phosphates, which it said was used to keep the chicken moist, in favor of vegetable starch.", "labels": [], "entities": []}, {"text": "The new recipe also does not use maltodextrin, which McDonald's said is generally used as a sugar to increase browning or as a carrier for seasoning.", "labels": [], "entities": []}, {"text": "Jessica Foust, director of culinary innovation at McDonald's, said the changes were made because customers said they want 'simple, clean ingredients' they are familiar with......", "labels": [], "entities": []}, {"text": "And Panera Bread has said it plans to purge artificial colors, flavors and preservatives from its food by 2016......", "labels": [], "entities": [{"text": "Panera Bread", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9344744682312012}]}], "datasetContent": [{"text": "We introduce the dataset and implementation details of our method evaluated in our experiments.", "labels": [], "entities": []}, {"text": "We evaluate our models on the CNN/Daily Mail dataset () which contains news stories in CNN and Daily Mail websites.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.8877733707427978}, {"text": "Daily Mail websites", "start_pos": 95, "end_pos": 114, "type": "DATASET", "confidence": 0.8315737048784891}]}, {"text": "Each article in this dataset is paired with one humanwritten multi-sentence summary.", "labels": [], "entities": []}, {"text": "This dataset has two versions: anonymized and non-anonymized.", "labels": [], "entities": []}, {"text": "The former contains the news stories with all the named entities replaced by special tokens (e.g., @entity2); while the latter contains the raw text of each news story.", "labels": [], "entities": []}, {"text": "We follow and obtain the non-anonymized version of this dataset which has 287,113 training pairs, 13,368 validation pairs and 11,490 test pairs.", "labels": [], "entities": []}, {"text": "We perform human evaluation on Amazon Mechanical Turk (MTurk) 2 to evaluate the informativity, conciseness and readability of the summaries.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk) 2", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.9240502204213824}]}, {"text": "We compare our best model (end2end with inconsistency loss) with pointer-generator (, generative adversarial network ( ) and deep reinforcement model (.", "labels": [], "entities": []}, {"text": "For these three models, we use the test set outputs provided by the authors 3 . We randomly pick 100 examples in the test set.", "labels": [], "entities": []}, {"text": "All generated summaries are re-capitalized and de-tokenized.", "labels": [], "entities": []}, {"text": "Since trained their model on anonymized data, we also recover the anonymized entities and numbers of their outputs.", "labels": [], "entities": []}, {"text": "We show the article and 6 summaries (reference summary, 4 generated summaries and a random summary) to each human evaluator.", "labels": [], "entities": []}, {"text": "The random summary is a reference summary randomly picked from other articles and is used as a trap.", "labels": [], "entities": []}, {"text": "We show the instructions of three different aspects as: Informativity: how well does the summary capture the important parts of the article?", "labels": [], "entities": []}, {"text": "(2) Conciseness: is the summary clear enough to explain everything without being redundant?", "labels": [], "entities": []}, {"text": "(3) Readability: how well-written (fluent and grammatical) the summary is?", "labels": [], "entities": []}, {"text": "The user interface of our human evaluation is shown in the supplementary material.", "labels": [], "entities": []}, {"text": "We ask the human evaluator to evaluate each summary by scoring the three aspects with 1 to 5 score (higher the better).", "labels": [], "entities": []}, {"text": "We reject all the evaluations that score the informativity of the random summary as 3, 4 and 5.", "labels": [], "entities": []}, {"text": "By using this trap mechanism, we can ensure a much better quality of our human evaluation.", "labels": [], "entities": []}, {"text": "For each example, we first ask 5 human evaluators to evaluate.", "labels": [], "entities": []}, {"text": "However, for those articles that are too long, which are always skipped by the evaluators, it is hard to collect 5 reliable evaluations.", "labels": [], "entities": []}, {"text": "Hence, we collect at least 3 evaluations for every example.", "labels": [], "entities": []}, {"text": "For each summary, we average the scores over different human evaluators.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The reference summaries get the best score on conciseness since the recent abstractive models tend to copy sentences from the input articles.", "labels": [], "entities": []}, {"text": "However, our model learns well to select important information and form complete sentences so we even get slightly better scores on informativity and readability than the reference summaries.", "labels": [], "entities": []}, {"text": "We show atypical example of our model comparing with other state-of-Original article (truncated): A chameleon balances carefully on a branch, waiting calmly for its prey...", "labels": [], "entities": []}, {"text": "except that if you look closely, you will see that this picture is not all that it seems.", "labels": [], "entities": []}, {"text": "For the 'creature' poised to pounce is not a colourful species of lizard but something altogether more human.", "labels": [], "entities": []}, {"text": "Featuring two carefully painted female models, it is a clever piece of sculpture designed to create an amazing illusion.", "labels": [], "entities": []}, {"text": "It is the work of Italian artist Johannes Stoetter.", "labels": [], "entities": []}, {"text": "Italian artist Johannes Stoetter has painted two naked women to look like a chameleon.", "labels": [], "entities": []}, {"text": "The 37-year-old has previously transformed his models into frogs and parrots but this maybe his most intricate and impressive piece to date.", "labels": [], "entities": []}, {"text": "Stoetter daubed water-based body paint on the naked models to create the multicoloured effect, then intertwined them to form the shape of a chameleon.", "labels": [], "entities": []}, {"text": "To complete the deception, the models rested on a bench painted to match their skin and held the green branch in the air beneath them.", "labels": [], "entities": []}, {"text": "Stoetter can take weeks to plan one of his pieces and hours to paint it.", "labels": [], "entities": []}, {"text": "Speaking about The Chameleon, he said: 'I worked about four days to design the motif bigger and paint it with colours.", "labels": [], "entities": [{"text": "The Chameleon", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.725678950548172}]}, {"text": "The body painting took me about six hours with the help of an assistant.", "labels": [], "entities": [{"text": "body painting", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8996229767799377}]}, {"text": "I covered the hair with natural clay to make the heads look bald.'", "labels": [], "entities": []}, {"text": "Camouflage job: A few finishing touches are applied to the two naked models to complete the transformation.", "labels": [], "entities": []}, {"text": "'There are different difficulties on different levels as in every work, but I think that my passion and love to my work is so big, that I figure out away to deal with difficulties.", "labels": [], "entities": []}, {"text": "My main inspirations are nature, my personal life-philosophy, every-day-life and people themselves.'", "labels": [], "entities": []}, {"text": "However, the finished result existed only briefly before the models were able to getup and wash the paint off with just a video and some photographs to record it.", "labels": [], "entities": []}, {"text": "Reference summary: Johannes Stoetter's artwork features two carefully painted female models.", "labels": [], "entities": []}, {"text": "The 37-year-old has previously transformed models into frogs and parrots.", "labels": [], "entities": []}, {"text": "Daubed water-based body paint on naked models to create the effect.", "labels": [], "entities": []}, {"text": "Completing the deception, models rested on bench painted to match skin.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  \u00b10.33.", "labels": [], "entities": []}, {"text": " Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most \u00b10.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.", "labels": [], "entities": [{"text": "ROUGE F-1", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7355055510997772}, {"text": "CNN/Daily Mail test set", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.9393076499303182}]}, {"text": " Table 3: Comparing human evaluation results with state-of-the-art methods.", "labels": [], "entities": []}, {"text": " Table 4: Inconsistency rate of our end-to-end  trained model with and without inconsistency loss.", "labels": [], "entities": [{"text": "Inconsistency rate", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9701524376869202}]}]}