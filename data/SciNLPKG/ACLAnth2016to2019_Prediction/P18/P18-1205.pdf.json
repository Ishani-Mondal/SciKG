{"title": [{"text": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "labels": [], "entities": []}], "abstractContent": [{"text": "Chitchat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating.", "labels": [], "entities": []}, {"text": "In this work we present the task of making chitchat more engaging by conditioning on profile information.", "labels": [], "entities": []}, {"text": "We collect data and train models to (i) condition on their given profile information ; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction.", "labels": [], "entities": [{"text": "next utterance prediction", "start_pos": 194, "end_pos": 219, "type": "TASK", "confidence": 0.606031984090805}]}, {"text": "Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors .", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6472214559713999}]}, {"text": "It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting.", "labels": [], "entities": []}, {"text": "Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (.", "labels": [], "entities": []}, {"text": "Common issues with chit-chat models include: (i) the lack of a consistent personality () as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (; Work done while at Facebook AI Research. and (iii) a tendency to produce non-specific answers like \"I don't know\" (.", "labels": [], "entities": []}, {"text": "Those three problems combine to produce an unsatisfying overall experience fora human to engage with.", "labels": [], "entities": []}, {"text": "We believe some of those problems are due to there being no good publicly available dataset for general chit-chat.", "labels": [], "entities": []}, {"text": "Because of the low quality of current conversational models, and because of the difficulty in evaluating these models, chit-chat is often ignored as an end-application.", "labels": [], "entities": []}, {"text": "Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking, or else single-turn information seeking, i.e. question answering (.", "labels": [], "entities": [{"text": "airline or restaurant booking", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.6078746542334557}, {"text": "single-turn information seeking", "start_pos": 123, "end_pos": 154, "type": "TASK", "confidence": 0.6502295434474945}, {"text": "question answering", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.8049562871456146}]}, {"text": "Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat ().", "labels": [], "entities": []}, {"text": "For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called \"Meformers\").", "labels": [], "entities": []}, {"text": "In this work we make a step towards more engaging chit-chat dialogue agents by endowing them with a configurable, but persistent persona, encoded by multiple sentences of textual description, termed a profile.", "labels": [], "entities": []}, {"text": "This profile can be stored in a memory-augmented neural network and then used to produce more personal, specific, consistent and engaging responses than a persona-free model, thus alleviating some of the common issues in chit-chat models.", "labels": [], "entities": []}, {"text": "Using the same mechanism, any existing information about the persona of the dialogue partner can also be used in the same way.", "labels": [], "entities": []}, {"text": "Our models are thus trained to both ask and answer questions about personal topics, and the resulting dialogue can be used to build a model of the persona of the speaking partner.", "labels": [], "entities": []}, {"text": "To support the training of such models, we present the PERSONA-CHAT dataset, anew dialogue dataset consisting of 164,356 utterances between crowdworkers who were randomly paired and each asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers).", "labels": [], "entities": [{"text": "PERSONA-CHAT dataset", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.7384342849254608}]}, {"text": "The paired workers were asked to chat naturally and to get to know each other during the conversation.", "labels": [], "entities": []}, {"text": "This produces interesting and engaging conversations that our agents can try to learn to mimic.", "labels": [], "entities": []}, {"text": "Studying the next utterance prediction task during dialogue, we compare a range of models: both generative and ranking models, including Seq2Seq models and Memory Networks ( as well as other standard retrieval baselines.", "labels": [], "entities": [{"text": "utterance prediction task", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.8290471037228903}]}, {"text": "We show experimentally that in either the generative or ranking case conditioning the agent with persona information gives improved prediction of the next dialogue utterance.", "labels": [], "entities": []}, {"text": "The PERSONA-CHAT dataset is designed to facilitate research into alleviating some of the issues that traditional chitchat models face, and with the aim of making such models more consistent and engaging, by endowing them with a persona.", "labels": [], "entities": [{"text": "PERSONA-CHAT dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7473779171705246}]}, {"text": "By comparing against chit-chat models built using the OpenSubtitles and Twitter datasets, human evaluations show that our dataset provides more engaging models, that are simultaneously capable of being fluent and consistent via conditioning on a persistent, recognizable profile.", "labels": [], "entities": [{"text": "OpenSubtitles and Twitter datasets", "start_pos": 54, "end_pos": 88, "type": "DATASET", "confidence": 0.7384247556328773}]}], "datasetContent": [{"text": "The aim of this work is to facilitate more engaging and more personal chit-chat dialogue.", "labels": [], "entities": []}, {"text": "The PERSONA-CHAT dataset is a crowd-sourced dataset, collected via Amazon Mechanical Turk, where each of the pair of speakers condition their dialogue on a given profile, which is provided.", "labels": [], "entities": [{"text": "PERSONA-CHAT dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8177217543125153}, {"text": "Amazon Mechanical Turk", "start_pos": 67, "end_pos": 89, "type": "DATASET", "confidence": 0.8780669371287028}]}, {"text": "The data collection consists of three stages: (i) Personas: we crowdsource a set of 1155 possible personas, each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation, and 100 for test.", "labels": [], "entities": []}, {"text": "(ii) Revised personas: to avoid modeling that takes advantage of trivial word overlap, we crowdsource additional rewritten sets of the same 1155 personas, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging.", "labels": [], "entities": []}, {"text": "(iii) Persona chat: we pair two Turkers and assign them each a random (original) persona from the pool, and ask them to chat.", "labels": [], "entities": []}, {"text": "This resulted in a dataset of 164,356 utterances over 10,981 dialogs, 15,705 utterances (968 dialogs) of which are set aside for validation, and 15,119 utterances (1000 dialogs) for test.", "labels": [], "entities": []}, {"text": "The final dataset and its corresponding data collection source code, as well as models trained on the data, are all available open source in ParlAI 2 . In the following, we describe each data collection stage and the resulting tasks in more detail.", "labels": [], "entities": []}, {"text": "We focus on the standard dialogue task of predicting the next utterance given the dialogue history, but consider this task both with and without the profile information being given to the learning agent.", "labels": [], "entities": [{"text": "predicting the next utterance", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8585791885852814}]}, {"text": "Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user.", "labels": [], "entities": []}, {"text": "We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both.", "labels": [], "entities": []}, {"text": "These scenarios can be tried using either the original personas, or the revised ones.", "labels": [], "entities": []}, {"text": "We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following . The latter consists of choosing N random distractor responses from other dialogues (in our setting, N =19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9906127154827118}, {"text": "next utterance classification loss", "start_pos": 145, "end_pos": 179, "type": "METRIC", "confidence": 0.7119695246219635}]}, {"text": "We first report results using automated evaluation metrics, and subsequently perform an extrinsic evaluation where crowdsourced workers perform a human evaluation of our models.", "labels": [], "entities": []}, {"text": "As automated metrics are notoriously poor for evaluating dialogue () we also perform human evaluation using crowdsourced workers.", "labels": [], "entities": []}, {"text": "The procedure is as follows.", "labels": [], "entities": []}, {"text": "We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3.", "labels": [], "entities": []}, {"text": "In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat.", "labels": [], "entities": []}, {"text": "Here, from the Turker's point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this).", "labels": [], "entities": []}, {"text": "In this setting, for both the Turker and the model, the personas come from the test set pool.", "labels": [], "entities": []}, {"text": "After the dialogue, we then ask the Turker some additional questions in order to evaluate the quality of the model.", "labels": [], "entities": []}, {"text": "We ask them to evaluate fluency, engagingness and consistency (scored between 1-5).", "labels": [], "entities": [{"text": "consistency", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9988310933113098}]}, {"text": "Finally, we measure the ability to detect the other speaker's profile by displaying two possible profiles, and ask which is more likely to be the profile of the person the Turker just spoke to.", "labels": [], "entities": []}, {"text": "More details of these measures are given in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.78055340051651}]}, {"text": "The results are reported in for the best performing generative and ranking models, in both the No Persona and Self Persona categories, 100 dialogues each.", "labels": [], "entities": []}, {"text": "We also evaluate the scores of human performance by replacing the chatbot with a human (another Turker).", "labels": [], "entities": []}, {"text": "This effectively gives us upper bound scores which we can aim for with our models.", "labels": [], "entities": []}, {"text": "Finally, and importantly, we compare our models trained on PERSONA-CHAT with chit-chat models trained with the Twitter and OpenSubtitles datasets (2009 and 2018 versions) instead, following.", "labels": [], "entities": [{"text": "PERSONA-CHAT", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.6542477607727051}, {"text": "OpenSubtitles datasets", "start_pos": 123, "end_pos": 145, "type": "DATASET", "confidence": 0.9303813874721527}]}, {"text": "Example chats from a few of the models are shown in the Appendix in, 9, 10, 11 and 12.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8990289568901062}]}, {"text": "Firstly, we see a difference in fluency, engagingness and consistency between all PERSONA-CHAT models and the models trained on OpenSubtitles and Twitter.", "labels": [], "entities": [{"text": "consistency", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9959229230880737}]}, {"text": "PERSONA-CHAT is a resource that is particularly strong at providing training data for the beginning of conversations, when the two speakers do not know each other, focusing on asking and answering questions, in contrast to other resources.", "labels": [], "entities": [{"text": "PERSONA-CHAT", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8474997282028198}]}, {"text": "We also see suggestions of more subtle differences between the models, although these differences are obscured by the high variance of the human raters' evaluations.", "labels": [], "entities": []}, {"text": "For example, in both the generative and ranking model cases, models endowed with a persona can be detected by the human conversation partner, as evidenced by the persona detection accuracies, whilst maintaining fluency and consistency compared to their nonpersona driven counterparts.", "labels": [], "entities": [{"text": "consistency", "start_pos": 223, "end_pos": 234, "type": "METRIC", "confidence": 0.9671274423599243}]}, {"text": "Finding the balance between fluency, engagement, consistency, and a persistent persona remains a strong challenge for future research.", "labels": [], "entities": [{"text": "consistency", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9763898849487305}]}], "tableCaptions": [{"text": " Table 3: Evaluation of dialog utterance prediction with various models in three settings: without  conditioning on a persona, conditioned on the speakers given persona (\"Original Persona\"), or a revised  persona that does not have word overlap.", "labels": [], "entities": [{"text": "dialog utterance prediction", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.7993451555569967}]}]}