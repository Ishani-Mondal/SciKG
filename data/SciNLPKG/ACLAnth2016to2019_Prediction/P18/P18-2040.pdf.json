{"title": [{"text": "Improving Topic Quality by Promoting Named Entities in Topic Modeling", "labels": [], "entities": [{"text": "Improving Topic Quality", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8060721357663473}, {"text": "Topic Modeling", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7101842761039734}]}], "abstractContent": [{"text": "News-related content has been extensively studied in both topic modeling research and named entity recognition.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7269683182239532}, {"text": "named entity recognition", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7027373711268107}]}, {"text": "However, expressive power of named entities and their potential for improving the quality of discovered topics has not received much attention.", "labels": [], "entities": []}, {"text": "In this paper we use named entities as domain-specific terms for news-centric content and present anew weight-ing model for Latent Dirichlet Allocation.", "labels": [], "entities": []}, {"text": "Our experimental results indicate that involving more named entities in topic de-scriptors positively influences the overall quality of topics, improving their inter-pretability, specificity and diversity.", "labels": [], "entities": []}], "introductionContent": [{"text": "News-centric content conveys information about events, individuals and other entities.", "labels": [], "entities": []}, {"text": "Analysis of news-related documents includes identifying hidden features for classifying them or summarizing the content.", "labels": [], "entities": [{"text": "summarizing the content", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.8526729742685953}]}, {"text": "Topic modeling is the standard technique for such purposes, and Latent Dirichlet Allocation (LDA) () is the most used algorithm, which models the documents as distribution over topics and topics as distribution over words.", "labels": [], "entities": [{"text": "Topic modeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7963502109050751}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.9407874147097269}]}, {"text": "A good topic model is characterized by its coherence: any coherent topic should contain related words belonging to the same concept.", "labels": [], "entities": []}, {"text": "A good topic must also be distinctive enough to include domain-specific content.", "labels": [], "entities": []}, {"text": "For news-related texts domain-specific content can be represented by named entities (NE), describing facts, events and people involved in news and discussions.", "labels": [], "entities": []}, {"text": "It explains the need to include named entities in topic modeling process.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7272510528564453}]}, {"text": "The main contribution of this work is improving topic quality with LDA by increasing the importance of named entities in the model.", "labels": [], "entities": []}, {"text": "The idea is to adapt the topic model to include more domainspecific terms (NE) in the topic descriptors.", "labels": [], "entities": []}, {"text": "We designed our model to be flexible, in order to be used in different variations of LDA.", "labels": [], "entities": []}, {"text": "We ultimately employ a term-weighting approach for the LDA input.", "labels": [], "entities": []}, {"text": "Our results show that: i) named entities can serve as favorable candidates for high-quality topic descriptors, and ii) weighting model based on pseudo term frequencies is able to improve overall topic quality without the need to interfere with LDA's generative process, which makes it adaptable to other LDA variations.", "labels": [], "entities": []}, {"text": "The paper is organized in the following manner: in Section 2 we present the related work; Section 3 describes the proposed solution and is followed by Section 4, where the details of evaluation process and results are outlined.", "labels": [], "entities": []}, {"text": "We finish with Section 5, concluding the results and next steps.", "labels": [], "entities": []}], "datasetContent": [{"text": "We designed a series of tests to evaluate our proposed model: a) Baseline Unigram: basic model on the corpus consisting of single tokens (no named entities involved); b) Baseline NE: basic model on the corpus with named entities (the strategy of injecting NE in all tests is replacement instead of supplementation, as suggested by); c) NE Independent: independent named entity promoting model described in Section 3.1; and d) NE Document Dependent: document dependent named entity promoting model described in Section 3.2.", "labels": [], "entities": []}, {"text": "We evaluate the tests using the topic quality measures presented below.", "labels": [], "entities": []}, {"text": "Our test corpora consists of news-related publiclyavailable datasets: 1) 20 Newsgroups 1 : widely studied by NLP research community dataset.", "labels": [], "entities": [{"text": "NLP research community dataset", "start_pos": 109, "end_pos": 139, "type": "DATASET", "confidence": 0.8058289587497711}]}, {"text": "Contains 18846 documents with messages discussing news, people, events and other entities.", "labels": [], "entities": [{"text": "18846 documents", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.9061441421508789}]}, {"text": "2) Reuters-2013: a set of 14595 news articles from Reuters for year 2013, obtained from Financial News Dataset 2 , first compiled and used in ().", "labels": [], "entities": [{"text": "Reuters-2013: a set of 14595 news articles from Reuters for year 2013", "start_pos": 3, "end_pos": 72, "type": "DATASET", "confidence": 0.8380228877067566}, {"text": "Financial News Dataset 2", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.9591432958841324}]}, {"text": "The documents in Reuters-2013 are generally longer than in 20 Newsgroups.", "labels": [], "entities": [{"text": "Reuters-2013", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9478825330734253}]}, {"text": "For NE recognition we used NeuroNER 3 , a tool designed by, trained on CONLL2003 dataset and recognizing four types of NE: person, location, organization and miscellaneous.", "labels": [], "entities": [{"text": "NE recognition", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9528016149997711}, {"text": "CONLL2003 dataset", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9725950956344604}]}, {"text": "The further preprocessing pipeline consists of classic steps used in topic modeling.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.8363519906997681}]}], "tableCaptions": []}