{"title": [{"text": "A Structured Variational Autoencoder for Contextual Morphological Inflection", "labels": [], "entities": [{"text": "Contextual Morphological Inflection", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6626848876476288}]}], "abstractContent": [{"text": "Statistical morphological inflectors are typically trained on fully supervised, type-level data.", "labels": [], "entities": []}, {"text": "One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance?", "labels": [], "entities": []}, {"text": "To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.7340224385261536}]}, {"text": "To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm.", "labels": [], "entities": []}, {"text": "We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9322405457496643}]}], "introductionContent": [{"text": "The majority of the world's languages overtly encodes syntactic information on the word form itself, a phenomenon termed inflectional morphology).", "labels": [], "entities": []}, {"text": "In English, for example, the verbal lexeme with lemma talk has the four forms: talk, talks, talked and talking.", "labels": [], "entities": []}, {"text": "Other languages, such as Archi, distinguish more than a thousand verbal forms.", "labels": [], "entities": []}, {"text": "Despite the cornucopia of unique variants a single lexeme may mutate into, native speakers can flawlessly predict the correct variant that the lexeme's syntactic context dictates.", "labels": [], "entities": []}, {"text": "Thus, in computational linguistics, a natural question is the following: Can we estimate a probability model that can do the same?", "labels": [], "entities": []}, {"text": "The topic of inflection generation has been the focus of a flurry of individual attention of late and, moreover, has been the subject of two shared tasks * All authors contributed equally.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7274485528469086}]}, {"text": "(1) and overlayed with example values of the random variables in the sequence.", "labels": [], "entities": []}, {"text": "We highlight that all the conditionals in the Bayesian network are recurrent neural networks, e.g., we note that mi depends on m<i because we employ a recurrent neural network to model the morphological tag sequence.", "labels": [], "entities": []}, {"text": "Most work, however, has focused on the fully supervised case-a source lemma and the morpho-syntactic properties are fed into a model, which is asked to produce the desired inflection.", "labels": [], "entities": []}, {"text": "In contrast, our work focuses on the semi-supervised case, where we wish to make use of unannotated raw text, i.e., a sequence of inflected tokens.", "labels": [], "entities": []}, {"text": "Concretely, we develop a generative directed graphical model of inflected forms in context.", "labels": [], "entities": []}, {"text": "A contextual inflection model works as follows: Rather than just generating the proper inflection fora single given word form out of context (for example walking as the gerund of walk), our generative model is actually a fully-fledged language model.", "labels": [], "entities": []}, {"text": "In other words, it generates sequences of inflected words.", "labels": [], "entities": []}, {"text": "The graphical model is displayed in and examples of words it may generate are pasted on top of the graphical model notation.", "labels": [], "entities": []}, {"text": "That our model is a language model enables it to exploit both inflected lexicons and unlabeled raw text in a principled semi-supervised way.", "labels": [], "entities": []}, {"text": "In order to train using raw-text corpora (which is useful when we have less annotated data), we marginalize out the unobserved lemmata and morpho-syntactic annotation from unlabeled data.", "labels": [], "entities": []}, {"text": "In terms of, this refers to marginalizing out m 1 , . .", "labels": [], "entities": []}, {"text": ", m 4 and 1 , . .", "labels": [], "entities": []}, {"text": ", . As this marginalization is intractable, we derive a variational inference procedure that allows for efficient approximate inference.", "labels": [], "entities": []}, {"text": "Specifically, we modify the wake-sleep procedure of . It is the inclusion of raw text in this fashion that makes our model token level, a novelty in the camp of inflection generation, as much recent work in inflection generation (), trains a model on type-level lexicons.", "labels": [], "entities": []}, {"text": "We offer empirical validation of our model's utility with experiments on 23 languages from the Universal Dependencies corpus in a simulated lowresource setting.", "labels": [], "entities": []}, {"text": "1 Our semi-supervised scheme improves inflection generation by over 10% absolute accuracy in some cases.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7142095863819122}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.893129289150238}]}], "datasetContent": [{"text": "While we estimate all the parameters in the generative model, the purpose of this work is to improve the performance of morphological inflectors through semi-supervised learning with the incorporation of unlabeled data.", "labels": [], "entities": []}, {"text": "The end product of our procedure is a morphological inflector, whose performance is to be improved through the incorporation of unlabeled data.", "labels": [], "entities": []}, {"text": "Thus, we evaluate using the standard metric accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8616520762443542}]}, {"text": "We will evaluate at the type level, as is traditional in the morphological inflection literature, even though the UD treebanks on which we evaluate are token-level resources.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.7479249536991119}]}, {"text": "Concretely, we compile an incomplete type-level morphological lexicon from the tokenlevel resource.", "labels": [], "entities": []}, {"text": "To create this resource, we gather all unique form-lemma-tag triples f, , m present in the UD test data.", "labels": [], "entities": [{"text": "UD test data", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8383848667144775}]}], "tableCaptions": [{"text": " Table 3: Type-level morphological inflection accuracy across different models, training scenarios, and languages", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8588275909423828}]}]}