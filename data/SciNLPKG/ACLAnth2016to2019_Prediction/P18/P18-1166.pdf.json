{"title": [{"text": "Accelerating Neural Transformer via an Average Attention Network", "labels": [], "entities": [{"text": "Accelerating", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9578865170478821}, {"text": "Neural Transformer", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6193942576646805}]}], "abstractContent": [{"text": "With parallelizable attention networks, the neural Transformer is very fast to train.", "labels": [], "entities": []}, {"text": "However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow.", "labels": [], "entities": []}, {"text": "To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer.", "labels": [], "entities": []}, {"text": "The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network.", "labels": [], "entities": []}, {"text": "We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model.", "labels": [], "entities": []}, {"text": "With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance.", "labels": [], "entities": []}, {"text": "We conduct a series of experiments on WMT17 translation tasks, whereon 6 different language pairs, we obtain robust and consistent speed-ups in decoding.", "labels": [], "entities": [{"text": "WMT17 translation tasks", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8215095599492391}]}], "introductionContent": [{"text": "The past few years have witnessed the rapid development of neural machine translation (NMT), which translates a source sentence into the target language with an encoder-attention-decoder framework: Illustration of the decoding procedure under different neural architectures.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.8549688657124838}]}, {"text": "We show which previous target words are required to predict the current target wordy j in different NMT architectures.", "labels": [], "entities": []}, {"text": "k indicates the filter size of the convolution layer.", "labels": [], "entities": []}, {"text": "as the backbone network for translation, ranging from recurrent neural networks (RNN), convolutional neural networks (CNN) to full attention networks without recurrence and convolution.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9696657061576843}]}, {"text": "Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art performance on several language pairs.", "labels": [], "entities": []}, {"text": "Most interestingly, the neural Transformer is capable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path.", "labels": [], "entities": []}, {"text": "The parallelization property enables training NMT very quickly, while the dependency modeling property endows the Transformer with strong ability in inducing sentence semantics as well as translation correspondences.", "labels": [], "entities": [{"text": "dependency modeling", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7148071825504303}, {"text": "translation correspondences", "start_pos": 188, "end_pos": 215, "type": "TASK", "confidence": 0.9333208799362183}]}, {"text": "However, the decoding of the Transformer cannot enjoy the speed strength of parallelization due to the auto-regressive generation schema in the decoder.", "labels": [], "entities": []}, {"text": "And the self-attention network in the decoder even further slows it.", "labels": [], "entities": []}, {"text": "We explain this using, where we provide a comparison to RNN-and CNN-based NMT systems.", "labels": [], "entities": []}, {"text": "To capture dependencies from previously predicted target words, the self-attention in the neural Transformer requires to calculate adaptive attention weights on all these words ().", "labels": [], "entities": []}, {"text": "By contrast, CNN only requires previous k target words), while RNN merely 1 ().", "labels": [], "entities": []}, {"text": "Due to the auto-regressive generation schema, decoding inevitably follows a sequential manner in the Transformer.", "labels": [], "entities": []}, {"text": "Therefore the decoding procedure cannot be parallelized.", "labels": [], "entities": []}, {"text": "Furthermore, the more target words are generated, the more time the self-attention in the decoder will take to model dependencies.", "labels": [], "entities": []}, {"text": "Therefore, preserving the training efficiency of the Transformer on the one hand and accelerating its decoding on the other hand becomes anew and serious challenge.", "labels": [], "entities": []}, {"text": "In this paper, we propose an average attention network (AAN) to handle this challenge.", "labels": [], "entities": []}, {"text": "We show the architecture of AAN in, which consists of two layers: an average layer and gating layer.", "labels": [], "entities": []}, {"text": "The average layer summarizes history information via a cumulative average operation over previous positions.", "labels": [], "entities": []}, {"text": "This is equivalent to a simple attention network where original adaptively computed attention weights are replaced with averaged weights.", "labels": [], "entities": []}, {"text": "Upon this layer, we stack a feed forward gating layer to improve the model's expressiveness in describing its inputs.", "labels": [], "entities": []}, {"text": "We use AAN to replace the self-attention part of the neural Transformer's decoder.", "labels": [], "entities": [{"text": "AAN", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.7575446963310242}]}, {"text": "Considering the characteristic of the cumulative average operation, we develop a masking method to enable parallel computation just like the original selfattention network in the training.", "labels": [], "entities": []}, {"text": "In this way, the whole AAN model can be trained totally in parallel so that the training efficiency is ensured.", "labels": [], "entities": []}, {"text": "As for the decoding, we can substantially accelerate it by feeding only the previous hidden state to the Transformer decoder just like RNN does.", "labels": [], "entities": []}, {"text": "This is achieved with a dynamic programming method.", "labels": [], "entities": []}, {"text": "In spite of its simplicity, our model is capable of modeling complex dependencies.", "labels": [], "entities": []}, {"text": "This is because AAN regards each previous word as an equal contributor to current word representation.", "labels": [], "entities": [{"text": "AAN", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.7771449089050293}]}, {"text": "Therefore, no matter how long the input is, our model can always buildup connection signals with previous inputs, which we argue is very crucial for inducing long-range dependencies for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 186, "end_pos": 205, "type": "TASK", "confidence": 0.7944516241550446}]}, {"text": "We examine our model on WMT17 translation tasks.", "labels": [], "entities": [{"text": "WMT17 translation tasks", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.8255323767662048}]}, {"text": "On 6 different language pairs, our model achieves a speed-up of over 4 times with almost no loss in both translation quality and training speed.", "labels": [], "entities": []}, {"text": "In-depth analyses further demonstrate the convergency and advantages of translating long sentences of the proposed AAN.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Case-sensitive tokenized BLEU score  on WMT14 English-German translation. BLEU  scores are calculated using multi-bleu.perl.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9646401703357697}, {"text": "WMT14 English-German translation", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.9437808791796366}, {"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9974145889282227}]}, {"text": " Table 3: Time required for training and decod- ing. Training denotes the number of global train- ing steps processed per second; Decoding indi- cates the amount of time in seconds required for  translating one sentence, which is averaged over  the whole newstest2014 dataset. r shows the ra- tio between the Transformer and our model.", "labels": [], "entities": [{"text": "decod- ing", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.6458463271458944}, {"text": "newstest2014 dataset", "start_pos": 255, "end_pos": 275, "type": "DATASET", "confidence": 0.9104936718940735}]}, {"text": " Table 4: Detokenized BLEU scores for WMT17 translation tasks. Results are reported with multi-bleu- detok.perl. \"winner\" denotes the translation results generated by the WMT17 winning systems. d  indicates the difference between our model and the Transformer.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9097516536712646}, {"text": "WMT17 translation tasks", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.8025980591773987}]}, {"text": " Table 5: Average seconds required for decoding  one source sentence on WMT17 translation tasks.", "labels": [], "entities": [{"text": "WMT17 translation tasks", "start_pos": 72, "end_pos": 95, "type": "TASK", "confidence": 0.8016706307729086}]}, {"text": " Table 5. On all lan- guages in both directions, our model yields signif- icant and consistent improvements over the Trans- former in terms of decoding speed. Our model  decodes more than 4 times faster than the Trans- former. Surprisingly, our model just consumes  0.02968 seconds to translate one source sentence  on the En\u2192Tr language pair, only a seventh of  the decoding time of the Transformer. These re- sults show that the benefit of decoding accelera-", "labels": [], "entities": []}]}