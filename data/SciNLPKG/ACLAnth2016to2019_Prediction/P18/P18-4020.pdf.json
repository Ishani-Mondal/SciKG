{"title": [{"text": "Marian: Fast Neural Machine Translation in C++", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6170458098252615}]}], "abstractContent": [{"text": "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.7108972271283468}]}, {"text": "Marian is written entirely in C++.", "labels": [], "entities": [{"text": "Marian", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8830938339233398}]}, {"text": "We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present Marian, an efficient Neural Machine Translation framework written in pure C++ with minimal dependencies.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.739876409371694}]}, {"text": "It has mainly been developed at the Adam Mickiewicz University in Pozna\u00b4nPozna\u00b4n and at the University of Edinburgh.", "labels": [], "entities": []}, {"text": "It is currently being deployed in multiple European projects and is the main translation and training engine behind the neural MT launch at the World Intellectual Property Organization.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.7506285905838013}, {"text": "World Intellectual Property Organization", "start_pos": 144, "end_pos": 184, "type": "DATASET", "confidence": 0.6828803569078445}]}, {"text": "In the evolving eco-system of open-source NMT toolkits, Marian occupies its own niche best characterized by two aspects: \u2022 It is written completely in C++11 and intentionally does not provide Python bindings; model code and meta-algorithms are meant to be implemented in efficient C++ code.", "labels": [], "entities": []}, {"text": "\u2022 It is self-contained with its own back end, which provides reverse-mode automatic differentiation based on dynamic graphs.", "labels": [], "entities": []}, {"text": "Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels.", "labels": [], "entities": []}, {"text": "Intel has contributed and is optimizing a CPU backend.", "labels": [], "entities": [{"text": "Intel", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9287709593772888}]}, {"text": "Marian grew out of a C++ re-implementation of Nematus (, and still maintains binary-compatibility for common models.", "labels": [], "entities": [{"text": "Marian", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9185617566108704}]}, {"text": "Hence, we will compare speed mostly against Nematus., perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus.", "labels": [], "entities": [{"text": "speed", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9920237064361572}]}, {"text": "Marian is distributed under the MIT license and available from https://marian-nmt.", "labels": [], "entities": [{"text": "Marian", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8930580019950867}]}, {"text": "github.io or the GitHub repository https: //github.com/marian-nmt/marian.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU results for our replication of the  UEdin WMT17 system for the en-de news transla- tion task. We reproduced most steps and replaced  the deep RNN model with a Transformer model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991821646690369}, {"text": "UEdin WMT17 system", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.8432726661364237}, {"text": "en-de news transla- tion task", "start_pos": 78, "end_pos": 107, "type": "TASK", "confidence": 0.5172083576520284}]}, {"text": " Table 2: Translation time in seconds for newstest- 2017 (3,004 sentences, 76,501 source BPE tokens)  for different architectures and batch sizes.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9411252737045288}, {"text": "BPE", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.8540890216827393}]}]}