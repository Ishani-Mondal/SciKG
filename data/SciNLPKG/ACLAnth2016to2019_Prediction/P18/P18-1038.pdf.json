{"title": [], "abstractContent": [{"text": "We demonstrate that an SHRG-based parser can produce semantic graphs much more accurately than previously shown, by relating synchronous production rules to the syntacto-semantic composition process.", "labels": [], "entities": []}, {"text": "Our parser achieves an accuracy of 90.35 for EDS (89.51 for DMRS) in terms of ELEMENTARY DEPENDENCY MATCH, which is a 4.87 (5.45) point improvement over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993921518325806}, {"text": "EDS", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6827254891395569}, {"text": "ELEMENTARY DEPENDENCY MATCH", "start_pos": 78, "end_pos": 105, "type": "METRIC", "confidence": 0.7905625104904175}, {"text": "data-driven semantic parsing", "start_pos": 272, "end_pos": 300, "type": "TASK", "confidence": 0.6842535634835561}]}, {"text": "This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9995948672294617}]}], "introductionContent": [{"text": "Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG;), Elementary Dependency Structure (EDS;), Abstract Meaning Representation (AMR;, Dependency-based Minimal Recursion Semantics (DMRS;, and Universal Conceptual Cognitive Annotation (UCCA;, provide a lightweight yet effective way to encode rich semantic information of natural language sentences (.", "labels": [], "entities": []}, {"text": "Parsing to semantic graphs has been extensively studied recently.", "labels": [], "entities": [{"text": "Parsing to semantic graphs", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8859890103340149}]}, {"text": "At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled.", "labels": [], "entities": []}, {"text": "Parsers of the first type throw an input sentence into a sequence-to-sequence model and leverage the power of deep learning technologies to obtain auxiliary symbols to transform the output sequence into a graph (.", "labels": [], "entities": []}, {"text": "The strategy of the second type is to gradually generate a graph in a greedy search fashion ().", "labels": [], "entities": []}, {"text": "Usually, a transition system is defined to handle graph construction.", "labels": [], "entities": [{"text": "graph construction", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.766575962305069}]}, {"text": "The last solution explicitly associates each basic part with a target graph score, and casts parsing as the search for the graphs with highest sum of partial scores.", "labels": [], "entities": [{"text": "parsing", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.9635196328163147}]}, {"text": "Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages.", "labels": [], "entities": []}, {"text": "In theory, Synchronous Hyperedge Replacement Grammar (SHRG;) provides a mathematically sound framework to construct semantic graphs.", "labels": [], "entities": [{"text": "Synchronous Hyperedge Replacement Grammar (SHRG", "start_pos": 11, "end_pos": 58, "type": "TASK", "confidence": 0.739563857515653}]}, {"text": "In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8371883034706116}]}, {"text": "In this paper, we show that the performance that can be achieved by an SHRG-based parser is far higher than what has previously been demonstrated.", "labels": [], "entities": []}, {"text": "We focus hereon relating SHRG rules to the syntactosemantic composition process because we feel that information about syntax-semantics interface has been underexploited in the data-driven parsing architecture.", "labels": [], "entities": []}, {"text": "We demonstrate the feasibility of inducing a high-quality, linguistically-informed SHRG from compositional semantic annotations licensed by English Resource Grammar), dubbed English Resource Semantics: Parsing accuracy of the best existing grammar-free and -based models as well as our SHRG-based model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9939184188842773}]}, {"text": "ing techniques, we build a robust SHRG parser that is able to produce semantic analysis for all sentences.", "labels": [], "entities": [{"text": "SHRG parser", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.7790918350219727}]}, {"text": "Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of EL-EMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model) by a significant margin (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9994196891784668}, {"text": "EL-EMENTARY DEPENDENCY MATCH (EDM)", "start_pos": 80, "end_pos": 114, "type": "METRIC", "confidence": 0.8890103995800018}]}, {"text": "This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7071788758039474}]}, {"text": "On sentences that can be parsed by ERG-guided parsers, e.g. PET or ACE 3 , significant accuracy gaps between ERG-guided parsers and data-driven parsers are repeatedly reported (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.998063862323761}]}, {"text": "The main challenge for ERG-guided parsing is limited coverage.", "labels": [], "entities": [{"text": "ERG-guided parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8190058171749115}, {"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9764725565910339}]}, {"text": "Even for treebanking on WSJ sentences from PTB, such a parser lacks analyses for c.a.", "labels": [], "entities": [{"text": "PTB", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5275198221206665}]}, {"text": "11% of sentences ( ).", "labels": [], "entities": []}, {"text": "Our parser yields equivalent accuracy to ERG-guided parsers and equivalent coverage, full-coverage in fact, to data-driven parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9991891980171204}, {"text": "coverage", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9673377871513367}]}, {"text": "We see this investigation as striking a balance between data-driven and grammar-driven parsing.", "labels": [], "entities": [{"text": "grammar-driven parsing", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.5512969195842743}]}, {"text": "It is not our goal to argue against the use of unification grammar in high-performance deep linguistic processing.", "labels": [], "entities": [{"text": "unification grammar", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9133943915367126}]}, {"text": "Nevertheless, we do take it as a reflection of two points: (1) (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations; (2) practical parsing may benefit from transforming a model-theoretic grammar into a generative-enumerative grammar.", "labels": [], "entities": []}, {"text": "The architecture of our parser has potential uses beyond establishing a strong string-to-graph parser.", "labels": [], "entities": []}, {"text": "Our grammar extraction algorithm has some freedom to induce different SHRGs following different linguistic hypothesis, and allows some issues in theoretical linguistics to be empirically investigated.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7358527630567551}]}, {"text": "In this paper, we examine the: A partial rewriting process of HRG on the semantic graph associated with \"Some boys want to go.\"", "labels": [], "entities": []}, {"text": "Lowercase symbols indicate terminal edges, while bold, uppercase symbols indicate nonterminal edges.", "labels": [], "entities": []}, {"text": "Red edges are the hyperedges that will be replaced in the next step, while the blue edges in the next step constitute their corresponding RHS graphs.", "labels": [], "entities": []}, {"text": "lexicalist/constructivist hypothesis, a divide across a variety of theoretical frameworks, in an empirical setup.", "labels": [], "entities": [{"text": "lexicalist/constructivist hypothesis", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.609498493373394}]}, {"text": "The lexicalist tradition traces its origins to and is widely accepted by various computational grammar formalisms, including CCG, LFG, HPSG and LTAG.", "labels": [], "entities": []}, {"text": "A lexicalist approach argues that the lexical properties of words determine their syntactic and semantic behaviors.", "labels": [], "entities": []}, {"text": "The constructivist perspective, e.g. Borer's ExoSkeletal approach, emphasizes the role of syntax in constructing meanings.", "labels": [], "entities": []}, {"text": "In this paper, we focus on lexicalist and constructivist hypotheses for syntacto-semantic composition.", "labels": [], "entities": [{"text": "syntacto-semantic composition", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.7665819823741913}]}, {"text": "We present our computation-oriented analysis in \u00a76.", "labels": [], "entities": []}, {"text": "Under the architecture of our neural parser, a construction grammar works much better than a lexicalized grammar.", "labels": [], "entities": []}, {"text": "Our parser is available at https://github.", "labels": [], "entities": []}, {"text": "com/draplater/hrg-parser/.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parsing accuracy of the best existing  grammar-free and -based models as well as our  SHRG-based model. Results are copied from", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9855345487594604}]}, {"text": " Table 2: Hyperparamters used in the experiments.", "labels": [], "entities": [{"text": "Hyperparamters", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9241999387741089}]}, {"text": " Table 3: Statistics of SHRG rules with different la- bel type by the count of external points in EDS and  DMRS representations.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of grammars extracted  from unlabeled gold trees and randomly-generated  alignment-compatible trees (\"fuzzy\" trees).", "labels": [], "entities": []}, {"text": " Table 5: Accuracy of syntactic parsing under dif- ferent labels on development data. We add the  count of external nodes of corresponding HRG  rule. \"POS\" concerns the prediction of pre- terminals, while \"BCKT\" denotes bracketing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6954033374786377}, {"text": "POS", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9488470554351807}]}, {"text": " Table 6: The EDM score on EDS development  data with different model: count based greedy  search, rule embedding greedy search and beam  search. We use syntactic trees with coarse-grained  labels.", "labels": [], "entities": [{"text": "EDM", "start_pos": 14, "end_pos": 17, "type": "METRIC", "confidence": 0.7453708648681641}, {"text": "EDS development  data", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.6191740234692892}]}, {"text": " Table 7: Accuracy on the development data under  different labels of syntactic tree and beam search.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.994384765625}]}, {"text": " Table 8: Accuracy on the test set. We use syntactic  trees of coarse-grained labels and beam search.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9896712899208069}]}, {"text": " Table 9: Rules of lexicalized and construction grammars that are extracted from the running example.", "labels": [], "entities": []}, {"text": " Table 11: Comparison of the construction gram- mar and the lexicalized grammar extracted from  EDS data. We use syntax trees of coarse-grained  labels.", "labels": [], "entities": [{"text": "EDS data", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9265533983707428}]}]}