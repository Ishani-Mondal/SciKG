{"title": [{"text": "Automatic Metric Validation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Automatic Metric Validation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.543465385834376}, {"text": "Grammatical Error Correction", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.7120188971360525}]}], "abstractContent": [{"text": "Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings.", "labels": [], "entities": [{"text": "Metric validation in Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8060814407136705}]}, {"text": "However , such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement.", "labels": [], "entities": []}, {"text": "We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.6486323475837708}, {"text": "GEC metric validation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6823259393374125}]}, {"text": "Experiments with MAEGE shed anew light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8516972661018372}]}, {"text": "Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.5396762490272522}]}], "introductionContent": [{"text": "Much recent effort has been devoted to automatic evaluation, both within GEC (, and more generally in text-to-text generation tasks.", "labels": [], "entities": [{"text": "automatic evaluation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7488068342208862}, {"text": "GEC", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8103708624839783}, {"text": "text-to-text generation tasks", "start_pos": 102, "end_pos": 131, "type": "TASK", "confidence": 0.7957923114299774}]}, {"text": "Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8448403596878051}]}, {"text": "Metric validation is also raising interest in GEC, with several recent works on the subject (), all using correlation with human rankings (henceforth, CHR) as their methodology.", "labels": [], "entities": [{"text": "Metric validation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7507685422897339}, {"text": "GEC", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.5484209060668945}]}, {"text": "Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7196505963802338}]}, {"text": "Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT, and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see \u00a73).", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9811431169509888}]}, {"text": "These difficulties have motivated several proposals to alter the MT metric validation protocol, leading to a recent abandoning of evaluation by human rankings due to its unreliability (.", "labels": [], "entities": [{"text": "MT metric validation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8356234431266785}]}, {"text": "These conclusions have not yet been implemented in GEC, despite their relevance.", "labels": [], "entities": [{"text": "GEC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.6985413432121277}]}, {"text": "In \u00a73 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies.", "labels": [], "entities": [{"text": "GEC", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.6121732592582703}]}, {"text": "The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties.", "labels": [], "entities": [{"text": "GEC", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.9492093324661255}, {"text": "GEC Evaluation)", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.5411809881528219}]}, {"text": "MAEGE requires no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammaticality.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7009412050247192}]}, {"text": "For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each.", "labels": [], "entities": []}, {"text": "It then computes the correlation of the induced partial order with the metric-induced rankings.", "labels": [], "entities": []}, {"text": "MAEGE addresses many of the problems with existing methodology: \u2022 Human rankings yield low inter-rater and intra-rater agreement ( \u00a73).", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6843825578689575}]}, {"text": "Indeed, show that while annotators often generate different corrections given a sentence, they generally agree on whether a correction is valid or not.", "labels": [], "entities": []}, {"text": "Unlike CHR, MAEGE bases its scores on human corrections, rather than on rankings.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.6253212690353394}]}, {"text": "\u2022 CHR uses system outputs to obtain human rankings, which maybe misleading, as systems may share similar biases, thus neglecting to evaluate some types of valid corrections ( \u00a77).", "labels": [], "entities": []}, {"text": "MAEGE addresses this issue by systematically traversing an inclusive space of corrections.", "labels": [], "entities": []}, {"text": "\u2022 The difficulty in handling ties is addressed by only evaluating correction pairs where one contains a sub-set of the errors of the other, and is therefore clearly better.", "labels": [], "entities": []}, {"text": "\u2022 MAEGE uses established statistical tests for determining the significance of its results, thereby avoiding ad-hoc methodologies used in CHR to tackle potential biases inhuman rankings ( \u00a75, \u00a76).", "labels": [], "entities": []}, {"text": "In experiments on the standard NUCLE test set (, we find that MAEGE often disagrees with CHR as to the quality of existing metrics.", "labels": [], "entities": [{"text": "NUCLE test set", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.94184676806132}, {"text": "MAEGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.7813699245452881}, {"text": "CHR", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.6180179715156555}]}, {"text": "For example, we find that the standard GEC metric, M 2 , is a poor predictor of corpuslevel ranking, but a good predictor of sentencelevel pair-wise rankings.", "labels": [], "entities": []}, {"text": "The best predictor of corpus-level quality by MAEGE is the referenceless LT metric), while of the reference-based metrics, GLEU () fares best.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9915282726287842}]}, {"text": "In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivities of the metrics to corrections of different types, which to our knowledge is a novel contribution of this work.", "labels": [], "entities": [{"text": "MAEGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.6127815842628479}]}, {"text": "Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Section 7).", "labels": [], "entities": []}, {"text": "The importance of interpretability and detail in evaluation practices (as opposed to just providing bottom-line figures), has also been stressed in MT evaluation (e.g.,).", "labels": [], "entities": [{"text": "detail", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9849008321762085}, {"text": "MT evaluation", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.9858652353286743}]}], "datasetContent": [{"text": "Correlation with human rankings (CHR) is the standard methodology for assessing the validity of GEC metrics.", "labels": [], "entities": [{"text": "Correlation with human rankings (CHR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.4627685078552791}]}, {"text": "While informative, human rankings are costly to produce, present low inter-rater agreement (shown for MT evaluation in), and introduce methodological difficulties that are hard to overcome.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.8733433187007904}]}, {"text": "We begin by showing that existing sets of human rankings produce inconsistent results with respect to the quality of different metrics, and proceed by proposing an improved protocol for computing this correlation in the future.", "labels": [], "entities": []}, {"text": "There are two existing sets of human rankings for GEC that were compiled concurrently: GJG15 by, and NSPT15 by.", "labels": [], "entities": [{"text": "GEC", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.7993159890174866}, {"text": "GJG15", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.6380407214164734}, {"text": "NSPT15", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.83873450756073}]}, {"text": "Both sets are based on system outputs from the CoNLL 2014 () shared task, using sentences from the NUCLE test set.", "labels": [], "entities": [{"text": "CoNLL 2014 () shared task", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.9251518487930298}, {"text": "NUCLE test set", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.9732626477877299}]}, {"text": "We compute CHR against each.", "labels": [], "entities": [{"text": "CHR", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9884946942329407}]}, {"text": "System-level correlations are computed by), which adopts its methodology from MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "DATASET", "confidence": 0.626774787902832}]}, {"text": "1 shows CHR with Spearman \u03c1 (Pearson r shows similar trends).", "labels": [], "entities": [{"text": "CHR", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.7225205898284912}, {"text": "Spearman \u03c1", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9825641810894012}]}, {"text": "Results on the two datasets diverge considerably, despite their use of the same systems and corpus (albeit a different sub-set thereof).", "labels": [], "entities": []}, {"text": "For example, BLEU receives a high positive correlation on GJG15, but a negative one on NSPT15; GLEU receives a correlation of 0.51 against GJG15 and 0.76 against NSPT15; and M 2 ranges between 0.4 (GJG15) and 0.7 (NSPT15).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9984328150749207}, {"text": "GJG15", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9422531127929688}, {"text": "NSPT15", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9517127275466919}, {"text": "GLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.8595332503318787}, {"text": "GJG15", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.8239781856536865}, {"text": "NSPT15", "start_pos": 162, "end_pos": 168, "type": "DATASET", "confidence": 0.908111572265625}, {"text": "M 2", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9843484163284302}, {"text": "NSPT15", "start_pos": 214, "end_pos": 220, "type": "DATASET", "confidence": 0.9197836518287659}]}, {"text": "In fact, this variance is already apparent in the published correlations of GLEU, e.g., reported a \u03c1 of 0.56 against NSPT15 and reported a \u03c1 of 0.85 against GJG15.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.5438138246536255}, {"text": "NSPT15", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.9413804411888123}, {"text": "GJG15", "start_pos": 157, "end_pos": 162, "type": "DATASET", "confidence": 0.971555233001709}]}, {"text": "This variance in the metrics' scores is an example of the low agreement between human rankings, echoing similar findings in MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9422239065170288}]}, {"text": "Another source of inconsistency in CHR is that the rankings are relative and sampled, so datasets rank different sets of outputs (.", "labels": [], "entities": []}, {"text": "For example, if a system is judged against the best systems more often then others, it may unjustly receive a lower score.", "labels": [], "entities": []}, {"text": "TrueSkill is the best known practice to tackle such issues (), but it produces a probabilistic corpus-level score, which can vary between runs (.", "labels": [], "entities": [{"text": "TrueSkill", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8331763744354248}]}, {"text": "This makes CHR more difficult to interpret, compared to classic correlation coefficients.", "labels": [], "entities": []}, {"text": "We conclude by proposing a practice for reporting CHR in future work.", "labels": [], "entities": [{"text": "reporting CHR", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.7619859278202057}]}, {"text": "First, we combine both sets of human judgments to arrive at the statistically most powerful test.", "labels": [], "entities": []}, {"text": "Second, we compute the metrics' corpus-level rankings according to the same subset of sentences used for human rankings.", "labels": [], "entities": []}, {"text": "The current practice of allowing metrics to rank systems based on their output on the entire CoNLL test set (while human rankings are only collected fora sub-set thereof), may bias the results due to potential non-uniform system performance on the test set.", "labels": [], "entities": [{"text": "CoNLL test set", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9476715524991354}]}, {"text": "We report CHR according to the proposed protocol in (left column).", "labels": [], "entities": [{"text": "CHR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8613541126251221}]}, {"text": "We sample chains using the same sampling method as in \u00a76, and uniformly sample a source from each chain.", "labels": [], "entities": []}, {"text": "For each edit type t, we detect all pairs of corrections in the sampled chains that only differ in an edit of type t, and use them to compute \u2206 m,t . We use the set of 27 edit types given in the NUCLE corpus.", "labels": [], "entities": [{"text": "NUCLE corpus", "start_pos": 195, "end_pos": 207, "type": "DATASET", "confidence": 0.9802699685096741}]}, {"text": "presents the results, showing that under all metrics, some edits types are penalized and others rewarded.", "labels": [], "entities": []}, {"text": "iBLEU and LT penalize the least edit types, and GLEU penalizes the most, providing another perspective on GLEU's negative Kendall \u03c4 ( \u00a76).", "labels": [], "entities": [{"text": "GLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9699934124946594}, {"text": "GLEU's negative Kendall \u03c4", "start_pos": 106, "end_pos": 131, "type": "METRIC", "confidence": 0.5622598350048065}]}, {"text": "Certain types are penalized by almost all metrics.", "labels": [], "entities": []}, {"text": "One such type is Vm, wrong verb modality (e.g., \"as they [\u2205 ; may] not want to know\").", "labels": [], "entities": []}, {"text": "Another such type is Npos, a problem in noun possessive (e.g., \"their [facebook's ; Facebook] page\").", "labels": [], "entities": [{"text": "noun possessive", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7781801819801331}]}, {"text": "Other types, such as Mec, mechanical (e.g., \"[real-life ; real life]\"), and V0, missing verb (e.g., \"'Privacy', this is the word that [\u2205 ; is] popular\"), are often rewarded by the metrics.", "labels": [], "entities": [{"text": "V0", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9235474467277527}]}, {"text": "In general, the tendency of reference-based metrics (the vast majority of GEC metrics) to penalize edits of various types suggests that many edit  types are under-represented in available reference sets.", "labels": [], "entities": []}, {"text": "Automatic evaluation of systems that perform these edit types may, therefore, be unreliable.", "labels": [], "entities": []}, {"text": "Moreover, not addressing these biases in the metrics may hinder progress in GEC.", "labels": [], "entities": [{"text": "GEC", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.8736093044281006}]}, {"text": "Indeed, M 2 and GLEU, two of the most commonly used metrics, only award a small sub-set of edit types, thus offering no incentive for systems to improve performance on such types.", "labels": [], "entities": [{"text": "GLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9885097146034241}]}], "tableCaptions": [{"text": " Table 1: Metrics correlation with human judgments. The  Combined column presents the Spearman correlation coef- ficient (\u03c1) according to the combined set of human rank- ings, with its associated P-value. The GJG15 and NSPT15  columns present the Spearman correlation according to the  two sets of human rankings, as well as the rank of the metric  according to this correlation. Measures are ordered by their  rank in the combined human judgments. The discrepancy  between the \u03c1 values obtained against GJG15 and NSPT15  demonstrate low inter-rater agreement in human rankings.", "labels": [], "entities": [{"text": "Spearman correlation coef- ficient (\u03c1)", "start_pos": 86, "end_pos": 124, "type": "METRIC", "confidence": 0.6513185240328312}, {"text": "GJG15", "start_pos": 209, "end_pos": 214, "type": "DATASET", "confidence": 0.9573965668678284}, {"text": "NSPT15", "start_pos": 219, "end_pos": 225, "type": "DATASET", "confidence": 0.7859565615653992}, {"text": "GJG15", "start_pos": 504, "end_pos": 509, "type": "DATASET", "confidence": 0.9805517196655273}, {"text": "NSPT15", "start_pos": 514, "end_pos": 520, "type": "DATASET", "confidence": 0.9399158358573914}]}, {"text": " Table 2: Corpus-level Spearman \u03c1, sentence-level Pearson r and Kendall \u03c4 with the metrics (left).  \u2020 represents P-value < 0.001.", "labels": [], "entities": [{"text": "sentence-level Pearson r", "start_pos": 35, "end_pos": 59, "type": "METRIC", "confidence": 0.6448318660259247}, {"text": "Kendall \u03c4", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.8730014264583588}]}, {"text": " Table 3: Average change in metric score by metric and edit types (\u2206m,t; see text). Rows correspond to edit types (abbreviations  in Dahlmeier et al. (2013)); columns correspond to metrics. Some edit types are consistently penalized.", "labels": [], "entities": [{"text": "Average change", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9289534986019135}, {"text": "metric score", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.862690269947052}]}, {"text": " Table 4: Corpus-level Spearman \u03c1, sentence-level Pearson r and Kendall \u03c4 correlations using origin as the source with the  various metrics (left). Correlations using a random source are found in parenthesis.  \u2020 represents P \u2212 value < 0.001. LT is the  best corpus correlated, and has the best \u03c4 while iBLEU has the best r", "labels": [], "entities": [{"text": "sentence-level Pearson r", "start_pos": 35, "end_pos": 59, "type": "METRIC", "confidence": 0.6408483783404032}, {"text": "Kendall \u03c4 correlations", "start_pos": 64, "end_pos": 86, "type": "METRIC", "confidence": 0.8112526337305704}]}]}