{"title": [{"text": "Efficient and Robust Question Answering from Minimal Context over Documents", "labels": [], "entities": [{"text": "Question Answering from Minimal Context over Documents", "start_pos": 21, "end_pos": 75, "type": "TASK", "confidence": 0.7871982966150556}]}], "abstractContent": [{"text": "Neural models for question answering (QA) over documents have achieved significant performance improvements.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8654923558235168}]}, {"text": "Although effective, these models do not scale to large corpora due to their complex mod-eling of interactions between the document and the question.", "labels": [], "entities": []}, {"text": "Moreover, recent work has shown that such models are sensitive to adversarial inputs.", "labels": [], "entities": []}, {"text": "In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences.", "labels": [], "entities": []}, {"text": "Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model.", "labels": [], "entities": []}, {"text": "Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9995176792144775}, {"text": "NewsQA", "start_pos": 192, "end_pos": 198, "type": "DATASET", "confidence": 0.8909534215927124}]}, {"text": "Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of textual question answering (QA), in which a machine reads a document and answers a question, is an important and challenging problem in natural language processing.", "labels": [], "entities": [{"text": "textual question answering (QA)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.8186897486448288}]}, {"text": "Recent progress in performance of QA models has been largely due to the variety of available QA datasets ().", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.8035916388034821}]}, {"text": "Many neural QA models have been proposed for these datasets, the most successful of which tend to leverage coattention or bidirectional attention mechanisms that build codependent representations of the document and the question ().", "labels": [], "entities": []}, {"text": "Yet, learning the full context over the document is challenging and inefficient.", "labels": [], "entities": []}, {"text": "In particular, when the model is given along document, or multiple documents, learning the full context is intractably slow and hence difficult to scale to large corpora.", "labels": [], "entities": []}, {"text": "In addition, show that, given adversarial inputs, such models tend to focus on wrong parts of the context and produce incorrect answers.", "labels": [], "entities": []}, {"text": "In this paper, we aim to develop a QA system that is scalable to large documents as well as robust to adversarial inputs.", "labels": [], "entities": []}, {"text": "First, we study the context required to answer the question by sampling examples in the dataset and carefully analyzing them.", "labels": [], "entities": []}, {"text": "We find that most questions can be answered using a few sentences, without the consideration of context over entire document.", "labels": [], "entities": []}, {"text": "In particular, we observe that on the SQuAD dataset), 92% of answerable questions can be answered using a single sentence.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9084097743034363}]}, {"text": "Second, inspired by this observation, we propose a sentence selector to select the minimal set of sentences to give to the QA model in order to answer the question.", "labels": [], "entities": []}, {"text": "Since the minimum number of sentences depends on the question, our sentence selector chooses a different number of sentences for each question, in contrast with previous models that select a fixed number of sentences.", "labels": [], "entities": []}, {"text": "Our sentence selector leverages three simple techniques -weight transfer, data modification and score normalization, which we show to be highly effective on the task of sentence selection.", "labels": [], "entities": [{"text": "sentence selector", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6875469535589218}, {"text": "weight transfer", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.676097497344017}, {"text": "data modification", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7189125269651413}, {"text": "sentence selection", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7331214249134064}]}, {"text": "We compare the standard QA model given the full document (FULL) and the QA model given the Correct (Not exactly same 58 Gothic architecture is represented in the majestic churches but also at the burgher What type of architecture is represented as grountruth) houses and fortifications.", "labels": [], "entities": [{"text": "FULL", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9507530331611633}, {"text": "Correct", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9796468019485474}]}, {"text": "1 Fail to select precise span 6 Brownlee argues that disobedience in opposition to the decisions of non-governmental Brownlee argues disobedience can be agencies such as trade unions, banks, and private universities can be justified if it justified toward what institutions?", "labels": [], "entities": []}, {"text": "reflects 'a larger challenge to the legal system that permits those decisions to be taken;.", "labels": [], "entities": []}, {"text": "2 Complex semantics in 34 Newton was limited by Denver's defense, which sacked him seven times and forced him How many times did the Denver defense sentence/question into three turnovers, including a fumble which they recovered fora touchdown.", "labels": [], "entities": []}, {"text": "3 Not answerable even with 2 He encourages a distinction between lawful protest demonstration, nonviolent civil What type of civil disobedience is full paragraph disobedience, and violent civil disobedience.", "labels": [], "entities": [{"text": "full paragraph disobedience", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.5799724360307058}]}, {"text": "accompanied by aggression?: Error cases (on exact match (EM)) of DCN+ given oracle sentence on SQuAD.", "labels": [], "entities": [{"text": "exact match (EM))", "start_pos": 44, "end_pos": 61, "type": "METRIC", "confidence": 0.9507257223129273}]}, {"text": "50 examples are sampled randomly.", "labels": [], "entities": []}, {"text": "Grountruth span is in underlined text, and model's prediction is in bold text.", "labels": [], "entities": [{"text": "Grountruth span", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9450305104255676}]}, {"text": "minimal set of sentences (MINIMAL) on five different QA tasks with varying sizes of documents.", "labels": [], "entities": []}, {"text": "On SQuAD, NewsQA, TriviaQA(Wikipedia) and SQuAD-Open, MINIMAL achieves significant reductions in training and inference times (up to 15\u00d7 and 13\u00d7, respectively), with accuracy comparable to or better than FULL.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9556025862693787}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.999554455280304}, {"text": "FULL", "start_pos": 204, "end_pos": 208, "type": "METRIC", "confidence": 0.7856677770614624}]}, {"text": "On three of those datasets, this improvements leads to the new stateof-the-art.", "labels": [], "entities": []}, {"text": "In addition, our experimental results and analyses show that our approach is more robust to adversarial inputs.", "labels": [], "entities": []}, {"text": "On the development set of SQuAD-Adversarial (Jia and Liang, 2017), MINIMAL outperforms the previous state-of-theart model by up to 13%.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train and evaluate our model on five different datasets as shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Human analysis of the context required to answer questions on SQuAD and TriviaQA. 50  examples from each dataset are sampled randomly. 'N sent' indicates the number of sentences required  to answer the question, and 'N/A' indicates the question is not answerable even given all sentences in  the document. 'Document' and 'Question' are from the representative example from each category on  SQuAD. Examples on TriviaQA are shown in Appendix B. The groundtruth answer span is in red text,  and the oracle sentence (the sentence containing the grountruth answer span) is in bold text.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.871863603591919}]}, {"text": " Table 3: Dataset used for experiments. 'N word', 'N sent' and 'N doc' refer to the average number of  words, sentences and documents, respectively. All statistics are calculated on the development set. For  SQuAD-Open, since the task is in open-domain, we calculated the statistics based on top 10 documents  from Document Retriever in DrQA (Chen et al., 2017).", "labels": [], "entities": [{"text": "Document Retriever in DrQA", "start_pos": 315, "end_pos": 341, "type": "DATASET", "confidence": 0.8323285430669785}]}, {"text": " Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7026481330394745}, {"text": "NewsQA", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9311038851737976}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9257701635360718}, {"text": "Mean Average Pre- cision (MAP)", "start_pos": 172, "end_pos": 202, "type": "METRIC", "confidence": 0.96909299492836}, {"text": "accuracy", "start_pos": 394, "end_pos": 402, "type": "METRIC", "confidence": 0.9993834495544434}, {"text": "Acc", "start_pos": 426, "end_pos": 429, "type": "METRIC", "confidence": 0.9899360537528992}, {"text": "weight transfer", "start_pos": 502, "end_pos": 517, "type": "TASK", "confidence": 0.6674003750085831}, {"text": "data modification", "start_pos": 519, "end_pos": 536, "type": "TASK", "confidence": 0.6679061502218246}]}, {"text": " Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9420974254608154}, {"text": "NewsQA", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.9483818411827087}, {"text": "GNR", "start_pos": 163, "end_pos": 166, "type": "DATASET", "confidence": 0.770746111869812}, {"text": "FastQA", "start_pos": 231, "end_pos": 237, "type": "DATASET", "confidence": 0.9461105465888977}]}, {"text": " Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).", "labels": [], "entities": [{"text": "Acc", "start_pos": 525, "end_pos": 528, "type": "METRIC", "confidence": 0.9518673419952393}, {"text": "accuracy", "start_pos": 540, "end_pos": 548, "type": "METRIC", "confidence": 0.9659810066223145}, {"text": "DrQA", "start_pos": 1066, "end_pos": 1070, "type": "DATASET", "confidence": 0.9058924913406372}]}, {"text": " Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).", "labels": [], "entities": []}]}