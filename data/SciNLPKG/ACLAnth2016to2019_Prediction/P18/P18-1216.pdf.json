{"title": [{"text": "Joint Embedding of Words and Labels for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7684275507926941}]}], "abstractContent": [{"text": "Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences.", "labels": [], "entities": []}, {"text": "We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors.", "labels": [], "entities": [{"text": "text classification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.735318660736084}]}, {"text": "We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels.", "labels": [], "entities": []}, {"text": "The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones.", "labels": [], "entities": []}, {"text": "Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences.", "labels": [], "entities": []}, {"text": "Extensive results on the several large text datasets show that the proposed framework out-performs the state-of-the-art methods by a large margin, in terms of both accuracy and speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9992338418960571}]}], "introductionContent": [{"text": "Text classification is a fundamental problem in natural language processing (NLP).", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8362129926681519}, {"text": "natural language processing (NLP)", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.7961778342723846}]}, {"text": "The task is to annotate a given text sequence with one (or multiple) class label(s) describing its textual content.", "labels": [], "entities": []}, {"text": "A key intermediate step is the text representation.", "labels": [], "entities": [{"text": "text representation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8157446980476379}]}, {"text": "Traditional methods represent text with hand-crafted features, such as sparse lexical features (e.g., n-grams) (.", "labels": [], "entities": []}, {"text": "Recently, neural models have been employed to learn text representations, including convolutional neural networks (CNNs) (Kalchbrenner * Corresponding author et al., and recurrent neural networks (RNNs) based on long short-term memory (LSTM).", "labels": [], "entities": []}, {"text": "To further increase the representation flexibility of such models, attention mechanisms) have been introduced as an integral part of models employed for text classification ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.8465517163276672}]}, {"text": "The attention module is trained to capture the dependencies that make significant contributions to the task, regardless of the distance between the elements in the sequence.", "labels": [], "entities": []}, {"text": "It can thus provide complementary information to the distance-aware dependencies modeled by RNN/CNN.", "labels": [], "entities": [{"text": "RNN/CNN", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.6423328121503195}]}, {"text": "The increasing representation power of the attention mechanism comes with increased model complexity.", "labels": [], "entities": []}, {"text": "Alternatively, several recent studies show that the success of deep learning on text classification largely depends on the effectiveness of the word embeddings ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7613359987735748}]}, {"text": "Particularly, quantitatively show that the word-embeddings-based text classification tasks can have the similar level of difficulty regardless of the employed models, using the concept of intrinsic dimension ().", "labels": [], "entities": [{"text": "word-embeddings-based text classification", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.6040691037972769}]}, {"text": "Thus, simple models are preferred.", "labels": [], "entities": []}, {"text": "As the basic building blocks in neural-based NLP, word embeddings capture the similarities/regularities between words ().", "labels": [], "entities": []}, {"text": "This idea has been extended to compute embeddings that capture the semantics of word sequences (e.g., phrases, sentences, paragraphs and documents) ().", "labels": [], "entities": []}, {"text": "These representations are built upon various types of compositions of word vectors, ranging from simple averaging to sophisticated architectures.", "labels": [], "entities": []}, {"text": "Further, they suggest that simple models are efficient and interpretable, and have the poten-tial to outperform sophisticated deep neural models.", "labels": [], "entities": []}, {"text": "It is therefore desirable to leverage the best of both lines of works: learning text representations to capture the dependencies that make significant contributions to the task, while maintaining low computational cost.", "labels": [], "entities": []}, {"text": "For the task of text classification, labels play a central role of the final performance.", "labels": [], "entities": [{"text": "text classification", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8492768108844757}]}, {"text": "A natural question to ask is how we can directly use label information in constructing the text-sequence representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Setup We use 300-dimensional GloVe word embeddings as initialization for word embeddings and label embeddings in our model.", "labels": [], "entities": []}, {"text": "Out-Of-Vocabulary (OOV) words are initialized from a uniform distribution with range [\u22120.01, 0.01].", "labels": [], "entities": []}, {"text": "The final classifier is implemented as an MLP layer followed by a sigmoid or softmax function depending on specific task.", "labels": [], "entities": []}, {"text": "We train our model's parameters with the Adam Optimizer (, with an initial learning rate of 0.001, and a minibatch size of 100.", "labels": [], "entities": []}, {"text": "Dropout regularization () is employed on the final MLP layer, with dropout rate 0.5.: Summary statistics of five datasets, including the number of classes, number of training samples and number of testing samples.", "labels": [], "entities": [{"text": "Dropout regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7105639576911926}]}, {"text": "We test our model on the same five standard benchmark datasets as in ().", "labels": [], "entities": []}, {"text": "The summary statistics of the data are shown in, with content specified below: \u2022 AGNews: Topic classification over four categories of Internet news articles) composed of titles plus description classified into: World, Entertainment, Sports and Business.", "labels": [], "entities": []}, {"text": "\u2022 Yelp Review Full: The dataset is obtained from the Yelp Dataset Challenge in 2015, the task is sentiment classification of polarity star labels ranging from 1 to 5.", "labels": [], "entities": [{"text": "Yelp Review Full", "start_pos": 2, "end_pos": 18, "type": "DATASET", "confidence": 0.7938939730326334}, {"text": "Yelp Dataset Challenge in 2015", "start_pos": 53, "end_pos": 83, "type": "DATASET", "confidence": 0.9657615780830383}]}, {"text": "\u2022 Yelp Review Polarity: The same set of text reviews from Yelp Dataset Challenge in 2015, except that a coarser sentiment definition is considered: 1 and 2 are negative, and 4 and 5 as positive.", "labels": [], "entities": [{"text": "Yelp Review Polarity", "start_pos": 2, "end_pos": 22, "type": "DATASET", "confidence": 0.9142180681228638}, {"text": "Yelp Dataset Challenge", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.9565427501996359}]}, {"text": "\u2022 DBPedia: Ontology classification over fourteen non-overlapping classes picked from DBpedia 2014 (Wikipedia).", "labels": [], "entities": [{"text": "Ontology classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.8246987462043762}, {"text": "DBpedia 2014 (Wikipedia)", "start_pos": 85, "end_pos": 109, "type": "DATASET", "confidence": 0.9002290010452271}]}], "tableCaptions": [{"text": " Table 2: Summary statistics of five datasets, in- cluding the number of classes, number of training  samples and number of testing samples.", "labels": [], "entities": []}, {"text": " Table 3: Test Accuracy on document classification tasks, in percentage. We ran Bi-BloSAN using the  authors' implementation; all other results are directly cited from the respective papers.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9355729222297668}, {"text": "document classification tasks", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.8201500773429871}]}, {"text": " Table 5: Quantitative results for doctor-notes multi-label classification task.", "labels": [], "entities": [{"text": "Quantitative", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.8375795483589172}, {"text": "doctor-notes multi-label classification task", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.6434685662388802}]}]}