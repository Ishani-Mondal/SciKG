{"title": [{"text": "The price of debiasing automatic metrics in natural language evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.99846351146698}]}, {"text": "On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive.", "labels": [], "entities": []}, {"text": "In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone.", "labels": [], "entities": []}, {"text": "In practice, however, we obtain only a 7-13% cost reduction on evaluating summa-rization and open-response question answering systems.", "labels": [], "entities": [{"text": "open-response question answering", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.5890548527240753}]}, {"text": "We then prove that our estimator is optimal: there is no unbi-ased estimator with lower cost.", "labels": [], "entities": []}, {"text": "Our theory further highlights the two fundamental bottlenecks-the automatic metric and the prompt shown to human evaluators-both of which need to be improved to obtain greater cost savings.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, there has been an increasing interest in tasks that require generating natural language, including abstractive summarization, open-response question answering (, image captioning (), and open-domain dialogue ().", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.6183159053325653}, {"text": "open-response question answering", "start_pos": 143, "end_pos": 175, "type": "TASK", "confidence": 0.591385006904602}, {"text": "image captioning", "start_pos": 179, "end_pos": 195, "type": "TASK", "confidence": 0.7013139575719833}]}, {"text": "Unfortunately, the evaluation of these systems remains a thorny issue because of the diversity of possible correct responses.", "labels": [], "entities": []}, {"text": "As the gold standard of performing human evaluation is often too expensive, there has been a large effort develop- * Authors contributed equally.", "labels": [], "entities": []}, {"text": "ing automatic metrics such as BLEU (), ROUGE (), ME-TEOR () and CiDER (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9988166093826294}, {"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9956122636795044}, {"text": "ME-TEOR", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9513079524040222}]}, {"text": "However, these have shown to be biased, correlating poorly with human metrics across different datasets and systems (.", "labels": [], "entities": []}, {"text": "Can we combine automatic metrics and human evaluation to obtain an unbiased estimate at lower cost than human evaluation alone?", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple estimator based on control variates, where we average differences between human judgments and automatic metrics rather than averaging the human judgments alone.", "labels": [], "entities": []}, {"text": "Provided the two are correlated, our estimator will have lower variance and thus reduce cost.", "labels": [], "entities": [{"text": "variance", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9645418524742126}]}, {"text": "We prove that our estimator is optimal in the sense that no unbiased estimator using the same automatic metric can have lower variance.", "labels": [], "entities": []}, {"text": "We also analyze its data efficiency (equivalently, cost savings)-the factor reduction in number of human judgments needed to obtain the same accuracy versus naive human evaluation-and show that it depends solely on two factors: (a) the annotator variance (which is a function of the human evaluation prompt) and (b) the correlation between human judgments and the automatic metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9896367192268372}]}, {"text": "This factorization allows us to calculate typical and best-case data efficiencies and accordingly refine the evaluation prompt or automatic metric.", "labels": [], "entities": []}, {"text": "Finally, we evaluate our estimator on stateof-the-art systems from two tasks, summarization on the CNN/Daily Mail dataset ( and openresponse question answering on the MS MARCOv1.0 dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.9384021997451782}, {"text": "question answering", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.6949748247861862}, {"text": "MS MARCOv1.0 dataset", "start_pos": 167, "end_pos": 187, "type": "DATASET", "confidence": 0.9038281043370565}]}, {"text": "To study our estimators offline, we preemptively collected 10,000 human judgments which cover several Figure 1: (a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but (b) the instance-level correlation plot (where each point is a system prediction) shows that the instancelevel correlation is quite low (\u03c1 = 0.31).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.8004682064056396}]}, {"text": "As a consequence, if we try to locally improve systems to produce better answers ( in (a)), they do not significantly improve ROUGE scores and vice versa ().", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 126, "end_pos": 138, "type": "METRIC", "confidence": 0.9741861522197723}]}, {"text": "1 As predicted by the theory, we find that the data efficiency depends not only on the correlation between the human and automatic metrics, but also on the evaluation prompt.", "labels": [], "entities": []}, {"text": "If the automatic metric had perfect correlation, our data efficiency would be around 3, while if we had noiseless human judgments, our data efficiency would be about 1.5.", "labels": [], "entities": []}, {"text": "In reality, the reduction in cost we obtained was only about 10%, suggesting that improvements in both automatic metric and evaluation prompt are needed.", "labels": [], "entities": [{"text": "automatic metric", "start_pos": 103, "end_pos": 119, "type": "METRIC", "confidence": 0.8171089887619019}]}, {"text": "As one case study in improving the latter, we show that, when compared to a Likert survey, measuring the amount of post-editing needed to fix a generated sentence reduced the annotator variance by three-fold.", "labels": [], "entities": []}], "datasetContent": [{"text": "It is well understood that current automatic metrics tend to correlate poorly with human judgment at the instance-level.", "labels": [], "entities": []}, {"text": "For example,  report correlations less than 0.3 fora large suite of word-based and grammar-based evaluation methods on a generation task.", "labels": [], "entities": []}, {"text": "Similarly, find correlations less than 0.35 for automatic metrics on a dialog generation task in one domain, but find correlations with the same metric dropped significantly to less than 0.16 when used in another domain.", "labels": [], "entities": [{"text": "dialog generation task", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.8232324520746866}]}, {"text": "Still, somewhat surprisingly, several automatic metrics have been found to have high system-level correlations ( . What, then, are the implications of having a low instance-level correlation?", "labels": [], "entities": []}, {"text": "As a case study, consider the task of openresponse question answering: here, a system receives a human-generated question and must generate an answer from some given context, e.g. a document or several webpages.", "labels": [], "entities": [{"text": "openresponse question answering", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6039427121480306}]}, {"text": "We collected the responses of several systems on the MS MARCOv1 dataset and crowdsourced human evaluations of the system output (see Section 4 for details).", "labels": [], "entities": [{"text": "MS MARCOv1 dataset", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.8605549136797587}]}, {"text": "The instance-level correlation) is only \u03c1 = 0.31.", "labels": [], "entities": [{"text": "instance-level correlation", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.6046252548694611}]}, {"text": "A closer look at the instance-level correlation reveals that while ROUGE is able to correctly assign low scores to bad examples (lower left), it is bad at judging good examples and often assigns them low ROUGE scores (lower right)-see for examples.", "labels": [], "entities": []}, {"text": "This observation agrees with a finding reported in  that automatic metrics correlate better with human judgments on bad examples than average or good examples.", "labels": [], "entities": []}, {"text": "Thus, as(a) shows, we can improve low-scoring ROUGE examples without improving their human judgment () and vice versa ().", "labels": [], "entities": []}, {"text": "Indeed, report that summarization systems were optimized for ROUGE during the DUC challenge) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems The Direct Marketing Commission probing B2C Data and Data Bubble.", "labels": [], "entities": [{"text": "summarization", "start_pos": 20, "end_pos": 33, "type": "TASK", "confidence": 0.9781349897384644}, {"text": "DUC challenge", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.8315244019031525}, {"text": "B2C Data", "start_pos": 235, "end_pos": 243, "type": "DATASET", "confidence": 0.7578731179237366}]}, {"text": "Investigating whether they breached rules on the sale of private data.", "labels": [], "entities": []}, {"text": "Chief commissioner described allegations made about firms as 'serious'.", "labels": [], "entities": []}, {"text": "Data obtained by the Mail's marketing commission said it would probe both companies over claims that they had breached the rules on the sale of private data.", "labels": [], "entities": [{"text": "the Mail's marketing commission", "start_pos": 17, "end_pos": 48, "type": "DATASET", "confidence": 0.8910252571105957}]}, {"text": "The FSA said it would probe both companies over claims they had breached the rules on the sale of private data.", "labels": [], "entities": [{"text": "FSA", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9187753200531006}]}, {"text": "(se2seq; 1.00 / 0.72) Examples where system Edit < 0.3 and VecSim < 0.5 (14.5% or 290 of 2000 responses) Death toll rises to more than . Pemba Tamang, , shows no apparent signs of serious injury after rescue.", "labels": [], "entities": [{"text": "VecSim", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.895106315612793}, {"text": "Death toll", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9791231751441956}]}, {"text": "Americans special forces helicopter , including Americans, to safety.: Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree.", "labels": [], "entities": []}, {"text": "On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L.", "labels": [], "entities": [{"text": "MS MARCO task", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.6520380775133768}, {"text": "ROUGE-L", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.9950132966041565}]}, {"text": "On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans.", "labels": [], "entities": [{"text": "CNN/Daily Mail task", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.8888696193695068}, {"text": "VecSim", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8659119009971619}]}, {"text": "had hardly improved on human evaluation.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6821275502443314}]}, {"text": "Hillclimbing on ROUGE can also lead to a system that does worse on human scores, e.g. in machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.706278845667839}]}, {"text": "Conversely, genuine quality improvements might not be reflected in improvements in ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9152085781097412}]}, {"text": "This bias also appears in pool-based evaluation for knowledge base population (.", "labels": [], "entities": []}, {"text": "Thus the problems with automatic metrics clearly motivate the need for human evaluation, but can we still use the automatic metrics somehow to save costs?", "labels": [], "entities": []}, {"text": "We will now formalize the problem of combining human evaluation with an automatic metric.", "labels": [], "entities": []}, {"text": "Let X be a set of inputs (e.g., articles), and let S be the system (e.g. for summarization), which takes x \u2208 X and returns output S(x) (e.g. a summary).", "labels": [], "entities": [{"text": "summarization)", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.914345920085907}]}, {"text": "Let Z = {(x, S(x)) : x \u2208 X } be the set of system predictions.", "labels": [], "entities": []}, {"text": "Let Y (z) be the random variable representing the human judgment according to some evaluation prompt (e.g. grammaticality or correctness), and define f (z) = E[Y (z)] to be the (unknown) human metric corresponding to averaging over an infinite number of human judgments.", "labels": [], "entities": []}, {"text": "Our goal is to estimate the average across all examples: with as few queries to Y as possible.", "labels": [], "entities": []}, {"text": "Let g bean automatic metric (e.g. ROUGE), which maps z to areal number.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9859778881072998}]}, {"text": "We assume evaluating g(z) is free.", "labels": [], "entities": []}, {"text": "The central question is how to use gin conjunction with calls to Y to produce an unbiased estimat\u00ea \u00b5 (that is, E[\u02c6 \u00b5] = \u00b5).", "labels": [], "entities": []}, {"text": "In this section, we will construct a simple estimator based on control variates, and prove that it is minimax optimal.", "labels": [], "entities": []}, {"text": "In order to compare different approaches to evaluating systems, we first collected human judgments for the output of several automatic summarization and open-response question answering systems using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "summarization and open-response question answering", "start_pos": 135, "end_pos": 185, "type": "TASK", "confidence": 0.5802318751811981}, {"text": "Amazon Mechanical Turk", "start_pos": 200, "end_pos": 222, "type": "DATASET", "confidence": 0.9462388753890991}]}, {"text": "Details of instructions provided and quality assurance steps taken are provided in Appendix A of the supplementary material.", "labels": [], "entities": [{"text": "Appendix A of the supplementary material", "start_pos": 83, "end_pos": 123, "type": "DATASET", "confidence": 0.6790320922931036}]}, {"text": "In this section, we'll briefly describe how we collected this data.", "labels": [], "entities": []}, {"text": "Evaluating language quality in automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.8025296330451965}]}, {"text": "In automatic summarization, systems must generate a short (on average two or three sentence) summary of an article: for our study, we chose articles from the CNN/Daily Mail (CDM) dataset () which come paired with reference summaries in the form of story highlights.", "labels": [], "entities": [{"text": "summarization", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8970012664794922}, {"text": "CNN/Daily Mail (CDM) dataset", "start_pos": 158, "end_pos": 186, "type": "DATASET", "confidence": 0.9077067673206329}]}, {"text": "We focus on the language quality of summaries and leave evaluating content selection to future work.", "labels": [], "entities": []}, {"text": "For each summary, we collected human judgments on a scale from 1-3) for fluency, (lack of) redundancy, and overall quality of the summary using guidelines from the DUC summarization challenge).", "labels": [], "entities": [{"text": "DUC summarization challenge", "start_pos": 164, "end_pos": 191, "type": "DATASET", "confidence": 0.8925933837890625}]}, {"text": "As an alternate human metric, we also asked workers to postedit the system's summary to improve its quality, similar to the post-editing step in MT evaluations ().", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.8801371455192566}]}, {"text": "Obtaining judgments costs about $0.15 per summary and this cost rises to about $0.40 per summary for post-editing.", "labels": [], "entities": []}, {"text": "We collected judgments on the summaries generated by the seq2seq and pointer models of, the ml and ml+rl models of, and the reference summaries.", "labels": [], "entities": []}, {"text": "Before presenting the summaries to human annotators, we performed some minimal post-processing: we true-cased and de-tokenized the output of seq2seq and pointer using Stanford CoreNLP () and replaced \"unknown\" tokens in each system with a special symbol ().", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 167, "end_pos": 183, "type": "DATASET", "confidence": 0.8994163572788239}]}, {"text": "Next, we look at evaluating the correctness of system outputs in question answering using the MS MARCO question answering dataset.", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8519460558891296}, {"text": "MS MARCO question answering dataset", "start_pos": 94, "end_pos": 129, "type": "DATASET", "confidence": 0.8818753957748413}]}, {"text": "Here, each system is provided with a question and up to 10 paragraphs of context.", "labels": [], "entities": []}, {"text": "The system generates open-response answers that do not need to be tied to a span in any paragraph.", "labels": [], "entities": []}, {"text": "We first ask annotators to judge if the output is even plausible for the question, and if yes, ask them identify if it is correct according to each context paragraph.", "labels": [], "entities": []}, {"text": "We found that requiring annotators to highlight regions in the text that support their decision substantially improved the quality of the output without increasing costs.", "labels": [], "entities": []}, {"text": "While our goal is to evaluate the correctness of the provided answer, we found that there are often answers which maybe corrector incorrect depending on the context.", "labels": [], "entities": []}, {"text": "For example, the question \"what is a pothole\" is typically understood to refer to a hole in a roadway, but also refers to a geological feature).", "labels": [], "entities": []}, {"text": "This is reflected when annotators mark one context paragraph to support the given answer but mark another to contradict it.", "labels": [], "entities": []}, {"text": "We evaluated systems based on both the average correctness (AvgCorrect) of their answers across all paragraphs as well as whether their answer is correct according to any paragraph (AnyCorrect).", "labels": [], "entities": [{"text": "average correctness (AvgCorrect)", "start_pos": 39, "end_pos": 71, "type": "METRIC", "confidence": 0.7081745386123657}, {"text": "AnyCorrect", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9461846351623535}]}, {"text": "We collected annotations on the systems generated by the fastqa and fastqa ext from and the snet and snet.ens(emble) models from, along with reference answers.", "labels": [], "entities": []}, {"text": "The answers generated by the systems were used without any postprocessing.", "labels": [], "entities": []}, {"text": "Surprisingly, we found that the correctness of the reference answers (according to the AnyCorrect metric) was only 73.5%, only 2% above that of the leading system (snet.ens).", "labels": [], "entities": []}, {"text": "We manually inspected 30 reference answers which were annotated incorrectly and found that of those, about 95% were indeed incorrect.", "labels": [], "entities": []}, {"text": "However, 62% are actually answerable from some paragraph, indicating that the real ceiling performance on this dataset is around 90% and that there is still room for improvement on this task.", "labels": [], "entities": []}, {"text": "We are now ready to evaluate the performance of our control variates estimator proposed in Section 3 using the datasets presented in Section 4.", "labels": [], "entities": []}, {"text": "specify which passage they used to generate the answer.", "labels": [], "entities": []}, {"text": "Recall that our primary quantity of interest is data efficiency, the ratio of the number of human judgments required to estimate the overall human evaluation score for the control variates estimator versus the sample mean.", "labels": [], "entities": []}, {"text": "We'll briefly review the automatic metrics used in our evaluation before analyzing the results.", "labels": [], "entities": []}, {"text": "We consider the following frequently used automatic word-overlap based metrics in our work: BLEU (), ROUGE () and ME-TEOR (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9989610910415649}, {"text": "ROUGE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9967442750930786}, {"text": "ME-TEOR", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9243513941764832}]}, {"text": "Following  and, we also compared a vector-based sentence-similarity using sent2vec () to compare sentences (VecSim).", "labels": [], "entities": []}, {"text": "shows how each of these metrics is correlated with human judgment for the systems being evaluated.", "labels": [], "entities": []}, {"text": "Unsurprisingly, the correlation varies considerably across systems, with token-based metrics correlating more strongly for systems that are more extractive in nature (fastqa and fastqa ext).", "labels": [], "entities": []}, {"text": "In Section 3 we proved that the control variates estimator is not only unbiased but also has the least variance among other unbiased estimators.", "labels": [], "entities": []}, {"text": "plots the width of the 80% confidence interval, estimated using bootstrap, measured as a function of the number of samples collected for different tasks and prompts.", "labels": [], "entities": [{"text": "width", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9723849892616272}]}, {"text": "As expected, the control variates estimator reduces the width of the confidence interval.", "labels": [], "entities": [{"text": "width", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9868441820144653}]}, {"text": "We measure data efficiency by the averaging of the ratio of squared confidence intervals between the human baseline Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics. and control variates estimates.", "labels": [], "entities": []}, {"text": "We observe that the data efficiency depends on the task, prompt and system, ranging from about 1.08 (a 7% cost reduction) to 1.15 (a 13% cost reduction) using current automatic metrics.", "labels": [], "entities": []}, {"text": "As we showed in Section 3, further gains are fundamentally limited by the quality of the evaluation prompts and automatic metrics.", "labels": [], "entities": []}, {"text": "show how improving the quality of the evaluation prompt from a Likert-scale prompt for quality (Overall) to using post-editing (Edit) noticeably decreases variance and hence allows better automatic metrics to increase data efficiency.", "labels": [], "entities": [{"text": "variance", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9805166125297546}]}, {"text": "Likewise, shows how using a better automatic metric (ROUGE-L instead of VecSim) also reduces variance.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9831767082214355}, {"text": "VecSim", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8785424828529358}, {"text": "variance", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9721395969390869}]}, {"text": "also shows the conjectured confidence intervals if we were able to eliminate noise inhuman judgments (noiseless humans) or have a automatic metric that correlated perfectly with average human judgment (perfect metric).", "labels": [], "entities": []}, {"text": "In particular, we use the mean of all (2-3) humans on each z for the perfect g(z) and use the mean of all humans on each z for the \"noiseless\" Y (z).", "labels": [], "entities": []}, {"text": "In both cases, we are able to significantly increase data efficiency (i.e. decrease estimator variance).", "labels": [], "entities": [{"text": "estimator variance", "start_pos": 84, "end_pos": 102, "type": "METRIC", "confidence": 0.946976512670517}]}, {"text": "With zero annotator variance and using existing automatic metrics, the data efficiency ranges from 1.42 to 1.69.", "labels": [], "entities": []}, {"text": "With automatic metrics with perfect correlation and current variance of human judgments, it ranges from 2.38 to 7.25.", "labels": [], "entities": []}, {"text": "Thus, we conclude that it is important not only to improve our automatic metrics but also the evaluation prompts we use during human evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: A summary of the key statistics, human  metric variance (\u03c3 2  f ) and annotator variance (\u03c3 2  a )  for different datasets, CNN/Daily Mail (CDM)  and MS MARCO in our evaluation benchmark.  We observe that the relative variance (\u03b3) is fairly  high for most evaluation prompts, upper bounding  the data efficiency on these tasks. A notable ex- ception is the Edit prompt wherein systems are  compared on the number of post-edits required to  improve their quality.", "labels": [], "entities": [{"text": "annotator variance (\u03c3 2  a )", "start_pos": 80, "end_pos": 108, "type": "METRIC", "confidence": 0.9322166357721601}, {"text": "CNN/Daily Mail (CDM)", "start_pos": 134, "end_pos": 154, "type": "DATASET", "confidence": 0.9369955233165196}, {"text": "MS", "start_pos": 160, "end_pos": 162, "type": "DATASET", "confidence": 0.8961427807807922}, {"text": "MARCO", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.42784738540649414}, {"text": "relative variance (\u03b3)", "start_pos": 219, "end_pos": 240, "type": "METRIC", "confidence": 0.8760377645492554}]}]}