{"title": [{"text": "SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines", "labels": [], "entities": []}], "abstractContent": [{"text": "Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances.", "labels": [], "entities": [{"text": "encoding natural language utterances", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.7593315243721008}]}, {"text": "In this paper we present SoPa, anew model that aims to bridge these two approaches.", "labels": [], "entities": []}, {"text": "SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns.", "labels": [], "entities": [{"text": "neural representation learning", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.724889894326528}]}, {"text": "We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly , to a restricted form of WFSA.", "labels": [], "entities": [{"text": "WFSA", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.9471401572227478}]}, {"text": "Empirically , on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7768928507963816}]}], "introductionContent": [{"text": "Recurrent neural networks and convolutional neural networks (CNNs;) are two of the most useful text representation learners in NLP.", "labels": [], "entities": [{"text": "text representation learners", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.7881934444109598}]}, {"text": "These methods are generally considered to be quite different: the former encodes an arbitrarily long sequence of text, and is highly expressive.", "labels": [], "entities": []}, {"text": "The latter is more local, encoding fixed length windows, and accordingly less expressive.", "labels": [], "entities": []}, {"text": "In this paper, we seek to bridge the gap between RNNs and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them.", "labels": [], "entities": []}, {"text": "SoPa is a neural version of a weighted finitestate automaton (WFSA), with a restricted set of transitions.", "labels": [], "entities": []}, {"text": "Linguistically, SoPa is appealing as it \u21e4 The first two authors contributed equally.", "labels": [], "entities": []}, {"text": "Self-loops allow for repeatedly inserting words (e.g., \"funny\").", "labels": [], "entities": []}, {"text": "\u270f-transitions allow for dropping words (e.g., \"a\"). is able to capture a soft notion of surface patterns (e.g., \"what a great X !\"; Hearst, 1992), where some words maybe dropped, inserted, or replaced with similar words (see).", "labels": [], "entities": []}, {"text": "From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms) that SoPa can take advantage of.", "labels": [], "entities": []}, {"text": "SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3).", "labels": [], "entities": [{"text": "WFSA", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8311971426010132}]}, {"text": "While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end.", "labels": [], "entities": []}, {"text": "SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document.", "labels": [], "entities": []}, {"text": "Because SoPa defines a hidden state that depends on the input token and the previous state, it can bethought of as a simple type of RNN.", "labels": [], "entities": []}, {"text": "We show that SoPa is an extension of a onelayer CNN (Section 4).", "labels": [], "entities": []}, {"text": "Accordingly, one-layer CNNs can be viewed as a collection of linearchain WFSAs, each of which can only match fixed-length spans, while our extension allows matches of flexible-length.", "labels": [], "entities": []}, {"text": "As a simple type of RNN that is more expressive than a CNN, SoPa helps to link CNNs and RNNs.", "labels": [], "entities": []}, {"text": "To test the utility of SoPa, we experiment with three text classification tasks (Section 5).", "labels": [], "entities": [{"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.6931089460849762}]}, {"text": "We compare against four baselines, including both a bidirectional LSTM and a CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9329224228858948}]}, {"text": "Our model performs on par with or better than all baselines on all tasks (Section 6).", "labels": [], "entities": []}, {"text": "Moreover, when training with smaller datasets, SoPa is particularly useful, outperforming all models by substantial margins.", "labels": [], "entities": []}, {"text": "Finally, building on the connections discovered in this paper, we offer anew, simple method to interpret SoPa (Section 7).", "labels": [], "entities": [{"text": "interpret SoPa", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.6968519985675812}]}, {"text": "This method applies equally well to CNNs.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.9274876117706299}]}, {"text": "We release our code at https://github.com/ Noahs-ARK/soft_patterns.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate SoPa, we apply it to text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.8352569540341696}]}, {"text": "Below we describe our datasets and baselines.", "labels": [], "entities": []}, {"text": "More details can be found in Appendix A. Datasets.", "labels": [], "entities": [{"text": "Appendix A. Datasets", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.8525649706522623}]}, {"text": "We experiment with three binary classification datasets.", "labels": [], "entities": []}, {"text": "The Stanford Sentiment Treebank (Socher et al., 2013) 12 contains roughly 10K movie reviews from Rotten Tomatoes, 13 labeled on a scale of 1-5.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (Socher et al., 2013) 12", "start_pos": 4, "end_pos": 56, "type": "DATASET", "confidence": 0.9164923971349542}]}, {"text": "We consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s).", "labels": [], "entities": []}, {"text": "It is worth noting that this dataset also contains syntactic phrase level annotations, providing a sentiment label to parts of sentences.", "labels": [], "entities": []}, {"text": "In order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time.", "labels": [], "entities": []}, {"text": "The number of training/development/test sentences in the dataset is 6,920/872/1,821.", "labels": [], "entities": []}, {"text": "The Amazon Review Corpus ( contains electronics product reviews, a subset of a larger review dataset.", "labels": [], "entities": [{"text": "Amazon Review Corpus", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9771586457888285}]}, {"text": "Each document in the dataset contains a review and a summary.", "labels": [], "entities": []}, {"text": "Following Yogatama et al., we only use the reviews part, focusing on positive and negative reviews.", "labels": [], "entities": []}, {"text": "The number of training/development/test samples is 20K/5K/25K.", "labels": [], "entities": []}, {"text": "The ROC story cloze task () is a story understanding task.", "labels": [], "entities": [{"text": "ROC story cloze task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.503276027739048}, {"text": "story understanding task", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.781852106253306}]}, {"text": "The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent.", "labels": [], "entities": []}, {"text": "Following, we treat it as a style detection task: we treat all \"right\" endings as positive samples and all \"wrong\" ones as negative, and we ignore the story prefix.", "labels": [], "entities": [{"text": "style detection task", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7552991509437561}]}, {"text": "We split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences).", "labels": [], "entities": []}, {"text": "In order to test our model's ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances.", "labels": [], "entities": []}, {"text": "Development and test sets remain the same.", "labels": [], "entities": []}, {"text": "We compare to four baselines: a BiL-STM, a one-layer CNN, DAN (a simple alternative to RNNs) and a feature-based classifier trained with hard-pattern features.", "labels": [], "entities": []}, {"text": "Bidirectional LSTMs have been successfully used in the past for text classification tasks (.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.8703956802686056}]}, {"text": "We learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP.", "labels": [], "entities": []}, {"text": "CNNs are particularly useful for text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8657995164394379}]}, {"text": "We train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP.", "labels": [], "entities": []}, {"text": "We learn a deep averaging network with word dropout), a simple but strong text-classification baseline.", "labels": [], "entities": []}, {"text": "We train a logistic regression classifier with hard-pattern features.", "labels": [], "entities": []}, {"text": "Following , we replace low frequency words with a special wildcard symbol.", "labels": [], "entities": []}, {"text": "We learn sequences of 1-6 concrete words, where any number of wildcards can come between two adjacent words.", "labels": [], "entities": []}, {"text": "We consider words occurring with frequency of at least 0.01% of our training set as concrete words, and words occurring in frequency 1% or less as wildcards.", "labels": [], "entities": []}, {"text": "SoPa requires specifying the number of patterns to be learned, and their lengths.", "labels": [], "entities": []}, {"text": "Preliminary experiments showed that the model doesn't benefit from more than a few dozen patterns.", "labels": [], "entities": []}, {"text": "We experiment with several configurations of patterns of different lengths, generally considering 0, 10 or 20 patterns of each pattern length between 2-7.", "labels": [], "entities": []}, {"text": "The total number of patterns learned ranges between 30-70.", "labels": [], "entities": []}, {"text": "shows the number of parameters used by each model for each task.", "labels": [], "entities": []}, {"text": "Given enough data, models with more parameters should be expected to perform better.", "labels": [], "entities": []}, {"text": "However, SoPa performs better or roughly the same as a BiLSTM, which has 3-6 times as many parameters.", "labels": [], "entities": []}, {"text": "shows a comparison of all models on the SST and Amazon datasets with varying training set sizes.", "labels": [], "entities": [{"text": "SST and Amazon datasets", "start_pos": 40, "end_pos": 63, "type": "DATASET", "confidence": 0.7192234918475151}]}, {"text": "SoPa is substantially outperforming all baselines, in particular BiLSTM, on small datasets (100 samples).", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.6361775994300842}]}, {"text": "This suggests that SoPa is better fit to learn from small datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test classification accuracy (and the  number of parameters used). The bottom part  shows our ablation results: SoPa: our full model.  SoPa ms 1 : running with max-sum semiring (rather  than max-product), with the identity function as  our encoder E (see Equation 3). sl: self-loops,  \u270f: \u270f transitions. The final row is equivalent to a  one-layer CNN.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9520710706710815}]}]}