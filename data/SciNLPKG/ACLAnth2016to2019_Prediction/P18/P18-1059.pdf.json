{"title": [{"text": "Inherent Biases in Reference-based Evaluation for Grammatical Error Correction and Text Simplification", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7133965194225311}]}], "abstractContent": [{"text": "The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (hence-forth, low coverage bias or LCB).", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7249704450368881}, {"text": "LCB", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.6475814580917358}]}, {"text": "This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions.", "labels": [], "entities": [{"text": "LCB in Grammatical Error Correction (GEC) evaluation", "start_pos": 33, "end_pos": 85, "type": "TASK", "confidence": 0.6068085134029388}]}, {"text": "This is due to the long-tailed distribution of valid corrections fora sentence.", "labels": [], "entities": []}, {"text": "Concretely, we show that LCB in-centivizes GEC systems to avoid correcting even when they can generate a valid correction.", "labels": [], "entities": []}, {"text": "Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input.", "labels": [], "entities": []}, {"text": "Similar effects on Text Simplification further support our claims.", "labels": [], "entities": [{"text": "Text Simplification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7010769546031952}]}], "introductionContent": [{"text": "Evaluation in monolingual translation () and in particular in GEC) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (.", "labels": [], "entities": [{"text": "monolingual translation", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.7625623047351837}, {"text": "GEC", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7630842924118042}]}, {"text": "Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU ( ) and I-measure.", "labels": [], "entities": [{"text": "GEC", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.5887812972068787}, {"text": "GLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9962852001190186}]}, {"text": "The Low Coverage Bias (LCB) was previously discussed by, who showed that inter-annotator agreement in producing references is low, and concluded that RBMs underestimate the performance of GEC systems.", "labels": [], "entities": [{"text": "Low Coverage Bias (LCB)", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.8272538085778555}]}, {"text": "To address this, they proposed anew measure, Ratio Scoring, which re-scales M 2 by the interannotator agreement (i.e., the score of a human corrector), interpreted as an upper bound.", "labels": [], "entities": [{"text": "Ratio Scoring", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.6176390647888184}]}, {"text": "We claim that the LCB has more far-reaching implications than previously discussed.", "labels": [], "entities": []}, {"text": "First, while we agree with that a human correction should receive a perfect score, we show that LCB does not merely scale system performance by a constant factor, but rather that some correction policies are less prone to be biased against.", "labels": [], "entities": []}, {"text": "Concretely, we show that by only correcting closed class errors, where few possible corrections are valid, systems can outperform humans.", "labels": [], "entities": []}, {"text": "Indeed, in Section 2.3 we show that some existing systems outperform humans on M 2 and GLEU, while only applying few changes to the source.", "labels": [], "entities": [{"text": "M 2", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8233534097671509}, {"text": "GLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7403011322021484}]}, {"text": "We thus argue that the development of GEC systems against low coverage RBMs disincentivizes systems from making changes to the source in cases where there are plentiful valid corrections (open class errors), as necessarily only some of them are covered by the reference set.", "labels": [], "entities": []}, {"text": "To support our claim we show that (1) existing GEC systems under-correct, often performing an order of magnitude less corrections than a human does ( \u00a73.2); (2) increasing the number of references alleviates under-correction ( \u00a73.3); and (3) under-correction is more pronounced in error types that are more varied in their valid corrections ( \u00a73.4).", "labels": [], "entities": []}, {"text": "A different approach for addressing LCB was taken by, who propose to increase the number of references (henceforth, M ).", "labels": [], "entities": [{"text": "addressing LCB", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7760483026504517}]}, {"text": "In Section 2 we estimate the distribution of corrections per sentence, and find that increasing M is unlikely to overcome LCB, due to the vast number of valid corrections fora sentence and their long-tailed distribution.", "labels": [], "entities": [{"text": "LCB", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.9943706393241882}]}, {"text": "Indeed, even short sentences have over 1000 valid corrections on average.", "labels": [], "entities": []}, {"text": "Empirically assessing the effect of increasing Mon the bias, we find diminishing returns using three standard GEC measures (M 2 , accuracy and GLEU), underscoring the difficulty in this approach.", "labels": [], "entities": [{"text": "M 2", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.8847239315509796}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9948073029518127}, {"text": "GLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9983037710189819}]}, {"text": "Similar trends are found when conducting such experiments to Text Simplification (TS) ( \u00a74).", "labels": [], "entities": [{"text": "Text Simplification (TS)", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.8597816824913025}]}, {"text": "Specifically we show that (1) the distribution of valid simplifications fora given sentence is longtailed; (2) common measures for TS dramatically under-estimate performance; (3) additional references alleviate this under-prediction.", "labels": [], "entities": [{"text": "TS", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.8777198195457458}]}, {"text": "To recap, we find that the LCB hinders the reliability of RBMs for GEC, and incentivizes systems developed to optimize these measures not to correct.", "labels": [], "entities": [{"text": "GEC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8420742154121399}]}, {"text": "LCB cannot be overcome by re-scaling or increasing Min any feasible range.", "labels": [], "entities": [{"text": "LCB", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7586831450462341}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Estimating the distribution of corrections Dx. The  table presents the mean number of corrections per sentence  with probability more than \u03b3 (top row), as well as their total  probability mass (bottom row).", "labels": [], "entities": []}, {"text": " Table 4: Estimating the distribution of simplifications Dx.  The table presents the mean number of simplifications per  sentence with probability more than \u03b3 (top row), as well as  their total probability mass (bottom row).", "labels": [], "entities": []}]}