{"title": [{"text": "A Language Model based Evaluator for Sentence Compression", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9102815091609955}]}], "abstractContent": [{"text": "We herein present a language-model-based evaluator for deletion-based sentence compression, and viewed this task as a series of deletion-and-evaluation operations using the evaluator.", "labels": [], "entities": [{"text": "deletion-based sentence compression", "start_pos": 55, "end_pos": 90, "type": "TASK", "confidence": 0.6316295564174652}]}, {"text": "More specifically , the evaluator is a syntactic neural language model that is first built by learning the syntactic and structural collocation among words.", "labels": [], "entities": []}, {"text": "Subsequently, a series of trial-and-error deletion operations are conducted on the source sentences via a reinforcement learning framework to obtain the best target compression.", "labels": [], "entities": []}, {"text": "An empirical study shows that the proposed model can effectively generate more readable compression , comparable or superior to several strong baselines.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a 200-sentence test set fora large-scale dataset, setting anew baseline for the future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence.", "labels": [], "entities": [{"text": "Deletion-based sentence compression", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6823656260967255}]}, {"text": "Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression.", "labels": [], "entities": []}, {"text": "For example, proposed a syntactictree-based method that considers the sentence compression task as an optimization problem by using integer linear programming, whereas) viewed the sentence compression task as a sequence labeling problem using the recurrent neural network (RNN), using maximum likelihood as the objective function for optimization.", "labels": [], "entities": [{"text": "sentence compression task", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7889005045096079}, {"text": "sentence compression task", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.7887885173161825}]}, {"text": "The latter sets a relatively strong baseline by training the model on a large-scale parallel corpus.", "labels": [], "entities": []}, {"text": "Although an RNN (e.g., Long short-term memory networks) can implicitly model syntactic information, it still produces ungrammatical sentences.", "labels": [], "entities": []}, {"text": "We argue that this is because (i) the labels (or compressions) are automatically yielded by employing the syntactic-tree-pruning method.", "labels": [], "entities": []}, {"text": "It thus contains some errors caused by syntactic tree parsing error, (ii) more importantly, the optimization objective of an RNN is the likelihood function that is based on individual words instead of readability (or informativeness) of the whole compressed sentence.", "labels": [], "entities": [{"text": "syntactic tree parsing", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7139358719189962}]}, {"text": "A gap exists between optimization objective and evaluation.", "labels": [], "entities": []}, {"text": "As such, we are of great interest that: (i) can we take the readability of the whole compressed sentence as a learning objective and (ii) can grammar errors be recovered through a language-model-based evaluator to yield compression with better quality?", "labels": [], "entities": []}, {"text": "To answer the above questions, a syntax-based neural language model is trained on large-scale datasets as a readability evaluator.", "labels": [], "entities": []}, {"text": "The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics.", "labels": [], "entities": []}, {"text": "Subsequently, we formulate the deletionbased sentence compression as a series of trialand-error deletion operations through a reinforcement learning framework.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7375351786613464}]}, {"text": "The policy network performs either RETAIN or REMOVE action to form a compression, and receives a reward (e.g., readability score) to update the network.", "labels": [], "entities": [{"text": "RETAIN", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9441817402839661}, {"text": "REMOVE", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.8870444297790527}]}, {"text": "The empirical study shows that the proposed method can produce more readable sentences that preserve the source sentences, comparable or superior to several strong baselines.", "labels": [], "entities": []}, {"text": "In short, our contributions are two-fold: (i) an effective syntaxbased evaluator is built as a post-hoc checker, yielding compression with better quality based upon the evaluation metrics; (ii) a large scale news dataset with 1.02 million sentence compression pairs are compiled for this task in addition to 200 manually created sentences.", "labels": [], "entities": []}, {"text": "We made it publicly available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F 1 and RASP-F 1 results for Gigaword dataset.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9781743884086609}, {"text": "RASP-F 1", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6546996831893921}, {"text": "Gigaword dataset", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9398051500320435}]}, {"text": " Table 2: Human Evaluation for Gigaword dataset.   \u2020stands for significant difference with 0.95 confi- dence in the column.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.7908500134944916}]}, {"text": " Table 3: F 1 and RASP-F 1 results for Google  dataset.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9812441766262054}, {"text": "RASP-F 1", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6555959582328796}, {"text": "Google  dataset", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9658908247947693}]}]}