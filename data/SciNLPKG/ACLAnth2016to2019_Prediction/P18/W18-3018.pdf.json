{"title": [{"text": "Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation", "labels": [], "entities": [{"text": "Injecting Lexical Contrast", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8199067711830139}]}], "abstractContent": [{"text": "Word vector space specialisation models offer a portable, lightweight approach to fine-tuning arbitrary distributional vector spaces to discern between synonymy and antonymy.", "labels": [], "entities": [{"text": "Word vector space specialisation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5564064159989357}]}, {"text": "Their effectiveness is drawn from external linguistic constraints that specify the exact lexical relation between words.", "labels": [], "entities": []}, {"text": "In this work, we show that a careful selection of the external constraints can steer and improve the specialisation.", "labels": [], "entities": []}, {"text": "By simply selecting appropriate constraints, we report state-of-the-art results on a suite of tasks with well-defined benchmarks where modeling lexical contrast is crucial: 1) true semantic similarity, with highest reported scores on SimLex-999 and SimVerb-3500 to date; 2) detecting antonyms; and 3) distinguishing antonyms from synonyms.", "labels": [], "entities": [{"text": "detecting antonyms", "start_pos": 274, "end_pos": 292, "type": "TASK", "confidence": 0.8429970443248749}]}], "introductionContent": [{"text": "Representation models grounded in the distributional hypothesis generally fail to distinguish highly contrasting words (antonyms) from highly similar ones (synonyms), due to similar word co-occurrence signatures in text corpora.", "labels": [], "entities": []}, {"text": "In addition to antonymy and synonymy being fundamental lexical relations that are central to the organisation of the mental lexicon, this undesirable property of distributional word vector spaces has grave implications on their application in NLP reasoning and understanding tasks.", "labels": [], "entities": [{"text": "NLP reasoning", "start_pos": 243, "end_pos": 256, "type": "TASK", "confidence": 0.9293862581253052}]}, {"text": "As shown in prior work ( Take a mini-batch of ATTRACT and REPEL pairs...", "labels": [], "entities": [{"text": "ATTRACT", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.6926581859588623}, {"text": "REPEL", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.8738109469413757}]}, {"text": "For each pair, find two pseudo-negative examples...", "labels": [], "entities": []}, {"text": "...and fine-tune the vectors so that ATTRACT pairs are closest...", "labels": [], "entities": [{"text": "ATTRACT", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9028511047363281}]}, {"text": ": An illustration of specialisation for lexical contrast with toy examples.", "labels": [], "entities": []}, {"text": "The specialisation model operates with two sets of external linguistic constraints: 1) ATTRACT word pairs, which have to be as close as possible in the fine-tuned vector space (e.g., irritating and annoying); and 2) REPEL word pairs, which have to be as faraway from each other as possible (e.g., expensive and inexpensive).", "labels": [], "entities": [{"text": "ATTRACT", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9585655927658081}]}, {"text": "In this work, we investigate how different constraints affect specialisation.", "labels": [], "entities": []}, {"text": "We show that a careful selection of external constraints can guide specialisation models to emphasise lexical contrast in the fine-tuned vector space: e.g., we indicate that direct (i.e., 1-step) WordNet hypernymyhyponymy pairs are useful for boosting lexical contrast.", "labels": [], "entities": []}, {"text": "Our specialised word vector spaces yield stateof-the-art results on a range of tasks where modeling lexical contrast is crucial: 1) true semantic similarity; 2) antonymy detection; and 3) distinguishing antonyms from synonyms.", "labels": [], "entities": [{"text": "antonymy detection", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7218214273452759}]}, {"text": "Our SimLex-999 (  and) scores are the highest reported results on these datasets to date: the result on SimLex-999 is the first result on the dataset surpassing the ceiling of mean inter-annotator agreement.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train the state-of-the-art specialisation model of Mrk\u0161i\u00b4cMrk\u0161i\u00b4c et al.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4cMrk\u0161i\u00b4c et al", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.9285629590352377}]}, {"text": "(2017) using suggested settings: 4 Adagrad () is used for stochastic optimisation, batch size is 50, and we train for 15 epochs.", "labels": [], "entities": []}, {"text": "To emphasise lexical contrast in the specialised space we set the respective ATTRACT and REPEL margins \u03b4 att and \u03b4 rpl to the same value: 1.0.", "labels": [], "entities": [{"text": "ATTRACT", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9706158638000488}, {"text": "REPEL margins \u03b4 att and \u03b4 rpl", "start_pos": 89, "end_pos": 118, "type": "METRIC", "confidence": 0.8481675556727818}]}, {"text": "We use large 300-dim skip gram vectors with bag-of-words contexts and negative sampling (SGNS-GN) (, pre-trained on the 100B Google News corpus.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 125, "end_pos": 143, "type": "DATASET", "confidence": 0.8235853711764017}]}, {"text": "As all other components of the model are kept fixed, the difference in performance can be attributed to the difference in the constraints used.", "labels": [], "entities": []}, {"text": "We experiment with external constraints employed in prior work (: these were extracted from WordNet) and the Roget thesaurus, and comprise 1,023,082 synonymy (syn) pairs and 380,873 ant pairs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9733707904815674}]}, {"text": "The expanded antexp set of antonyms contains a total of 10,334,811 word pairs.", "labels": [], "entities": []}, {"text": "Finally, the hyp1 set extracted from WordNet contains 326,187 word pairs.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.781225860118866}]}, {"text": "We evaluate all specialised spaces in three standard tasks with well-defined benchmarks where modeling lexical contrast is beneficial: 1) semantic similarity, 2) antonymy detection, and 3) distinguishing antonyms from synonyms.", "labels": [], "entities": [{"text": "antonymy detection", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7053490579128265}]}, {"text": "For each task, we compare against a representative selection of baselines, currently holding peak scores on the respective benchmarks.", "labels": [], "entities": []}, {"text": "Due to a large space of models in our comparison, we refer the interested reader to the original papers for their full descriptions.", "labels": [], "entities": []}, {"text": "Task 1: Word Similarity.", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.751943051815033}]}, {"text": "We evaluate all models on the SimLex-999 dataset ( , and SimVerb-3500 (), a recent verb pair similarity dataset with 3,500 verb pairs.", "labels": [], "entities": [{"text": "SimLex-999 dataset", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.913437008857727}]}, {"text": "The evaluation metric is Spearman's \u03c1 rank correlation.", "labels": [], "entities": [{"text": "Spearman's \u03c1 rank correlation", "start_pos": 25, "end_pos": 54, "type": "METRIC", "confidence": 0.5852216362953186}]}, {"text": "Task 2: Antonymy Detection.", "labels": [], "entities": [{"text": "Antonymy Detection", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.9629305899143219}]}, {"text": "For this task, we rely on the widely used Graduate Record Examination (GRE) dataset.", "labels": [], "entities": [{"text": "Graduate Record Examination (GRE) dataset", "start_pos": 42, "end_pos": 83, "type": "DATASET", "confidence": 0.9067243167332241}]}, {"text": "The task, given an input cue word, is to select the best antonym from five options.", "labels": [], "entities": []}, {"text": "Given a word vector space, we take the word with the largest cosine distance to the cue as the best antonym.", "labels": [], "entities": []}, {"text": "The GRE dataset contains 950 questions in total.", "labels": [], "entities": [{"text": "GRE dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8567656874656677}]}, {"text": "We report balanced F 1 scores on the entire dataset.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9885096549987793}]}, {"text": "Task 3: Synonymy vs. Antonymy.", "labels": [], "entities": []}, {"text": "In this binary classification task, the system must decide whether the relation between two words is synonymy or antonymy.", "labels": [], "entities": []}, {"text": "We use the recent dataset of 0.563 0.328 Non-distributional  0.578 0.596 Joint Specialisation ( 0.590 0.516 Paragram-SL999 ( 0.690 0.540 Counter-fitting 0", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Task 1. Results on two word similarity  benchmarks (Spearman's \u03c1). Best-scoring baseline  models from the literature are reported. The dashed  line separates purely distributional models from the  ones leveraging external lexical knowledge.", "labels": [], "entities": []}, {"text": " Table 3: Task 2. Results (F 1 scores) on the full  GRE multiple-choice antonymy detection dataset.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9681385358174642}, {"text": "GRE multiple-choice antonymy detection", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.6207420155405998}]}, {"text": " Table 4: Task 3. Results (F 1 ) on the synonymy-vs- antonymy evaluation set (", "labels": [], "entities": []}]}