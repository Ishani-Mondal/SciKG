{"title": [{"text": "Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention", "labels": [], "entities": [{"text": "Document Embedding Enhanced Event Detection", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8427198052406311}]}], "abstractContent": [{"text": "Document-level information is very important for event detection even at sentence level.", "labels": [], "entities": [{"text": "event detection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7867855131626129}]}, {"text": "In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences.", "labels": [], "entities": [{"text": "DEEB-RNN", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8481480479240417}]}, {"text": "This model first learns event detection oriented embed-dings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.", "labels": [], "entities": []}, {"text": "It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences.", "labels": [], "entities": []}, {"text": "Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.", "labels": [], "entities": [{"text": "ACE-2005 dataset", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.987953394651413}, {"text": "DEEB-RNN", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.5994120836257935}]}], "introductionContent": [{"text": "Event Detection (ED) is an important subtask of event extraction.", "labels": [], "entities": [{"text": "Event Detection (ED)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8074478328227996}, {"text": "event extraction", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.7598637640476227}]}, {"text": "It extracts event triggers from individual sentences and further identifies the type of the corresponding events.", "labels": [], "entities": []}, {"text": "For instance, according to the ACE-2005 annotation guideline, in the sentence \"Jane and John are married\", an ED system should be able to identify the word \"married\" as a trigger of the event \"Marry\".", "labels": [], "entities": [{"text": "ACE-2005 annotation guideline", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.8992376526196798}]}, {"text": "However, it maybe difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts.", "labels": [], "entities": []}, {"text": "Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g.,) and representation-based methods (e.g.,;).", "labels": [], "entities": []}, {"text": "The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information.", "labels": [], "entities": []}, {"text": "In general, most of these existing methods mainly exploit sentence-level contextual information.", "labels": [], "entities": []}, {"text": "However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document.", "labels": [], "entities": [{"text": "ED", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9773164391517639}]}, {"text": "For example, there are the following sentences in ...", "labels": [], "entities": []}, {"text": "I knew it was time to leave.", "labels": [], "entities": []}, {"text": "Isn't that a great argument for term limits?", "labels": [], "entities": [{"text": "term limits", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.734206810593605}]}, {"text": "If we only examine the first sentence, it is hard to determine whether the trigger \"leave\" indicates a \"Transport\" event meaning that he wants to leave the current place, or an \"End-Position\" event indicating that he will stop working for his current organization.", "labels": [], "entities": []}, {"text": "However, if we can capture the contextual information of this sentence, it is more confident for us to label \"leave\" as the trigger of an \"End-Position\" event.", "labels": [], "entities": []}, {"text": "Upon such observation, there have been some feature-based studies) that construct rules to capture document-level information for improving sentence-level ED.", "labels": [], "entities": []}, {"text": "However, they suffer from two major limitations.", "labels": [], "entities": []}, {"text": "First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible.", "labels": [], "entities": []}, {"text": "Besides, a representation-based study has been presented in (, which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier.", "labels": [], "entities": []}, {"text": "However, as being limited by the unsupervised training In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level.", "labels": [], "entities": [{"text": "DEEB-RNN", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.765916109085083}, {"text": "ED", "start_pos": 152, "end_pos": 154, "type": "TASK", "confidence": 0.9344191551208496}]}, {"text": "This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.", "labels": [], "entities": []}, {"text": "It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences.", "labels": [], "entities": []}, {"text": "This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated.", "labels": [], "entities": [{"text": "event type identification", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.6523464123407999}]}, {"text": "It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation ( , question answering (, document summarization (, etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7878841161727905}, {"text": "question answering", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.793427050113678}, {"text": "document summarization", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7082207202911377}]}, {"text": "We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance.", "labels": [], "entities": [{"text": "DEEB-RNN", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.7416266202926636}, {"text": "benchmark dataset, ACE-2005", "start_pos": 48, "end_pos": 75, "type": "DATASET", "confidence": 0.8207119554281235}]}, {"text": "Experimental results show that the DEEB-RNN model outperforms both feature-based and representation-based state-of-the-art methods in terms of recall and F1-measure.", "labels": [], "entities": [{"text": "DEEB-RNN", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.63192218542099}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9988903403282166}, {"text": "F1-measure", "start_pos": 154, "end_pos": 164, "type": "METRIC", "confidence": 0.9959539175033569}]}], "datasetContent": [{"text": "We validate the proposed model through comparison with state-of-the-art methods on the ACE-2005 dataset.", "labels": [], "entities": [{"text": "ACE-2005 dataset", "start_pos": 87, "end_pos": 103, "type": "DATASET", "confidence": 0.9890368282794952}]}, {"text": "In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents.", "labels": [], "entities": []}, {"text": "All the data preprocessing and evaluation criteria follow those in (.", "labels": [], "entities": []}, {"text": "Hyper-parameters are tuned on the validation set.", "labels": [], "entities": []}, {"text": "We set the dimension of the hidden layers corresponding to GRU w , GRU s , and GRU e to 300, 200, and 300, respectively, the output size of W wand W s to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5.", "labels": [], "entities": []}, {"text": "In addition, we utilize the pre-trained word embeddings with 300 dimensions from () for initialization.", "labels": [], "entities": []}, {"text": "For entity types, their embeddings are randomly initialized.", "labels": [], "entities": []}, {"text": "We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout () for regularization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results with different atten- tion strategies.", "labels": [], "entities": []}, {"text": " Table 2: Comparison between different methods.   \u2020 indicates that the corresponding ED method uses", "labels": [], "entities": []}]}