{"title": [{"text": "Efficient Online Scalar Annotation with Bounded Support", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7328491508960724}, {"text": "system quality estimation", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.6421895821889242}]}, {"text": "We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from anno-tator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here.", "labels": [], "entities": []}, {"text": "Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency , suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.", "labels": [], "entities": [{"text": "dataset creation", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7116808295249939}]}], "introductionContent": [{"text": "We are concerned herewith the construction of datasets and evaluation of systems within natural language processing (NLP).", "labels": [], "entities": []}, {"text": "Specifically, humans providing responses that are used to derive graded values on natural language contexts, or in the ordering of systems corresponding to their perceived performance on some task.", "labels": [], "entities": []}, {"text": "Many NLP datasets involve eliciting from annotators some graded response.", "labels": [], "entities": []}, {"text": "The most popular annotation scheme is the n-ary ordinal approach as illustrated in(a).", "labels": [], "entities": []}, {"text": "For example, text maybe labeled for sentiment as positive, neutral or negative (, inter alia); or under political spectrum analysis as liberal, neutral, or conservative (O'.", "labels": [], "entities": [{"text": "O'.", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.8259369532267252}]}, {"text": "A response may correspond to a likelihood judgment, e.g., how likely a predicate is factive (, or that some natural language inference may hold (.", "labels": [], "entities": []}, {"text": "Responses may correspond to a notion of semantic", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare different annotation methods, we conduct three experiments: (1) lexical frequency inference, (2) political spectrum inference, and (3) human evaluation for machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.738829106092453}]}, {"text": "In all experiments, data collection is conducted through Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 57, "end_pos": 85, "type": "DATASET", "confidence": 0.9327497680981954}]}, {"text": "We ask annotators who meet the following minimum requirements: 12 living in the US, overall approval rate > 98%, and number of tasks approved > 500.", "labels": [], "entities": [{"text": "overall approval rate", "start_pos": 84, "end_pos": 105, "type": "METRIC", "confidence": 0.7605304916699728}]}, {"text": "The experimental setting for DA is straightforward.", "labels": [], "entities": [{"text": "DA", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9830119013786316}]}, {"text": "We ask annotators to annotate a scalar value for each instance, one item at a time.", "labels": [], "entities": []}, {"text": "We collect ten annotations for each instance to seethe relation between the number of annotations and accuracy (i.e., correlation).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9992109537124634}, {"text": "correlation", "start_pos": 118, "end_pos": 129, "type": "METRIC", "confidence": 0.9921954870223999}]}, {"text": "To setup the online update in RA and EASL, we use a partial ranking framework with scalars, where annotators are asked to rank and score n instances atone time as illustrated in.", "labels": [], "entities": [{"text": "EASL", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.6794748306274414}]}, {"text": "In all three experiments, we fix n = 5.", "labels": [], "entities": []}, {"text": "The partial ranking yields n 2 pairwise comparisons for RA and n scalar values for EASL.", "labels": [], "entities": [{"text": "RA", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.7019497156143188}, {"text": "EASL", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.6251609921455383}]}, {"text": "It is important to note that we can simultaneously retrieve pairwise In all experiments, we set the reward of single instance to be $0.01 (i.e., $0.05 in RA and EASL).", "labels": [], "entities": [{"text": "RA", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.8163701295852661}, {"text": "EASL", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.7511350512504578}]}, {"text": "This is $8/hour, assuming that annotating one instance takes five seconds.", "labels": [], "entities": []}, {"text": "Prior to annotation, we run a pilot to make sure that the participants understand the task correctly and the instructions are clear.", "labels": [], "entities": []}, {"text": "The partial ranking can be regarded as mini-batching.  that are randomly selected from the corpus of Contemporary American English (COCA) . We include this task for evaluation owing to its nonsubjective ground truth (relative corpus frequency) which can be used as an oracle response we would like to maximally correlate with.", "labels": [], "entities": []}, {"text": "We randomly select 150 verbs from COCA; the log frequency (log 10 ) is regarded as the oracle.", "labels": [], "entities": []}, {"text": "In DA, each instance is annotated by 10 different annotators.", "labels": [], "entities": []}, {"text": "In the RA and EASL, annotators are asked to rank/score five verbs for each HIT (n = 5).", "labels": [], "entities": [{"text": "EASL", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.5029507875442505}, {"text": "HIT", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7341393828392029}]}, {"text": "Each iteration contains 20 HITS and we run 10 iterations, which means that total number of annotations is the same in DA, RA, and EASL.", "labels": [], "entities": [{"text": "RA", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.8751996755599976}]}, {"text": "Figure 6 presents Spearman's and Pearson's correlations, indicating how accurately each annotation method obtains scalar values for each instance.", "labels": [], "entities": [{"text": "Pearson's correlations", "start_pos": 33, "end_pos": 55, "type": "METRIC", "confidence": 0.8270645538965861}]}, {"text": "Overall, in all three methods, the correlations are increased as more annotations are made.", "labels": [], "entities": [{"text": "correlations", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.952115535736084}]}, {"text": "The result also shows that RA and EASL ap-15 https://www.wordfrequency.info/ 16 Lexical frequency inference is an established experiment in (computational) psycholinguistics.", "labels": [], "entities": [{"text": "RA", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9306858777999878}, {"text": "EASL ap-15", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9124511778354645}, {"text": "Lexical frequency inference", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.8073676427205404}]}, {"text": "E.g., human behavioral measures have been compared with predictability and bias in various corpora (.", "labels": [], "entities": []}, {"text": "The agreement rate in DA (10 annotators) is 0.37 in Spearman's \u03c1.", "labels": [], "entities": [{"text": "agreement rate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9733991622924805}]}, {"text": "Considering the difficulty of ranking 150 verbs, this rate is fair.", "labels": [], "entities": []}, {"text": "Technically, the number of annotations per instance vary in RA and EASL, because they choose instances by match quality at each iteration.", "labels": [], "entities": [{"text": "EASL", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.6720594763755798}]}, {"text": "proaches achieve high correlation more efficiently than DA.", "labels": [], "entities": [{"text": "correlation", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9768776893615723}]}, {"text": "The gain of efficiency from DA to EASL is about 50%; two iterations in EASL achieves a close Spearman's \u03c1 to three annotators in DA.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.9467639923095703}]}, {"text": "presents the results of the final scalar values that each method annotated.", "labels": [], "entities": []}, {"text": "The distribution of the histograms shows that overall three methods successfully capture the latent distribution of scalar values in the data.", "labels": [], "entities": []}, {"text": "shows a dynamic change of match quality.", "labels": [], "entities": []}, {"text": "In the beginning (iteration 0), all the instances are equally competitive because we have no information about them and initialize them with the same parameters.", "labels": [], "entities": []}, {"text": "As iterations goon, the instances along the diagonal have higher match quality, indicating that competitive matches are more likely to be selected fora next iteration.", "labels": [], "entities": []}, {"text": "In other words, match-quality helps to choose informative pairs to compare at each iteration, which reduces the number of less informative annotations (e.g., a pairwise comparison between the highest and lowest instances).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example propositions and the scalar political spec- trum ranged between 0 (very conservative) and 100 (very lib- eral) by each approach: direct assessment, online ranking ag- gregation, and EASL. The dashed lines indicate a split by 5- ary ordinal scale.", "labels": [], "entities": []}]}