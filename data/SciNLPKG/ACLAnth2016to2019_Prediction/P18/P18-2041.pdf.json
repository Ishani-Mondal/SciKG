{"title": [{"text": "Obligation and Prohibition Extraction Using Hierarchical RNNs", "labels": [], "entities": [{"text": "Prohibition Extraction", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7831301689147949}]}], "abstractContent": [{"text": "We consider the task of detecting contractual obligations and prohibitions.", "labels": [], "entities": [{"text": "detecting contractual obligations and prohibitions", "start_pos": 24, "end_pos": 74, "type": "TASK", "confidence": 0.8658993244171143}]}, {"text": "We show that a self-attention mechanism improves the performance of a BILSTM clas-sifier, the previous state of the art for this task, by allowing it to focus on indicative tokens.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.8906401991844177}]}, {"text": "We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9860222339630127}]}, {"text": "Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9815067052841187}]}], "introductionContent": [{"text": "Legal text processing) is a growing research area, comprising tasks such as legal question answering, contract element extraction ( , and legal text generation.", "labels": [], "entities": [{"text": "Legal text processing)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8411689102649689}, {"text": "legal question answering", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6960601409276327}, {"text": "contract element extraction", "start_pos": 102, "end_pos": 129, "type": "TASK", "confidence": 0.6545554002126058}, {"text": "legal text generation", "start_pos": 138, "end_pos": 159, "type": "TASK", "confidence": 0.7253074447313944}]}, {"text": "We consider obligation and prohibition extraction from contracts, i.e., detecting sentences (or clauses) that specify what should or should not happen).", "labels": [], "entities": [{"text": "obligation and prohibition extraction from contracts", "start_pos": 12, "end_pos": 64, "type": "TASK", "confidence": 0.7530504961808523}]}, {"text": "This task is important for legal firms and legal departments, especially when they process large numbers of contracts to monitor the compliance of each party.", "labels": [], "entities": []}, {"text": "Methods that would automatically identify (e.g., highlight) sentences (or clauses) specifying obligations and prohibitions would allow lawyers and paralegals to inspect contracts more quickly.", "labels": [], "entities": []}, {"text": "They would also be a step towards populating databases with information extracted from contracts, along with methods that extract contractors, particular dates (e.g., start and end dates), applicable law, legislation references etc.", "labels": [], "entities": []}, {"text": "Obligation and prohibition extraction is a kind of deontic sentence (or clause) classification (O').", "labels": [], "entities": [{"text": "Obligation and prohibition extraction", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6976451873779297}, {"text": "deontic sentence (or clause) classification (O')", "start_pos": 51, "end_pos": 99, "type": "TASK", "confidence": 0.674461305141449}]}, {"text": "Different firms may use different or finer deontic classes (e.g., distinguishing between payment and delivery obligations), but obligations and prohibitions are the most common coarse deontic classes.", "labels": [], "entities": []}, {"text": "Using similar classes, O' reported that a bidirectional LSTM (BILSTM) classifier ( outperformed several others (including logistic regression, SVM, AdaBoost, Random Forests) in legal sentence classification, possibly because longterm dependencies (e.g., modal verbs or negations interacting with distant dependents) are common and crucial in legal texts, and LSTMs can cope with long-term dependencies better than methods relying on fixed-size context windows.", "labels": [], "entities": [{"text": "legal sentence classification", "start_pos": 177, "end_pos": 206, "type": "TASK", "confidence": 0.7512789169947306}]}, {"text": "We improve upon the work of O' in four ways.", "labels": [], "entities": []}, {"text": "First, we show that selfattention () improves the performance of the BILSTM classifier, by allowing the system to focus on indicative words.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.8098527789115906}]}, {"text": "Second, we introduce a hierarchical BILSTM, where a first BILSTM processes each sentence word by, which fit better the target task, where nested clauses are frequent.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.931443989276886}]}], "datasetContent": [{"text": "Hyper-parameters were tuned by grid-searching the following sets, and selecting the values with the best validation loss: LSTM hidden units {100, 200, 300}, batch size {8, 16, 32}, drop-out rate {0.4, 0.5, 0.6}.", "labels": [], "entities": []}, {"text": "The red dashed lines of are drop-out layers.", "labels": [], "entities": []}, {"text": "We used categorical crossentropy loss, Glorot initialization (Glorot and Bengio, 2010), Adam (Kingma and Ba, 2015), learning rate 0.001, and early stopping on the validation loss.", "labels": [], "entities": [{"text": "learning rate 0.001", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.9559570550918579}, {"text": "validation", "start_pos": 163, "end_pos": 173, "type": "TASK", "confidence": 0.9737921953201294}]}, {"text": "reports the precision, recall, F1 score, area under the precision-recall curve (AUC) per class, as well as micro-and macro-averages.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9996660947799683}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9973775148391724}, {"text": "F1 score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9887734949588776}, {"text": "precision-recall curve (AUC)", "start_pos": 56, "end_pos": 84, "type": "METRIC", "confidence": 0.976072120666504}]}, {"text": "The self-attention mechanism (BILSTM-ATT) leads to clear overall improvements (in macro and micro F1 and AUC,) comparing to the plain BILSTM, supporting the hypothesis that selfattention allows the classifier to focus on indicative tokens.", "labels": [], "entities": [{"text": "BILSTM-ATT", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9813961386680603}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.9498559236526489}, {"text": "AUC", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.8776875734329224}, {"text": "BILSTM", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.8346495628356934}]}, {"text": "Allowing the BILSTM to consider tokens of neighboring sentences (X-BILSTM-ATT) does not lead to any clear overall improvements.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9233737587928772}]}, {"text": "We resample the drop-out mask at each time-step.", "labels": [], "entities": []}, {"text": "The hierarchical H-BILSTM-ATT clearly outperforms the other three methods, supporting the hypothesis that considering entire sections and allowing the sentence embeddings to interact in the upper BILSTM is beneficial.", "labels": [], "entities": []}, {"text": "Notice that the three flat methods (BILSTM, BILSTM-ATT, X-BILSTM-ATT) obtain particularly lower F1 and AUC scores, compared to H-BILSTM-ATT, in the classes that correspond to nested clauses (obligation list item, prohibition list item).", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.980566143989563}, {"text": "BILSTM-ATT", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9759560823440552}, {"text": "F1", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9996155500411987}, {"text": "AUC", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9327843189239502}]}, {"text": "This is due to the fact that the flat methods have no (or only limited, in the case of X-BILSTM-ATT) view of the previous sentences, which often indicate if a nested clause is an obligation or prohibition (see, for example, examples 4-6 in).", "labels": [], "entities": []}, {"text": "H-BILSTM-ATT is also much faster to train than BILSTM and BILSTM-ATT, even though it has more parameters, because it converges faster (5-7 epochs vs. 12-15).", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9791499972343445}, {"text": "BILSTM-ATT", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.978456974029541}]}, {"text": "X-BILSTM-ATT is particularly slow, because its BILSTM processes the same sentences multiple times, when they are classified and when they are neighboring sentences.", "labels": [], "entities": [{"text": "BILSTM", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9558484554290771}]}], "tableCaptions": [{"text": " Table 1: Examples of sentences and clauses, with human annotations of classes. Terms that are highly  indicative of the classes are shown in bold and underlined here, but are not marked by the annotators.", "labels": [], "entities": []}, {"text": " Table 2: Sentences/clauses after sentence splitting.", "labels": [], "entities": [{"text": "Sentences/clauses after sentence splitting", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.7079693873723348}]}, {"text": " Table 3: Precision, recall, F1, and AUC scores, with the best results in bold and gray background.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990806579589844}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9975265860557556}, {"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9983325600624084}, {"text": "AUC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9948667287826538}]}]}