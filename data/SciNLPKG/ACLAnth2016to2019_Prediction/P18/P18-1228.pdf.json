{"title": [{"text": "SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment", "labels": [], "entities": [{"text": "Sentiment", "start_pos": 87, "end_pos": 96, "type": "TASK", "confidence": 0.4888589382171631}]}], "abstractContent": [{"text": "Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge.", "labels": [], "entities": []}, {"text": "Here, we propose SEMAXIS, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment.", "labels": [], "entities": [{"text": "characterize word semantics", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6976053913434347}]}, {"text": "We demonstrate that SEMAXIS can capture nuanced semantic representations in multiple online communities.", "labels": [], "entities": [{"text": "SEMAXIS", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.8718854784965515}]}, {"text": "We also show that, when the sentiment axis is examined, SEMAXIS outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "In lexicon-based text analysis, a common, tacit assumption is that the meaning of each word does not change significantly across contexts.", "labels": [], "entities": [{"text": "lexicon-based text analysis", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6996978918711344}]}, {"text": "This approximation, however, falls short because context can strongly alter the meaning of words).", "labels": [], "entities": []}, {"text": "For instance, the word kill maybe used much more positively in the context of video games than it would be in a news story; the word soft maybe used much more negatively in the context of sports than it is in the context of toy animals ().", "labels": [], "entities": []}, {"text": "Thus, lexicon-based analysis exhibits a clear limitation when two groups with strongly dissimilar lexical contexts are compared.", "labels": [], "entities": []}, {"text": "Recent breakthroughs in vector-space representation, such as word2vec (, provide new opportunities to tackle this challenge of context-dependence, because in these approaches, the representation of each word is learned from its context.", "labels": [], "entities": [{"text": "vector-space representation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.7258673459291458}]}, {"text": "For instance, a recent study shows that a propagation method on the vector space embedding can infer contextdependent sentiment values of words ().", "labels": [], "entities": []}, {"text": "Yet, it remains to be seen whether it is possible to generalize this idea to general word semantics other than sentiment.", "labels": [], "entities": []}, {"text": "In this work, we propose SEMAXIS, a lightweight framework to characterize domainspecific word semantics beyond sentiment.", "labels": [], "entities": []}, {"text": "SE-MAXIS characterizes word semantics with respect to many semantic perspectives in a domainspecific word-vector space.", "labels": [], "entities": []}, {"text": "To systematically discover the manifold of word semantics, we induce 732 semantic axes based on the antonym pairs from ConceptNet (.", "labels": [], "entities": []}, {"text": "We would like to emphasize that, although some of the induced axes can be considered as an extended version of sentiment analysis, such as an axis of 'respectful' (positive) and 'disrespectful' (negative), some cannot be mapped to a positive and negative relationship, such as 'exogeneous' Based on this rich set of semantic axes, SEMAXIS captures nuanced semantic representations across corpora.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8005788326263428}]}, {"text": "The key contributions of this paper are: \u2022 We propose a general framework to characterize the manifold of domain-specific word semantics.", "labels": [], "entities": []}, {"text": "\u2022 We systematically identify semantic axes based on the antonym pairs in ConceptNet.", "labels": [], "entities": []}, {"text": "\u2022 We demonstrate that SEMAXIS can capture semantic differences between two corpora.", "labels": [], "entities": [{"text": "SEMAXIS", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9220375418663025}]}, {"text": "\u2022 We provide a systematic evaluation in comparison to the state-of-the-art, domainspecific sentiment lexicon construction methodologies.", "labels": [], "entities": [{"text": "domainspecific sentiment lexicon construction", "start_pos": 76, "end_pos": 121, "type": "TASK", "confidence": 0.644958071410656}]}, {"text": "Although the idea of defining a semantic axis and assessing the meaning of a word with a vector projection is not new, it has not been demonstrated that this simple method can effectively in-duce context-aware semantic lexicons.", "labels": [], "entities": []}, {"text": "All of the inferred lexicons along with code for SEMAXIS and all methods evaluated are made available in the SEMAXIS package released with this paper 1 .", "labels": [], "entities": [{"text": "SEMAXIS", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.7342970371246338}]}], "datasetContent": [{"text": "We compare our method against state-of-the-art approaches that generate domain-specific sentiment lexicons.", "labels": [], "entities": []}, {"text": "State-of-the-art approaches: Our baseline for the standard English is a WordNet-based method, which performs label propagation over a WordNet-derived graph).", "labels": [], "entities": [{"text": "label propagation", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.7077610790729523}]}, {"text": "For Twitter, we use Sentiment140, a distantly supervised approach that uses signals from emoticons (.", "labels": [], "entities": []}, {"text": "Moreover, on both datasets, we compare against two state-ofthe-art sentiment induction methods: DENSIFIER, a method that learns orthogonal transformations of word vectors (, and SENTPROP, a method with a label propagation approach on word embeddings ().", "labels": [], "entities": [{"text": "sentiment induction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7479910552501678}, {"text": "DENSIFIER", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.8790870308876038}, {"text": "SENTPROP", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9133979082107544}, {"text": "label propagation", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.7184606790542603}]}, {"text": "Seed words, which are called pole words in our work, are listed in.", "labels": [], "entities": []}, {"text": "Evaluation metrics: We evaluate the aforementioned approaches according to (i) their binary classification accuracy (positive and negative), (ii) ternary classification performance (positive, neutral, and negative), and (iii) Kendall \u03c4 rankcorrelation with continuous human-annotated polarity scores.", "labels": [], "entities": []}, {"text": "Since all methods result in sentiment scores of words rather than assigning a class of sentiment, we label words as positive, neutral, or negative using the class-mass normalization method (.", "labels": [], "entities": []}, {"text": "This normalization uses knowledge of the label distribution of a test dataset and simply assigns labels to best match this distribution.", "labels": [], "entities": []}, {"text": "For the implementation of other methods, we directly use the source code without any modification or tuning used in (Hamilton et al., 2016a).: Evaluation results.", "labels": [], "entities": []}, {"text": "Our method performs best on both Standard English and Twitter.", "labels": [], "entities": []}, {"text": "We use Reddit 2016 comment datasets that are publicly available (/u/Dewarim, 2017).", "labels": [], "entities": [{"text": "Reddit 2016 comment datasets", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.8113167881965637}]}, {"text": "We build a corpus from each subreddit by extracting all the comments posted on that subreddit.", "labels": [], "entities": []}, {"text": "When the size of two corpora used for comparison is considerably different, we undersample the bigger corpus fora fair comparison.", "labels": [], "entities": []}, {"text": "Every corpus then undergoes the same pre-processing, where we first remove punctuation and stop words, then replace URLs with its domain name.", "labels": [], "entities": []}, {"text": "Reference model for Reddit data As we discussed earlier, many datasets of our interest are likely too small to obtain good vector representations.", "labels": [], "entities": []}, {"text": "For example, two popular subreddits, /r/The Donald and /r/SandersForPresident 2 , show only 59.8% and 42.1% in analogy test, respectively.", "labels": [], "entities": [{"text": "SandersForPresident 2", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.9317202568054199}, {"text": "analogy test", "start_pos": 111, "end_pos": 123, "type": "METRIC", "confidence": 0.8013394773006439}]}, {"text": "3 Therefore, as we proposed, we first create a pretrained word embedding with a larger background corpus and perform additional training with target subreddits.", "labels": [], "entities": []}, {"text": "We sample 1 million comments from each of the top 200 subreddits, resulting in 20 million comments.", "labels": [], "entities": []}, {"text": "Using this sample, we build a word embedding, denoted as Reddit20M, using the CBOW model with a window size of five, a minimum word count of 10, the negative sampling, and down-sampling of frequent terms as suggested in (.", "labels": [], "entities": []}, {"text": "For the subsequent training with the target corpora, we train the model with a small starting learning rate, 0.005; Using different rates, such as 0.01 and 0.001, did not make much difference.", "labels": [], "entities": []}, {"text": "We further tune the model with the dimension size of 300 and the number of the epoch of 100 using the analogy test results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation results. Our method performs  best on both Standard English and Twitter.", "labels": [], "entities": []}, {"text": " Table 3: Results of analogy tests, comparing 20M  sample texts from Reddit vs. Google 100B News.", "labels": [], "entities": [{"text": "Google 100B News", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.7856757044792175}]}]}