{"title": [{"text": "The First Multilingual Surface Realisation Shared Task (SR'18): Overview and Evaluation Results", "labels": [], "entities": [{"text": "Multilingual Surface Realisation Shared Task (SR'18)", "start_pos": 10, "end_pos": 62, "type": "TASK", "confidence": 0.8140605762600899}]}], "abstractContent": [{"text": "We report results from the SR'18 Shared Task, anew multilingual surface realisa-tion task organised as part of the ACL'18 Workshop on Multilingual Surface Reali-sation.", "labels": [], "entities": [{"text": "SR'18 Shared Task", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.8118147055308024}]}, {"text": "As in its English-only predecessor task SR'11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed.", "labels": [], "entities": []}, {"text": "The shallow track was offered in ten, and the deep track in three languages.", "labels": [], "entities": []}, {"text": "Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity.", "labels": [], "entities": []}, {"text": "This report presents the evaluation results , along with descriptions of the SR'18 tracks, data and evaluation methods.", "labels": [], "entities": [{"text": "SR'18 tracks", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9013792276382446}]}, {"text": "For full descriptions of the participating systems , please seethe separate system reports elsewhere in this volume.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For three of the languages in the shallow track (English, Spanish and French), we replicated the human evaluation method from WMT'17, known as Direct Assessment (DA) (, exactly, except that we also ran (separate) experiments to assess the Readability criterion, using the same method.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.9089010953903198}, {"text": "Direct Assessment (DA)", "start_pos": 143, "end_pos": 165, "type": "METRIC", "confidence": 0.7277521073818207}]}, {"text": "Quality assurance: System outputs are randomly assigned to HITs (following Mechanical Turk terminology) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a 'damaged' version and (iii) some are replaced by their corresponding reference texts.", "labels": [], "entities": [{"text": "Quality assurance", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.757893979549408}]}, {"text": "In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high.", "labels": [], "entities": [{"text": "HIT", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.5643341541290283}]}, {"text": "For full details of how these additional texts are created and thresholds applied, please refer to.", "labels": [], "entities": []}, {"text": "Below we report QA figures for the MTurk evaluations (Section 3.2.1).", "labels": [], "entities": [{"text": "QA", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.8398534059524536}, {"text": "MTurk evaluations", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.6957082599401474}]}, {"text": "Code: We were able to reuse, with minor adaptations, the code produced for the WMT'17 evaluations.", "labels": [], "entities": [{"text": "WMT'17 evaluations", "start_pos": 79, "end_pos": 97, "type": "DATASET", "confidence": 0.7341818511486053}]}, {"text": "In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google's internal 'Data Compute' system evaluation service, where experienced evaluators carefully assess each system output.", "labels": [], "entities": []}, {"text": "We used an interface that matches the WMT'17 interface above, as closely as was possible within the constraints of the Data Compute platform.", "labels": [], "entities": [{"text": "WMT'17", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.8264696002006531}]}, {"text": "Everything stated at the beginning of Section 3.2 also holds for the expert annotator evaluations with Google Data Compute.", "labels": [], "entities": [{"text": "Google Data Compute", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.9076299071311951}]}, {"text": "Quality assurance: Because in the Google Data Compute version of the evaluation experiment we were using expert evaluators from a pool of workers routinely employed to perform such tasks, we did not replicate the WMT'17 QA techniques precisely, opting fora simpler test of self-consistency, or intra-evaluator agreement (IEA) instead.", "labels": [], "entities": [{"text": "WMT'17 QA", "start_pos": 213, "end_pos": 222, "type": "DATASET", "confidence": 0.806632936000824}, {"text": "intra-evaluator agreement (IEA)", "start_pos": 294, "end_pos": 325, "type": "METRIC", "confidence": 0.796344655752182}]}, {"text": "Test set items were randomly grouped into sets of 100 (which we are also calling HITs here for uniformity) and order was again randomised before presentation to evaluators.", "labels": [], "entities": []}, {"text": "Each evaluator did at least one HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.6567305326461792}]}, {"text": "Each HIT contained 5 items which were duplicated to test for IEA which we computed as the average Pearson correlation coefficient per HIT.", "labels": [], "entities": [{"text": "IEA", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.784507155418396}, {"text": "Pearson correlation coefficient", "start_pos": 98, "end_pos": 129, "type": "METRIC", "confidence": 0.9542301098505656}]}, {"text": "The average IEA for English was 0.75 on the raw scores for Meaning Similarity, and 0.66 for Readability.", "labels": [], "entities": [{"text": "IEA", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9929792284965515}, {"text": "Meaning Similarity", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.884090930223465}]}, {"text": "In terms of BLEU-4, in the Shallow Track, Tilburg obtained the best scores for four languages (French, Italian, Dutch, Portuguese), OSU for three (Arabic, Spanish, Finnish), BinLin for two (Czech, Russian), and ADAPT for one (English).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9983269572257996}, {"text": "Shallow Track", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9431861340999603}, {"text": "OSU", "start_pos": 132, "end_pos": 135, "type": "DATASET", "confidence": 0.7068624496459961}, {"text": "BinLin", "start_pos": 174, "end_pos": 180, "type": "DATASET", "confidence": 0.7372053265571594}, {"text": "ADAPT", "start_pos": 211, "end_pos": 216, "type": "METRIC", "confidence": 0.8833065032958984}]}, {"text": "The highest BLEU-4 scores across languages were obtained on the English and Spanish datasets, with BLEU-4 scores of 69.14 (ADAPT) and 65.31 (OSU) respectively.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9989479184150696}, {"text": "BLEU-4", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9991337656974792}, {"text": "ADAPT", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.974480390548706}, {"text": "OSU)", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.7534317672252655}]}, {"text": "Results are identical for DIST, except that AX, rather than BinLin, has the highest score for Czech.", "labels": [], "entities": [{"text": "DIST", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.7780119180679321}, {"text": "Czech", "start_pos": 94, "end_pos": 99, "type": "DATASET", "confidence": 0.9253749251365662}]}, {"text": "The picture for NIST is also very similar to that for BLEU-4, except that ADAPT and OSU are tied for best NIST score for English, and BinLin (rather than Tilburg) has the best NIST score for Dutch.", "labels": [], "entities": [{"text": "NIST", "start_pos": 16, "end_pos": 20, "type": "DATASET", "confidence": 0.8165425658226013}, {"text": "BLEU-4", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8935149908065796}, {"text": "ADAPT", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.8060374855995178}]}, {"text": "In the Deep Track, only ADAPT submitted system outputs (English), and as expected, the scores are much lower than for the Shallow Track, across all metrics.", "labels": [], "entities": []}, {"text": "Given the small number of submissions in the Deep Track, we conducted human evaluations for the Shallow Track only.", "labels": [], "entities": [{"text": "Deep Track", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8850382566452026}, {"text": "Shallow Track", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.962804913520813}]}, {"text": "We used Mechanical Turk for the three languages for which this is feasible (English, Spanish and French), and our aim was to also conduct evaluations via Google's Data Compute service for three additional languages which had the next highest numbers of submissions, as    well as for English in order to enable us to compare results obtained with the two different methods.", "labels": [], "entities": []}, {"text": "However, most of the latter evaluations are still ongoing and will be reported separately in a future paper.", "labels": [], "entities": []}, {"text": "Below, we report Google Data Compute results and comparisons with Mechanical Turk results, for English only. with OSU achieving the highest overall score in terms of both average raw DA scores and corresponding z-scores.", "labels": [], "entities": [{"text": "Google Data Compute", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.8839952945709229}, {"text": "OSU", "start_pos": 114, "end_pos": 117, "type": "DATASET", "confidence": 0.8073645234107971}, {"text": "DA", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.749697208404541}]}, {"text": "Readability scores for the same set of systems range from 78.7% to 41.3%, revealing that MTurk workers rate the Meaning Similarity between generated texts and corresponding reference sentences higher in general than Readability.", "labels": [], "entities": []}, {"text": "In order to investigate how Readability of system outputs compare to human-produced text, we included the original test sentences as a system in the Readability evaluation (for Meaning Similarity the notional score is 100%).", "labels": [], "entities": []}, {"text": "Unsurprisingly, human text achieves the highest score in terms of Readability (78.7%) but is quite closely followed by the best performing system in terms of Readability, ADAPT (73.9%).", "labels": [], "entities": [{"text": "ADAPT", "start_pos": 171, "end_pos": 176, "type": "METRIC", "confidence": 0.5211523771286011}]}], "tableCaptions": [{"text": " Table 2: SR'18 dataset sizes for training, development and test sets.", "labels": [], "entities": [{"text": "SR'18 dataset", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.7984612286090851}]}, {"text": " Table 3: BLEU-4 scores for the test data. Bold = best score per language. * = late submission.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9984594583511353}, {"text": "late", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9606598615646362}]}, {"text": " Table 4: NIST scores for the test data. Bold = best score per language. * = late submission.", "labels": [], "entities": [{"text": "NIST", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8546884059906006}, {"text": "late", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9643760919570923}]}, {"text": " Table 5: DIST scores for the test data. Bold = best score per language. * = late submission.", "labels": [], "entities": [{"text": "DIST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5248161554336548}, {"text": "late", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9636691808700562}]}, {"text": " Table 6: MTurk DA human evaluation results for English Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.", "labels": [], "entities": [{"text": "MTurk DA human evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7688217610120773}, {"text": "English Shallow Track", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.8400428295135498}, {"text": "DA score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9473273754119873}, {"text": "Assess", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9951642751693726}]}, {"text": " Table 7: MTurk DA human evaluation results for French Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.", "labels": [], "entities": [{"text": "MTurk DA human evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7568715363740921}, {"text": "French Shallow Track", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9783793489138285}, {"text": "DA score", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9462378025054932}, {"text": "Assess", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9963437914848328}]}, {"text": " Table 8: MTurk DA human evaluation results for Spanish Shallow Track; % = average DA score (0-100);  z = z-score; n = number of distinct sentences assessed; Assess. = total number of sentences assessed.", "labels": [], "entities": [{"text": "MTurk DA human evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7740633338689804}, {"text": "Spanish Shallow Track", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9111383557319641}, {"text": "DA score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9476891160011292}, {"text": "Assess", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.995844304561615}]}, {"text": " Table 9: Pearson correlation of DA human evaluation scores with Automatic Metrics for English, French  and Spanish Shallow Track.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9837726950645447}, {"text": "DA human evaluation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.841314415136973}]}, {"text": " Table 10: Google Data Compute human evaluation results for the English shallow track, where % =  average score (0-100) for generated sentences; n distinct sentences assessed per system.", "labels": [], "entities": [{"text": "Google Data", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8834976553916931}]}, {"text": " Table 11: Pearson correlation between human  evaluations carried out using MTurk DA and  Google Data Compute.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.8748565912246704}, {"text": "MTurk DA", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.8751009106636047}, {"text": "Google Data Compute", "start_pos": 90, "end_pos": 109, "type": "DATASET", "confidence": 0.8975552519162496}]}]}