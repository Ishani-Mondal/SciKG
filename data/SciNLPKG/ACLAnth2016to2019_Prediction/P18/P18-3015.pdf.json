{"title": [{"text": "Reinforced Extractive Summarization with Question-Focused Rewards", "labels": [], "entities": [{"text": "Reinforced Extractive Summarization", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8828306595484415}]}], "abstractContent": [], "introductionContent": [{"text": "We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary ().", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.6233729124069214}]}, {"text": "Existing supervised approaches to extractive summarization frequently use human abstracts to create annotations for extraction units).", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6133776903152466}]}, {"text": "E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise.", "labels": [], "entities": []}, {"text": "Despite the usefulness, there are two issues with this scheme.", "labels": [], "entities": []}, {"text": "First, avast majority of the source words are tagged 0s, only a small portion are 1s.", "labels": [], "entities": []}, {"text": "This is due to the fact that human abstracts are short and concise; they often contain words not present in the source.", "labels": [], "entities": []}, {"text": "Second,: Example source document, the top sentence of the abstract, and system-generated Cloze-style questions.", "labels": [], "entities": []}, {"text": "Source content related to the abstract is italicized.", "labels": [], "entities": []}, {"text": "not all labels are accurate.", "labels": [], "entities": []}, {"text": "Source words that are labelled 0 maybe paraphrases, generalizations, or otherwise related to words in the abstracts.", "labels": [], "entities": []}, {"text": "These source words are often mislabelled.", "labels": [], "entities": []}, {"text": "Consequently, leveraging human abstracts to provide supervision for extractive summarization remains a challenge.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.717058926820755}]}, {"text": "Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary (.", "labels": [], "entities": [{"text": "Neural abstractive summarization", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8164747953414917}]}, {"text": "While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the original.", "labels": [], "entities": []}, {"text": "Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.7045000791549683}]}, {"text": "This study thus chooses to focus on neural extractive summarization.", "labels": [], "entities": [{"text": "neural extractive summarization", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7671134372552236}]}, {"text": "We explore anew training paradigm for extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8652638792991638}]}, {"text": "We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword.", "labels": [], "entities": []}, {"text": "Because the questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (\u2248 human abstract) such that the summaries can work as a document surrogate to predict correct answers.", "labels": [], "entities": []}, {"text": "We use an attention mechanism to locate segments of a summary that are relevant to a given question so that the summary can be used to answer multiple questions.", "labels": [], "entities": []}, {"text": "This study extends the work of () to use reinforcement learning to explore the space of extractive summaries.", "labels": [], "entities": []}, {"text": "While the original work focuses on generating rationales to support supervised classification, the goal of our study is to produce fluent, generic document summaries.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6672528982162476}]}, {"text": "The question-answering (QA) task is designed to fulfill this goal and the QA performance is only secondary.", "labels": [], "entities": []}, {"text": "Our research contributions can be summarized as follows: \u2022 we investigate an alternative training scheme for extractive summarization where the summaries are encouraged to be semantically close to human abstracts in addition to sharing common words; \u2022 we compare two methods to convert human abstracts to Cloze-style questions and investigate its impact on QA and summarization performance.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.6077324151992798}, {"text": "summarization", "start_pos": 364, "end_pos": 377, "type": "TASK", "confidence": 0.9644012451171875}]}, {"text": "Our results surpass those of previous systems on a standard summarization dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "All training, validation, and testing was performed using the CNN dataset () containing news articles paired with human-written highlights (i.e., abstracts).", "labels": [], "entities": [{"text": "CNN dataset", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.9710015058517456}]}, {"text": "We observe that a source article contains 29.8 sentences and an abstract contains 3.54 sentences on average.", "labels": [], "entities": []}, {"text": "The train/valid/test splits contain 90,266, 1,220, 1,093 articles respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the CNN test set (full-length F1 scores).", "labels": [], "entities": [{"text": "CNN test set", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9866670966148376}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.992889404296875}]}, {"text": " Table 3: Train/valid accuracy and R-2 F-scores when using  varying numbers of QA pairs (K=1 to 5) in the reward func.", "labels": [], "entities": [{"text": "Train", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.946459949016571}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.7316169142723083}, {"text": "R-2", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9301331043243408}, {"text": "F-scores", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.7303378582000732}]}]}