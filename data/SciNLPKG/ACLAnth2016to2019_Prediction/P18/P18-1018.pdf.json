{"title": [{"text": "Comprehensive Supersense Disambiguation of English Prepositions and Possessives", "labels": [], "entities": [{"text": "Supersense Disambiguation of English Prepositions and Possessives", "start_pos": 14, "end_pos": 79, "type": "TASK", "confidence": 0.8617684926305499}]}], "abstractContent": [{"text": "Semantic relations are often signaled with prepositional or possessive marking-but extreme polysemy bedevils their analysis and automatic interpretation.", "labels": [], "entities": []}, {"text": "We introduce anew annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English.", "labels": [], "entities": [{"text": "disambiguation of prepositions and possessives", "start_pos": 62, "end_pos": 108, "type": "TASK", "confidence": 0.8005534052848816}]}, {"text": "Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory ; and distinguish between a marker's lexical contribution and the role it marks in the context of a predicate or scene.", "labels": [], "entities": []}, {"text": "Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar, as per a common metaphor, gives speakers of a language a shared toolbox to construct and deconstruct meaningful and fluent utterances.", "labels": [], "entities": []}, {"text": "Being highly analytic, English relies heavily on word order and closed-class function words like prepositions, determiners, and conjunctions.", "labels": [], "entities": []}, {"text": "Though function words bear little semantic content, they are nevertheless crucial to the meaning.", "labels": [], "entities": []}, {"text": "Consider prepositions: they serve, for example, to convey place and time (We met at/in/outside the restaurant for/after an hour), to express configurational relationships like quantity, possession, part/whole, and membership (the coats of dozens of children in the class), and to indicate semantic roles in argument structure (Grandma cooked dinner for the children * nathan.schneider@georgetown.edu (1) I was booked for/DURATION 2 nights at/LOCUS this hotel in/TIME Oct 2007 . (2) I went to/GOAL ohm after/EXPLANATION;TIME reading some of/QUANTITY;WHOLE the reviews .  vs. Grandma cooked the children for dinner).", "labels": [], "entities": [{"text": "TIME Oct 2007", "start_pos": 462, "end_pos": 475, "type": "DATASET", "confidence": 0.8443334698677063}, {"text": "GOAL ohm after/EXPLANATION", "start_pos": 492, "end_pos": 518, "type": "METRIC", "confidence": 0.8560268521308899}, {"text": "TIME", "start_pos": 519, "end_pos": 523, "type": "METRIC", "confidence": 0.9122733473777771}, {"text": "WHOLE", "start_pos": 549, "end_pos": 554, "type": "METRIC", "confidence": 0.9362542629241943}]}, {"text": "Frequent prepositions like for are maddeningly polysemous, their interpretation depending especially on the object of the preposition-I rode the bus for 5 dollars/minutes-and the governor of the prepositional phrase (PP): I Ubered/asked for $5.", "labels": [], "entities": [{"text": "Ubered", "start_pos": 224, "end_pos": 230, "type": "METRIC", "confidence": 0.9694249033927917}]}, {"text": "Possessives are similarly ambiguous: Whistler's mother/painting/hat/death.", "labels": [], "entities": []}, {"text": "Semantic interpretation requires some form of sense disambiguation, but arriving at a linguistic representation that is flexible enough to generalize across usages and types, yet simple enough to support reliable annotation, has been a daunting challenge ( \u00a72).", "labels": [], "entities": [{"text": "Semantic interpretation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8429010212421417}, {"text": "sense disambiguation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.7338663637638092}]}, {"text": "This work represents anew attempt to strike that balance.", "labels": [], "entities": []}, {"text": "Building on prior work, we argue for an approach to describing English preposition and possessive semantics with broad coverage.", "labels": [], "entities": [{"text": "English preposition and possessive semantics", "start_pos": 63, "end_pos": 107, "type": "TASK", "confidence": 0.5523729443550109}]}, {"text": "Given the semantic overlap between prepositions and possessives (the hood of the car vs. the car's hood or its hood), we analyze them using the same inventory of semantic labels.", "labels": [], "entities": []}, {"text": "1 Our contributions include: \u2022 anew hierarchical inventory (\"SNACS\") of 50 supersense classes, extensively documented in guidelines for English ( \u00a73); \u2022 a gold-standard corpus with comprehensive annotations: all types and tokens of prepositions and possessives are disambiguated ( \u00a74; example sentences appear in); \u2022 an interannotator agreement study that shows the scheme is reliable and generalizes across genres-and for the first time demonstrating empirically that the lexical semantics of a preposition can sometimes be detached from the PP's semantic role ( \u00a75); \u2022 disambiguation experiments with two supervised classification architectures to establish the difficulty of the task ( \u00a76).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments use the reviews corpus described in \u00a74.", "labels": [], "entities": []}, {"text": "We adopt the official training/development/ test splits of the Universal Dependencies (UD) project; their sizes are presented in table 1.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) project", "start_pos": 63, "end_pos": 98, "type": "DATASET", "confidence": 0.6022489567597707}]}, {"text": "All systems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters.", "labels": [], "entities": []}, {"text": "Gold tokenization was used throughout.", "labels": [], "entities": [{"text": "Gold tokenization", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.583147406578064}]}, {"text": "Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation-i.e., tokens with special labels (see \u00a74) were excluded.", "labels": [], "entities": []}, {"text": "To test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0 ().", "labels": [], "entities": [{"text": "automatic syntactic parsing", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6386834879716238}]}, {"text": "13 Named entity tags from the default 12-class CoreNLP model were used in all conditions.", "labels": [], "entities": []}, {"text": "6.2 Target Identification \u00a73.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as The CoreNLP parser was trained on all 5 genres of the English Web Treebank-i.e., a superset of our training set.", "labels": [], "entities": [{"text": "English Web Treebank-i.e.", "start_pos": 332, "end_pos": 357, "type": "DATASET", "confidence": 0.903610348701477}]}, {"text": "Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested with UDv1 parses produced by well as possessive case markers and multiword expressions.", "labels": [], "entities": [{"text": "UDv2 standard", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9206088781356812}]}, {"text": "61.2% of the units annotated in our corpus are adpositions according to gold POS annotation, 20.2% are possessives, and 18.6% belong to other POS classes.", "labels": [], "entities": []}, {"text": "Furthermore, 14.1% of tokens labeled as adpositions or possessives are not annotated because they are part of a multiword expression (MWE).", "labels": [], "entities": []}, {"text": "It is therefore neither obvious nor trivial to decide which tokens and groups of tokens should be selected as targets for SNACS annotation.", "labels": [], "entities": [{"text": "SNACS annotation", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.9237953424453735}]}, {"text": "To facilitate both manual annotation and automatic classification, we developed heuristics for identifying annotation targets.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.5148567259311676}]}, {"text": "The algorithm first scans the sentence for known multiword expressions, using a blacklist of non-prepositional MWEs that contain preposition tokens (e.g., take_care_of ) and a whitelist of prepositional MWEs (multiword prepositions like out_of and PP idioms like in_town).", "labels": [], "entities": []}, {"text": "Both lists were constructed from the training data.", "labels": [], "entities": []}, {"text": "From segments unaffected by the MWE heuristics, single-word candidates are identified by matching a high-recall set of parts of speech, then filtered through 5 different heuristics for adpositions, possessives, subordinating conjunctions, adverbs, and infinitivals.", "labels": [], "entities": []}, {"text": "Most of these filters are based on lexical lists learned from the training portion of the STREUSLE corpus, but there are some specific rules for infinitivals that handle forsubjects (I opened the door for Steve to takeout the trash-to, but not for, should receive a supersense) and comparative constructions with too and enough (too short to ride).", "labels": [], "entities": [{"text": "STREUSLE corpus", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.8256948888301849}]}], "tableCaptions": [{"text": " Table 1: Counts for the data splits used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Most and least frequent role and function labels.", "labels": [], "entities": []}, {"text": " Table 3: Interannotator agreement rates (pairwise averages)  on Little Prince sample (216 tokens) with different levels of  hierarchy coarsening according to", "labels": [], "entities": [{"text": "Little Prince sample", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.9716216723124186}]}, {"text": " Table 4: Target identification results for disambiguation.", "labels": [], "entities": [{"text": "Target identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.6711657792329788}, {"text": "disambiguation", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.980728805065155}]}, {"text": " Table 5: Overall performance of SNACS disambiguation systems on the test set. Results are reported for the role supersense  (Role), the function supersense (Func.), and their conjunction (Full). All figures are percentages. Left: Accuracies with gold  standard target identification (480 targets). Right: Precision, recall, and F 1 with automatic target identification ( \u00a76.2 and table 4).", "labels": [], "entities": [{"text": "SNACS disambiguation", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8929491639137268}, {"text": "Accuracies", "start_pos": 231, "end_pos": 241, "type": "METRIC", "confidence": 0.9909201860427856}, {"text": "Precision", "start_pos": 306, "end_pos": 315, "type": "METRIC", "confidence": 0.9508081078529358}, {"text": "recall", "start_pos": 317, "end_pos": 323, "type": "METRIC", "confidence": 0.9986053109169006}, {"text": "F 1", "start_pos": 329, "end_pos": 332, "type": "METRIC", "confidence": 0.9903152585029602}]}]}