{"title": [{"text": "Compositional Representation of Morphologically-Rich Input for Neural Machine Translation", "labels": [], "entities": [{"text": "Compositional Representation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7876847386360168}, {"text": "Neural Machine Translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7228015263875326}]}], "abstractContent": [{"text": "Neural machine translation (NMT) models are typically trained with fixed-size input and output vocabularies, which creates an important bottleneck on their accuracy and generalization capability.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8334562182426453}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9980979561805725}]}, {"text": "As a solution , various studies proposed segmenting words into sub-word units and performing translation at the sub-lexical level.", "labels": [], "entities": []}, {"text": "However , statistical word segmentation methods have recently shown to be prone to morphological errors, which can lead to inaccurate translations.", "labels": [], "entities": [{"text": "statistical word segmentation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6329399744669596}]}, {"text": "In this paper, we propose to overcome this problem by replacing the source-language embedding layer of NMT with a bi-directional recurrent neural network that generates compo-sitional representations of the input at any desired level of granularity.", "labels": [], "entities": []}, {"text": "We test our approach in a low-resource setting with five languages from different morphological typologies, and under different composition assumptions.", "labels": [], "entities": []}, {"text": "By training NMT to compose word representations from character trigrams, our approach consistently outperforms (from 1.71 to 2.48 BLEU points) NMT learning embeddings of statistically generated sub-word units.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9982931017875671}]}], "introductionContent": [{"text": "An important problem in neural machine translation (NMT) is translating infrequent or unseen words.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 24, "end_pos": 56, "type": "TASK", "confidence": 0.8515645364920298}, {"text": "translating infrequent or unseen words", "start_pos": 60, "end_pos": 98, "type": "TASK", "confidence": 0.8466399669647217}]}, {"text": "The reasons are twofold: the necessity of observing many examples of a word until its input representation (embedding) becomes reliable, and the computational requirement of limiting the input and output vocabularies to few tens of thousands of words.", "labels": [], "entities": []}, {"text": "These requirements eventually lead to coverage issues when dealing with lowresource and/or morphologically-rich languages, due to their high lexical sparseness.", "labels": [], "entities": []}, {"text": "To cope with this well-known problem, several approaches have been proposed redefining the model vocabulary in terms of interior orthographic units compounding the words, ranging from character ngrams ( to statistically-learned sub-word units.", "labels": [], "entities": []}, {"text": "While the former provide an ideal open vocabulary solution, they mostly failed to achieve competitive results.", "labels": [], "entities": []}, {"text": "This might be related to the semantic ambiguity caused by solely relying on input representations based on character n-grams which are generally learned by disregarding any morphological information.", "labels": [], "entities": []}, {"text": "In fact, the second approach is now prominent and has established a pre-processing step for constructing a vocabulary of sub-word units before training the NMT model.", "labels": [], "entities": []}, {"text": "However, several studies have shown that segmenting words into sub-word units without preserving morpheme boundaries can lead to loss of semantic and syntactic information and, thus, inaccurate translations.", "labels": [], "entities": []}, {"text": "In this paper, we propose to improve the quality of input (source language) representations of rare words in NMT by augmenting its embedding layer with a bi-directional recurrent neural network (bi-RNN), which can learn compositional input representations at different levels of granularity.", "labels": [], "entities": []}, {"text": "Compositional word embeddings have recently been applied in language modeling and obtained successful results.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7204332649707794}]}, {"text": "The apparent advantage of our approach is that by feeding NMT with simple character n-grams, our bi-RNN can potentially learn the morphology necessary to create word-level representations of the in-put language directly at training time, thus, avoiding the burden of a separate and sub-optimal word segmentation step.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 294, "end_pos": 311, "type": "TASK", "confidence": 0.736993670463562}]}, {"text": "We compare our approach against conventional embedding-based representations learned from statistical word segmentation in a public evaluation benchmark, which provides low-resource training conditions by pairing English with five morphologically-rich languages: Arabic, Czech, German, Italian and Turkish, where each language represents a distinct morphological typology and language family.", "labels": [], "entities": [{"text": "statistical word segmentation", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6903267502784729}]}, {"text": "The experimental results show that our compositional input representations lead to significantly and consistently better translation quality in all language directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our approach along with statistical word segmentation based open vocabulary NMT methods in an evaluation benchmark simulating a lowresource translation setting pairing English (En) with five languages from different language families and morphological typologies: Arabic (Ar), Czech (Cs), German (De), Italian (It) and Turkish (TR).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7478291988372803}]}, {"text": "The characteristics of each language are given in, whereas  The simple NMT model constitutes the baseline in our study and performs translation directly at the level of sub-word units, which can be of four different types: characters, character trigrams, BPE sub-word units, and LMVR sub-word units.", "labels": [], "entities": []}, {"text": "The compositional model, on the other hand, performs NMT with input representations composed from sub-lexical vocabulary units.", "labels": [], "entities": []}, {"text": "In our study, we evaluate representations composed from character trigrams, BPE, and LMVR units.", "labels": [], "entities": [{"text": "BPE", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.7637506127357483}]}, {"text": "In order to choose the segmentation method to apply on the English side (the output of NMT decoder), we compare BPE and LMVR sub-word units by carrying out an evaluation on the official data sets of Morpho Challenge 2010 2 (.", "labels": [], "entities": [{"text": "NMT decoder", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.944768637418747}, {"text": "BPE", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.8677576780319214}, {"text": "official data sets of Morpho Challenge 2010 2", "start_pos": 177, "end_pos": 222, "type": "DATASET", "confidence": 0.9149950072169304}]}, {"text": "The results of this evaluation, as given in, suggest that LMVR seems to provide a segmentation that is more consistent with morpheme boundaries, which motivates us to use sub-word tokens generated by LMVR for the target side.", "labels": [], "entities": []}, {"text": "This choice aids us in evaluating the morphological knowledge contained in input representations in terms of the translation accuracy in NMT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.8356977701187134}]}, {"text": "The compositional bi-RNN layer is implemented in and integrated into the Nematus NMT toolkit (.", "labels": [], "entities": [{"text": "Nematus NMT toolkit", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.8736179868380228}]}, {"text": "In our experiments, we use a compositional bi-RNN with 256 hidden units, an NMT model with a one-layer bi-directional GRU encoder and one-layer GRU decoder of 512 hidden units, and an embedding dimension of 256 for both models.", "labels": [], "entities": []}, {"text": "We use a highly restricted dictionary size of 30,000 for both source and target languages, and train the segmentation models (BPE and LMVR) to generate sub-word vocabularies of the same size.", "labels": [], "entities": [{"text": "BPE", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.8420961499214172}]}, {"text": "We train the NMT models using the) optimizer with a mini-batch size of 50, a learning rate of 0.01, and a dropout rate of 0.1 (in all layers and embeddings).", "labels": [], "entities": [{"text": "learning rate", "start_pos": 77, "end_pos": 90, "type": "METRIC", "confidence": 0.9455048739910126}]}, {"text": "In order to prevent over-fitting, we stop training if the perplexity on the validation does not decrease for 5 epochs, and use the best model to translate the test set.", "labels": [], "entities": []}, {"text": "The model outputs are evaluated using the (case-sensitive) BLEU () metric and the", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9872766137123108}]}], "tableCaptions": [{"text": " Table 3: The performance of different segmenta- tion models trained on the English portion of our  benchmark in the Morpho Challenge shared task.", "labels": [], "entities": []}, {"text": " Table 4: Experiment results. Best scores for each translation direction are in bold font. All improvements  over the baseline (simple model with BPE) are statistically significant (p-value < 0.05).", "labels": [], "entities": [{"text": "BPE", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.9700424671173096}]}]}