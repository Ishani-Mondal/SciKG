{"title": [{"text": "NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing", "labels": [], "entities": [{"text": "NASH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8939693570137024}]}], "abstractContent": [{"text": "Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems.", "labels": [], "entities": [{"text": "fast similarity search", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.6298764149347941}]}, {"text": "While fairly successful, previous techniques generally require two-stage training , and the binary constraints are handled ad-hoc.", "labels": [], "entities": []}, {"text": "In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables.", "labels": [], "entities": [{"text": "Semantic Hashing (NASH)", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.7933535218238831}]}, {"text": "A neural variational inference framework is proposed for training , where gradients are directly back-propagated through the discrete latent variable to optimize the hash function.", "labels": [], "entities": []}, {"text": "We also draw connections between proposed method and rate-distortion theory , which provides a theoretical foundation for the effectiveness of the proposed framework.", "labels": [], "entities": []}, {"text": "Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsuper-vised and supervised scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of similarity search, also called nearest-neighbor search, consists of finding documents from a large collection of documents, or corpus, which are most similar to a query document of interest.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.6814451366662979}]}, {"text": "Fast and accurate similarity search is at the core of many information retrieval applications, such as plagiarism analysis, collaborative filtering, content-based multimedia retrieval () and caching ().", "labels": [], "entities": [{"text": "similarity search", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6132315993309021}, {"text": "information retrieval", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7161466926336288}, {"text": "plagiarism analysis", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7382520884275436}, {"text": "content-based multimedia retrieval", "start_pos": 149, "end_pos": 183, "type": "TASK", "confidence": 0.6463622748851776}]}, {"text": "Semantic hashing is an effective approach for fast similarity search; Zhang * Equal contribution.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7004555761814117}]}, {"text": "By representing every document in the corpus as a similaritypreserving discrete (binary) hashing code, the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing codes, i.e., the number of bits that are different between two codes.", "labels": [], "entities": []}, {"text": "Given that today, an ordinary PC is able to execute millions of Hamming distance computations in just a few milliseconds (, this semantic hashing strategy is very computationally attractive.", "labels": [], "entities": []}, {"text": "While considerable research has been devoted to text (semantic) hashing, existing approaches typically require two-stage training procedures.", "labels": [], "entities": [{"text": "text (semantic) hashing", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6824562966823577}]}, {"text": "These methods can be generally divided into two categories: (i) binary codes for documents are first learned in an unsupervised manner, then l binary classifiers are trained via supervised learning to predict the l-bit hashing code (; (ii) continuous text representations are first inferred, which are binarized as a second (separate) step during testing ().", "labels": [], "entities": []}, {"text": "Because the model parameters are not learned in an end-to-end manner, these two-stage training strategies may result in suboptimal local optima.", "labels": [], "entities": []}, {"text": "This happens because different modules within the model are optimized separately, preventing the sharing of information between them.", "labels": [], "entities": []}, {"text": "Further, in existing methods, binary constraints are typically handled adhoc by truncation, i.e., the hashing codes are obtained via direct binarization from continuous representations after training.", "labels": [], "entities": []}, {"text": "As a result, the information contained in the continuous representations is lost during the (separate) binarization process.", "labels": [], "entities": []}, {"text": "Moreover, training different modules (mapping and classifier/binarization) separately often requires additional hyperparameter tuning for each training stage, which can be laborious and timeconsuming.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple and generic neural architecture for text hashing that learns binary latent codes for documents in an end-toend manner.", "labels": [], "entities": [{"text": "text hashing", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.7508932650089264}]}, {"text": "Inspired by recent advances in neural variational inference (NVI) for text processing (), we approach semantic hashing from a generative model perspective, where binary (hashing) codes are represented as either deterministic or stochastic Bernoulli latent variables.", "labels": [], "entities": [{"text": "neural variational inference (NVI)", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.6742314149936041}]}, {"text": "The inference (encoder) and generative (decoder) networks are optimized jointly by maximizing a variational lower bound to the marginal distribution of input documents (corpus).", "labels": [], "entities": []}, {"text": "By leveraging a simple and effective method to estimate the gradients with respect to discrete (binary) variables, the loss term from the generative (decoder) network can be directly backpropagated into the inference (encoder) network to optimize the hash function.", "labels": [], "entities": []}, {"text": "Motivated by the rate-distortion theory, we propose to inject data-dependent noise into the latent codes during the decoding stage, which adaptively accounts for the tradeoff between minimizing rate (number of bits used, or effective code length) and distortion (reconstruction error) during training.", "labels": [], "entities": [{"text": "distortion (reconstruction error)", "start_pos": 251, "end_pos": 284, "type": "METRIC", "confidence": 0.8311231970787049}]}, {"text": "The connection between the proposed method and ratedistortion theory is further elucidated, providing a theoretical foundation for the effectiveness of our framework.", "labels": [], "entities": []}, {"text": "Summarizing, the contributions of this paper are: (i) to the best of our knowledge, we present the first semantic hashing architecture that can be trained in an end-to-end manner; (ii) we propose a neural variational inference framework to learn compact (regularized) binary codes for documents, achieving promising results on both unsupervised and supervised text hashing; (iii) the connection between our method and rate-distortion theory is established, from which we demonstrate the advantage of injecting data-dependent noise into the latent variable during training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the following three standard publicly available datasets for training and evaluation: (i) Reuters21578, containing 10,788 news documents, which have been classified into 90 different categories.", "labels": [], "entities": [{"text": "Reuters21578", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9477620720863342}]}, {"text": "(ii) 20Newsgroups, a collection of 18,828 newsgroup documents, which are categorized into 20 different topics.", "labels": [], "entities": []}, {"text": "(iii) TMC (stands for SIAM text mining competition), containing air traffic reports provided by NASA.", "labels": [], "entities": [{"text": "TMC", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.6802978515625}, {"text": "SIAM text mining competition)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8081861853599548}]}, {"text": "TMC consists 21,519 training documents divided into 22 different categories.", "labels": [], "entities": [{"text": "TMC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6678889393806458}]}, {"text": "To make direct comparison with prior works, we employed the TFIDF features on these datasets supplied by, where the vocabulary sizes for the three datasets are set to 10,000, 7,164 and 20,000, respectively.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9326776266098022}]}, {"text": "To evaluate the hashing codes for similarity search, we consider each document in the testing set as a query document.", "labels": [], "entities": [{"text": "similarity search", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.784615159034729}]}, {"text": "Similar documents to the query in the corresponding training set need to be retrieved based on the Hamming distance of their hashing codes, i.e. number of different bits.", "labels": [], "entities": []}, {"text": "To facilitate comparison with prior work (, the performance is measured with precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9989771842956543}]}, {"text": "Specifically, during testing, fora query document, we first retrieve the 100 nearest/closest documents according to the Hamming distances of the corresponding hash codes (i.e., the number of different bits).", "labels": [], "entities": []}, {"text": "We then examine the percentage of documents among these 100 retrieved ones that belong to the same label (topic) with the query document (we consider documents having the same label as relevant pairs).", "labels": [], "entities": []}, {"text": "The ratio of the number of relevant documents to the number of retrieved documents (fixed value of 100) is calculated as the precision score.", "labels": [], "entities": [{"text": "precision score", "start_pos": 125, "end_pos": 140, "type": "METRIC", "confidence": 0.9804267585277557}]}, {"text": "The precision scores are further averaged overall test (query) documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995150566101074}]}, {"text": "We experimented with four variants for our NASH model: (i) NASH: with deterministic decoder; (ii) NASH-N: with fixed random noise injected to decoder; (iii) NASH-DN: with data-dependent noise injected to decoder; (iv) NASH-DN-S: NASH-DN with supervised information during training.", "labels": [], "entities": []}, {"text": "presents the results of all models on Reuters dataset.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9801236093044281}]}, {"text": "Regarding unsupervised semantic hashing, all the NASH variants consistently outperform the baseline methods by a substantial margin, indicating that our model makes the most effective use of unlabeled data and manage to assign similar hashing codes, i.e., with small Hamming distance to each other, to documents that belong to the same label.", "labels": [], "entities": []}, {"text": "It can be also observed that the injection of noise into the decoder networks has improved the robustness of learned binary representations, resulting in better retrieval performance.", "labels": [], "entities": []}, {"text": "More importantly, by making the variances of noise adaptive to the specific input, our NASH-DN achieves even better results, compared with NASH-N, highlighting the importance of exploring/learning the trade-off between rate and distortion objectives by the data itself.", "labels": [], "entities": [{"text": "NASH-DN", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.8922708034515381}]}, {"text": "We observe the same trend and superiority of our NASH-DN models on the other two benchmarks, as shown in.", "labels": [], "entities": []}, {"text": "Another observation is that the retrieval results tend to drop a bit when we set the length of hashing codes to be 64 or larger, which also happens for some baseline models.", "labels": [], "entities": []}, {"text": "This phenomenon has been reported previously in ;;;, and the reasons could be twofold: (i) for longer codes, the number of data points that are assigned to a certain binary code decreases exponentially.", "labels": [], "entities": []}, {"text": "As a result, many queries may fail to return any neighbor documents ( ; (ii) considering the size of training data, it is likely that the model may overfit with long hash codes.", "labels": [], "entities": []}, {"text": "However, even with longer hashing codes,     our NASH models perform stronger than the baselines inmost cases (except for the 20Newsgroups dataset), suggesting that NASH can effectively allocate documents to informative/meaningful hashing codes even with limited training data.", "labels": [], "entities": [{"text": "20Newsgroups dataset", "start_pos": 126, "end_pos": 146, "type": "DATASET", "confidence": 0.9520855247974396}]}, {"text": "We also evaluate the effectiveness of NASH in a supervised scenario on the Reuters dataset, where the label or topic information is utilized during training.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.9799240827560425}]}, {"text": "As shown in, our NASH-DN-S model consistently outperforms several supervised semantic hashing baselines, with various choices of hashing bits.", "labels": [], "entities": []}, {"text": "Notably, our model exhibits higher Top-100 retrieval precision than VDSH-S and VDSH-SP, proposed by.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.8611252903938293}, {"text": "VDSH-S", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.85996413230896}, {"text": "VDSH-SP", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.865541398525238}]}, {"text": "This maybe attributed to the fact that in VDSH models, the continuous embeddings are not optimized with their future binarization in mind, and thus could hurt the relevance of learned binary codes.", "labels": [], "entities": []}, {"text": "On the contrary, our model is optimized in an end-to-end manner, where the gradients are directly backpropagated to the inference network (through the binary/discrete latent variable), and thus gives rise to a more robust hash function.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision of the top 100 retrieved docu- ments on Reuters dataset (Unsupervised hashing).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9841098189353943}, {"text": "Reuters dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9745198786258698}]}, {"text": " Table 2: The five nearest words in the semantic space learned by NASH, compared with the results from  NVDM (Miao et al., 2016).", "labels": [], "entities": [{"text": "NASH", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9286503195762634}, {"text": "NVDM", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.8951970934867859}]}, {"text": " Table 4: Precision of the top 100 retrieved docu- ments on TMC dataset.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9901384115219116}, {"text": "TMC dataset", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9713335335254669}]}, {"text": " Table 5: Examples of learned compact hashing codes on 20Newsgroups dataset.", "labels": [], "entities": [{"text": "20Newsgroups dataset", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.972192108631134}]}, {"text": " Table 6: Ablation study with different en- coder/decoder networks.", "labels": [], "entities": []}]}