{"title": [{"text": "Tackling the Story Ending Biases in The Story Cloze Test", "labels": [], "entities": [{"text": "Tackling the Story Ending Biases", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9083637237548828}, {"text": "Story Cloze", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.8083215057849884}]}], "abstractContent": [{"text": "The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning.", "labels": [], "entities": [{"text": "script learning", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.8747891187667847}]}, {"text": "There have been a variety of models tackling the SCT so far.", "labels": [], "entities": [{"text": "SCT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.6794134378433228}]}, {"text": "Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leverag-ing human-authorship biases discovered in the SCT dataset.", "labels": [], "entities": [{"text": "SCT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9645568132400513}, {"text": "narrative understanding", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.715301901102066}, {"text": "SCT dataset", "start_pos": 308, "end_pos": 319, "type": "DATASET", "confidence": 0.7345753759145737}]}, {"text": "In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task.", "labels": [], "entities": []}, {"text": "Given the statistics we have ag-gregated, we have designed anew crowd-sourcing scheme that creates anew SCT dataset, which overcomes some of the biases.", "labels": [], "entities": [{"text": "SCT dataset", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.8944022953510284}]}, {"text": "We benchmark a few models on the new dataset and show that the top-performing model on the original SCT dataset fails to keep up its performance.", "labels": [], "entities": [{"text": "SCT dataset", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.8008988201618195}]}, {"text": "Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Story comprehension has been one of the longestrunning ambitions in artificial intelligence.", "labels": [], "entities": [{"text": "Story comprehension", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8621910512447357}]}, {"text": "One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested.", "labels": [], "entities": []}, {"text": "introduced the Story Cloze Test (SCT) evaluation framework to address * This work was performed at this issue.", "labels": [], "entities": []}, {"text": "This test evaluates a story comprehension system where the system is given a foursentence short story as the 'context' and two alternative endings and to the story, labeled 'right ending' and 'wrong ending.'", "labels": [], "entities": []}, {"text": "Then, the system's task is to choose the right ending.", "labels": [], "entities": []}, {"text": "In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "ROC Stories dataset", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.866750160853068}]}, {"text": "Each story follows a character through a fairly simple series of events to a conclusion.", "labels": [], "entities": []}, {"text": "Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines ( for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.", "labels": [], "entities": [{"text": "tackling the task", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.8680697480837504}]}, {"text": "A variety of new systems were proposed) as apart of the first shared task on SCT at LSDSem'17 workshop (.", "labels": [], "entities": [{"text": "SCT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.7016663551330566}, {"text": "LSDSem'17 workshop", "start_pos": 84, "end_pos": 102, "type": "DATASET", "confidence": 0.7290636599063873}]}, {"text": "Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (), discarding the narrative context.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9994686245918274}]}, {"text": "Clearly, this success does not seem to reflect the intent of the original task, where the systems should leverage narrative understanding as opposed to the statistical biases in the data.", "labels": [], "entities": []}, {"text": "In this paper, we study the effect of such biases between the ending choices and present anew scheme to reduce such stylistic artifacts.", "labels": [], "entities": []}, {"text": "The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop anew strong classifier for tackling the SCT that uses a variety", "labels": [], "entities": [{"text": "SCT dataset", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.8301191329956055}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The mean value for the 'right endings' and the 'wrong endings' for the two sample T-tests  conducted for each feature.", "labels": [], "entities": []}, {"text": " Table 3: Classification results on SCT-v1.0 using  each of the feature sets designated in the columns.", "labels": [], "entities": [{"text": "SCT-v1.0", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.6048281192779541}]}, {"text": " Table 5: Standard deviation of the word and char- acter n-gram counts, as well as the part of speech  (POS) counts, between the right and wrong end- ings.", "labels": [], "entities": []}, {"text": " Table 6: Classification accuracy for various mod- els on the SCT-v1.0 and SCT-v1.5 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9611949920654297}, {"text": "SCT-v1.0", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9153369069099426}, {"text": "SCT-v1.5 datasets", "start_pos": 75, "end_pos": 92, "type": "DATASET", "confidence": 0.844542533159256}]}]}