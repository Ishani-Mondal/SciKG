{"title": [{"text": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement", "labels": [], "entities": [{"text": "Evaluating neural network explanation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7776445150375366}]}], "abstractContent": [{"text": "The behavior of deep neural networks (DNNs) is hard to understand.", "labels": [], "entities": []}, {"text": "This makes it necessary to explore post hoc explanation methods.", "labels": [], "entities": []}, {"text": "We conduct the first comprehensive evaluation of explanation methods for NLP.", "labels": [], "entities": []}, {"text": "To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems.", "labels": [], "entities": []}, {"text": "Both paradigms require no manual annotation and are therefore broadly applicable.", "labels": [], "entities": []}, {"text": "We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP.", "labels": [], "entities": [{"text": "LIMSSE", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.8639593124389648}]}, {"text": "We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.", "labels": [], "entities": [{"text": "LIMSSE", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.7839163541793823}]}], "introductionContent": [{"text": "DNNs are complex models that combine linear transformations with different types of nonlinearities.", "labels": [], "entities": []}, {"text": "If the model is deep, i.e., has many layers, then its behavior during training and inference is notoriously hard to understand.", "labels": [], "entities": []}, {"text": "This is a problem for both scientific methodology and real-world deployment.", "labels": [], "entities": []}, {"text": "Scientific methodology demands that we understand our models.", "labels": [], "entities": []}, {"text": "In the real world, a decision (e.g., \"your blog post is offensive and has been removed\") by itself is often insufficient; in addition, an explanation of the decision maybe required (e.g., \"our system flagged the following words as offensive\").", "labels": [], "entities": []}, {"text": "The European Union plans to mandate that intelligent systems used for sensitive applications provide such explanations (European General Data Protection Regulation, expected 2018, cf. Goodman and Flaxman).", "labels": [], "entities": [{"text": "European General Data Protection Regulation", "start_pos": 120, "end_pos": 163, "type": "DATASET", "confidence": 0.8717039108276368}]}, {"text": "A number of post hoc explanation methods for DNNs have been proposed.", "labels": [], "entities": []}, {"text": "Due to the complexity of the DNNs they explain, these methods are necessarily approximations and come with their own sources of error.", "labels": [], "entities": []}, {"text": "At this point, it is not clear which of these methods to use when reliable explanations fora specific DNN architecture are needed.", "labels": [], "entities": []}, {"text": "(i) A task method solves an NLP problem, e.g., a GRU that predicts sentiment.", "labels": [], "entities": [{"text": "predicts sentiment", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7371910214424133}]}, {"text": "(ii) An explanation method explains the behavior of a task method on a specific input.", "labels": [], "entities": []}, {"text": "For our purpose, it is a function \u03c6(t, k, X) that assigns real-valued relevance scores fora target class k (e.g., positive) to positions tin an input text X (e.g., \"great food\").", "labels": [], "entities": []}, {"text": "For this example, an explanation method might assign: \u03c6(1, k, X) > \u03c6(2, k, X).", "labels": [], "entities": []}, {"text": "(iii) An (explanation) evaluation paradigm quantitatively evaluates explanation methods fora task method, e.g., by assigning them accuracies.", "labels": [], "entities": []}, {"text": "(i) We present novel evaluation paradigms for explanation methods for two classes of common NLP tasks (see \u00a72).", "labels": [], "entities": []}, {"text": "Crucially, neither paradigm requires manual annotations and our methodology is therefore broadly applicable.", "labels": [], "entities": []}, {"text": "(ii) Using these paradigms, we perform a comprehensive evaluation of explanation methods for NLP ( \u00a73).", "labels": [], "entities": []}, {"text": "We cover the most important classes of task methods, RNNs and CNNs, as well as the recently proposed Quasi-RNNs.", "labels": [], "entities": []}, {"text": "(iii) We introduce LIMSSE ( \u00a73.6), an explanation method inspired by LIME (", "labels": [], "entities": [{"text": "LIMSSE", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.7912335395812988}]}], "datasetContent": [{"text": "The assumptions made by our automatic evaluation paradigms have exceptions: (i) the correlation between fragment of origin and relevance does not always hold (e.g., a positive review may contain negative fragments, and will almost certainly contain neutral fragments); (ii) in morphological prediction, we cannot always expect the subject to be the only predictor for number.", "labels": [], "entities": [{"text": "morphological prediction", "start_pos": 277, "end_pos": 301, "type": "TASK", "confidence": 0.76975879073143}]}, {"text": "for example, \"few\" is a reasonable clue for plural despite not being a noun.", "labels": [], "entities": []}, {"text": "This imperfect ground truth means that absolute pointing game accuracies should betaken with a grain of salt; but we argue that this does not invalidate them for comparisons.", "labels": [], "entities": []}, {"text": "We also point out that there are characteristics of explanations that maybe desirable but are not reflected by the pointing game..", "labels": [], "entities": []}, {"text": "Both explanations get hit points, but the lrp explanation appears \"cleaner\" than limsse ms p , with relevance concentrated on fewer tokens.", "labels": [], "entities": []}, {"text": "For the hybrid document experiment, we use the 20 newsgroups corpus (topic classification) and reviews from the 10th yelp dataset challenge (binary sentiment analysis)  In all five architectures, the resulting document representation is projected to 20 (resp.", "labels": [], "entities": [{"text": "yelp dataset challenge", "start_pos": 117, "end_pos": 139, "type": "DATASET", "confidence": 0.8431146542231241}]}, {"text": "two) dimensions using a fully connected layer, followed by a softmax.", "labels": [], "entities": []}, {"text": "See supplementary material for details on training and regularization.", "labels": [], "entities": [{"text": "regularization", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.9096242189407349}]}, {"text": "After training, we sentence-tokenize the test sets, shuffle the sentences, concatenate ten sentences at a time and classify the resulting hybrid documents.", "labels": [], "entities": []}, {"text": "Documents that are assigned a class that is not the gold label of at least one constituent word are discarded (yelp: < 0.1%; 20 newsgroups: 14% -20%).", "labels": [], "entities": [{"text": "yelp", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.8818848729133606}]}, {"text": "On the remaining documents, we use the explanation methods from \u00a73 to find the maximally relevant word for each prediction.", "labels": [], "entities": []}, {"text": "The random baseline samples the maximally relevant word from a uniform distribution.", "labels": [], "entities": []}, {"text": "For reference, we also evaluate on a human judgment benchmark (Mohseni and Ragan (2018),: Pointing game accuracies in hybrid document experiment (left), on manually annotated benchmark (middle) and in morphosyntactic agreement experiment (right).", "labels": [], "entities": []}, {"text": "hit target (resp. hit feat ): maximal relevance on subject (resp. on noun with the predicted number feature).", "labels": [], "entities": []}, {"text": "Bold: top explanation method.", "labels": [], "entities": []}, {"text": "Underlined: within 5 points of top explanation method.", "labels": [], "entities": []}, {"text": "188 documents from the 20 newsgroups test set (classes sci.med and sci.electronics), with one manually created list of relevant words per document.", "labels": [], "entities": []}, {"text": "We discard documents that are incorrectly classified (20% -27%) and define: hit(\u03c6, X) = I[rmax(X, \u03c6) \u2208 gt(X)], where gt(X) is the manual ground truth.", "labels": [], "entities": []}, {"text": "For the morphosyntactic agreement experiment, we use automatically annotated English Wikipedia sentences by  The gold label of a sentence is the number of its verb, i.e., y(X) = feat(X, T + 1).", "labels": [], "entities": []}, {"text": "www.tallinzen.net/media/rnn_ agreement/agr_50_mostcommon_10K.tsv.gz As task methods, we replicate's unidirectional LSTM (R 50 randomly initialized word embeddings, hidden size 50).", "labels": [], "entities": []}, {"text": "We also train unidirectional GRU, QGRU and QLSTM architectures with the same dimensionality.", "labels": [], "entities": [{"text": "QGRU", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.830350935459137}]}, {"text": "We use the explanation methods from \u00a73 to find the most relevant word for predictions on the test set.", "labels": [], "entities": []}, {"text": "As described in \u00a72.2, explanation methods are awarded a hit target (resp. hit feat ) point if this word is the subject (resp. a noun with the predicted number feature).", "labels": [], "entities": []}, {"text": "For reference, we use a random baseline as well as a baseline that assumes that the most relevant word directly precedes the verb.", "labels": [], "entities": []}, {"text": "According to's taxonomy of explanation evaluation paradigms, application-grounded paradigms test how well an explanation method helps real users solve real tasks (e.g., doctors judge automatic diagnoses); human-grounded paradigms rely on proxy tasks (e.g., humans rank task methods based on explanations); functionally-grounded paradigms work without human input, like our approach.) propose a functionally-grounded explanation evaluation paradigm for NLP where words in a correctly (resp.", "labels": [], "entities": []}, {"text": "incorrectly) classified document are deleted in descending (resp. ascending) order of relevance.", "labels": [], "entities": []}, {"text": "They assume that the fewer words must be deleted to reduce (resp. increase) accuracy, the better the explanations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9713748693466187}]}, {"text": "According to this metric, LRP ( \u00a73. An issue with the word deletion paradigm is that it uses syntactically broken inputs, which may introduce artefacts).", "labels": [], "entities": []}, {"text": "In our hybrid document paradigm, inputs are syntactically intact (though semantically incoherent at the document level); the morphosyntactic agreement paradigm uses unmodified inputs.", "labels": [], "entities": []}, {"text": "Another class of functionally-grounded evaluation paradigms interprets the performance of a secondary task method, on inputs that are derived from (or altered by) an explanation method, as a proxy for the quality of that explanation method.", "labels": [], "entities": []}, {"text": "build a rule-based classifier from the most relevant phrases in a corpus (task method: LSTM).", "labels": [], "entities": []}, {"text": "The classifier based on decomp ( \u00a73.4) outperforms the gradient-based classifier, which is inline with our results.", "labels": [], "entities": []}, {"text": "(2017a) build document representations by summing over word embeddings weighted by relevance scores (task method: CNN).", "labels": [], "entities": [{"text": "summing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9677785634994507}]}, {"text": "They show that K-nearest neighbor performs better on doc-ument representations derived with LRP than on those derived with grad L2 , which also matches our results.", "labels": [], "entities": []}, {"text": "condense documents by extracting top-K relevant sentences, and let the original task method (CNN) classify them.", "labels": [], "entities": []}, {"text": "The accuracy loss, relative to uncondensed documents, is smaller for grad dot than for heuristic baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995214939117432}]}, {"text": "In the domain of human-based evaluation paradigms, compare different variants of LIME ( \u00a73.6) by how well they help non-experts clean a corpus from words that lead to overfitting.", "labels": [], "entities": []}, {"text": "assess how well explanation methods help non-experts identify the more accurate out of two object recognition CNNs.", "labels": [], "entities": [{"text": "object recognition CNNs", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.7709096968173981}]}, {"text": "These experiments come closer to real use cases than functionally-grounded paradigms; however, they are less scalable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Pointing game accuracies in hybrid document experiment (left), on manually annotated bench- mark (middle) and in morphosyntactic agreement experiment (right). hit target (resp. hit feat ): maximal  relevance on subject (resp. on noun with the predicted number feature). Bold: top explanation method.  Underlined: within 5 points of top explanation method.", "labels": [], "entities": []}]}