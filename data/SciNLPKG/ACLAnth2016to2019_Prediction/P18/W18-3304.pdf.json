{"title": [{"text": "Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data", "labels": [], "entities": [{"text": "Convolutional Attention Networks", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8145899971326193}, {"text": "Multimodal Emotion Recognition from Speech and Text Data", "start_pos": 37, "end_pos": 93, "type": "TASK", "confidence": 0.7652079723775387}]}], "abstractContent": [{"text": "Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction.", "labels": [], "entities": [{"text": "Emotion recognition", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9463641345500946}]}, {"text": "Previous works involve unimodal analysis of emotion , while recent efforts focus on multi-modal emotion recognition from vision and speech.", "labels": [], "entities": [{"text": "unimodal analysis of emotion", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7408105134963989}, {"text": "multi-modal emotion recognition", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.697512666384379}]}, {"text": "In this paper, we propose anew method of learning about the hidden representations between just speech and text data using convolutional attention networks.", "labels": [], "entities": []}, {"text": "Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 201, "end_pos": 218, "type": "DATASET", "confidence": 0.9526717662811279}]}], "introductionContent": [{"text": "Emotion not only is a key driver to people's actions and thoughts, but also is a fundamental part of human communication.", "labels": [], "entities": []}, {"text": "As such, emotion recognition technology has become growingly important in improving how humans interact with machines.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8625301420688629}]}, {"text": "For instance, emotion recognition has been applied to analyze people's reactions to advertisements, thus creating better neuromarketing campaigns.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8129511475563049}]}, {"text": "It has also gained in popularity amongst various other domains such as healthcare, customer service, or gaming.", "labels": [], "entities": []}, {"text": "However, effective emotion recognition still remains a challenging task, due to the sheer complexity of generalizing human emotions.", "labels": [], "entities": [{"text": "emotion recognition", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7844383120536804}]}, {"text": "For example, individuals express and perceive emotions differently, depending on numerous personal characteristics such as but not limited to age, gender and race.", "labels": [], "entities": []}, {"text": "Previous efforts have used deep learning based approaches to analyze emotion from single mode of expression, such as facial expression or speech.", "labels": [], "entities": []}, {"text": "Since deep learning based approaches have been proven to be effective at learning and generalizing data with high-dimensional feature spaces like images, similar efforts to capture complex feature space of emotional data have also shown promising results with several emotion databases such as EmoDB or IEMOCAP.", "labels": [], "entities": []}, {"text": "Unfortunately, human emotion in real-life is often expressed through complex combination of multiple modes of expression, and a lot of information is lost by employing unimodal analysis.", "labels": [], "entities": []}, {"text": "To solve this problem, using deep learning based approaches for multimodal emotion recognition has been researched extensively in recent years.", "labels": [], "entities": [{"text": "multimodal emotion recognition", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.649433841307958}]}, {"text": "Work of Tzirakis et al. uses deep residual networks to extract features from facial expressions, convolutional neural networks to extract features from speech, and concatenates them to input into a LSTM network.", "labels": [], "entities": []}, {"text": "Work of Ranganathan et al. uses deep believe networks on facial expressions, body expressions, vocal expressions, and physiological signals.", "labels": [], "entities": []}, {"text": "Inspired by these approaches, we suggest anew approach to multimodal emotion recognition from just speech and text data.", "labels": [], "entities": [{"text": "multimodal emotion recognition", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7302637497584025}]}, {"text": "Feature vectors from embedded text sequences and speech spectrograms are extracted using convolutional neural network based architectures.", "labels": [], "entities": []}, {"text": "A direct way to learn about the relationship between these two ------------------------------------------------* Corresponding Author: cchoi@orbisai.co feature vectors would be to utilize a shallow model, which is a simple concatenation of two feature vectors.", "labels": [], "entities": []}, {"text": "However, since the correlations between feature vectors from speech and text is highly non-linear, it is difficult fora shallow model to properly learn multimodal representations.", "labels": [], "entities": []}, {"text": "Therefore, we utilize trainable attention mechanisms to learn nonlinear correlations between these feature vectors.", "labels": [], "entities": []}, {"text": "Attention mechanisms also help retain information in the timedomain by forming temporal embedding between two feature vectors.", "labels": [], "entities": []}, {"text": "Since speech features and context shares the same time domain, using attention mechanism may help to discover new information for emotion classification.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.7452090680599213}]}, {"text": "Attention models have previously been successfully applied to tasks such as image caption generation, machine translation, and speech recognition.", "labels": [], "entities": [{"text": "image caption generation", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.8442584276199341}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.8377554714679718}, {"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8255544602870941}]}, {"text": "To demonstrate the benefits of this new approach, we use it to classify emotions from speech and text data provided in the CMU-MOSEI dataset into six classes: happy, angry, sad, surprised, disgusted, and fear.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.9768829047679901}]}, {"text": "We also compare this approach to the shallow model approach to show how the attention mechanism can improve capturing of multimodal correlations between text and speech.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use audio and text data from CMUMultimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset for all experiments.", "labels": [], "entities": [{"text": "CMUMultimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset", "start_pos": 32, "end_pos": 105, "type": "DATASET", "confidence": 0.6070216327905655}]}, {"text": "The videos, totaling 23,141 files, are chosen from YouTube speakers including various topics and monologue, and are gender balanced.", "labels": [], "entities": []}, {"text": "Text embedding was prepared using GloVe word2vec method.", "labels": [], "entities": []}, {"text": "Each word embedding is fixed at a length of 300.", "labels": [], "entities": []}, {"text": "The duration of each word utterance is also provided by the P2FA forced alignment.", "labels": [], "entities": [{"text": "duration", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.972762405872345}]}, {"text": "In this section, we describe the experiment methodologies and report the recognition performance proposed attention network architecture on the CMU-MOSEI dataset.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 144, "end_pos": 161, "type": "DATASET", "confidence": 0.9816687405109406}]}, {"text": "For each experiment, we report an overall accuracy (each sentence across the dataset has an equal weight; weighted accuracy) and a class accuracy (first evaluated for each emotion and then averaged; unweighted accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9968847632408142}, {"text": "class accuracy", "start_pos": 131, "end_pos": 145, "type": "METRIC", "confidence": 0.770887017250061}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.600034236907959}]}, {"text": "All the classification results are listed in precision, recall, and f-1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.99981290102005}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9987963438034058}]}, {"text": "Confusion matrices are also provided to show how well the model correctly classifies each emotion, using the top-1 class prediction as a metric.", "labels": [], "entities": []}, {"text": "In this section, we report the results of training the shallow model with the CMU-MOSEI dataset.", "labels": [], "entities": [{"text": "CMU-MOSEI dataset", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9871852099895477}]}, {"text": "Since the shallow model is a common and the simplest method of multimodal emotion classification, we use it as a baseline model for comparison.", "labels": [], "entities": [{"text": "multimodal emotion classification", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.6547418832778931}]}, {"text": "The overall validation accuracy (weighted) is 83.11% and class validation accuracy (unweighted) is 77.23% as shown in.", "labels": [], "entities": [{"text": "validation", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.9405490159988403}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.8965373635292053}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.6066181659698486}]}, {"text": "The multi-class confusion matrix is shown in, showing the highest accuracies for anger and happy emotions, and lowest accuracies for fear and surprise emotions.", "labels": [], "entities": []}, {"text": "The results of shallow model  In this section, we report the results of attention model to compare to the baseline results.", "labels": [], "entities": []}, {"text": "The overall accuracy (weighted) is 88.89% and class accuracy (unweighted) is 84.08 % as shown in for the attention model, a significant improvement from the same metrics of shallow model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9950485825538635}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.5038847327232361}]}, {"text": "According to the confusion matrix shown in, validation accuracies have increased throughout all emotion classes compared to the baseline.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.6939706206321716}]}], "tableCaptions": [{"text": " Table 1 The results of shallow model", "labels": [], "entities": []}, {"text": " Table 2 The results of attention model", "labels": [], "entities": []}]}