{"title": [{"text": "Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders", "labels": [], "entities": []}], "abstractContent": [{"text": "Asking good questions in large-scale, open-domain conversational systems is quite significant yet rather untouched.", "labels": [], "entities": []}, {"text": "This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics.", "labels": [], "entities": [{"text": "question generation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7201528400182724}]}, {"text": "We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words.", "labels": [], "entities": []}, {"text": "Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence.", "labels": [], "entities": [{"text": "topic transition", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7016888111829758}]}, {"text": "We devise two typed decoders (soft typed de-coder and hard typed decoder) in which a type distribution over the three types is estimated and used to modulate the final generation distribution.", "labels": [], "entities": []}, {"text": "Extensive experiments show that the typed decoders out-perform state-of-the-art baselines and can generate more meaningful questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning to ask questions (or, question generation) aims to generate a question to a given input.", "labels": [], "entities": [{"text": "question generation)", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7875183721383413}]}, {"text": "Deciding what to ask and how is an indicator of machine understanding, as demonstrated in machine comprehension ( and question answering (.", "labels": [], "entities": [{"text": "machine understanding", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6952437460422516}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8162121772766113}]}, {"text": "Raising good questions is essential to conversational systems because a good system can well interact with users by asking and responding (.", "labels": [], "entities": [{"text": "Raising good questions", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8949064215024313}]}, {"text": "Furthermore, asking * Authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "\u2020 Corresponding author: Minlie Huang.", "labels": [], "entities": []}, {"text": "questions is one of the important proactive behaviors that can drive dialogues to go deeper and further (.", "labels": [], "entities": []}, {"text": "Question generation in open-domain conversational systems differs substantially from the traditional QG tasks.", "labels": [], "entities": [{"text": "Question generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8330468237400055}]}, {"text": "The ultimate goal of this task is to enhance the interactiveness and persistence of human-machine interactions, while for traditional QG tasks, seeking information through a generated question is the major purpose.", "labels": [], "entities": []}, {"text": "The response to a generated question will be supplied in the following conversations, which maybe novel but not necessarily occur in the input as that in traditional QG ().", "labels": [], "entities": []}, {"text": "Thus, the purpose of this task is to spark novel yet related information to drive the interactions to continue.", "labels": [], "entities": []}, {"text": "Due to the different purposes, this task is unique in two aspects: it requires to question not only in various patterns but also about diverse yet relevant topics.", "labels": [], "entities": []}, {"text": "First, there are various questioning patterns for the same input, such as Yes-no questions and Wh-questions with different interrogatives.", "labels": [], "entities": []}, {"text": "Diversified questioning patterns make dialogue interactions richer and more flexible.", "labels": [], "entities": []}, {"text": "Instead, traditional QG tasks can be roughly addressed by syntactic transformation), or implicitly modeled by neural models (.", "labels": [], "entities": []}, {"text": "In such tasks, the information questioned on is pre-specified and usually determines the pattern of questioning.", "labels": [], "entities": []}, {"text": "For instance, asking Whoquestion fora given person, or Where-question fora given location.", "labels": [], "entities": []}, {"text": "Second, this task requires to address much more transitional topics of a given input, which is the nature of conversational systems.", "labels": [], "entities": []}, {"text": "For instance, for the input \"I went to dinner with my friends\", we may question about topics such as friend, cuisine, price, place and taste.", "labels": [], "entities": []}, {"text": "Thus, this task generally requires scene understanding to imagine and comprehend a scenario (e.g., dining at a restaurant) that can be interpreted by topics related to the input.", "labels": [], "entities": []}, {"text": "However, in traditional QG tasks, the core information to be questioned on is pre-specified and rather static, and paraphrasing is more required.", "labels": [], "entities": []}, {"text": "Undoubtedly, asking good questions in conversational systems needs to address the above issues (questioning with diversified patterns, and addressing transitional topics naturally in a generated question).", "labels": [], "entities": []}, {"text": "As shown in, a good question is a natural composition of interrogatives, topic words, and ordinary words.", "labels": [], "entities": []}, {"text": "Interrogatives indicate the pattern of questioning, topic words address the key information of topic transition, and ordinary words play syntactical and grammatical roles in making a natural sentence.", "labels": [], "entities": [{"text": "topic transition", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.6917512267827988}]}, {"text": "We thus classify the words in a question into three types: interrogative, topic word, and ordinary word automatically.", "labels": [], "entities": []}, {"text": "We then devise two decoders, Soft Typed Decoder (STD) and Hard Typed Decoder (HTD), for question generation in conversational systems . STD deals with word types in a latent and implicit manner, while HTD in a more explicit way.", "labels": [], "entities": [{"text": "question generation", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.7799250483512878}]}, {"text": "At each decoding position, we firstly estimate a type distribution over word types.", "labels": [], "entities": []}, {"text": "STD applies a mixture of type-specific generation distributions where type probabilities are the coefficients.", "labels": [], "entities": [{"text": "STD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8050163984298706}]}, {"text": "By contrast, HTD reshapes the type distribution by Gumbel-softmax and modulates the generation distribution by type probabilities.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 To the best of our knowledge, this is the first study on question generation in the setting of conversational systems.", "labels": [], "entities": [{"text": "question generation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7350976914167404}]}, {"text": "We analyze the key differences between this new task and other traditional question generation tasks.", "labels": [], "entities": [{"text": "question generation tasks", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7956159313519796}]}, {"text": "\u2022 We devise soft and hard typed decoders to ask good questions by capturing different roles of different word types.", "labels": [], "entities": []}, {"text": "Such typed decoders maybe applicable to other generation tasks if word semantic types can be identified.", "labels": [], "entities": []}], "datasetContent": [{"text": "To estimate the probabilities in PMI, we collected about 9 million post-response pairs from Weibo.", "labels": [], "entities": [{"text": "PMI", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9564546942710876}, {"text": "Weibo", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9744360446929932}]}, {"text": "To train our question generation models, we distilled the pairs whereby the responses are in question form with the help of around 20 hand-crafted templates.", "labels": [], "entities": []}, {"text": "The templates contain a list of interrogatives and other implicit questioning patterns.", "labels": [], "entities": []}, {"text": "Such patterns detect sentences led by words like what, how many, how about or sentences ended with a question mark.", "labels": [], "entities": []}, {"text": "After that, we removed the pairs whose responses are universal questions that can be used to reply many different posts.", "labels": [], "entities": []}, {"text": "This is a simple yet effective way to avoid situations where the type probability distribution is dominated by interrogatives and ordinary words.", "labels": [], "entities": []}, {"text": "Ultimately, we obtained the dataset comprising about 491,000 post-response pairs.", "labels": [], "entities": []}, {"text": "We randomly selected 5,000 pairs for testing and another 5,000 for validation.", "labels": [], "entities": []}, {"text": "The average number of words in post/response is 8.3/9.3 respectively.", "labels": [], "entities": []}, {"text": "The dataset contains 66,547 different words, and 18,717 words appear more than 10 times.", "labels": [], "entities": []}, {"text": "The dataset is available at: http://coai.cs.tsinghua.edu.", "labels": [], "entities": []}, {"text": "cn/hml/dataset/.", "labels": [], "entities": []}, {"text": "Parameters were set as follows: we set the vocabulary size to 20, 000 and the dimension of word vectors as 100.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9567534923553467}]}, {"text": "The word vectors were pretrained with around 9 million post-response pairs from Weibo and were being updated during the training of the decoders.", "labels": [], "entities": [{"text": "Weibo", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9788081645965576}]}, {"text": "We applied the 4-layer GRU units (hidden states have 512 dimensions).", "labels": [], "entities": []}, {"text": "These settings were also applied to all the baselines.", "labels": [], "entities": []}, {"text": "We set different values of \u03c4 in Gumbel-softmax at different stages of training.", "labels": [], "entities": []}, {"text": "At the early stage, we set \u03c4 to a small value (0.6) to obtain a sharper reformed distribution (more like argmax).", "labels": [], "entities": [{"text": "argmax", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9895656108856201}]}, {"text": "After several steps, we set \u03c4 to a larger value (0.8) to apply a more smoothing distribution.", "labels": [], "entities": []}, {"text": "Our codes are available at: https://github.com/victorywys/ Learning2Ask_TypedDecoder.", "labels": [], "entities": [{"text": "victorywys", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.931566059589386}]}, {"text": "We conducted automatic evaluation over the 5, 000 test posts.", "labels": [], "entities": []}, {"text": "For each post, we obtained responses from the six models, and there are 30, 000 post-response pairs in total.", "labels": [], "entities": []}, {"text": "Each of the following metrics is evaluated independently on each pair-wise comparison: Appropriateness: measures whether a question is reasonable in logic and content, and whether it is questioning on the key information.", "labels": [], "entities": [{"text": "Appropriateness", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9979572296142578}]}, {"text": "Inappropriate questions are either irrelevant to the post, or have grammatical errors, or universal questions.", "labels": [], "entities": []}, {"text": "Richness: measures whether a response contains topic words that are relevant to a given post.", "labels": [], "entities": []}, {"text": "Willingness to respond: measures whether a user will respond to a generated question.", "labels": [], "entities": []}, {"text": "This metric is to justify how likely the generated questions can elicit further interactions.", "labels": [], "entities": []}, {"text": "If people are willing to respond, the interactions can go further.", "labels": [], "entities": []}, {"text": "We resorted to a crowdsourcing service for manual annotation.", "labels": [], "entities": []}, {"text": "500 posts were sampled for manual annotation . We conducted pair-wise comparison between two responses generated by two models for the same post.", "labels": [], "entities": []}, {"text": "In total, there are 4,500 pairs to be compared.", "labels": [], "entities": []}, {"text": "For each response pair, five judges were hired to give a preference between the two responses, in terms of the following three metrics.", "labels": [], "entities": []}, {"text": "Tie was allowed, and system identifiers were masked during annotation.", "labels": [], "entities": [{"text": "Tie", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9347148537635803}]}], "tableCaptions": [{"text": " Table 1. STD  and HTD perform fairly well with lower perplex- ities, higher distinct-1 and distinct-2 scores, and  remarkably better topical response ratio (TRR).  Note that MA has the lowest perplexity because  the model tends to generate more universal re- sponses.", "labels": [], "entities": [{"text": "topical response ratio (TRR)", "start_pos": 134, "end_pos": 162, "type": "METRIC", "confidence": 0.848997582991918}]}, {"text": " Table 1: Results of automatic evaluation.", "labels": [], "entities": []}, {"text": " Table 2: Annotation results. Win for \"A vs. B\" means A is better than B. Significance tests with Z-test  were conducted. Values marked with  *  means p-value < 0.05, and  *  *  for p-value < 0.01.", "labels": [], "entities": [{"text": "Annotation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9469857215881348}, {"text": "Win", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9949755668640137}]}, {"text": " Table 3: KL divergence between the questioning  pattern distribution by a model and that by human.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.751343160867691}]}]}