{"title": [{"text": "Connecting Language and Vision to Actions ACL 2018 Tutorial", "labels": [], "entities": [{"text": "Connecting Language and Vision to Actions ACL 2018 Tutorial", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.814695828490787}]}], "abstractContent": [{"text": "A long-term goal of AI research is to build intelligent agents that can seethe rich visual environment around us, communicate this understanding in natural language to humans and other agents, and act in a physical or embodied environment.", "labels": [], "entities": []}, {"text": "To this end, recent advances at the intersection of language and vision have made incredible progress-from being able to generate natural language descriptions of images/videos, to answering questions about them, to even holding free-form conversations about visual content!", "labels": [], "entities": []}, {"text": "However, while these agents can passively describe images or answer (a sequence of) questions about them, they cannot act in the world (what if I cannot answer a question from my current view, or I am asked to move or manipulate something?).", "labels": [], "entities": []}, {"text": "Thus, the challenge now is to extend this progress in language and vision to embodied agents that take actions and actively interact with their visual environments.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}