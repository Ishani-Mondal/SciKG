{"title": [], "abstractContent": [{"text": "In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.6335729360580444}, {"text": "repetition", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9508544206619263}]}, {"text": "To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context.", "labels": [], "entities": []}, {"text": "It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information.", "labels": [], "entities": []}, {"text": "Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of generating summary of higher quality and reducing repetition 1 .", "labels": [], "entities": [{"text": "LCSTS", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.9368186593055725}, {"text": "English Gigaword", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.9078292846679688}, {"text": "repetition", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.9944034814834595}]}], "introductionContent": [{"text": "Abstractive summarization can be regarded as a sequence mapping task that the source text should be mapped to the target summary.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5594789683818817}]}, {"text": "Therefore, sequence-to-sequence learning can be applied to neural abstractive summarization), whose model consists of an encoder and a decoder.", "labels": [], "entities": [{"text": "neural abstractive summarization", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.6118679145971934}]}, {"text": "Attention mechanism has been broadly used in seq2seq models where the decoder extracts information from the encoder based on the attention scores on the source-side information ().", "labels": [], "entities": []}, {"text": "Many attention-based seq2seq models have been proposed for abstractive summarization, which outperformed the conventional statistical methods.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8379606306552887}]}, {"text": "Text: the mainstream fatah movement on monday officially chose mahmoud abbas, chairman of the palestine liberation organization (plo), as its candidate to run for the presidential election due on jan.", "labels": [], "entities": [{"text": "monday", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9192296862602234}, {"text": "palestine liberation organization (plo)", "start_pos": 94, "end_pos": 133, "type": "TASK", "confidence": 0.7856651643911997}]}, {"text": "#, ####, the official wafa news agency reported.", "labels": [], "entities": []}, {"text": "seq2seq: fatah officially officially elects abbas as candidate for candidate . Gold: fatah officially elects abbas as candidate for presidential election: An example of the summary of the conventional attention-based seq2seq model on the Gigaword dataset.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 238, "end_pos": 254, "type": "DATASET", "confidence": 0.9835573434829712}]}, {"text": "The text highlighted indicates repetition, \"#\" refers to masked number.", "labels": [], "entities": [{"text": "repetition", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9922159314155579}]}, {"text": "However, recent studies show that there are salient problems in the attention mechanism.", "labels": [], "entities": []}, {"text": "pointed out that there is no obvious alignment relationship between the source text and the target summary, and the encoder outputs contain noise for the attention.", "labels": [], "entities": []}, {"text": "For example, in the summary generated by the seq2seq in, \"officially\" is followed by the same word, as the attention mechanism still attends to the word with high attention score.", "labels": [], "entities": []}, {"text": "Attention-based seq2seq model for abstractive summarization can suffer from repetition and semantic irrelevance, causing grammatical errors and insufficient reflection of the main idea of the source text.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.66816446185112}, {"text": "repetition", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9507043361663818}]}, {"text": "To tackle this problem, we propose a model of global encoding for abstractive summarization.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.5922134518623352}]}, {"text": "We set a convolutional gated unit to perform global encoding on the source context.", "labels": [], "entities": []}, {"text": "The gate based on convolutional neural network (CNN) filters each encoder output based on the global context due to the parameter sharing, so that the representations at each time step are refined with consideration of the global context.", "labels": [], "entities": []}, {"text": "We conduct experiments on LCSTS and Gigaword, two benchmark datasets for sentence summarization, which shows that our model outperforms the state-of-theart methods with ROUGE-2 F1 score 26.8 and 17.8 respectively.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.698363408446312}, {"text": "ROUGE-2 F1 score 26.8", "start_pos": 169, "end_pos": 190, "type": "METRIC", "confidence": 0.8505890369415283}]}, {"text": "Moreover, the analysis shows: Structure of our proposed Convolutional Gated Unit.", "labels": [], "entities": []}, {"text": "We implement 1-dimensional convolution with a structure similar to the Inception () over the outputs of the RNN encoder, where k refers to the kernel size. that our model is capable of reducing repetition compared with the seq2seq model.", "labels": [], "entities": [{"text": "Inception", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9452148675918579}, {"text": "repetition", "start_pos": 194, "end_pos": 204, "type": "METRIC", "confidence": 0.9926895499229431}]}], "datasetContent": [{"text": "In the following, we introduce the datasets that we conduct experiments on and our experiment settings as well as the baseline models that we compare with.", "labels": [], "entities": []}, {"text": "LCSTS is a large-scale Chinese short text summarization dataset collected from Sina Weibo, a famous Chinese social media website (, consisting of more than 2.4 million textsummary pairs.", "labels": [], "entities": [{"text": "LCSTS", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8379964828491211}, {"text": "Chinese short text summarization dataset collected from Sina Weibo", "start_pos": 23, "end_pos": 89, "type": "DATASET", "confidence": 0.738225562704934}]}, {"text": "The original texts are shorter than 140 Chinese characters, and the summaries are created manually.", "labels": [], "entities": []}, {"text": "We follow the previous research () to split the dataset for training, validation and testing, with 2.4M sentence pairs for training, 8K for validation and 0.7K for testing.", "labels": [], "entities": []}, {"text": "The English Gigaword is a sentence summarization dataset based on Annotated Gigaword (), a dataset consisting of sentence pairs, which are the first sentence of the collected news articles and the corresponding headlines.", "labels": [], "entities": [{"text": "sentence summarization", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.6615400612354279}]}, {"text": "We use the data preprocessed by with 3.8M sentence pairs for training, 8K for validation and 2K for testing.", "labels": [], "entities": []}, {"text": "We implement our experiments in PyTorch on an NVIDIA 1080Ti GPU.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9057618379592896}]}, {"text": "The word embedding dimension and the number of hidden units are both 512.", "labels": [], "entities": []}, {"text": "In both experiments, the batch size is set to 64.", "labels": [], "entities": []}, {"text": "We use Adam optimizer () with the default setting \u03b1 = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and = 1 \u00d7 10 \u22128 . The learning rate is halved every epoch.", "labels": [], "entities": []}, {"text": "Gradient clipping is applied with range.", "labels": [], "entities": [{"text": "range", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9807852506637573}]}, {"text": "Following the previous studies, we choose ROUGE score to evaluate the performance of our model (.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9724387228488922}]}, {"text": "ROUGE score is to  calculate the degree of overlapping between generated summary and reference, including the number of n-grams.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9646795690059662}]}, {"text": "F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L are used as the evaluation metrics.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9947858452796936}, {"text": "ROUGE-1", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9903950095176697}, {"text": "ROUGE-2", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.978149950504303}, {"text": "ROUGE-L", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9866529107093811}]}], "tableCaptions": [{"text": " Table 2: F-Score of ROUGE on LCSTS.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9984093308448792}, {"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.901171863079071}, {"text": "LCSTS", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9061982035636902}]}, {"text": " Table 3: F-Score of ROUGE on Gigaword.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9981168508529663}, {"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8417599201202393}, {"text": "Gigaword", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.8975489139556885}]}, {"text": " Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.", "labels": [], "entities": []}]}