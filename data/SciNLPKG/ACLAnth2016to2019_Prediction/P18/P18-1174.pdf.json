{"title": [{"text": "Learning How to Actively Learn: A Deep Imitation Learning Approach", "labels": [], "entities": [{"text": "Deep Imitation Learning Approach", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7461913377046585}]}], "abstractContent": [{"text": "Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary.", "labels": [], "entities": [{"text": "Heuristic-based active learning (AL)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6721585591634115}]}, {"text": "We introduce a method that learns an AL policy using imitation learning (IL).", "labels": [], "entities": []}, {"text": "Our IL-based approach makes use of an efficient and effective algorithmic expert, which provides the policy learner with good actions in the encountered AL situations.", "labels": [], "entities": []}, {"text": "The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints.", "labels": [], "entities": []}, {"text": "We evaluate our method on two different tasks: text classification and named entity recognition.", "labels": [], "entities": [{"text": "text classification", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8335970640182495}, {"text": "named entity recognition", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.6272246340910593}]}, {"text": "Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.", "labels": [], "entities": [{"text": "IL-based AL", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.7078320682048798}]}], "introductionContent": [{"text": "For many real-world NLP tasks, labeled data is rare while unlabelled data is abundant.", "labels": [], "entities": []}, {"text": "Active learning (AL) seeks to learn an accurate model with minimum amount of annotation cost.", "labels": [], "entities": [{"text": "Active learning (AL)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8258230805397033}]}, {"text": "It is inspired by the observation that a model can get better performance if it is allowed to choose the data points on which it is trained.", "labels": [], "entities": []}, {"text": "For example, the learner can identify the areas of the space where it does not have enough knowledge, and query those data points which bridge its knowledge gap.", "labels": [], "entities": []}, {"text": "Traditionally, AL is performed using engineered heuristics in order to estimate the usefulness of unlabeled data points as queries to an annotator.", "labels": [], "entities": [{"text": "AL", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9781845211982727}]}, {"text": "Recent work have focused on learning the AL querying strategy, as engineered heuristics are not flexible to exploit characteristics inherent to a given problem.", "labels": [], "entities": [{"text": "AL querying strategy", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8160249392191569}]}, {"text": "The basic idea is to cast AL as a decision process, where the most informative unlabeled data point needs to be selected based on the history of previous queries.", "labels": [], "entities": []}, {"text": "However, previous works train for the AL policy by a reinforcement learning (RL) formulation, where the rewards are provided at the end of sequences of queries.", "labels": [], "entities": []}, {"text": "This makes learning the AL policy difficult, as the policy learner needs to deal with the credit assignment problem.", "labels": [], "entities": []}, {"text": "Intuitively, the learner needs to observe many pairs of query sequences and the resulting end-rewards to be able to associate single queries with their utility scores.", "labels": [], "entities": []}, {"text": "In this work, we formulate learning AL strategies as an imitation learning problem.", "labels": [], "entities": []}, {"text": "In particular, we consider the popular pool-based AL scenario, where an AL agent is presented with a pool of unlabelled data.", "labels": [], "entities": []}, {"text": "Inspired by the Dataset Aggregation (DAGGER) algorithm), we develop an effective AL policy learning method by designing an efficient and effective algorithmic expert, which provides the AL agent with good decisions in the encountered states.", "labels": [], "entities": []}, {"text": "We then use a deep feedforward network to learn the AL policy to associate states to actions.", "labels": [], "entities": []}, {"text": "Unlike the RL approach, our method can get observations and actions directly from the expert's trajectory.", "labels": [], "entities": [{"text": "RL", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9623278379440308}]}, {"text": "Therefore, our trained policy can make better rankings of unlabelled datapoints in the pool, leading to more effective AL strategies.", "labels": [], "entities": []}, {"text": "We evaluate our method on text classification and named entity recognition.", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8323178589344025}, {"text": "named entity recognition", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6697307030359904}]}, {"text": "The results show our method performs better than strong AL methods using heuristics and reinforcement learning, in that it boosts the performance of the underlying model with fewer labelling queries.", "labels": [], "entities": []}, {"text": "An open source implementation of our model is available at: https://github.com/Grayming/ ALIL.", "labels": [], "entities": [{"text": "ALIL", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8376154899597168}]}], "datasetContent": [{"text": "We conduct experiments on text classification and named entity recognition (NER).", "labels": [], "entities": [{"text": "text classification", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8366848826408386}, {"text": "named entity recognition (NER)", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.7738721370697021}]}, {"text": "The AL scenarios include cross-domain sentiment classification, cross-lingual authorship profiling, and crosslingual named entity recognition (NER), whereby an AL policy trained on a source domain/language is transferred to the target domain/language.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.772695779800415}, {"text": "cross-lingual authorship profiling", "start_pos": 64, "end_pos": 98, "type": "TASK", "confidence": 0.6440802911917368}, {"text": "crosslingual named entity recognition (NER)", "start_pos": 104, "end_pos": 147, "type": "TASK", "confidence": 0.7190628009183067}]}, {"text": "We compare our proposed AL method using imitation learning (ALIL) with the followings: \u2022 Random sampling: The query datapoint is chosen randomly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. We  have run the cold-start and warm-start AL for 25  times, and reported the average accuracy in Ta- ble 2. As seen from the results, both the cold and  warm start AL settings outperform the direct trans- fer significantly, and the warm start consistently  gets higher accuracy than the cold start. The dif- ference between the results are statistically signif- icant, with a p-value of .001, according to McNe- mar test", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9989388585090637}, {"text": "Ta- ble 2", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.8430850803852081}, {"text": "accuracy", "start_pos": 280, "end_pos": 288, "type": "METRIC", "confidence": 0.9975173473358154}, {"text": "dif- ference", "start_pos": 314, "end_pos": 326, "type": "METRIC", "confidence": 0.6050357321898142}, {"text": "McNe- mar test", "start_pos": 417, "end_pos": 431, "type": "DATASET", "confidence": 0.7458544671535492}]}, {"text": " Table 2: Classifiers performance under three dif- ferent transfer settings.", "labels": [], "entities": []}, {"text": " Table 3: The first four rows show MRR and accu- racy of instances returned by ALIL under the rank- ings of uncertainty and diversity sampling, the last  row give average accuracy of instances under PAL.", "labels": [], "entities": [{"text": "MRR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9415233731269836}, {"text": "accu- racy", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9665200312932333}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9917749166488647}, {"text": "PAL", "start_pos": 199, "end_pos": 202, "type": "DATASET", "confidence": 0.7696154713630676}]}]}