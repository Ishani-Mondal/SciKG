{"title": [{"text": "A Multi-sentiment-resource Enhanced Attention Network for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.9787169098854065}]}], "abstractContent": [{"text": "Deep learning approaches for sentiment classification do not fully exploit sentiment linguistic knowledge.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9728924036026001}]}, {"text": "In this paper, we propose a Multi-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the problem by integrating three kinds of sentiment linguistic knowledge (e.g., sentiment lexicon, negation words, intensity words) into the deep neural network via attention mechanisms.", "labels": [], "entities": []}, {"text": "By using various types of sentiment resources, MEAN utilizes sentiment-relevant information from different representation sub-spaces, which makes it more effective to capture the overall semantics of the sentiment , negation and intensity words for sentiment prediction.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 249, "end_pos": 269, "type": "TASK", "confidence": 0.8756846189498901}]}, {"text": "The experimental results demonstrate that MEAN has robust superiority over strong competitors.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.4589046537876129}]}], "introductionContent": [{"text": "Sentiment classification is an important task of natural language processing (NLP), aiming to classify the sentiment polarity of a given text as positive, negative, or more fine-grained classes.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.934486985206604}, {"text": "natural language processing (NLP)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.82004714012146}]}, {"text": "It has obtained considerable attention due to its broad applications in natural language processing ().", "labels": [], "entities": []}, {"text": "Most existing studies setup sentiment classifiers using supervised machine learning approaches, such as support vector machine (SVM) (), convolutional neural network (CNN), long short-term memory (LSTM), Tree-LSTM (, and attention-based methods (.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8363552689552307}]}, {"text": "Despite the remarkable progress made by the previous work, we argue that sentiment analysis still remains a challenge.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.9693991243839264}]}, {"text": "Sentiment resources including sentiment lexicon, negation words, intensity words play a crucial role in traditional sentiment classification approaches ().", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.8314599990844727}]}, {"text": "Despite its usefulness, to date, the sentiment linguistic knowledge has been underutilized inmost recent deep neural network models (e.g.,.", "labels": [], "entities": []}, {"text": "In this work, we propose a Multi-sentimentresource Enhanced Attention Network (MEAN) for sentence-level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi-path attention mechanism.", "labels": [], "entities": [{"text": "sentence-level sentiment classification", "start_pos": 89, "end_pos": 128, "type": "TASK", "confidence": 0.7498379647731781}]}, {"text": "Specifically, we first design a coupled word embedding module to model the word representation from character-level and word-level semantics.", "labels": [], "entities": []}, {"text": "This can help to capture the morphological information such as prefixes and suffixes of words.", "labels": [], "entities": []}, {"text": "Then, we propose a multisentiment-resource attention module to learn more comprehensive and meaningful sentiment-specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively.", "labels": [], "entities": [{"text": "sentiment-specific sentence representation", "start_pos": 103, "end_pos": 145, "type": "TASK", "confidence": 0.6339839895566305}]}, {"text": "In this way, we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the overall semantics of the sentiment, negation and intensity words for sentiment prediction.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 243, "end_pos": 263, "type": "TASK", "confidence": 0.8541883528232574}]}, {"text": "The main contributions of this paper are summarized as follows.", "labels": [], "entities": []}, {"text": "First, we design a coupled word embedding obtained from character-level embedding and word-level embedding to capture both the character-level morphological information and word-level semantics.", "labels": [], "entities": []}, {"text": "Second, we propose a multi-sentiment-resource attention module to learn more comprehensive sentiment-specific sentence representation from multiply subspaces implied by three kinds of sentiment resources including sentiment lexicon, intensity words, negation words.", "labels": [], "entities": []}, {"text": "Finally, the experimental results show that MEAN consistently outperforms competitive methods.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.43615755438804626}]}], "datasetContent": [{"text": "Movie Review (MR) and Stanford Sentiment Treebank (SST) 3 are used to evaluate our model.", "labels": [], "entities": [{"text": "Movie Review (MR)", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.6938772439956665}, {"text": "Stanford Sentiment Treebank (SST) 3", "start_pos": 22, "end_pos": 57, "type": "DATASET", "confidence": 0.8537218741008213}]}, {"text": "MR dataset has 5,331 positive samples and 5,331 negative samples.", "labels": [], "entities": [{"text": "MR dataset", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8597672283649445}]}, {"text": "We adopt the same data split as in (.", "labels": [], "entities": []}, {"text": "SST consists of 8,545 training samples, 1,101 validation samples, 2210 test samples.", "labels": [], "entities": [{"text": "SST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8531197309494019}]}, {"text": "Each sample is marked as very negative, negative, neutral, positive, or very positive.", "labels": [], "entities": []}, {"text": "Sentiment lexicon combines the sentiment words from both ( and (), resulting in 10,899 sentiment words in total.", "labels": [], "entities": []}, {"text": "We collect negation and intensity words manually as the number of these words is limited.", "labels": [], "entities": [{"text": "negation and intensity words", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.7133531272411346}]}, {"text": "In our experiments, to be consistent with the recent baseline methods, we adopt classification accuracy as evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.7384566068649292}]}, {"text": "We summarize the experimental results in.", "labels": [], "entities": []}, {"text": "Our model has robust superiority over competitors and sets stateof-the-art on MR and SST datasets.", "labels": [], "entities": [{"text": "MR and SST datasets", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.5280550271272659}]}, {"text": "First, our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge (e.g., RNTN, LSTM, BiLSTM, C-NN and ID-LSTM) on both datasets.", "labels": [], "entities": []}, {"text": "This verifies the effectiveness of leveraging sentiment linguistic resource with the deep learning algorithms.", "labels": [], "entities": []}, {"text": "Second, our model also consistently outperforms LR-Bi-LSTM which integrates linguistic roles of sentiment, negation and intensity words into neural networks via the linguistic regularization.", "labels": [], "entities": []}, {"text": "For example, our model achieves 2.4% improvements over the MR dataset and 0.8% improvements over the SST dataset compared to LR-Bi-LSTM.", "labels": [], "entities": [{"text": "MR dataset", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8398943841457367}, {"text": "SST dataset", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.7807914912700653}]}, {"text": "This is because that MEAN designs attention mechanisms to leverage sentiment resources efficiently, which utilizes the interactive information between context words and sentiment resource words.", "labels": [], "entities": []}, {"text": "In order to analyze the effectiveness of each component of MEAN, we also report the ablation test in terms of discarding character-level embedding (denoted as MEAN w/o CharCNN) and sentiment words/negation words/intensity words (denoted as MEAN w/o sentiment words/negation words/intensity words).", "labels": [], "entities": [{"text": "CharCNN", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.9302915334701538}]}, {"text": "All the tested factors con-tribute greatly to the improvement of the MEAN.", "labels": [], "entities": [{"text": "MEAN", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.545443058013916}]}, {"text": "In particular, the accuracy decreases sharply when discarding the sentiment words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999643087387085}]}, {"text": "This is within our expectation since sentiment words are vital when classifying the polarity of the sentences.: Evaluation results.", "labels": [], "entities": []}, {"text": "The best result for each dataset is in bold.", "labels": [], "entities": []}, {"text": "The result marked with # are retrieved from, and the results marked with * denote the results are obtained by our implementation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results. The best result for  each dataset is in bold. The result marked with #  are retrieved from", "labels": [], "entities": []}]}