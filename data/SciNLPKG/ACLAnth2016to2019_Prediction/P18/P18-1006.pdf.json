{"title": [{"text": "Triangular Architecture for Rare Language Translation", "labels": [], "entities": [{"text": "Rare Language Translation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.8350875576337179}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) performs poor on the low-resource language pair (X, Z), especially when Z is a rare language.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7728278756141662}]}, {"text": "By introducing another rich language Y , we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y, Z) (may be small) and (X, Y) (can be rich) to improve the translation performance of low-resource pairs.", "labels": [], "entities": []}, {"text": "In this triangular architecture , Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidi-rectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y).", "labels": [], "entities": []}, {"text": "Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.", "labels": [], "entities": [{"text": "MultiUN", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.925714373588562}, {"text": "IWSLT2012 datasets", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.8154891133308411}]}], "introductionContent": [{"text": "In recent years, Neural Machine Translation (NMT)) has achieved remarkable performance on many translation tasks (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT))", "start_pos": 17, "end_pos": 50, "type": "TASK", "confidence": 0.7752635478973389}, {"text": "translation tasks", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.8978612720966339}]}, {"text": "Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word byword with the attention mechanism (.", "labels": [], "entities": []}, {"text": "During training, NMT systems are optimized to maximize the translation probability of a given language pair * Contribution during internship at MSRA. with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space.", "labels": [], "entities": [{"text": "MSRA.", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9168830513954163}]}, {"text": "Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (.", "labels": [], "entities": []}, {"text": "In order to deal with the data sparsity problem for NMT, exploiting monolingual data () is the most common method.", "labels": [], "entities": []}, {"text": "With monolingual data, the back-translation method generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one.", "labels": [], "entities": []}, {"text": "By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (.", "labels": [], "entities": []}, {"text": "Similar to joint training (, dual learning ( ) designs a reinforcement learning framework to better capitalize on monolingual data and jointly train two models.", "labels": [], "entities": []}, {"text": "Instead of leveraging monolingual data (X or Z) to enrich the low-resource bilingual pair (X, Z), in this paper, we are motivated to introduce another rich language Y , by which additionally acquired bilingual data (Y, Z) and (X, Y ) can be exploited to improve the translation performance of (X, Z).", "labels": [], "entities": []}, {"text": "This requirement is easy to satisfy, especially when Z is a rare language but X is not.", "labels": [], "entities": []}, {"text": "Under this scenario, (X, Y ) can be a rich-resource pair and provide much bilingual data, while (Y, Z) would also be a low-resource pair mostly because Z is rare.", "labels": [], "entities": []}, {"text": "For example, in the dataset IWSLT2012, there are only 112.6K bilingual sentence pairs of English-Hebrew, since Hebrew is a rare language.", "labels": [], "entities": [{"text": "IWSLT2012", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.8672997355461121}]}, {"text": "If French is introduced as the third language, we can have another lowresource bilingual data of French-Hebrew (116.3K sentence pairs), and easily-acquired bilingual data of the rich-resource pair English-French.", "labels": [], "entities": []}, {"text": "With the introduced rich language Y , in this paper, we propose a novel triangular architecture (TA-NMT) to exploit the additional bilingual data of (Y, Z) and (X, Y ), in order to get better translation performance on the low-resource pair (X, Z), as shown in.", "labels": [], "entities": [{"text": "TA-NMT", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.8294413089752197}]}, {"text": "In this architecture, (Y, Z) is used for training another translation model to score the translation model of (X, Z), while (X, Y ) is used to provide large bilingual data with favorable alignment information.", "labels": [], "entities": []}, {"text": "Under the motivation to exploit the richresource pair (X, Y ), instead of modeling X \u21d2 Z directly, our method starts from modeling the translation task X \u21d2 Y while taking Z as a latent variable.", "labels": [], "entities": []}, {"text": "Then, we decompose X \u21d2 Y into two phases for training two translation models of low-resource pairs ((X, Z) and (Y, Z)) respectively.", "labels": [], "entities": []}, {"text": "The first translation model generates a sequence in the hidden space of Z from X, based on which the second one generates the translation in Y . These two models can be optimized jointly with an Expectation Maximization (EM) framework with the goal of maximizing the translation probability p(y|x).", "labels": [], "entities": []}, {"text": "In this framework, the two models can boost each other by generating pseudo bilingual data for model training with the weights scored from the other.", "labels": [], "entities": []}, {"text": "By reversing the translation direction of X \u21d2 Y , our method can be used to train another two translation models p(z|y) and p(x|z).", "labels": [], "entities": []}, {"text": "Therefore, the four translation models (p(z|x), p(x|z), p(z|y) and p(y|z)) of the rare language Z can be optimized jointly with our proposed unified bidirectional EM algorithm.", "labels": [], "entities": []}, {"text": "Experimental results on the MultiUN and IWSLT2012 datasets demonstrate that our method can achieve significant improvements for rare languages translation.", "labels": [], "entities": [{"text": "MultiUN", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.9010168313980103}, {"text": "IWSLT2012 datasets", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.876324325799942}, {"text": "rare languages translation", "start_pos": 128, "end_pos": 154, "type": "TASK", "confidence": 0.6513893703619639}]}, {"text": "By incorporating backtranslation (a method leveraging more monolingual data) into our method, TA-NMT can achieve even further improvements.", "labels": [], "entities": []}, {"text": "Our contributions are listed as follows: \u2022 We propose a novel triangular training architecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framework.", "labels": [], "entities": []}, {"text": "\u2022 Our method can exploit two additional bilingual datasets at both the model and data levels by introducing another rich language.", "labels": [], "entities": []}, {"text": "\u2022 Our method is a unified bidirectional EM algorithm, in which four translation models on two low-resource pairs are trained jointly and boost each other.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to verify our method, we conduct experiments on two multilingual datasets.", "labels": [], "entities": []}, {"text": "The one is, which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (, which is a set of multilingual transcriptions of TED talks.", "labels": [], "entities": [{"text": "IWSLT2012", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.8727034330368042}]}, {"text": "As is mentioned in section 1, our method is compatible with methods exploiting monolingual data.", "labels": [], "entities": []}, {"text": "So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method.", "labels": [], "entities": []}, {"text": "MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ).", "labels": [], "entities": []}, {"text": "Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z.", "labels": [], "entities": []}, {"text": "We randomly choose subsets of bilingual data of (X, Z) and (Y, Z) in the original dataset to simulate low-resource situations, and make sure there is no overlap in Z between chosen data of (X, Z) and (Y, Z).", "labels": [], "entities": []}, {"text": "IWSLT2012 1 : English-French is used as the rich-resource pair (X, Y ), and two rare languages Z are Hebrew (HE) and Romanian (RO) in our choice.", "labels": [], "entities": [{"text": "IWSLT2012", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9348626732826233}]}, {"text": "Note that in this dataset, low-resource pairs (X, Z) and (Y, Z) are severely overlapped in Z.", "labels": [], "entities": []}, {"text": "In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair.", "labels": [], "entities": [{"text": "WMT2014 dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9801056385040283}]}, {"text": "We also use additional EnglishRomanian bilingual data from Europarlv7 dataset ().", "labels": [], "entities": [{"text": "EnglishRomanian bilingual data", "start_pos": 23, "end_pos": 53, "type": "DATASET", "confidence": 0.6905319889386495}, {"text": "Europarlv7 dataset", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9271481037139893}]}, {"text": "The monolingual data of Z (HE and RO) are taken from the web 2 . In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization.", "labels": [], "entities": []}, {"text": "Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data.", "labels": [], "entities": []}, {"text": "The size of training data of all language pairs are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: training data size of each language pair.", "labels": [], "entities": []}, {"text": " Table 3: Test BLEU on MultiUN Dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9891840219497681}, {"text": "MultiUN Dataset", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.920591801404953}]}, {"text": " Table 4: Test BLEU on IWSLT Dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9895221590995789}, {"text": "IWSLT Dataset", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.9710623919963837}]}]}