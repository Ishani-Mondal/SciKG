{"title": [{"text": "Cold-Start Aware User and Product Attention for Sentiment Classification", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.969741940498352}]}], "abstractContent": [{"text": "The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.9701793193817139}]}, {"text": "However , current models do not deal with the cold-start problem which is typical in review websites.", "labels": [], "entities": []}, {"text": "In this paper, we present Hybrid Contextualized Sentiment Classi-fier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors.", "labels": [], "entities": []}, {"text": "HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start).", "labels": [], "entities": []}, {"text": "This is decided by a frequency-guided selective gate vector.", "labels": [], "entities": []}, {"text": "Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.8465070724487305}]}, {"text": "More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is the fundamental task of sentiment analysis), where we are to classify the sentiment of a given text.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9572969973087311}, {"text": "sentiment analysis", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8815819323062897}]}, {"text": "It is widely used on online review websites as they contain huge amounts of review data that can be clas- sified a sentiment.", "labels": [], "entities": []}, {"text": "In these websites, a sentiment is usually represented as an intensity (e.g. 4 out of 5).", "labels": [], "entities": []}, {"text": "The reviews are written by users who have bought a product.", "labels": [], "entities": []}, {"text": "Recently, sentiment analysis research has focused on personalization to recommend product to users, and vise versa.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.9675412178039551}]}, {"text": "To this end, many have used user and product information not only to develop personalization but also to improve the performance of the classification model ().", "labels": [], "entities": []}, {"text": "Indeed, these information are important in two ways.", "labels": [], "entities": []}, {"text": "First, some expressions are user-specific fora certain sentiment intensity.", "labels": [], "entities": []}, {"text": "For example, the phrase \"very salty\" may have different sentiments fora person who likes salty food and a person who likes otherwise.", "labels": [], "entities": []}, {"text": "This is also apparent in terms of products.", "labels": [], "entities": []}, {"text": "Second, these additional contexts help mitigate data sparsity and cold-start problems.", "labels": [], "entities": []}, {"text": "Coldstart is a problem when the model cannot draw useful information from users/products where data is insufficient.", "labels": [], "entities": []}, {"text": "User and product information can help by introducing a frequent user/product with similar attributes to the cold-start user/product.", "labels": [], "entities": []}, {"text": "Thanks to the promising results of deep neural networks to the sentiment classification task), more recent models incorporate user and product information to convolutional neural networks ( and deep memory networks, and have shown significant improvements.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.9459524154663086}]}, {"text": "The current state-of-the-art model, NSC, introduced an attention mechanism called UPA which is based on user and product information and applied this to a hierarchical LSTM.", "labels": [], "entities": [{"text": "NSC", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9505841732025146}]}, {"text": "The main problem with current models is that they use user and product information naively as an ordinary additional context, not considering the possible existence of cold-start problems.", "labels": [], "entities": []}, {"text": "This makes NSC more problematic than helpful in reality since majority of the users in review websites have very few number of reviews.", "labels": [], "entities": [{"text": "NSC", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.6801024079322815}]}, {"text": "To this end, we propose the idea shown in Figure 1.", "labels": [], "entities": []}, {"text": "It can be described as follows: If the model does not have enough information to create a user/product vector, then we use a vector computed from other user/product vectors that are similar.", "labels": [], "entities": []}, {"text": "We introduce anew model called Hybrid Contextualized Sentiment Classifier (HCSC), which consists of two modules.", "labels": [], "entities": [{"text": "Hybrid Contextualized Sentiment Classifier (HCSC)", "start_pos": 31, "end_pos": 80, "type": "TASK", "confidence": 0.730903080531529}]}, {"text": "First, we build a fast yet effective word encoder that accepts word vectors and outputs new encoded vectors that are contextualized with short-and long-range contexts.", "labels": [], "entities": []}, {"text": "Second, we combine these vectors into one pooled vector through a novel attention mechanism called Cold-Start Aware Attention (CSAA).", "labels": [], "entities": []}, {"text": "The CSAA mechanism has three components: (a) a user/product-specific distinct vector derived from the original user/product information of the review, (b) a user/product-specific shared vector derived from other users/products, and (c) a frequency-guided selective gate which decides which vector to use.", "labels": [], "entities": []}, {"text": "Multiple experiments are conducted with the following results: In the original non-sparse datasets, our model performs significantly better than the previous state-of-the-art, NSC, in terms of RMSE, despite being less complex.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.6054757237434387}]}, {"text": "In the sparse datasets, HCSC performs significantly better than previous competing models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experiments and the corresponding results.", "labels": [], "entities": []}, {"text": "We use the models described in Section 2 as baseline models: JMARS  Implementation We set the size of the word, user, and product vectors to 300 dimensions.", "labels": [], "entities": [{"text": "JMARS  Implementation", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.6363303363323212}]}, {"text": "We use pre-trained GloVe embeddings 2 () to initialize our word vectors.", "labels": [], "entities": []}, {"text": "We simply set the parameters for both BiLSTMs and CNN to produce an output with 300 dimensions: For the BiLSTMs, we set the state sizes of the LSTMs to 75 dimensions, fora total of 150 dimensions.", "labels": [], "entities": []}, {"text": "For CNN, we set h = 3, 5, 7, each with 50: Dataset statistics feature maps, fora total of 150 dimensions.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8509939908981323}]}, {"text": "These two are concatenated to create a 300-dimension encoded word vectors.", "labels": [], "entities": []}, {"text": "We use dropout () on all non-linear connections with a dropout rate of 0.5.", "labels": [], "entities": []}, {"text": "We set the batch size to 32.", "labels": [], "entities": []}, {"text": "Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule, with l 2 constraint (Hinton et al., 2012) of 3.", "labels": [], "entities": [{"text": "Adadelta update rule", "start_pos": 85, "end_pos": 105, "type": "METRIC", "confidence": 0.6316004395484924}]}, {"text": "We perform early stopping using a subset of the given development dataset.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7285885214805603}]}, {"text": "Training and experiments are all done using a NVIDIA GeForce GTX 1080 Ti graphics card.", "labels": [], "entities": []}, {"text": "Additionally, we also implement two versions of our model where the word encoder is a subpart of HCSC, i.e. (a) the CNN-based model (CNN+CSAA) and (b) the RNN-based model (RNN+CSAA).", "labels": [], "entities": []}, {"text": "For the CNN-based model, we use 100 feature maps for each of the filter sizes h = 3, 5, 7, fora total of 300 dimensions.", "labels": [], "entities": []}, {"text": "For the RNN-based model, we set the state sizes of the LSTMs to 150, fora total of 300 dimensions.", "labels": [], "entities": []}, {"text": "Datasets and evaluation We evaluate and compare our models with other competing models using two widely used sentiment classification datasets with available user and product information: IMDB and Yelp 2013.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.718620166182518}, {"text": "IMDB and Yelp 2013", "start_pos": 188, "end_pos": 206, "type": "DATASET", "confidence": 0.7475185096263885}]}, {"text": "Both datasets are curated by, where they are divided into train, dev, and test sets using a 8:1:1 ratio, and are tokenized and sentence-splitted using Stanford CoreNLP ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 151, "end_pos": 167, "type": "DATASET", "confidence": 0.9324465692043304}]}, {"text": "In addition, we create three subsets of the train dataset to test the robustness of the models on sparse datasets.", "labels": [], "entities": []}, {"text": "To create these datasets, we randomly remove all the reviews of x% of all users and products, where x = 20, 50, 80.", "labels": [], "entities": []}, {"text": "These datasets are not only more sparse than the original datasets, but also have smaller number of users and products, introducing cold-start users and products.", "labels": [], "entities": []}, {"text": "All datasets are summarized in.", "labels": [], "entities": []}, {"text": "Evaluation is done using two metrics: the Accuracy which measures the overall sentiment classification performance and the RMSE which measures the diver-  gence between predicted and ground truth classes.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9986513257026672}, {"text": "sentiment classification", "start_pos": 78, "end_pos": 102, "type": "TASK", "confidence": 0.8681028485298157}, {"text": "RMSE", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.965829610824585}]}, {"text": "We notice very minimal differences among performances of different runs.", "labels": [], "entities": []}, {"text": "We report the results on the original datasets in.", "labels": [], "entities": []}, {"text": "On both datasets, HCSC outperforms all previous models based on both accuracy and RMSE.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9996705055236816}, {"text": "RMSE", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9387429356575012}]}, {"text": "Based on accuracy, HCSC performs significantly better than all previous models except NSC, where it performs slightly better with 0.9% and 0.7% increase on IMDB and Yelp 2013 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.999583899974823}, {"text": "NSC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.8943378329277039}, {"text": "IMDB and Yelp 2013 datasets", "start_pos": 156, "end_pos": 183, "type": "DATASET", "confidence": 0.8287907123565674}]}, {"text": "Based on RMSE, HCSC performs significantly better than all previous models, except when compared with UPDMN on the Yelp 2013 datasets, where it performs slightly better.", "labels": [], "entities": [{"text": "Yelp 2013 datasets", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9738218188285828}]}, {"text": "We note that RMSE is a better metric because it measures how close the wrongly predicted sentiment and the ground truth sentiment are.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9399473667144775}]}, {"text": "Although NSC performs as well as HCSC based on accuracy, it performs worse based on RMSE, which means that its predictions deviate far from the original sentiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9987909197807312}, {"text": "RMSE", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9684240221977234}]}, {"text": "It is also interesting to note that when CSAA is used as attentive pooling, both simple CNN and RNN models perform just as well as NSC, despite NSC being very complex and modeling the documents with compositionality).", "labels": [], "entities": []}, {"text": "This is especially true when com-: Accuracy values of competing models when the training data used is sparse.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.978010892868042}]}, {"text": "Bold-faced values are the best accuracies in the column, while red values are accuracies worse than NSC(LA).", "labels": [], "entities": [{"text": "NSC(LA)", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.8251688927412033}]}, {"text": "pared using RMSE, where both CNN+CSAA and RNN+CSAA perform significantly better (p < 0.01) than NSC.", "labels": [], "entities": []}, {"text": "This proves that CSAA is an effective use of the user and product information for sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.9616420865058899}]}, {"text": "shows the accuracy of NSC (Chen et al., 2016a) and our models CNN+CSAA, RNN+CSAA, and HCSC on the sparse datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996119141578674}, {"text": "HCSC", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.9006320238113403}]}, {"text": "As shown in the table, on all datasets with different levels of sparsity, HCSC performs the best among the competing models.", "labels": [], "entities": []}, {"text": "The difference between the accuracy of HCSC and NSC increases as the level of sparsity intensifies: While the HCSC only gains 0.8% and 1.0% over NSC on the less sparse Sparse20 IMDB and Yelp 2013 datasets, it improves over NSC significantly with 7.6% and 2.7% increase on the more sparse Sparse80 IMDB and Yelp 2013 datasets, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9993435740470886}, {"text": "Sparse20 IMDB and Yelp 2013 datasets", "start_pos": 168, "end_pos": 204, "type": "DATASET", "confidence": 0.8672343790531158}, {"text": "Sparse80 IMDB and Yelp 2013 datasets", "start_pos": 288, "end_pos": 324, "type": "DATASET", "confidence": 0.8312181135018667}]}, {"text": "We also run our experiments using NSC without user and product information, i.e. NSC(LA) which reduces the model into a hierarchical LSTM model (.", "labels": [], "entities": []}, {"text": "Results show that although the use of user and product information in NSC improves the model on less sparse datasets (as also shown in the original paper), it decreases the performance of the model on more sparse datasets: It performs 2.0%, 1.7%, and 1.2% worse than NSC(LA) on Sparse50 IMDB, Sparse80 IMDB, and Sparse80 Yelp 2013 datasets.", "labels": [], "entities": [{"text": "Sparse80 IMDB", "start_pos": 293, "end_pos": 306, "type": "DATASET", "confidence": 0.9222307503223419}, {"text": "Sparse80 Yelp 2013 datasets", "start_pos": 312, "end_pos": 339, "type": "DATASET", "confidence": 0.9511019736528397}]}, {"text": "We argue that this is because NSC does not consider the existence of cold-start problems, which makes the additional user and product in- formation more noisy than helpful.", "labels": [], "entities": [{"text": "NSC", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8042304515838623}]}], "tableCaptions": [{"text": " Table 2: Accuracy and RMSE values of competing  models on the original non-sparse datasets. An aster- isk indicates that HCSC is significantly better than the  model (p < 0.01).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9926183819770813}, {"text": "RMSE", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9923845529556274}, {"text": "HCSC", "start_pos": 122, "end_pos": 126, "type": "DATASET", "confidence": 0.6985654234886169}]}, {"text": " Table 3: Accuracy values of competing models when  the training data used is sparse. Bold-faced values are  the best accuracies in the column, while red values are  accuracies worse than NSC(LA).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9901066422462463}, {"text": "NSC(LA)", "start_pos": 188, "end_pos": 195, "type": "METRIC", "confidence": 0.6325498223304749}]}, {"text": " Table 4: Time (in seconds) to process the first 100  batches of competing models for each dataset. The  numbers in the parenthesis are the speedup of time  when compared to NSC.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.965489387512207}, {"text": "NSC", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.847423255443573}]}]}