{"title": [{"text": "'Lighter' Can Still Be Dark: Modeling Comparative Color Descriptions", "labels": [], "entities": [{"text": "Modeling Comparative Color Descriptions", "start_pos": 29, "end_pos": 68, "type": "TASK", "confidence": 0.7744873911142349}]}], "abstractContent": [{"text": "We propose a novel paradigm of grounding comparative adjectives within the realm of color descriptions.", "labels": [], "entities": []}, {"text": "Given a reference RGB color and a comparative term (e.g., 'lighter', 'darker'), our model learns to ground the comparative as a direction in the RGB space such that the colors along the vector, rooted at the reference color, satisfy the comparison.", "labels": [], "entities": []}, {"text": "Our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9977491497993469}]}, {"text": "These vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception.", "labels": [], "entities": []}, {"text": "Our approach makes use of a newly created dataset for this task derived from existing labeled color data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal approaches to object recognition have achieved a degree of success by grounding adjectives and nouns from descriptive text in image features (.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8225425779819489}]}, {"text": "One limitation of this approach, particularly for fine-grained object recognition, is when objects are differentiated not by having unique sets of attributes but by a difference in the strengths of their shared attributes ().", "labels": [], "entities": [{"text": "object recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7197961211204529}]}, {"text": "In text, this difference is described using comparative adjectives.", "labels": [], "entities": []}, {"text": "For example, the sexual dimorphism of the American black duck is described with the phrase \"females tend to be slightly paler than males, with duller olive bills\".", "labels": [], "entities": []}, {"text": "Ina recent study of pragmatic referring expression interpretation in the context of color selection, found that speakers almost always used comparative adjectives when the target color was very similar to a distractor, rather than using multiple positive form adjectives to create a highly specific description of the color independent of its surroundings.", "labels": [], "entities": [{"text": "referring expression interpretation", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.6444131036599478}]}, {"text": "Though color has been studied in terms of its contextual dependence and vagueness in grounding, no approaches have focused explicitly on learning to ground comparative adjective; in this work we focus on comparative color descriptions.", "labels": [], "entities": []}, {"text": "The presence of distractors in the study is important -comparatives describe a change in a feature with respect to a reference point.", "labels": [], "entities": []}, {"text": "While the description light blue can be understood to represent a particular subset of colors in RGB, for example, neither 'lighter' nor 'lighter blue' have explicit representations; it is only with a reference that we can image what color either might refer to.", "labels": [], "entities": []}, {"text": "If the reference color is a deep navy blue, then we imagine the target to be much closer to navy than, for example, a sky blue.", "labels": [], "entities": []}, {"text": "We propose anew paradigm of learning to ground comparative adjectives within the realm of color descriptions: given a reference RGB color and a comparative term (e.g. 'lighter', 'paler'), our deep learning model learns to ground the comparative as a direction in RB space such that the colors along the vector, rooted at the reference color, satisfy the comparison (Section 3).", "labels": [], "entities": []}, {"text": "The reference color does more than quantify the specific RGB values to apply the comparative to: it also affects the grounding of the comparative.", "labels": [], "entities": []}, {"text": "For example, 'darker' might seem like a simple change -simply reduce the values of all color channels equally towards 0.", "labels": [], "entities": []}, {"text": "But asdarker' refers to a different direction in RGB space depending on the reference color, and thus we need a reference-dependent approach.", "labels": [], "entities": []}, {"text": "Our approach makes use of a newly created dataset for this task derived from an existing labeled color dataset (McMahan and Stone, 2015) (Section 2).", "labels": [], "entities": []}, {"text": "Our results in Section 5 show that our model generates grounded representations of comparative adjectives with an average accuracy of 0.65 cosine similarity to the desired direction of change.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9967960715293884}]}, {"text": "These learned vectors approach colors with Delta-E scores of under 7 compared to the target colors, indicating the differences are very small with respect to human perception.", "labels": [], "entities": []}], "datasetContent": [{"text": "The reference color label, the comparative adjective and their pairing have been seen in the training data.", "labels": [], "entities": []}, {"text": "The reference color label and the comparative adjective have been seen in the training data, but not their pairing.", "labels": [], "entities": []}, {"text": "The reference color label, and thus all the corresponding RGB color datapoints, have not be seen in training, while the comparative has been seen in the training data.", "labels": [], "entities": []}, {"text": "The comparative adjective has not been seen in training, but the reference color label has been seen.", "labels": [], "entities": []}, {"text": "Neither the comparative adjective nor the reference color have been seen in the training.", "labels": [], "entities": []}, {"text": "For the conditions where the reference color label has been seen in training, the actual RGB reference color datapoints associated with the labels were different from the ones used in training: 15% of the datapoints from each training reference color label were set aside for testing, providing RGB values close but not equivalent to those seen in training.", "labels": [], "entities": []}, {"text": "10% of the reference color labels were set aside for testing, as were 10% of the comparative words; this amounted to 8 reference colors and 8 comparatives.", "labels": [], "entities": []}, {"text": "The number of tuples and actual datapoints instances for each test condition is given in.", "labels": [], "entities": []}, {"text": "The network was trained at a 0.001 learning rate for 800 epochs, with the output of the first layer of dimension d=30.", "labels": [], "entities": []}, {"text": "shows examples of learned groundings of comparatives for each of the five test conditions (Test Type column).", "labels": [], "entities": []}, {"text": "It shows the reference RGB color datapoint r c (always unseen), the comparative word w, the learned grounding vector w g , the target color t c , and two scores: cosine similarity and Delta-E. The upper sample for each test type is an example of a highly accurate result, while the lower sample exemplifies failure.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 162, "end_pos": 179, "type": "METRIC", "confidence": 0.8139880895614624}]}], "tableCaptions": []}