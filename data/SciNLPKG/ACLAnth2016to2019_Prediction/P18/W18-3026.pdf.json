{"title": [{"text": "Jointly Embedding Entities and Text with Distant Supervision", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications.", "labels": [], "entities": []}, {"text": "However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora.", "labels": [], "entities": []}, {"text": "We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of mappings between entities and surface forms.", "labels": [], "entities": []}, {"text": "We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure.", "labels": [], "entities": []}, {"text": "Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomed-ical datasets and anew Wikipedia-based dataset that we release to the community.", "labels": [], "entities": [{"text": "Wikipedia-based dataset", "start_pos": 128, "end_pos": 151, "type": "DATASET", "confidence": 0.8159009218215942}]}, {"text": "Results on analogy completion and entity sense disambiguation indicate that entities and words capture complementary information that can be effectively combined for downstream use.", "labels": [], "entities": [{"text": "analogy completion", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8133298754692078}, {"text": "entity sense disambiguation", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6985505223274231}]}], "introductionContent": [{"text": "Distributed representations of knowledge base entities and concepts have become key elements of many recent NLP systems, for applications from document ranking and knowledge base completion ( to clinical diagnosis code prediction (.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7443474531173706}, {"text": "knowledge base completion", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.6051768263181051}, {"text": "clinical diagnosis code prediction", "start_pos": 195, "end_pos": 229, "type": "TASK", "confidence": 0.6322022303938866}]}, {"text": "These works have taken two broad tacks for the challenge of learning to represent entities, each of which may have multiple unique surface forms in text.", "labels": [], "entities": []}, {"text": "Knowledge-based approaches learn entity representations based on the structure of a large knowledge base, often augmented by annotated text resources ().", "labels": [], "entities": []}, {"text": "Other methods utilize explicitly annotated data, and have been more popular in the biomedical domain (.", "labels": [], "entities": []}, {"text": "Both approaches, however, are often limited by ignoring some or most of the available textual information.", "labels": [], "entities": []}, {"text": "Furthermore, such rich structures and annotations are lacking for many specialized domains, and can be prohibitively expensive to obtain.", "labels": [], "entities": []}, {"text": "We propose a fully text-based method for jointly learning representations of words, the surface forms of entities, and the entities themselves, from an unannotated text corpus.", "labels": [], "entities": []}, {"text": "We use distant supervision from a terminology, which maps entities to known surface forms.", "labels": [], "entities": []}, {"text": "We augment the well-known log-linear skip-gram model) with additional term-and entity-based objectives, and evaluate our learned embeddings in both intrinsic and extrinsic settings.", "labels": [], "entities": []}, {"text": "Our joint embeddings clearly outperform prior entity embedding methods on similarity and relatedness evaluations.", "labels": [], "entities": []}, {"text": "Entity and word embeddings capture complementary information, yielding improved performance when they are combined.", "labels": [], "entities": []}, {"text": "Analogy completion results further illustrate these differences, demonstrating that entities capture domain knowledge, while word embeddings capture morphological and lexical information.", "labels": [], "entities": [{"text": "Analogy completion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8583519160747528}]}, {"text": "Finally, we see that an oracle combination of entity and text embeddings nearly matches a state of the art unsupervised method for biomedical word sense disambiguation that uses complex knowledge-based approaches.", "labels": [], "entities": [{"text": "biomedical word sense disambiguation", "start_pos": 131, "end_pos": 167, "type": "TASK", "confidence": 0.6907241269946098}]}, {"text": "However, our embeddings show a significant drop in performance compared to prior work in a newswire disambiguation dataset, indicating that knowledge graph structure contains entity information that a purely text-based approach does not capture.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following,, and others, we evaluate our embeddings on both intrinsic and extrinsic tasks.", "labels": [], "entities": []}, {"text": "To evaluate the semantic organization of the space, we use the standard intrinsic evaluations of similarity and relatedness and analogy completion.", "labels": [], "entities": [{"text": "analogy completion", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.7354273200035095}]}, {"text": "To explore the applicability of our embeddings to downstream applications, we apply them to named entity disambiguation.", "labels": [], "entities": [{"text": "named entity disambiguation", "start_pos": 92, "end_pos": 119, "type": "TASK", "confidence": 0.6528270443280538}]}, {"text": "Results and analyses for each experiment are discussed in the following subsections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the many-to-many mapping  between terms and entities in our terminologies,  including the maximum # of terms per entity.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of our embedding training cor- pora. # mentions is the number of exact matches  found for terms in the relevant terminology. CP =  corpus polysemy of a given entity. B = billion.", "labels": [], "entities": [{"text": "B", "start_pos": 187, "end_pos": 188, "type": "METRIC", "confidence": 0.9952387809753418}]}, {"text": " Table 3: Spearman's \u03c1 for similarity/relatedness  predictions in UMNSRS. Filtered results indi- cate performance on the shared-vocabulary subset.  *=significantly better (p < 0.05) than word base- line (full), DeVine et al (filtered).", "labels": [], "entities": [{"text": "similarity/relatedness  predictions", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6204593107104301}]}, {"text": " Table 4: Spearman's \u03c1 for similarity/relatedness  predictions in WikiSRS, training on two cor- pora. All Proposed results are significantly better  than MPME; *=significantly better than strongest  word-level baseline (p < 0.05).", "labels": [], "entities": [{"text": "similarity/relatedness  predictions", "start_pos": 27, "end_pos": 62, "type": "TASK", "confidence": 0.6392461806535721}]}, {"text": " Table 5: Top 3 pairs in the Relatedness datasets, as ranked by different embedding methods.", "labels": [], "entities": [{"text": "Relatedness datasets", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.8562954366207123}]}, {"text": " Table 6: Accuracy % on 5 of the relations in  BMASS with greatest absolute difference in word  performance vs entity performance: B3 (gene- encodes-product), H1 (refers-to), C6 (associated- with), L1 (form-of ), and L6 (has-free-acid-or- base-form). The better of word and entity per- formance is highlighted; all entity vs word differ- ences are significant (McNemar's test; p 0.01).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9909405708312988}, {"text": "BMASS", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.8854939937591553}]}, {"text": " Table 7: Analogy completion accuracy % on the  semantic relations in the Google analogy dataset.  W=Wikipedia, G=Gigaword.", "labels": [], "entities": [{"text": "Analogy completion", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.602134570479393}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8679315447807312}, {"text": "Google analogy dataset", "start_pos": 74, "end_pos": 96, "type": "DATASET", "confidence": 0.7454956968625387}, {"text": "Wikipedia", "start_pos": 101, "end_pos": 110, "type": "DATASET", "confidence": 0.9727984070777893}]}, {"text": " Table 8: MSH WSD disambiguation accuracy.  Definitions is comparable to Pakhomov et al.  (2016), using jointly-embedded words. All differ- ences are significant (McNemar's test, p 0.01).", "labels": [], "entities": [{"text": "MSH WSD disambiguation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.5597149332364401}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9517142176628113}]}, {"text": " Table 11: The intraclass correlation coefficient  (ICC) among Amazon Mechanical Turk worker  judgments of similarity and relatedness of pairs of  Wikipedia entities. As ICC requires a fixed num- ber of raters, but we had variable numbers of re- sponses to each HIT, we break down the datasets  by the number of workers who rated each item.", "labels": [], "entities": [{"text": "intraclass correlation coefficient  (ICC)", "start_pos": 15, "end_pos": 56, "type": "METRIC", "confidence": 0.8321929375330607}]}]}