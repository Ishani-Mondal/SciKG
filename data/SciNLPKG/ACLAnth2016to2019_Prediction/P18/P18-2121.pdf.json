{"title": [{"text": "Pretraining Sentiment Classifiers with Unlabeled Dialog Data", "labels": [], "entities": [{"text": "Pretraining Sentiment Classifiers", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7197146813074747}]}], "abstractContent": [{"text": "The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.9692771136760712}]}, {"text": "Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models.", "labels": [], "entities": []}, {"text": "In this paper, we take the concept a step further by using a conditional language model, instead of a language model.", "labels": [], "entities": []}, {"text": "Specifically, we address a sentiment classification task fora tweet analysis service as a case study and propose a pretraining strategy with unla-beled dialog data (tweet-reply pairs) via an encoder-decoder model.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.9154541492462158}]}, {"text": "Experimental results show that our strategy can improve the performance of sentiment clas-sifiers and outperform several state-of-the-art strategies including language model pretraining.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment classification is a task to predict a sentiment label, such as positive/negative, fora given text and has been applied to many domains such as movie/product reviews, customer surveys, news comments, and social media.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9081002473831177}]}, {"text": "A common problem of this task is the lack of labeled training data due to costly annotation work, especially for social media without explicit sentiment feedback such as review scores.", "labels": [], "entities": []}, {"text": "To overcome this problem, recently proposed a semi-supervised sequence learning framework, where a sentiment classifier based on recurrent neural networks (RNNs) is trained with labeled data after initializing it with the parameters of an RNN-based language model pretrained with a large amount of unlabeled data.", "labels": [], "entities": []}, {"text": "The concept of their framework is simple but effective, and their work yielded many related studies of semi-supervised training based on sequence modeling, as described in Section 4.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7026778906583786}]}, {"text": "In this paper, we take their concept a step further by using a conditional language model with unlabeled dialog data (i.e., tweet-reply pairs) instead of a language model with unpaired data . An important observation of the dialog data that underpins our strategy is that the sentiment or mood in a message often affects messages in reply to it.", "labels": [], "entities": []}, {"text": "People tend to write angry responses to angry messages, empathetic replies to sad remarks, or congratulatory phrases to good news.", "labels": [], "entities": []}, {"text": "Our contributions are listed as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a pretraining strategy with unlabeled dialog data (tweet-reply pairs) via an encoder-decoder model for sentiment classifiers (Section 2).", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.739177793264389}]}, {"text": "To the best of our knowledge, our proposal is the first such proposal, as clarified in Section 4.", "labels": [], "entities": []}, {"text": "\u2022 We report on a case study based on a costly labeled sentiment dataset of 99.5K items and a large-scale unlabeled dialog dataset of 22.3M, which were provided from a tweet analysis service (Section 3.1).", "labels": [], "entities": []}, {"text": "\u2022 Experimental results of sentiment classification show that our method outperforms the current semi-supervised methods based on a language model, autoencoder, and distant supervision, as well as linear classifiers (Section 3.4).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.9823661148548126}]}], "datasetContent": [{"text": "We used two datasets, a dialog dataset for pretraining the encoder-decoder model and a sentiment dataset for training (fine-tuning) the sentiment classifier, as shown in.", "labels": [], "entities": []}, {"text": "Those datasets were provided by Yahoo!", "labels": [], "entities": []}, {"text": "JAPAN, which is the largest portal site in Japan.", "labels": [], "entities": [{"text": "JAPAN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9430356025695801}]}, {"text": "The dialog dataset contains about 22.3 million tweet-reply pairs extracted from Twitter Firehose data.", "labels": [], "entities": [{"text": "Twitter Firehose data", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.9006255467732748}]}, {"text": "In its preprocessing, we filtered out spam and bot posts by using user-level signals such as the follower count, the friend count, the favorite count, and whether a profile image is set or not.", "labels": [], "entities": []}, {"text": "Also, we replaced all the URLs in the text with \"\" and all the user mentions with \"[m]\", considering them as noise.", "labels": [], "entities": []}, {"text": "The rest of the text was used Train Valid Test Dialog 22,300,000 10,000 50,000 Sentiment 80,591 4,000 15,000: Details of dialog and sentiment datasets as it was.", "labels": [], "entities": [{"text": "Train Valid Test Dialog 22,300,000 10,000 50,000 Sentiment 80,591", "start_pos": 30, "end_pos": 95, "type": "DATASET", "confidence": 0.9132167365815904}]}, {"text": "On average, source and target (or reply) tweets after preprocessing were 31.5 and 27.8 characters long, respectively.", "labels": [], "entities": []}, {"text": "While redistribution of tweets is prohibited, we are planning to publicize tweet IDs of this dataset for reproducibility.", "labels": [], "entities": []}, {"text": "The sentiment dataset includes about 100K tweets with manually annotated three-class sentiment labels: positive, negative, and neutral.", "labels": [], "entities": []}, {"text": "The breakdown of positive, negative, and neutral in the training set was 15.0, 18.6, and 66.4%, respectively.", "labels": [], "entities": []}, {"text": "Note that the tweets were sampled separately from those of the dialog dataset.", "labels": [], "entities": []}, {"text": "The procedure for text preprocessing was the same with that of the dialog dataset.", "labels": [], "entities": [{"text": "text preprocessing", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7912780344486237}]}, {"text": "The average length of the tweets after preprocessing was 17 characters.", "labels": [], "entities": []}, {"text": "Each tweet was judged by a majority vote of three experienced editors in the company providing the sentiment-analysis service.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement ratio assessed with Fleiss' \u03ba was 0.495.", "labels": [], "entities": []}, {"text": "The overall annotation work took roughly 300 person-days.", "labels": [], "entities": []}, {"text": "This means that the cost is at least 24K dollars, 8 hours \u00d7 300 days \u00d7 legal minimum wage in Japan 10 dollars/hour.", "labels": [], "entities": []}, {"text": "Considering that the in-house annotators are well-educated, skilled proper employees, the actual cost would be much higher than this rough estimate and much more costly than collecting unlabeled dialog data.", "labels": [], "entities": []}, {"text": "In addition, the annotators had gone through a few days of training to become able to appropriately judge the sentiment before they got down to actual annotation work, but the number, 300 person-days, does not include the time for this training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Macro-average F-measure of sentiment  classification of each model versus labeled data  size. Dial is our proposed method, and  \u2020 in  its row indicates statistically significant difference  from the corresponding value of Lang (p <  0.05).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8539643287658691}, {"text": "sentiment  classification", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.9182578921318054}]}]}