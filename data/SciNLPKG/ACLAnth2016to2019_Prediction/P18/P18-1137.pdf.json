{"title": [{"text": "Tailored Sequence to Sequence Models to Different Conversation Scenarios", "labels": [], "entities": []}], "abstractContent": [{"text": "Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7743764221668243}]}, {"text": "However , the requirements for different conversation scenarios are distinct.", "labels": [], "entities": []}, {"text": "For example , customer service requires the generated responses to be specific and accurate , while chatbot prefers diverse responses so as to attract different users.", "labels": [], "entities": []}, {"text": "The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria.", "labels": [], "entities": []}, {"text": "As a result, it usually generates safe and commonplace responses , such as 'I don't know'.", "labels": [], "entities": []}, {"text": "In this paper , we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario.", "labels": [], "entities": []}, {"text": "Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.", "labels": [], "entities": [{"text": "Ubuntu dialogue corpus", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.7190403938293457}, {"text": "Chinese Weibo dataset", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.797207256158193}]}], "introductionContent": [{"text": "This paper focuses on the problem of the singleturn dialogue generation, which is critical in many natural language processing applications such as customer services, intelligent assistant and chatbot.", "labels": [], "entities": [{"text": "singleturn dialogue generation", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.8445608615875244}]}, {"text": "Recently, sequence to sequence (Seq2Seq) models) have been widely used in this area.", "labels": [], "entities": []}, {"text": "In these Seq2Seq models, a recurrent neural network (RNN) based encoder is first utilized to encode the input post to a vector, and another RNN decoder is then used to automatically generate the response word byword.", "labels": [], "entities": []}, {"text": "The parameters of the encoder and decoder are learned by maximizing the averaged likelihood of the training data.", "labels": [], "entities": []}, {"text": "It is clear that the requirements for generated responses are distinct in different dialogue scenarios.", "labels": [], "entities": []}, {"text": "For instance, in the scenario of customer service or mobile assistant, users mainly expect the system to help them solve a problem.", "labels": [], "entities": []}, {"text": "Therefore, the responses should be specific and accurate to provide useful assistance.", "labels": [], "entities": []}, {"text": "For example, if the user asks a question 'How can I get the AMD driver running on Ubuntu 12.10?', the system is expected to reply 'The fglrx driver is in the repo.", "labels": [], "entities": []}, {"text": "But it may depend on your exact chipset.', rather than 'I do not know about the package.', even though the latter can also be viewed as relevant for the proposed question.", "labels": [], "entities": []}, {"text": "We called this kind of scenario as specific-requirement scenario.", "labels": [], "entities": []}, {"text": "While in other scenarios such as chatbot, users are interacting with the dialogue system for fun.", "labels": [], "entities": []}, {"text": "Therefore, the generated responses should be diverse to attract different users.", "labels": [], "entities": []}, {"text": "Take the post 'Can you recommend me a tourist city?'", "labels": [], "entities": []}, {"text": "If the user prefers the magnificent mountains and rivers, it is better to reply 'You may like the Bernina Express to the Alps'.", "labels": [], "entities": [{"text": "Bernina Express", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.9529751539230347}]}, {"text": "While if the user loves literature, it is better to reply 'Paris is a beautiful city with full of the literary atmosphere'.", "labels": [], "entities": []}, {"text": "This kind of scenario is called diverse-requirement scenario.", "labels": [], "entities": []}, {"text": "However, the current generation model Seq2Seq () usually tend to generate common responses, such as 'I don't know' and 'What does this mean?'", "labels": [], "entities": []}, {"text": "(, which fails to meet diverse requirements for different conversation scenarios.", "labels": [], "entities": []}, {"text": "Intrinsically, conversation is atypical one-to-many application, i.e., multiple responses with different semantic meanings are correspondent to a same post.", "labels": [], "entities": []}, {"text": "That means there are various post-response matching patterns in the training data.", "labels": [], "entities": []}, {"text": "Seq2Seq optimizes an averaged likelihood, so it can only capture the common matching patterns, leading to common responses.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to propose two tailored optimization criteria for Seq2Seq models to accommodate different conversation scenarios, i.e. specific-requirement scenario and diverserequirement scenario.", "labels": [], "entities": []}, {"text": "The key idea is to how capture the required post-response matching patterns.", "labels": [], "entities": []}, {"text": "For the specific-requirement scenario, we define the maximum generated likelihood as the objective function.", "labels": [], "entities": [{"text": "maximum generated likelihood", "start_pos": 53, "end_pos": 81, "type": "METRIC", "confidence": 0.5991201102733612}]}, {"text": "With this kind of criterion, we just require one ground-truth response to be close to the given post, instead of requiring the average of multiple ground-truth responses to be close to the post.", "labels": [], "entities": []}, {"text": "Therefore, the most significant post-response matching pattern will be learned from the data, to facilitate generating a specific response.", "labels": [], "entities": []}, {"text": "While for the diverse-requirement scenario, the conditional value-at-risk (CVaR) is used as the objective function.", "labels": [], "entities": [{"text": "conditional value-at-risk (CVaR)", "start_pos": 48, "end_pos": 80, "type": "METRIC", "confidence": 0.6519473671913147}]}, {"text": "CVaR is a risk-sensitive function widely used in finances (, defined to assessing the likelihood (at a specific confidence level) that a specific loss will exceed the value at risk.", "labels": [], "entities": []}, {"text": "With CVaR as the objective function, the worst 1-\u03b1 responses are required to be close to the post, therefore various post-response patterns can be captured, and the learned model has the ability to generate diverse responses.", "labels": [], "entities": []}, {"text": "We use public data to evaluate our proposed models.", "labels": [], "entities": []}, {"text": "For the specific-requirement scenario, the experiments on public Ubuntu dialogue corpus(Ubuntu service) show that optimizing the maximum generated likelihood produces more specific and accurate responses than traditional Seq2Seq models.", "labels": [], "entities": [{"text": "Ubuntu dialogue corpus(Ubuntu service)", "start_pos": 65, "end_pos": 103, "type": "DATASET", "confidence": 0.8543317488261631}]}, {"text": "While for the diverserequirement scenario, the experiments on the public Chinese Weibo dataset (social chatbot) show that optimizing CVaR produces diverse responses, as compared with Seq2Seq and the variants.", "labels": [], "entities": [{"text": "Chinese Weibo dataset", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.8323480288187662}]}], "datasetContent": [{"text": "In this section, we conduct experiments on both specific-requirement and diverse-requirement scenarios, to evaluate the performances of our proposed methods.", "labels": [], "entities": []}, {"text": "We use two public datasets in our experiments.", "labels": [], "entities": []}, {"text": "For the specific-requirement scenario, we use the Ubuntu dialogue corpus 1 extracted from Ubuntu question-answering forum, named Ubuntu ().", "labels": [], "entities": [{"text": "Ubuntu dialogue corpus 1 extracted from Ubuntu question-answering forum", "start_pos": 50, "end_pos": 121, "type": "DATASET", "confidence": 0.893942395846049}, {"text": "Ubuntu", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.9443303942680359}]}, {"text": "The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012.", "labels": [], "entities": []}, {"text": "The validation data are conversational pairs from April 27,2014 to August 7,2012, and the test data are from August 7,2012 to December 1,2012.", "labels": [], "entities": []}, {"text": "We set the number of positive examples as 4,000,000 in the Github to directly sample data from the whole corpus.", "labels": [], "entities": []}, {"text": "Then we construct post and response pairs based on the period from both context and utterance.", "labels": [], "entities": []}, {"text": "We also conduct some data pro-processing.", "labels": [], "entities": []}, {"text": "For example, we use the official script to tokenize, stem and lemmatize, and the duplicates and sentences with length less than 5 or longer than 50 are removed.", "labels": [], "entities": []}, {"text": "Finally, we obtain 3,200,000, 100,000 and 100,000 for training, validation and testing, respectively.", "labels": [], "entities": [{"text": "validation", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.9138860106468201}]}, {"text": "For the diverse-requirement scenario, we use the Chinese Weibo dataset, named STC ().", "labels": [], "entities": [{"text": "Chinese Weibo dataset", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8596427838007609}]}, {"text": "It consists of 3,788,571 postresponse pairs extracted from the Chinese Weibo website and cleaned by the data publishers.", "labels": [], "entities": [{"text": "Chinese Weibo website", "start_pos": 63, "end_pos": 84, "type": "DATASET", "confidence": 0.912453273932139}]}, {"text": "We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively.  beddings.", "labels": [], "entities": []}, {"text": "For STC, we utilize character-level embeddings rather than word-level embeddings, due to the word sparsity, segmentation mistakes and unknown Chinese words which may lead to inferior performance (.", "labels": [], "entities": [{"text": "STC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8588672280311584}]}, {"text": "For Ubuntu, we use word embeddings trained by word2vec on the training dataset.", "labels": [], "entities": []}, {"text": "In the training process, the dimension is set to be 300, the size of negative sample is set to be 3, and the learning rate is 0.05.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 109, "end_pos": 122, "type": "METRIC", "confidence": 0.9633825123310089}]}, {"text": "For fair comparison among all the baseline methods and our methods, the number of hidden nodes is all set to 300, and batch size is set to 200.", "labels": [], "entities": []}, {"text": "Stochastic gradient decent (SGD) is utilized in our experiment for optimization, instead of Adam, because SGD yields better performances in our experiments.", "labels": [], "entities": []}, {"text": "The learning rate is set to be 0.5, and adaptively decays with rate 0.99 in the optimization process.", "labels": [], "entities": []}, {"text": "We run our model on a Tesla K80 GPU card with Tensorflow framework.", "labels": [], "entities": []}, {"text": "All the methods are pretrained with the same Seq2Seq model.", "labels": [], "entities": []}, {"text": "For maximum generated likelihood(MGL) model, some people may argue that the specific results maybe due to the usage of single postresponse pair.", "labels": [], "entities": []}, {"text": "Thus we also implement the baseline of using a single post-response pair, by random selecting the response from the ground-truth for each post, denoted as Single Model.", "labels": [], "entities": []}, {"text": "We use both quantitative metrics and human judgements to evaluate the proposed MGL model and the CVaR model.", "labels": [], "entities": [{"text": "MGL", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.7016340494155884}]}, {"text": "Specifically, we use two kinds of metrics for quantitative comparisons.", "labels": [], "entities": []}, {"text": "The first one kind is the traditional metric, including PPL and Bleu score ().", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9528879523277283}]}, {"text": "They are both widely used in natural language processing, and here we use them to evaluate the quality of the generated responses.", "labels": [], "entities": []}, {"text": "The other kind is to evaluate the specific degree 3 in.", "labels": [], "entities": []}, {"text": "It measures the specific degree of the generated responses, by calculating the number of distinct unigrams and bigrams in the generated responses, denoted as distinct.", "labels": [], "entities": []}, {"text": "If a model usually generates common responses, the distinct will below.", "labels": [], "entities": []}, {"text": "For the diverse-requirement scenario, we define two measures to evaluate the performance.", "labels": [], "entities": []}, {"text": "Specifically, we set the beam as 10.", "labels": [], "entities": []}, {"text": "defined to calculate the difference between each two generations for one post, denoted as divrs.", "labels": [], "entities": []}, {"text": "Group-overlap is defined to calculate the overlap between each two generations for one post, denoted as overlap.", "labels": [], "entities": [{"text": "overlap", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9580053091049194}]}, {"text": "The detailed definitions are shown as follows.", "labels": [], "entities": []}, {"text": "where G i1 and G i2 are the generated responses from the model for post X, cosine(G i1 , G i2 ) is the cosine similarity, and the overlap(G i1 , G i2 ) is defined as the intersection divided by union.", "labels": [], "entities": []}, {"text": "For human evaluation, given 200 randomly sampled post and it's generated responses, three annotators, randomly selected from a class of computer science majored students(48 students), are required to give 3-graded judgements.", "labels": [], "entities": []}, {"text": "The annotation criteria are defined as follows: 1.", "labels": [], "entities": []}, {"text": "the response is nonfluent or has wrong logic; or the response is fluent but not related with the post; 2.", "labels": [], "entities": []}, {"text": "the response is fluent and weak related, but it's common which can reply many other posts; 3.", "labels": [], "entities": []}, {"text": "the response is fluent and strong related with its post, which is like following areal person's tone.", "labels": [], "entities": []}, {"text": "The quantitative evaluation results are shown in  The human evaluation results are shown in.", "labels": [], "entities": []}, {"text": "From the results, we can see MGL and CVaR models achieve comparable results, which are significantly better than baseline methods.", "labels": [], "entities": []}, {"text": "Specifically, the averaged score of MGL and CVaR is 2.15 and 1.995, which is significantly higher than that of Adver-REGS and Mechanism, i.e., 1.83 and 1.775, respectively.", "labels": [], "entities": []}, {"text": "The percentage of strongly related sentences (i.e., the grade '3') of MGL Model and CVaR are 52% and 44%, which are also significantly higher than that of Adver-REGS and Mechanism, i.e., 31.5% and 30%.", "labels": [], "entities": [{"text": "MGL Model", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.7728960514068604}]}, {"text": "We conducted significant test for the improvement.", "labels": [], "entities": []}, {"text": "As compared with Adver-REGS and Mechanism, both the metric-based improvements and human evaluation improvements of CVaR are significant on STC datasets (p-value < 0.01).", "labels": [], "entities": [{"text": "STC datasets", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.7508567869663239}]}], "tableCaptions": [{"text": " Table 1: The metric-based evaluation results(%)  of different models on Ubuntu.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9589350819587708}]}, {"text": " Table 2: The comparisons of different models by  human evaluation on Ubuntu.", "labels": [], "entities": [{"text": "Ubuntu", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.973930299282074}]}, {"text": " Table 4: The metric-based evaluation results(%)", "labels": [], "entities": []}, {"text": " Table 5: The comparisons of different models by  human evaluation on STC.", "labels": [], "entities": [{"text": "STC", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.603706955909729}]}]}