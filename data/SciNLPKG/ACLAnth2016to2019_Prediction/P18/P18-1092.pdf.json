{"title": [{"text": "Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module", "labels": [], "entities": [{"text": "Augmenting Memory Networks", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.8656173348426819}]}], "abstractContent": [{"text": "During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks.", "labels": [], "entities": []}, {"text": "To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms.", "labels": [], "entities": []}, {"text": "These architectures, however , lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning.", "labels": [], "entities": [{"text": "relational reasoning", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8562018275260925}]}, {"text": "Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks.", "labels": [], "entities": [{"text": "relational reasoning tasks", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.8928404251734415}]}, {"text": "Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems.", "labels": [], "entities": []}, {"text": "To solve these issues, we introduce the Working Memory Network , a MemNN architecture with a novel working memory storage and reasoning module.", "labels": [], "entities": []}, {"text": "Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear.", "labels": [], "entities": []}, {"text": "We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR.", "labels": [], "entities": [{"text": "text QA dataset bAbI", "start_pos": 27, "end_pos": 47, "type": "DATASET", "confidence": 0.6699507087469101}, {"text": "QA dataset NLVR", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.7181520710388819}]}, {"text": "In the jointly trained bAbI-10k, we set anew state-of-the-art, achieving a mean error of less than 0.5%.", "labels": [], "entities": [{"text": "mean error", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.8966729640960693}]}, {"text": "Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.", "labels": [], "entities": []}], "introductionContent": [{"text": "A central ability needed to solve daily tasks is complex reasoning.", "labels": [], "entities": [{"text": "complex reasoning", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.6781828552484512}]}, {"text": "It involves the capacity to comprehend and represent the environment, retain information from past experiences, and solve problems based on the stored information.", "labels": [], "entities": []}, {"text": "Our ability to solve those problems is supported by multiple specialized components, including shortterm memory storage, long-term semantic and procedural memory, and an executive controller that, among others, controls the attention over memories.", "labels": [], "entities": []}, {"text": "Many promising advances for achieving complex reasoning with neural networks have been obtained during the last years.", "labels": [], "entities": []}, {"text": "Unlike symbolic approaches to complex reasoning, deep neural networks can learn representations from perceptual information.", "labels": [], "entities": []}, {"text": "Because of that, they do not suffer from the symbol grounding problem), and can generalize better than classical symbolic approaches.", "labels": [], "entities": []}, {"text": "Most of these neural network models make use of an explicit memory storage and an attention mechanism.", "labels": [], "entities": []}, {"text": "For instance, Memory Networks (MemNN), Dynamic Memory Networks (DMN) or Neural Turing Machines (NTM) () build explicit memories from the perceptual inputs and access these memories using learned attention mechanisms.", "labels": [], "entities": []}, {"text": "After that some memories have been attended, using a multi-step procedure, the attended memories are combined and passed through a simple output layer that produces a final answer.", "labels": [], "entities": []}, {"text": "While this allows some multi-step inferential process, these networks lack a more complex reasoning mechanism, needed for more elaborated tasks such as inferring relations among entities (relational reasoning).", "labels": [], "entities": []}, {"text": "On the contrary, Relation Networks (RNs), proposed in, have shown outstanding performance in relational reasoning tasks.", "labels": [], "entities": [{"text": "relational reasoning tasks", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.8720788359642029}]}, {"text": "Nonetheless, a major drawback of RNs is that they consider each of the input objects in pairs, having to process a quadratic number of relations.", "labels": [], "entities": []}, {"text": "That limits the usability of the model on large problems and makes forward and backward computations quite expensive.", "labels": [], "entities": []}, {"text": "To solve these problems we propose a novel Memory Network: The W-MemNN model applied to textual question answering.", "labels": [], "entities": [{"text": "textual question answering", "start_pos": 88, "end_pos": 114, "type": "TASK", "confidence": 0.5956595440705618}]}, {"text": "Each input fact is processed using a GRU, and the output representation is stored in the short-term memory storage.", "labels": [], "entities": []}, {"text": "Then, the attentional controller computes an output vector that summarizes relevant parts of the memories.", "labels": [], "entities": []}, {"text": "This process is repeated H hops (a dotted line delimits each hop), and each output is stored in the working memory buffer.", "labels": [], "entities": []}, {"text": "Finally, the output of each hop is passed to the reasoning module that produces the final output.", "labels": [], "entities": []}, {"text": "architecture called the Working Memory Network (W-MemNN).", "labels": [], "entities": []}, {"text": "Our model augments the original MemNN with a relational reasoning module and anew working memory buffer.", "labels": [], "entities": [{"text": "MemNN", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8720518946647644}]}, {"text": "The attention mechanism of the Memory Network allows the filtering of irrelevant inputs, reducing a lot of the computational complexity while keeping the relational reasoning capabilities of the RN.", "labels": [], "entities": []}, {"text": "Three main components compose the W-MemNN: An input module that converts the perceptual inputs into an internal vector representation and save these representations into a short-term storage, an attentional controller that attend to these internal representations and update a working memory buffer, and a reasoning module that operates on the set of objects stored in the working memory buffer in order to produce a final answer.", "labels": [], "entities": []}, {"text": "This component-based architecture is inspired by the well-known model from cognitive sciences called the multi-component working memory model, proposed in.", "labels": [], "entities": []}, {"text": "We studied the proposed model on the text-based QA benchmark bAbI ( ) which consists of 20 different toy tasks that measure different reasoning skills.", "labels": [], "entities": [{"text": "QA benchmark bAbI", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.5844245652357737}]}, {"text": "While models such as EntNet () have focused on the pertask training version of the benchmark (where a different model is trained for each task), we decided to focus on the jointly trained version of the task, where the model is trained on all tasks simultaneously.", "labels": [], "entities": []}, {"text": "In the jointly trained bAbI-10k benchmark we achieved state-of-the-art performance, improving the previous state-of-the-art on more than 2%.", "labels": [], "entities": [{"text": "bAbI-10k benchmark", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.718693345785141}]}, {"text": "Moreover, a simple ensemble of two of our models can solve all 20 tasks simultaneously.", "labels": [], "entities": []}, {"text": "Also, we tested our model on the visual QA dataset NLVR.", "labels": [], "entities": [{"text": "QA dataset NLVR", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7557228207588196}]}, {"text": "In that dataset, we obtained performance at the level of the Module Neural Networks (.", "labels": [], "entities": []}, {"text": "Our model, however, achieves these results using the raw input statements, without the extra text processing used in the Module Networks.", "labels": [], "entities": []}, {"text": "Finally, qualitative and quantitative analysis shows that the inclusion of the Relational Reasoning module is crucial to improving the performance of the MemNN on tasks that involve relational reasoning.", "labels": [], "entities": [{"text": "Relational Reasoning", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7511967420578003}]}, {"text": "We can achieve this performance by also reducing the computation times of the RN considerably.", "labels": [], "entities": []}, {"text": "Consequently, we hope that this contribution may allow applying RNs to larger problems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test accuracies on the jointly trained bAbI-10k dataset. MN-S stands for strongly supervised  Memory Network, MN-U for end-to-end Memory Network without supervision, and WMN for Working  Memory Network. Results for LSTM, MN-U, and MN-S are took from Sukhbaatar et al. (2015). Results  for SDNC are took from Rae et al. (2016). WMN  \u2020 is an ensemble of two Working Memory Networks.", "labels": [], "entities": [{"text": "bAbI-10k dataset", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.8165114223957062}]}, {"text": " Table 2: Examples of visualizations of attention for textual and visual QA. Top: Visualization of attention  values for the NLVR dataset. To get more aesthetic figures we applied a gaussian blur to the attention  matrix. Bottom: Attention values for the bAbI dataset. In each cell, the sum of the attention for all heads  is shown.", "labels": [], "entities": [{"text": "NLVR dataset", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.9805586338043213}, {"text": "bAbI dataset", "start_pos": 255, "end_pos": 267, "type": "DATASET", "confidence": 0.7156492918729782}]}]}