{"title": [{"text": "Baseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Baseline: a library for reproducible deep learning research and fast model development for NLP.", "labels": [], "entities": []}, {"text": "The library provides easily extensible abstractions and implementations for data loading , model development, training and export of deep learning architectures.", "labels": [], "entities": [{"text": "data loading", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.7423413991928101}]}, {"text": "It also provides implementations for simple, high-performance, deep learning models for various NLP tasks, against which newly developed models can be compared.", "labels": [], "entities": []}, {"text": "Deep learning experiments are hard to reproduce , Baseline provides functionalities to track them.", "labels": [], "entities": []}, {"text": "The goal is to allow a researcher to focus on model development, delegating the repetitive tasks to the library .", "labels": [], "entities": [{"text": "model development", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7017443031072617}]}], "introductionContent": [{"text": "Deep Neural Network models (DNNs) now dominate the NLP literature.", "labels": [], "entities": []}, {"text": "However, the immense progress comes with some issues.", "labels": [], "entities": []}, {"text": "Often the research is not reproducible.", "labels": [], "entities": []}, {"text": "Sometimes the code is not open source.", "labels": [], "entities": []}, {"text": "Other times, available implementations fail to match the reported performance.", "labels": [], "entities": []}, {"text": "When training DNNs, even simple baselines can take a lot of time and resources to reach peak performance (.", "labels": [], "entities": []}, {"text": "Additionally, a simple, canonical way to evaluate new models is lacking.", "labels": [], "entities": []}, {"text": "Institutional pressures exist to show large relative gains in papers (.", "labels": [], "entities": []}, {"text": "As a result, new models are often compared with weak baselines.", "labels": [], "entities": []}, {"text": "When software is provided, it is common for authors to provide the source code as a standalone application.", "labels": [], "entities": []}, {"text": "These projects include data processing, data cleaning, model training, and evaluation code, yielding an error-prone and timeconsuming development process.", "labels": [], "entities": [{"text": "data processing", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7621830999851227}, {"text": "data cleaning", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.8527566492557526}]}, {"text": "A complete library should be used to automate the mundane portions of development, allowing a researcher to focus on model improvements.", "labels": [], "entities": []}, {"text": "Also, it should be easy to compare the results of anew model across various hyper-parameter configurations and strong baselines.", "labels": [], "entities": []}, {"text": "To solve these problems, we have developed Baseline 1 . It has three components.", "labels": [], "entities": []}, {"text": "Core: An object-oriented Python library for rapid development of deep learning algorithms.", "labels": [], "entities": []}, {"text": "Core provides extensible base classes for common components in a deep learning architecture (data loading, model development, training, evaluation, and export) in TensorFlow and PyTorch, with experimental support for DyNet.", "labels": [], "entities": [{"text": "data loading", "start_pos": 93, "end_pos": 105, "type": "TASK", "confidence": 0.7143512219190598}]}, {"text": "In addition, it provides strong, deep learning baselines for four fundamental NLP tasks -Classification, Sequence Tagging, Sequence-to-Sequence EncoderDecoders and Language Modeling.", "labels": [], "entities": [{"text": "Classification", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.9115585088729858}, {"text": "Sequence Tagging", "start_pos": 105, "end_pos": 121, "type": "TASK", "confidence": 0.876596063375473}, {"text": "Language Modeling", "start_pos": 164, "end_pos": 181, "type": "TASK", "confidence": 0.7394773364067078}]}, {"text": "Many NLP problems can be seen as variants of these tasks.", "labels": [], "entities": []}, {"text": "For example, Part of Speech (POS) Tagging, Named Entity Recognition (NER) and Slot-filling are all Sequence Tagging tasks, Neural Machine Translation (NMT) is typically modeled as an Encoder-Decoder task.", "labels": [], "entities": [{"text": "Part of Speech (POS) Tagging", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.727740866797311}, {"text": "Named Entity Recognition (NER)", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.7706921696662903}, {"text": "Sequence Tagging", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.8067752122879028}, {"text": "Neural Machine Translation (NMT)", "start_pos": 123, "end_pos": 155, "type": "TASK", "confidence": 0.85114586353302}]}, {"text": "An end-user can easily implement anew model and delegate the rest to the library.", "labels": [], "entities": []}, {"text": "MEAD: A library built on Core for fast Modeling, Experimentation And Development.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5917900800704956}]}, {"text": "MEAD contains driver programs to run experiments from JSON or YAML configuration files to completely control the reader, trainer, model, and hyperparameters.", "labels": [], "entities": []}, {"text": "XPCTL: A command-line interface to track experimental results and provide access to a global leaderboard.", "labels": [], "entities": [{"text": "XPCTL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8671684861183167}]}, {"text": "After running an experiment through MEAD, the results and the logs are committed to a database.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8917406797409058}]}, {"text": "Several commands are provided to show the best experimental results under various constraints.", "labels": [], "entities": []}, {"text": "The workflow for developing a deep learning model using Baseline is simple: 1.", "labels": [], "entities": []}, {"text": "Map the problem to one of the existing tasks using a <task, dataset> tuple, eg., NER on CoNLL 2003 dataset is a <tagger task, conll> 2.", "labels": [], "entities": [{"text": "CoNLL 2003 dataset", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.9532219370206197}]}, {"text": "Use the existing implementations in Core or extend the base model class to create anew architecture; 3.", "labels": [], "entities": []}, {"text": "Define a configuration file in MEAD and run an experiment.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8546362519264221}]}, {"text": "4. Use XPCTL to compare the result with the previous experiments, commit the results to the leaderboard database and the model files to a persistent storage if desired.", "labels": [], "entities": []}, {"text": "Additionally, the base models provided by the library can be exported from saved checkpoints directly into TensorFlow Serving 2 for deployment in a production environment.", "labels": [], "entities": [{"text": "TensorFlow Serving", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8963413834571838}]}, {"text": "the framework can be run within a Docker container to reduce the installation complexity and to isolate experiment configurations and variants.", "labels": [], "entities": []}, {"text": "It is open-sourced, actively maintained by a team of core developers and accepts public contributions.", "labels": [], "entities": []}, {"text": "While some components from the library can be used for generic machine learning and computer vision tasks, the primary focus of Baseline is NLP: currently the data loaders, models and evaluation codes are provided for NLP tasks only.", "labels": [], "entities": []}], "datasetContent": [{"text": "return MyModel.create(embeddings, labels, ** kwargs) def load_model(modelname, ** kwargs): return MyModel.load(modelname, ** kwargs) Listing 1: Methods to override and expose in a user-defined model nificantly since our implementation and we anticipate releasing anew baseline model in the future (.", "labels": [], "entities": [{"text": "MyModel.load", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9424484372138977}]}, {"text": "Our Encoder-Decoder model is tested on English-Vietnamese Translation Task on TED tst2013 corpus () and achieves strong results.", "labels": [], "entities": [{"text": "English-Vietnamese Translation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6088809818029404}, {"text": "TED tst2013 corpus", "start_pos": 78, "end_pos": 96, "type": "DATASET", "confidence": 0.8968463738759359}]}, {"text": "Apart from these base models, we provide implementations of more advanced models that can demonstrate the software architecture and provide a steppingstone for researchers developing their own models.", "labels": [], "entities": []}, {"text": "Some of these implementations show better results than the existing state of the art.", "labels": [], "entities": []}, {"text": "For example, using pre-trained ELMo embeddings from TensorFlow Hub (, our tagger has 42.19% F1 for the wnut17 dataset, which is better than the last reported highest score (41.86%) on the dataset (.", "labels": [], "entities": [{"text": "TensorFlow Hub", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.7370588779449463}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9993368983268738}, {"text": "wnut17 dataset", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.9517125487327576}]}, {"text": "The repository 4 is updated continually with the list of available implementations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for TensorFlow implementations in Baseline", "labels": [], "entities": []}]}