{"title": [{"text": "Learning Distributional Token Representations from Visual Features", "labels": [], "entities": [{"text": "Learning Distributional Token Representations from Visual Features", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.7992542386054993}]}], "abstractContent": [{"text": "In this study, we compare token representations constructed from visual features (i.e., pixels) with standard lookup-based embeddings.", "labels": [], "entities": []}, {"text": "Our goal is to gain insight about the challenges of encoding a text representation from low-level features, e.g. from characters or pixels.", "labels": [], "entities": []}, {"text": "We focus on Chinese, which-as a logographic language-has properties that make a representation via visual features challenging and interesting.", "labels": [], "entities": []}, {"text": "To train and evaluate different models for the token representation, we chose the task of character-based neu-ral machine translation (NMT) from Chi-nese to English.", "labels": [], "entities": [{"text": "character-based neu-ral machine translation (NMT)", "start_pos": 90, "end_pos": 139, "type": "TASK", "confidence": 0.7148586085864476}]}, {"text": "We found that a token representation computed only from visual features can achieve competitive results to lookup embeddings.", "labels": [], "entities": []}, {"text": "However, we also show different strengths and weaknesses in the models' performance in a part-of-speech tagging task and also a semantic similarity task.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 89, "end_pos": 116, "type": "TASK", "confidence": 0.7458711465199789}]}, {"text": "In summary, we show that it is possible to achieve a text representation only from pixels.", "labels": [], "entities": []}, {"text": "We hope that this is a useful steppingstone for future studies that exclusively rely on visual input, or aim at exploiting visual features of written language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language representation beyond the word level can be advantageous for words from the tail of the distribution, as has been shown in recent neural approaches for various tasks.", "labels": [], "entities": [{"text": "Language representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.654265746474266}]}, {"text": "In these approaches, a neural model represents an input text based on its sequence of characters or character n-grams (instead of its words).", "labels": [], "entities": []}, {"text": "This helps the model to handle outof-vocabulary tokens and avoids the need of text segmentation or tokenization, which remains an unsolved problem for many languages.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7010059058666229}]}, {"text": "For some applications, e.g. language processing for social media, models should be able to capture the creative use of language.", "labels": [], "entities": [{"text": "language processing", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.725218802690506}]}, {"text": "However, a disadvantage is that we loose the certainty of a known vocabulary.", "labels": [], "entities": [{"text": "certainty", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9910560846328735}]}, {"text": "Also, regarding computational complexity, there is a trade-off between the memory that is required for large embedding lookup tables and the additional computational cost that is required to compose representations from low-level features.", "labels": [], "entities": []}, {"text": "In contrast to the characters of languages based on the Latin alphabet, the Chinese written language is defined over a set of \u2248 8000 characters that already carry meaning.", "labels": [], "entities": []}, {"text": "The characters can either appear in a traditional or simplified form, and many share visual components that can indicate a related meaning.", "labels": [], "entities": []}, {"text": "Thus, it is reasonable to hypothesize that encoding Chinese characters directly from their visual components might improve their token representation in a neural network model.", "labels": [], "entities": []}, {"text": "In recent studies, evaluated character encodings from visual features by classifying Wikipedia titles into 12 categories and Su and Lee (2017) evaluated such encodings by measuring correlation with human similarity judgments.", "labels": [], "entities": []}, {"text": "Both studies found that using character encodings from visual input did not outperform or were equal to lookup-based embeddings.", "labels": [], "entities": []}, {"text": "No support for the hypothesis above is thus provided.", "labels": [], "entities": []}, {"text": "In this study, we aim to explore the question of whether and when visual-feature representations are beneficial or detrimental.", "labels": [], "entities": []}, {"text": "We make the following contributions: (i) Since the evaluation tasks from prior studies did not test the capabilities of the visual features for text representation, we propose to employ the task of neural machine trans-lation (NMT) for training and evaluation.", "labels": [], "entities": [{"text": "text representation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7524840235710144}]}, {"text": "We argue that NMT requires the token representation to serve as a reliable syntactic and semantic signal.", "labels": [], "entities": []}, {"text": "(ii) Prior work reported evaluations for one architecture to encode the visual features.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate different settings and argue for architecture choices that performed well in our experiments.", "labels": [], "entities": []}, {"text": "(iii) We provide evidence for the possibility to compute token representations from visual features that perform on-par with lookup-based embeddings in NMT.", "labels": [], "entities": []}, {"text": "(iv) Finally, we revisit two of the tasks from prior work that use visual features: measuring correlation with semantic similarity judgments by humans as well as joint segmentation and part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 185, "end_pos": 207, "type": "TASK", "confidence": 0.7164903432130814}]}, {"text": "We use the best models from the NMT evaluation in these two tasks.", "labels": [], "entities": []}, {"text": "For semantic similarity, we find that token representation from pixels are clearly beneficial for unseen characters, while a lookup embedding performs better for seen characters.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7482327818870544}]}, {"text": "For joint segmentation and part-of-speech tagging we find no clear difference between lookup embeddings and character representation from pixels.", "labels": [], "entities": [{"text": "joint segmentation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7008646726608276}, {"text": "part-of-speech tagging", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7366160750389099}]}], "datasetContent": [{"text": "In this section, we describe the results of our experimental study.", "labels": [], "entities": []}, {"text": "First, we report how the character images were created, followed by the experiments for the neural machine translation task.", "labels": [], "entities": [{"text": "neural machine translation task", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.7238167747855186}]}, {"text": "To expand on these results, we describe experiments and results for joint segmentation and partof-speech tagging, as well as for measuring correlation with semantic similarity judgments.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.7962746322154999}]}, {"text": "For the machine translation experiments, we trained a NMT model to translate from Chinese to English.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7822403907775879}, {"text": "translate from Chinese to English", "start_pos": 67, "end_pos": 100, "type": "TASK", "confidence": 0.8360054850578308}]}, {"text": "In the following sections, we describe the model, the data and the training settings, followed by the results.", "labels": [], "entities": []}, {"text": "NMT Model We used a standard sequence-tosequence model with a recurrent encoder and attention-based decoder (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9018475413322449}]}, {"text": "This architecture does not represent the state of the art in neural machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6871130267779032}]}, {"text": "However, due to its wide adoption in empirical research, there is broad knowledge about suitable hyper-parameters, which makes it a preferred choice for our study.", "labels": [], "entities": []}, {"text": "The coarse architecture of this model can be described by an encoder enc and a decoder dec.", "labels": [], "entities": []}, {"text": "The encoder enc is a function that takes a tokenized text T in a source vocabulary as input and computes a representation that is the input for the subsequent decoder.", "labels": [], "entities": []}, {"text": "The decoder dec then creates a sequence of tokens in the target vocabulary.", "labels": [], "entities": []}, {"text": "The final output is dec(enc(T )).", "labels": [], "entities": []}, {"text": "The input of the encoder enc is first transformed to a dense vector representation, i.e., each input token is transformed by function D of Section 3.1.", "labels": [], "entities": []}, {"text": "In the following experiments, we evaluate different choices for D.) with the standard parameters of PyTorch.", "labels": [], "entities": []}, {"text": "We used a learning rate of 10 \u22123 for 4 epochs, then we halved the learning rate every epoch until we reached 10 \u22125 . For regularization, we used dropout with probability 0.2 in both encoder and decoder RNNs.", "labels": [], "entities": [{"text": "regularization", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.9668790698051453}]}, {"text": "We pretrained the decoder for 20 epochs to a perplexity of 108.21 on the validation data.", "labels": [], "entities": []}, {"text": "This did result in: faster convergence, improved fluency of translations, and less variance of evaluation results.", "labels": [], "entities": [{"text": "convergence", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9506014585494995}, {"text": "variance", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9858272075653076}]}, {"text": "We trained all the models for 10 epochs.", "labels": [], "entities": []}, {"text": "We selected the models by the best batchbased approximate BLEU score on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9828229546546936}]}, {"text": "This worked slightly better than selecting them by the best perplexity.", "labels": [], "entities": []}, {"text": "We perform translation via beam search with a beam-size of 10 and length normalization of 0.9.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9781950116157532}, {"text": "length normalization", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.9594374001026154}]}, {"text": "We trained the NMT models with each of the character encoders of as well as with lookup-based embeddings (EMB) as a baseline.", "labels": [], "entities": []}, {"text": "Results summarizes the results of the machine translation experiments.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7702532708644867}]}, {"text": "We include the result from a WMT 2017 system to give context for the expected BLEU score in this task.", "labels": [], "entities": [{"text": "WMT 2017 system", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.847130298614502}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9748644828796387}]}, {"text": "This result is from a baseline NMT system for Chinese to English (, which is most comparable to our approach (i.e., no reranking, no ensembles, no special treatment of names and numbers).", "labels": [], "entities": []}, {"text": "However, a crucial difference is that the WMT 2017 system uses pre-segmented text, which yields a vocabulary size of 300k on the   source side, while our system is character-based.", "labels": [], "entities": [{"text": "WMT 2017 system", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.9228889147440592}]}, {"text": "Our results indicate that using only one fully connected transformation layer FC 1L for character encoding is possible but does not yield comparable results to the best-performing convolutional architectures.", "labels": [], "entities": [{"text": "character encoding", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8417190909385681}]}, {"text": "However, the FC 2L comes close to the CNN models.", "labels": [], "entities": []}, {"text": "Note that CNN+SM+FC 4096 needs a larger number of CNN features to perform comparable to CNN+FC+ReLU 1024.", "labels": [], "entities": [{"text": "CNN+SM+FC 4096", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9308358728885651}, {"text": "CNN+FC+ReLU 1024", "start_pos": 88, "end_pos": 104, "type": "DATASET", "confidence": 0.8919955889383951}]}, {"text": "In terms of BLEU score, the best character encoders perform equal to a standard lookup embedding.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9784203469753265}]}, {"text": "To gain insight into the differences between the models, we split the test data into three buckets.", "labels": [], "entities": []}, {"text": "Each sentence is scored with 1/n n i=1 #t i , where #t i is the training data frequency of character ti . We partition the test data into a low and high bucket with the sentences scored in the lower and upper quartile, respectively, and amid bucket with the remaining 2nd and 3rd quartile.", "labels": [], "entities": []}, {"text": "Figure 1: Distributions of the top-3 softmax activation magnitudes per character.", "labels": [], "entities": []}, {"text": "shows the results per bucket for CNN+FC+ReLU 1024, CNN+SM+FC 4096, FC 2L and EMB.", "labels": [], "entities": [{"text": "CNN+FC+ReLU 1024", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8923952976862589}, {"text": "CNN+SM+FC 4096", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9302789767583212}, {"text": "EMB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.9144755601882935}]}, {"text": "Interestingly, the CNN+FC+ReLU 1024 and EMB perform differently well in the frequency buckets.", "labels": [], "entities": [{"text": "CNN+FC+ReLU 1024", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.8824742635091146}, {"text": "EMB", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8249772787094116}]}, {"text": "The CNN+FC+ReLU 1024 model, surprisingly, performs better than EMB for highfrequency characters, while this is inversed for the low-frequency characters.", "labels": [], "entities": [{"text": "CNN+FC+ReLU 1024", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8645644883314768}]}, {"text": "For the CNN+SM+FC encoders, our hypothesis was that the sparse activation of the softmax could act like a soft lookup function; i.e., it selects only few rows for the subsequent fully connected layer.", "labels": [], "entities": [{"text": "CNN+SM+FC encoders", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8501245776812235}]}, {"text": "We measured the top-3 softmax activation magnitudes per character from a random sample of 350 sentences from the training data (a total of 11234 words).", "labels": [], "entities": []}, {"text": "As shown in, the activations are indeed spiked, which supports our hypothesis.", "labels": [], "entities": []}, {"text": "However, in our evaluation, we found no advantage over CNN+FC+ReLU encoders.", "labels": [], "entities": [{"text": "CNN+FC+ReLU encoders", "start_pos": 55, "end_pos": 75, "type": "DATASET", "confidence": 0.8709986905256907}]}, {"text": "Tagging and Segmentation In the translation experiment we evaluated the token representations fora syntactic and semantic signal.", "labels": [], "entities": []}, {"text": "In this experiment, we want to investigate the morpho-syntactic information in the representations.", "labels": [], "entities": []}, {"text": "To evaluate this, we employ the task of joint part-of-speech tagging and segmentation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6938153803348541}]}, {"text": "Model We used a Linear-Chain-CRF with a CNN encoder () to compute the emissions.", "labels": [], "entities": []}, {"text": "The encoder is a three-layer CNN with kernel size 3, iteratively growing dilations, residual connections, ReLU activations, and a hidden size of 512.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8961923718452454}]}, {"text": "We do not report numbers fora BI-RNN-CRF similar to, because our implementation did not yield their results for unigrams which is most likely caused by the different embeddings..", "labels": [], "entities": [{"text": "BI-RNN-CRF", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9713747501373291}]}, {"text": "Instead of the BIES tagging scheme (begin, inside, end, single), we used the BI scheme, which worked better for our models.", "labels": [], "entities": [{"text": "BIES tagging", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.7073455154895782}, {"text": "BI", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9885376691818237}]}, {"text": "Training We use the parameters from the EMB, CNN+FC+ReLU 1024, FC2L 512 and CNN+SM+FC 4096 of the NMT task.", "labels": [], "entities": [{"text": "EMB", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8900829553604126}, {"text": "CNN+FC+ReLU 1024", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.7821320593357086}, {"text": "CNN+SM+FC 4096", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.863548070192337}]}, {"text": "We either adjust these parameters during training (adapt) or keep them fixed (fix).", "labels": [], "entities": []}, {"text": "For training, we used Adagrad () with an initial learning rate of 0.1, and dropout for regularization with probability 0.1 for the encoder and 0.5 for the output classifier.", "labels": [], "entities": []}, {"text": "We trained the models for 50 epochs and selected the model with the best accuracy on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9991868138313293}]}, {"text": "Results Our results reported in are averaged over two distinct runs each.", "labels": [], "entities": []}, {"text": "For comparison, we show results reported for the 1-gram and 3-gram lookup model of.", "labels": [], "entities": []}, {"text": "Our models correspond to the 1-gram model, as they are character-based.", "labels": [], "entities": []}, {"text": "We find no clear difference between lookup-based and the best character encoder models.", "labels": [], "entities": []}, {"text": "Interestingly, CNN+SM+FC 4096 (softmax) is the weakest model.", "labels": [], "entities": [{"text": "CNN+SM+FC 4096", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.8305535316467285}]}, {"text": "The sparseness in the in the signal apparently removes syntactic information.", "labels": [], "entities": []}, {"text": "Comparing the adapt and the fix setting, we see that adapting the character encoder during training is -in most cases-not helpful.", "labels": [], "entities": []}, {"text": "This is most likely due to the small amount of training data.", "labels": [], "entities": []}, {"text": "In our final experiment, we evaluated the representation of semantic information.", "labels": [], "entities": []}, {"text": "The task is to compute a similarity score for pairs of words.", "labels": [], "entities": []}, {"text": "The evaluation is the Spearman's correlation between the scores and numerical similarity judgments by humans.", "labels": [], "entities": []}, {"text": "The words in this data set are translated into traditional Chinese.", "labels": [], "entities": []}, {"text": "The training data from the NMT task is mostly from news and web sources, which is why our vocabulary contains many but not all Chinese characters.", "labels": [], "entities": [{"text": "NMT task", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.6078594624996185}]}, {"text": "The characters in the modern simplified Chinese sometimes can be visually similar to their predecessors in traditional Chinese, i.e. they share visual components with a related meaning.", "labels": [], "entities": []}, {"text": "Therefore, we can also evaluate the capability of the models to generalize to unseen characters.", "labels": [], "entities": []}, {"text": "For the experiments we use the WordSim-240, WordSim-296 and SimLex-999 datasets provided and described by.", "labels": [], "entities": [{"text": "WordSim-240", "start_pos": 31, "end_pos": 42, "type": "DATASET", "confidence": 0.9781153798103333}, {"text": "WordSim-296", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9740064144134521}, {"text": "SimLex-999 datasets", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.8224431872367859}]}, {"text": "Due to the use of traditional Chinese we are missing at least one character in 430 out of 1536 test examples.", "labels": [], "entities": []}, {"text": "On average, we are missing 1.1 characters per word, where each word has in average 2.13 characters.", "labels": [], "entities": []}, {"text": "We split the data into word pairs in which all characters are completely covered (seen words), and into word pairs whereat least one character is not seen (unseen words).", "labels": [], "entities": []}, {"text": "We evaluate unseen words in two settings: either we remove the unseen characters (seen chars) or not (all chars).", "labels": [], "entities": []}, {"text": "We did not train any model for this experiments.", "labels": [], "entities": []}, {"text": "Most of the words in the dataset are composed of multiple characters, therefore, we average the output of the character encoder into a single wordvector.", "labels": [], "entities": []}, {"text": "Subsequently, we compute the cosine similarity between these vectors of word pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: BLEU scores for translation zh-en on the  WMT17 test data. Result for a word-segmented  baseline model reported by Wang et al. (2017).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999406099319458}, {"text": "WMT17 test data", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9788711468378702}]}, {"text": " Table 5: BLEU scores on the WMT17 test data  for sentence buckets with low, medium and high  frequency characters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994441866874695}, {"text": "WMT17 test data", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.9731628894805908}]}, {"text": " Table 7: Results for semantic similarity on the WordSim-240, WordSim-296 and SimLex-999 data.  Lookup SkipGram (SG-EMB) and SkipGram char. encoder (SG-CNN) reported by Su and Lee (2017).", "labels": [], "entities": [{"text": "WordSim-240", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.976377546787262}, {"text": "WordSim-296 and SimLex-999 data", "start_pos": 62, "end_pos": 93, "type": "DATASET", "confidence": 0.7043018117547035}]}]}