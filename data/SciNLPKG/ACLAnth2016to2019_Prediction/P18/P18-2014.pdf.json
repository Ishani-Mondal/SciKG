{"title": [{"text": "A Walk-based Model on Entity Graphs for Relation Extraction", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.9495266675949097}]}], "abstractContent": [{"text": "We present a novel graph-based neural network model for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.9340550303459167}]}, {"text": "Our model treats multiple pairs in a sentence simultaneously and considers interactions among them.", "labels": [], "entities": []}, {"text": "All the entities in a sentence are placed as nodes in a fully-connected graph structure.", "labels": [], "entities": []}, {"text": "The edges are represented with position-aware contexts around the entity pairs.", "labels": [], "entities": []}, {"text": "In order to consider different relation paths between two entities, we construct up to l-length walks between each pair.", "labels": [], "entities": []}, {"text": "The resulting walks are merged and iteratively used to update the edge representations into longer walks representations.", "labels": [], "entities": []}, {"text": "We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools.", "labels": [], "entities": [{"text": "ACE 2005 dataset", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.9621110955874125}]}], "introductionContent": [{"text": "Relation extraction (RE) is a task of identifying typed relations between known entity mentions in a sentence.", "labels": [], "entities": [{"text": "Relation extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8809750556945801}, {"text": "identifying typed relations between known entity mentions in a sentence", "start_pos": 38, "end_pos": 109, "type": "TASK", "confidence": 0.5245616137981415}]}, {"text": "Most existing RE models treat each relation in a sentence individually.", "labels": [], "entities": [{"text": "RE", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9480248093605042}]}, {"text": "However, a sentence typically contains multiple relations between entity mentions.", "labels": [], "entities": []}, {"text": "RE models need to consider these pairs simultaneously to model the dependencies among them.", "labels": [], "entities": [{"text": "RE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8458755016326904}]}, {"text": "The relation between a pair of interest (namely \"target\" pair) can be influenced by other pairs in the same sentence.", "labels": [], "entities": []}, {"text": "The example illustrated in explains this phenomenon.", "labels": [], "entities": []}, {"text": "The relation between the pair of interest Toefting and capital, can be extracted directly from the target entities or indirectly by incorporating information from other related pairs in the sentence.", "labels": [], "entities": []}, {"text": "The person entity (PER) Toefting is directly related with teammates through the preposition with.", "labels": [], "entities": []}, {"text": "Similarly, teammates is directly related with the geopolitical entity (GPE) capital through the preposition in.", "labels": [], "entities": []}, {"text": "Toefting and capital can be directly related through in or indirectly related through teammates.", "labels": [], "entities": []}, {"text": "Substantially, the path from Toefting to teammates to capital can additionally support the relation between Toefting and capital.", "labels": [], "entities": []}, {"text": "Multiple relations in a sentence between entity mentions can be represented as a graph.", "labels": [], "entities": []}, {"text": "Neural graph-based models have shown significant improvement in modelling graphs over traditional feature-based approaches in several tasks.", "labels": [], "entities": []}, {"text": "They are most commonly applied on knowledge graphs (KG) for knowledge graph completion ( and the creation of knowledge graph embeddings ().", "labels": [], "entities": [{"text": "knowledge graph completion", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6603923141956329}]}, {"text": "These models rely on paths between existing relations in order to infer new associations between entities in KGs.", "labels": [], "entities": []}, {"text": "However, for relation extraction from a sentence, related pairs are not predefined and consequently all entity pairs need to be considered to extract relations.", "labels": [], "entities": [{"text": "relation extraction from a sentence", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.8391198635101318}]}, {"text": "In addition, state-of-the-art RE models sometimes depend on external syntactic tools to build the shortest dependency path (SDP) between two entities in a sentence (.", "labels": [], "entities": [{"text": "RE", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.970811128616333}]}, {"text": "This dependence on external tools leads to domain dependent models.", "labels": [], "entities": []}, {"text": "In this study, we propose a neural relation extraction model based on an entity graph, where entity mentions constitute the nodes and directed edges correspond to ordered pairs of entity mentions.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.717950701713562}]}, {"text": "The overview of the model is shown in.", "labels": [], "entities": []}, {"text": "We initialize the representation of an edge (an ordered pair of entity mentions) from the representations of the entity mentions and their context.", "labels": [], "entities": []}, {"text": "The context representation is achieved by employing an attention mechanism on context words.", "labels": [], "entities": []}, {"text": "We then use an iterative process to aggregate up-to l-length walk representations between two entities into a single representation, which corresponds to the final representation of the edge.", "labels": [], "entities": []}, {"text": "The contributions of our model can be summarized as follows: \u2022 We propose a graph walk based neural model that considers multiple entity pairs in relation extraction from a sentence.", "labels": [], "entities": [{"text": "relation extraction from a sentence", "start_pos": 146, "end_pos": 181, "type": "TASK", "confidence": 0.8398329019546509}]}, {"text": "\u2022 We propose an iterative algorithm to form a single representation for up-to l-length walks between the entities of a pair.", "labels": [], "entities": []}, {"text": "\u2022 We show that our model performs comparably to the state-of-the-art without the use of external syntactic tools.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of our model on ACE 2005 1 for the task of relation extraction.", "labels": [], "entities": [{"text": "ACE 2005 1", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9531779686609904}, {"text": "relation extraction", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.9018509387969971}]}, {"text": "ACE 2005 includes 7 entity types and 6 relation types between named entities.", "labels": [], "entities": [{"text": "ACE 2005", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9549751877784729}]}, {"text": "We follow the preprocessing described in Miwa and Bansal (2016).", "labels": [], "entities": []}, {"text": "We implemented our model using the Chainer library ().", "labels": [], "entities": []}, {"text": "The model was trained with Adam optimizer (.", "labels": [], "entities": []}, {"text": "We initialized the word representations with existing pre-trained embeddings with dimensionality of 200.", "labels": [], "entities": []}, {"text": "Our model did not use any external tools except these embeddings.", "labels": [], "entities": []}, {"text": "The forget bias of the LSTM layer was initialized with a value equal to one following the work of.", "labels": [], "entities": []}, {"text": "We use a batchsize of 10 sentences and fix the pair representation dimensionality to 100.", "labels": [], "entities": []}, {"text": "We use gradient clipping, dropout on the embedding and output layers and L2 regularization without regularizing the biases, to avoid overfitting.", "labels": [], "entities": []}, {"text": "We also incorporate early stopping with patience equal to five, to chose the number of training epochs and parameter averaging.", "labels": [], "entities": [{"text": "patience", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9715178608894348}]}, {"text": "We tune the model hyper-parameters on the respective development set using the RoBO Toolkit (.", "labels": [], "entities": [{"text": "RoBO Toolkit", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.7580401003360748}]}, {"text": "Please refer to the supplementary material for the values.", "labels": [], "entities": []}, {"text": "We extract all possible pairs in a sentence based on the number of entities it contains.", "labels": [], "entities": []}, {"text": "If a pair is not found in the corpus, it is assigned the \"no relation\" class.", "labels": [], "entities": []}, {"text": "We report the micro precision, recall and F1 score following and.", "labels": [], "entities": [{"text": "micro", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9824459552764893}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.7162737250328064}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9995737671852112}, {"text": "F1 score", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9846077859401703}]}, {"text": "illustrates the performance of our proposed model in comparison with SPTree system on ACE 2005.", "labels": [], "entities": [{"text": "SPTree system on ACE 2005", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.7438999295234681}]}, {"text": "We use the same data split with SPTree to compare with their model.", "labels": [], "entities": []}, {"text": "We retrained their model with gold entities in order to compare the performances on the relation extraction task.", "labels": [], "entities": [{"text": "relation extraction task", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.8749873240788778}]}, {"text": "The Baseline corresponds to a model that classifies relations by using only the representations of entities in a target pair.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relation extraction performance on ACE  2005 test dataset. * denotes significance at p <  0.05 compared to SPTree, denotes significance  at p < 0.05 compared to the Baseline.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.862312912940979}, {"text": "ACE  2005 test dataset", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.969726026058197}, {"text": "significance", "start_pos": 79, "end_pos": 91, "type": "METRIC", "confidence": 0.9719452857971191}]}, {"text": " Table 2: Relation extraction performance (F1 %)  on ACE 2005 development set for different num- ber of entities. * denotes significance at p < 0.05  compared to l = 1.", "labels": [], "entities": [{"text": "Relation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9947910308837891}, {"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9987898468971252}, {"text": "ACE 2005 development set", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.9666521847248077}]}, {"text": " Table 3: Hyper-parameters optimization options.", "labels": [], "entities": [{"text": "Hyper-parameters optimization", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7717449069023132}]}]}