{"title": [{"text": "Corpus specificity in LSA and word2vec: the role of out-of-domain documents", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite the popularity of word embed-dings, the precise way by which they acquire semantic relations between words remain unclear.", "labels": [], "entities": []}, {"text": "In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size.", "labels": [], "entities": []}, {"text": "One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases.", "labels": [], "entities": []}, {"text": "However, if corpus size grows in topics which are not specific to the domain of interest , signal to noise ratio may weaken.", "labels": [], "entities": []}, {"text": "Here we investigate the effect of corpus specificity and size in word-embeddings, and for this, we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task.", "labels": [], "entities": []}, {"text": "We show that word2vec can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus.", "labels": [], "entities": []}, {"text": "On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of dimensionality, can increase LSA word-representation quality while speeding up the processing time.", "labels": [], "entities": []}, {"text": "From a cognitive-modeling point of view, we point out that LSA's word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas word2vec does.", "labels": [], "entities": []}], "introductionContent": [{"text": "The main idea behind corpus-based semantic representation is that words with similar meanings tend to occur in similar contexts.", "labels": [], "entities": [{"text": "corpus-based semantic representation", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.6666723291079203}]}, {"text": "This proposition is called distributional hypothesis and provides a practical framework to understand and compute the semantic relationship between words.", "labels": [], "entities": []}, {"text": "Based in the distributional hypothesis, Latent Semantic Analysis (LSA)) and word2vec (, are one of the most important methods for word meaning representation, which describes each word in a vectorial space, where words with similar meanings are located close to each other.", "labels": [], "entities": [{"text": "word meaning representation", "start_pos": 130, "end_pos": 157, "type": "TASK", "confidence": 0.7468887766202291}]}, {"text": "Word embeddings have been applied in a wide variety of areas such as information retrieval), psychiatry, treatment optimization(), literature) and cognitive sciences.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.775897353887558}, {"text": "treatment optimization", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7557482123374939}]}, {"text": "LSA takes as input a training Corpus formed by a collection of documents.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6182440519332886}]}, {"text": "Then a word by document co-occurrence matrix is constructed, which contains the distribution of occurrence of the different words along the documents.", "labels": [], "entities": []}, {"text": "Then, usually, a mathematical transformation is applied to reduce the weight of uninformative high-frequency words in the words-documents matrix.", "labels": [], "entities": []}, {"text": "Finally, a linear dimensionality reduction is implemented by a truncated Singular Value Decomposition, SVD, which projects every word in a subspace of a predefined number of dimensions, k.", "labels": [], "entities": []}, {"text": "The success of LSA in capturing the latent meaning of words comes from this low-dimensional mapping.", "labels": [], "entities": [{"text": "capturing the latent meaning of words", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.8007038633028666}]}, {"text": "This representation improvement can be explained as a consequence of the elimination of the noisiest dimensions.", "labels": [], "entities": []}, {"text": "Word2vec consists of two neural network models, Continuous Bag of Words (CBOW) and Skipgram.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9016919732093811}]}, {"text": "To train the models, a sliding window is moved along the corpus.", "labels": [], "entities": []}, {"text": "In the CBOW scheme, in each step, the neural network is trained to predict the center word (the word in the center of the window based) given the context words (the other words in the window).", "labels": [], "entities": []}, {"text": "While in the skip-gram scheme, the model is trained to predict the context words based on the central word.", "labels": [], "entities": []}, {"text": "In the present paper, we use the skip-gram, which has produced better performance in.", "labels": [], "entities": [{"text": "skip-gram", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.8317397236824036}]}, {"text": "Despite the development of new word representation methods, LSA is still intensively used and has been shown that produce better performances than word2vec methods in small to medium size training corpus).", "labels": [], "entities": [{"text": "word representation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7934683859348297}]}], "datasetContent": [], "tableCaptions": []}