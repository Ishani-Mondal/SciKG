{"title": [{"text": "Automatic Estimation of Simultaneous Interpreter Performance", "labels": [], "entities": []}], "abstractContent": [{"text": "Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding.", "labels": [], "entities": [{"text": "Simultaneous interpretation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6838657259941101}, {"text": "translation of the spoken word", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8675086259841919}]}, {"text": "Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computer-assisted interpretation interfaces or ped-agogical tools.", "labels": [], "entities": []}, {"text": "We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output.", "labels": [], "entities": [{"text": "predicting simultaneous interpreter", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.824442982673645}, {"text": "quality estimation (QE) of machine translation output", "start_pos": 111, "end_pos": 164, "type": "TASK", "confidence": 0.629960685968399}]}, {"text": "In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.", "labels": [], "entities": [{"text": "METEOR evaluation metric", "start_pos": 143, "end_pos": 167, "type": "METRIC", "confidence": 0.8114552696545919}, {"text": "accuracy", "start_pos": 287, "end_pos": 295, "type": "METRIC", "confidence": 0.9486233592033386}]}], "introductionContent": [{"text": "Simultaneous Interpretation (SI) is an inherently difficult task that carries significant cognitive and attentional burdens.", "labels": [], "entities": [{"text": "Simultaneous Interpretation (SI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9143915891647338}]}, {"text": "The role of the simultaneous interpreter is to accurately render the source speech in a given target language in a timely and precise manner.", "labels": [], "entities": []}, {"text": "Interpreters employ a range of strategies, including generalization and summarization, to convey the source message as efficiently and reliably as possible (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.938734233379364}]}, {"text": "Unfortunately, the interpreter is pitched against the limits of human memory and stamina, and after only minutes of interpreting, the number of errors made by an interpreter begins to increase exponentially).", "labels": [], "entities": [{"text": "interpreting", "start_pos": 116, "end_pos": 128, "type": "TASK", "confidence": 0.9545739889144897}]}, {"text": "We examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling.", "labels": [], "entities": []}, {"text": "This has several immediate potential applications, one of which being in Computer-Assisted Interpretation (CAI).", "labels": [], "entities": [{"text": "Computer-Assisted Interpretation (CAI)", "start_pos": 73, "end_pos": 111, "type": "TASK", "confidence": 0.7808470845222473}]}, {"text": "CAI is quickly gaining traction in the interpreting community, with software products such as InterpretBank deployed in interpreting booths to provide live and interactive terminology support.(b) shows how this might work; both the interpreter and the CAI system receive the source message and the system displays assistive information in the form of terminology and informational support.", "labels": [], "entities": [{"text": "interpreting", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.9649232029914856}]}, {"text": "While this might improve the quality of interpreter output, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter.", "labels": [], "entities": []}, {"text": "Intuitively, the ideal level of support depends on current interpreter performance.", "labels": [], "entities": []}, {"text": "The system can minimize distraction by providing assistance only when an interpreter is struggling.", "labels": [], "entities": []}, {"text": "This level of support could be moderated appropriately if interpreter performance can be accurately predicted.(c) demonstrates how our proposed quality estimation (QE) system receives and evaluates interpreter output, allowing the CAI system to appropriately lower the amount of information passed to the interpreter, maximizing the quality of interpreter output.", "labels": [], "entities": []}, {"text": "As a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems, which takes in the source sentence and MT-generated outputs and estimates a measure of quality.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.8482838869094849}]}, {"text": "In doing so, we arrive at two natural research questions: 1.", "labels": [], "entities": []}, {"text": "Do existing methods for performing QE on MT output also allow for accurate estimation of interpreter performance, despite the inherent differences between MT and SI?", "labels": [], "entities": [{"text": "MT output", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.8577550053596497}]}, {"text": "2. What unique aspects of the problem of interpreter performance estimation, such as the availability of prosody and other linguistic cues, can be exploited to further improve the accuracy of our predictions?", "labels": [], "entities": [{"text": "interpreter performance estimation", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6580344041188558}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9949925541877747}]}, {"text": "The remainder of the paper describes methods and experiments on English-Japanese (EN-JA), English-French (EN-FR), and English-Italian (EN-IT) interpretation data attempting to answer these questions.", "labels": [], "entities": [{"text": "English-Italian (EN-IT) interpretation", "start_pos": 118, "end_pos": 156, "type": "TASK", "confidence": 0.6436193645000458}]}, {"text": "first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU () rely on the availability of reference translations to evaluate MT output quality, which aren't always available.", "labels": [], "entities": [{"text": "MT output", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.9104854166507721}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.997986912727356}, {"text": "MT output", "start_pos": 194, "end_pos": 203, "type": "TASK", "confidence": 0.8984411954879761}]}, {"text": "As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers) to neural models ().", "labels": [], "entities": [{"text": "QE", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.8811019062995911}, {"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9744409918785095}, {"text": "Statistical Machine Translation", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.747848649819692}]}, {"text": "QuEst++ () is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE.", "labels": [], "entities": []}, {"text": "Its effectiveness and flexibility make it an attractive candidate for our proposed task.", "labels": [], "entities": []}, {"text": "There are two main modules to QuEst++: a feature extractor and a learning module.", "labels": [], "entities": []}, {"text": "The feature extractor produces an intermediate representation of the source and translation in a continuous feature vector.", "labels": [], "entities": []}, {"text": "The goal of the learning module, given a source and translation pair, is to predict the quality of the translation, either as a label or as a continuous value.", "labels": [], "entities": []}, {"text": "This module is trained on example translations that have an assigned score (such as BLEU) and then predicts the score of anew example.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9981253743171692}]}, {"text": "QuEst++ offers a range of learning algorithms but defaults to Support Vector Regression for sentence-level QE.", "labels": [], "entities": []}], "datasetContent": [{"text": "Novice interpreters are assessed for accuracy on the number of omissions, additions and the inaccurate renditions of lexical items and longer phrases, but recovery of content and correct terminology are highly valued.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991562366485596}]}, {"text": "While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.9445540010929108}]}, {"text": "One important design decision is which evaluation metric to target in our QE system.", "labels": [], "entities": []}, {"text": "There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (), NIST) and ME-TEOR (), all of which compare the similarity between reference translations and translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9937686920166016}, {"text": "WER", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9877247214317322}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9988766312599182}, {"text": "NIST", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.5769850015640259}, {"text": "ME-TEOR", "start_pos": 107, "end_pos": 114, "type": "METRIC", "confidence": 0.9152438640594482}]}, {"text": "Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation ().", "labels": [], "entities": []}, {"text": "As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9824641346931458}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9904367327690125}]}, {"text": "To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9524226188659668}, {"text": "MT evaluation", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.8943249583244324}]}, {"text": "Better handling of these divergences for evaluation of interpreter output, or fine-grained evaluation based on measures from interpretation studies is an interesting direction for future work.", "labels": [], "entities": []}, {"text": "To evaluate the quality of our QE system, we use the Pearson's r correlation between the predicted and true METEOR for each language pair.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 53, "end_pos": 76, "type": "METRIC", "confidence": 0.9575759917497635}, {"text": "METEOR", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9750725030899048}]}, {"text": "As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2).", "labels": [], "entities": []}, {"text": "We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT source-interpreter language pairs with a held-out development set and test set for each fold.", "labels": [], "entities": []}, {"text": "For each experiment setting, we run the experiment for each fold (ten iterations for each set) and evaluate average Pearson's r correlation on the development set.", "labels": [], "entities": [{"text": "Pearson's r correlation", "start_pos": 116, "end_pos": 139, "type": "METRIC", "confidence": 0.9495522230863571}]}, {"text": "In our baseline setting, we extract features based on the default QuEst++ sentence-level feature set (baseline).", "labels": [], "entities": []}, {"text": "We ablate baseline features through cross-validation and remove features relating to bigram and trigram frequency and punctuation frequency in the source utterance, creating: Pearson's r scores for predicted ME-TEOR for baseline, trimmed and proposed feature sets on the test set (highest accuracy for each dataset indicated in bold).", "labels": [], "entities": [{"text": "Pearson's r scores", "start_pos": 175, "end_pos": 193, "type": "METRIC", "confidence": 0.7947277873754501}, {"text": "ME-TEOR", "start_pos": 208, "end_pos": 215, "type": "METRIC", "confidence": 0.8990823030471802}, {"text": "accuracy", "start_pos": 289, "end_pos": 297, "type": "METRIC", "confidence": 0.9894715547561646}]}, {"text": "a more effective trimmed model (trimmed).", "labels": [], "entities": []}, {"text": "Subsequently, we add our interpreter features (Section 3.1) and arrive at our proposed model.", "labels": [], "entities": []}, {"text": "We then repeat each experiment using the test set data from each fold and compare the resulting average Pearson's r scores.", "labels": [], "entities": [{"text": "Pearson's r scores", "start_pos": 104, "end_pos": 122, "type": "METRIC", "confidence": 0.8941903412342072}]}, {"text": "shows our primary results comparing the baseline, trimmed, and proposed feature sets.", "labels": [], "entities": []}, {"text": "Our first observation is that, even with the baseline feature set, QE obtains respectable correlation scores, proving feasible as a method to predict interpreter performance.", "labels": [], "entities": [{"text": "correlation", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9745278358459473}]}, {"text": "Our trimmed feature set performs moderately better than the baseline for Japanese, and slightly under-performs for French and Italian.", "labels": [], "entities": []}, {"text": "However, our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank) (+0.104), achieving its highest accuracy on the EN-FR dataset.", "labels": [], "entities": [{"text": "A-Rank)", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.913837343454361}, {"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9987519979476929}, {"text": "EN-FR dataset", "start_pos": 186, "end_pos": 199, "type": "DATASET", "confidence": 0.9574642181396484}]}, {"text": "Over all datasets, the gain of the proposed model is statistically significant at p < 0.05 by the pairwise bootstrap).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's r scores for predicted ME- TEOR for baseline, trimmed and proposed fea- ture sets on the test set (highest accuracy for each  dataset indicated in bold).", "labels": [], "entities": [{"text": "ME- TEOR", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8772466778755188}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9928358197212219}]}]}