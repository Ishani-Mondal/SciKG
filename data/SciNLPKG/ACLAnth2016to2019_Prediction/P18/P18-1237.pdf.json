{"title": [{"text": "Modeling Deliberative Argumentation Strategies on Wikipedia", "labels": [], "entities": [{"text": "Modeling Deliberative Argumentation Strategies", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8475106656551361}, {"text": "Wikipedia", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.8696584701538086}]}], "abstractContent": [{"text": "This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to predict the best next deliberative move of each participant.", "labels": [], "entities": []}, {"text": "In this paper, we present a model for delibera-tive discussions and we illustrate its oper-ationalization.", "labels": [], "entities": []}, {"text": "Previous models have been built manually based on a small set of discussions , resulting in a level of abstraction that is not suitable for move recommendation.", "labels": [], "entities": [{"text": "move recommendation", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.8531137704849243}]}, {"text": "In contrast, we derive our model statistically from several types of metadata that can be used for move description.", "labels": [], "entities": [{"text": "move description", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.8038447201251984}]}, {"text": "Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames.", "labels": [], "entities": []}, {"text": "On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories.", "labels": [], "entities": []}, {"text": "We then oper-ationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deliberation is the type of discussions where the aim is to find the best choice from a set of possible actions.", "labels": [], "entities": []}, {"text": "This type is influential for making decisions in different processes including collaborative writing.", "labels": [], "entities": []}, {"text": "Studies have shown the positive impact of deliberation on the quality of several document types, such as scientific papers, research proposals, political reports, and Wikipedia articles, among others (.", "labels": [], "entities": []}, {"text": "However, deliberative discussions may fail, either by agreeing on the wrong action, or by reaching no agreement.", "labels": [], "entities": []}, {"text": "While the former is hard to measure, the latter is, for example, clearly reflected in the number of disputed discussions on Wikipedia (.", "labels": [], "entities": []}, {"text": "Although agreement can never be guaranteed, a deliberative argumentation strategy of a discussion's participants makes it more likely).", "labels": [], "entities": []}, {"text": "With strategy, we here mean the sequence of moves that participants take during the discussion.", "labels": [], "entities": []}, {"text": "Such a sequence is effective if it leads to a successful discussion.", "labels": [], "entities": []}, {"text": "To achieve effectiveness, every participant has to understand the current state of a discussion and to come up with a next deliberative move that best serves the discussion.", "labels": [], "entities": []}, {"text": "For newcomers, this requires substantial effort and time, especially when a discussion grows due to conflicts and back-and-forth arguments.", "labels": [], "entities": []}, {"text": "Here, automated tools can help by annotating ongoing discussions with a label for each move or by providing a textual summary of past moves (.", "labels": [], "entities": []}, {"text": "A way to go beyond that is to let the tool recommend the best possible moves according to an effective strategy.", "labels": [], "entities": []}, {"text": "This is the ultimate goal of our research.", "labels": [], "entities": []}, {"text": "As a substantial step towards this goal, two fundamental research questions are addressed in the paper at hand: (1) How to model deliberative discussions in light of the aim of agreement, and (2) how to operationalize the model in order to identify different argumentation strategies and to learn about their effectiveness.", "labels": [], "entities": []}, {"text": "Different models of deliberative discussions have been proposed in previous studies.", "labels": [], "entities": []}, {"text": "These models were developed based on expert analyses of a small set of sampled discussions (see Section 2).", "labels": [], "entities": []}, {"text": "However, the small size, in fact, confines the ability to develop a representative model, which should ideally cover a wide range of moves while being abstract to fit the majority of discussions.", "labels": [], "entities": []}, {"text": "To overcome this limitation, we propose to derive a model statistically from a large set of discussions.", "labels": [], "entities": []}, {"text": "We approach this based on different types of metadata that people use to describe their moves on Wikipedia talk pages, the richest source of deliberative discussions on the web.", "labels": [], "entities": []}, {"text": "Particularly, we extract the entire set of about six million discussions from all English Wikipedia talk pages.", "labels": [], "entities": [{"text": "English Wikipedia talk pages", "start_pos": 82, "end_pos": 110, "type": "DATASET", "confidence": 0.8287064582109451}]}, {"text": "We parse each discussion to identify its structural components, such as turns, users, and time stamps.", "labels": [], "entities": []}, {"text": "Besides, we store four types of metadata from the turns: the user tag, a shortcut, an in-line template, and links.", "labels": [], "entities": []}, {"text": "To learn from the metadata, we cluster the types' instances based on their semantic similarity.", "labels": [], "entities": []}, {"text": "Then, we map each cluster to a specific concept (e.g., 'providing a source'), and the related concepts into a set of categories (e.g., 'providing evidence').", "labels": [], "entities": []}, {"text": "shows the categories of our model.", "labels": [], "entities": []}, {"text": "Analyzing the distribution of these categories, we find that each turn ideally should have (1) one of six categories that we call discourse acts, (2) one of three categories that we call argumentative relations, and (3) one of four categories that we call frames.", "labels": [], "entities": []}, {"text": "As such, our model is inline with three well-established theories: speech act theory, argumentation theory (, and framing theory (P..", "labels": [], "entities": [{"text": "speech act theory", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7147546609242758}, {"text": "argumentation theory", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.90091672539711}, {"text": "framing theory", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.9244825541973114}]}, {"text": "A model instance is sketched in.", "labels": [], "entities": []}, {"text": "Based on the model, we generate anew largescale corpus using the metadata automatically: Webis-WikiDebate-18 corpus.", "labels": [], "entities": [{"text": "Webis-WikiDebate-18 corpus", "start_pos": 89, "end_pos": 115, "type": "DATASET", "confidence": 0.9202422797679901}]}, {"text": "Basically, if a turn in a discussion has metadata that belongs to a specific category according to the above-mentioned analysis, it is labeled with that category.", "labels": [], "entities": []}, {"text": "The corpus includes 2400 turns labeled with a discourse act, 7437 turns labeled with a relation, and 182,321 turns labeled with a frame.", "labels": [], "entities": []}, {"text": "To operationalize our model, we train three supervised classifiers for acts, relations, and frames on the corpus.", "labels": [], "entities": []}, {"text": "The classifiers employ a rich set of linguistic features that has been shown to be effective in similar tasks.", "labels": [], "entities": []}, {"text": "The results of our experiments suggest that we are able to predict the labels with a comparable performance to the one achieved in similar tasks.", "labels": [], "entities": []}, {"text": "Overall, the contribution of this paper is threefold: (1) A data-driven approach for creating anew model of deliberative discussions that is aligned with well-established theories, (2) a corpus with about 200,000 turns labeled for 13 different categories, and (3) a classification approach that predicts the labels of turns.", "labels": [], "entities": []}, {"text": "All developed resources are freely available at https://www.webis.", "labels": [], "entities": []}, {"text": "de/data/data.html.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a preprocessing step, we cleaned the turns in the Webis-WikiDebate-18 Corpus by removing all the metadata: user tags, shortcuts, user and time stamps, etc.", "labels": [], "entities": [{"text": "Webis-WikiDebate-18 Corpus", "start_pos": 53, "end_pos": 79, "type": "DATASET", "confidence": 0.973274439573288}]}, {"text": "Then, we grouped the turns that belong to the discourse act categories in a single dataset (say, the 'discourse act dataset').", "labels": [], "entities": []}, {"text": "The same was performed for the turns belonging to relations and frames.", "labels": [], "entities": []}, {"text": "We then split each of the three datasets randomly into training (60%), development (20%), and test (20%) sets.", "labels": [], "entities": []}, {"text": "We ensured that turns from the same discussion should appear only in either of the split sets, in order to avoid biasing the classifiers by topical information.", "labels": [], "entities": []}, {"text": "We trained different machine learning models on the training sets and evaluated them on the development sets.", "labels": [], "entities": []}, {"text": "The models included those which had been used before in similar tasks, such as naive bayes, logistic regression, support vector machine, and random forest.", "labels": [], "entities": []}, {"text": "We tried both under and oversampling on the training sets.", "labels": [], "entities": []}, {"text": "The best results in the three tasks were achieved by using support vector: The precision, recall, and F 1 -score of our classifiers for all categories of the three dimensions.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9996945858001709}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9990276098251343}, {"text": "F 1 -score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.98850516974926}]}, {"text": "machine without sampling the training sets.", "labels": [], "entities": []}, {"text": "We used the support vector machine implementation from the LibLinear library on the test sets and report the results in.", "labels": [], "entities": []}, {"text": "Overall, the three classifiers achieved results that are comparable to the results of previous methods on the corresponding tasks).", "labels": [], "entities": []}, {"text": "We obtained the best results in the frame task, followed by relations and then discourse acts.", "labels": [], "entities": []}, {"text": "Apparently, the results correlate with the size of the datasets.", "labels": [], "entities": []}, {"text": "In case of discourse acts, the classifier achieves low F 1 -scores for 'socializing', 'recommending an act', and 'asking a question'.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.990856260061264}]}, {"text": "These categories have a significantly smaller number of turns compared to other categories, which makes identifying them harder.", "labels": [], "entities": []}, {"text": "The effectiveness of classifying the relation and frame categories, on the other hand, appears promising given the difficulty of these tasks.", "labels": [], "entities": []}, {"text": "We point that we considered mainly the turns' texts in our experiments.", "labels": [], "entities": []}, {"text": "In principle, this helps to get an idea about the effectiveness of our approach in Wikipedia as well as other registers for discussions.", "labels": [], "entities": []}, {"text": "Nevertheless, including the metadata and structural information of the analyzed discussions is definitely worthwhile in general, and will naturally tend to lead to notably higher effectiveness.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Instance counts of the different compo- nents of the Webis-WikiDiscussions-18 corpus.", "labels": [], "entities": [{"text": "Instance", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9777299165725708}, {"text": "Webis-WikiDiscussions-18 corpus", "start_pos": 63, "end_pos": 94, "type": "DATASET", "confidence": 0.9632837474346161}]}, {"text": " Table 2: The concepts covered by each category of each of the three principle dimensions of our model.", "labels": [], "entities": []}, {"text": " Table 3: Number of turns in each category of Webis- WikiDebate-18 corpus and the precision of sampled  turns for each category according to an expert.", "labels": [], "entities": [{"text": "Webis- WikiDebate-18 corpus", "start_pos": 46, "end_pos": 73, "type": "DATASET", "confidence": 0.8868862986564636}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9990735054016113}]}, {"text": " Table 4: The precision, recall, and F 1 -score of our  classifiers for all categories of the three dimensions.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9997052550315857}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9989194869995117}, {"text": "F 1 -score", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.989064633846283}]}]}