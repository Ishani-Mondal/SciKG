{"title": [], "abstractContent": [{"text": "Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications.", "labels": [], "entities": []}, {"text": "However, the extensions of these models require carefully tailored graphi-cal models and re-deduced inference algorithms , limiting their variations and applications.", "labels": [], "entities": []}, {"text": "We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC).", "labels": [], "entities": []}, {"text": "It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions.", "labels": [], "entities": []}, {"text": "Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts.", "labels": [], "entities": []}, {"text": "To illustrate the flexibility offered by the neu-ral network based framework, we present three extensions base on NSTC without re-deduced inference algorithms.", "labels": [], "entities": []}, {"text": "Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models out-perform existing methods.", "labels": [], "entities": [{"text": "20Newsgroups datasets", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.9103606343269348}]}], "introductionContent": [{"text": "Topic models with sparsity enhancement have proven to be effective tools for exploratory analysis of the overload of short text content.", "labels": [], "entities": []}, {"text": "The latent representations learned by these models are central to many applications.", "labels": [], "entities": []}, {"text": "However, these models have trouble to rapidly explore variations for the approximate inference methods of them.", "labels": [], "entities": []}, {"text": "Even subtle variations on models can increase the complexity of the inference methods, which is especially apparent for non-conjugate models.", "labels": [], "entities": []}, {"text": "With the development of deep learning, many works combine topic models with neural language model to overcome the computation complexity of topic models).", "labels": [], "entities": []}, {"text": "Most of these methods adopt multiple neural network layers to model the generation process of each document.", "labels": [], "entities": []}, {"text": "Nevertheless, these methods yield the same poor performance in short texts as traditional topic models.", "labels": [], "entities": []}, {"text": "There are also many works introducing new techniques such as word embeddings to traditional topic models.", "labels": [], "entities": []}, {"text": "Word embeddings has proven to be effective at capturing syntactic and semantic information of words.", "labels": [], "entities": []}, {"text": "Many works ( have shown that the additional semantics in word embeddings can enhance the performance of traditional topic models.", "labels": [], "entities": []}, {"text": "Yet these models have the same trouble in making extensions as traditional topic models.", "labels": [], "entities": []}, {"text": "Base on the above observations, we propose Neural Sparse Topical Coding (NSTC) by jointly utilizing word embeddings and neural network with a sparsity-enhanced topic model, Sparse Topical Coding (STC).", "labels": [], "entities": [{"text": "Neural Sparse Topical Coding (NSTC)", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.7299813500472477}, {"text": "Sparse Topical Coding (STC)", "start_pos": 173, "end_pos": 200, "type": "TASK", "confidence": 0.7625638842582703}]}, {"text": "We adopt neural network to model the generation process of STC to simplify the complex inference and improve flexibility, and incorporate external semantics provided byword embeddings to improve the overall accuracy.", "labels": [], "entities": [{"text": "flexibility", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9654042720794678}, {"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9961942434310913}]}, {"text": "To illustrate the dramatic flexibility offered by the end-to-end neural network, we present three extensions base on NSTC.", "labels": [], "entities": []}, {"text": "Our proposed models all benefit from both sides: 1) when compared with the neural based topic models, which stuck in the sparseness of word co-occurrence information, they show how the sparsity mechanism and word embeddings enrich the features and improve the performance; 2) while with topic models with sparsity enhancement, our models present how the black-box inference method of neural network ac-celerates the training process and increases the flexibility.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our models by conducting experiments on 20 Newsgroups and Web Snippet datasets.", "labels": [], "entities": [{"text": "Newsgroups and Web Snippet datasets", "start_pos": 76, "end_pos": 111, "type": "DATASET", "confidence": 0.6909650564193726}]}], "datasetContent": [{"text": "To further evaluate our models as a generative model of documents, we show the test document perplexity achieved by each topic model on the 20NewsGroups with 50 topic numbers in table 2.", "labels": [], "entities": []}, {"text": "Notice that the topic number in TopicVec cannot be specified to a fixed value, thus we follow the set in () with 281 topics.", "labels": [], "entities": []}, {"text": "In table 3, we show the top-9 words of learned focused topics in 20 Newsgroups datasets.", "labels": [], "entities": [{"text": "Newsgroups datasets", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.9430778324604034}]}, {"text": "For each topic, we list top-9 words according to their probabili-, we also use the 2-dimensional t-SNE method to get the visualization of the learned latent representations for Web Snippet and 20Newsgroups Dataset with 200 topics.", "labels": [], "entities": [{"text": "20Newsgroups Dataset", "start_pos": 193, "end_pos": 213, "type": "DATASET", "confidence": 0.87913379073143}]}, {"text": "For Web Snippet, we sample 10% of the whole dataset.", "labels": [], "entities": []}, {"text": "For 20newsgroups, we sample 30% of the dataset.", "labels": [], "entities": []}, {"text": "It is obvious to see that all documents are clustered into 8 and 20 distinct categories.", "labels": [], "entities": []}, {"text": "It proves the semantic effectiveness of the documents codes learned by our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Classification accuracy of different models on Web snippet and 20NG, with different number of  topic K settings.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.931849479675293}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9593245387077332}]}, {"text": " Table 2: Perplexity on test  dataset.", "labels": [], "entities": []}, {"text": " Table 3: Top Words of Learned Topics for 20Newsgroups.", "labels": [], "entities": []}]}