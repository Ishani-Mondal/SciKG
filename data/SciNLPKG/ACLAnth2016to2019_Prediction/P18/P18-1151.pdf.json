{"title": [{"text": "GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data", "labels": [], "entities": [{"text": "Sentence Generation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9410021007061005}]}], "abstractContent": [{"text": "A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject , a predicate (relationship), and an object.", "labels": [], "entities": []}, {"text": "The RDF triple representation offers a simple interface for applications to access the facts.", "labels": [], "entities": []}, {"text": "However, this representation is not in a natural language form, which is difficult for humans to understand.", "labels": [], "entities": []}, {"text": "We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework.", "labels": [], "entities": []}, {"text": "To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder.", "labels": [], "entities": []}, {"text": "The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed en-coder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9979932308197021}, {"text": "METEOR", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9791561365127563}, {"text": "TER", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.9984903335571289}]}], "introductionContent": [{"text": "Knowledge bases (KBs) are becoming an enabling resource for many applications including Q&A systems, recommender systems, and summarization tools.", "labels": [], "entities": [{"text": "summarization", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.98023521900177}]}, {"text": "KBs are designed based on a W3C standard called the Resource Description Framework (RDF) . An RDF triple consists of three elements in the form of subject, predicate (relationship), object.", "labels": [], "entities": []}, {"text": "It describes a relationship between an entity (the subject) and another entity or literal (the object)", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our framework on two datasets.", "labels": [], "entities": []}, {"text": "The first is the dataset from.", "labels": [], "entities": []}, {"text": "We call it the WebNLG dataset.", "labels": [], "entities": [{"text": "WebNLG dataset", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9872851371765137}]}, {"text": "This dataset contains 25,298 RDF triple set-text pairs, with 9,674 unique sets of RDF triples.", "labels": [], "entities": []}, {"text": "The dataset consists of a Train+Dev dataset and a Test Unseen dataset.", "labels": [], "entities": [{"text": "Train+Dev dataset", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.6605841219425201}, {"text": "Test Unseen dataset", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.6156402230262756}]}, {"text": "We split Train+Dev into a training set (80%), a development set (10%), and a Seen testing set (10%).", "labels": [], "entities": []}, {"text": "The Train+Dev dataset contains RDF triples in ten categories (topics, e.g., astronaut, monument, food, etc.), while the Test Unseen dataset has five other unseen categories.", "labels": [], "entities": [{"text": "Train+Dev dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.8294612467288971}, {"text": "Test Unseen dataset", "start_pos": 120, "end_pos": 139, "type": "DATASET", "confidence": 0.7147309978802999}]}, {"text": "The maximum number of triples in each RDF triple set is seven.", "labels": [], "entities": [{"text": "RDF triple set", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.6199896931648254}]}, {"text": "For the second dataset, we collected data from Wikipedia pages regarding landmarks.", "labels": [], "entities": []}, {"text": "We call it the GKB dataset.", "labels": [], "entities": [{"text": "GKB dataset", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.9824960827827454}]}, {"text": "We first extract RDF triples from Wikipedia infoboxes and sentences from the Wikipedia text that contain entities mentioned in the RDF triples.", "labels": [], "entities": []}, {"text": "Human annotators then filter out false matches to obtain 1,000 RDF triple set-text pairs.", "labels": [], "entities": []}, {"text": "This dataset is split into the training and development set (80%) and the testing set (20%).", "labels": [], "entities": []}, {"text": "illustrates an example of the data pairs of WebNLG and GKB dataset.", "labels": [], "entities": [{"text": "WebNLG", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9701027870178223}, {"text": "GKB dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9372715055942535}]}, {"text": "We implement the existing models, the adapted model, and the proposed model using Keras . We use three common evaluation metrics including BLEU (), ME-TEOR (, and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9989927411079407}, {"text": "ME-TEOR", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.970811665058136}, {"text": "TER", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9978840947151184}]}, {"text": "For the metric computation and significance testing, we use MultEval).", "labels": [], "entities": [{"text": "significance testing", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.722971647977829}, {"text": "MultEval", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9028341770172119}]}, {"text": "To complement the automatic evaluation, we conduct human evaluations for all of the masked models.", "labels": [], "entities": []}, {"text": "We ask five human annotators.", "labels": [], "entities": []}, {"text": "Each of them  has studied English for at least ten years and completed education in a full English environment for at least two years.", "labels": [], "entities": []}, {"text": "We provide a website 4 that shows them the RDF triples and the generated text.", "labels": [], "entities": []}, {"text": "The annotators are given training on the scoring criteria.", "labels": [], "entities": []}, {"text": "We also provide scoring examples.", "labels": [], "entities": []}, {"text": "We randomly selected 100 sets of triples along with the output of each model.", "labels": [], "entities": []}, {"text": "We only select sets of triples that contain more than two triples.", "labels": [], "entities": []}, {"text": "Following (), we use three evaluation metrics including correctness, grammaticality, and fluency.", "labels": [], "entities": [{"text": "correctness", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.9674370288848877}]}, {"text": "For each pair of triple set and generated sentences, the annotators are asked to give a score between one to three for each metric.", "labels": [], "entities": []}, {"text": "Correctness is used to measure the semantics of the output sentence.", "labels": [], "entities": []}, {"text": "A score of 3 is given to generated sentences that contain no errors in the relationships between entities; a score of 2 is given to generated sentences that contain one error in the relationship; and a score of 1 is given to generated sentences that contain more than one errors in the relationships.", "labels": [], "entities": []}, {"text": "Grammaticality is used to rate the grammatical and spelling errors of the generated sentences.", "labels": [], "entities": [{"text": "Grammaticality", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8846747279167175}]}, {"text": "Similar to the correctness metric, a score of 3 is given to generated sentences with no grammatical and spelling errors; a score of 2 is given to generated sentences with one error; and a score of 1 for the others.", "labels": [], "entities": []}, {"text": "The last metric, fluency, is used to measure the fluency of the sentence output.", "labels": [], "entities": []}, {"text": "We ask the annotators to give a score based on the aggregation of the sentences and the existence of sentence repetition.", "labels": [], "entities": []}, {"text": "shows the results of the human evaluations.", "labels": [], "entities": []}, {"text": "The results confirm the automatic evaluation in which our proposed model achieves the best scores.", "labels": [], "entities": []}, {"text": "We further perform a manual inspection of 100 randomly selected output sentences of GTR-LSTM and BLSTM on the Seen and Unseen test data.", "labels": [], "entities": [{"text": "GTR-LSTM", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.8953699469566345}, {"text": "BLSTM", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9205122590065002}, {"text": "Seen and Unseen test data", "start_pos": 110, "end_pos": 135, "type": "DATASET", "confidence": 0.6324461579322815}]}, {"text": "We find that 32% of BLSTM output contains wrong relationships between entities.", "labels": [], "entities": []}, {"text": "In comparison, only 8% of GTR-LSTM output contains such errors.", "labels": [], "entities": []}, {"text": "Besides, we find duplicate sub-sentences in http://bit.ly/gkb-mappings the output of GTR-LSTM (15%).", "labels": [], "entities": []}, {"text": "The following output is an example: \"beef kway teow is a dish from singapore, where english language is spoken and the leader is tony tan.", "labels": [], "entities": []}, {"text": "the leader of singapore is tony tan.\"", "labels": [], "entities": []}, {"text": "While the duplicate sentence is not wrong, it affects the reading experience.", "labels": [], "entities": []}, {"text": "We conjecture that the LSTM in the decoder caused such an issue.", "labels": [], "entities": []}, {"text": "We aim to solve this problem in future work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of model performance.", "labels": [], "entities": []}, {"text": " Table 4: Human evaluation results.", "labels": [], "entities": []}]}