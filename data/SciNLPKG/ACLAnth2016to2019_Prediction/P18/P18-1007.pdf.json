{"title": [{"text": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates", "labels": [], "entities": [{"text": "Subword Regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9038881063461304}, {"text": "Improving Neural Network Translation", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.8216457515954971}]}], "abstractContent": [{"text": "Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 80, "end_pos": 112, "type": "TASK", "confidence": 0.8499981562296549}]}, {"text": "While sentences are usually converted into unique subword sequences, subword seg-mentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary.", "labels": [], "entities": []}, {"text": "The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robust-ness of NMT.", "labels": [], "entities": []}, {"text": "We present a simple regu-larization method, subword regularization, which trains the model with multiple sub-word segmentations probabilistically sampled during training.", "labels": [], "entities": [{"text": "subword regularization", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7936583757400513}]}, {"text": "In addition, for better subword sampling, we propose anew sub-word segmentation algorithm based on a unigram language model.", "labels": [], "entities": [{"text": "subword sampling", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.8122519850730896}, {"text": "sub-word segmentation", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7670215666294098}]}, {"text": "We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) models () often operate with fixed word vocabularies, as their training and inference depend heavily on the vocabulary size.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7872184713681539}]}, {"text": "However, limiting vocabulary size increases the amount of unknown words, which makes the translation inaccurate especially in an open vocabulary setting.", "labels": [], "entities": []}, {"text": "A common approach for dealing with the open vocabulary issue is to breakup rare words into subword units (BPE) () is a de facto standard subword segmentation algorithm applied to many NMT systems and achieving top translation quality in several shared tasks.", "labels": [], "entities": [{"text": "breakup rare words into subword units (BPE)", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.6772832870483398}, {"text": "subword segmentation", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7888852953910828}]}, {"text": "BPE segmentation gives a good balance between the vocabulary size and the decoding efficiency, and also sidesteps the need fora special treatment of unknown words.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7089086323976517}]}, {"text": "BPE encodes a sentence into a unique subword sequence.", "labels": [], "entities": []}, {"text": "However, a sentence can be represented in multiple subword sequences even with the same vocabulary.", "labels": [], "entities": []}, {"text": "While these sequences encode the same input \"Hello World\", NMT handles them as completely different inputs.", "labels": [], "entities": []}, {"text": "This observation becomes more apparent when converting subword sequences into id sequences (right column in).", "labels": [], "entities": []}, {"text": "These variants can be viewed as a spurious ambiguity, which might not always be resolved in decoding process.", "labels": [], "entities": []}, {"text": "At training time of NMT, multiple segmentation candidates will make the model robust to noise and segmentation errors, as they can indirectly help the model to learn the compositionality of words, e.g., \"books\" can be decomposed into \"book\" + \"s\".", "labels": [], "entities": []}, {"text": "In this study, we propose anew regularization method for open-vocabulary NMT, called subword regularization, which employs multiple subword segmentations to make the NMT model accurate and robust.", "labels": [], "entities": [{"text": "subword regularization", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7695175111293793}]}, {"text": "Subword regularization consists of the following two sub-contributions: \u2022 We propose a simple NMT training algorithm to integrate multiple segmentation candidates.", "labels": [], "entities": [{"text": "Subword regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9153942167758942}]}, {"text": "Our approach is implemented as an on-the-fly data sampling, which is not specific to NMT architecture.", "labels": [], "entities": []}, {"text": "Subword regularization can be applied to any NMT system without changing the model structure.", "labels": [], "entities": [{"text": "Subword regularization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8115439713001251}]}, {"text": "\u2022 We also propose anew subword segmentation algorithm based on a language model, which provides multiple segmentations with probabilities.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7374459505081177}]}, {"text": "The language model allows to emulate the noise generated during the segmentation of actual data.", "labels": [], "entities": []}, {"text": "Empirical experiments using multiple corpora with different sizes and languages show that subword regularization achieves significant improvements over the method using a single subword sequence.", "labels": [], "entities": [{"text": "subword regularization", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8320426344871521}]}, {"text": "In addition, through experiments with out-of-domain corpora, we show that subword regularization improves the robustness of the NMT model.", "labels": [], "entities": [{"text": "subword regularization", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.8294331729412079}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Multiple subword sequences encoding  the same sentence \"Hello World\"", "labels": [], "entities": [{"text": "Hello World\"", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.6685413718223572}]}, {"text": " Table 2: Details of evaluation data set", "labels": [], "entities": []}, {"text": " Table 3: Main Results (BLEU(%)) (l: sampling size in SR, \u03b1: smoothing parameter). * indicates statistically significant", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9948358535766602}, {"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9990942478179932}]}, {"text": " Table 4: Results with out-of-domain corpus", "labels": [], "entities": []}, {"text": " Table 6: Comparison on different regularization  strategies (IWSLT15/17, l = 64, \u03b1 = 0.1)", "labels": [], "entities": [{"text": "IWSLT15/17", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.8035954236984253}]}]}