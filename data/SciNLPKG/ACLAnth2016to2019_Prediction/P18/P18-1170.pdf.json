{"title": [{"text": "AMR Dependency Parsing with a Typed Semantic Algebra", "labels": [], "entities": [{"text": "AMR Dependency Parsing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7268064121405283}]}], "abstractContent": [{"text": "We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representations", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.7257238825162252}]}, {"text": "This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system.", "labels": [], "entities": [{"text": "dependency tree parsing", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6997878750165304}]}, {"text": "We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and out-perform strong baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.998814582824707}]}], "introductionContent": [{"text": "Over the past few years, Abstract Meaning Representations (AMRs,) have become a popular target representation for semantic parsing.", "labels": [], "entities": [{"text": "Abstract Meaning Representations (AMRs", "start_pos": 25, "end_pos": 63, "type": "TASK", "confidence": 0.7510643482208252}, {"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.8378018438816071}]}, {"text": "AMRs are graphs which describe the predicate-argument structure of a sentence.", "labels": [], "entities": []}, {"text": "Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination.", "labels": [], "entities": []}, {"text": "However, it is technically much more challenging to parse a string into a graph than into a tree.", "labels": [], "entities": []}, {"text": "For instance, grammar-based approaches ( require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees.", "labels": [], "entities": []}, {"text": "Neural sequence-to-sequence models, which do very well on string-to-tree parsing (, can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences.", "labels": [], "entities": [{"text": "string-to-tree parsing", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7390241920948029}]}, {"text": "In this paper, we tackle this challenge by making the compositional structure of the AMR explicit.", "labels": [], "entities": []}, {"text": "As in our previous work,, we view an AMR as consisting of atomic graphs representing the meanings of the individual words, which were combined compositionally using linguistically motivated operations for combining ahead with its arguments and modifiers.", "labels": [], "entities": []}, {"text": "We represent this structure as terms over the AM algebra as defined in.", "labels": [], "entities": []}, {"text": "This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step.", "labels": [], "entities": []}, {"text": "The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7413100004196167}]}, {"text": "More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of for identifying the operations of the algebra.", "labels": [], "entities": []}, {"text": "One key challenge is that the resulting term of the AM algebra must be semantically well-typed.", "labels": [], "entities": []}, {"text": "This makes the decoding problem NP-complete.", "labels": [], "entities": []}, {"text": "We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are projective.", "labels": [], "entities": []}, {"text": "We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-theart results on the other (Smatch f-scores of 71.0 and 70.2 respectively).", "labels": [], "entities": []}, {"text": "Our approach clearly outperforms strong but non-compositional baselines.", "labels": [], "entities": []}, {"text": "After reviewing related work in Section 2, we explain the AM algebra in Section 3 and extend it to a dependency view in Section 4.", "labels": [], "entities": []}, {"text": "We explain model training in Section 5 and decoding in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 evaluates a number of variants of our system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on the LDC2015E86 and LDC2017T10 3 datasets.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.9529863595962524}, {"text": "LDC2017T10 3 datasets", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.855428417523702}]}, {"text": "Technical details and hyperparameters of our implementation can be found in Sections B to D of the Supplementary Materials.", "labels": [], "entities": [{"text": "Supplementary Materials", "start_pos": 99, "end_pos": 122, "type": "DATASET", "confidence": 0.7667061984539032}]}], "tableCaptions": [{"text": " Table 1: 2015 & 2017 test set Smatch scores", "labels": [], "entities": [{"text": "Smatch", "start_pos": 31, "end_pos": 37, "type": "TASK", "confidence": 0.5014296174049377}]}]}