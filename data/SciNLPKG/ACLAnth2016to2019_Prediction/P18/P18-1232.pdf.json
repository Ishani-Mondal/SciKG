{"title": [{"text": "Learning Domain-Sensitive and Sentiment-Aware Word Embeddings *", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.9561846852302551}]}, {"text": "Given reviews from different domains , some existing methods for word embeddings exploit sentiment information , but they cannot produce domain-sensitive embeddings.", "labels": [], "entities": []}, {"text": "On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity.", "labels": [], "entities": []}, {"text": "We propose anew method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words.", "labels": [], "entities": []}, {"text": "Our method can automatically determine and produce domain-common embeddings and domain-specific embed-dings.", "labels": [], "entities": []}, {"text": "The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time.", "labels": [], "entities": []}, {"text": "Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.8640487194061279}]}], "introductionContent": [{"text": "Sentiment classification aims to predict the sentiment polarity, such as \"positive\" or \"negative\", over apiece of review.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9472193419933319}]}, {"text": "It has been a long-standing research topic because of its importance for many applications such as social media analysis, ecommerce, and marketing.", "labels": [], "entities": [{"text": "social media analysis", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7279767394065857}]}, {"text": "Deep learning has brought in progress in various NLP tasks, including sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.9739649891853333}]}, {"text": "Some researchers focus on designing RNN or CNN based models for predicting sentence level or aspect level sentiment ().", "labels": [], "entities": [{"text": "predicting sentence level or aspect level sentiment", "start_pos": 64, "end_pos": 115, "type": "TASK", "confidence": 0.718029797077179}]}, {"text": "These works directly take the word embeddings pre-trained for general purpose as initial word representations and may conduct fine tuning in the training process.", "labels": [], "entities": []}, {"text": "Some other researchers look into the problem of learning taskspecific word embeddings for sentiment classification aiming at solving some limitations of applying general pre-trained word embeddings.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.9776498377323151}]}, {"text": "For example, develop a neural network model to convey sentiment information in the word embeddings.", "labels": [], "entities": []}, {"text": "As a result, the learned embeddings are sentiment-aware and able to distinguish words with similar syntactic context but opposite sentiment polarity, such as the words \"good\" and \"bad\".", "labels": [], "entities": []}, {"text": "In fact, sentiment information can be easily obtained or derived in large scale from some data sources (e.g., the ratings provided by users), which allows reliable learning of such sentiment-aware embeddings.", "labels": [], "entities": []}, {"text": "Apart from these words (e.g. \"good\" and \"bad\") with consistent sentiment polarity in different contexts, the polarity of some sentiment words is domain-sensitive.", "labels": [], "entities": []}, {"text": "For example, the word \"lightweight\" usually connotes a positive sentiment in the electronics domain since a lightweight device is easier to carry.", "labels": [], "entities": []}, {"text": "In contrast, in the movie domain, the word \"lightweight\" usually connotes a negative opinion describing movies that do not invoke deep thoughts among the audience.", "labels": [], "entities": []}, {"text": "This observation motivates the study of learning domainsensitive word representations ().", "labels": [], "entities": []}, {"text": "They basically learn separate embeddings of the same word for different domains.", "labels": [], "entities": []}, {"text": "To bridge the semantics of individual embedding spaces, they select a subset of words that are likely to be domain-insensitive and align the dimensions of their embeddings.", "labels": [], "entities": []}, {"text": "However, the sentiment information is not exploited in these methods although they intend to tackle the task of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.8958032131195068}]}, {"text": "In this paper, we aim at learning word embeddings that are both domain-sensitive and sentiment-aware.", "labels": [], "entities": []}, {"text": "Our proposed method can jointly model the sentiment semantics and domain specificity of words, expecting the learned embeddings to achieve superior performance for the task of sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 176, "end_pos": 200, "type": "TASK", "confidence": 0.8691365718841553}]}, {"text": "Specifically, our method can automatically determine and produce domain-common embeddings and domainspecific embeddings.", "labels": [], "entities": []}, {"text": "Domain-common embeddings represent the fact that the semantics of a word including its sentiment and meaning in different domains are very similar.", "labels": [], "entities": []}, {"text": "For example, the words \"good\" and \"interesting\" are usually domain-common and convey consistent semantic meanings and positive sentiments in different domains.", "labels": [], "entities": []}, {"text": "Thus, they should have similar embeddings across domains.", "labels": [], "entities": []}, {"text": "On the other hand, domain-specific word embeddings represent the fact that the sentiments or meanings across domains are different.", "labels": [], "entities": []}, {"text": "For example, the word \"lightweight\" represents different sentiment polarities in the electronics domain and the movie domain.", "labels": [], "entities": []}, {"text": "Moreover, some polysemous words have different meanings in different domains.", "labels": [], "entities": []}, {"text": "For example, the term \"apple\" refers to the famous technology company in the electronics domain or a kind of fruit in the food domain.", "labels": [], "entities": []}, {"text": "Our model exploits the information of sentiment labels and context words to distinguish domain-common and domain-specific words.", "labels": [], "entities": []}, {"text": "If a word has similar sentiments and contexts across domains, it indicates that the word has common semantics in these domains, and thus it is treated as domain-common.", "labels": [], "entities": []}, {"text": "Otherwise, the word is considered as domain-specific.", "labels": [], "entities": []}, {"text": "The learning of domain-common embeddings can allow the advantage of data augmentation of common semantics of multiple domains, and meanwhile, domainspecific embeddings allow us to capture the varied semantics of specific words in different domains.", "labels": [], "entities": []}, {"text": "Specifically, for each word in the vocabulary, we design a distribution to depict the probability of the word being domain-common.", "labels": [], "entities": []}, {"text": "The inference of the probability distribution is conducted based on the observed sentiments and contexts.", "labels": [], "entities": []}, {"text": "As mentioned above, we also exploit the information of sentiment labels for the learning of word embeddings that can distinguish words with similar syntactic context but opposite sentiment polarity.", "labels": [], "entities": []}, {"text": "To demonstrate the advantages of our domainsensitive and sentiment-aware word embeddings, we conduct experiments on four domains, including books, DVSs, electronics, and kitchen appliances.", "labels": [], "entities": []}, {"text": "The experimental results show that our model can outperform the state-of-the-art models on the task of sentence level sentiment classification.", "labels": [], "entities": [{"text": "sentence level sentiment classification", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.7522296756505966}]}, {"text": "Moreover, we conduct lexicon term sentiment classification in two common sentiment lexicon sets to evaluate the effectiveness of our sentiment-aware embeddings learned from multiple domains, and it shows that our model outperforms the state-of-the-art models on most domains.", "labels": [], "entities": [{"text": "lexicon term sentiment classification", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.7085576802492142}]}], "datasetContent": [{"text": "We conducted experiments on the Amazon product reviews collected by.", "labels": [], "entities": [{"text": "Amazon product reviews collected", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.9396750330924988}]}, {"text": "We use four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K).", "labels": [], "entities": []}, {"text": "A category corresponds to a domain.", "labels": [], "entities": []}, {"text": "For each domain, there are 17,457 unlabeled reviews on average associated with rating scores from 1.0 to 5.0 for each domain.", "labels": [], "entities": []}, {"text": "We use unlabeled reviews with rating score higher than 3.0 as positive reviews and unlabeled reviews with rating score lower than 3.0 as negative reviews for embedding learning.", "labels": [], "entities": []}, {"text": "We first remove reviews whose length is less than 5 words.", "labels": [], "entities": []}, {"text": "We also remove punctuations and the stop words.", "labels": [], "entities": []}, {"text": "We also stem each word to its root form using Porter Stemmer.", "labels": [], "entities": []}, {"text": "Note that this review data is used for embedding learning, and the learned embeddings are used as feature vectors of words to conduct the experiments in the later two subsections.", "labels": [], "entities": []}, {"text": "Given the reviews from two domains, namely, D p and D q , we compare our results with the following baselines and state-of-the-art methods: SSWE The SSWE model", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of review sentiment classification. The markers  \u2020 and refer to p-value < 0.05 when  comparing with Yang and SSWE respectively.", "labels": [], "entities": [{"text": "review sentiment classification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6631255348523458}, {"text": "Yang", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9228588342666626}]}, {"text": " Table 2: Results of lexicon term sentiment classification.", "labels": [], "entities": [{"text": "lexicon term sentiment classification", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.783761203289032}]}, {"text": " Table 3: Statistics of the sentiment lexicons.", "labels": [], "entities": []}, {"text": " Table 4: Learned domain-commonality for some words. p(z = 1) denotes the probability that the word  is domain-common. The letter in parentheses indicates the domain of the review.", "labels": [], "entities": []}]}