{"title": [{"text": "Orthographic Features for Bilingual Lexicon Induction", "labels": [], "entities": [{"text": "Bilingual Lexicon Induction", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.8132452368736267}]}], "abstractContent": [{"text": "Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance , which can be helpful for pairs of related languages.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6926949421564738}]}, {"text": "This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9964518547058105}]}], "introductionContent": [{"text": "Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.7434129019578298}]}, {"text": "Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable.", "labels": [], "entities": []}, {"text": "One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary.", "labels": [], "entities": []}, {"text": "More recent work has focused on reducing that constraint.", "labels": [], "entities": []}, {"text": "Vuli\u00b4c and use document-aligned data to learn bilingual embeddings instead of a seed dictionary.", "labels": [], "entities": []}, {"text": "use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; use an adversarial training method to learn a similar mapping.", "labels": [], "entities": []}, {"text": "use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by as the initialization fora completely unsupervised machine translation system.", "labels": [], "entities": []}, {"text": "These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts.", "labels": [], "entities": [{"text": "bilingual lexicon induction", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6440074940522512}]}, {"text": "However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syntactic/semantic information encoded in the word embeddings).", "labels": [], "entities": []}, {"text": "This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity: investigate using linguistic features for word alignment, and use linguistic features for unsupervised bilingual lexicon induction.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 208, "end_pos": 222, "type": "TASK", "confidence": 0.7644965946674347}, {"text": "bilingual lexicon induction", "start_pos": 269, "end_pos": 296, "type": "TASK", "confidence": 0.7004249095916748}]}, {"text": "These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni).", "labels": [], "entities": []}, {"text": "The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods.", "labels": [], "entities": []}, {"text": "In this work, we extend the modern embeddingbased approach of with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9955847859382629}, {"text": "bilingual lexicon induction", "start_pos": 178, "end_pos": 205, "type": "TASK", "confidence": 0.684286872545878}]}], "datasetContent": [{"text": "We use the datasets used by, consisting of three language pairs: EnglishItalian, English-German, and English-Finnish.", "labels": [], "entities": []}, {"text": "The English-Italian dataset was introduced in; the other datasets were created by.", "labels": [], "entities": [{"text": "English-Italian dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7484649419784546}]}, {"text": "Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b)) for both languages and a bilingual dictionary, separated into a training and test set.", "labels": [], "entities": []}, {"text": "We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations (such as 2-2, 3-3, et cetera) as in.", "labels": [], "entities": []}, {"text": "However, because the methods presented in this work feature tunable hyperparameters, we use a portion of the training set as devel-: Comparison of methods on test data.", "labels": [], "entities": []}, {"text": "Scaling constants c e and c s were selected based on performance on development data overall three language pairs.", "labels": [], "entities": []}, {"text": "The last two rows report the results of using both methods together.", "labels": [], "entities": []}, {"text": "In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations.", "labels": [], "entities": []}, {"text": "On average, the number of translations for each source (non-English) word was 1.2 for English-Italian, 1.3 for English-German, and 1.4 for English-Finnish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of methods on test data. Scaling constants c e and c s were selected based on  performance on development data over all three language pairs. The last two rows report the results of  using both methods together.", "labels": [], "entities": []}]}