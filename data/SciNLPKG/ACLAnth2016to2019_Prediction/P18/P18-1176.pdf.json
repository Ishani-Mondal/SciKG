{"title": [{"text": "Did the Model Understand the Question?", "labels": [], "entities": []}], "abstractContent": [{"text": "We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text.", "labels": [], "entities": [{"text": "question answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8546916246414185}]}, {"text": "Using the notion of at-tribution (word importance), we find that these deep networks often ignore important question terms.", "labels": [], "entities": []}, {"text": "Leveraging such behavior , we perturb questions to craft a variety of adversarial examples.", "labels": [], "entities": []}, {"text": "Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9995844960212708}, {"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7014251351356506}, {"text": "tabular question answering", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.6315944592157999}]}, {"text": "Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models.", "labels": [], "entities": []}, {"text": "Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9967768788337708}]}, {"text": "When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, deep learning has been applied to a variety of question answering tasks.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8543234666188558}]}, {"text": "For instance, to answer questions about images (e.g. (), tabular data (e.g.), and passages of text (e.g. ().", "labels": [], "entities": []}, {"text": "Developers, end-users, and reviewers (in academia) would all like to understand the capabilities of these models.", "labels": [], "entities": []}, {"text": "The standard way of measuring the goodness of a system is to evaluate its error on a test set.", "labels": [], "entities": []}, {"text": "High accuracy is indicative of a good model only if the test set is representative of the underlying realworld task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9991216063499451}]}, {"text": "Most tasks have large test and training sets, and it is hard to manually check that they are representative of the real world.", "labels": [], "entities": []}, {"text": "In this paper, we propose techniques to analyze the sensitivity of a deep learning model to question words.", "labels": [], "entities": []}, {"text": "We do this by applying attribution (as discussed in section 3), and generating adversarial questions.", "labels": [], "entities": []}, {"text": "Here is an illustrative example: recall Visual Question Answering ( where the task is to answer questions about images.", "labels": [], "entities": [{"text": "recall Visual Question Answering", "start_pos": 33, "end_pos": 65, "type": "TASK", "confidence": 0.710016280412674}]}, {"text": "Consider the question \"how symmetrical are the white bricks on either side of the building?\"", "labels": [], "entities": []}, {"text": "(corresponding image in).", "labels": [], "entities": []}, {"text": "The system that we study gets the answer right (\"very\").", "labels": [], "entities": []}, {"text": "But, we find (using an attribution approach) that the system relies on only a few of the words like \"how\" and \"bricks\".", "labels": [], "entities": []}, {"text": "Indeed, we can construct adversarial questions about the same image that the system gets wrong.", "labels": [], "entities": []}, {"text": "For instance, \"how spherical are the white bricks on either side of the building?\" returns the same answer (\"very\").", "labels": [], "entities": []}, {"text": "A key premise of our work is that most humans have expertise in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.8697670996189117}]}, {"text": "Even if they cannot manually check that a dataset is representative of the real world, they can identify important question words, and anticipate their function in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.8568828403949738}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: VQA network (Kazemi and Elqursh, 2017): Ac-", "labels": [], "entities": [{"text": "VQA network", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.9256992638111115}, {"text": "Ac", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.8121551275253296}]}, {"text": " Table 2: Attributions to column names for table-specific default programs (programs returned by NP on empty input ques-", "labels": [], "entities": []}, {"text": " Table 3: Neural Programmer (Neelakantan et al., 2017):", "labels": [], "entities": []}]}