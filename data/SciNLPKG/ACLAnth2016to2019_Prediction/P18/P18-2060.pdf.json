{"title": [{"text": "Neural Hidden Markov Model for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7814215421676636}]}], "abstractContent": [{"text": "This work aims to investigate alternative neural machine translation (NMT) approaches and thus proposes a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models.", "labels": [], "entities": [{"text": "machine translation (NMT)", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.8162471055984497}]}, {"text": "The neural models make use of encoder and decoder components , but drop the attention component.", "labels": [], "entities": []}, {"text": "The training is end-to-end and the stand-alone decoder is able to provide comparable performance with the state-of-the-art attention-based models on three different translation tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Attention-based neural translation models attend to specific positions on the source side to generate translation.", "labels": [], "entities": [{"text": "Attention-based neural translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6830177903175354}]}, {"text": "Using the attention component provides significant improvements over the pure encoder-decoder sequence-to-sequence approach) that uses no such attention mechanism.", "labels": [], "entities": []}, {"text": "In this work, we aim to compare the performance of attention-based models to another baseline, namely, neural hidden Markov models.", "labels": [], "entities": []}, {"text": "The neural HMM has been successfully applied in the literature on top of conventional phrasebased systems (.", "labels": [], "entities": []}, {"text": "In this work, our purpose is to explore its application in standalone decoding, i.e. the model is used to generate and score candidates without assistance from a phrase-based system.", "labels": [], "entities": []}, {"text": "Because translation is done standalone using only neural models, we still refer to this as NMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9681552052497864}]}, {"text": "In addition, while applied feedforward networks to model alignment and translation, the recurrent structures proposed in this work surpass the feedforward variants by up to 1.3% in BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.6961955428123474}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9128458499908447}]}, {"text": "By comparing neural HMM and attention-based NMT, we shed light on the role of the attention component.", "labels": [], "entities": []}, {"text": "To this end, we use an alignmentbased model that has a recurrent bidirectional encoder and a recurrent decoder, but use no attention component.", "labels": [], "entities": []}, {"text": "We replace the attention mechanism by a first-order HMM alignment model.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8147692978382111}]}, {"text": "Attention levels are deterministic normalized similarity scores part of the architecture design of an otherwise fully supervised classifier.", "labels": [], "entities": []}, {"text": "HMM-style alignments on the other hand are discrete random variables and (unlike attention levels) must be marginalized.", "labels": [], "entities": []}, {"text": "Once alignments are marginalized, which is tractable fora first-order HMM, parameters can be estimated to attain a local optimum of log-likelihood of observations as usual.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on the WMT 2017 German\u2194English and Chinese\u2192English translation tasks, which consist of 5M and 23M parallel sentence pairs respectively.", "labels": [], "entities": [{"text": "WMT 2017 German\u2194English and Chinese\u2192English translation tasks", "start_pos": 37, "end_pos": 98, "type": "TASK", "confidence": 0.8271226341074164}]}, {"text": "Translation quality is measured with the case sensitive BLEU) and TER () metric on newstests 2017, which contain 3004 (German\u2194English) and 2001 (Chinese\u2192English) sentence pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9869837760925293}, {"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9980046153068542}]}, {"text": "For German and English preprocessing, we use the Moses tokenizer with hyphen splitting, and perform truecasing with Moses scripts (.", "labels": [], "entities": []}, {"text": "For German\u2194English subword segmentation ( , we use 20K joint BPE operations.", "labels": [], "entities": [{"text": "English subword segmentation", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.6025521059830984}, {"text": "BPE", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7402985095977783}]}, {"text": "For the Chinese data, we segment it using the Jieba 1 segmenter.", "labels": [], "entities": [{"text": "Chinese data", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.86342453956604}, {"text": "Jieba 1 segmenter", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9233518044153849}]}, {"text": "We then learn a BPE model on the segmented Chinese, also using 20K merge operations.", "labels": [], "entities": [{"text": "BPE", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.6943717002868652}]}, {"text": "During training, sentences with a length greater than 50 subwords are filtered out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results on WMT 2017 German\u2194English and Chinese\u2192English test sets.  All models are trained without synthetic data. Single model is used for decoding.  1 (Wang et al., 2017) but applied in decoding instead of rescoring  2 This work  3 (Bahdanau et al., 2015) with small modifications (Section 5.1)", "labels": [], "entities": [{"text": "WMT 2017 German\u2194English and Chinese\u2192English test sets", "start_pos": 34, "end_pos": 87, "type": "DATASET", "confidence": 0.8624868284572255}]}]}