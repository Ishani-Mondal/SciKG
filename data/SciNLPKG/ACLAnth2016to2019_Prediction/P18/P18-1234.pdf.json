{"title": [{"text": "Aspect Based Sentiment Analysis with Gated Convolutional Networks", "labels": [], "entities": [{"text": "Aspect Based Sentiment Analysis", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8136076629161835}]}], "abstractContent": [{"text": "Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text.", "labels": [], "entities": [{"text": "Aspect based sentiment analysis (ABSA)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7953871829169137}, {"text": "general sentiment analysis", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.72164386510849}]}, {"text": "We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA).", "labels": [], "entities": [{"text": "aspect-category sentiment analysis", "start_pos": 52, "end_pos": 86, "type": "TASK", "confidence": 0.7598164975643158}, {"text": "aspect-term sentiment analysis (ATSA)", "start_pos": 98, "end_pos": 135, "type": "TASK", "confidence": 0.6540067940950394}]}, {"text": "Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time.", "labels": [], "entities": []}, {"text": "We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient.", "labels": [], "entities": []}, {"text": "First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity.", "labels": [], "entities": []}, {"text": "The architecture is much simpler than attention layer used in the existing models.", "labels": [], "entities": []}, {"text": "Second , the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently.", "labels": [], "entities": []}, {"text": "The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.7614696025848389}]}], "introductionContent": [{"text": "Opinion mining and sentiment analysis) on user-generated reviews can provide valuable information for providers and consumers.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6627388894557953}, {"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8138114809989929}]}, {"text": "Instead of predicting the overall sen-timent polarity, fine-grained aspect based sentiment analysis (ABSA) () is proposed to better understand reviews than traditional sentiment analysis.", "labels": [], "entities": [{"text": "fine-grained aspect based sentiment analysis (ABSA)", "start_pos": 55, "end_pos": 106, "type": "TASK", "confidence": 0.6536435000598431}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8054747581481934}]}, {"text": "Specifically, we are interested in the sentiment polarity of aspect categories or target entities in the text.", "labels": [], "entities": []}, {"text": "Sometimes, it is coupled with aspect term extractions (.", "labels": [], "entities": [{"text": "aspect term extractions", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.5947258869806925}]}, {"text": "A number of models have been developed for ABSA, but there are two different subtasks, namely aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA).", "labels": [], "entities": [{"text": "ABSA", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9827600717544556}, {"text": "aspect-category sentiment analysis", "start_pos": 94, "end_pos": 128, "type": "TASK", "confidence": 0.7240609725316366}, {"text": "aspect-term sentiment analysis (ATSA)", "start_pos": 140, "end_pos": 177, "type": "TASK", "confidence": 0.6631761888662974}]}, {"text": "The goal of ACSA is to predict the sentiment polarity with regard to the given aspect, which is one of a few predefined categories.", "labels": [], "entities": [{"text": "ACSA", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9185109734535217}]}, {"text": "On the other hand, the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead, which could be a multi-word phrase or a single word.", "labels": [], "entities": [{"text": "ATSA", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9047335386276245}]}, {"text": "The number of distinct words contributing to aspect terms could be more than a thousand.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"Average to good Thai food, but terrible delivery.\", ATSA would ask the sentiment polarity towards the entity Thai food; while ACSA would ask the sentiment polarity toward the aspect service, even though the word service does not appear in the sentence.", "labels": [], "entities": [{"text": "ATSA", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.6945462822914124}]}, {"text": "Many existing models use LSTM layers) to distill sentiment information from embedding vectors, and apply attention mechanisms) to enforce models to focus on the text spans related to the given aspect/entity.", "labels": [], "entities": []}, {"text": "Such models include Attention-based LSTM with Aspect Embedding (ATAE-LSTM) () for ACSA; Target-Dependent Sentiment Classification (TD-LSTM) (), Gated Neural Networks ( and Recurrent Attention Memory Network (RAM)) for ATSA.", "labels": [], "entities": [{"text": "Target-Dependent Sentiment Classification", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.6244565943876902}]}, {"text": "Attention mechanisms has been successfully used in many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8947229981422424}]}, {"text": "It first computes the alignment scores between context vectors and target vector; then carryout a weighted sum with the scores and the context vectors.", "labels": [], "entities": []}, {"text": "However, the context vectors have to encode both the aspect and sentiment information, and the alignment scores are applied across all feature dimensions regardless of the differences between these two types of information.", "labels": [], "entities": []}, {"text": "Both LSTM and attention layer are very timeconsuming during training.", "labels": [], "entities": []}, {"text": "LSTM processes one token in a step.", "labels": [], "entities": []}, {"text": "Attention layer involves exponential operation and normalization of all alignment scores of all the words in the sentence ().", "labels": [], "entities": []}, {"text": "Moreover, some models needs the positional information between words and targets to produce weighted LSTM (, which can be unreliable in noisy review text.", "labels": [], "entities": []}, {"text": "Certainly, it is possible to achieve higher accuracy by building more and more complicated LSTM cells and sophisticated attention mechanisms; but one has to hold more parameters in memory, get more hyper-parameters to tune and spend more time in training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.998371422290802}]}, {"text": "In this paper, we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms, which has much less training time than LSTM based networks, but with better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9982544779777527}]}, {"text": "For ACSA task, our model has two separate convolutional layers on the top of the embedding layer, whose outputs are combined by novel gating units.", "labels": [], "entities": [{"text": "ACSA task", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.8866657912731171}]}, {"text": "Convolutional layers with multiple filters can efficiently extract n-gram features at many granularities on each receptive field.", "labels": [], "entities": []}, {"text": "The proposed gating units have two nonlinear gates, each of which is connected to one convolutional layer.", "labels": [], "entities": []}, {"text": "With the given aspect information, they can selectively extract aspect-specific sentiment information for sentiment prediction.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.9640474319458008}]}, {"text": "For example, in the sentence \"Average to good Thai food, but terrible delivery.\", when the aspect food is provided, the gating units automatically ignore the negative sentiment of aspect delivery from the second clause, and only output the positive sentiment from the first clause.", "labels": [], "entities": []}, {"text": "Since each component of the proposed model could be easily parallelized, it has much less training time than the models based on LSTM and attention mechanisms.", "labels": [], "entities": []}, {"text": "For ATSA task, where the aspect terms consist of multiple words, we extend our model to include another convolutional layer for the target expressions.", "labels": [], "entities": [{"text": "ATSA task", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.6323137581348419}]}, {"text": "We evaluate our models on the SemEval datasets, which contains restaurants and laptops reviews with labels on aspect level.", "labels": [], "entities": [{"text": "SemEval datasets", "start_pos": 30, "end_pos": 46, "type": "DATASET", "confidence": 0.8315476477146149}]}, {"text": "To the best of our knowledge, no CNNbased model has been proposed for aspect based sentiment analysis so far.", "labels": [], "entities": [{"text": "aspect based sentiment analysis", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.6668408215045929}]}], "datasetContent": [{"text": "We conduct experiments on public datasets from SemEval workshops (), which consist of customer reviews about restaurants and laptops.", "labels": [], "entities": []}, {"text": "Some existing work () removed \"conflict\" labels from four sentiment labels, which makes their results incomparable to those from the workshop report).", "labels": [], "entities": []}, {"text": "We reimplemented the compared methods, and used hyper-parameter settings described in these references.", "labels": [], "entities": []}, {"text": "The sentences which have different sentiment labels for different aspects or targets in the sentence are more common in review data than in standard sentiment classification benchmark.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.7592129409313202}]}, {"text": "The sentence in shows the reviewer's different attitude towards two aspects: food and delivery.", "labels": [], "entities": []}, {"text": "Therefore, to access how the models perform on review sentences more accurately, we create small but difficult datasets, which are made up of the sentences having opposite or different sentiments on different aspects/targets.", "labels": [], "entities": []}, {"text": "In, the two identical sentences but with different sentiment labels are both included in the dataset.", "labels": [], "entities": []}, {"text": "If a sentence has 4 aspect targets, this sentence would have 4 copies in the data set, each of which is associated with different target and sentiment label.", "labels": [], "entities": []}, {"text": "For ACSA task, we conduct experiments on restaurant review data of SemEval 2014 Task 4.", "labels": [], "entities": [{"text": "restaurant review data of SemEval 2014 Task 4", "start_pos": 41, "end_pos": 86, "type": "DATASET", "confidence": 0.7790806517004967}]}, {"text": "There are 5 aspects: food, price, service, ambience, and misc; 4 sentiment polarities: positive, negative, neutral, and conflict.", "labels": [], "entities": []}, {"text": "By merging restaurant reviews of three years 2014 -2016, we obtain a larger dataset called \"Restaurant-Large\".", "labels": [], "entities": []}, {"text": "Incompatibilities of data are fixed during merging.", "labels": [], "entities": []}, {"text": "We replace conflict labels with neutral labels in the 2014 dataset.", "labels": [], "entities": [{"text": "2014 dataset", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.7950548827648163}]}, {"text": "In the 2015 and 2016 datasets, there could be multiple pairs of \"aspect terms\" and \"aspect category\" in one sentence.", "labels": [], "entities": []}, {"text": "For each sentence, let p denote the number of positive labels minus the number of negative labels.", "labels": [], "entities": []}, {"text": "We assign a sentence a positive label if p > 0, a negative label if p < 0, or a neutral label if p = 0.", "labels": [], "entities": []}, {"text": "After removing duplicates, the statistics are show in.", "labels": [], "entities": []}, {"text": "The resulting dataset has 8 aspects: restaurant, food, drinks, ambience, service, price, misc and location.", "labels": [], "entities": []}, {"text": "For ATSA task, we use restaurant reviews and laptop reviews from SemEval 2014 Task 4.", "labels": [], "entities": [{"text": "ATSA task", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.422548770904541}, {"text": "SemEval 2014 Task 4", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6039588302373886}]}, {"text": "On each dataset, we duplicate each sentence n a times, which is equal to the number of associated aspect categories (ACSA) or aspect terms (ATSA).", "labels": [], "entities": [{"text": "ATSA", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.8398609757423401}]}, {"text": "The statistics of the datasets are shown in.", "labels": [], "entities": []}, {"text": "The sizes of hard data sets are also shown in Table 2.", "labels": [], "entities": []}, {"text": "The test set is designed to measure whether a model can detect multiple different sentiment polarities in one sentence toward different entities.", "labels": [], "entities": []}, {"text": "Without such sentences, a classifier for overall sentiment classification might be good enough for the sentences associated with only one sentiment label.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8080559968948364}]}, {"text": "In our experiments, word embedding vectors are initialized with 300-dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens ().", "labels": [], "entities": []}, {"text": "Words out of the vocabulary of GloVe are randomly initialized with a uniform distribution U (\u22120.25, 0.25).", "labels": [], "entities": [{"text": "GloVe", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8984817266464233}]}, {"text": "We use Adagrad () with a batch size of 32 instances, default learning rate of 1e\u22122, and maximal epochs of 30.", "labels": [], "entities": [{"text": "default learning rate", "start_pos": 53, "end_pos": 74, "type": "METRIC", "confidence": 0.7312853833039602}]}, {"text": "We only fine tune early stopping with 5-fold cross validation on training datasets.", "labels": [], "entities": []}, {"text": "All neural models are implemented in PyTorch.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9230108857154846}]}], "tableCaptions": [{"text": " Table 2: Statistics of the datasets for ACSA task. The hard dataset is only made up of sentences having  multiple aspect labels associated with multiple sentiments.", "labels": [], "entities": [{"text": "ACSA task", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.8375270068645477}]}, {"text": " Table 3: Statistics of the datasets for ATSA task.", "labels": [], "entities": [{"text": "ATSA task", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.7750156819820404}]}, {"text": " Table 4: The accuracy of all models on test sets and on the subsets made up of test sentences that have  multiple sentiments and multiple aspect terms. Restaurant-Large dataset is created by merging all the  restaurant reviews of SemEval workshops within three years. '*': the results with SVM are retrieved  from NRC-Canada (Kiritchenko et al., 2014).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9990783929824829}, {"text": "NRC-Canada", "start_pos": 315, "end_pos": 325, "type": "DATASET", "confidence": 0.9581944346427917}]}, {"text": " Table 5: The accuracy of ATSA subtask on SemEval 2014 Task 4. '*': the results with SVM are retrieved  from NRC-Canada (Kiritchenko et al., 2014)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9993984699249268}, {"text": "SemEval 2014 Task", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.6781508525212606}, {"text": "NRC-Canada", "start_pos": 109, "end_pos": 119, "type": "DATASET", "confidence": 0.949400007724762}]}, {"text": " Table 6: The time to converge in seconds on ATSA  task.", "labels": [], "entities": [{"text": "ATSA  task", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.5599421858787537}]}, {"text": " Table 7: The accuracy of different gating units on  restaurant reviews on ACSA task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995133876800537}, {"text": "ACSA task", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.6201045513153076}]}]}