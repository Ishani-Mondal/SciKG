{"title": [{"text": "Simple and Effective Multi-Paragraph Reading Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input.", "labels": [], "entities": [{"text": "paragraph-level question answering", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.5870119035243988}]}, {"text": "Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text.", "labels": [], "entities": [{"text": "question answering", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7912270128726959}]}, {"text": "We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs.", "labels": [], "entities": []}, {"text": "Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output.", "labels": [], "entities": []}, {"text": "We additionally identify and improve upon a number of other design decisions that arise when working with document-level data.", "labels": [], "entities": []}, {"text": "Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.9200440049171448}, {"text": "TriviaQA", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.9429290294647217}]}], "introductionContent": [{"text": "Teaching machines to answer arbitrary usergenerated questions is a long-term goal of natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.6510756512482961}]}, {"text": "For a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7265169620513916}]}, {"text": "However, automatically extracting the answer from those texts remains an open challenge.", "labels": [], "entities": []}, {"text": "The recent success of neural models at answering questions given a related paragraph () suggests they have the potential to be a key part of * Work completed while interning at the Allen Institute for Artificial Intelligence a solution to this problem.", "labels": [], "entities": []}, {"text": "Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input.", "labels": [], "entities": []}, {"text": "There are two basic approaches to this task.", "labels": [], "entities": []}, {"text": "Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer ().", "labels": [], "entities": []}, {"text": "Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence ().", "labels": [], "entities": []}, {"text": "Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph.", "labels": [], "entities": [{"text": "paragraph selection", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.747901976108551}]}, {"text": "As we shall show, naively trained models often struggle to meet this requirement.", "labels": [], "entities": []}, {"text": "In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results.", "labels": [], "entities": []}, {"text": "Then we introduce a method for training models to produce accurate per-paragraph confidence scores, and we show how combining this method with multiple paragraph selection further increases performance.", "labels": [], "entities": [{"text": "accurate per-paragraph confidence scores", "start_pos": 58, "end_pos": 98, "type": "METRIC", "confidence": 0.6500457376241684}]}, {"text": "Our pipelined method focuses on addressing the challenges that come with training on documentlevel data.", "labels": [], "entities": []}, {"text": "We use a linear classifier to select which paragraphs to train and test on.", "labels": [], "entities": []}, {"text": "Since annotating entire documents is expensive, data of this sort is typically distantly supervised, meaning only the answer text, not the answer spans, are known.", "labels": [], "entities": []}, {"text": "To handle the noise this creates, we use a summed objective function that marginalizes the model's output overall locations the answer text occurs.", "labels": [], "entities": []}, {"text": "We apply this approach with a model design that integrates some recent ideas in reading comprehension models, including selfattention () and bi-directional attention ().", "labels": [], "entities": []}, {"text": "Our confidence method extends this approach to better handle the multi-paragraph setting.", "labels": [], "entities": []}, {"text": "Previous approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer.", "labels": [], "entities": []}, {"text": "This has several downsides: the model is not trained to produce low confidence scores for paragraphs that do not contain an answer, and the training objective does not require confidence scores to be comparable between paragraphs.", "labels": [], "entities": []}, {"text": "We resolve these problems by sampling paragraphs from the context documents, including paragraphs that do not contain an answer, to train on.", "labels": [], "entities": []}, {"text": "We then use a shared-normalization objective where paragraphs are processed independently, but the probability of an answer candidate is marginalized overall paragraphs sampled from the same document.", "labels": [], "entities": []}, {"text": "This requires the model to produce globally correct output even though each paragraph is processed independently.", "labels": [], "entities": []}, {"text": "We evaluate our work on TriviaQA () in the wiki, web, and unfiltered setting.", "labels": [], "entities": []}, {"text": "Our model achieves a nearly 10 point lead over published prior work.", "labels": [], "entities": []}, {"text": "We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD ( where only the correct document, not the correct paragraph, is known.", "labels": [], "entities": []}, {"text": "Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system 1 , and show it performs well on questions from the TREC question answering task ().", "labels": [], "entities": [{"text": "TREC question answering task", "start_pos": 154, "end_pos": 182, "type": "TASK", "confidence": 0.7394801527261734}]}, {"text": "We release our code 2 to facilitate future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on four datasets: TriviaQA unfiltered (, a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an individual training point; and SQuAD (, a collection of Wikipedia articles and crowdsourced questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on TriviaQA web using our  pipelined method.", "labels": [], "entities": [{"text": "TriviaQA web", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.8411248624324799}]}, {"text": " Table 3: Published TriviaQA results. Our approach advances the state of the art by about 10 points on  these datasets 4", "labels": [], "entities": []}, {"text": " Table 4: Results on the Curated TREC corpus, Yo- daQA results extracted from its github page 7", "labels": [], "entities": [{"text": "Curated TREC corpus", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.566436638434728}]}, {"text": " Table 5: Error analysis on TriviaQA web.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9710640907287598}, {"text": "TriviaQA web", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9585413336753845}]}]}