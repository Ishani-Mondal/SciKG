{"title": [{"text": "Word Error Rate Estimation for Speech Recognition: e-WER", "labels": [], "entities": [{"text": "Word Error Rate Estimation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.617333821952343}, {"text": "Speech Recognition", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7359575182199478}]}], "abstractContent": [{"text": "Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.815865566333135}, {"text": "word error rate (WER)", "start_pos": 131, "end_pos": 152, "type": "METRIC", "confidence": 0.8662288387616476}]}, {"text": "In this paper, we propose a novel approach to estimate WER, or eWER , which does not require a gold-standard transcription of the test set.", "labels": [], "entities": [{"text": "WER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.933962345123291}]}, {"text": "Our eWER framework uses a comprehensive set of features: ASR recognised text, character recognition results to complement recognition output, and internal decoder features.", "labels": [], "entities": [{"text": "ASR recognised text", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8618359565734863}]}, {"text": "We report results for the two features ; black-box and glass-box using unseen 24 Arabic broadcast programs.", "labels": [], "entities": []}, {"text": "Our system achieves 16.9% WER root mean squared error (RMSE) across 1,400 sentences.", "labels": [], "entities": [{"text": "WER root mean squared error (RMSE)", "start_pos": 26, "end_pos": 60, "type": "METRIC", "confidence": 0.9578618630766869}]}, {"text": "The estimated overall WER eWER was 25.3% for the three hours test set, while the actual WER was 28.5%.", "labels": [], "entities": [{"text": "WER eWER", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.8128776550292969}, {"text": "WER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9970940351486206}]}], "introductionContent": [{"text": "Automatic Speech Recognition (ASR) has made rapid progress in recent years, primarily due to advances in deep learning and powerful computing platforms.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8541310429573059}]}, {"text": "As a result, the quality of ASR has improved dramatically, leading to various applications, such as speech-to-speech translation, personal assistants, and broadcast media monitoring.", "labels": [], "entities": [{"text": "ASR", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9922852516174316}, {"text": "speech-to-speech translation", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.6958747655153275}]}, {"text": "Despite this progress, ASR performance is still closely tied to how well the acoustic model (AM) and language model (LM) training data matches the test conditions.", "labels": [], "entities": [{"text": "ASR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9916085600852966}]}, {"text": "Thus, it is important to be able to estimate the accuracy of an ASR system in a particular target environment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.99922776222229}, {"text": "ASR", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9903203845024109}]}, {"text": "Word Error Rate (WER) is the standard approach to evaluate the performance of a large vocabulary continuous speech recognition (LVCSR) system.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.7174507702390353}, {"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 80, "end_pos": 134, "type": "TASK", "confidence": 0.791411966085434}]}, {"text": "The word sequence hypothesised by the ASR system is aligned with a reference transcription, and the number of errors is computed as the sum of substitutions (S), insertions (I), and deletions (D).", "labels": [], "entities": [{"text": "ASR", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9150848984718323}]}, {"text": "If there are N total words in the reference transcription, then the word error rate WER is computed as follows: To obtain a reliable estimate of the WER, at least two hours of test data are required fora typical LVCSR system.", "labels": [], "entities": [{"text": "word error rate WER", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.7131974995136261}, {"text": "WER", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9797359704971313}]}, {"text": "In order to perform the alignment, the test data needs to be manually transcribed at the word level -a time-consuming and expensive process.", "labels": [], "entities": []}, {"text": "It is, thus, of interest to develop techniques which can estimate the quality of an automatically generated transcription without requiring a gold-standard reference.", "labels": [], "entities": []}, {"text": "Such quality estimation techniques have been extensively investigated for machine translation, with extensions to spoken language translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7924468815326691}, {"text": "spoken language translation", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7490179936091105}]}, {"text": "Although there is along history of exploring wordlevel confidence measures for speech recognition, there has been less work on the direct estimation of speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.777306854724884}]}, {"text": "Seigel and Woodland (2014) studied the detection of deletions in ASR output using a conditional random field (CRF) sequence model to detect one or more deleted word regions in ASR output.", "labels": [], "entities": [{"text": "ASR output", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.8867935538291931}, {"text": "ASR output", "start_pos": 176, "end_pos": 186, "type": "TASK", "confidence": 0.9170366525650024}]}, {"text": "used word embeddings to build a confidence classifier which labeled each word in the recognised word sequence with an error or a correct label.", "labels": [], "entities": []}, {"text": "investigated the use of a recurrent neural network (RNN) language model (LM) with complementary deep neural network (DNN) and Gaussian Mix-ture Model (GMM) acoustic models in order to identify ASR errors, based on the assumption that when two ASR systems disagree on an utterance region, then it is most likely an error.", "labels": [], "entities": [{"text": "ASR", "start_pos": 193, "end_pos": 196, "type": "TASK", "confidence": 0.9186824560165405}]}, {"text": "investigated using deep bidirectional recurrent neural networks (DBRNNs) to detect errors in ASR results.", "labels": [], "entities": [{"text": "ASR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9880459308624268}]}, {"text": "They explored four tasks for ASR error detection and recognition rate estimation: confidence estimation, out-of-vocabulary (OOV) word detection, error type classification, and recognition rate estimation.", "labels": [], "entities": [{"text": "ASR error detection", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.9176532427469889}, {"text": "recognition rate estimation", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6363288859526316}, {"text": "confidence estimation", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7195637226104736}, {"text": "out-of-vocabulary (OOV) word detection", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.5959339042504629}, {"text": "error type classification", "start_pos": 145, "end_pos": 170, "type": "TASK", "confidence": 0.5926951368649801}, {"text": "recognition rate estimation", "start_pos": 176, "end_pos": 203, "type": "TASK", "confidence": 0.6938508947690328}]}, {"text": "In an extension to this work,; investigated the estimation of speech recognition accuracy based on the classification of error types, in which sequence classification was performed by a CRF.", "labels": [], "entities": [{"text": "estimation of speech recognition", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7959347367286682}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.7967531681060791}, {"text": "sequence classification", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.7375784814357758}]}, {"text": "Each word in a hypothesised word sequence was classified into one of three categories: correct, substitution error, or insertion error.", "labels": [], "entities": [{"text": "correct", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9747690558433533}, {"text": "substitution error", "start_pos": 96, "end_pos": 114, "type": "METRIC", "confidence": 0.7505127489566803}, {"text": "insertion error", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.9151434898376465}]}, {"text": "Their study did not estimate the presence of deletions, and consequently cannot estimate the WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9649072289466858}]}, {"text": "developed a tool for ASR quality estimation, TranscRater, which is capable of predicting WER per utterance.", "labels": [], "entities": [{"text": "ASR quality estimation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8717604875564575}, {"text": "WER", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.915448784828186}]}, {"text": "This approach is based on a large set of extracted features (which do not require internal access to the ASR system) used to train a regression model (e.g., extremely randomised trees), and can also rank different transcriptions from multiple sources (.", "labels": [], "entities": []}, {"text": "TranscRater provides a WER per utterance, reporting the results as the MAE with respect to a reference transcription.", "labels": [], "entities": [{"text": "WER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9969863295555115}, {"text": "MAE", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9925153851509094}]}, {"text": "This work did not report WER estimates for complete recordings or test sets, although it is possible that this could be done using utterance length estimates.", "labels": [], "entities": [{"text": "WER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9478655457496643}]}, {"text": "In this paper, we build on these contributions to develop a system to directly estimate the WER of an ASR output hypothesis.", "labels": [], "entities": [{"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9953508377075195}, {"text": "ASR output hypothesis", "start_pos": 102, "end_pos": 123, "type": "TASK", "confidence": 0.8986466725667318}]}, {"text": "Our contributions are: (i) a novel approach to estimate WER per sentence and to aggregate them to provide WER estimation per recording or fora whole test set; (ii) an evaluation of our approach which compares the use of \"black-box\" features (without ASR decoder information) and \"glass-box\" features which use internal information from the decoder; and (iii) a release of the code and the data used for this paper for further research 1 . 1 https://github.com/qcri/e-wer", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained two DNN systems to estimat\u00ea N and ERR separately.", "labels": [], "entities": [{"text": "ERR", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8840706944465637}]}, {"text": "We explored training both a black-box based DNN system (without the decoder features) and a glass-box system using the decoder features.", "labels": [], "entities": []}, {"text": "Overall, four systems were trained: two glass-box systems and two blackbox systems.", "labels": [], "entities": []}, {"text": "We used the same hyper-parameters across the four systems.", "labels": [], "entities": []}, {"text": "present the e-WER performance in terms of the mean absolute error (MAE) and root mean squared error (RMSE) per sentence for ERR, \u02c6 N and the estimated WER for the dev and test sets with reference to the errors computed using a gold-standard reference.", "labels": [], "entities": [{"text": "mean absolute error (MAE)", "start_pos": 46, "end_pos": 71, "type": "METRIC", "confidence": 0.83736652135849}, {"text": "root mean squared error (RMSE)", "start_pos": 76, "end_pos": 106, "type": "METRIC", "confidence": 0.8754417896270752}, {"text": "ERR", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.7208740711212158}, {"text": "WER", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9892339706420898}]}, {"text": "As expected, the glass-box features help to reduce MAE and RMSE for both ERR and\u02c6Nand\u02c6 and\u02c6N . Although the difference between the black-box estimation and the glass-box results is not big for ERR and N , we can see that the impact becomes substantial on the estimated WER per sentence, which is almost double the error in both MAE and RMSE per sentence.", "labels": [], "entities": [{"text": "MAE", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8081289529800415}, {"text": "RMSE", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.907170295715332}, {"text": "WER", "start_pos": 269, "end_pos": 272, "type": "METRIC", "confidence": 0.9900732040405273}]}, {"text": "reports the overall performance on the dev and on the test set.", "labels": [], "entities": []}, {"text": "Across the 17 programs in the MGB-2 dev data, the actual WER is 33.1%, and the glass-box e-WER is 29.3%, while the black-box e-WER is 30.9%.", "labels": [], "entities": [{"text": "MGB-2 dev data", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9560770988464355}, {"text": "WER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9830622673034668}]}, {"text": "Evaluating the same models on the 24 programs in the test data set results in an actual WER of 28.5%, while the glassbox e-WER is 25.3%, and the black-box e-WER is 30.3%.", "labels": [], "entities": [{"text": "WER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9912889003753662}]}, {"text": "plots the cumulative WER and e-WER across the three hours test set.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9821955561637878}]}, {"text": "This plot indicates that the glass-box estimate is continually lower than the black-box estimate.", "labels": [], "entities": []}, {"text": "The large difference during the first 30 minutes arises owing the glass-box system is capable of better estimation with less data compared to the black-box system.", "labels": [], "entities": [{"text": "estimation", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.7968071699142456}]}, {"text": "We estimat\u00ea N and ERR separately.", "labels": [], "entities": [{"text": "ERR", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7090105414390564}]}, {"text": "Therefore, our system is capable of estimating the WER at different levels of granularity.", "labels": [], "entities": [{"text": "WER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8298837542533875}]}, {"text": "We visualise the prediction per program.", "labels": [], "entities": []}, {"text": "In scenarios such as media-monitoring, where the main objective is to have a robust monitoring system for specific programs, we plot the WER across the 24 programs in the test set, and we can see in that both the glass-box and black-box estimation are following the gold-standard WER per program.", "labels": [], "entities": [{"text": "WER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.8503828048706055}, {"text": "WER", "start_pos": 280, "end_pos": 283, "type": "METRIC", "confidence": 0.9820496439933777}]}, {"text": "However, unlike predicting word count\u02c6Ncount\u02c6 count\u02c6N or error count ERR, we can see that the black-box, in general, over-estimates the WER, while the glass-box system under-estimates WER similar to figure 1.", "labels": [], "entities": [{"text": "error count ERR", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9277371366818746}, {"text": "WER", "start_pos": 136, "end_pos": 139, "type": "METRIC", "confidence": 0.8865913152694702}]}, {"text": "One can argue from figure 2 that the decoder features are not helping in programs with high WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9915680289268494}]}, {"text": "We found both systems to be useful for reporting WER per program.", "labels": [], "entities": [{"text": "WER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.6532140374183655}]}], "tableCaptions": [{"text": " Table 1: Analysis of the train, dev and test data.", "labels": [], "entities": []}, {"text": " Table 2: MAE per sentence reported for the glass- box and black-box features.", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9988504648208618}]}, {"text": " Table 3: RMSE per sentence reported for the  glass-box and the black-box features.", "labels": [], "entities": [{"text": "RMSE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9973117113113403}]}, {"text": " Table 4: Overall WER across the dev and the test  data set.", "labels": [], "entities": [{"text": "WER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9917682409286499}]}]}