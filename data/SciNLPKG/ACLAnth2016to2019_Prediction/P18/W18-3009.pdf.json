{"title": [{"text": "Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding", "labels": [], "entities": [{"text": "Context-based Sentence Representation Learning", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.8270306140184402}]}], "abstractContent": [{"text": "Context plays an important role inhuman language understanding, thus it may also be useful for machines learning vector representations of language.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8065617978572845}]}, {"text": "In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning.", "labels": [], "entities": [{"text": "context-based sentence representation learning", "start_pos": 83, "end_pos": 129, "type": "TASK", "confidence": 0.6987875923514366}]}, {"text": "We carefully designed experiments to show that neither an autoregressive decoder nor an RNN de-coder is required.", "labels": [], "entities": []}, {"text": "After that, we designed a model which still keeps an RNN as the encoder, while using a non-autoregressive convolutional decoder.", "labels": [], "entities": []}, {"text": "We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance.", "labels": [], "entities": []}, {"text": "Our model is trained on two different large unlabelled corpora, and in both cases the transferabil-ity is evaluated on a set of downstream NLP tasks.", "labels": [], "entities": []}, {"text": "We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning distributed representations of sentences is an important and hard topic in both the deep learning and natural language processing communities, since it requires machines to encode a sentence with rich language content into a fixed-dimension vector filled with real numbers.", "labels": [], "entities": []}, {"text": "Our goal is to build a distributed sentence encoder learnt in an unsupervised fashion by exploiting the structure and relationships in a large unlabelled corpus.", "labels": [], "entities": []}, {"text": "Numerous studies inhuman language processing have supported that rich semantics of a word or sentence can be inferred from its context.", "labels": [], "entities": []}, {"text": "The idea of learning from the co-occurrence) was recently successfully applied to vector representation learning for words in and.", "labels": [], "entities": [{"text": "vector representation learning", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.7518338561058044}]}, {"text": "Avery recent successful application of the distributional hypothesis at the sentencelevel is the skip-thoughts model (.", "labels": [], "entities": []}, {"text": "The skip-thoughts model learns to encode the current sentence and decode the surrounding two sentences instead of the input sentence itself, which achieves overall good performance on all tested downstream NLP tasks that cover various topics.", "labels": [], "entities": []}, {"text": "The major issue is that the training takes too long since there are two RNN decoders to reconstruct the previous sentence and the next one independently.", "labels": [], "entities": []}, {"text": "Intuitively, given the current sentence, inferring the previous sentence and inferring the next one should be different, which supports the usage of two independent decoders in the skip-thoughts model.", "labels": [], "entities": []}, {"text": "However, proposed the skip-thought neighbour model, which only decodes the next sentence based on the current one, and has similar performance on downstream tasks compared to that of their implementation of the skipthoughts model.", "labels": [], "entities": []}, {"text": "In the encoder-decoder models for learning sentence representations, only the encoder will be used to map sentences to vectors after training, which implies that the quality of the generated language is not our main concern.", "labels": [], "entities": []}, {"text": "This leads to our twostep experiment to check the necessity of applying an autoregressive model as the decoder.", "labels": [], "entities": []}, {"text": "In other words, since the decoder's performance on language modelling is not our main concern, it is preferred to reduce the complexity of the decoder to speedup the training process.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7344138920307159}]}, {"text": "In our experiments, the first step is to check whether \"teacherforcing\" is required during training if we stick to using an autoregressive model as the decoder, and the second step is to check whether an autoregres-sive decoder is necessary to learn a good sentence encoder.", "labels": [], "entities": []}, {"text": "Briefly, the experimental results show that an autoregressive decoder is indeed not essential in learning a good sentence encoder; thus the two findings of our experiments lead to our final model design.", "labels": [], "entities": []}, {"text": "Our proposed model has an asymmetric encoderdecoder structure, which keeps an RNN as the encoder and has a CNN as the decoder, and the model explores using only the subsequent context information as the supervision.", "labels": [], "entities": []}, {"text": "The asymmetry in both model architecture and training pair reduces a large amount of the training time.", "labels": [], "entities": []}, {"text": "The contribution of our work is summarised as: 1.", "labels": [], "entities": []}, {"text": "We design experiments to show that an autoregressive decoder or an RNN decoder is not necessary in the encoder-decoder type of models for learning sentence representations, and based on our results, we present two findings.", "labels": [], "entities": []}, {"text": "Finding I: It is not necessary to input the correct words into an autoregressive decoder for learning sentence representations.", "labels": [], "entities": [{"text": "learning sentence representations", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.6469178299109141}]}, {"text": "Finding II: The model with an autoregressive decoder performs similarly to the model with a predict-all-words decoder.", "labels": [], "entities": []}, {"text": "2. The two findings above lead to our final model design, which keeps an RNN encoder while using a CNN decoder and learns to encode the current sentence and decode the subsequent contiguous words all at once.", "labels": [], "entities": []}, {"text": "3. With a suite of techniques, our model performs decently on the downstream tasks, and can be trained efficiently on a large unlabelled corpus.", "labels": [], "entities": []}, {"text": "The following sections will introduce the components in our \"RNN-CNN\" model, and discuss our experimental design.", "labels": [], "entities": [{"text": "RNN-CNN\" model", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.8614214062690735}]}], "datasetContent": [{"text": "The vocabulary for unsupervised training contains the 20k most frequent words in BookCorpus.", "labels": [], "entities": [{"text": "BookCorpus", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9783987402915955}]}, {"text": "In order to generalise the model trained with a relatively small, fixed vocabulary to the much larger set of all possible English words, we followed the vocabulary expansion method proposed in, which learns a linear mapping from the pretrained word vectors to the learnt RNN word: Related Work and Comparison.", "labels": [], "entities": [{"text": "vocabulary expansion", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.684847429394722}]}, {"text": "As presented, our designed asymmetric RNN-CNN model has strong transferability, and is overall better than existing unsupervised models in terms of fast training speed and good performance on evaluation tasks.", "labels": [], "entities": []}, {"text": "\" \u2020\"s refer to our models, and \"small/large\" refers to the dimension of representation as 1200/4800.", "labels": [], "entities": []}, {"text": "Bold numbers are the best ones among the models with same training and transferring setting, and underlined numbers are best results among all transfer learning models.", "labels": [], "entities": []}, {"text": "The training time of each model was collected from the paper that proposed it. vectors.", "labels": [], "entities": []}, {"text": "Thus, the model benefits from the generalisation ability of the pretrained word embeddings.", "labels": [], "entities": []}, {"text": "The downstream tasks for evaluation include semantic relatedness (SICK,), paraphrase detection (MSRP,), question-type classification (TREC,), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, Pang and Lee (2005), SST, Socher et al.), customer product reviews (CR,), subjectivity/objectivity classification (SUBJ,), opinion polarity (MPQA,), semantic textual similarity (STS14,), and SNLI (.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.8021458685398102}, {"text": "question-type classification", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.6959163546562195}, {"text": "subjectivity/objectivity classification", "start_pos": 312, "end_pos": 351, "type": "TASK", "confidence": 0.6866783946752548}]}, {"text": "After unsupervised training, the encoder is fixed, and applied as a representation extractor on the 10 tasks.", "labels": [], "entities": []}, {"text": "To compare the effect of different corpora, we also trained two models on Amazon Book Review dataset (without ratings) which is the largest subset of the Amazon Review dataset) with 142 million sentences, about twice as large as BookCorpus.", "labels": [], "entities": [{"text": "Amazon Book Review dataset", "start_pos": 74, "end_pos": 100, "type": "DATASET", "confidence": 0.9526968151330948}, {"text": "Amazon Review dataset", "start_pos": 154, "end_pos": 175, "type": "DATASET", "confidence": 0.9700024326642355}, {"text": "BookCorpus", "start_pos": 229, "end_pos": 239, "type": "DATASET", "confidence": 0.980344295501709}]}, {"text": "Both training and evaluation of our models were conducted in PyTorch , and we used SentEval 5 provided by to evaluate the transferability of our models.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9189794659614563}]}, {"text": "All the models were trained for the same number of iterations with the same batch size, and the performance was measured at the end of training for each of the models.", "labels": [], "entities": []}, {"text": "presents the results on 9 evaluation tasks of our proposed RNN-CNN models, and related work.", "labels": [], "entities": []}, {"text": "The \"small RNN-CNN\" refers to the model with the dimension of representation as 1200, and the \"large RNN-CNN\" refers to that as 4800.", "labels": [], "entities": []}, {"text": "The results of our \"large RNN-CNN\" model on SNLI is presented in.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8562350869178772}]}, {"text": "88.9 Our work was inspired by analysing the skipthoughts model ().", "labels": [], "entities": []}, {"text": "The skipthoughts model successfully applied this form of learning from the context information into unsupervised representation learning for sentences, and then, augmented the LSTM with proposed layer-normalisation (Skip-thought+LN), which improved the skip-thoughts model generally on downstream tasks.", "labels": [], "entities": []}, {"text": "In contrast, proposed the FastSent model which only learns source and target word embeddings and is an adaptation of Skip-gram ( to sentence-level learning without word order information.", "labels": [], "entities": []}, {"text": "applied a CNN as the encoder, but still applied LSTMs for decoding the adjacent sentences, which is called CNN-LSTM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in each  direction). The default way of producing the representation is a concatenation of outputs from a global  mean-pooling and a global max-pooling, while \"\u00b7-MaxOnly\" refers to the model with only global max- pooling. Bold numbers are the best results among all presented models. We found that 1) inputting correct  words to an autoregressive decoder is not necessary; 2) predict-all-words decoders work roughly the same  as autoregressive decoders; 3) mean+max pooling provides stronger transferability than the max-pooling  alone does. The table supports our choice of the predict-all-words CNN decoder and the way of producing  vector representations from the bi-directional RNN encoder.", "labels": [], "entities": []}, {"text": " Table 2: Related Work and Comparison. As presented, our designed asymmetric RNN-CNN model  has strong transferability, and is overall better than existing unsupervised models in terms of fast training  speed and good performance on evaluation tasks. \" \u2020\"s refer to our models, and \"small/large\" refers to  the dimension of representation as 1200/4800. Bold numbers are the best ones among the models with  same training and transferring setting, and underlined numbers are best results among all transfer learning  models. The training time of each model was collected from the paper that proposed it.", "labels": [], "entities": []}, {"text": " Table 3: We implemented the same classifier as  mentioned in Vendrov et al. (2015) on top of the  features computed by our model. Our proposed  RNN-CNN model gets similar result on SNLI as  skip-thoughts, but with much less training time.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.769719660282135}]}]}