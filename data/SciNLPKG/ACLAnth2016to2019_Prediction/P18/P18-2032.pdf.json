{"title": [{"text": "Diachronic degradation of language models: Insights from social media", "labels": [], "entities": []}], "abstractContent": [{"text": "Natural languages changeover time because they evolve to the needs of their users and the socio-technological environment.", "labels": [], "entities": []}, {"text": "This study investigates the di-achronic accuracy of pre-trained language models for downstream tasks in machine learning and user profiling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9905315637588501}, {"text": "user profiling", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7702597379684448}]}, {"text": "It asks the question: given that the social media platform and its users remain the same, how is language changing over time?", "labels": [], "entities": []}, {"text": "How can these differences be used to track the changes in the affect around a particular topic?", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first study to show that it is possible to measure diachronic semantic drifts within social media and within the span of a few years.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural languages are dynamic-they are constantly evolving and adapting to the needs of their users and the environment of their use (.", "labels": [], "entities": []}, {"text": "The arrival of large-scale collections of historic texts and online libraries and Google Books have greatly facilitated computational investigations of language changeover the span of decades.", "labels": [], "entities": []}, {"text": "Diachronic differences measure semantic drift specifically for languages overtime.", "labels": [], "entities": []}, {"text": "For instance, the meaning of the word 'follow' has changed from a reference, then to surveillance, and finally to the act of subscribing to asocial media user's feed.", "labels": [], "entities": []}, {"text": "Ina quantitative analysis, diachronic differences may explain why predictive models go 'stale'.", "labels": [], "entities": []}, {"text": "For instance, a sentiment model trained on Victorian-era language would label 'aweful' as positive sentiment; however, in contemporary usage, 'awful' is considered a negative word ().", "labels": [], "entities": []}, {"text": "Thus motivated, we raise the following research questions: \u2022 How do language models trained atone point in time, perform at predicting age and gender on language from a subsequent time?", "labels": [], "entities": []}, {"text": "\u2022 What is the practical benefit of measuring diachronic differences on Twitter?", "labels": [], "entities": []}, {"text": "To our knowledge, there is no existing work which has investigated whether, and how, language models degrade overtime, i.e. why predictive models trained on an older sample of language, may fail to work on contemporary language.", "labels": [], "entities": []}, {"text": "While previous studies have explored the change in word meanings spanning decades or hundreds of years, we address a research gap by exploring finer temporal granularity and using a more accessible language corpus.", "labels": [], "entities": []}, {"text": "Twitter's 1 discourse is rather different from traditional English writing.", "labels": [], "entities": [{"text": "Twitter's 1 discourse", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.794478565454483}]}, {"text": "So far, word embeddings trained on Twitter () have considered it a static corpus, and have not used it to study short term changes in word connotations.", "labels": [], "entities": []}, {"text": "It contributes with the following observations: \u2022 Diachronic differences are greater (hence, language change is faster) for younger social media users than older social media users.", "labels": [], "entities": []}, {"text": "\u2022 Diachronic language differences enable the measurement of the change in social attitudes (captured byword embeddings).", "labels": [], "entities": []}, {"text": "In order to study this phenomenon, we define the notion of temporal cohorts as a set of social media users who have posted on Twitter during the same time period, e.g., in the year 2011.", "labels": [], "entities": []}, {"text": "In this study, we evaluate the linguistic differences between temporal cohorts, e.g. 20-year-olds in 2011 vs. 20-year-olds in 2015.", "labels": [], "entities": []}], "datasetContent": [{"text": "Primary data: Our primary dataset consists of the Twitter posts of adults in the United States of America who were recruited by Qualtrics (a crowdsourcing platform similar to Amazon Mechanical Turk) for an online survey, and consented to share access to their Twitter posts.", "labels": [], "entities": []}, {"text": "This data was collected in a previous study by Preot\u00b8iucPreot\u00b8iuc-Pietro et al. and is available online . We restrict our analysis to tweets posted between January 2011 and December 2015, by those users who indicated English as a primary language, have written at least 10 posts in their posts in each year, and have reported age and binary gender as apart of the survey.", "labels": [], "entities": [{"text": "Preot\u00b8iucPreot\u00b8iuc-Pietro et al.", "start_pos": 47, "end_pos": 79, "type": "DATASET", "confidence": 0.8922834311212812}]}, {"text": "This resulted in a dataset of N = 554 users, who wrote a mean of 265 and a median of 156 posts per year and over 13.5 million words collectively.", "labels": [], "entities": []}, {"text": "The mean age of the population was 33.54 years.", "labels": [], "entities": []}, {"text": "59% of them self-identified as male.", "labels": [], "entities": []}, {"text": "Decahose (Twitter 10%) dataset: For insights based on word embeddings, we used the decahose samples for the years 2011 and 2014 collected by the TrendMiner project (, which comprises a 10% random sample of the real-time Twitter firehose using a realtime sampling algorithm.", "labels": [], "entities": [{"text": "Twitter 10%) dataset", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.5837169289588928}]}, {"text": "To match with our primary data, we used bounding boxes to consider only those tweets with geolocation information which were posted in the United States.", "labels": [], "entities": []}, {"text": "In this manner, we obtained 130 and 179 million Twitter posts for 2011 and 2014 respectively.", "labels": [], "entities": []}, {"text": "Pre-processing: Ina dataset of 554 users, the absolute vocabulary overlap maybe low.", "labels": [], "entities": [{"text": "absolute vocabulary overlap", "start_pos": 46, "end_pos": 73, "type": "METRIC", "confidence": 0.6981646517912546}]}, {"text": "By converting each users string of words into their probabilistic usage of 2000 topics, we expected to get more stable estimates than using word-based language models.", "labels": [], "entities": []}, {"text": "We represent the language of each user as a probabilistic distribution of 2000 topics derived using Latent Dirichlet Allocation (LDA) with \u03b1 set to 0.30 to favor fewer topics per document.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 100, "end_pos": 133, "type": "METRIC", "confidence": 0.8984450995922089}]}, {"text": "These topics are modeled as open-ended clusters of words from actual distributions in social media over approximately 18 million Facebook updates, and are provided as an open-sourced resource in the DLATK python toolkit ( . Predictive evaluation: We use Python's sklearn library to conduct a ten-fold cross-validation and train weighted linear regression models forage, and binary logistic regression models for gender, on the LDA-derived features for users in nine folds, and test on the users in the held out fold.", "labels": [], "entities": [{"text": "Predictive", "start_pos": 224, "end_pos": 234, "type": "METRIC", "confidence": 0.9464436173439026}]}, {"text": "We use feature selection, elastic-net regularization, and randomized PCA to avoid over-fitting.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7219094932079315}]}, {"text": "Although we tested other linguistic features such as n-grams, the best predictive performance was for models trained on the topic features.", "labels": [], "entities": []}, {"text": "Word embeddings: We separately train word embeddings on the language of the Twitter 10% sample from 2011, and the sample from 2014.", "labels": [], "entities": [{"text": "Twitter 10% sample from 2011", "start_pos": 76, "end_pos": 104, "type": "DATASET", "confidence": 0.779164065917333}]}, {"text": "We use Google's Tensorflow framework ( to optimize the prediction of cooccurrence relationships using an approximate objective known as skip-gram with negative sampling () with incremental initialization and optimizing our embeddings with a stochastic gradient descent.", "labels": [], "entities": []}, {"text": "Embeddings were trained using the top-50000 words by their average frequency over the entire time period.", "labels": [], "entities": []}, {"text": "A similar threshold has also been applied in previous papers.", "labels": [], "entities": []}, {"text": "We experimented with different window sizes and parameter settings, finally choosing a window size of 4, embeddings with 1000 dimensions, and the negative sample prior \u03b1 set to log(5) and the number of negative samples set to 500.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The features whose coefficients had the biggest  change and flipped sign when comparing the age and gen- der prediction models trained on 2011 language against those  trained on 2015 language. (0) depicts that the feature was no  longer significant in the 2015 model. *:(X10 \u2212 4)", "labels": [], "entities": []}]}