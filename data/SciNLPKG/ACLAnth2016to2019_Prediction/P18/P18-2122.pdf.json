{"title": [{"text": "Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection", "labels": [], "entities": [{"text": "Disambiguating False-Alarm Hashtag Usages", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8318039029836655}, {"text": "Irony Detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.6106007546186447}]}], "abstractContent": [{"text": "The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9640443921089172}]}, {"text": "This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.9291641116142273}]}, {"text": "We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 255, "end_pos": 270, "type": "TASK", "confidence": 0.9350124299526215}]}, {"text": "Furthermore, we apply our model to prune the self-labeled training data.", "labels": [], "entities": []}, {"text": "Experimental results show that the irony detection model trained on the less but cleaner training instances out-performs the models trained on all data.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.8666163980960846}]}], "introductionContent": [{"text": "Self-labeled data available on the Internet are popular research materials in many NLP areas.", "labels": [], "entities": []}, {"text": "Metadata such as tags and emoticons given by users are considered as labels for training and testing learning-based models, which usually benefit from large amount of data.", "labels": [], "entities": []}, {"text": "One of the sources of self-labeled data widely used in the research community is Twitter, where the short-text messages tweets written by the crowd are publicly shared.", "labels": [], "entities": []}, {"text": "Ina tweet, the author can tag the short text with some hashtags such as #excited, #happy, #UnbornLivesMatter, and #Hillary4President to express their emotion or opinion.", "labels": [], "entities": []}, {"text": "The tweets with a certain types of hashtags are collected as self-label data in a variety of research works including sentiment analysis, stance detection (, financial opinion mining (, and irony detection ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.9682901501655579}, {"text": "stance detection", "start_pos": 138, "end_pos": 154, "type": "TASK", "confidence": 0.8978682458400726}, {"text": "financial opinion mining", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.6426528990268707}, {"text": "irony detection", "start_pos": 190, "end_pos": 205, "type": "TASK", "confidence": 0.8311337828636169}]}, {"text": "In the case of irony detection, it is impractical to manually annotate the ironic sentences from randomly sampled data due to the relatively low occurrences of irony (.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.9370417296886444}]}, {"text": "Collecting the tweets with the hashtags like #sarcasm, #irony, and #not becomes the mainstream approach to dataset construction ().", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 107, "end_pos": 127, "type": "TASK", "confidence": 0.6929738521575928}]}, {"text": "As shown in (S1), the tweet with the hashtag #not is treated as a positive (ironic) instance by removing #not from the text.", "labels": [], "entities": []}, {"text": "(S1) @Anonymous doing a great job...", "labels": [], "entities": [{"text": "Anonymous", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.9456078410148621}]}, {"text": "#not What do I pay my extortionate council taxes for?", "labels": [], "entities": []}, {"text": "#Disgrace #Ongo-ingProblem http://t.co/FQZUUwKSoN However, the reliability of the self-labeled data is an important issue.", "labels": [], "entities": [{"text": "FQZUUwKSoN", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.825660228729248}, {"text": "reliability", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.982299268245697}]}, {"text": "As pointed out in the pioneering work, not all tweet writers know the definition of irony (Van Hee et al., 2016b).", "labels": [], "entities": []}, {"text": "For instance, (S2) is tagged with #irony by the writer, but it is just witty and amusing.", "labels": [], "entities": []}, {"text": "(S2) BestProAdvice @Anonymous More clean OR cleaner, nevermore cleaner.", "labels": [], "entities": []}, {"text": "#irony When the false-alarm instances like (S2) are collected and mixed in the training and test data, the models that learn from the unreliable data maybe misled, and the evaluation is also suspicious.", "labels": [], "entities": [{"text": "irony", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9921018481254578}]}, {"text": "The other kind of unreliable data comes from the hashtags not only functioning as metadata.", "labels": [], "entities": []}, {"text": "That is, a hashtag in a tweet may also function as a content word in its word form.", "labels": [], "entities": []}, {"text": "For example, the hashtag #irony in (S3) is apart of the sentence \"the irony of taking a break...\", in contrast to the hashtag #not in (S1), which can be removed without a change of meaning.", "labels": [], "entities": [{"text": "irony", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.928735077381134}]}, {"text": "(S3) The #irony of taking a break from reading about #socialmedia to check my social media.", "labels": [], "entities": [{"text": "irony", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9606693387031555}]}, {"text": "When the hashtag plays as a content word in a tweet, the tweet is not a good candidate of selflabeled ironic instances because the sentence will be incomplete once the hashtag is removed.", "labels": [], "entities": []}, {"text": "In this work, both kinds of unreliable data, the tweets with a misused hashtag and the tweets in which the hashtag serves as a content word, are our targets to remove from the training data.", "labels": [], "entities": []}, {"text": "Manual data cleaning is labor-intensive and inefficient).", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.7026954889297485}]}, {"text": "Compared to general training data cleaning approaches () such as boostingbased learning, this work leverages the characteristics of hashtag usages in tweets.", "labels": [], "entities": []}, {"text": "With small amount of golden labeled data, we propose a neural network classifier for pruning the self-labeled tweets, and train an ironic detector on the less but cleaner instances.", "labels": [], "entities": []}, {"text": "This approach is easily to apply to other NLP tasks that rely on self-labeled data.", "labels": [], "entities": []}, {"text": "The contributions of this work are three-fold: (1) We make an empirically study on an issue that is potentially inherited in a number of research topics based on self-labeled data.", "labels": [], "entities": []}, {"text": "(2) We propose a model for hashtag disambiguation.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8927047550678253}]}, {"text": "For this task, the human-verified ground-truth is quite limited.", "labels": [], "entities": []}, {"text": "To address the issue of sparsity, a novel neural network model for hashtag disambiguation is proposed.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7766332924365997}]}, {"text": "(3) The data pruning method, in which our model is applied to select reliable self-labeled data, is capable of improving the performance of irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 140, "end_pos": 155, "type": "TASK", "confidence": 0.9022193253040314}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes how we construct a dataset for disambiguating false-alarm hashtag usages based on Tweets.", "labels": [], "entities": []}, {"text": "In Section 3, our model for hashtag disambiguation is proposed.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.8671441674232483}]}, {"text": "Experimental results of hashtag disambiguation are shown in Section 4.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.9144248068332672}]}, {"text": "In addition, we apply our method to prune training data for irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.9600385427474976}]}, {"text": "The results are shown in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "The tweets with indication hashtags such as #irony are usually collected as a dataset in previous works on irony detection.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 107, "end_pos": 122, "type": "TASK", "confidence": 0.8089889585971832}]}, {"text": "As pointed out in Section 1, the hashtags are treated as ground-truth for training and testing.", "labels": [], "entities": []}, {"text": "To investigate the issue of false-alarm  self-labeled tweets, the tweets with human verification are indispensable.", "labels": [], "entities": []}, {"text": "In this study, we build the ground-truth based on the dataset released for SemEval 2018 Task 3, 1 which is targeted for finegrained irony detection ().", "labels": [], "entities": [{"text": "SemEval 2018 Task 3", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7971853017807007}, {"text": "finegrained irony detection", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.616401344537735}]}, {"text": "In the SemEval dataset, the tweets with one of the three indication hashtags #not, #sarcasm, and #irony, are collected and human-annotated as one of four types: verbal irony by means of a polarity contrast, other verbal irony, situational irony, and non-ironic.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7612140476703644}]}, {"text": "In other words, the false-alarm tweets, i.e., the non-ironic tweets with indication hashtags, are distinguished from the real ironic tweets in this dataset.", "labels": [], "entities": []}, {"text": "However, the hashtag itself has been removed in the SemEval dataset.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.8939697742462158}]}, {"text": "For example, the original tweet (S1) has been modified to (S4), where the hashtag #not disappears.", "labels": [], "entities": []}, {"text": "As a result, the hashtag information, the position and the word form of the hashtag (i.e., not, irony, or sarcasm), is missing from the SemEval dataset.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 136, "end_pos": 151, "type": "DATASET", "confidence": 0.7607327997684479}]}, {"text": "For hashtag disambiguation, the information of the hashtag in each tweet is mandatory.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.802202433347702}]}, {"text": "Thus, we recover the original tweets by using Twitter search.", "labels": [], "entities": []}, {"text": "As shown in, a total of 1,359 tweets with hashtags information are adopted as the ground-truth.", "labels": [], "entities": []}, {"text": "Note that more than 20% of selflabeled data are false-alarm, and this can bean issue when they are adopted as training or test data.", "labels": [], "entities": []}, {"text": "For performing the experiment of irony detection in Section 5, we reserve the other 1,072 tweets in the SemEval dataset that are annotated as real ironic as the test data.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.8537109792232513}, {"text": "SemEval dataset", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.8902989327907562}]}, {"text": "In addition to the issue of hashtag disambiguation, the irony tweets without an indication hashtag, which are regarded as non-irony instances in previous work, are another kind of misleading data for irony detection.", "labels": [], "entities": [{"text": "hashtag disambiguation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7137733101844788}, {"text": "irony detection", "start_pos": 200, "end_pos": 215, "type": "TASK", "confidence": 0.8537923693656921}]}, {"text": "Fortunately, the occurrence of such \"false-negative\" instances is insignificant due to the relatively low occurrence of irony ().", "labels": [], "entities": [{"text": "occurrence of irony", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.8375515143076578}]}, {"text": "We compare our model with popular neural network-based sentence classifiers including CNN, GRU, and attentive GRU.", "labels": [], "entities": []}, {"text": "We also train a logistic regression (LR) classifier with the handcrafted features introduced in Section 3.", "labels": [], "entities": [{"text": "logistic regression (LR) classifier", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.6360325415929159}]}, {"text": "For the imbalance data, we assign class-weights inversely proportional to class frequencies.", "labels": [], "entities": []}, {"text": "Early-stop is employed with a patience of 5 epoches.", "labels": [], "entities": [{"text": "Early-stop", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9345277547836304}, {"text": "patience", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9806299805641174}]}, {"text": "In each fold, we further keep 10% of training data for tuning the model.", "labels": [], "entities": []}, {"text": "The hidden dimension is 50, the batch size is 32, and the Adam optimizer is employed.", "labels": [], "entities": []}, {"text": "shows the experimental results reported in Precision (P), Recall (R), and F-score (F).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9562160074710846}, {"text": "Recall (R)", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9709291309118271}, {"text": "F-score (F)", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9634794592857361}]}, {"text": "Our goal is to select the real ironic tweets for training the irony detection model.", "labels": [], "entities": [{"text": "irony detection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.8336252570152283}]}, {"text": "Thus, the real ironic tweets are regarded as positive, and the falsealarm ones are negative.", "labels": [], "entities": []}, {"text": "We apply t-test for significance testing.", "labels": [], "entities": []}, {"text": "The vanilla GRU and attentive GRU are slightly superior to the logistic regression model.", "labels": [], "entities": [{"text": "attentive GRU", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.7014839053153992}]}, {"text": "The CNN model performs the worst in this task because it suffers from over-fitting problem.", "labels": [], "entities": []}, {"text": "We explored a number of layouts and hyperparameters for the CNN model, and consistent results are observed.", "labels": [], "entities": []}, {"text": "Our method is evaluated with either CNN, GRU, or attentive GRU for encoding the context preceding and following the targeting hashtag.", "labels": [], "entities": [{"text": "CNN", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9322766065597534}, {"text": "GRU", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8464292883872986}]}, {"text": "By integrating various kinds of information, our method outperforms all baseline models no matter which encoder is used.", "labels": [], "entities": []}, {"text": "The best model is the one integrating the attentive GRU encoder, which is significantly superior to all baseline models (p < 0.05), achieves an F-score of 88.49%, To confirm the effectiveness of the language modeling of POS sequence, we also try to exclude the GRU language model from our best model.", "labels": [], "entities": [{"text": "F-score", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9994720816612244}]}, {"text": "Experimental results show that the addition of language model significantly improves the perfor-  mance (p < 0.05).", "labels": [], "entities": [{"text": "perfor-  mance", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.9538997610410055}]}, {"text": "As shown in the last row of, the F-score is dropped to 84.17%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9992251396179199}]}, {"text": "From the data, we observe that the instances whose p \u00af s p\u00af h usually contain a indication hashtag function as a content word, and vice versa.", "labels": [], "entities": []}, {"text": "For instances, (S5) and (S6) show the instances with the highest and the lowest p\u00af s p\u00af h , respectively.", "labels": [], "entities": []}, {"text": "(S5) when your #sarcasm is so advanced people actually think you are #stupid ..", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the Ground-Truth Data.", "labels": [], "entities": []}, {"text": " Table 2: Results of Hashtag Disambiguation.", "labels": [], "entities": [{"text": "Hashtag Disambiguation", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8274655938148499}]}, {"text": " Table 3: Performance of Irony Detection.", "labels": [], "entities": [{"text": "Irony Detection", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6977216005325317}]}]}