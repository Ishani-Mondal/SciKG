{"title": [{"text": "A Multi-task Approach to Learning Multilingual Representations", "labels": [], "entities": [{"text": "Learning Multilingual Representations", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.5785981118679047}]}], "abstractContent": [{"text": "We present a novel multi-task model-ing approach to learning multilingual distributed representations of text.", "labels": [], "entities": []}, {"text": "Our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model.", "labels": [], "entities": []}, {"text": "Our architecture can transparently use both monolin-gual and sentence aligned bilingual corpora to learn multilingual embeddings, thus covering a vocabulary significantly larger than the vocabulary of the bilingual corpora alone.", "labels": [], "entities": []}, {"text": "Our model shows competitive performance in a standard cross-lingual document classification task.", "labels": [], "entities": [{"text": "cross-lingual document classification task", "start_pos": 54, "end_pos": 96, "type": "TASK", "confidence": 0.7097017243504524}]}, {"text": "We also show the effectiveness of our method in a limited resource scenario.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning distributed representations of text, whether it beat the level of words, phrases, sentences or documents has been one of the most widely researched subjects in natural language processing in recent years (.", "labels": [], "entities": []}, {"text": "Word/sentence/document embeddings, as they are now commonly referred to, have quickly become essential ingredients of larger and more complex NLP systems looking to leverage the rich semantic and linguistic information present in distributed representations (.", "labels": [], "entities": []}, {"text": "Research that has been taking place in the context of distributed text representations is learning multilingual text representations shared across languages.", "labels": [], "entities": []}, {"text": "Multilingual embeddings open up the possibility of transferring knowledge across languages and building complex systems even for languages with limited amount of supervised resources.", "labels": [], "entities": []}, {"text": "By far the most popular approach to learning multilingual embeddings is to train a multilingual word embedding model that is then used to derive representations for sentences and documents by composition ().", "labels": [], "entities": []}, {"text": "These models are typically trained solely on word or sentence aligned corpora and the composition models are usually simple predefined functions like averages over word embeddings ( or parametric composition models learned along with the word embeddings ( . For a thorough survey of cross-lingual text embedding models, please refer to.", "labels": [], "entities": []}, {"text": "In this work we learn word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model.", "labels": [], "entities": []}, {"text": "Our multilingual skip-gram model is similar to (.", "labels": [], "entities": []}, {"text": "It transparently consumes (word, context) pairs constructed from monolingual as well as sentence aligned bilingual corpora.", "labels": [], "entities": []}, {"text": "We process word embeddings with a bidirectional LSTM and then take an average of the LSTM outputs, which can be viewed as context dependent word embeddings, to produce sentence embeddings.", "labels": [], "entities": []}, {"text": "Since our multilingual skipgram and cross-lingual sentence similarity models are trained jointly, they can inform each other through the shared word embedding layer and promote the compositionality of learned word embeddings at training time.", "labels": [], "entities": []}, {"text": "Further, the gradients flowing back from the sentence similarity model can affect the embeddings learned for words outside the vocabulary of the parallel corpora.", "labels": [], "entities": []}, {"text": "We hypothesize these two aspects of approach lead to more robust sentence embeddings.", "labels": [], "entities": []}, {"text": "The main motivation behind our approach is to learn high quality multilingual sentence and document embeddings in the low resource scenario where parallel corpus sizes are limited.", "labels": [], "entities": []}, {"text": "The main novelty of our approach is the joint training of multilingual skip-gram and cross-lingual sentence similarity objectives with a shared word embedding layer which allows the gradients from the sentence similarity task to affect the embeddings learned for words outside the vocabulary of the parallel corpora.", "labels": [], "entities": []}, {"text": "By jointly training these two objectives, we can transparently use monolingual and parallel data for learning multilingual sentence embeddings.", "labels": [], "entities": []}, {"text": "Using a BiLSTM layer to contextualize word embeddings prior to averaging is orthogonal to the joint multi-task learning idea.", "labels": [], "entities": []}, {"text": "We observed that this additional layer is beneficial inmost settings and this is consistent with the observations of recent works on learning sentence and document embeddings such as", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results for models trained on en-de  language pair. *no-mono means no monolin- gual data was used in training. We compare our  models to: BiCVM-add+ (", "labels": [], "entities": []}, {"text": " Table 2: Sent-LSTM vs. JMT-Sent-LSTM at dif- ferent data conditions (en-de, dim=128).", "labels": [], "entities": [{"text": "JMT-Sent-LSTM", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.837763249874115}]}, {"text": " Table 3: Multilingual vs. bilingual* models  (dim=128).", "labels": [], "entities": []}]}