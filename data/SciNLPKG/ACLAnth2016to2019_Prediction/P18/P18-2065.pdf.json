{"title": [], "abstractContent": [{"text": "Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation.", "labels": [], "entities": [{"text": "Conventional Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.758150938898325}, {"text": "syntactic parsing", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7310721278190613}]}, {"text": "In this paper, we propose a neural Open IE approach with an encoder-decoder framework.", "labels": [], "entities": []}, {"text": "Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system.", "labels": [], "entities": []}, {"text": "An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (Open IE) involves generating a structured representation of information in text, usually in the form of triples or n-ary propositions.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7908987019743238}]}, {"text": "An Open IE system not only extracts arguments but also relation phrases from the given text, which does not rely on pre-defined ontology schema.", "labels": [], "entities": []}, {"text": "For instance, given the sentence \"deep learning is a subfield of machine learning\", the triple (deep learning; is a subfield of ; machine learning) can be extracted, where the relation phrase \"is a subfield of \" indicates the semantic relationship between two arguments.", "labels": [], "entities": []}, {"text": "Open IE plays a key role in natural language understanding and fosters many downstream NLP applications such as knowledge base construction, question answering, text comprehension, and others.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.5285248756408691}, {"text": "natural language understanding", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6485256850719452}, {"text": "knowledge base construction", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.6285543044408163}, {"text": "question answering", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.9099614322185516}]}, {"text": "The Open IE system was first introduced by TEXTRUNNER (), followed by several popular systems such as REVERB and OPENIE5 2 . Although these systems have been widely used in a variety of applications, most of them were built on hand-crafted patterns from syntactic parsing, which causes errors in propagation and compounding at each stage (.", "labels": [], "entities": [{"text": "REVERB", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9589903354644775}]}, {"text": "Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples.", "labels": [], "entities": []}, {"text": "To this end, we propose a neural Open IE approach with an encoder-decoder framework.", "labels": [], "entities": []}, {"text": "The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation, image caption ( ), abstractive summarization ( and recently keyphrase extraction (.", "labels": [], "entities": [{"text": "text generation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.716891810297966}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8150493204593658}, {"text": "image caption", "start_pos": 139, "end_pos": 152, "type": "TASK", "confidence": 0.6877078413963318}, {"text": "abstractive summarization", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.6045154929161072}, {"text": "keyphrase extraction", "start_pos": 199, "end_pos": 219, "type": "TASK", "confidence": 0.7700891494750977}]}, {"text": "Generally, the encoder encodes the input sequence to an internal representation called 'context vector' which is used by the decoder to generate the output sequence.", "labels": [], "entities": []}, {"text": "The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences.", "labels": [], "entities": []}, {"text": "In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders.", "labels": [], "entities": [{"text": "Open IE", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.6761009097099304}, {"text": "sequence-to-sequence generation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.7535712122917175}]}, {"text": "For instance, given the input sequence \"deep learning is a subfield of machine learning\", the output sequence will be \"arg1 deep learning /arg1 rel is a subfield of /rel arg2 machine: The encoder-decoder model architecture for the neural Open IE system learning /arg2\".", "labels": [], "entities": []}, {"text": "We obtain the input and output sequence pairs from highly confident tuples bootstrapped from a state-of-the-art Open IE system.", "labels": [], "entities": []}, {"text": "Experiment results on a large benchmark dataset illustrate that the neural Open IE approach is significantly better than others in precision and recall, while also reducing the dependencies on other NLP tools.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9994297623634338}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9985639452934265}]}, {"text": "The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, the encoder-decoder framework learns the sequence-to-sequence task directly, bypassing other hand-crafted patterns and alleviating error propagation.", "labels": [], "entities": []}, {"text": "Second, a large number of highquality training examples can be bootstrapped from state-of-the-art Open IE systems, which is released for future research.", "labels": [], "entities": []}, {"text": "Third, we conduct comprehensive experiments on a large benchmark dataset to compare different Open IE systems to show the neural approach's promising potential.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}