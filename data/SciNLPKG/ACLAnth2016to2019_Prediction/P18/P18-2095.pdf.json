{"title": [{"text": "Will it Blend? Blending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "The process of obtaining high quality labeled data for natural language understanding tasks is often slow, error-prone, complicated and expensive.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.7101350650191307}]}, {"text": "With the vast usage of neural networks, this issue becomes more notorious since these networks require a large amount of labeled data to produce satisfactory results.", "labels": [], "entities": []}, {"text": "We propose a methodology to blend high quality but scarce labeled data with noisy but abundant weak labeled data during the training of neural networks.", "labels": [], "entities": []}, {"text": "Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme.", "labels": [], "entities": [{"text": "topic-dependent evidence detection", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.6454243361949921}]}, {"text": "In addition, we provide a manually annotated data set for the task of topic-dependent evidence detection.", "labels": [], "entities": [{"text": "topic-dependent evidence detection", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.6781915823618571}]}], "introductionContent": [{"text": "In recent years, neural networks have been widely used for natural language understanding tasks.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.7296726927161217}]}, {"text": "Such networks demand a considerable amount of labeled data for each specific task.", "labels": [], "entities": []}, {"text": "However, for many tasks, the process of obtaining high quality labeled data is slow, expensive, and complicated (.", "labels": [], "entities": []}, {"text": "In this work, we propose a method for improving network training when a small amount of labeled data is available.", "labels": [], "entities": []}, {"text": "Several works have suggested methods for generating weak labeled data (WLD) whose quality for the task of interest is low, but that can be easily obtained.", "labels": [], "entities": []}, {"text": "One approach for gathering WLD is to apply heuristics to a large corpus.", "labels": [], "entities": [{"text": "gathering WLD", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.6793800443410873}]}, {"text": "For example, Hearst (1992) considered a noun to be the hypernym of another noun if they are connected by the is a pattern in a sentence.", "labels": [], "entities": []}, {"text": "Distant supervision is another form of WLD used in various tasks such as relation extraction and sentiment analysis ().", "labels": [], "entities": [{"text": "WLD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.959793746471405}, {"text": "relation extraction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.9106173813343048}, {"text": "sentiment analysis", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.9476804733276367}]}, {"text": "Other works use emojis or hashtags as weak labels describing the texts in which they appear (e.g., in the context of sarcasm detection).", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.7265653163194656}]}, {"text": "WLD can be freely obtained, however it comes with a price: it is often very noisy.", "labels": [], "entities": [{"text": "WLD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.4687362015247345}]}, {"text": "Therefore, systems trained only on WLD are at a serious disadvantage compared to systems trained on high quality labeled data, which we term henceforth strong labeled data (SLD).", "labels": [], "entities": [{"text": "WLD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9399718642234802}]}, {"text": "However, we suggest that the easily accessible WLD is still useful when used alongside SLD, which is naturally limited in size.", "labels": [], "entities": []}, {"text": "In this work we propose a method for blending WLD and SLD in the training of neural networks.", "labels": [], "entities": [{"text": "WLD", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.8603038787841797}]}, {"text": "Focusing on the argumentation mining field, we create and release a data set for the task of topic-dependent evidence detection.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.960942804813385}, {"text": "topic-dependent evidence detection", "start_pos": 93, "end_pos": 127, "type": "TASK", "confidence": 0.6286316215991974}]}, {"text": "Our evaluation shows that such blending improves the accuracy of the network compared to not using WLD or not blending it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9989829659461975}, {"text": "WLD", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.7680175304412842}]}, {"text": "This improvement is even more evident when SLD is not abundantly available.", "labels": [], "entities": [{"text": "SLD", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9023120403289795}]}, {"text": "We believe that blending WLD and SLD is a general notion that maybe applicable to many language understanding tasks, and can especially assist researchers who wish to train a network but have a small amount of SLD for their task of interest.", "labels": [], "entities": [{"text": "WLD", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9367050528526306}, {"text": "language understanding tasks", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.783690869808197}]}], "datasetContent": [{"text": "We use the data set described in Section 4, training the network on the train set and evaluating its accuracy on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9991274476051331}]}, {"text": "We empirically explore several blending configurations and evaluate their impact on the accuracy of the network.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.998356282711029}]}, {"text": "To validate our assumption that WLD contribution would be more prominent when SLD is limited, we test each configuration with varying sizes of SLD between 500 and 4,000.", "labels": [], "entities": [{"text": "WLD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8642207384109497}, {"text": "SLD", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9057905673980713}]}, {"text": "Following some preliminary exploration, on a different data set, we noticed that the parameter m, the number of initialization epochs, does not make a significant difference, and we set it to be 1 (trying m > 1 resulted in slightly worse accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9989390969276428}]}, {"text": "As mentioned in Section 3.2, our stopping criterion was set to ensure that in any configuration, we have four blending epochs in which the input for the network is mostly SLD, i.e. it is at least 95% of the data seen by the network.", "labels": [], "entities": []}, {"text": "For the blending factor we tried \u03b1 \u2208 {0, 0.05, 0.2}, and quickly learned that choosing a blending factor value larger than 0.05 is typically ineffective.", "labels": [], "entities": []}, {"text": "Since the blending factor determines the numbers of epochs in which the WLD is significant, and since it is reasonable to limit this number due to the noisy nature of the WLD, it is not surprising that a small value of \u03b1 is preferable.", "labels": [], "entities": []}, {"text": "We note that setting \u03b1 = 0 means WLD is only used in the initialization epochs.", "labels": [], "entities": [{"text": "WLD", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.8320421576499939}]}, {"text": "Finally, to keep results reliable, as SLD size can get quite small, we repeat each configuration run five times with different SLD slices to reduce variance.", "labels": [], "entities": []}, {"text": "For each run we record the best accuracy out of all its epochs and report the micro average of the best accuracies of the five runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9990977048873901}, {"text": "accuracies", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9611457586288452}]}, {"text": "Blending WLD throughout several epochs of training (the thick green curve with round dots), improves performance over using it only for initialization, as most previous works do (the dashed red curve), and over not using WLD at all (the blue curve with triangles).", "labels": [], "entities": [{"text": "initialization", "start_pos": 136, "end_pos": 150, "type": "TASK", "confidence": 0.9693796038627625}]}, {"text": "This effect is significantly more notable as we useless SLD.", "labels": [], "entities": [{"text": "SLD", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.7926562428474426}]}, {"text": "For example, in the left plot, which presents the usage of Webis-Debate-16 as WLD, we see that using 1,000 instances of SLD with WLD yields results comparable to using 2,500 SLD instances.", "labels": [], "entities": [{"text": "Webis-Debate-16", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9255679845809937}]}, {"text": "Similarly, 2,000 SLD instances plus WLD, are comparable to using 3,000 SLD instances.", "labels": [], "entities": [{"text": "WLD", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.5574343204498291}]}, {"text": "The effect is smaller when the WLD is based on the \"that + topic concept\" query, but the trend is similar.", "labels": [], "entities": [{"text": "WLD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7070769667625427}]}, {"text": "One may claim that the signal in WLD is stronger than we hypothesized and therefore the performance improves simply because we are adding labeled data for training.", "labels": [], "entities": [{"text": "WLD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.6625223159790039}]}, {"text": "To test this claim we train the network with all available WLD and only it.", "labels": [], "entities": [{"text": "WLD", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.892947793006897}]}, {"text": "The single triangles on the Y-axis of each plot show that the accuracy of the network with such training is much lower than using the entire SLD, reflecting the inferior quality of the WLD.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9995518326759338}]}, {"text": "In addition, we note that the accuracy on the test set of the \"that + topic concept\" query, which was used to collect one of our WLD types, is only 17%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.999542236328125}]}, {"text": "Another claim maybe that just by utilizing WLD in addition to SLD the accuracy improves, and that there is no need for any blending method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9997323155403137}]}, {"text": "To answer that, we unify the WLD and the SLD, without applying any blending method (single squares on the right border of each plot).", "labels": [], "entities": [{"text": "WLD", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.7059199810028076}]}, {"text": "For the WLD constructed by the \"that + topic concept\" query the accuracy is well below the accuracy achieved when using SLD alone, as can be seen in the right plot.", "labels": [], "entities": [{"text": "WLD", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.898123025894165}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996203184127808}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9991650581359863}]}, {"text": "On the left plot, we see that unifying the WLD with the SLD does not help nor harm compared to using the SLD alone.", "labels": [], "entities": [{"text": "WLD", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5954140424728394}]}, {"text": "We conclude that even though WLD is not nearly as accurate as SLD, it has the potential to improve performance, if blended correctly.", "labels": [], "entities": [{"text": "WLD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9140853881835938}]}, {"text": "We also tried gradually increasing the amount of WLD in each blending epoch, instead of decreasing it.", "labels": [], "entities": [{"text": "WLD", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.5600903034210205}]}, {"text": "We tested several increasing factors on both types of WLD.", "labels": [], "entities": [{"text": "WLD", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9211388826370239}]}, {"text": "Results were similar to the proposed blending method.", "labels": [], "entities": []}], "tableCaptions": []}