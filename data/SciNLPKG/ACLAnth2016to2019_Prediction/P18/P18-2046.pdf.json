{"title": [{"text": "Injecting Relational Structural Representation in Neural Networks for Question Similarity", "labels": [], "entities": [{"text": "Injecting Relational Structural Representation", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8719796538352966}, {"text": "Question Similarity", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7381298243999481}]}], "abstractContent": [{"text": "Effectively using full syntactic parsing information in Neural Networks (NNs) to solve relational tasks, e.g., question similarity , is still an open problem.", "labels": [], "entities": [{"text": "question similarity", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7859281301498413}]}, {"text": "In this paper , we propose to inject structural representations in NNs by (i) learning an SVM model using Tree Kernels (TKs) on relatively few pairs of questions (few thousands) as gold standard (GS) training data is typically scarce, (ii) predicting labels on a very large corpus of question pairs, and (iii) pre-training NNs on such large corpus.", "labels": [], "entities": []}, {"text": "The results on Quora and SemEval question similarity datasets show that NNs trained with our approach can learn more accurate models, especially after fine tuning on GS.", "labels": [], "entities": [{"text": "Quora and SemEval question similarity datasets", "start_pos": 15, "end_pos": 61, "type": "DATASET", "confidence": 0.6956086456775665}]}], "introductionContent": [{"text": "Recent years have seen an exponential growth and use of web forums, where users can exchange and find information just asking questions in natural language.", "labels": [], "entities": []}, {"text": "Clearly, the possibility of reusing previously asked questions makes forums much more useful.", "labels": [], "entities": []}, {"text": "Thus, many tasks have been proposed to build automatic systems for detecting duplicate questions.", "labels": [], "entities": [{"text": "detecting duplicate questions", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.8387255271275839}]}, {"text": "These were both organized in academia, e.g.,, or companies, e.g., Quora . An interesting outcome of the SemEval challenge was that syntactic information is essential to achieve high accuracy in question reranking tasks.", "labels": [], "entities": [{"text": "Quora", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9652541875839233}, {"text": "SemEval challenge", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.8757263720035553}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9901099801063538}, {"text": "question reranking tasks", "start_pos": 194, "end_pos": 218, "type": "TASK", "confidence": 0.7462937931219736}]}, {"text": "Indeed, the top-systems were built using Support Vector Machines (SVMs) trained with Tree Kernels (TKs), which were applied to a syntactic representation of question text ().", "labels": [], "entities": []}, {"text": "In contrast, NNs-based models struggled to get good accuracy as (i) large training sets are typically not available 2 , and (ii) effectively exploiting full-syntactic parse information in NNs is still an open issue.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9987055063247681}]}, {"text": "Indeed, despite showed that NNs are very effective to manage lexical variability, no neural model encoding syntactic information has shown a clear improvement.", "labels": [], "entities": []}, {"text": "Indeed, also NNs directly exploiting syntactic information, such as the Recursive Neural Networks by or the Tree-LSTM by, have been shown to be outperformed by well-trained sequential models (.", "labels": [], "entities": []}, {"text": "Finally, such tree-based approaches depend on sentence structure, thus are difficult to optimize and parallelize.", "labels": [], "entities": []}, {"text": "This is a shame as NNs are very flexible in general and enable an easy system deployment in real applications, while TK models require syntactic parsing and longer testing time.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach that aims at injecting syntactic information in NNs, still keeping them simple.", "labels": [], "entities": []}, {"text": "It consists of the following steps: (i) train a TK-based model on a few thousands training examples; (ii) apply such classifier to a much larger set of unlabeled training examples to generate automatic annotation; (iii) pretrain NNs on the automatic data; and (iv) fine-tune NNs on the smaller GS data.", "labels": [], "entities": [{"text": "GS data", "start_pos": 294, "end_pos": 301, "type": "DATASET", "confidence": 0.8697499632835388}]}, {"text": "Our experiments on two different datasets, i.e., Quora and Qatar Living (QL) from SemEval, show that (i) when NNs are pre-trained on the predicted data, they achieve accuracy higher than the one of TK models and (ii) NNs can be further boosted by fine-tuning them on the available GS data.", "labels": [], "entities": [{"text": "Quora", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9417136311531067}, {"text": "Qatar Living (QL) from SemEval", "start_pos": 59, "end_pos": 89, "type": "DATASET", "confidence": 0.9048400861876351}, {"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9988825917243958}, {"text": "GS data", "start_pos": 281, "end_pos": 288, "type": "DATASET", "confidence": 0.9062624573707581}]}, {"text": "This suggests that the TK properties are captured by NNs, which can exploit syntactic information even more effectively, thanks to their wellknown generalization ability.", "labels": [], "entities": []}, {"text": "In contrast to other semi-supervised approaches, e.g., self-training, we show that the improvement of our approach is obtained only when a very different classifier, i.e., TK-based, is used to label a large portion of the data.", "labels": [], "entities": []}, {"text": "Indeed, using the same NNs in a self-training fashion (or another NN in a co-training approach) to label the semi-supervised data does not provide any improvement.", "labels": [], "entities": []}, {"text": "Similarly, when SVMs using standard similarity lexical features are applied to label data, no improvement is observed in NNs.", "labels": [], "entities": []}, {"text": "One evident consideration is the fact that TKsbased models mainly exploit syntactic information to classify data.", "labels": [], "entities": []}, {"text": "Although, assessing that NNs specifically learn such syntax should require further investigation, our results show that only the transfer from TKs produces improvement: this is a significant evidence that makes it worth to further investigate the main claim of our paper.", "labels": [], "entities": []}, {"text": "In any case, our approach increases the accuracy of NNs, when small datasets are available to learn highlevel semantic task such as question similarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993416666984558}]}, {"text": "It consists in (i) using heavier syntactic/semantic models, e.g., based on TKs, to produce training data; and (ii) exploit the latter to learn a neural model, which can then be fine-tuned on the small available GS data.", "labels": [], "entities": [{"text": "GS data", "start_pos": 211, "end_pos": 218, "type": "DATASET", "confidence": 0.7870295345783234}]}], "datasetContent": [{"text": "We experiment with two datasets comparing models trained on gold and automatic data and their combination, before and after fine tuning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy on the Quora dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981822967529297}, {"text": "Quora dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9748237133026123}]}]}