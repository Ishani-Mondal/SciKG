{"title": [{"text": "Automated essay scoring with string kernels and word embeddings", "labels": [], "entities": [{"text": "Automated essay scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6552262703577677}]}], "abstractContent": [{"text": "In this work, we present an approach based on combining string kernels and word em-beddings for automatic essay scoring.", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.6929896473884583}]}, {"text": "String kernels capture the similarity among strings based on counting common character n-grams, which area low-level yet powerful type of feature, demonstrating state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.7203617691993713}, {"text": "Arabic dialect identification", "start_pos": 231, "end_pos": 260, "type": "TASK", "confidence": 0.6061404148737589}, {"text": "native language identification", "start_pos": 264, "end_pos": 294, "type": "TASK", "confidence": 0.6523955166339874}]}, {"text": "To our best knowledge , we are the first to apply string kernels to automatically score essays.", "labels": [], "entities": []}, {"text": "We are also the first to combine them with a high-level semantic feature representation, namely the bag-of-super-word-embeddings.", "labels": [], "entities": []}, {"text": "We report the best performance on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings, surpassing recent state-of-the-art deep learning approaches .", "labels": [], "entities": [{"text": "Automated Student Assessment Prize data set", "start_pos": 38, "end_pos": 81, "type": "DATASET", "confidence": 0.7166091203689575}]}], "introductionContent": [{"text": "Automatic essay scoring (AES) is the task of assigning grades to essays written in an educational setting, using a computer-based system with natural language processing capabilities.", "labels": [], "entities": [{"text": "Automatic essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7392025291919708}]}, {"text": "The aim of designing such systems is to reduce the involvement of human graders as far as possible.", "labels": [], "entities": []}, {"text": "AES is a challenging task as it relies on grammar as well as semantics, pragmatics and discourse (.", "labels": [], "entities": [{"text": "AES", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7345476746559143}]}, {"text": "Although traditional AES methods typically rely on handcrafted features;, recent results indicate that state-of-the-art deep learning methods reach better performance (, perhaps because these methods are able to capture subtle and complex information that is relevant to the task (.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine string kernels (low-level character n-gram features) and word embeddings (high-level semantic features) to obtain state-of-the-art AES results.", "labels": [], "entities": []}, {"text": "Since recent methods based on string kernels have demonstrated remarkable performance in various text classification tasks ranging from authorship identification ( and sentiment analysis () to native language identification ( and dialect identification ( , we believe that string kernels can reach equally good results in AES.", "labels": [], "entities": [{"text": "text classification", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7402247786521912}, {"text": "authorship identification", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.7857995629310608}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7776823937892914}, {"text": "native language identification", "start_pos": 193, "end_pos": 223, "type": "TASK", "confidence": 0.647241860628128}, {"text": "dialect identification", "start_pos": 230, "end_pos": 252, "type": "TASK", "confidence": 0.7191035598516464}]}, {"text": "To the best of our knowledge, string kernels have never been used for this task.", "labels": [], "entities": []}, {"text": "As string kernels area simple approach that relies solely on character n-grams as features, it is fairly obvious that such an approach will not to cover several aspects (e.g.: semantics, discourse) required for the AES task.", "labels": [], "entities": [{"text": "AES task", "start_pos": 215, "end_pos": 223, "type": "TASK", "confidence": 0.6387612521648407}]}, {"text": "To solve this problem, we propose to combine string kernels with a recent approach based on word embeddings, namely the bag-of-super-word-embeddings (BOSWE) ( . To our knowledge, this is the first successful attempt to combine string kernels and word embeddings.", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 150, "end_pos": 155, "type": "METRIC", "confidence": 0.8754917979240417}]}, {"text": "We evaluate our approach on the Automated Student Assessment Prize data set, in both in-domain and cross-domain settings.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize data set", "start_pos": 32, "end_pos": 75, "type": "DATASET", "confidence": 0.7630875358978907}]}, {"text": "The empirical results indicate that our approach yields a better performance than state-of-the-art approaches ().", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our approach, we use the Automated Student Assessment Prize (ASAP) 1 data set from Kaggle.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) 1 data set from Kaggle", "start_pos": 37, "end_pos": 101, "type": "DATASET", "confidence": 0.73959053059419}]}, {"text": "The ASAP data set contains 8 prompts of different genres.", "labels": [], "entities": [{"text": "ASAP data set", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9162732561429342}]}, {"text": "The number of essays per prompt along with the score ranges are presented in.", "labels": [], "entities": []}, {"text": "Since the official test data of the ASAP competition is not released to the public, we, as well as others before us (, use only the training data in our experiments.", "labels": [], "entities": [{"text": "ASAP competition", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8392812609672546}]}, {"text": "As Dong and Zhang (2016), we scaled the essay scores into the range 1 https://www.kaggle.com/c/asap-aes/data 0-1.", "labels": [], "entities": []}, {"text": "We closely followed the same settings for data preparation as (.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7771811187267303}]}, {"text": "For the in-domain experiments, we use 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "The 5-fold crossvalidation procedure is repeated for 10 times and the results were averaged to reduce the accuracy variation introduced by randomly selecting the folds.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9989081621170044}]}, {"text": "We note that the standard deviation in all cases in below 0.2%.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 17, "end_pos": 35, "type": "METRIC", "confidence": 0.9314264357089996}]}, {"text": "For the cross-domain experiments, we use the same source\u2192target domain pairs as (, namely, 1\u21922, 3\u21924, 5\u21926 and 7\u21928.", "labels": [], "entities": []}, {"text": "All essays in the source domain are used as training data.", "labels": [], "entities": []}, {"text": "Target domain samples are randomly divided into 5 folds, where one fold is used as test data, and the other 4 folds are collected together to sub-sample target domain train data.", "labels": [], "entities": []}, {"text": "The sub-sample sizes are n t = {10, 25, 50, 100}.", "labels": [], "entities": []}, {"text": "The sub-sampling is repeated for 5 times as in ( to reduce bias.", "labels": [], "entities": []}, {"text": "As our approach performs very well in the cross-domain setting, we also present experiments without sub-sampling data from the target domain, i.e. when the subsample size is n t = 0.", "labels": [], "entities": []}, {"text": "As evaluation metric, we use the quadratic weighted kappa (QWK).", "labels": [], "entities": [{"text": "quadratic weighted kappa (QWK)", "start_pos": 33, "end_pos": 63, "type": "METRIC", "confidence": 0.7507009208202362}]}, {"text": "We compare our approach with stateof-the-art methods based on handcrafted features (, as well as deep features).", "labels": [], "entities": []}, {"text": "We note that results for the crossdomain setting are reported only in some of these recent works (.", "labels": [], "entities": []}, {"text": "For the string kernels approach, we used the histogram intersection string kernel (HISK) based on the blended range of character n-grams from 1 to 15.", "labels": [], "entities": []}, {"text": "To compute the intersection string kernel, we used the open-source code provided by.", "labels": [], "entities": []}, {"text": "For the BOSWE approach, we used the pre-trained word embeddings computed by the word2vec toolkit () on the Google News data set using the Skip-gram model, which produces 300-dimensional vectors for 3 million words and phrases.", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.6603829860687256}, {"text": "Google News data set", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.8823786675930023}]}, {"text": "We used functions from the VLFeat library ( for the other steps involved in the BOSWE approach, such as the k-means clustering and the randomized forest of k-d trees.", "labels": [], "entities": [{"text": "VLFeat library", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8891672790050507}, {"text": "BOSWE", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.7084690928459167}]}, {"text": "We set the number of clusters (dimension of the vocabulary) to k = 500.", "labels": [], "entities": []}, {"text": "After: In-domain automatic essay scoring results of our approach versus several state-of-the-art methods (.", "labels": [], "entities": []}, {"text": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using 5-fold cross-validation.", "labels": [], "entities": [{"text": "quadratic weighted kappa (QWK) measure", "start_pos": 37, "end_pos": 75, "type": "METRIC", "confidence": 0.8655514631952558}]}, {"text": "The best QWK score (among the machine learning systems) for each prompt is highlighted in bold.", "labels": [], "entities": []}, {"text": "computing the BOSWE representation, we apply the L 1 -normalized intersection kernel.", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.9478037357330322}]}, {"text": "We combine HISK and BOSWE in the dual form by summing up the two corresponding matrices.", "labels": [], "entities": [{"text": "HISK", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8977205753326416}, {"text": "BOSWE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.9939670562744141}]}, {"text": "For the learning phase, we employ the dual implementation of \u03bd-SVR available in LibSVM (Chang and Lin, 2011).", "labels": [], "entities": []}, {"text": "We set its regularization parameter to c = 10 3 and \u03bd = 10 \u22121 in all our experiments.", "labels": [], "entities": []}, {"text": "The results for the in-domain automatic essay scoring task are presented in Table 2.", "labels": [], "entities": [{"text": "essay scoring task", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7764070133368174}]}, {"text": "In our empirical study, we also include feature ablation results.", "labels": [], "entities": []}, {"text": "We report the QWK measure on each prompt as well as the overall average.", "labels": [], "entities": [{"text": "QWK measure", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.881468266248703}]}, {"text": "We first note that the histogram intersection string kernel alone reaches better overall performance (0.780) than all previous works.", "labels": [], "entities": []}, {"text": "Remarkably, the overall performance of the HISK is also higher than the inter-human agreement.", "labels": [], "entities": []}, {"text": "Although the BOSWE model can be regarded as a shallow approach, its overall results are comparable to those of deep learning approaches ().", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.5974937677383423}]}, {"text": "When we combine the two models (HISK and BOSWE), we obtain even better results.", "labels": [], "entities": [{"text": "HISK", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9299976825714111}, {"text": "BOSWE", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9933652281761169}]}, {"text": "Indeed, the combination of string kernels and word embeddings attains the best performance on 7 out of 8 prompts.", "labels": [], "entities": []}, {"text": "The average QWK score of HISK and BOSWE (0.785) is more than 2% better the average scores of the best-performing state-of-the-art approaches (.", "labels": [], "entities": [{"text": "QWK", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9184879064559937}, {"text": "HISK", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.5964601039886475}, {"text": "BOSWE", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9960511326789856}]}, {"text": "The results for the crossdomain automatic essay scoring task are presented in.", "labels": [], "entities": [{"text": "crossdomain automatic essay scoring task", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.7455740809440613}]}, {"text": "For each and every source\u2192target pair, we report better results than both state-of-the-art methods (.", "labels": [], "entities": []}, {"text": "We observe that the difference between our best QWK scores and the other approaches are sometimes much higher in the cross-domain setting than in the in-domain setting.", "labels": [], "entities": []}, {"text": "We particularly notice that the difference from () when n t = 0 is always higher than 10%.", "labels": [], "entities": []}, {"text": "Our highest improvement (more than 54%, from 0.187 to 0.728) over () is recorded for the pair 5\u21926, when n t = 0.", "labels": [], "entities": []}, {"text": "Our score in this case (0.728) is even higher than both scores of and when they use n t = 50.", "labels": [], "entities": []}, {"text": "Different from the in-domain setting, we note that the combination of string kernels and word embeddings does not always provide better results than string kernels alone, particularly when the number of target samples (n t ) added into the training set is lessor equal to 25.", "labels": [], "entities": []}, {"text": "It is worth noting that in a set of preliminary experiments (not included in the paper), we actually considered another approach based on word embeddings.", "labels": [], "entities": []}, {"text": "We tried to obtain a document embedding by averaging the word vectors for each document.", "labels": [], "entities": []}, {"text": "We computed the average as well as the standard deviation for each component of the word vectors, resulting in a total of 600 features, since the word vectors are 300-dimensional.", "labels": [], "entities": []}, {"text": "We applied this method in the in-domain setting and we obtained a surprisingly low overall QWK score, around 0.251.", "labels": [], "entities": [{"text": "QWK score", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9736723005771637}]}, {"text": "We concluded that this simple approach is not useful, and decided to use BOSWE (Butnaru and Ionescu, 2017) instead.", "labels": [], "entities": [{"text": "BOSWE", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9803547263145447}]}, {"text": "It would have been interesting to present an error analysis based on the discriminant features weighted higher by the \u03bd-SVR method.", "labels": [], "entities": []}, {"text": "Unfortu-Source\u2192Target Method n t = 0 n t = 10 n t = 25 n t = 50 n t = 100: Corss-domain automatic essay scoring results of our approach versus two state-of-the-art methods (.", "labels": [], "entities": []}, {"text": "Results are reported in terms of the quadratic weighted kappa (QWK) measure, using the same evaluation procedure as (.", "labels": [], "entities": [{"text": "quadratic weighted kappa (QWK) measure", "start_pos": 37, "end_pos": 75, "type": "METRIC", "confidence": 0.7881677533899035}]}, {"text": "The best QWK scores for each source\u2192target domain pair are highlighted in bold.", "labels": [], "entities": []}, {"text": "nately, this is not possible because our approach works in the dual space and we cannot transform the dual weights into primal weights, as long as the histogram intersection kernel does not have an explicit embedding map associated to it.", "labels": [], "entities": []}, {"text": "In future work, however, we aim to replace the histogram intersection kernel with the presence bits kernel, which will enable us to perform an error analysis based on the overused or underused patterns, as described by .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of essays and the score  ranges for the 8 different prompts in the Auto- mated Student Assessment Prize (ASAP) data set.", "labels": [], "entities": [{"text": "Auto- mated Student Assessment Prize (ASAP) data set", "start_pos": 88, "end_pos": 140, "type": "DATASET", "confidence": 0.5736767161976207}]}, {"text": " Table 2: In-domain automatic essay scoring results of our approach versus several state-of-the-art meth- ods (", "labels": [], "entities": []}, {"text": " Table 3: Corss-domain automatic essay scoring results of our approach versus two state-of-the-art meth- ods (", "labels": [], "entities": []}]}