{"title": [{"text": "Multi-Input Attention for Unsupervised OCR Correction", "labels": [], "entities": [{"text": "Multi-Input Attention", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.659486398100853}]}], "abstractContent": [{"text": "We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding.", "labels": [], "entities": [{"text": "OCR post-correction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.872737854719162}]}, {"text": "A sequence-to-sequence model with attention is applied for single-input correction , and anew decoder with multi-input attention averaging is developed to search for consensus among multiple sequences.", "labels": [], "entities": []}, {"text": "We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model.", "labels": [], "entities": []}, {"text": "On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.", "labels": [], "entities": [{"text": "word error rates", "start_pos": 116, "end_pos": 132, "type": "METRIC", "confidence": 0.71619779864947}]}], "introductionContent": [{"text": "Optical character recognition (OCR) software has made vast quantities of printed material available for retrieval and analysis, but severe recognition errors in corpora with low quality of printing and scanning or physical deterioration often hamper accessibility ().", "labels": [], "entities": [{"text": "Optical character recognition (OCR)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8884193499883016}]}, {"text": "Many digitization projects have employed manual proofreading to further correct OCR output, but this is time consuming and depends on fostering a community of volunteer workers.", "labels": [], "entities": []}, {"text": "These problems with OCR are exacerbated in library-scale digitization by commercial (e.g., Google Books, Newspapers.com), government (e.g., Library of Congress, Biblioth\u00e8que nationale de France), and nonprofit (e.g., Internet Archive) organizations.", "labels": [], "entities": []}, {"text": "The scale of these projects not only makes it difficult to adapt OCR models to their diverse layouts and typefaces but also makes it impractical to present any OCR output other than a single-best transcript.", "labels": [], "entities": []}, {"text": "Existing methods for automatic OCR postcorrection are mostly supervised methods that correct recognition errors in a single OCR output).", "labels": [], "entities": [{"text": "OCR postcorrection", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8680460155010223}]}, {"text": "Those systems are not scalable since human annotations are expensive to acquire, and they are not capable of utilizing complementary sources of information.", "labels": [], "entities": []}, {"text": "Another line of work is ensemble methods ( combining OCR results from multiple scans of the same document.", "labels": [], "entities": []}, {"text": "Most of these ensemble methods, however, require aligning multiple OCR outputs, which is intractable in general and might introduce noise into the later correction stage.", "labels": [], "entities": []}, {"text": "Furthermore, voting-based ensemble methods only work where the correct output exists in one of the inputs, while classification methods () are also trained on human annotations.", "labels": [], "entities": []}, {"text": "To address these challenges, we propose an unsupervised OCR post-correction framework both to correct single input text sequences and also to exploit multiple candidate texts by simultaneously aligning, correcting, and voting among input sequences.", "labels": [], "entities": []}, {"text": "Our proposed method is based on the observation that significant number of duplicate and near-duplicate documents exist in many corpora (, resulting in OCR output containing repeated texts with various quality.", "labels": [], "entities": []}, {"text": "As shown by the example in, different errors (characters in red) are introduced when the OCR system scans the same text in multiple editions, each with its own layout, fonts, etc.", "labels": [], "entities": []}, {"text": "For ex-ample, in is recognized as min the first output and a is recognized as u in the third output, while the second output is correctly recognized.", "labels": [], "entities": []}, {"text": "Therefore, duplicated texts with diverse errors could serve as complementary information sources for each other.", "labels": [], "entities": []}, {"text": "OCR eor**y that I have been slam in battle, for 1 Output sorry that I have been slain in battle, for I sorry tha' I have been s uin in battle, fr I Original sorry that I have been slain in battle, for I Text In this paper, we aim to train an unsupervised correction model via utilizing the duplication in OCR output.", "labels": [], "entities": []}, {"text": "We propose to map each erroneous OCR'd text unit to either its high-quality duplication or a consensus correction among its duplications via bootstrapping from an uniform error model.", "labels": [], "entities": []}, {"text": "The baseline correction system is a sequence-to-sequence model with attention, which has been shown to be effective in text correction tasks.", "labels": [], "entities": [{"text": "text correction tasks", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.8900055090586344}]}, {"text": "We also seek to improve the correction performance for duplicated texts by integrating multiple inputs.", "labels": [], "entities": []}, {"text": "Previous work on combining multiple inputs in neural translation deal with data from different domains, e.g., multilingual or multimodal (Libovick\u00b4yLibovick\u00b4y and Helcl, 2017) data.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7773962020874023}]}, {"text": "Therefore, their models need to be trained on multiple inputs to learn parameters to combine inputs from each domain.", "labels": [], "entities": []}, {"text": "Given that the inputs of our task are all from the same domain, our model is trained on a single input and introduces multi-input attention to generate a consensus result merely for decoding.", "labels": [], "entities": []}, {"text": "It does not require learning extra parameters for attention combination and thus is more efficient to train.", "labels": [], "entities": [{"text": "attention combination", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7420401871204376}]}, {"text": "Furthermore, average attention combination, a simple multi-input attention mechanism, is proposed to improve both the effectiveness and efficiency of multi-input combination on the OCR post-correction task.", "labels": [], "entities": [{"text": "OCR post-correction task", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.8912731210390726}]}, {"text": "We experiment with both supervised and unsupervised training and with single-and multiinput decoding on data from two manually transcribed collections in English with diverse typefaces, genres, and time periods: newspaper articles from the Richmond (Virginia) Daily Dispatch (RDD) from 1860-1865 and books from 1500-1800 from the Text Creation Partnership (TCP).", "labels": [], "entities": [{"text": "Text Creation Partnership (TCP)", "start_pos": 330, "end_pos": 361, "type": "TASK", "confidence": 0.7943336168924967}]}, {"text": "For both collections, which were manually transcribed by other researchers and are in the public domain, we aligned the one-best output of an OCR system to the manual transcripts.", "labels": [], "entities": []}, {"text": "We also aligned the OCR in the training and evaluation sets to other public-domain newspaper issues (from the Library of Congress) and books (from the Internet Archive) to find multiple duplicates as \"witnesses\", where available, for each line.", "labels": [], "entities": []}, {"text": "Experimental results on both datasets show that our proposed averarge attention combination mechanism is more effective than existing methods in integrating multiple inputs.", "labels": [], "entities": []}, {"text": "Moreover, our noisy error correction model achieves comparable performance with the supervised model via multiple-input decoding on duplicated texts.", "labels": [], "entities": [{"text": "noisy error correction", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.6357897818088531}]}, {"text": "In summary, our contributions are: (1) a scalable framework needing no supervision from human annotations to train the correction model; (2) a multi-input attention mechanism incorporating aligning, correcting, and voting on multiple sequences simultaneously for consensus decoding, which is more efficient and effective than existing ensemble methods; and (3) a method that corrects text either with or without duplicated versions, while most existing methods can only deal with one of these cases.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first introduce the details of our experimental setup ( \u00a74.1).", "labels": [], "entities": []}, {"text": "Then, the results of preliminary experiments comparing the performance of different options for the single-input Seq2Seq model and the multi-input attention combination strategies are presented in \u00a74.2.", "labels": [], "entities": []}, {"text": "The main experimental results for evaluating the correction model trained in different training settings and decoded with/without multi-input attention are reported and explained in \u00a74.3.", "labels": [], "entities": []}, {"text": "Further discussions of our model are described in \u00a74.4.", "labels": [], "entities": []}, {"text": "We begin by describing the data split, training details, baseline systems, and evaluation metrics.", "labels": [], "entities": []}, {"text": "Word error rate (WER) and character error rate (CER) are used to compare the performance of each method.", "labels": [], "entities": [{"text": "Word error rate (WER)", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.8800745209058126}, {"text": "character error rate (CER)", "start_pos": 26, "end_pos": 52, "type": "METRIC", "confidence": 0.9349360962708791}]}, {"text": "Lattice word error rate (LWER) and lattice character error rate (LCER) are also computed as the oracle performance for each method, which could reveal the capability of each model to be applied to downstream tasks taking lattices as input, e.g., reranking or retrieval of the correction hypotheses ().", "labels": [], "entities": [{"text": "Lattice word error rate (LWER)", "start_pos": 0, "end_pos": 30, "type": "METRIC", "confidence": 0.7809672057628632}, {"text": "lattice character error rate (LCER)", "start_pos": 35, "end_pos": 70, "type": "METRIC", "confidence": 0.830967515707016}]}, {"text": "We compute the macro average for each type of error rate, which allows us to use a paired permutation significance test.", "labels": [], "entities": []}, {"text": "In this section, we conduct two preliminary experiments to study different options for both the single-input correction models and the multi-input attention combination strategies.", "labels": [], "entities": []}, {"text": "We first compare the attention-based Seq2Seq (Attn-Seq2Seq) model, with a traditional Seq2Seq model, PCRF, on single input correction task.", "labels": [], "entities": [{"text": "single input correction task", "start_pos": 110, "end_pos": 138, "type": "TASK", "confidence": 0.7658200860023499}]}, {"text": "As the PCRF implementation of is highly memory and time consuming for training on long sequences, we compare it with Attn-Seq2Seq model on a smaller dataset with 100K lines randomly sampled from RDD newspapers training set.", "labels": [], "entities": [{"text": "RDD newspapers training set", "start_pos": 195, "end_pos": 222, "type": "DATASET", "confidence": 0.7947764843702316}]}, {"text": "The trained correction model is then applied to correct the full test set.", "labels": [], "entities": []}, {"text": "CER and WER of the correction results from both models are listed in.", "labels": [], "entities": [{"text": "CER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9823904037475586}, {"text": "WER", "start_pos": 8, "end_pos": 11, "type": "METRIC", "confidence": 0.9987572431564331}]}, {"text": "We can find that the Attn-Seq2Seq neural translation model works significantly better than the PCRF when trained on a dataset of the same size.", "labels": [], "entities": [{"text": "Attn-Seq2Seq neural translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6331644455591837}]}, {"text": "The performance of the Attn-Seq2seq model could be further improved by including more training data or by multi-input decoding for duplicated texts, while the PCRF could only be trained on limited data and is notable to work on multiple inputs.", "labels": [], "entities": [{"text": "PCRF", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.8646413087844849}]}, {"text": "Thus, we choose AttnSeq2Seq as our error correction model.", "labels": [], "entities": [{"text": "error correction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7391209006309509}]}], "tableCaptions": [{"text": " Table 3: CER and WER on single-input correction for", "labels": [], "entities": [{"text": "CER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9795915484428406}, {"text": "WER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9975602626800537}]}, {"text": " Table 4: Results of correcting lines in the RDD newspapers and TCP books with multiple witnesses when decoding with", "labels": [], "entities": [{"text": "RDD newspapers and TCP books", "start_pos": 45, "end_pos": 73, "type": "DATASET", "confidence": 0.8664271354675293}]}, {"text": " Table 5: Results from model trained under different settings on single-input decoding and multiple-input decoding for both", "labels": [], "entities": []}]}