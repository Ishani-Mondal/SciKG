{"title": [{"text": "Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Adaptive Knowledge Sharing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8243876099586487}, {"text": "Improving Low-Resource Neural Machine Translation", "start_pos": 51, "end_pos": 100, "type": "TASK", "confidence": 0.6470694363117218}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8686772783597311}]}, {"text": "An effective approach to compensate for this requirement is Multi-Task Learning (MTL) to leverage different linguistic resources as a source of inductive bias.", "labels": [], "entities": []}, {"text": "Current MTL architec-tures are based on the SEQ2SEQ transduc-tion, and (partially) share different components of the models among the tasks.", "labels": [], "entities": []}, {"text": "However, this MTL approach often suffers from task interference, and is notable to fully capture commonalities among subsets of tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9835917949676514}]}, {"text": "We address this issue by extending the recurrent units with multiple blocks along with a trainable routing network.", "labels": [], "entities": []}, {"text": "The routing network enables adap-tive collaboration by dynamic sharing of blocks conditioned on the task at hand, input , and model state.", "labels": [], "entities": []}, {"text": "Empirical evaluation of two low-resource translation tasks, En-glish to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9720065593719482}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has shown remarkable progress in recent years.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8284280896186829}]}, {"text": "However, it requires large amounts of bilingual data to learn a translation model with reasonable quality.", "labels": [], "entities": []}, {"text": "This requirement can be compensated by leveraging curated monolingual linguistic resources in a multi-task learning framework.", "labels": [], "entities": []}, {"text": "Essentially, learned knowledge from auxiliary linguistic tasks serves as inductive bias for the translation task to lead to better generalizations.", "labels": [], "entities": []}, {"text": "Multi-Task Learning (MTL) is an effective approach for leveraging commonalities of related tasks to improve performance.", "labels": [], "entities": [{"text": "Multi-Task Learning (MTL)", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.776391226053238}]}, {"text": "Various recent works have attempted to improve NMT by scaffolding translation task on a single auxiliary task.", "labels": [], "entities": [{"text": "NMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9735549688339233}, {"text": "translation task", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.835332065820694}]}, {"text": "Recently, have made use of several linguistic tasks to improve NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9396654963493347}]}, {"text": "Their method shares components of the SEQ2SEQ model among the tasks, e.g. encoder, decoder or the attention mechanism.", "labels": [], "entities": []}, {"text": "However, this approach has two limitations: (i) it fully shares the components, and (ii) the shared component(s) are shared among all of the tasks.", "labels": [], "entities": []}, {"text": "The first limitation can be addressed using deep stacked layers in encoder/decoder, and sharing the layers partially (.", "labels": [], "entities": []}, {"text": "The second limitation causes this MTL approach to suffer from task interference or inability to leverages commonalities among a subset of tasks.", "labels": [], "entities": [{"text": "MTL", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.979077160358429}]}, {"text": "Recently, () tried to address this issue; however, their method is restrictive for SEQ2SEQ scenarios and does not consider the input at each time step to modulate parameter sharing.", "labels": [], "entities": []}, {"text": "In this paper, we address the task interference problem by learning how to dynamically control the amount of sharing among all tasks.", "labels": [], "entities": []}, {"text": "We extended the recurrent units with multiple blocks along with a routing network to dynamically control sharing of blocks conditioning on the task at hand, the input, and model state.", "labels": [], "entities": []}, {"text": "Empirical results on two low-resource translation scenarios, English to Farsi and Vietnamese, show the effectiveness of the proposed model by achieving +1 BLEU score improvement compared to strong baselines.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 155, "end_pos": 165, "type": "METRIC", "confidence": 0.9699956774711609}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The performance measures of the baselines vs our MTL architecture on the bilingual datasets.", "labels": [], "entities": []}]}