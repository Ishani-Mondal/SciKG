{"title": [{"text": "Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings", "labels": [], "entities": [{"text": "Incorporating Latent Meanings of Morphological Compositions", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8013257881005605}]}], "abstractContent": [{"text": "Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes.", "labels": [], "entities": []}, {"text": "Furthermore, existing morphology-based models directly incorporate morphemes to train word embeddings, but still neglect the latent meanings of morphemes.", "labels": [], "entities": []}, {"text": "In this paper, we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings.", "labels": [], "entities": []}, {"text": "Based on this purpose, we propose three Latent Meaning Models (LMMs), named LMM-A, LMM-S and LMM-M respectively, which adopt different strategies to incorporate the latent meanings of morphemes during the training process.", "labels": [], "entities": []}, {"text": "Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7596215605735779}, {"text": "syntactic analogy", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.740206852555275}, {"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7601636350154877}]}, {"text": "The results demonstrate that our models outperform the baselines on five word similarity datasets.", "labels": [], "entities": []}, {"text": "On Wordsim-353 and RG-65 datasets, our models nearly achieve 5% and 7% gains over the classic CBOW model, respectively.", "labels": [], "entities": [{"text": "Wordsim-353", "start_pos": 3, "end_pos": 14, "type": "DATASET", "confidence": 0.9284459948539734}, {"text": "RG-65 datasets", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9244596660137177}]}, {"text": "For the syntactic analogy and text classification tasks, our models also surpass all the baselines including a morphology-based model.", "labels": [], "entities": [{"text": "syntactic analogy", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7898055613040924}, {"text": "text classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8024497926235199}]}], "introductionContent": [{"text": "Word embedding, which is also termed distributed word representation, has been a hot topic in the area of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "distributed word representation", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6732995609442393}, {"text": "Natural Language Processing (NLP)", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.6978669365247091}]}, {"text": "The derived word embeddings have been used in plenty of tasks such as text classification (Liu * This is the corresponding author.", "labels": [], "entities": [{"text": "text classification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8423328697681427}]}, {"text": "et al., 2015), information retrieval (, sentiment analysis (, machine translation () and soon.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.910862386226654}, {"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9100558161735535}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7153033465147018}]}, {"text": "Recently, some classic word embedding methods have been proposed, like Continuous Bag-ofWord (CBOW), Skip-gram (), Global Vectors (GloVe) ().", "labels": [], "entities": []}, {"text": "These methods can usually capture word-level semantic information but ignore the meaningful inner structures of words like English morphemes or Chinese characters.", "labels": [], "entities": []}, {"text": "The effectiveness of exploiting the internal compositions of words has been validated by some previous work ().", "labels": [], "entities": []}, {"text": "Some of them compute the word embeddings by directly adding the representations of morphemes/characters to context words or optimizing a joint objective over distributional statistics and morphological properties (, while others introduce some probabilistic graphical models to build relationship between words and their internal compositions.", "labels": [], "entities": []}, {"text": "e.g., treat word embeddings as latent variables fora prior distribution, which reflects words' morphological properties, and feed the latent variables into a neural sequence model to obtain final word embeddings.", "labels": [], "entities": []}, {"text": "construct a Gaussian graphical model that binds the morphological analysis to pre-trained word embeddings, which can help to smooth the noisy embeddings.", "labels": [], "entities": []}, {"text": "Besides, these two methods also have the ability to predict embeddings for unseen words.", "labels": [], "entities": []}, {"text": "Different from all the above models (we regard them as Explicit models in  composition embeddings like morpheme embeddings are generated as by-products, we explore anew way to employ the latent meanings of morphological compositions rather than the compositions themselves to train word embeddings.", "labels": [], "entities": []}, {"text": "As shown in, according to the distributional semantics hypothesis, incredible and unbelievable probably have similar word embeddings because they have similar context.", "labels": [], "entities": []}, {"text": "As a matter of fact, incredible is a synonym of unbelievable and their embeddings are expected to be close enough.", "labels": [], "entities": []}, {"text": "Since the morphemes of the two words are different, especially the roots cred and believ, the explicit models may not significantly shorten the distance between the words in the vector space.", "labels": [], "entities": []}, {"text": "Fortunately, the latent meanings of the different morphemes are the same (e.g., the latent meanings of roots cred, believ are \"believe\") as listed in the lookup table (derived from the resources provided by Michigan State University), 1 which evidently implies that incredible and unbelievable share the same meanings.", "labels": [], "entities": []}, {"text": "In addition, by replacing morphemes with their latent meanings, we can directly and simply quantize the similarities between words and their sub-compositions with the same metrics used inmost NLP tasks, e.g., cosine similarity.", "labels": [], "entities": []}, {"text": "Subsequently, the similarities are utilized to calculate the weights of latent meanings of morphemes for each word.", "labels": [], "entities": []}, {"text": "In this paper, we try different strategies to modify the input layer and update rules of a neural language model, e.g., CBOW, Skipgram, and propose three lightweight and efficient models, which are termed Latent Meaning Models (LMMs), to not only encode morphological properties into words but also enhance the semantic similarities among word embeddings.", "labels": [], "entities": []}, {"text": "Usually, the vocabulary derived from the corpus contains vast majority or even all of the latent meanings.", "labels": [], "entities": []}, {"text": "Rather than generating and training extra embeddings for latent meanings, we directly override the embeddings of the corresponding words in the vocabulary.", "labels": [], "entities": []}, {"text": "Moreover, a word map is created to describe the relations between words and the latent meanings of their morphemes.", "labels": [], "entities": []}, {"text": "For comparison, our models together with the state-of-the-art baselines are tested on two basic NLP tasks, which are word similarity and syntactic analogy, and one downstream text classification task.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7259745597839355}, {"text": "syntactic analogy", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7261300981044769}, {"text": "text classification", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.6981424987316132}]}, {"text": "The results show that LMMs outperform the baselines and get satisfactory improvement on these tasks.", "labels": [], "entities": []}, {"text": "In all, the main contributions of this paper are summarized as follows.", "labels": [], "entities": []}, {"text": "\u2022 Rather than directly incorporating the morphological compositions (surface forms) of words, we decide to employ the latent meanings of the compositions (underlying forms) to train the word embeddings.", "labels": [], "entities": []}, {"text": "To validate the feasibility of our purpose, three specific models, named LMMs, are proposed with different strategies to incorporate the latent meanings.", "labels": [], "entities": []}, {"text": "\u2022 We utilize a medium-sized English corpus to train LMMs and the state-of-the-art baselines, and evaluate their performance on two basic NLP tasks, i.e., word similarity and syntactic analogy, and one downstream text classification task.", "labels": [], "entities": [{"text": "word similarity and syntactic analogy", "start_pos": 154, "end_pos": 191, "type": "TASK", "confidence": 0.6743238031864166}, {"text": "text classification", "start_pos": 212, "end_pos": 231, "type": "TASK", "confidence": 0.6995953917503357}]}, {"text": "The results show that LMMs outperform the baselines on five word similarity datasets.", "labels": [], "entities": []}, {"text": "On the golden standard Wordsim-353 and RG-65, LMMs approximately achieve 5% and 7% gains over CBOW, respectively.", "labels": [], "entities": [{"text": "Wordsim-353", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.838654100894928}, {"text": "RG-65", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.7630292177200317}]}, {"text": "For the syntactic analogy and text classification tasks, LMMs also surpass all the baselines.", "labels": [], "entities": [{"text": "syntactic analogy", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7776018679141998}, {"text": "text classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.805550217628479}]}, {"text": "\u2022 We conduct experiments to analyze the impacts of parameter settings, and the results demonstrate that the performance of LMMs on the smallest corpus is similar to the performance of CBOW on the corpus that is five times as large, which convinces us that LMMs are of great advantages to enhance word embeddings compared with traditional methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before conducting experiments, some experimental settings are firstly introduced in this section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of datasets. The column \"Pairs\"  shows the number of word pairs in each dataset.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison (%) of our LMMs and the baselines on two basic NLP tasks (word  similarity & syntactic analogy) and one downstream task (text classification). The bold digits indicate  the best performances.", "labels": [], "entities": [{"text": "word  similarity & syntactic analogy", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.6918538987636567}, {"text": "text classification", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.6819866746664047}]}]}