{"title": [{"text": "Confidence Modeling for Neural Semantic Parsing", "labels": [], "entities": [{"text": "Confidence Modeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8771151304244995}, {"text": "Neural Semantic Parsing", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6076313654581705}]}], "abstractContent": [{"text": "In this work we focus on confidence mod-eling for neural semantic parsers which are built upon sequence-to-sequence models.", "labels": [], "entities": [{"text": "neural semantic parsers", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6907133162021637}]}, {"text": "We outline three major causes of uncertainty , and design various metrics to quantify these factors.", "labels": [], "entities": []}, {"text": "These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct.", "labels": [], "entities": []}, {"text": "Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input.", "labels": [], "entities": []}, {"text": "Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8382330536842346}]}, {"text": "The neural sequenceto-sequence architecture) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7188232243061066}]}, {"text": "However, despite achieving promising results, neural semantic parsers remain difficult to interpret, acting inmost cases as a black box, not providing any information about what made them arrive at a particular decision.", "labels": [], "entities": [{"text": "neural semantic parsers", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.68999844789505}]}, {"text": "In this work, we explore ways to estimate and interpret the * Work carried out during an internship at Microsoft Research.", "labels": [], "entities": []}, {"text": "model's confidence in its predictions, which we argue can provide users with immediate and meaningful feedback regarding uncertain outputs.", "labels": [], "entities": []}, {"text": "An explicit framework for confidence modeling would benefit the development cycle of neural semantic parsers which, contrary to more traditional methods, do not make use of lexicons or templates and as a result the sources of errors and inconsistencies are difficult to trace.", "labels": [], "entities": [{"text": "confidence modeling", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7689578235149384}]}, {"text": "Moreover, from the perspective of application, semantic parsing is often used to build natural language interfaces, such as dialogue systems.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7385667562484741}]}, {"text": "In this case it is important to know whether the system understands the input queries with high confidence in order to make decisions more reliably.", "labels": [], "entities": []}, {"text": "For example, knowing that some of the predictions are uncertain would allow the system to generate clarification questions, prompting users to verify the results before triggering unwanted actions.", "labels": [], "entities": []}, {"text": "In addition, the training data used for semantic parsing can be small and noisy, and as a result, models do indeed produce uncertain outputs, which we would like our framework to identify.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7961498498916626}]}, {"text": "A widely-used confidence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model's prediction.", "labels": [], "entities": []}, {"text": "For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger.", "labels": [], "entities": []}, {"text": "Neural models, in contrast, learn a complicated function that often overfits the training data.", "labels": [], "entities": []}, {"text": "Posterior probability is effective when making decisions about model output, but is no longer a good indicator of confidence due in part to the nonlinearity of neural networks.", "labels": [], "entities": [{"text": "Posterior probability", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.96916463971138}]}, {"text": "This observation motivates us to develop a confidence modeling framework for sequenceto-sequence models.", "labels": [], "entities": []}, {"text": "We categorize the causes of uncertainty into three types, namely model uncertainty, data uncertainty, and input uncertainty and design different metrics to characterize them.", "labels": [], "entities": []}, {"text": "We compute these confidence metrics fora given prediction and use them as features in a regression model which is trained on held-out data to fit prediction F1 scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 157, "end_pos": 159, "type": "METRIC", "confidence": 0.9685667157173157}]}, {"text": "At test time, the regression model's outputs are used as confidence scores.", "labels": [], "entities": []}, {"text": "Our approach does not interfere with the training of the model, and can be thus applied to various architectures, without sacrificing test accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9874045848846436}]}, {"text": "Furthermore, we propose a method based on backpropagation which allows to interpret model behavior by identifying which parts of the input contribute to uncertain predictions.", "labels": [], "entities": []}, {"text": "Experimental results on two semantic parsing datasets (IFTTT,) show that our model is superior to a method based on posterior probability.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7647610306739807}, {"text": "IFTTT", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.6899422407150269}]}, {"text": "We also demonstrate that thresholding confidence scores achieves a good trade-off between coverage and accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9986336827278137}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9903766512870789}]}, {"text": "Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores.", "labels": [], "entities": []}], "datasetContent": [{"text": "IFTTT turn android phone to full volume at 7am monday to friday date time\u2212every day of the week at\u2212((time of day DJANGO for every key in sorted list of user settings for key in sorted(user settings): the network (lines 6-9), and finally into the neurons of the input words.", "labels": [], "entities": [{"text": "IFTTT", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5702522397041321}, {"text": "friday date time", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.918379565080007}, {"text": "DJANGO", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.8943031430244446}]}, {"text": "We summarize them and compute the token-level scores for interpreting the results (line 10-13).", "labels": [], "entities": [{"text": "interpreting", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9671388268470764}]}, {"text": "For input word vector qt , we use the summation of its neuron-level scores as the token-level score: where c \u2208 qt represents the neurons of word vector qt , and \ud97b\udf59 |q| t=1\u00fbt=1\u02c6t=1\u00fb qt = 1.", "labels": [], "entities": []}, {"text": "We use the normalized scor\u00ea u qt to indicate token qt 's contribution to prediction uncertainty.", "labels": [], "entities": []}, {"text": "In this section we describe the datasets used in our experiments and various details concerning our models.", "labels": [], "entities": []}, {"text": "We present our experimental results and analysis of model behavior.", "labels": [], "entities": []}, {"text": "Our code is publicly available at https://github.com/ donglixp/confidence.", "labels": [], "entities": []}, {"text": "We trained the neural semantic parser introduced in Section 3 on two datasets covering different domains and meaning representations.", "labels": [], "entities": [{"text": "neural semantic parser", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7391934792200724}]}, {"text": "IFTTT This dataset) contains a large number of if-this-then-that programs crawled from the IFTTT website.", "labels": [], "entities": [{"text": "IFTTT This dataset", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9513858159383138}, {"text": "IFTTT website", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9617490768432617}]}, {"text": "The programs are written for various applications, such as home security (e.g., \"email me if the window opens\"), and task automation (e.g., \"save instagram photos to dropbox\").", "labels": [], "entities": [{"text": "home security", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.7119860053062439}]}, {"text": "Whenever a program's trigger is satisfied, an action is performed.", "labels": [], "entities": []}, {"text": "Triggers and actions represent functions with arguments; they are selected from different channels (160 in total) representing various services (e.g., Android).", "labels": [], "entities": []}, {"text": "There are 552 trigger functions and 229 action functions.", "labels": [], "entities": []}, {"text": "The original split contains 77, 495 training, 5, 171 development, and 4, 294 test instances.", "labels": [], "entities": []}, {"text": "The subset that removes non-English descriptions was used in our experiments.", "labels": [], "entities": []}, {"text": "DJANGO This dataset () is built upon the code of the Django web framework.", "labels": [], "entities": [{"text": "DJANGO", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8860232830047607}, {"text": "Django web framework", "start_pos": 53, "end_pos": 73, "type": "DATASET", "confidence": 0.9099747935930887}]}, {"text": "Each line of Python code has a manually annotated natural language description.", "labels": [], "entities": []}, {"text": "Our goal is to map the English pseudo-code to Python statements.", "labels": [], "entities": []}, {"text": "This dataset contains diverse use cases, such as iteration, exception handling, and string manipulation.", "labels": [], "entities": [{"text": "exception handling", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.8416137993335724}, {"text": "string manipulation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7367666959762573}]}, {"text": "The original split has 16, 000 training, 1, 000 development, and 1, 805 test examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman \u03c1 correlation between confi- dence scores and F1. Best results are shown in  bold. All correlations are significant at p < 0.01.", "labels": [], "entities": [{"text": "Spearman \u03c1 correlation", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9369038740793864}, {"text": "F1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9996566772460938}]}, {"text": " Table 3: Correlation matrix for F1 and individual  confidence metrics on the IFTTT dataset. All cor- relations are significant at p < 0.01. Best predic- tors are shown in bold. Dout is short for dropout,  PR for posterior probability, PPL for perplexity,  LM for probability based on a language model,  #UNK for number of unknown tokens, Var for  variance of top candidates, and Ent for Entropy.", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9882065653800964}, {"text": "IFTTT dataset", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9807108640670776}, {"text": "Dout", "start_pos": 178, "end_pos": 182, "type": "METRIC", "confidence": 0.9783968329429626}, {"text": "UNK", "start_pos": 305, "end_pos": 308, "type": "METRIC", "confidence": 0.955467164516449}, {"text": "Var", "start_pos": 339, "end_pos": 342, "type": "METRIC", "confidence": 0.9714353084564209}, {"text": "Ent", "start_pos": 380, "end_pos": 383, "type": "METRIC", "confidence": 0.9680401682853699}]}, {"text": " Table 4: Correlation matrix for F1 and individual  confidence metrics on the DJANGO dataset. All  correlations are significant at p < 0.01. Best pre- dictors are shown in bold. Same shorthands apply  as in", "labels": [], "entities": [{"text": "Correlation", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9729158878326416}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9978419542312622}, {"text": "DJANGO dataset", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9753827154636383}]}, {"text": " Table 5: Importance scores of confidence metrics  (normalized by maximum value on each dataset).  Best results are shown in bold. Same shorthands  apply as in", "labels": [], "entities": [{"text": "Importance", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9968769550323486}]}, {"text": " Table 6: Uncertainty interpretation against in- ferred ground truth; we compute the overlap be- tween tokens identified as contributing to uncer- tainty by our method and those found in the gold  standard. Overlap is shown for top 2 and 4 tokens.  Best results are in bold.", "labels": [], "entities": [{"text": "Uncertainty interpretation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7844883799552917}, {"text": "Overlap", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9783113598823547}]}]}