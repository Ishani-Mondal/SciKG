{"title": [{"text": "DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension", "labels": [], "entities": [{"text": "DuoRC", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6639692187309265}]}], "abstractContent": [{"text": "We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6509746015071869}, {"text": "language understanding", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7187162488698959}]}, {"text": "DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie-one from Wikipedia and the other from IMDb-written by two different authors.", "labels": [], "entities": []}, {"text": "We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extractor synthesize answers from the other version.", "labels": [], "entities": []}, {"text": "This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version.", "labels": [], "entities": []}, {"text": "Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge.", "labels": [], "entities": []}, {"text": "Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences.", "labels": [], "entities": []}, {"text": "Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset (Rajpurkar et al., 2016b), even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset).", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 108, "end_pos": 121, "type": "DATASET", "confidence": 0.8321343660354614}, {"text": "F1 score", "start_pos": 275, "end_pos": 283, "type": "METRIC", "confidence": 0.9858149886131287}, {"text": "SQuAD dataset", "start_pos": 314, "end_pos": 327, "type": "DATASET", "confidence": 0.9262917041778564}]}, {"text": "This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding .", "labels": [], "entities": [{"text": "language understanding", "start_pos": 148, "end_pos": 170, "type": "TASK", "confidence": 0.7153291702270508}]}], "introductionContent": [{"text": "Natural Language Understanding is widely accepted to be one of the key capabilities required for AI systems.", "labels": [], "entities": [{"text": "Natural Language Understanding", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6915520230929056}]}, {"text": "Scientific progress on this endeavor is measured through multiple tasks such as machine translation, reading comprehension, questionanswering, and others, each of which requires the machine to demonstrate the ability to \"comprehend\" the given textual input (apart from other aspects) and achieve their task-specific goals.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7087720632553101}]}, {"text": "In particular, Reading Comprehension (RC) systems are required to \"understand\" a given text passage as input and then answer questions based on it.", "labels": [], "entities": [{"text": "Reading Comprehension (RC)", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6569024384021759}]}, {"text": "It is therefore critical, that the dataset benchmarks established for the RC task keep progressing in complexity to reflect the challenges that arise in true language understanding, thereby enabling the development of models and techniques to solve these challenges.", "labels": [], "entities": [{"text": "true language understanding", "start_pos": 153, "end_pos": 180, "type": "TASK", "confidence": 0.6318584382534027}]}, {"text": "For RC in particular, there has been significant progress over the recent years with several benchmark datasets, the most popular of which are the SQuAD dataset (), TriviaQA (, MS MARCO (), MovieQA () and clozestyle datasets.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.8634414374828339}, {"text": "MS MARCO", "start_pos": 177, "end_pos": 185, "type": "DATASET", "confidence": 0.6491043120622635}, {"text": "MovieQA", "start_pos": 190, "end_pos": 197, "type": "DATASET", "confidence": 0.8972588181495667}]}, {"text": "However, these benchmarks, owing to both the nature of the passages and the QA pairs to evaluate the RC task, have 2 primary limitations in studying language understanding: (i) Other than MovieQA, which is a small dataset of 15K QA pairs, all other largescale RC datasets deal only with factual descriptive passages and not narratives (involving events with causality linkages that require reasoning and background knowledge) which is the case with a lot of real-world content such as story books, movies, news reports, etc.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.7417970895767212}]}, {"text": "(ii) their questions possess a large lexical overlap with segments of the passage, or have a high noise level in QA pairs themselves.", "labels": [], "entities": []}, {"text": "As demonstrated by recent work, this makes it easy for even simple keyword matching algorithms to achieve high accuracy (.", "labels": [], "entities": [{"text": "keyword matching", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.7220172435045242}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9955427050590515}]}, {"text": "In fact, these models have been shown to perform poorly in the presence of adversarially inserted sentences which have a high word overlap with the question but do not contain the answer.", "labels": [], "entities": []}, {"text": "While this problem does not exist in TriviaQA it is admittedly noisy because of the use of distant supervision.", "labels": [], "entities": [{"text": "TriviaQA", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9080078601837158}]}, {"text": "Similarly, for cloze-style datasets, due to the automatic question generation process, it is very easy for current models to reach near human performance.", "labels": [], "entities": [{"text": "question generation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7205507606267929}]}, {"text": "This therefore limits the complexity in language understanding that a machine is required to demonstrate to do well on the RC task.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7293221354484558}]}, {"text": "Motivated by these shortcomings and to push the state-of-the-art in language understanding in RC, in this paper we propose DuoRC, which specifically presents the following challenges beyond the existing datasets: 1.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7214827090501785}]}, {"text": "DuoRC is especially designed to contain a large number of questions with low lexical overlap between questions and their corresponding passages.", "labels": [], "entities": []}, {"text": "2. It requires the use of background and commonsense knowledge to arrive at the answer and go beyond the content of the passage itself.", "labels": [], "entities": []}, {"text": "3. It contains narrative passages from movie plots that require complex reasoning across multiple sentences to infer the answer.", "labels": [], "entities": []}, {"text": "4. Several of the questions in DuoRC, while seeming relevant, cannot actually be answered from the given passage, thereby requiring the machine to detect the unanswerability of questions.", "labels": [], "entities": []}, {"text": "In order to capture these four challenges, DuoRC contains QA pairs created from pairs of documents describing movie plots which were gathered as follows.", "labels": [], "entities": []}, {"text": "Each document in a pair is a different version of the same movie plot written by different authors; one version of the plot is taken from the Wikipedia page of the movie whereas the other from its IMDb page (see for portions of an example pair of plots from the movie \"Twelve Monkeys\").", "labels": [], "entities": []}, {"text": "We first showed crowd workers on Amazon Mechanical Turk (AMT) the first version of the plot and asked them to create QA pairs from it.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.92230158050855}]}, {"text": "We then showed the second version of the plot along with the questions created from the first version to a different set of workers on AMT and asked them to provide answers by reading the second version only.", "labels": [], "entities": [{"text": "AMT", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.8978481888771057}]}, {"text": "Since the two versions contain different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version exhibits all of the four challenges mentioned above.", "labels": [], "entities": []}, {"text": "We now make several interesting observations from the example in.", "labels": [], "entities": []}, {"text": "For 4 out of the 8 questions (Q1, Q2, Q4, and Q7), though the answers extracted from the two plots are exactly the same, the analysis required to arrive at this answer is very different in the two cases.", "labels": [], "entities": []}, {"text": "In particular, for Q1 even though there is no explicit mention of the prisoner living in a subterranean shelter and hence no lexical overlap with the question, the workers were still able to infer that the answer is Philadelphia because that is the city to which James Cole travels to for his mission.", "labels": [], "entities": []}, {"text": "Another interesting characteristic of this dataset is that fora few questions (Q6, Q8) alternative but valid answers are obtained from the second plot.", "labels": [], "entities": []}, {"text": "Further, note the kind of complex reasoning required for answering Q8 where the machine needs to resolve coreferences over multiple sentences (that man refers to Dr. Peters) and use commonsense knowledge that if an item clears an airport screening, then a person can likely board the plane with it.", "labels": [], "entities": [{"text": "answering Q8", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.8377984464168549}]}, {"text": "To re-emphasize, these examples exhibit the need for machines to demonstrate new capabilities in RC such as: (i) employing a knowledge graph (e.g. to know that Philadelphia is a city in Q1), (ii) common-sense knowledge (e.g., clearing airport security implies boarding) (iii) paraphrase/semantic understanding (e.g. revolver is a type of handgun in Q7) (iv) multiple-sentence inferencing across events in the passage including coreference resolution of named entities and nouns, and (v) educated guesswork when the question is not directly answerable but there are subtle hints in the passage (as in Q1).", "labels": [], "entities": [{"text": "paraphrase/semantic understanding", "start_pos": 276, "end_pos": 309, "type": "TASK", "confidence": 0.6657721102237701}, {"text": "coreference resolution of named entities and nouns", "start_pos": 427, "end_pos": 477, "type": "TASK", "confidence": 0.9155105096953255}]}, {"text": "Finally, for quite a few questions, there wasn't sufficient information in the second plot to obtain their answers.", "labels": [], "entities": []}, {"text": "In such cases, the workers marked the question as \"unanswerable\".", "labels": [], "entities": []}, {"text": "This brings out a very important challenge for machines (detect unanswerability of questions) because a practical system should be able to know when it is not possible for it to answer a question given the data available to it, and in such cases, possibly delegate the task to a human instead.", "labels": [], "entities": []}, {"text": "Current RC systems built using existing datasets are far from possessing these capabilities to solve the above challenges.", "labels": [], "entities": []}, {"text": "In Section 4, we seek to establish solid baselines for DuoRC employing stateof-the-art RC models coupled with a collection of standard NLP techniques to address few of the above challenges.", "labels": [], "entities": [{"text": "DuoRC", "start_pos": 55, "end_pos": 60, "type": "TASK", "confidence": 0.807673454284668}]}, {"text": "Proposing novel neural models that solve all of the challenges in DuoRC is out of the scope of this paper.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that when the existing state-of-the-art RC systems are trained and evaluated on DuoRC they perform poorly leaving a lot of scope for improvement and open new avenues for research in RC.", "labels": [], "entities": [{"text": "DuoRC", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.9196484088897705}]}, {"text": "Do note that this dataset is not a substitute for existing RC datasets but can be coupled with them to collectively address a large set of challenges in language understanding with RC (the more the merrier).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we elaborate on the three phases of our dataset collection process.", "labels": [], "entities": [{"text": "dataset collection", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6971140503883362}]}, {"text": "Extracting parallel movie plots: We first collected top 40K movies from IMDb across different genres (crime, drama, comedy, etc.) whose plot synopsis were crawled from Wikipedia as well as IMDb.", "labels": [], "entities": [{"text": "Extracting parallel movie plots", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8924763202667236}]}, {"text": "We retained only 7680 movies for which both the plots were available and longer than 100 words.", "labels": [], "entities": []}, {"text": "In general, we found that the IMDb plots were usually longer (avg.", "labels": [], "entities": []}, {"text": "length 926 words) and more descriptive than the Wikipedia plots (avg. length 580 words).", "labels": [], "entities": [{"text": "Wikipedia plots", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9094404876232147}]}, {"text": "To make sure that the content between the two plots are indeed different and one is not just a subset of another, we calculated wordlevel jaccard distance between them i.e. the ratio of intersection to union of the bag-of-words in the two plots and found it to be 26%.", "labels": [], "entities": [{"text": "wordlevel jaccard distance", "start_pos": 128, "end_pos": 154, "type": "METRIC", "confidence": 0.679227332274119}]}, {"text": "This indicates that one of the plots is usually longer and descriptive, and, the two plots are infact quite different, even though the information content is very similar.", "labels": [], "entities": []}, {"text": "Collecting QA pairs from shorter version of the plot (SelfRC): As mentioned earlier, on average the longer version of the plot is almost double the size of the shorter version which is itself usually 500 words long.", "labels": [], "entities": [{"text": "SelfRC", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8138307332992554}]}, {"text": "Intuitively, the longer version should have more details and the questions asked from the shorter version should be answerable from the longer one.", "labels": [], "entities": []}, {"text": "Hence, we first showed the shorter version of the plot to workers on AMT and asked them to create QA pairs from it.", "labels": [], "entities": [{"text": "AMT", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8627051115036011}]}, {"text": "The instructions given to the workers for this phase are as follows: (i) the answer must preferably be a single word or a short phrase, (ii) subjective questions (like asking for opinion) are not allowed, (iii) questions should be answerable only from the passage and not require any external knowledge, and (iv) questions and answers should be well formed and grammatically correct.", "labels": [], "entities": []}, {"text": "The workers were also given freedom to either pick an answer which directly matches a span in the document or synthesize the answer from scratch.", "labels": [], "entities": []}, {"text": "This option allowed them to be creative and ask hard questions where possible.", "labels": [], "entities": []}, {"text": "We found that in 70% of the cases the workers picked an answer directly from the document and in 30% of the cases they synthesized the answer.", "labels": [], "entities": []}, {"text": "We thus collected 85,773 such QA pairs along with their corresponding documents.", "labels": [], "entities": []}, {"text": "We refer to this as the SelfRC dataset because the answers were derived from the same document from which the questions were asked.", "labels": [], "entities": [{"text": "SelfRC dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9833182096481323}]}, {"text": "Collecting answers from longer version of the plot (ParaphraseRC): We then paired the questions from the SelfRC dataset with the corresponding longer version of the plot and showed it to a different set of AMT workers asking them to answer these questions from the longer version of the plot.", "labels": [], "entities": [{"text": "SelfRC dataset", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9776387512683868}]}, {"text": "They now have the option to either (i) select an answer which matches a span in the longer version, (ii) synthesize the answer from scratch, or (iii) mark the question not-answerable because of lack of information in the given passage.", "labels": [], "entities": []}, {"text": "One trick we used to reduce the fatigue of workers (caused by reading long pieces of text), and thus maintain the answer quality is to split the long plots into multiple segments.", "labels": [], "entities": []}, {"text": "Every question obtained from the first phase of annotation is paired separately with each of these segments and each (question, segment) pair is posted as a different job.", "labels": [], "entities": []}, {"text": "With this approach, we essentially get multiple answers to the same question, if it is answerable from more than one segment.", "labels": [], "entities": []}, {"text": "However, on an average we get approximately one unique answer for each question.", "labels": [], "entities": []}, {"text": "We found that in 50% of the cases the workers selected an answer which matched a span in the document, whereas in 37% cases they synthesized the answer and in 13% cases they said that question was not answerable.", "labels": [], "entities": []}, {"text": "The workers were strictly instructed to keep the answers short, derive the answer from the plot and use general knowledge or logic to answer the questions.", "labels": [], "entities": []}, {"text": "They were not allowed to rely on personal knowledge about the movie (in any case given the large number of movies in our dataset the chance of a worker remembering all the plot details fora given movie is very less).", "labels": [], "entities": []}, {"text": "For quality assessment purposes, various levels of manual and semi-automated inspections were done, especially in the second phase of annotation, such as:(i) weeding out annotators who mark majority of answers as non-answerable, by taking into account their response time, and (ii) annotators for whom a high percentage of answers have no entity (or noun phrase) overlap with the entire passage were subjected to strict manual inspection and blacklisted if necessary.", "labels": [], "entities": []}, {"text": "Further, await period of 2-3 weeks was deliberately introduced between the two phases of data collection to ensure the availability of afresh pool of workers as well as to reduce information bias among workers common to both the tasks.", "labels": [], "entities": []}, {"text": "Overall 2559 workers took part in the first phase of the annotation, and 8021 workers in the second phase.", "labels": [], "entities": []}, {"text": "Only 703 workers were common between the phases.", "labels": [], "entities": []}, {"text": "We refer to this dataset, where the questions are taken from one version of the document and the answers are obtained from a different version, as ParaphraseRC which contains 100,316 such {question, answer, document} triplets.", "labels": [], "entities": []}, {"text": "Overall, 62% of the questions in SelfRC and ParaphraseRC have partial overlap in their answers, which is indicative of the fact that quality is reasonable.", "labels": [], "entities": [{"text": "SelfRC", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9311054348945618}]}, {"text": "The remaining 38% where there is no overlap can be attributed to nonanswerablity of the question from the bigger plot, information gap, or paraphrasing of information between the two plots.", "labels": [], "entities": []}, {"text": "Note that the number of unique questions in the ParaphraseRC dataset is the same as that in SelfRC because we do not create any new questions from the longer version of the plot.", "labels": [], "entities": [{"text": "ParaphraseRC dataset", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8451511263847351}, {"text": "SelfRC", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9354029893875122}]}, {"text": "We end up with a greater number of {question, answer, document} triplets in ParaphraseRC as compared to SelfRC (100,316 v/s 85,773) since movies that are remakes of a previous movie had very little difference in their Wikipedia plots.", "labels": [], "entities": [{"text": "SelfRC", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.8560599088668823}]}, {"text": "Therefore, we did not separately collect questions from the Wikipedia plot of the remake.", "labels": [], "entities": []}, {"text": "However, the IMDb plots of the two movies are very different and so we have two different longer versions of the movie (one for the original and one for the remake).", "labels": [], "entities": []}, {"text": "We can thus pair the questions created from the Wikipedia plot with both the IMDb versions of the plot thus augmenting the {question, answer, document} triplets.", "labels": [], "entities": []}, {"text": "Another notable observation is that in many cases the answers to the same question are different in the two versions.", "labels": [], "entities": []}, {"text": "Specifically, only 40.7% of the questions have the same answer in the two documents.", "labels": [], "entities": []}, {"text": "For around 37.8% of the questions there is no overlap between the words in the two answers.", "labels": [], "entities": [{"text": "overlap", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9524872899055481}]}, {"text": "For the remaining 21% of the questions there is a partial overlap between the two answers.", "labels": [], "entities": [{"text": "overlap", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9533064961433411}]}, {"text": "For e.g., the answer derived from the shorter version could be \"using his wife's gun\" and from the longer version could be \"with Dana's handgun\" where Dana is the name of the wife.", "labels": [], "entities": []}, {"text": "In Appendix A, we provide a few randomly picked examples from our dataset which should convince the reader of the difficulty of ParaphraseRC and its differences with SelfRC.", "labels": [], "entities": [{"text": "SelfRC", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.9430541396141052}]}, {"text": "We refer to this combined dataset containing a total shows the distribution of different Wh-type questions in our dataset.", "labels": [], "entities": []}, {"text": "Some interesting comparative analysis are presented in and also in Appendix B. In, we compare various RC datasets with two embodiments of our dataset i.e. the SelfRC and ParaphraseRC.", "labels": [], "entities": [{"text": "SelfRC", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.8854602575302124}]}, {"text": "We use NER and noun phrase/verb phrase extraction over the entire dataset to iden-tify key entities in the question, plot and answer which is in turn used to compute the metrics mentioned in the table.", "labels": [], "entities": [{"text": "noun phrase/verb phrase extraction", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.6151695102453232}]}, {"text": "The metrics \"Avg word distance\" and \"Avg sentence distance\" indicate the average distance (in terms of words/sentences) between the occurrence of the question entities and closest occurrence of the answer entities in the passage.", "labels": [], "entities": [{"text": "Avg word distance", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8584379156430563}, {"text": "Avg sentence distance", "start_pos": 37, "end_pos": 58, "type": "METRIC", "confidence": 0.9012161294619242}]}, {"text": "\"Number of sentences for inferencing\" is indicative of the minimum number of sentences required to coverall the question and answer entities.", "labels": [], "entities": []}, {"text": "It is evident that tackling ParaphraseRC is much harder than the others on account of (i) larger distance between the query and answer, (ii) low word-overlap between query & passage, and (iii) higher number of sentences required to infer an answer.", "labels": [], "entities": [{"text": "tackling ParaphraseRC", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.8198793530464172}]}, {"text": "In the following sub-sections we describe (i) the evaluation metrics, and (ii) the choices considered for augmenting the training data for the answer generation model.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.8093267679214478}]}, {"text": "Note that when creating the train, validation and test set, we ensure that the test set does not contain QA pairs for any movie that was seen during training.", "labels": [], "entities": [{"text": "QA pairs", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9655033349990845}]}, {"text": "We split the movies in such away that the resulting train, valid, test sets respectively contain 70%, 15% and 15% of the total number of QA pairs.", "labels": [], "entities": []}, {"text": "Span-Based Test Set and Full Test Set As mentioned earlier, the SpanModel only predicts the span in the document whereas the GenModel generates the answer after predicting the span.", "labels": [], "entities": []}, {"text": "Ideally, the SpanModel should only be evaluated on those instances in the test set where the answer matches a span in the document.", "labels": [], "entities": []}, {"text": "We refer to this subset of the test set as the Span-based Test Set.", "labels": [], "entities": [{"text": "Span-based Test Set", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.8472263415654501}]}, {"text": "Though not ideal, we also evaluate the SpanModel model on the entire test set.", "labels": [], "entities": []}, {"text": "This is not ideal because there are many answers in the test set which do not correspond to a span in the document whereas the model was only trained to predict spans.", "labels": [], "entities": []}, {"text": "We refer to this as the Full Test Set.", "labels": [], "entities": [{"text": "Full Test Set", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8745577931404114}]}, {"text": "We also evaluate the GenModel on both the test sets.", "labels": [], "entities": [{"text": "GenModel", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9362562894821167}]}, {"text": "Training Data for the GenModel As mentioned earlier, the GenModel contains two stages; the first stage predicts the span and the second stage then generates an answer from the predicted span.", "labels": [], "entities": []}, {"text": "For the first step we plug-in the best performing SpanModel from our earlier exploration.", "labels": [], "entities": []}, {"text": "To train the second stage we need training data of the form {x = span, y= answer} which comes from two types of instances: one where the answer matches a span and the other where the answer is synthesized and the span corresponding to it is not known.", "labels": [], "entities": []}, {"text": "In the first case x=y and there is nothing interesting for the model to learn (except for copying the input to the output).", "labels": [], "entities": []}, {"text": "In the second case x is not known.", "labels": [], "entities": []}, {"text": "To overcome this problem, for the second type of instances, we consider various approaches for finding the approximate span from which the answer could have been generated, and augment the training data with {x = approx span, y= answer}.", "labels": [], "entities": []}, {"text": "The easiest method was to simply treat the entire document as the true span from which the answer was generated (x = document, y = answer).", "labels": [], "entities": []}, {"text": "The second alternative that we tried was to first extract the named entities, noun phrases and verb phrases from the question and create a lucene query from these components.", "labels": [], "entities": []}, {"text": "We then used the lucene search engine to extract the most relevant portions of the document given this query.", "labels": [], "entities": []}, {"text": "We then considered this portion of the document as the true span (as opposed to treating the entire document as the true span).", "labels": [], "entities": []}, {"text": "Note that lucene could return multiple relevant spans in which case we treat all these {x = approx span, y= answer} as training instances.", "labels": [], "entities": []}, {"text": "Another alternative was to find the longest common subsequence (LCS) between the document and the question and treat this subsequence as the span from which the answer was generated.", "labels": [], "entities": [{"text": "longest common subsequence (LCS)", "start_pos": 36, "end_pos": 68, "type": "METRIC", "confidence": 0.6868502249320348}]}, {"text": "Of these, we found that the model trained using {x = approx span, y= answer} pairs created using the LCS based method gave the best results.", "labels": [], "entities": []}, {"text": "We report numbers only for this model.", "labels": [], "entities": []}, {"text": "Evaluation Metrics Similar to) we use Accuracy and F-score as the evaluation metrics.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.999154806137085}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9948645234107971}]}, {"text": "We also report the BLEU scores for each task.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.9739758372306824}]}, {"text": "While accuracy, being a stricter metric, considers a predicted answer to be correct only if it exactly matches the true answer, F-score and BLEU also give credit to predictions partially overlapping with the true answer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9993333220481873}, {"text": "F-score", "start_pos": 128, "end_pos": 135, "type": "METRIC", "confidence": 0.9981833100318909}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9991816878318787}]}], "tableCaptions": [{"text": " Table 1: Comparison between various RC datasets", "labels": [], "entities": []}, {"text": " Table 2: Performance of the preprocessing. Plot compression  is the % size of the extracted plot w.r.t the original plot size", "labels": [], "entities": [{"text": "Plot compression", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9494168162345886}]}, {"text": " Table 3: Performance of the SpanModel and GenModel on  the Span Test subset and the Full Test Set of the Self and  ParaphraseRC.", "labels": [], "entities": [{"text": "Span Test subset", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.7174117863178253}]}, {"text": " Table 4: Combined and Cross-Testing between Self and Para- phraseRC Dataset, by taking the best performing SpanModel  from", "labels": [], "entities": [{"text": "phraseRC Dataset", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.9257479608058929}]}, {"text": " Table 3.ParaRC is an abbreviation of ParaphraseRC", "labels": [], "entities": [{"text": "ParaphraseRC", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7111984491348267}]}]}