{"title": [], "abstractContent": [{"text": "Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools.", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9644559025764465}, {"text": "dependency parsing", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.703397199511528}]}, {"text": "We broaden En-glish dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating anew dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework.", "labels": [], "entities": [{"text": "En-glish dependency parsing", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.6875206430753072}, {"text": "social media African-American English (AAE)", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.6899316906929016}]}, {"text": "We describe our standards for handling Twitter-and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including anew data synthesis approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9654333591461182}, {"text": "data synthesis", "start_pos": 209, "end_pos": 223, "type": "TASK", "confidence": 0.7840207517147064}]}, {"text": "We analyze these methods' impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features.", "labels": [], "entities": [{"text": "AAE and Mainstream American English tweets", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.5573258002599081}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9750304222106934}]}, {"text": "Our annotated data and a parsing model are available at:", "labels": [], "entities": []}], "introductionContent": [{"text": "Language on Twitter diverges from well-edited Mainstream American English (MAE, also called Standard American English) in a number of ways, presenting significant challenges to current NLP tools.", "labels": [], "entities": []}, {"text": "It contains, among other phenomena, nonstandard spelling, punctuation, capitalization, and syntax, as well as Twitter-specific conventions such as hashtags, usernames, and retweet tokens.", "labels": [], "entities": []}, {"text": "Additionally, it contains an abundance of dialectal language, including African-American English (AAE), a dialect of American English spoken by millions of individuals, which contains lexical, phonological, and syntactic features not present in MAE.", "labels": [], "entities": []}, {"text": "Since standard English NLP tools are typically trained on well-edited MAE text, their performance is degraded on Twitter, and even more so for AAE tweets compared to MAE tweetsgaps exist for part-of-speech tagging, language identification, and dependency parsing.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 191, "end_pos": 213, "type": "TASK", "confidence": 0.7384930551052094}, {"text": "language identification", "start_pos": 215, "end_pos": 238, "type": "TASK", "confidence": 0.7564774453639984}, {"text": "dependency parsing", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.83272984623909}]}, {"text": "Expanding the linguistic coverage of NLP tools to include minority and colloquial dialects would help support equitable language analysis across sociolinguistic communities, which could help information retrieval, translation, or opinion analysis applications.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.7465939223766327}, {"text": "opinion analysis", "start_pos": 230, "end_pos": 246, "type": "TASK", "confidence": 0.665688619017601}]}, {"text": "For example, sentiment analysis systems ought to count the opinions of all types of people, whether they use standard dialects or not.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.9734871089458466}]}, {"text": "In this work, we broaden Universal Dependencies () parsing 1 to better handle social media English, in particular social media AAE.", "labels": [], "entities": []}, {"text": "First, we develop standards to handle Twitter-specific and AAE-specific features within Universal Dependencies 2.0 ( \u00a73), by selecting and annotating anew dataset of 500 tweets, 250 of which are in AAE.", "labels": [], "entities": []}, {"text": "Second, we evaluate several state-of-the-art dependency parsers, finding that, as expected, they perform poorly on our dataset relative to the UD English Treebank ( \u00a74).", "labels": [], "entities": [{"text": "UD English Treebank", "start_pos": 143, "end_pos": 162, "type": "DATASET", "confidence": 0.9530667662620544}]}, {"text": "Third, since the UD English Treebank contains substantial amounts of traditional MAE data for training, we investigate cross-domain training methods to improve Twitter AAE dependency parsing with no, or very little, in-domain labeled data, by using Twitter-specific taggers, embeddings, and a novel heuristic training data synthesis procedure.", "labels": [], "entities": [{"text": "UD English Treebank", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.9605666200319926}, {"text": "Twitter AAE dependency parsing", "start_pos": 160, "end_pos": 190, "type": "TASK", "confidence": 0.6334333121776581}]}, {"text": "This helps close some of the gap between MAE and AAE performance.", "labels": [], "entities": [{"text": "MAE", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8180657029151917}, {"text": "AAE", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9851873517036438}]}, {"text": "Finally, we provide an error analysis of the parsers' performance on AAE lexical and syntactic constructions in our dataset ( \u00a75.4).", "labels": [], "entities": [{"text": "AAE lexical and syntactic constructions", "start_pos": 69, "end_pos": 108, "type": "TASK", "confidence": 0.7352926135063171}]}], "datasetContent": [{"text": "Our dataset contains 500 tweets, with a total of 5,951 non-punctuation edges, sampled from the publicly available TwitterAAE corpus.", "labels": [], "entities": [{"text": "TwitterAAE corpus", "start_pos": 114, "end_pos": 131, "type": "DATASET", "confidence": 0.9596154987812042}]}, {"text": "Each tweet in that corpus is accompanied by a model's demographically-aligned topic model probabilities jointly inferred from Census demographics and word likelihood by, including the African-American and White topics.", "labels": [], "entities": [{"text": "word likelihood", "start_pos": 150, "end_pos": 165, "type": "METRIC", "confidence": 0.7300170361995697}]}, {"text": "We create a balanced sample to get a range of dialectal language, sampling 250 tweets from those where the African-American topic has at least 80% probability, and 250 from those where the White topic has at least 80% probability.", "labels": [], "entities": []}, {"text": "We refer to these two subcorpora as AA and WH; showed the former exhibits linguistic features typical of AAE.", "labels": [], "entities": [{"text": "AA", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.8917532563209534}]}, {"text": "The 250 AA tweets include many alternate spellings of common words that correspond to well-known phonological phenomena-including da, tha (the), dat, dhat (that), dis, dhis (this), ion, iont (I don't), ova (over), yo (your), dere, der (there), den, dhen (then), ova (over), and nall, null (no, nah)-where each of the mentioned italicized AAE terms appears in the AAE data, but never in the MAE data.", "labels": [], "entities": [{"text": "AAE data", "start_pos": 363, "end_pos": 371, "type": "DATASET", "confidence": 0.8775843977928162}, {"text": "MAE data", "start_pos": 390, "end_pos": 398, "type": "DATASET", "confidence": 0.9497529566287994}]}, {"text": "We examine these lexical variants more closely in \u00a75.4.", "labels": [], "entities": []}, {"text": "Across the AA tweets, 18.0% of tokens were not in a standard English dictionary, while the WH tweets' OOV rate was 10.7%.", "labels": [], "entities": [{"text": "AA tweets", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.8425936102867126}, {"text": "WH tweets'", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.7800298035144806}, {"text": "OOV rate", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9465664327144623}]}, {"text": "We further observe a variety of AAE syntactic phenomena in our AA tweets, several of which are described in \u00a73.2 and \u00a75.4.", "labels": [], "entities": [{"text": "AAE syntactic", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.8236591219902039}]}, {"text": "We considered a series of experiments within both a cross-domain scenario ( \u00a74.2.1), where we trained only on UD Treebank data, and an indomain scenario ( \u00a74.2.2) using small amounts of our labeled data.", "labels": [], "entities": [{"text": "UD Treebank data", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9288594126701355}]}, {"text": "We use the parsing systems' default hyperparameters (e.g. minibatch size and learning rate) and the default training/development split of the treebank (both systems perform early stopping based on development set performance).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results from cross-domain training set- tings (see  \u00a74.2.1).", "labels": [], "entities": []}, {"text": " Table 2: Results from in-domain training settings  (with the ARK Tagger setting, see  \u00a74.2.2).", "labels": [], "entities": [{"text": "ARK Tagger setting", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.8260582089424133}]}, {"text": " Table 3: AA and WH tweets' labeled attachment scores for UD Treebank-trained models (see  \u00a75.3 for  discussion); Gap is the WH \u2212 AA difference in LAS.", "labels": [], "entities": [{"text": "AA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9980899691581726}, {"text": "WH tweets' labeled attachment", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.4851146340370178}, {"text": "UD Treebank-trained", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.8497958779335022}, {"text": "WH \u2212 AA difference", "start_pos": 125, "end_pos": 143, "type": "METRIC", "confidence": 0.6805003434419632}]}, {"text": " Table 4: Recall by relation type under UDPipe's Morpho-Tagger and ARK Tagger settings (+syn- thetic+embeddings; (3) and (6) from Table 3;  \u00a75.3). Reduction is the reduction in performance gap  from the Morpho-Tagger setting to the ARK Tagger setting; bolded numbers indicate a gap reduction of  \u2265 10.0.", "labels": [], "entities": [{"text": "ARK Tagger", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.8785564303398132}, {"text": "ARK Tagger setting", "start_pos": 232, "end_pos": 250, "type": "DATASET", "confidence": 0.8360998233159384}]}, {"text": " Table 5: Examples of AAE syntactic phenomena and occurrence counts in the 250 AA and 250 WH  tweet sets.", "labels": [], "entities": [{"text": "AAE syntactic", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.8114316761493683}, {"text": "AA", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.956641435623169}]}]}