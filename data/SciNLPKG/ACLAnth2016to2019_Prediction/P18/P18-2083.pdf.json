{"title": [{"text": "PhraseCTM: Correlated Topic Modeling on Phrases within Markov Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans.", "labels": [], "entities": []}, {"text": "But these models are lack of the ability to capture the correlation structure among the discovered numerous topics.", "labels": [], "entities": []}, {"text": "We propose a novel topic model PhraseCTM and a two-stage method to find out the correlated topics at phrase level.", "labels": [], "entities": []}, {"text": "In the first stage, we train PhraseCTM, which models the generation of words and phrases simultaneously by linking the phrases and component words within Markov Random Fields when they are semantically coherent.", "labels": [], "entities": []}, {"text": "In the second stage, we generate the correlation of topics from PhraseCTM.", "labels": [], "entities": [{"text": "PhraseCTM", "start_pos": 64, "end_pos": 73, "type": "DATASET", "confidence": 0.8796463012695312}]}, {"text": "We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, topic modeling on phrases has been developed for providing more interpretable topics.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7566528022289276}]}, {"text": "They represent each topic as a list of phrases, which are easy to read for humans.", "labels": [], "entities": []}, {"text": "For example, the topic represented in \"grounding conductor, grounding wire, aluminum wiring, neutral ground, ...\" is easier to read than the topic with words \"ground, wire, use, power, cable, wires, ...\", although they are both about the topic of household electricity.", "labels": [], "entities": []}, {"text": "But when the number of topics grows, it's hard to review all the topics, even they are represented in phrases.", "labels": [], "entities": []}, {"text": "The correlation structure is introduced by CTM () to figure out the correlated relationship between topics and group the similar topics together.", "labels": [], "entities": []}, {"text": "And the correlated topics mined from the scientific papers, news corpus (, and social science data (, showed their practical utility on grasping the semantic meaning of text documents.", "labels": [], "entities": [{"text": "grasping the semantic meaning of text documents", "start_pos": 136, "end_pos": 183, "type": "TASK", "confidence": 0.7108832129410335}]}, {"text": "However, it's nontrivial to apply CTM directly on phrases.", "labels": [], "entities": []}, {"text": "The reasons are mainly due to two facts: (1) phrases are much less than words in each document; (2) similar to LDA, CTM doesn't perform well on short documents.", "labels": [], "entities": []}, {"text": "Therefore, CTM needs more contextual information to build a good enough model, rather than only using the extracted phrases.", "labels": [], "entities": [{"text": "CTM", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9335285425186157}]}, {"text": "To find out the correlated topics at phrase level, we take full advantage of contextual information about the phrases.", "labels": [], "entities": []}, {"text": "Firstly, the topic of a phrase in a document is highly related to the topics of other words and phrases in the same document.", "labels": [], "entities": []}, {"text": "Secondly, some phrases' meaning can be implied from their component words.", "labels": [], "entities": []}, {"text": "Taking a document in as an example, the phrase \"orbital vehicle\" shares the same topic as the word \"DC-X\" (a reusable spaceship), as well as its component words \"orbital\", and \"vehicle\", which are all about the topic of space exploration.", "labels": [], "entities": []}, {"text": "The assumption that the words within the same phrase tend to have the same latent topic is directly used by PhraseLDA ().", "labels": [], "entities": []}, {"text": "Note that not all the phrases always have the same topic as their component words (e.g., the newspaper Boston Globe) ().", "labels": [], "entities": [{"text": "Boston Globe)", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9225901563962301}]}, {"text": "It's difficult to distinguish the \"orbital vehicle\" type phrases from \"Boston Globe\" type phrases, but we can use the data-driven method to find out the semantically coherent ones by the NPMI metric, and put them in Markov Random Fields to align the topics of phrases and their component words.", "labels": [], "entities": [{"text": "Boston Globe\" type phrases", "start_pos": 71, "end_pos": 97, "type": "DATASET", "confidence": 0.9433952093124389}]}], "datasetContent": [{"text": "PhraseCTM is supposed to get benefits from two aspects: (1) generating high-quality phrase-level topics; (2) providing the correlation among phrase topics to help users to understand the underlying themes of a corpus.", "labels": [], "entities": []}, {"text": "To check the first claim, we compare PhraseCTM with existing topic models on phrases.", "labels": [], "entities": []}, {"text": "To evaluate the second claim, we design a user study to compare PhraseCTM with standard CTM that runs only on words.", "labels": [], "entities": []}, {"text": "We choose several public text corpora, including 20Newsgroups, subsets of English Wikipedia, a subset of PubMed Abstracts ().", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 74, "end_pos": 91, "type": "DATASET", "confidence": 0.894633948802948}]}, {"text": "Due to efficiency problem, we do not test on the whole Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9608441889286041}]}, {"text": "We construct the Mathematics, Chemistry, and Argentina subsets of English Wikipedia as ( . For each corpus, we extract the phrases by the implementation 1 of AutoPhrase (, and build the semantically coherent links as subsection 3.1.", "labels": [], "entities": [{"text": "Mathematics, Chemistry, and Argentina subsets of English Wikipedia", "start_pos": 17, "end_pos": 83, "type": "DATASET", "confidence": 0.599819365143776}]}, {"text": "More specifically, in phrase extraction process, we set the minimum support as 5, and leave other parameters in AutoPhrase as its suggestion.", "labels": [], "entities": [{"text": "phrase extraction process", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8880056540171305}]}, {"text": "In average, each document of the resulted 20Newsgroups only contains 2.7 phrases while 72.3 words, showing that phrases are much less than words.", "labels": [], "entities": []}, {"text": "More statistics about the datasets are shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of the datasets. In average, phrases appear more sparse than words. Phrases are  extracted by AutoPhrase (Shang et al., 2018).", "labels": [], "entities": []}, {"text": " Table 2: Human time consumption on topic label- ing for correlated topics generated by CTM and  PhraseCTM, measured in minutes.", "labels": [], "entities": []}]}