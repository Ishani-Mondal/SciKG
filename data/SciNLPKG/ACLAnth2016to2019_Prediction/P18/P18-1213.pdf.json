{"title": [{"text": "Modeling Naive Psychology of Characters in Simple Commonsense Stories", "labels": [], "entities": [{"text": "Modeling Naive Psychology of Characters", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8971635818481445}]}], "abstractContent": [{"text": "Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people's mental states-a capability that is trivial for humans but remarkably hard for machines.", "labels": [], "entities": []}, {"text": "To facilitate research addressing this challenge, we introduce anew annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions.", "labels": [], "entities": []}, {"text": "Our work presents anew large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding a story requires reasoning about the causal links between the events in the story and the mental states of the characters, even when those relationships are not explicitly stated.", "labels": [], "entities": []}, {"text": "As shown by the commonsense story cloze shared task (, this reasoning is remarkably hard for both statistical and neural machine readers -despite being trivial for humans.", "labels": [], "entities": []}, {"text": "This stark performance gap between humans and machines is not surprising as most powerful language models have been designed to effectively learn local fluency patterns.", "labels": [], "entities": []}, {"text": "Consequently, they generally lack the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters' mental states or predicting their plausible next actions.", "labels": [], "entities": []}, {"text": "In this paper, we construct anew annotation formalism to densely label commonsense short stories () in terms of the mental states of the characters.", "labels": [], "entities": []}, {"text": "The resultThe band instructor told the band to start playing.", "labels": [], "entities": []}, {"text": "He often stopped the music when players were off-tone.", "labels": [], "entities": []}, {"text": "They grew tired and started playing worse after awhile.", "labels": [], "entities": []}, {"text": "The instructor was furious and threw his chair.", "labels": [], "entities": []}], "datasetContent": [{"text": "Cost The tasks corresponding to the theory category assignments are the hardest and most expensive in the pipeline (\u21e0$4 per story).", "labels": [], "entities": [{"text": "Cost", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.951220691204071}]}, {"text": "Therefore, we obtain theory category labels only fora third of our annotated stories, which we assign to the development and test sets.", "labels": [], "entities": []}, {"text": "The training data is annotated with a shortened pipeline with only open text descriptions of motivations and emotional reactions from two workers (\u21e0$1 per story).", "labels": [], "entities": []}, {"text": "Scale Our dataset to date includes a total of 300k low-level annotations for motivation and emotion across 15,000 stories (randomly selected from the ROC story training set).", "labels": [], "entities": [{"text": "ROC story training set", "start_pos": 150, "end_pos": 172, "type": "DATASET", "confidence": 0.9212634414434433}]}, {"text": "It covers over 150,000 character-line pairs, in which 56k character-line pairs have an annotated motivation and 105k have an annotated change in emotion (i.e. a label other than none).", "labels": [], "entities": []}, {"text": "shows the breakdown across training, development, and test splits.", "labels": [], "entities": []}, {"text": "shows the frequency of different labels being selected for motivational and emotional categories in cases with positive change.", "labels": [], "entities": []}, {"text": "Agreements For quality control, we removed workers who consistently produced low-quality work, as discussed in the Appendix.", "labels": [], "entities": [{"text": "quality control", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.7039196491241455}]}, {"text": "In the categorization sets (Maslow, Reiss and Plutchik), we compare the performance of annotators by treating each individual category as a binary label (1    if they included the category in their set of responses) and averaging the agreement per category.", "labels": [], "entities": []}, {"text": "For Plutchik scores, we count 'moderately associated' ratings as agreeing with 'highly' associated' ratings.", "labels": [], "entities": []}, {"text": "The percent agreement and Krippendorff's alpha are shown in.", "labels": [], "entities": [{"text": "Krippendorff's alpha", "start_pos": 26, "end_pos": 46, "type": "METRIC", "confidence": 0.7154389023780823}]}, {"text": "We also compute the percent agreement between the individual annotations and the majority labels.", "labels": [], "entities": []}, {"text": "These scores are difficult to interpret by themselves, however, as annotator agreement in our categorization system has a number of properties that are not accounted for by these metrics (disagreement preferences -joy and trust are closer than joy and anger -that are difficult to quantify in a principled way, hierarchical categories map-ping Reiss subcategories from Maslow categories, skewed category distributions that inflate PPA and deflate KA scores, and annotators that could select multiple labels for the same examples).", "labels": [], "entities": []}, {"text": "To provide a clearer understanding of agreement within this dataset, we create aggregated confusion matrices for annotator pairs.", "labels": [], "entities": []}, {"text": "First, we sum the counts of combinations of answers between all paired annotations (excluding none labels).", "labels": [], "entities": []}, {"text": "If an annotator selected multiple categories, we split the count uniformly among the selected categories.", "labels": [], "entities": []}, {"text": "We compute NPMI over the total confusion matrix.", "labels": [], "entities": []}, {"text": "In, we show the NPMI confusion matrix for motivational categories.", "labels": [], "entities": [{"text": "NPMI confusion matrix", "start_pos": 16, "end_pos": 37, "type": "METRIC", "confidence": 0.5524614651997884}]}, {"text": "In the motivation annotations, we find the highest scores on the diagonal (i.e., Reiss agreement), with most confusions occurring between Reiss motives in the same Maslow category (outlined black in).", "labels": [], "entities": []}, {"text": "Other disagreements generally involve Reiss subcategories that are thematically similar, such as serenity (mental relaxation) and rest (physical relaxation).", "labels": [], "entities": []}, {"text": "We provide this analysis for Plutchik categories in the appendix, finding high scores along the diagonal with disagreements typically occurring between categories in a \"positive emotion\" cluster (joy, trust) or a \"negative emotion\" cluster (anger, disgust,sadness).", "labels": [], "entities": []}, {"text": "State Classification We show results on the test set for categorizing Maslow, Reiss, and Plutchik states in.", "labels": [], "entities": []}, {"text": "Despite the difficulty of the task, all models outperform the random baseline.", "labels": [], "entities": []}, {"text": "Interestingly, the performance boost from adding entity-specific contextual information (i.e., not ablating h c ) indicates that the models learn to condition on a character's previous experience to classify its mental state at the current time step.", "labels": [], "entities": []}, {"text": "This effect can be seen in a story about a man whose flight is cancelled.", "labels": [], "entities": []}, {"text": "The model without context predicts the same emotional reactions for the man, his wife and the pilot, but with context correctly predicts that the pilot will not have a reaction while predicting that the man and his wife will feel sad.", "labels": [], "entities": []}, {"text": "For the CNN, LSTM, REN, and NPN models, we also report results from pretraining encoder parameters using the free response annotations from the training set.", "labels": [], "entities": [{"text": "CNN", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.8639243841171265}, {"text": "REN", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7032558917999268}]}, {"text": "This pretraining offers a clear performance boost for all models on all three prediction tasks, showing that the parameters of the encoder can be pretrained on auxiliary tasks providing emotional and motivational state signal.", "labels": [], "entities": []}, {"text": "The best performing models in each task are most effective at predicting Maslow physiological needs, Reiss food motives, and Plutchik reactions of joy.: Vector average and extrema scores for generation of annotation explanations related to food (and physiological needs generally) maybe because they involve a more limited and concrete set of actions such as eating or cooking.", "labels": [], "entities": [{"text": "extrema", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9495581984519958}]}, {"text": "Annotation Classification shows that a simple model can learn to map open text responses to categorical labels.", "labels": [], "entities": [{"text": "Annotation Classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.692547932267189}]}, {"text": "This further supports our hypothesis that pretraining a classification model on the free-response annotations could be helpful in boosting performance on the category prediction.", "labels": [], "entities": [{"text": "category prediction", "start_pos": 158, "end_pos": 177, "type": "TASK", "confidence": 0.7449733912944794}]}, {"text": "Explanation Generation Finally, we provide results for the task of generating explanations of motivations and emotions in.", "labels": [], "entities": []}, {"text": "Because the explanations are closely tied to emotional and motivation states, the randomly selected explanation can often be close in embedding space to the reference explanations, making the random baseline fairly competitive.", "labels": [], "entities": []}, {"text": "However, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Annotated data statistics for each dataset", "labels": [], "entities": []}, {"text": " Table 2: Agreement Statistics (PPA = Pairwise  percent agreement of worker responses per binary  category, KA= Krippendorff's Alpha)", "labels": [], "entities": []}, {"text": " Table 4: F1 scores of predicting correct category  labels from free response annotations", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9994780421257019}, {"text": "predicting correct category  labels from free response annotations", "start_pos": 23, "end_pos": 89, "type": "TASK", "confidence": 0.8397496715188026}]}, {"text": " Table 5: Vector average and extrema scores for  generation of annotation explanations", "labels": [], "entities": []}]}