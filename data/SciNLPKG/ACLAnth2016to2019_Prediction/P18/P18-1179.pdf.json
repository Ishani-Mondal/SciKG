{"title": [], "abstractContent": [{"text": "A DAG automaton is a formal device for manipulating graphs.", "labels": [], "entities": []}, {"text": "By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel DAG transducer to perform graph-to-program transformation.", "labels": [], "entities": [{"text": "graph-to-program transformation", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.7244820594787598}]}, {"text": "The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures.", "labels": [], "entities": []}, {"text": "By executing such a program , we can easily get a surface string.", "labels": [], "entities": []}, {"text": "Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.8102164069811503}]}, {"text": "Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 117, "end_pos": 129, "type": "METRIC", "confidence": 0.9809556603431702}]}, {"text": "This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR;), Elementary Dependency Structure (EDS;) and Depedendency-based Minimal Recursion Semantics (DMRS;.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.8017339408397675}, {"text": "surface realization", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.774494081735611}, {"text": "Abstract Meaning Representation (AMR", "start_pos": 176, "end_pos": 212, "type": "TASK", "confidence": 0.7152939081192017}]}, {"text": "Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees.", "labels": [], "entities": []}, {"text": "Two such formalisms have recently been proposed and applied for NLP.", "labels": [], "entities": []}, {"text": "One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG;).", "labels": [], "entities": [{"text": "graph grammar", "start_pos": 7, "end_pos": 20, "type": "TASK", "confidence": 0.7543320655822754}, {"text": "Hyperedge Replacement Grammar (HRG", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7670392394065857}]}, {"text": "The other is DAG automata, originally studied by and extended by.", "labels": [], "entities": []}, {"text": "In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems.", "labels": [], "entities": [{"text": "DAG transducers", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.8947911560535431}, {"text": "natural language generation (NLG)", "start_pos": 107, "end_pos": 140, "type": "TASK", "confidence": 0.8062441150347391}]}, {"text": "The meaning representation studied in this work is what we call type-logical semantic graphs, i.e. semantic graphs grounded under type-logical semantics, one dominant theoretical framework for modeling natural language semantics.", "labels": [], "entities": []}, {"text": "In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments).", "labels": [], "entities": []}, {"text": "In the same spirit, generalized quantifiers, prepositions and function words in many languages other than English are also analyzed as higher-order functions.", "labels": [], "entities": []}, {"text": "Accordingly, all the linguistic elements are treated as roots in type-logical semantic graphs, such as EDS and DMRS.", "labels": [], "entities": [{"text": "EDS", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.7449874877929688}]}, {"text": "This makes the typological structure quite flat rather than hierachical, which is an essential distinction between natural language semantics and syntax.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by.", "labels": [], "entities": []}, {"text": "Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation.", "labels": [], "entities": [{"text": "AMR-to-text generation", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.9825102984905243}]}, {"text": "This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics.", "labels": [], "entities": []}, {"text": "Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and reported no result of practical generation.", "labels": [], "entities": [{"text": "graph recognition", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7077015191316605}]}, {"text": "It is still unknown to what extent a DAG transducer suits realistic NLG.", "labels": [], "entities": []}, {"text": "The design for string and tree transducers () focuses on not only the logic of the computation fora new data structure, but also the corresponding control flow.", "labels": [], "entities": []}, {"text": "This is very similar the imperative programming paradigm: implementing algorithms with exact details in explicit steps.", "labels": [], "entities": []}, {"text": "This design makes it very difficult to transform a type-logical semantic graph into a string, due to the fact their internal structures are highly diverse.", "labels": [], "entities": []}, {"text": "We borrow ideas from declarative programming, another programming paradigm, which describes what a program must accomplish, rather than how to accomplish it.", "labels": [], "entities": []}, {"text": "We propose a novel DAG transducer to perform graphto-program transformation ( \u00a73).", "labels": [], "entities": [{"text": "graphto-program transformation", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.7135864943265915}]}, {"text": "The input of our transducer is a semantic graph, while the output is a program licensed by a declarative programming language rather than linguistic structures.", "labels": [], "entities": []}, {"text": "By executing such a program, we can easily get a surface string.", "labels": [], "entities": []}, {"text": "This idea can be extended to other types of linguistic structures, e.g. syntactic trees or semantic representations of another language.", "labels": [], "entities": []}, {"text": "We conduct experiments on richly detailed semantic annotations licensed by English Resource Grammar).", "labels": [], "entities": [{"text": "English Resource Grammar", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.8944762746493021}]}, {"text": "We introduce a principled method to derive transduction rules from DeepBank (.", "labels": [], "entities": []}, {"text": "Furthermore, we introduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph.", "labels": [], "entities": []}, {"text": "Taking EDS graphs, a variable-free ERS format, as input, our NLG system achieves a BLEU-4 score of 68.07.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9825641214847565}]}, {"text": "On average, it produces more than 5 sentences in a second on an x86 64 GNU/Linux platform with two Intel Xeon E5-2620 CPUs.", "labels": [], "entities": []}, {"text": "Since the data for experiments is newswire data, i.e. WSJ sentences from PTB (, the input graphs are quite large on average.", "labels": [], "entities": [{"text": "PTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.5331020951271057}]}, {"text": "The remarkable accuracy, efficiency and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9990239143371582}, {"text": "resolve NLG", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.5404723137617111}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracy (BLEU-4 score) and coverage  of different systems. I denotes transduction only  using induced rules; I+E denotes transduction us- ing both induced and extended rules; I+E+D de- notes transduction using all kinds of rules. DFS- NN is a rough implementation of Konstas et al.  (2017) but with the EDS data, while AMR-NN  includes the results originally reported by Kon- stas et al., which are evaluated on the AMR data.  AMR-NRG includes the results obtained by a syn- chronous graph grammar (Song et al., 2017).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994274377822876}, {"text": "BLEU-4", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.994961142539978}, {"text": "EDS data", "start_pos": 314, "end_pos": 322, "type": "DATASET", "confidence": 0.6994375735521317}, {"text": "AMR data", "start_pos": 427, "end_pos": 435, "type": "DATASET", "confidence": 0.9336443245410919}]}, {"text": " Table 3: Efficiency of our NL generator.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9946551322937012}]}]}