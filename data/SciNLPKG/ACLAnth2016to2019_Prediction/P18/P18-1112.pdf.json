{"title": [{"text": "Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectiv-ity, or topic.", "labels": [], "entities": []}, {"text": "Through systematic comparative analyses, we establish this to be the case indeed.", "labels": [], "entities": []}, {"text": "Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.9152837097644806}]}], "introductionContent": [{"text": "Distributional analysis methods such as Word2Vec ( and) have been critical for the success of many large-scale natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "Distributional analysis", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7922920286655426}, {"text": "Word2Vec", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.94427490234375}]}, {"text": "These methods employ distributional hypothesis (i.e., words used in the same contexts tend to have similar meaning) to derive distributional meaning via context prediction tasks and produce dense word embeddings.", "labels": [], "entities": []}, {"text": "While there have been active and ongoing research on improving word embedding methods (see Section 5), there is a relative dearth of study on the impact that an input corpus may have on the quality of the word embeddings.", "labels": [], "entities": []}, {"text": "The previous preoccupation centers around corpus size, i.e., a larger corpus is perceived to be richer in statistical information.", "labels": [], "entities": []}, {"text": "For instance, popular corpora include Wikipedia, Common Crawl, and Google News.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.9539088010787964}, {"text": "Google News", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.8818324208259583}]}, {"text": "We postulate that there maybe variations across corpora owing to factors that affect language use.", "labels": [], "entities": []}, {"text": "Intuitively, the many things we write (a work email, a product review, an academic publication, etc.) may each involve certain stylistic, syntactic, and lexical choices, resulting in meaningfully different distributions of word cooccurrences.", "labels": [], "entities": []}, {"text": "Consequently, such factors maybe encoded in the word embeddings, and input corpora maybe differentially informative towards various NLP tasks.", "labels": [], "entities": []}, {"text": "In this work, we are interested in the notion of subjectivity.", "labels": [], "entities": []}, {"text": "Some NLP tasks, such as sentiment classification, revolve around subjective expressions of likes or dislikes.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.9719810485839844}]}, {"text": "Others, such as topic classification, revolve around more objective elements of whether a document belongs to a topic (e.g., science, politics).", "labels": [], "entities": [{"text": "topic classification", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8165338039398193}]}, {"text": "Our central hypothesis is that word embeddings learnt from input corpora of contrasting levels of subjectivity perform differently when classifying sentences by sentiment, subjectivity, or topic.", "labels": [], "entities": []}, {"text": "As the first contribution, we outline an experimental scheme to explore this hypothesis in Section 2, and conduct a series of controlled experiments in Section 3 establishing that there exists a meaningful difference between word embeddings derived from objective vs. subjective corpora.", "labels": [], "entities": []}, {"text": "We further systematically investigate factors that could potentially explain the differences.", "labels": [], "entities": []}, {"text": "Upon discovering from the investigation that sentiment words play a particularly important role in subjectivity-sensitive NLP tasks, such as sentiment classification, as the second contribution, in Section 4 we develop SentiVec, a novel word embedding method infused with information from lexical resources such as a sentiment lexicon.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.9167076647281647}]}, {"text": "We further identify two alternative lexical objectives: Logistic SentiVec based on discriminative logistic regression, and Spherical SentiVec based on soft clustering effect of von Mises-Fisher distributions.", "labels": [], "entities": []}, {"text": "In Section 6, the proposed word embeddings show evident improvements on sentiment classification, as compared to the base model Word2Vec and other baselines using the same lexical resource.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.9436696767807007}, {"text": "Word2Vec", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9531050324440002}]}], "datasetContent": [{"text": "To compare word embeddings, we need a common yardstick.", "labels": [], "entities": []}, {"text": "It is difficult to define an inherent quality to word embeddings.", "labels": [], "entities": []}, {"text": "Instead, we put them through several evaluation tasks that can leverage word embeddings and standardize their formulations as binary classification tasks.", "labels": [], "entities": []}, {"text": "To boil the comparisons down to the essences of word embeddings (which is our central focus), we rely on standardized techniques so as to attribute as much of the differences as possible to the word embeddings.", "labels": [], "entities": []}, {"text": "We use logistic regression for classification, and represent a text snippet (e.g., a sentence) in the feature space as the average of the word embeddings of tokens in the snippet (ignoring out-ofvocabulary tokens).", "labels": [], "entities": []}, {"text": "The evaluation metric is the average accuracy from 10-fold cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994617104530334}]}, {"text": "There are three evaluation tasks of varying degrees of hypothetical subjectivity, as outlined below.", "labels": [], "entities": []}, {"text": "Each may involve multiple datasets.", "labels": [], "entities": []}, {"text": "Sentiment Classification Task This task classifies a sentence into either positive or negative.", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8936832547187805}]}, {"text": "We use two groups of datasets as follows.", "labels": [], "entities": []}, {"text": "The first group consists of 24 datasets from UCSD Amazon product data 3 corresponding to various product categories.", "labels": [], "entities": [{"text": "UCSD Amazon product data", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8763173371553421}]}, {"text": "Each review has a rating from 1 to 5, which is transformed into positive (ratings 4 or 5) or negative (ratings 1 or 2) class.", "labels": [], "entities": []}, {"text": "For each dataset respectively, we sample 5000 sentences each from the positive and negative reviews.", "labels": [], "entities": []}, {"text": "Note that these sentences used for this evaluation task have not participated in the generation of word embeddings.", "labels": [], "entities": []}, {"text": "Due to space constraint, inmost cases we present the average accuracy across the datasets, but where appropriate we enumerate the results for each dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9985685348510742}]}, {"text": "The second is Cornell's sentence polarity dataset v1.0 4 (Pang and), made up of 5331 each of positive and negative sentences from Rotten Tomatoes movie reviews.", "labels": [], "entities": [{"text": "Cornell's sentence polarity dataset v1.0", "start_pos": 14, "end_pos": 54, "type": "DATASET", "confidence": 0.7256143738826116}, {"text": "Pang", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7732942700386047}]}, {"text": "The inclusion of this out-of-domain evaluation dataset is useful for examining whether the performance of word embeddings from the Subjective Corpus on the first group above may inadvertently be affected by indomain advantage arising from its Amazon origin.", "labels": [], "entities": []}, {"text": "Subjectivity Classification Task This task classifies a sentence into subjective or objective.", "labels": [], "entities": [{"text": "Subjectivity Classification", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8639196157455444}]}, {"text": "The dataset is Cornell's subjectivity dataset v1.0 5 , consisting of 5000 subjective sentences derived from Rotten Tomatoes (RT) reviews and 5000 objective sentences derived from IMDB plot summaries ().", "labels": [], "entities": [{"text": "Cornell's subjectivity dataset v1.0 5", "start_pos": 15, "end_pos": 52, "type": "DATASET", "confidence": 0.9347345232963562}, {"text": "Rotten Tomatoes (RT) reviews", "start_pos": 108, "end_pos": 136, "type": "DATASET", "confidence": 0.8032955328623453}]}, {"text": "This task is probably less sensitive to the subjectivity within word embeddings than sentiment classification, as determining whether a sentence is subjective or objective should ideally bean objective undertaking.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.8801449835300446}]}, {"text": "Topic Classification Task We use the 20 Newsgroups dataset 6 (\"bydate\" version), whereby the newsgroups are organized into six subject matter groupings.", "labels": [], "entities": [{"text": "Topic Classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8556119501590729}, {"text": "Newsgroups dataset 6", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.8203782836596171}]}, {"text": "We extract the message body and split them into sentences.", "labels": [], "entities": []}, {"text": "Each group's sentences then form the in-topic class, and we randomly sample an equivalent number of sentences from the remaining newsgroups to form the out-of-topic class.", "labels": [], "entities": []}, {"text": "This results in six datasets, each corresponding to a binary classification task.", "labels": [], "entities": []}, {"text": "In most cases, we present the average results, and where appropriate we enumerate the results for each dataset.", "labels": [], "entities": []}, {"text": "Hypothetically, this task is the least affected by the subjectivity within word embeddings.", "labels": [], "entities": []}, {"text": "The objective of experiments is to study the efficacy of Logistic SentiVec and Spherical SentiVec word embeddings on the aforementioned text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.7818984886010488}]}, {"text": "One natural baseline is Word2Vec, as SentiVec subsumes its context prediction objective, while further incorporating lexical category prediction.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.9282017946243286}, {"text": "context prediction", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7199993133544922}, {"text": "lexical category prediction", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.7313335339228312}]}, {"text": "We include two other baselines that can leverage the same lexical resource but in manners different from SentiVec, namely: Retrofitting (Faruqui et al., 2015) and Refining (.", "labels": [], "entities": [{"text": "Refining", "start_pos": 163, "end_pos": 171, "type": "TASK", "confidence": 0.9562537670135498}]}, {"text": "For these methods, we generate their word embeddings based on Setup III (see Section 3).", "labels": [], "entities": []}, {"text": "All the methods were run multiple times with various hyperparameters, optimized via grid-search; for each we present the best performing setting.", "labels": [], "entities": []}, {"text": "First, we discuss the sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.9538644353548685}]}, {"text": "shows the unfolded results for the 24 classification datasets of Amazon, as well as for Rotten Tomatoes.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.8100709319114685}, {"text": "Rotten Tomatoes", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.8911696970462799}]}, {"text": "For each classification dataset (row), and for the Objective and Subjective embedding corpora respectively, the best word embedding methods are shown in bold.", "labels": [], "entities": []}, {"text": "An asterisk indicates statistically significant 8 results at 5% in comparison to Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9732677340507507}]}, {"text": "Both SentiVec variants outperform Word2Vec in the vast majority of the cases.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9548446536064148}]}, {"text": "The degree of outperformance is higher for the Objective than the Subjective word embeddings.", "labels": [], "entities": []}, {"text": "This is a reasonable trend given our previous findings in Section 3.", "labels": [], "entities": []}, {"text": "As the Objective Corpus encodes less information than the Subjective Corpus for sentiment classification, the former is more likely to benefit from the infusion of sentiment information from additional lexical resources.", "labels": [], "entities": [{"text": "Objective Corpus", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.6856591999530792}, {"text": "sentiment classification", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.9657796919345856}]}, {"text": "Note that the sentiment infusion into the word embeddings comes from separate lexical resources, and does not involve any sentiment classification label.", "labels": [], "entities": []}, {"text": "SentiVec also outperforms the two baselines that benefit from the same lexical resources.", "labels": [], "entities": []}, {"text": "Retrofitting does not improve upon Word2Vec, with the two embeddings essentially indistinguishable (the difference is only noticeable at the second decimal point).", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.938875138759613}]}, {"text": "Refining makes the word embeddings perform worse on the sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9379730522632599}]}, {"text": "One possible explanation is that Refining normally requires fine-grained labeled lexicon, where the words are scored w.r.t. the sentiment scale, whereas we use sentiment lexicon of two labels (i.e., positive or negative).", "labels": [], "entities": [{"text": "Refining", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.9770479202270508}]}, {"text": "SentiVec accepts coarse-grained sentiment lexicons, and potentially could be extended to deal with fine-grained labels.", "labels": [], "entities": []}, {"text": "As previously alluded to, topic and subjectivity classifications are less sensitive to the subjectivity within word embeddings than sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.7694589793682098}]}, {"text": "One therefore would not expect much, if any, performance gain from infusion of sentiment information.", "labels": [], "entities": []}, {"text": "However, such infusion should not subtract or harm the quality of word embeddings either.", "labels": [], "entities": []}, {"text": "shows that the unfolded results for topic classification on the six datasets, and the result for subjectivity classification are similar across methods.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7810933291912079}, {"text": "subjectivity classification", "start_pos": 97, "end_pos": 124, "type": "TASK", "confidence": 0.7302682250738144}]}, {"text": "Neither the SentiVec variants, nor Retrofitting and Refining, change the subjectivity and topic classification capabilities much, which means that the used sentiment lexicon is targeted only at the sentiment subspace of embeddings.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.6963325142860413}]}, {"text": "Illustrative Changes in Embeddings To give more insights on the difference between SentiVec and Word2Vec, we show \"flower\" diagrams in for Logistic SentiVec and for Spherical SentiVec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9393696784973145}]}, {"text": "Each is associated with a reference word (e.g., good for, and indicates relative changes in cosine distances between the reference word and the testing words surrounding the \"flower\".", "labels": [], "entities": []}, {"text": "Every testing word is associated with a \"petal\" or black axis extending from the center of the circle.", "labels": [], "entities": []}, {"text": "The \"petal\" length is proportional to the relative distance change in two word embeddings: \u03ba = d SentiV ec (w ref ,w testing ) d word2vec (w ref ,w testing ) , where d SentiV ec and d word2vec are cosine distances between reference w ref and testing w testing words in SentiVec and Word2Vec embeddings correspondingly.", "labels": [], "entities": []}, {"text": "If the distance remains unchanged (\u03ba = 1), then the \"petal\" points at the circumference; if the reference and testing words are closer in the SentiVec embedding than they are in Word2Vec (\u03ba < 1), the \"petal\" lies inside the circle; when the distance increases (\u03ba > 1), the \"petal\" goes beyond the circle.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 178, "end_pos": 186, "type": "DATASET", "confidence": 0.9509767889976501}]}, {"text": "The diagrams are presented for Objective Embeddings . We use three reference words: good (positive), bad (negative), time (neutral); as well as three groups of testing words: green for words randomly sampled from positive lexicon (Sector I-II), red for words randomly sampled from negative lexicon (Sector II-III), and gray for frequent neutral common nouns (Sector III-I).", "labels": [], "entities": []}, {"text": "shows changes produced by Logistic SentiVec.", "labels": [], "entities": []}, {"text": "For the positive reference word, the average distance to the green words is shortened, whereas the distance to the red words increases.", "labels": [], "entities": []}, {"text": "The reverse is observed for the negative reference word.", "labels": [], "entities": []}, {"text": "This observation complies with the lexical objective (7) of Logistic SentiVec, which aims to separate the words of two different classes.", "labels": [], "entities": []}, {"text": "Note that the gray words suffer only moderate change with respect to positive and negative reference words.", "labels": [], "entities": []}, {"text": "For the neutral reference word, the distances are only moderately affected across all testing groups.", "labels": [], "entities": []}, {"text": "shows that Spherical SentiVec tends to make embeddings more compact than Logistic SentiVec.", "labels": [], "entities": []}, {"text": "As the former's lexical objective (9) is designed for clustering, but not for separation, we look at the comparative strength of the clustering effect on the testing words.", "labels": [], "entities": []}, {"text": "For the positive reference word, the largest clustering effect is achieved for the green words.", "labels": [], "entities": []}, {"text": "For the negative reference word), as expected, the red words are affected the most.", "labels": [], "entities": []}, {"text": "The gray words suffer the least change for all the reference words.", "labels": [], "entities": []}, {"text": "In summary, SentiVec effectively provides an advantage for subjectivity-sensitive task such as sentiment classification, while not harming the performance of other text classification tasks.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.9633317291736603}, {"text": "text classification tasks", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.7810924251874288}]}], "tableCaptions": [{"text": " Table 1: Controlled comparison of Objective and Subjective corpora", "labels": [], "entities": []}, {"text": " Table 3: With and without sentiment", "labels": [], "entities": []}, {"text": " Table 4: Comparison of Sentiment-Infused Word Embeddings on Sentiment Classification Task", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 61, "end_pos": 85, "type": "TASK", "confidence": 0.7522951066493988}]}, {"text": " Table 5: Comparison of Word Embeddings on Subjectivity and Topic Classification Tasks", "labels": [], "entities": [{"text": "Topic Classification Tasks", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.7786491612593333}]}]}