{"title": [{"text": "Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication", "labels": [], "entities": [{"text": "Compositional Language Modeling", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7640444040298462}, {"text": "Icon-Based Augmentative and Alternative Communication", "start_pos": 36, "end_pos": 89, "type": "TASK", "confidence": 0.8871549487113952}]}], "abstractContent": [{"text": "Icon-based communication systems are widely used in the field of Augmentative and Alternative Communication.", "labels": [], "entities": [{"text": "Augmentative and Alternative Communication", "start_pos": 65, "end_pos": 107, "type": "TASK", "confidence": 0.66063541918993}]}, {"text": "Typically , icon-based systems have lagged behind word-and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models.", "labels": [], "entities": []}, {"text": "We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Individuals who experience speech and language impairments often are helped by Augmentative and Alternative Communication (AAC) techniques that facilitate the expression or comprehension of spoken or written language; American Speech Language Hearing.", "labels": [], "entities": [{"text": "American Speech Language Hearing", "start_pos": 218, "end_pos": 250, "type": "DATASET", "confidence": 0.6931552141904831}]}, {"text": "Impairments may result from developmental disorders affecting speech and language (Cerebral Palsy, Down Syndrome, some forms of Autism Spectrum Disorder, etc.), or they maybe caused by injury (stroke, traumatic brain injury, neurodegenerative diseases such as ALS, etc.).", "labels": [], "entities": [{"text": "Impairments", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9288451075553894}]}, {"text": "AAC interventions can take many forms, but a common goal is to provide users with away to select symbols (words, phrases, etc.) for purposes of communication.", "labels": [], "entities": [{"text": "AAC", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9148264527320862}]}, {"text": "The field of AAC groups interventions into \"low-technology\" (i.e., printed) or \"high-technology\" (i.e., computerized) devices; both are commonly used, and there area number of factors that go in to the decision of which device to use.", "labels": [], "entities": [{"text": "AAC", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9736843109130859}]}, {"text": "In some cases, devices produce speech (or written language) based on those selections, whereas in other cases, the goal of the device is to support a user in producing their own speech.", "labels": [], "entities": []}, {"text": "In some cases, the unit of selection maybe icon-based rather than word-or character-based.", "labels": [], "entities": []}, {"text": "This is particularly common in devices used by children or individuals with impaired literacy, but is also common in adult use.", "labels": [], "entities": []}, {"text": "Icon-based systems can have higher selection speeds, and can be easier for individuals with neuromuscular impairments to operate; there area wide variety of symbol sets and symbol-based communication systems used.", "labels": [], "entities": []}, {"text": "Computerized text-based AAC devices often include some form of word prediction using a language model (see) for overviews of this area).", "labels": [], "entities": [{"text": "word prediction", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.7219628691673279}]}, {"text": "Icon-based systems often do not employ predictive features.", "labels": [], "entities": []}, {"text": "In part, this is because they typically rely on direct-selection or other input modalities; another barrier, is alack of relevant linguistic training data.", "labels": [], "entities": []}, {"text": "To our knowledge, there are no corpora of language produced using icon-based AAC systems.", "labels": [], "entities": []}, {"text": "In the present work, we apply modern language modeling techniques to a large-vocabulary icon set commonly used in AAC applications, but for which we have no in-domain (or even invocabulary) training data.", "labels": [], "entities": [{"text": "AAC", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9142084717750549}]}, {"text": "We are building our model as support fora brain-computer interface.", "labels": [], "entities": []}, {"text": "In this input modality, the user has very limited control over item selection, making accurate language modeling critical.", "labels": [], "entities": []}, {"text": "We propose a method to generate language models and evaluate their performance by experimenting with a process to generate a trainable language modeling corpus.", "labels": [], "entities": []}, {"text": "We also share an experi-1 One notable exception to this trend is the system used in SymbolPath (, which uses semantic frames to attempt non-sequential symbol prediction.", "labels": [], "entities": [{"text": "symbol prediction", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7602266073226929}]}, {"text": "This work, however, was limited to a specific and small icon set.", "labels": [], "entities": []}, {"text": "mental setup fora new language modeling architecture.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: \u2022 A proposed approach to synthesize a pseudocorpus with which to learn language models from a corpus-less symbol set \u2022 An experimental evaluation of the impact of various pieces of our corpus synthesis methodology on icon prediction accuracy \u2022 A preliminary attempt to apply a novel language model architecture suitable for iconbased, open-vocabulary AAC applications", "labels": [], "entities": [{"text": "icon prediction", "start_pos": 254, "end_pos": 269, "type": "TASK", "confidence": 0.7424404621124268}, {"text": "accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.6839731931686401}]}], "datasetContent": [{"text": "The Symbolstix icon set is used in several commercial AAC applications.", "labels": [], "entities": [{"text": "AAC", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9611368179321289}]}, {"text": "Each icon includes an image, and is associated with metadata information that textually describes the meaning assigned to the icon at hand.", "labels": [], "entities": []}, {"text": "An image in this set can represent a single word, a phrase, or a syntactic modifier (such as \"plural\").", "labels": [], "entities": []}, {"text": "The images cover 48 major topics such as actions, technology, nature etc.", "labels": [], "entities": []}, {"text": "They vary in meaning, demonstrating abstract concepts and tangible ones; they also present different levels of complexity and details.", "labels": [], "entities": []}, {"text": "The metadata primarily describe the associated term the image corresponds to, its synonyms, and its translation to different languages.", "labels": [], "entities": []}, {"text": "The Symbolstix icon set was designed for use by communities who are in need of icon-based communication, such as children with communication disabilities, TBI patients, etc.", "labels": [], "entities": []}, {"text": "One commercial application of the current icon-set is a newscast aimed at adult consumers; another is a communications platform for children with Cerebral Palsy.", "labels": [], "entities": []}, {"text": "The set of 34k icons is in practice broad enough to reflect those needs.", "labels": [], "entities": []}, {"text": "However, the creators of the system often do add new icons when requested by their user population.", "labels": [], "entities": []}, {"text": "On the left side of is an example of a single-word icon representing the term afraid.", "labels": [], "entities": []}, {"text": "This icon's human-assigned topic category is \"descriptives-feelings\".", "labels": [], "entities": []}, {"text": "Note that many icons are mapped to multiple synonyms.", "labels": [], "entities": []}, {"text": "Synonyms of this icon are: eerie, fear, feared, fearful, fears, frightened, Halloween, scary, terrified, upset, and nervous.", "labels": [], "entities": []}, {"text": "As such, an icon's meaning is highly context-dependent.", "labels": [], "entities": []}, {"text": "The right-hand side icon in is an example of a two-word icon representing the concept bite leg.", "labels": [], "entities": []}, {"text": "This icon's topic is of \"actions\".", "labels": [], "entities": []}, {"text": "Synonyms of this icon are: \"bad day\" and \"dog bite\".", "labels": [], "entities": []}, {"text": "As observed in, the nature of the Symbolstix leans toward conversational concepts of spoken language, due to their intended use in AAC.", "labels": [], "entities": [{"text": "AAC", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.868223249912262}]}, {"text": "Symbolstix contains 34, 837 icons.", "labels": [], "entities": []}, {"text": "13, 951 of these icons are of a single word; several of these are duplicates 2 , only 12, 434 of the single word icons are unique terms.", "labels": [], "entities": []}, {"text": "In our experiment, to avoid redundancy, we used this set of unique terms.", "labels": [], "entities": []}, {"text": "We chose the unique term-icon pair (from its non-unique group) that had the richest metadata and the highest overlap (within its group) to represent a concept.", "labels": [], "entities": []}, {"text": "This step was essential to reduce complexity; however, it did introduce a limitation to our approach, which we discuss in section 5.", "labels": [], "entities": []}, {"text": "Notably, the Symbolstix corpus comes with no dataset that demonstrates the intended usage of icons to construct a proper sentence.", "labels": [], "entities": []}, {"text": "Ideally, we would use a dataset of icon sentences for language modeling, as it would enable learning icon sequences and by that to infer the language rules from its patterns.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.6994812041521072}]}, {"text": "In the next section, we describe how we were able to overcome that obstacle.", "labels": [], "entities": []}, {"text": "As mentioned in Section 2, the Symbolstix data set of icons has no sentence-like corpus from which icon sequences can be readily learned to form a language model of icons.", "labels": [], "entities": [{"text": "Symbolstix data set", "start_pos": 31, "end_pos": 50, "type": "DATASET", "confidence": 0.7796655197938284}]}, {"text": "We attempted to synthesize an icon corpus by beginning with a textual corpus, and \"projecting\" our icons into the text space using pre-trained word embeddings using the methodology described below.", "labels": [], "entities": []}, {"text": "Since each icon is accompanied by metadata containing humanedited synonym lists, it was natural to represent icons as some composition of their synonyms in a vector space.", "labels": [], "entities": []}, {"text": "In this manner, we embedded our icons in text, and created pseudo-sequences (\"icon sentences\").", "labels": [], "entities": []}, {"text": "This solution is not without problems, first among them the issue is that our icon sentences may not represent realistic examples of how the Symbolstix icons are meant to be used.", "labels": [], "entities": []}, {"text": "Rather, they might instead represent the icons as subjected to the language conventions found in oral or written language.", "labels": [], "entities": []}, {"text": "For example comparing a possible icon sequence to the English language might look like: Icon: <I> <go> <here> <past> English: <I> <went> <here>, or English: <the> <dogs> <are> <at> <home> Icon: <the> <dog> <plural> [<be> optional] <at> <home> Some of the terms may disappear in translation, while others are added.", "labels": [], "entities": []}, {"text": "Another question is whether it is possible to fully represent an icon as the sum of its synonyms; or, put another way, whether the ways in which an icon's synonyms are used in written language can capture the totality of an icon's meaning.", "labels": [], "entities": []}, {"text": "For example, the icon in figure 1 does not precisely mean \"afraid\", but rather refers to a more general concept.", "labels": [], "entities": []}, {"text": "This is why we chose to explore a compositional approach to representing icon meaning in a continuous space.", "labels": [], "entities": []}, {"text": "Finally, how should we handle sentences in our textual data set that are not fully representable using icons from Symbolstix?", "labels": [], "entities": []}, {"text": "We evaluated our language model using three different metrics: \u2022 Mean Reciprocal Rank (MRR) of the \"correct\" predicted icon as seen in Equation 2 Q represents the token events of the target.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 65, "end_pos": 91, "type": "METRIC", "confidence": 0.9679765303929647}]}, {"text": "The choice of MRR metric was to internally look at the rank of the target, rather than to binary classify it for whether it was accurately predicted in the first rank.", "labels": [], "entities": [{"text": "MRR", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8449463844299316}]}, {"text": "\u2022 Accuracy@k: The percentage of predictions in which the \"correct\" icon was within the top k predictions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9989007711410522}]}, {"text": "The choice in ACC@k was to inform about the quality of the prediction generated by the models.", "labels": [], "entities": []}, {"text": "We have chosen ACC@1 to crudely understand whether the first choice was correct, and ACC@10 to get a sense of the prediction quality given that a user maybe able to choose from a limited list (depending on the user interface), and also to model the notion that different users may choose different words in a simialr context (and so there is not a single correct word in reality).", "labels": [], "entities": []}, {"text": "We performed three sets of experiments.", "labels": [], "entities": []}, {"text": "The first explored the effect of different approaches to word embedding, the second explored the effects of either including or excluding non-icon terms in model training, and the third looked at the effects of other (non-Subtlex) text corpora.", "labels": [], "entities": []}, {"text": "For both approaches to word embedding, we used pre-trained word vectors.", "labels": [], "entities": []}, {"text": "The pretrained set is the source for generating the icon embeddings.", "labels": [], "entities": []}, {"text": "Both the icon and the pretrained embeddings replace the terms in the textual data with their corresponding vectors to generate an embedding corpus.", "labels": [], "entities": []}, {"text": "All our experiments contained the same number of pretrained vectors as well as icon vectors.", "labels": [], "entities": []}, {"text": "If both vector sets contained the same term, the icon embedding was used.", "labels": [], "entities": []}, {"text": "The textual dataset was tokenized and punctuation was removed.", "labels": [], "entities": []}, {"text": "Each of these experiments was held in a 5 fold cross validation fashion.", "labels": [], "entities": []}, {"text": "The process to generate the corpus from which language models are learned is described in.", "labels": [], "entities": []}, {"text": "This process shows also the three different modules we experimented with: the pretrained corpus, which forms the icon embeddings; the icon set that forms (with or without the pretrained set (therefore the 'switch' between pretrained embeddings to textual dataset)) the textual embedding; and the textual dataset that provides the sequences of symbols to generate the textual embedding.", "labels": [], "entities": []}, {"text": "The Pretrained embeddings in our experiment are used both to construct the icon representations and After controlling for the number of icon-and pretrained-term types as well as for the textual corpus, shows that there are no meaningful differences resulted from the pretrained vectors type.", "labels": [], "entities": []}, {"text": "The dataset the vectors were trained on as well as the method by which the vectors were generated had no observable impact on the language model performance.", "labels": [], "entities": []}, {"text": "While Experiment 1 covers one aspect of comparing differences between different word embeddings, when choosing a pretrained set of vectors to use and generate icons from, there maybe additional considerations.", "labels": [], "entities": []}, {"text": "The coverage of the pretrained set is essential to produce icon representations, but also is important for terms in the textual dataset that cannot be represented with icons (which are then replaced with a pretrained vector if found) as described in Experiment 1.", "labels": [], "entities": []}, {"text": "The pretrained set coverage with regards to the icon set is measured not only by the total number of icon representations that were generated from the pretrained set, but also by how well each icon captures the broad meaning it stands for.", "labels": [], "entities": []}, {"text": "Since each icon is likely to have its name and synonyms composed together to represent it (as described in 3 in Data Preprocessing part), an optimal pretrained set would contain representations for all these terms.", "labels": [], "entities": []}, {"text": "As for the textual dataset, an optimal coverage of the text with the pretrained list ideally would consist of a large number of term types, but also term events that appear in the dataset.", "labels": [], "entities": []}, {"text": "Ideally, the corpus to learn language models from would consist of the icon vocabulary solely since the goal is to construct an Icon language model.", "labels": [], "entities": []}, {"text": "We therefore, experimented with transforming our synthetic corpus to only include terms representable using icon vectors (\"pure\") and compared LM and prediction accuracy with the original, \"non-pure\" results.", "labels": [], "entities": [{"text": "LM", "start_pos": 143, "end_pos": 145, "type": "METRIC", "confidence": 0.774709939956665}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.7862135171890259}]}, {"text": "We used Glove and c2v vectors in our experiment presented in. describes a similar pattern to as there was no meaningful change in the final icon language models' performances due to the \"pure\" condition.", "labels": [], "entities": []}, {"text": "We do note that there was a slight advantage to c2v embeddings which seemed to be predicting more correctly the target.", "labels": [], "entities": []}, {"text": "The \"pure\" condition resulted in a relatively smaller prediction accuracy.", "labels": [], "entities": [{"text": "prediction", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.869152843952179}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9151391386985779}]}, {"text": "On the one hand, this maybe surprising evidence, as it is reasonable to think that a smaller and more focused vocabulary set would result in an improved language model performance.", "labels": [], "entities": []}, {"text": "We assume that the reduction in vocabulary size caused as a result of employing icons solely created short, sparse, and uncommon patterns of sequences, which limited the models' ability to learn and predict accurately.", "labels": [], "entities": []}, {"text": "Under the \"pure\" condition, the model vocabulary consists solely of the icon set itself, whereas in the \"non-pure\" condition, the model vocabulary consists of the icon set as well as the pretrained embeddings together with <unk> terms.", "labels": [], "entities": []}, {"text": "While we cannot directly compare the two experiments (1 and 2) we can share our considerations when choosing to generate language models purely based on icons.", "labels": [], "entities": []}, {"text": "To support our explanation for interpretation, we conducted a qualitative test and looked into to the actual sentences produced by the icons in isolation, asking whether these sentences created \"meaningful\" (or at least useful) messages for LM training..", "labels": [], "entities": []}, {"text": "This might be helpful to get a deeper perspective on the corpus created and assist in making design choices.", "labels": [], "entities": []}, {"text": "While it is not feasible to qualitatively look at every sentence, one may consider comparing the amount of tokens prior to elimination and post, under the assumption that the greater the loss, the more likely that the quality of solely using the icon-set becomes a concern.", "labels": [], "entities": []}, {"text": "We would like to note that in Experiment 2 in particular, we used the same simulated \"icon language\" for both training and evaluation.", "labels": [], "entities": []}, {"text": "An ideal evaluation of our approach to producing synthetic in-domain training data would have been evaluating the language models trained on simulated icon language on \"real\" text composed using icons.", "labels": [], "entities": []}, {"text": "As we did not have such a useful resource, it is important to observe this as a limitation of the current experiment.", "labels": [], "entities": []}, {"text": "In our system, the role of the textual corpus is to provide the language model with training data regarding patterns of word (\"icon\") use.", "labels": [], "entities": []}, {"text": "Ideally, we would use a corpus made of symbols that represent the type of content and structure an AAC user would produce.", "labels": [], "entities": []}, {"text": "Finding an AAC-oriented corpus that would be big enough to train was a hurdle, and so for our previously-described experiments, we relied on SubtlexUS.", "labels": [], "entities": []}, {"text": "While not ideal, this corpus was closer to spontaneous speech than, say, a newswire corpus would have been, and featured smaller and more manageable sentences that we hoped would withstand being converted to pseudo-icon representations.", "labels": [], "entities": []}, {"text": "That said, we did wish to investigate the utility of using an existing corpus that was designed to be closer to AAC-style speech.", "labels": [], "entities": [{"text": "AAC-style speech", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.5802093148231506}]}, {"text": "Vertanen and Kristensson (2011) produced such a corpus, consisting of 6,142 sentences produced by Amazon Mechanical Turk users who were paid to generate plausible sentences and to evaluate the plausibility of other sentences generated by other workers.", "labels": [], "entities": []}, {"text": "This corpus, while valuable, was too small for use with our language modeling approach.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7251420319080353}]}, {"text": "Following Vertanen and Kristensson's insight that \"short text\" such as that seen in online media such as Twitter, etc.", "labels": [], "entities": []}, {"text": "might be a good proxy for true AAC-style speech, we therefore mixed the AACstyle corpus with second corpus, this one consisting of modified SMS messages.", "labels": [], "entities": [{"text": "AAC-style speech", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7519403696060181}, {"text": "AACstyle corpus", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9383819997310638}]}, {"text": "The second corpus was from Chen and Kan (2013) and consisted of 18,042 SMS messages, and was originally constructed for experiments in text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7904128432273865}]}, {"text": "As such, it includes messages written in heavily-abbreviated forms as well as \"cleaned up\" versions of each message, written in something approximating \"standard\" English orthography.", "labels": [], "entities": []}, {"text": "We used this subset of the corpus in the hopes that its short, informal, and speech-like sentences would complement the AAC-style corpus.", "labels": [], "entities": [{"text": "AAC-style corpus", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.8490301370620728}]}, {"text": "Our goal was to assemble a corpus containing language that is as close as possible to what would be produced by actual users of an AAC system.", "labels": [], "entities": []}, {"text": "We then repeated the language modeling experiments conducted earlier on this hybrid corpus, using identical procedures and evaluation metrics.: Textual corpus (AAC-SMS/pure) mean(standard deviation) other: the models trained using c2v embeddings outperformed the models that used Glove embeddings, which is different from what we observed in the previous experiments-though with this corpus, the overall performance numbers were much lower than with the original, larger corpora.", "labels": [], "entities": []}, {"text": "The reason for overall low performance was probably due to the very small size of the AAC-SMS corpus, and possible overfitting as a result.", "labels": [], "entities": [{"text": "AAC-SMS corpus", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.8610889911651611}]}, {"text": "Digging more deeply into our data, we examined the individual cross-validation results at the fold level, thinking that perhaps the results were unstable due to the relatively small data set which indeed seem to be the case.", "labels": [], "entities": []}, {"text": "Nevertheless, in an attempt to indirectly evaluate different models, while the AAC experiments had substantially higher variance across folds than did the SubtlexUS experiments, the differences between the two approaches do appear to be real.", "labels": [], "entities": [{"text": "AAC", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.7493378520011902}]}, {"text": "Ultimately, we note that the substantial difference in corpus size between SubtlexUS and our AAC-SMS corpus make it difficult to draw any firm conclusions, and investigating this issue further will be a component of our future work in this area.", "labels": [], "entities": [{"text": "AAC-SMS corpus", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9324727356433868}]}], "tableCaptions": []}