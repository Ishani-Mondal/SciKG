{"title": [{"text": "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information", "labels": [], "entities": []}], "abstractContent": [{"text": "Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions.", "labels": [], "entities": []}, {"text": "In this work, we build a neural network model for the task of ranking clarification questions.", "labels": [], "entities": [{"text": "ranking clarification questions", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.845172385374705}]}, {"text": "Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful.", "labels": [], "entities": []}, {"text": "We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster.", "labels": [], "entities": []}, {"text": "We create a dataset of clarification questions consisting of \u223c77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser.", "labels": [], "entities": []}, {"text": "We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled base-lines.", "labels": [], "entities": []}], "introductionContent": [{"text": "A principle goal of asking questions is to fill information gaps, typically through clarification questions.", "labels": [], "entities": []}, {"text": "We take the perspective that a good question is the one whose likely answer will be useful.", "labels": [], "entities": []}, {"text": "Consider the exchange in, in which an initial poster (who we call \"Terry\") asks for help configuring environment variables.", "labels": [], "entities": []}, {"text": "This post is underspecified and a responder (\"Parker\") asks a clarifying question (a) below, but could alternatively have asked or (c): (a) What version of Ubuntu do you have?", "labels": [], "entities": []}, {"text": "Parker should not ask (b) because an answer is unlikely to be useful; they should not ask (c) because it is too specific and an answer like \"No\" or \"I do not know\" gives little help.", "labels": [], "entities": []}, {"text": "Parker's question (a) is much better: it is both likely to be useful, and is plausibly answerable by Terry.", "labels": [], "entities": []}, {"text": "In this work, we design a model to rank a candidate set of clarification questions by their usefulness to the given post.", "labels": [], "entities": []}, {"text": "We imagine a use case (more discussion in \u00a7 7) in which, while Terry is writing their post, a system suggests a shortlist of questions asking for information that it thinks people like Parker might need to provide a solution, thus enabling Terry to immediately clarify their post, potentially leading to a much quicker resolution.", "labels": [], "entities": []}, {"text": "Our model is based on the decision theoretic framework of the Expected Value of Perfect Information (EVPI), a measure of the value of gathering additional information.", "labels": [], "entities": []}, {"text": "In our setting, we use EVPI to calculate which questions are most likely to elicit an answer that would make the post more informative.", "labels": [], "entities": []}, {"text": "The behavior of our model during test time: Given a post p, we retrieve 10 posts similar to post p using Lucene.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9738321900367737}]}, {"text": "The questions asked to those 10 posts are our question candidates Q and the edits made to the posts in response to the questions are our answer candidates A.", "labels": [], "entities": []}, {"text": "For each question candidate q i , we generate an answer representation F (p, q i ) and calculate how close is the answer candidate a j to our answer representation F (p, q i ).", "labels": [], "entities": []}, {"text": "We then calculate the utility of the post p if it were updated with the answer a j . Finally, we rank the candidate questions Q by their expected utility given the post p (Eq 1).", "labels": [], "entities": []}, {"text": "Our work has two main contributions: 1.", "labels": [], "entities": []}, {"text": "A novel neural-network model for addressing the task of ranking clarification question built on the framework of expected value of perfect information ( \u00a72).", "labels": [], "entities": [{"text": "ranking clarification question", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.8471459150314331}]}, {"text": "2. A novel dataset, derived from StackExchange 2 , that enables us to learn a model to ask clarifying questions by looking at the types of questions people ask ( \u00a73).", "labels": [], "entities": []}, {"text": "We formulate this task as a ranking problem on a set of potential clarification questions.", "labels": [], "entities": []}, {"text": "We evaluate models both on the task of returning the original clarification question and also on the task of picking any of the candidate clarification questions marked as good by experts ( \u00a74).", "labels": [], "entities": []}, {"text": "We find that our EVPI model outperforms the baseline models when evaluated against expert human annotations.", "labels": [], "entities": []}, {"text": "We include a few examples of human annotations along with our model performance on them in the supplementary material.", "labels": [], "entities": []}, {"text": "We have released our dataset of \u223c77K (p, q, a) triples and the expert annotations on 500 triples to help facilitate further research in this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "StackExchange is a network of online question answering websites about varied topics like academia, ubuntu operating system, latex, etc.", "labels": [], "entities": []}, {"text": "The data dump of StackExchange contains timestamped information about the posts, comments on the post and the history of the revisions made to the post.", "labels": [], "entities": []}, {"text": "We use this data dump to create our dataset of (post, question, answer) triples: where the post is the initial unedited post, the question is the comment containing a question and the answer is either the edit made to the post after the question or the author's response to the question in the comments section.", "labels": [], "entities": []}, {"text": "Extract posts: We use the post histories to identify posts that have been updated by its author.", "labels": [], "entities": []}, {"text": "We use the timestamp information to retrieve the initial unedited version of the post.", "labels": [], "entities": []}, {"text": "Extract questions: For each such initial version of the post, we use the timestamp information of its comments to identify the first question comment made to the post.", "labels": [], "entities": []}, {"text": "We truncate the comment till its question mark '?'", "labels": [], "entities": []}, {"text": "to retrieve the question part of the comment.", "labels": [], "entities": []}, {"text": "We find that about 7% of these are rhetoric questions that indirectly suggest a solution to the post.", "labels": [], "entities": []}, {"text": "For e.g. \"have you considered installing X?\".", "labels": [], "entities": []}, {"text": "We do a manual analysis of shows the sizes of the train, tune and test split of our dataset for three domains.", "labels": [], "entities": [{"text": "tune", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9874153733253479}]}, {"text": "these non-clarification questions and hand-crafted a few rules to remove them.", "labels": [], "entities": []}, {"text": "Extract answers: We extract the answer to a clarification question in the following two ways: (a) Edited post: Authors tend to respond to a clarification question by editing their original post and adding the missing information.", "labels": [], "entities": []}, {"text": "In order to account for edits made for other reasons like stylistic updates and grammatical corrections, we consider only those edits that are longer than four words.", "labels": [], "entities": []}, {"text": "Authors can make multiple edits to a post in response to multiple clarification questions.", "labels": [], "entities": []}, {"text": "8 To identify the edit made corresponding to the given question comment, we choose the edit closest in time following the question.", "labels": [], "entities": []}, {"text": "In cases where both the methods above yield an answer, we pick the one that is the most semantically similar to the question, where the measure of similarity is the cosine distance between the average word embeddings of the question and the answer.", "labels": [], "entities": []}, {"text": "We extract a total of 77,097 (post, question, answer) triples across three domains in StackExchange).", "labels": [], "entities": []}, {"text": "We will release this dataset along with the the nine question and answer candidates per triple that we generate using lucene ( \u00a7 2.1).", "labels": [], "entities": []}, {"text": "We include an analysis of our dataset in the supplementary material.", "labels": [], "entities": []}, {"text": "We define our task as given a post p, and a set of candidate clarification questions Q, rank the questions according to their usefulness to the post.", "labels": [], "entities": []}, {"text": "Since the candidate set includes the original question q that was asked to the post p, one possible approach to evaluation would be to look at how often the original question is ranked higher up in the ranking predicted by a model.", "labels": [], "entities": []}, {"text": "However, there are two problems to this approach: 1) Our dataset creation process is noisy.", "labels": [], "entities": []}, {"text": "The original question paired with the post may not be a useful question.", "labels": [], "entities": []}, {"text": "For e.g. \"are you seriously asking this question?\", \"do you mind making that an answer?\"", "labels": [], "entities": []}, {"text": "9 . 2) The nine other questions in the candidate set are obtained by looking at questions asked to posts that are similar to the given post.", "labels": [], "entities": []}, {"text": "This greatly increases the possibility of some other question(s) being more useful than the original question paired with the post.", "labels": [], "entities": []}, {"text": "This motivates an evaluation design that does not rely solely on the original question but also uses human judgments.", "labels": [], "entities": []}, {"text": "We randomly choose a total of 500 examples from the test sets of the three domains proportional to their train set sizes (askubuntu:160, unix:90 and superuser:250) to construct our evaluation set.", "labels": [], "entities": []}, {"text": "Our primary research questions that we evaluate experimentally are: 1.", "labels": [], "entities": []}, {"text": "Does a neural network architecture improve upon non-neural baselines?: Model performances on 500 samples when evaluated against the union of the \"best\" annotations (B1 \u222a B2), intersection of the \"valid\" annotations (V 1 \u2229 V 2) and the original question paired with the post in the dataset.", "labels": [], "entities": []}, {"text": "The difference between the bold and the non-bold numbers is statistically significant with p < 0.05 as calculated using bootstrap test.", "labels": [], "entities": []}, {"text": "p@k is the precision of the k questions ranked highest by the model and MAP is the mean average precision of the ranking predicted by the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9985290765762329}, {"text": "MAP", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9978000521659851}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.6459060311317444}]}, {"text": "2. Does the EVPI formalism provide leverage over a similarly expressive feedforward network?", "labels": [], "entities": []}, {"text": "3. Are answers useful in identifying the right question?", "labels": [], "entities": []}, {"text": "4. How do the models perform when evaluated on the candidate questions excluding the original?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Model performances on 500 samples when evaluated against the union of the \"best\" annotations  (B1 \u222a B2), intersection of the \"valid\" annotations (V 1 \u2229 V 2) and the original question paired with the  post in the dataset. The difference between the bold and the non-bold numbers is statistically significant  with p < 0.05 as calculated using bootstrap test. p@k is the precision of the k questions ranked highest  by the model and MAP is the mean average precision of the ranking predicted by the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 379, "end_pos": 388, "type": "METRIC", "confidence": 0.9940804839134216}, {"text": "MAP", "start_pos": 441, "end_pos": 444, "type": "METRIC", "confidence": 0.9972787499427795}, {"text": "precision", "start_pos": 465, "end_pos": 474, "type": "METRIC", "confidence": 0.6369172930717468}]}]}