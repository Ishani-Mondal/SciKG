{"title": [{"text": "Sentiment Analysis using Imperfect Views from Spoken Language and Acoustic Modalities", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9696143269538879}]}], "abstractContent": [{"text": "Multimodal sentiment classification in practical applications may have to rely on erroneous and imperfect views, namely (a) language transcription from a speech rec-ognizer and (b) under-performing acoustic views.", "labels": [], "entities": [{"text": "Multimodal sentiment classification", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9091142018636068}, {"text": "language transcription", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.6989389210939407}]}, {"text": "This work focuses on improving the representations of these views by performing a deep canonical correlation analysis with the representations of the better performing manual transcription view.", "labels": [], "entities": []}, {"text": "Enhanced representations of the imperfect views can be obtained even in absence of the perfect views and give an improved performance during test conditions.", "labels": [], "entities": []}, {"text": "Evaluations on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed approach.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9752014875411987}, {"text": "CMU-MOSEI datasets", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9486382603645325}]}], "introductionContent": [{"text": "Use of multimodal cues is especially useful for analyzing sentiment in audio-visual data like opinion videos on social media websites, call-center audio recordings etc.", "labels": [], "entities": []}, {"text": "The different modalities, viz.", "labels": [], "entities": []}, {"text": "language (spoken words), acoustic (speech) and visual (facial and gestures), can carry a different view of the same information like for example, sentiment.", "labels": [], "entities": []}, {"text": "While the representations/features extracted from these individual different views add richness to the sentiment classification, the intra and inter view-interactions play an important role in better sentiment classification (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.8353249430656433}, {"text": "sentiment classification", "start_pos": 200, "end_pos": 224, "type": "TASK", "confidence": 0.8575988411903381}]}, {"text": "Although fusion of multi-view information is being extensively explored, the challenges associated with the presence of noise and irregularities in a view has received very less attention.", "labels": [], "entities": []}, {"text": "For instance, multimodal sentiment classification systems have typically used manual, and hence, error free language transcriptions and exploited the interaction of other views with this noise free language view ( . However, a practical system will have to rely on a language transcription from an Automatic Speech Recognition (ASR) engine, which is inherently prone to errors due to ambient/channel noises in acoustic environments), language domain mismatch, emotion in speech (), etc.", "labels": [], "entities": [{"text": "multimodal sentiment classification", "start_pos": 14, "end_pos": 49, "type": "TASK", "confidence": 0.7541906436284384}]}, {"text": "Similarly, existing and popularly used representations of the acoustic view have generally under-performed compared to the language view (, indicating that the acoustic view or its representations, by themselves, may not be discriminative enough for robust sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 257, "end_pos": 281, "type": "TASK", "confidence": 0.8074019253253937}]}, {"text": "Assuming the ASR (language transcription) and acoustic views as imperfect views, the focus of this work is on improving the representations of these noisy views, riding on the representations of the better performing view.", "labels": [], "entities": [{"text": "ASR (language transcription)", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.7551185250282287}]}, {"text": "We show that the representations obtained from automatic transcriptions of spoken language and those from the acoustic views can be enhanced using corresponding representations from manual transcriptions of spoken language.", "labels": [], "entities": []}, {"text": "Enhanced representations of the imperfect views can be obtained even in absence of the perfect views during test conditions.", "labels": [], "entities": []}, {"text": "Deep canonical correlation analysis (DCCA) () is used to improve the representations of the imperfect views.", "labels": [], "entities": [{"text": "Deep canonical correlation analysis (DCCA)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7419068600450244}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes a method to improve imperfect or erroneous views.", "labels": [], "entities": []}, {"text": "Section 3 presents the different components in our multimodal sentiment classification system.", "labels": [], "entities": [{"text": "multimodal sentiment classification", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.7032205561796824}]}, {"text": "Experiments are discussed in Section 4 followed by a discussion on results and conclusion in Section 5.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9716952443122864}]}], "datasetContent": [{"text": "We present our results and analysis on two datasets, namely, (a) CMU-MOSI ( and (b) CMU-MOSEI (Zadeh, 2018a).", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.933606743812561}, {"text": "CMU-MOSEI", "start_pos": 84, "end_pos": 93, "type": "DATASET", "confidence": 0.9297147393226624}]}, {"text": "CMU-MOSI consists of 93 movie related opinion videos from YouTube, segmented into 2199 clips/utterances.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9272847175598145}]}, {"text": "CMU-MOSEI consists of about 2500 multi-domain monologue videos from YouTube, segmented into 23, 500 clips/utterances.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9097009301185608}]}, {"text": "Both CMU-MOSI and CMU-MOSEI datasets are annotated with utterance level sentiment labels in the range.", "labels": [], "entities": [{"text": "CMU-MOSI", "start_pos": 5, "end_pos": 13, "type": "DATASET", "confidence": 0.9552516341209412}, {"text": "CMU-MOSEI datasets", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.952025979757309}]}, {"text": "We focus on binary sentiment classification in which labels [\u22123, 0] are considered as negative and are considered as positive sentiments.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6907106041908264}]}, {"text": "For CMU-MOSI, we used the train, validation and test split provided by the CMU Multimodal Data SDK (Zadeh, 2018b).", "labels": [], "entities": [{"text": "CMU Multimodal Data SDK (Zadeh, 2018b)", "start_pos": 75, "end_pos": 113, "type": "DATASET", "confidence": 0.8636262549294366}]}, {"text": "The SDK also provides a train, validation and test split for CMU-MOSEI.", "labels": [], "entities": [{"text": "CMU-MOSEI", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.9293140769004822}]}, {"text": "However, as the test set labels were not available at the time of submission of this paper, we treated 200 videos from the original validation set as our test set.", "labels": [], "entities": []}, {"text": "The remaining 100 videos from the original validation set and an additional 150 videos from the original train set are considered as our validation set.", "labels": [], "entities": []}, {"text": "We evaluate the performance of the spoken language and acoustic views, individually and in combination.", "labels": [], "entities": []}, {"text": "The manual and ASR transcriptions of the language view are denoted as MT and AT, respectively.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.8701748847961426}, {"text": "AT", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9674431681632996}]}, {"text": "The acoustic view is denoted as AU.", "labels": [], "entities": [{"text": "AU", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9871639013290405}]}, {"text": "Enhanced (representations of) ASR view and acoustic view are denoted as AT \u2191 and AU \u2191 , respectively.", "labels": [], "entities": [{"text": "AT \u2191", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9752446711063385}, {"text": "AU \u2191", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.8859816193580627}]}, {"text": "They were enhanced using (representations of) the manual transcription view, using the DCCA model described in Section 2.1.", "labels": [], "entities": []}, {"text": "Our DCCA models use DNNs with 3 hidden layers and sigmoids.", "labels": [], "entities": []}, {"text": "presents the % accuracy (Acc.) and Fscore (F1) for binary sentiment classification on the CMU-MOSI and CMU-MOSEI datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9059051871299744}, {"text": "Acc.", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.964771568775177}, {"text": "Fscore (F1)", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9216261208057404}, {"text": "binary sentiment classification", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6597911814848582}, {"text": "CMU-MOSI", "start_pos": 90, "end_pos": 98, "type": "DATASET", "confidence": 0.945559561252594}, {"text": "CMU-MOSEI datasets", "start_pos": 103, "end_pos": 121, "type": "DATASET", "confidence": 0.8501207530498505}]}, {"text": "The results are divided into four sections, viz.", "labels": [], "entities": []}, {"text": "(I) the 'ideal' baseline results achieved by the LSTM-RNN classifier on the manual transcription and acoustic views, (II) the 'practical' baseline results achieved with the imperfect ASR view, (III) the results obtained, for the practical scenario, by the proposed approach with DCCA enhanced views and (IV) the improvement on using DCCA enhanced acoustic view with manual transcriptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentiment classification performance us- ing a bi-directional LSTM-RNN classifier.  MOSI  MOSEI  Acc. F1 Acc. F1", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9377380311489105}, {"text": "MOSI  MOSEI  Acc. F1 Acc. F1", "start_pos": 94, "end_pos": 122, "type": "DATASET", "confidence": 0.7954652905464172}]}, {"text": " Table 2: Improvement in ASR view accuracy us- ing a non contextual classifier.  MOSI MOSEI  MT  71.1  67.5  AT  63.7  63.8  AT \u2191  65.1  65.7", "labels": [], "entities": [{"text": "ASR view", "start_pos": 25, "end_pos": 33, "type": "TASK", "confidence": 0.8262544274330139}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.8217428922653198}, {"text": "MOSI MOSEI  MT  71.1  67.5", "start_pos": 81, "end_pos": 107, "type": "DATASET", "confidence": 0.6821116983890534}, {"text": "AT", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9295597076416016}, {"text": "AT", "start_pos": 125, "end_pos": 127, "type": "METRIC", "confidence": 0.8787164092063904}]}]}