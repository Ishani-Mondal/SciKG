{"title": [], "abstractContent": [{"text": "We introduce anew entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity.", "labels": [], "entities": []}, {"text": "This formulation allows us to use anew type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in.", "labels": [], "entities": []}, {"text": "We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks.", "labels": [], "entities": []}, {"text": "We present a model that can predict open types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entities can often be described by very fine grained types.", "labels": [], "entities": []}, {"text": "Consider the sentences \"Bill robbed John.", "labels": [], "entities": [{"text": "Bill robbed John", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.8696765899658203}]}, {"text": "The noun phrases \"John,\" \"Bill,\" and \"he\" have very specific types that can be inferred from the text.", "labels": [], "entities": []}, {"text": "This includes the facts that \"Bill\" and \"he\" are both likely \"criminal\" due to the \"robbing\" and \"arresting,\" while \"John\" is more likely a \"victim\" because he was \"robbed.\"", "labels": [], "entities": []}, {"text": "Such fine-grained types (victim, criminal) are important for context-sensitive tasks such Our data and model can be downloaded from: http://nlp.cs.washington.edu/entity_type", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiment Setup The crowdsourced dataset (Section 2.1) was randomly split into train, development, and test sets, each with about 2,000 examples.", "labels": [], "entities": []}, {"text": "We use this relatively small manuallyannotated training set (Crowd in) alongside the two distant supervision sources: entity linking (KB and Wikipedia definitions) and head words.", "labels": [], "entities": []}, {"text": "To combine supervision sources of different magnitudes (2K crowdsourced data, 4.7M entity linking data, and 20M head words), we sample a batch of equal size from each source at each iteration.", "labels": [], "entities": []}, {"text": "We reimplement the recent AttentiveNER model (Shimaoka et al., 2017) for reference.", "labels": [], "entities": []}, {"text": "We report macro-averaged precision, recall, and F1, and the average mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9579166173934937}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9996920824050903}, {"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.9998140931129456}, {"text": "average mean reciprocal rank (MRR)", "start_pos": 60, "end_pos": 94, "type": "METRIC", "confidence": 0.7876949225153241}]}, {"text": "Results shows the performance of our model and our reimplementation of AttentiveNER.", "labels": [], "entities": []}, {"text": "Our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9989975094795227}, {"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9984713196754456}]}, {"text": "The MRR score shows that our We use the AttentiveNER model with no engineered features or hierarchical label encoding (as a hierarchy is not clear in our label setting) and let it predict from the same label space, training with the same supervision data.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.7094257473945618}]}, {"text": "model is slightly better than the baseline at ranking correct types above incorrect ones.", "labels": [], "entities": []}, {"text": "shows the performance breakdown for different type granularity and different supervision.", "labels": [], "entities": []}, {"text": "Overall, as seen in previous work on finegrained NER literature (), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types.", "labels": [], "entities": [{"text": "finegrained NER literature", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.6440494159857432}]}, {"text": "All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact.", "labels": [], "entities": []}, {"text": "Head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction.", "labels": [], "entities": [{"text": "predicting ultra-fine labels", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8281197945276896}, {"text": "entity linking", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7214165031909943}, {"text": "fine label prediction", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.6433073778947195}]}, {"text": "The low general type performance is partially because of nominal/pronoun mentions (e.g. \"it\"), and because of the large type inventory (sometimes \"location\" and \"place\" are annotated interchangeably).", "labels": [], "entities": []}, {"text": "Analysis We manually analyzed 50 examples from the development set, four of which we present in.", "labels": [], "entities": []}, {"text": "Overall, the model was able to generate accurate general types and a diverse set of type labels.", "labels": [], "entities": []}, {"text": "Despite our efforts to annotate a comprehensive typeset, the gold labels still miss many potentially correct labels (example (a): \"man\" is reasonable but counted as incorrect).", "labels": [], "entities": []}, {"text": "This makes the precision estimates lower than the actual performance level, with about half the precision errors belonging to this category.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9995216131210327}, {"text": "precision errors", "start_pos": 96, "end_pos": 112, "type": "METRIC", "confidence": 0.9813034236431122}]}, {"text": "Real precision errors include predicting co-hyponyms (example (b): \"accident\" instead of \"attack\"), and types that: Example and predictions from our best model on the development set.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9571377635002136}]}, {"text": "Entity mentions are marked with curly brackets, the correct predictions are boldfaced, and the missing labels are italicized and written in red.", "labels": [], "entities": []}, {"text": "maybe true, but are not supported by the context.", "labels": [], "entities": []}, {"text": "We found that the model often abstained from predicting any fine-grained types.", "labels": [], "entities": []}, {"text": "Especially in challenging cases as in example (c), the model predicts only general types, explaining the low recall numbers (28% of examples belong to this category).", "labels": [], "entities": [{"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9978043437004089}]}, {"text": "Even when the model generated correct fine-grained types as in example (d), the recall was often fairly low since it did not generate a complete set of related fine-grained labels.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9996484518051147}]}, {"text": "Estimating the performance of a model in an incomplete label setting and expanding label coverage are interesting areas for future work.", "labels": [], "entities": []}, {"text": "Our task also poses a potential modeling challenge; sometimes, the model predicts two incongruous types (e.g. \"location\" and \"person\"), which points towards modeling the task as a joint set prediction task, rather than predicting labels individually.", "labels": [], "entities": []}, {"text": "We provide sample outputs on the project website.", "labels": [], "entities": []}, {"text": "We compare performance to other published results and to our reimplementation of AttentiveNER ().", "labels": [], "entities": []}, {"text": "We also compare models trained with different sources of supervision.", "labels": [], "entities": []}, {"text": "For this dataset, we did not use our multitask objective (Section 4), since expanding types to include their ontological hypernyms largely eliminates the partial supervision as-: Ablation study on the OntoNotes finegrained entity typing development.", "labels": [], "entities": [{"text": "OntoNotes finegrained entity typing development", "start_pos": 201, "end_pos": 248, "type": "TASK", "confidence": 0.5687681496143341}]}, {"text": "The second row isolates dataset improvements, while the third row isolates the model. sumption.", "labels": [], "entities": []}, {"text": "Following prior work, we report macroand micro-averaged F1 score, as well as accuracy (exact set match).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9525012075901031}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9997842907905579}, {"text": "exact set match", "start_pos": 87, "end_pos": 102, "type": "METRIC", "confidence": 0.9472800294558207}]}, {"text": "Results shows the overall performance on the test set.", "labels": [], "entities": []}, {"text": "Our combination of model and training data shows a clear improvement from prior work, setting anew state-of-the art result.", "labels": [], "entities": []}, {"text": "In, we show an ablation study.", "labels": [], "entities": []}, {"text": "Our new supervision sources improve the performance of both the AttentiveNER model and our own.", "labels": [], "entities": []}, {"text": "We observe that every supervision source improves performance in its own right.", "labels": [], "entities": []}, {"text": "Particularly, the naturally-occurring head-word supervision seems to be the prime source of improvement, increasing performance by about 10% across all metrics.", "labels": [], "entities": []}, {"text": "Predicting Miscellaneous Types While analyzing the data, we observed that over half of the mentions in OntoNotes' development set were annotated only with the miscellaneous type (\"/other\").", "labels": [], "entities": [{"text": "OntoNotes' development set", "start_pos": 103, "end_pos": 129, "type": "DATASET", "confidence": 0.8817741274833679}]}, {"text": "For both models in our evaluation, detecting the miscellaneous category is substantially easier than", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Performance of our model and AttentiveNER (Shimaoka et al., 2017) on the new entity typing  benchmark, using same training data. We show results for both development and test sets.", "labels": [], "entities": [{"text": "AttentiveNER", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9731281399726868}]}, {"text": " Table 4: Results on the development set for different type granularity and for different supervision data  with our model. In each row, we remove a single source of supervision. Entity linking (EL) includes  supervision from both KB and Wikipedia definitions. The numbers in the first row are example counts  for each type granularity.", "labels": [], "entities": []}, {"text": " Table 6: Results on the OntoNotes fine-grained  entity typing test set. The first two models (At- tentiveNER++ and AFET) use only KB-based su- pervision. LNR uses a filtered version of the KB- based training set. Our model uses all our distant  supervision sources.", "labels": [], "entities": [{"text": "OntoNotes fine-grained  entity typing test set", "start_pos": 25, "end_pos": 71, "type": "DATASET", "confidence": 0.6238872359196345}, {"text": "AFET", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9517943263053894}]}, {"text": " Table 7: Ablation study on the OntoNotes fine- grained entity typing development. The second  row isolates dataset improvements, while the third  row isolates the model.", "labels": [], "entities": []}]}