{"title": [{"text": "Zeroshot Multimodal Named Entity Disambiguation for Noisy Social Media Posts", "labels": [], "entities": [{"text": "Zeroshot Multimodal Named Entity Disambiguation", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.4983674228191376}, {"text": "Noisy Social Media Posts", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.5462528839707375}]}], "abstractContent": [{"text": "We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images.", "labels": [], "entities": [{"text": "Multimodal Named Entity Disambiguation (MNED) task", "start_pos": 21, "end_pos": 71, "type": "TASK", "confidence": 0.744093619287014}]}, {"text": "Social media posts bring significant challenges for disam-biguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations , 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training.", "labels": [], "entities": []}, {"text": "To this end, we build anew dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base.", "labels": [], "entities": []}, {"text": "We then build a deep zeroshot mul-timodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well.", "labels": [], "entities": [{"text": "MNED", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.7288601398468018}]}, {"text": "The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.", "labels": [], "entities": [{"text": "MNED task", "start_pos": 127, "end_pos": 136, "type": "TASK", "confidence": 0.7878403663635254}]}], "introductionContent": [{"text": "Online communications are increasingly becoming fast-paced and frequent, and hidden in these abundant user-generated social media posts are insights for understanding users and their preferences.", "labels": [], "entities": []}, {"text": "However, these social media posts often come in unstructured text or images, making massive-scale opinion mining extremely challeng-(a) Traditional NED (b) Multimodal NED: Examples of (a) a traditional NED task, focused on disambiguating polysemous entities based on surrounding textual contexts, and (b) the proposed Multimodal NED task for short media posts, which leverages both visual and textual contexts to disambiguate an entity.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.7525515258312225}]}, {"text": "Note that mentions are often lexically inconsistent or incomplete, and thus a fixed candidates generation method (based on exact mention-entity statistics) is not viable. ing.", "labels": [], "entities": []}, {"text": "Named entity disambiguation (NED), the task of linking ambiguous entities from free-form text mention to specific entities in a pre-defined knowledge base (KB), is thus a critical step for extracting structured information which leads to its application for recommendations, advertisement, personalized assistance, etc.", "labels": [], "entities": [{"text": "Named entity disambiguation (NED)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7761372625827789}]}, {"text": "While many previous approaches on NED been successful for well-formed text in disambiguating polysemous entities via context resolution, several additional challenges remain for disambiguating entities from extremely short and coarse text found in social media posts (e.g. \"juuustin \" as opposed to \"I love Justin Bieber/Justin Trudeau/etc.\").", "labels": [], "entities": [{"text": "context resolution", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7445803582668304}]}, {"text": "In many of these cases it is simply impossible to disambiguate entities from text alone, due to enormous number of surface forms arising from incomplete and inconsistent notations.", "labels": [], "entities": []}, {"text": "In addition, social media posts often include mentions of newly emerging entities unseen in training sets, making traditional context-based entity linking often not viable.", "labels": [], "entities": [{"text": "context-based entity linking", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.7313713630040487}]}, {"text": "However, as popular social media platforms are increasingly incorporating a mix of text and images (e.g. Snapchat, Instargram, Pinterest, etc.), we can advance the disambiguation task to incorporate additional visual context for understanding posts.", "labels": [], "entities": []}, {"text": "For example, the mention of 'juuustin' is completely ambiguous in its textual form, but an accompanying snap image of a concert scene may help disambiguate or re-rank among several lexical candidates (e.g. Justin Bieber (a pop singer) versus Justin Trudeau (a politician) in).", "labels": [], "entities": []}, {"text": "To this end, we introduce anew task called Multimodal Named Entity Disambiguation (MNED) that handles unique challenges for social media posts composed of extremely short text and images, aimed at disambiguationg entities by leveraging both textual and visual contexts.", "labels": [], "entities": [{"text": "Multimodal Named Entity Disambiguation (MNED)", "start_pos": 43, "end_pos": 88, "type": "TASK", "confidence": 0.7303268696580615}]}, {"text": "We then propose a novel zeroshot MNED model, which obtains visual context vectors from images with a CNN (, and combines with textual context extracted from a bidirectional LSTM () (Section 2.2).", "labels": [], "entities": []}, {"text": "In addition, we obtain embeddings representation of 1M entities from a knowledge graph, and train the MNED network to predict label embeddings of entities in the same space as corresponding knowledge graph embeddings (Section 2.4).", "labels": [], "entities": []}, {"text": "This approach effectively allows for zeroshot prediction of unseen entities, which is critical for scarce-label scenario due to extensive human annotation efforts required.", "labels": [], "entities": [{"text": "zeroshot prediction of unseen entities", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.9300905227661133}]}, {"text": "Lastly, we develop a lexical embeddings model that determines lexical similarity between a mention and potential entities, to aid in prediction of a correct entity (Section 2.3).", "labels": [], "entities": []}, {"text": "Section 2.5 details the model combining the components above.", "labels": [], "entities": []}, {"text": "Note that our method takes different perspectives from the previous work on NED () in the following important ways.", "labels": [], "entities": []}, {"text": "First, while most of the previous methods generate fixed \"candidates\" for disambiguation given a mention from mentionentity pair statistics (thus disambiguation is limited for entities with exact surface form matches), we do not fixate candidate generation, due to intractable variety of surface forms for each named entity and unforeseen mentions of emerging entities.", "labels": [], "entities": []}, {"text": "Instead, we have a lexical model incorporated into the discriminative score function that serves as soft normalization of various surface forms.", "labels": [], "entities": []}, {"text": "Second, we extract auxiliary visual contexts for detected entities from user-generated images accompanied with textual posts, which is crucial because captions in our dataset are substantially shorter than text documents inmost other NED datasets.", "labels": [], "entities": [{"text": "NED datasets", "start_pos": 234, "end_pos": 246, "type": "DATASET", "confidence": 0.7478520274162292}]}, {"text": "To the best of our knowledge, our work is the first in using visual contexts for the named entity disambiguation task.", "labels": [], "entities": [{"text": "named entity disambiguation task", "start_pos": 85, "end_pos": 117, "type": "TASK", "confidence": 0.7340495735406876}]}, {"text": "See Section 4 for the detailed literature review.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: for the new MNED task we introduce, we propose a deep zeroshot multimodal network with (1) a CNN-LSTM hybrid module that extracts contexts from both image and text, (2) a zeroshot learning layer which via embeddings projection allows for entity linking with 1M knowledge graph entities even for entities unseen from captions in training set, and (3) a lexical language model called Deep Levenshtein to compute lexical similarities between mentions and entities, relaxing the need for fixed candidates generation.", "labels": [], "entities": [{"text": "MNED task", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.816525012254715}]}, {"text": "We show that the proposed approaches successfully disambiguate incomplete mentions as well as polysemous entities, outperforming the state-of-the-art models on our newly crawled SnapCaptionsKB dataset, composed of 12K image-caption pairs with named entities annotated and linked with an external KB.", "labels": [], "entities": [{"text": "SnapCaptionsKB dataset", "start_pos": 178, "end_pos": 200, "type": "DATASET", "confidence": 0.8228825032711029}]}, {"text": "illustrates the proposed model, which maps each multimodal social media post data to one of the corresopnding entities in the KB.", "labels": [], "entities": []}, {"text": "Given a multimodal input that contains a mention of an ambiguous entity, we first extract textual and visual features contexts with RCNNs and Bi-LSTMs, respectively (Section 2.2).", "labels": [], "entities": []}, {"text": "We also obtain lexical character-level representation of a mention to compare with lexical representation of KB entities, using a proposed model called Deep Levenshtein (Section 2.3).", "labels": [], "entities": [{"text": "Deep Levenshtein", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.7848453521728516}]}, {"text": "We then get highdimensional label embeddings of KB entities constructed from a knowledge graph, where similar entities are mapped as neighbors in the same space (Section 2.4).", "labels": [], "entities": []}, {"text": "Finally, we aggregate all the contextual information extracted from surrounding text, image, and lexical notation of a mention, and predict the best matching KB entity based on knowledge graph label representation and lexical notation of KB entity candidates (Section 2.5).", "labels": [], "entities": []}, {"text": "The main architecture of our Multimodal NED network.", "labels": [], "entities": []}, {"text": "We extract contextual information from an image, surrounding words, and lexical embeddings of a mention.", "labels": [], "entities": []}, {"text": "The modality attention module determines weights for modalities, the weighted projections of which produce label embeddings in the same space as knowledge-base (KB) entity embeddings.", "labels": [], "entities": []}, {"text": "We predict a final candidate by ranking based on similarities with KB entity knowledge graph embeddings as well as with lexical embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task: Given a caption and an accompanying image (if available), the goal is to disambiguate and link a target mention in a caption to a corresponding entity from the knowledge base (1M subset of the Freebase knowledge graph).", "labels": [], "entities": [{"text": "Freebase knowledge graph", "start_pos": 199, "end_pos": 223, "type": "DATASET", "confidence": 0.9744453827540079}]}, {"text": "Our SnapCaptionsKB dataset is composed of 12K user-generated image and textual caption pairs where named entities in captions and their links to KB entities are manually labeled by expert human annotators.", "labels": [], "entities": [{"text": "SnapCaptionsKB dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7324108928442001}]}, {"text": "These captions are collected exclusively from snaps submitted to public and crowd-sourced stories (aka Live Stories or Our Stories).", "labels": [], "entities": [{"text": "Live Stories or Our Stories)", "start_pos": 103, "end_pos": 131, "type": "DATASET", "confidence": 0.8616395692030588}]}, {"text": "Examples of such stories are \"New York Story\" or \"Thanksgiving Story\", which are aggregated collections of snaps for various public venues, events, etc.", "labels": [], "entities": [{"text": "New York Story\" or \"Thanksgiving Story\"", "start_pos": 30, "end_pos": 69, "type": "DATASET", "confidence": 0.6899333265092638}]}, {"text": "Our data do not contain raw images, and we only provide textual captions and obfuscated visual descriptor features extracted from the pre-trained InceptionNet.", "labels": [], "entities": []}, {"text": "We split the dataset randomly into train (70%), validation (15%), and test sets (15%).", "labels": [], "entities": []}, {"text": "The captions data have average length of 29.5 characters (5.57 words) with vocabulary size 16,553, where 6,803 are considered unknown tokens from Stanford GloVE embeddings ().", "labels": [], "entities": []}, {"text": "Named entities annotated in the dataset include many of new and emerging entities found in various surface forms.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our SnapCaptionsKB is the only dataset that contains image-caption pairs with human-annotated named entities and their links to KB entities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NED performance on the SnapCaptionsKB dataset at Top-1, 3, 5, 10, 50 accuracies. The classi- fication is over 1M entities. Candidates generation methods: N/A, or over a fixed number of candidates  generated with methods: m\u2192e hash list and kNN (lexical neighbors).", "labels": [], "entities": [{"text": "NED", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9430210590362549}, {"text": "SnapCaptionsKB dataset", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.8786681294441223}]}, {"text": " Table 2: MNED performance (Top-1, 5, 10 accu- racies) on SnapCaptionsKB with varying qualities  of KB embeddings. Model: DZMNED (W+C+V)", "labels": [], "entities": [{"text": "DZMNED", "start_pos": 122, "end_pos": 128, "type": "DATASET", "confidence": 0.8572696447372437}]}]}