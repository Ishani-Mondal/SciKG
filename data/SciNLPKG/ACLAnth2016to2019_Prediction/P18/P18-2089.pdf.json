{"title": [{"text": "Addressing Noise in Multidialectal Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings are crucial to many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6755722165107727}]}, {"text": "The quality of embeddings relies on large non-noisy corpora.", "labels": [], "entities": []}, {"text": "Arabic dialects lack large corpora and are noisy, being linguistically disparate with no standardized spelling.", "labels": [], "entities": []}, {"text": "We make three contributions to address this noise.", "labels": [], "entities": []}, {"text": "First, we describe simple but effective adaptations to word embedding tools to maximize the informative content leveraged in each training sentence.", "labels": [], "entities": []}, {"text": "Second , we analyze methods for representing disparate dialects in one embedding space, either by mapping individual dialects into a shared space or learning a joint model of all dialects.", "labels": [], "entities": []}, {"text": "Finally, we evaluate via dictionary induction, showing that two met-rics not typically reported in the task enable us to analyze our contributions' effects on low and high frequency words.", "labels": [], "entities": []}, {"text": "In addition to boosting performance between 2-53%, we specifically improve on noisy, low frequency forms without compromising accuracy on high frequency forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.998016357421875}]}], "introductionContent": [{"text": "Many natural language processing tasks require word embeddings as inputs, yet quality embeddings require large, non-noisy corpora.", "labels": [], "entities": []}, {"text": "Dialectal Arabic (DA), the low register of highly diglossic Arabic, is problematically noisy.", "labels": [], "entities": [{"text": "Dialectal Arabic (DA)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6227042317390442}]}, {"text": "While the high register, Modern Standard Arabic (MSA), is uniform across educated circles in the Arab World, many varieties of DA are not even mutually intelligible (  Seldom written previously, DA is becoming the dominant form of Arabic on social media, yet annotated data are still scarce.", "labels": [], "entities": []}, {"text": "While complex morphology contributes to sparsity in both MSA and DA, noise from inter-dialect variation and unstandardized spelling further reduces token-to-type ratios in DA.", "labels": [], "entities": []}, {"text": "This limits opportunities to learn accurate vector representations for any given word.", "labels": [], "entities": []}, {"text": "shows that the MSA token-to-type ratio is over three times larger than DA, controlling for corpus size.", "labels": [], "entities": [{"text": "DA", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9598027467727661}]}, {"text": "This is still not nearly as large as English due to English's morphological simplicity.", "labels": [], "entities": []}, {"text": "Furthermore, the percentage of tokens belonging to low frequency types is three times greater in DA.", "labels": [], "entities": []}, {"text": "Many previous works ignore inter-dialect variation, training dialect agnostic embeddings, yet we show that modeling dialects individually yields Examples are drawn from the MADAR lexicon ( . Arabic script follows CODA guidelines (  and transliteration is presented in the HSB scheme).", "labels": [], "entities": [{"text": "MADAR lexicon", "start_pos": 173, "end_pos": 186, "type": "DATASET", "confidence": 0.8893828392028809}]}, {"text": "2 Our DA corpora are described in Section 3 whereas the MSA and English sentences are randomly drawn from the parallel corpus described in.", "labels": [], "entities": []}, {"text": "Tokens with type frequency < 5 6% 6% 2% 1%: Token and type based comparisons between two dialects of Arabic, MSA, and English in corpora of 13 million words each.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the quality of our DA word embeddings, we use the task of dictionary induction.", "labels": [], "entities": [{"text": "dictionary induction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7546908557415009}]}, {"text": "Given source dialect words from the evaluation dictionary, we attempt to recall appropriate translations in the target dialect based on cosine distance in multidialectal embedding space.", "labels": [], "entities": []}, {"text": "The standard metric for this task is precision@k=1 (P@1) (, measuring the fraction of source words in the evaluation dictionary for which the nearest target dialect neighbor matches any of the possible translations in the evaluation dictionary.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9987730383872986}]}, {"text": "We, however, are also concerned with how well multiple translations are recalled, as many words become polysemous in DA with short vowels omitted and spelling not standardized.", "labels": [], "entities": []}, {"text": "For this reason, many words appearing both in the seed and evaluation dictionaries do not map to the exact same set of possible translations in each.", "labels": [], "entities": []}, {"text": "Thus, many precision errors maybe forgiveable, so we focus on recall, reporting the metric recall@k=5 (R@5).", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9983174800872803}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9990106821060181}, {"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9565907120704651}]}, {"text": "Lastly, because types appear in a Zipfian distribution and type-based metrics disproportionately reflect accuracy in the tail, we report a frequency weighted recall@k=5 (WR@5) as well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9983325600624084}, {"text": "frequency weighted recall@k", "start_pos": 139, "end_pos": 166, "type": "METRIC", "confidence": 0.7867789030075073}, {"text": "WR@5)", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.9219694882631302}]}, {"text": "Considering both R@5 and WR@5 avoids the risk of improving performance on high or low frequency types at the expense of the other.", "labels": [], "entities": [{"text": "R@5", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.893354614575704}]}, {"text": "In, models FT, EXT, and PP+EXT are trained on individual dialects, then mapped using supervised SVECMAP into bidialectal embedding spaces.", "labels": [], "entities": [{"text": "FT", "start_pos": 11, "end_pos": 13, "type": "DATASET", "confidence": 0.8540783524513245}]}, {"text": "We experimented with all combinations of mapping algorithms and embedding models, yet SVECMAP consistently outperformed the other mapping algorithms.", "labels": [], "entities": []}, {"text": "We also report results for unsupervised UMUSE leveraging PP+EXT embeddings.", "labels": [], "entities": [{"text": "UMUSE leveraging PP+EXT embeddings", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.5366585354010264}]}, {"text": "ID is an identity dictionary mapping  all source words to themselves, thus representing dialect similarity.", "labels": [], "entities": []}, {"text": "PP+EXT or EXT always outperform the baseline FT, with PP+EXT being the best model in all but one instance according to WR@5 and R@5.", "labels": [], "entities": [{"text": "FT", "start_pos": 45, "end_pos": 47, "type": "DATASET", "confidence": 0.820643961429596}, {"text": "WR@5", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.815852681795756}]}, {"text": "PP+EXT successfully addresses noise as its gains are larger on non-frequency weighted R@5 than WR@5; i.e., it improves on low frequency words without compromising high frequency word accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.8945505023002625}]}, {"text": "Additionally, the consistency in results for WR@5 and R@5 as compared to P@1 suggests the small k is contributing to noise in the P@1 metric.", "labels": [], "entities": [{"text": "consistency", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.990993320941925}]}, {"text": "While ALLDA generally performs worse than the supervised mapping approaches, it typically performs slightly better on words which were not found in their seed dictionaries according to R@5, likely because it can leverage more data to learn better representations for non-ambiguous, low frequency shared forms.", "labels": [], "entities": []}, {"text": "Depending on the intended application, system combination could be ideal, querying ALLDA for low frequency forms appearing in multiple dialects, but not the seed.: Correlation between mapping performance and dialect similarity, i.e., the ID baseline, using PP+EXT embeddings.", "labels": [], "entities": []}, {"text": "As for supervised mapping algorithms, shows that, depending on the dialect pair in question, SMUSE's adversarial learning approach correlates with ID's metric of dialect similarity 20-30% more strongly than SVECMAP, which takes greater advantage of seed-evaluation domain similarity.", "labels": [], "entities": [{"text": "SMUSE", "start_pos": 93, "end_pos": 98, "type": "TASK", "confidence": 0.94790118932724}]}, {"text": "Accordingly, SVECMAP beats SMUSE on in-seed forms by 3-23%.", "labels": [], "entities": [{"text": "SVECMAP", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.4559537470340729}, {"text": "SMUSE", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.9149383902549744}]}, {"text": "That said, SMUSE is more robust to seed coverage, slightly outperforming SVECMAP on out-of-seed forms and UMUSE successfully bootstraps without supervision, unlike UVECMAP.", "labels": [], "entities": [{"text": "SMUSE", "start_pos": 11, "end_pos": 16, "type": "TASK", "confidence": 0.7019153237342834}, {"text": "seed coverage", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.6805406510829926}]}, {"text": "Still, the best performing option in the unsupervised setup is ALLDA.", "labels": [], "entities": [{"text": "ALLDA", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.6628075242042542}]}, {"text": "UMUSE's performance does not approach that of supervised alternatives as reported in.", "labels": [], "entities": [{"text": "UMUSE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7632800936698914}]}, {"text": "This is likely because they (as do) impose bilingual data scarcity constraints on high resource languages but do not consider the sparsity effects of noise common in low resource languages.", "labels": [], "entities": []}, {"text": "They use large quantities of domain consistent, spelling standardized monolingual data which are not available for DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.937465488910675}]}], "tableCaptions": [{"text": " Table 2: Token and type based comparisons be- tween two dialects of Arabic, MSA, and English  in corpora of 13 million words each.", "labels": [], "entities": []}, {"text": " Table 3: Dictionary induction results comparing  various multidialectal embedding models mapped  via supervised (SVECMAP) and unsupervised  (ALLDA, UMUSE) techniques.", "labels": [], "entities": []}, {"text": " Table 4: Correlation between mapping perfor- mance and dialect similarity, i.e., the ID baseline,  using PP+EXT embeddings.", "labels": [], "entities": []}]}