{"title": [{"text": "Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing", "labels": [], "entities": [{"text": "Multi-Domain Belief Tracking", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6591756939888}, {"text": "Knowledge Sharing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7269452810287476}]}], "abstractContent": [{"text": "Robust dialogue belief tracking is a key component in maintaining good quality dialogue systems.", "labels": [], "entities": [{"text": "Robust dialogue belief tracking", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8300602734088898}]}, {"text": "The tasks that dialogue systems are trying to solve are becoming increasingly complex, requiring scalabil-ity to multi-domain, semantically rich dialogues.", "labels": [], "entities": []}, {"text": "However, most current approaches have difficulty scaling up with domains because of the dependency of the model parameters on the dialogue ontology.", "labels": [], "entities": []}, {"text": "In this paper, a novel approach is introduced that fully utilizes semantic similarity between dialogue utterances and the ontol-ogy terms, allowing the information to be shared across domains.", "labels": [], "entities": []}, {"text": "The evaluation is performed on a recently collected multi-domain dialogues dataset, one order of magnitude larger than currently available corpora.", "labels": [], "entities": []}, {"text": "Our model demonstrates great capability in handling multi-domain dialogues, simultaneously outperforming existing state-of-the-art models in single-domain dialogue tracking tasks.", "labels": [], "entities": [{"text": "single-domain dialogue tracking", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.6716969112555186}]}], "introductionContent": [{"text": "Spoken Dialogue Systems (SDS) are computer programs that can hold a conversation with a human.", "labels": [], "entities": []}, {"text": "These can be task-based systems that help the user achieve specific goals, e.g. finding and booking hotels or restaurants.", "labels": [], "entities": [{"text": "finding and booking hotels or restaurants", "start_pos": 80, "end_pos": 121, "type": "TASK", "confidence": 0.7984155714511871}]}, {"text": "In order for the SDS to infer the user goals/intentions during the conversation, its Belief Tracking (BT) component maintains a distribution of states, called a belief state, across dialogue turns (.", "labels": [], "entities": []}, {"text": "The belief state is used by the system to take actions in each turn until the conversation is concluded and the user goal is achieved.", "labels": [], "entities": []}, {"text": "In order to extract these belief states from the conversation, traditional approaches use a Spoken Language Understanding (SLU) unit that utilizes a semantic dictionary to holdall the key terms, rephrasings and alternative mentions of a belief state.", "labels": [], "entities": []}, {"text": "The SLU then delexicalises each turn using this semantic dictionary, before it passes it to the BT component (.", "labels": [], "entities": [{"text": "BT", "start_pos": 96, "end_pos": 98, "type": "DATASET", "confidence": 0.9020512700080872}]}, {"text": "However, this approach is not scalable to multi-domain dialogues because of the effort required to define a semantic dictionary for each domain.", "labels": [], "entities": []}, {"text": "More advanced approaches, such as the Neural Belief Tracker (NBT), use word embeddings to alleviate the need for delexicalisation and combine the SLU and BT into one unit, mapping directly from turns to belief states . Nevertheless, the NBT model does not tackle the problem of mixing different domains in a conversation.", "labels": [], "entities": [{"text": "Neural Belief Tracker (NBT)", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.7923068503538767}]}, {"text": "Moreover, as each slot is trained independently without sharing information between different slots, scaling such approaches to large multi-domain systems is greatly hindered.", "labels": [], "entities": []}, {"text": "In this paper, we propose a model that jointly identifies the domain and tracks the belief states corresponding to that domain.", "labels": [], "entities": []}, {"text": "It uses semantic similarity between ontology terms and turn utterances to allow for parameter sharing between different slots across domains and within a single domain.", "labels": [], "entities": []}, {"text": "In addition, the model parameters are independent of the ontology/belief states, thus the dimensionality of the parameters does not increase with the size of the ontology, making the model practically feasible to deploy in multidomain environments without any modifications.", "labels": [], "entities": []}, {"text": "Finally, we introduce anew, large-scale corpora of natural, human-human conversations providing new possibilities to train complex, neural-based models.", "labels": [], "entities": []}, {"text": "Our model systematically improves upon state-of-the-art neural approaches both in single and multi-domain conversations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Neural approaches to statistical dialogue development, especially in a task-oriented paradigm, are greatly hindered by the lack of large scale datasets.", "labels": [], "entities": [{"text": "statistical dialogue development", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.8328844706217448}]}, {"text": "That is why, following the Wizard-of-Oz (WOZ) approach), we ran text-based multi-domain corpus data collection scheme through Amazon MTurk.", "labels": [], "entities": [{"text": "multi-domain corpus data collection", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.5491574332118034}]}, {"text": "The main goal of the data collection was to acquire humanhuman conversations between a tourist visiting a city and a clerk from an information center.", "labels": [], "entities": []}, {"text": "At the beginning of each dialogue the user (visitor) was given explicit instructions about the goal to fulfill, which often spanned multiple domains.", "labels": [], "entities": []}, {"text": "The task of the system (wizard) is to assist a visitor having an access to databases over domains.", "labels": [], "entities": []}, {"text": "The WOZ paradigm allowed us to obtain natural and semantically rich multi-topic dialogues spanning over multiple domains such as hotels, attractions, restaurants, booking trains or taxis.", "labels": [], "entities": []}, {"text": "The dialogues cover from 1 up to 5 domains per dialogue greatly varying in length and complexity.", "labels": [], "entities": []}, {"text": "We also used the extended WOZ 2.0 dataset ).", "labels": [], "entities": [{"text": "WOZ 2.0 dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9016038576761881}]}, {"text": "2 WOZ2 dataset consists of 1200 single topic dialogues constrained to the restaurant domain.", "labels": [], "entities": [{"text": "WOZ2 dataset", "start_pos": 2, "end_pos": 14, "type": "DATASET", "confidence": 0.79536572098732}]}, {"text": "All the weights were initialised using normal distribution of zero mean and unit variance and biases were initialised to zero.", "labels": [], "entities": []}, {"text": "ADAM optimizer () (with 64 batch size) is used to train all the models for 600 epochs.", "labels": [], "entities": []}, {"text": "Dropout () was used for regularisation (50% dropout rate on all the intermediate representations).", "labels": [], "entities": [{"text": "regularisation", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.786810040473938}]}, {"text": "For each of the two datasets we compare our proposed architecture (using either Bi-LSTM or CNN as encoders) to the NBT model 3 ).", "labels": [], "entities": [{"text": "NBT", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9271563291549683}]}, {"text": "This is because the dialogues in the new dataset are richer and more noisier, as a closer resemblance to real environment dialogues.", "labels": [], "entities": []}, {"text": "presents the results on multi-domain dialogues from the new dataset described in Section 5.", "labels": [], "entities": []}, {"text": "To demonstrate the difficulty of the multidomain belief tracking problem, values of a theoretical baseline that samples the belief state uniformly at random are also presented.", "labels": [], "entities": [{"text": "multidomain belief tracking", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.6594978868961334}]}, {"text": "Our model gracefully handles such a difficult task.", "labels": [], "entities": []}, {"text": "In most of the cases, CNNs demonstrate better performance than Bi-LSTMs.", "labels": [], "entities": []}, {"text": "We hypothesize that this comes from the effectiveness of extracting local and position-invariant features, which are crucial for semantic similarities).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WOZ 2.0 and new dataset test set accuracies of the NBT-CNN and the two variants of the  proposed model, for slots food, price range, area and joint goals.", "labels": [], "entities": [{"text": "WOZ 2.0", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.5966922789812088}, {"text": "NBT-CNN", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9865451455116272}]}]}