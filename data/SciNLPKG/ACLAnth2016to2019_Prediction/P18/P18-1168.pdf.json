{"title": [{"text": "Weakly Supervised Semantic Parsing with Abstract Examples", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7073242515325546}]}], "abstractContent": [{"text": "Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways.", "labels": [], "entities": []}, {"text": "First, a large search space of potential programs needs to be explored at training time to find a correct program.", "labels": [], "entities": []}, {"text": "Second, spurious programs that accidentally lead to a correct denotation add noise to training.", "labels": [], "entities": []}, {"text": "In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form.", "labels": [], "entities": []}, {"text": "We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training.", "labels": [], "entities": []}, {"text": "To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spurious-ness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation.", "labels": [], "entities": [{"text": "CNLVR", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9247009754180908}, {"text": "FALSE", "start_pos": 223, "end_pos": 228, "type": "METRIC", "confidence": 0.9721753001213074}]}, {"text": "Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9936093091964722}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9375280141830444}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9940664768218994}]}], "introductionContent": [{"text": "The goal of semantic parsing is to map language utterances to executable programs.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7230416387319565}]}, {"text": "Early work on statistical learning of semantic parsers utilized * Authors equally contributed to this work.", "labels": [], "entities": [{"text": "statistical learning of semantic parsers", "start_pos": 14, "end_pos": 54, "type": "TASK", "confidence": 0.8147050976753235}]}, {"text": "x :There is a small yellow item not touching any wally :True z :Exist(Filter, IsSmall(x)), Not(IsTouchingWall(x, Side.Any)))))) Figure 1: Overview of our visual reasoning setup for the CN-LVR dataset.", "labels": [], "entities": [{"text": "CN-LVR dataset", "start_pos": 185, "end_pos": 199, "type": "DATASET", "confidence": 0.953054279088974}]}, {"text": "Given an image rendered from a KB k and an utterance x, our goal is to parse x to a program z that results in the correct denotation y.", "labels": [], "entities": []}, {"text": "Our training data includes (x, k, y) triplets.", "labels": [], "entities": []}, {"text": "supervised learning, where training examples included pairs of language utterances and programs.", "labels": [], "entities": []}, {"text": "However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required.", "labels": [], "entities": []}, {"text": "This has led to a body of work on weaklysupervised semantic parsing (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6944322288036346}]}, {"text": "In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see).", "labels": [], "entities": []}, {"text": "Naturally, collecting denotations is much easier, because it can be performed by non-experts.", "labels": [], "entities": [{"text": "collecting denotations", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.869083434343338}]}, {"text": "Training semantic parsers from denotations rather than programs complicates training in two ways: (a) Search: The algorithm must learn to search through the huge space of programs at training time, in order to find the correct program.", "labels": [], "entities": []}, {"text": "This is a difficult search problem due to the combinatorial nature of the search space.", "labels": [], "entities": []}, {"text": "(b) Spurious-ness: Incorrect programs can lead to correct denotations, and thus the learner can go astray based on these programs.", "labels": [], "entities": []}, {"text": "Of the two mentioned problems, spuriousness has attracted relatively less attention.", "labels": [], "entities": []}, {"text": "Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (, and has presented an opportunity to better investigate the problem of spuriousness.", "labels": [], "entities": [{"text": "Cornell Natural Language for Visual Reasoning corpus (CNLVR)", "start_pos": 14, "end_pos": 74, "type": "DATASET", "confidence": 0.8208314597606658}]}, {"text": "In this task, an image with boxes that contains objects of various shapes, colors and sizes is shown.", "labels": [], "entities": []}, {"text": "Each image is paired with a complex natural language statement, and the goal is to determine whether the statement is true or false.", "labels": [], "entities": []}, {"text": "The task comes in two flavors, wherein one the input is the image (pixels), and in the other it is the knowledge-base (KB) from which the image was synthesized.", "labels": [], "entities": []}, {"text": "Given the KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7749290764331818}]}, {"text": "Because there are only two return values, it is easy to generate programs that execute to the right denotation, and thus spuriousness is a major problem compared to previous datasets.", "labels": [], "entities": []}, {"text": "In this paper, we present the first semantic parser for CNLVR.", "labels": [], "entities": [{"text": "CNLVR", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9074537754058838}]}, {"text": "Semantic parsing can be coarsely divided into a lexical task (i.e., mapping words and phrases to program constants), and a structural task (i.e., mapping language composition to program composition operators).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8057425320148468}]}, {"text": "Our core insight is that in closed worlds with clear semantic types, like spatial and visual reasoning, we can manually construct a small lexicon that clusters language tokens and program constants, and create a partially abstract representation for utterances and programs) in which the lexical problem is substantially reduced.", "labels": [], "entities": []}, {"text": "This scenario is ubiquitous in many semantic parsing applications such as calendar, restaurant reservation systems, housing applications, etc: the formal language has a compact semantic schema and a well-defined typing system, and there are canonical ways to express many program constants.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7723481357097626}]}, {"text": "We show that with abstract representations we can share information across examples and better tackle the search and spuriousness challenges.", "labels": [], "entities": []}, {"text": "By pulling together different examples that share the same abstract representation, we can identify programs that obtain high reward across multiple examples, thus reducing the problem of spuriousness.", "labels": [], "entities": []}, {"text": "This can also be done at search time, by augmenting the search state with partial programs that have been shown to be useful in earlier iterations.", "labels": [], "entities": []}, {"text": "Moreover, we can annotate a small number of abstract utterance-program pairs, and automatically generate training examples, that will be used to warm-start our model to an initialization point in which search is able to find correct programs.", "labels": [], "entities": []}, {"text": "We develop a formal language for visual reasoning, inspired by, and train a semantic parser over that language from weak supervision, showing that abstract examples substantially improve parser accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.9438799023628235}]}, {"text": "Our parser obtains an accuracy of 82.5%, a 14.7% absolute accuracy improvement compared to stateof-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997043013572693}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9441866874694824}]}, {"text": "All our code is publicly available at https://github.com/udiNaveh/ nlvr_tau_nlp_final_proj.", "labels": [], "entities": []}], "datasetContent": [{"text": "Model and Training Parameters The Bi-LSTM state dimension is 30.", "labels": [], "entities": [{"text": "Bi-LSTM state dimension", "start_pos": 34, "end_pos": 57, "type": "METRIC", "confidence": 0.9249495069185892}]}, {"text": "The decoder has one hidden layer of dimension 50, that takes the  In the weakly-supervised parser we encourage exploration with meritocratic gradient updates with \u03b2 = 0.5 ().", "labels": [], "entities": []}, {"text": "In the weaklysupervised parser we warm-start the parameters with the supervised parser, as mentioned above.", "labels": [], "entities": []}, {"text": "For optimization, Adam is used (), with learning rate of 0.001, and mini-batch size of 8.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9708803296089172}]}, {"text": "Pre-processing Because the number of utterances is relatively small for training a neural model, we take the following steps to reduce sparsity.", "labels": [], "entities": []}, {"text": "We lowercase all utterance tokens, and also use their lemmatized form.", "labels": [], "entities": []}, {"text": "We also use spelling correction to replace words that contain typos.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.6146306395530701}]}, {"text": "After pre-processing we replace every word that occurs less than 5 times with an UNK symbol.", "labels": [], "entities": [{"text": "UNK symbol", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.8084812164306641}]}, {"text": "Evaluation We evaluate on the public development and test sets of CNLVR as well as on the hidden test set.", "labels": [], "entities": [{"text": "CNLVR", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9747289419174194}]}, {"text": "The standard evaluation metric is accuracy, i.e., how many examples are correctly classified.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9995524287223816}]}, {"text": "In addition, we report consistency, which is the proportion of utterances for which the decoded program has the correct denotation for all 4 images/KBs.", "labels": [], "entities": [{"text": "consistency", "start_pos": 23, "end_pos": 34, "type": "METRIC", "confidence": 0.9961620569229126}]}, {"text": "It captures whether a model consistently produces a correct answer.", "labels": [], "entities": []}, {"text": "Baselines We compare our models to the MA-JORITY baseline that picks the majority class (TRUE in our case).", "labels": [], "entities": [{"text": "MA-JORITY baseline", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.7183231115341187}, {"text": "TRUE", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9915100336074829}]}, {"text": "We also compare to the stateof-the-art model reported by Dev.", "labels": [], "entities": []}, {"text": "Test  when taking the KB as input, which is a maximum entropy classifier (MAXENT).", "labels": [], "entities": [{"text": "MAXENT", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9599772095680237}]}, {"text": "For our models, we evaluate the following variants of our approach: \u2022 RULE: The rule-based parser from Sec.", "labels": [], "entities": [{"text": "RULE", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9984349608421326}]}, {"text": "\u2022 SUP.: The supervised semantic parser trained on augmented data as in Sec.", "labels": [], "entities": []}, {"text": "5.2 (5, 598 examples for training and 560 for validation).", "labels": [], "entities": [{"text": "validation", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.9468922019004822}]}, {"text": "\u2022 WEAKSUP.: Our full weakly-supervised semantic parser that uses abstract examples.", "labels": [], "entities": []}, {"text": "\u2022 +DISC: We add a discriminative re-ranker (Sec. 3) for both SUP. and WEAKSUP.", "labels": [], "entities": [{"text": "DISC", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9624577760696411}]}, {"text": "Main results describes our main results.", "labels": [], "entities": []}, {"text": "Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-theart.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9947479367256165}, {"text": "consistency", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9951464533805847}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9860716462135315}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.9992103576660156}]}, {"text": "The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996726512908936}, {"text": "RULE", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9352371692657471}, {"text": "MAXENT", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9610631465911865}, {"text": "semantic parsing", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7309213280677795}]}, {"text": "The supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns.", "labels": [], "entities": [{"text": "consistency", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9719573259353638}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9930017590522766}]}, {"text": "Our weakly-supervised parser significantly improves over SUP., reaching an accuracy of 81.7 before reranking, and 84.0 after re-ranking (on the public test set).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9993621706962585}]}, {"text": "Consistency results show an even crisper trend of improvement across the models.", "labels": [], "entities": [{"text": "crisper", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9991875290870667}]}], "tableCaptions": [{"text": " Table 4: Results on the development, public test (Test-P) and  hidden test (Test-H) sets. For each model, we report both  accuracy and consistency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9994877576828003}, {"text": "consistency", "start_pos": 136, "end_pos": 147, "type": "METRIC", "confidence": 0.9939194917678833}]}, {"text": " Table 5: Results of ablations of our main models on the de- velopment set. Explanation for the nature of the models is in  the body of the paper.", "labels": [], "entities": [{"text": "Explanation", "start_pos": 76, "end_pos": 87, "type": "METRIC", "confidence": 0.9765290021896362}]}]}