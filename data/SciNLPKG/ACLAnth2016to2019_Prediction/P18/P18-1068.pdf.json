{"title": [{"text": "Coarse-to-Fine Decoding for Neural Semantic Parsing", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.6410579184691111}]}], "abstractContent": [{"text": "Semantic parsing aims at mapping natural language utterances into structured meaning representations.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8461100459098816}]}, {"text": "In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages.", "labels": [], "entities": [{"text": "semantic parsing process", "start_pos": 84, "end_pos": 108, "type": "TASK", "confidence": 0.7872787515322367}]}, {"text": "Given an input utterance , we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over.", "labels": [], "entities": []}, {"text": "Then, we fill in missing details by taking into account the natural language input and the sketch itself.", "labels": [], "entities": []}, {"text": "Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms).", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.836650550365448}]}, {"text": "The successful application of recurrent neural networks to a variety of NLP tasks) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7671857178211212}]}, {"text": "The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure.", "labels": [], "entities": []}, {"text": "Examples include tree decoders (, decoders constrained by a grammar model (, or modular decoders which use syntax to dynamically compose various submodels (.", "labels": [], "entities": []}, {"text": "In this work, we propose to decompose the decoding process into two stages.", "labels": [], "entities": []}, {"text": "The first decoder focuses on predicting a rough sketch of the meaning representation, which omits low-level details, such as arguments and variable names.", "labels": [], "entities": []}, {"text": "Example sketches for various meaning representations are shown in.", "labels": [], "entities": []}, {"text": "Then, a second decoder fills in missing details by conditioning on the natural language input and the sketch itself.", "labels": [], "entities": []}, {"text": "Specifically, the sketch constrains the generation process and is encoded into vectors to guide decoding.", "labels": [], "entities": []}, {"text": "We argue that there are at least three advantages to the proposed approach.", "labels": [], "entities": []}, {"text": "Firstly, the decomposition disentangles high-level from low-level semantic information, which enables the decoders to model meaning at different levels of granularity.", "labels": [], "entities": []}, {"text": "As shown in, sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go.", "labels": [], "entities": []}, {"text": "Secondly, the model can explicitly share knowledge of coarse structures for the examples that have the same sketch (i.e., basic meaning), even though their actual meaning representations are different (e.g., due to different details).", "labels": [], "entities": []}, {"text": "Thirdly, after generating the sketch, the decoder knows what the basic meaning of the utterance looks like, and the model can use it as global context to improve the prediction of the final details.", "labels": [], "entities": []}, {"text": "Our framework is flexible and not restricted to specific tasks or any particular model.", "labels": [], "entities": []}, {"text": "We conduct experiments on four datasets representative of various semantic parsing tasks ranging from logical form parsing, to code generation, and SQL query generation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.7515203058719635}, {"text": "logical form parsing", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.6554428339004517}, {"text": "code generation", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.718486025929451}, {"text": "SQL query generation", "start_pos": 148, "end_pos": 168, "type": "TASK", "confidence": 0.8724015355110168}]}, {"text": "We adapt our architecture to these tasks and present several ways to obtain sketches from their respective meaning representations.", "labels": [], "entities": []}, {"text": "Experimental results show that our framework achieves competitive performance compared", "labels": [], "entities": []}], "datasetContent": [{"text": "Length Example GEO 7.6 13.7 6.9 x : which state has the most rivers running through it?", "labels": [], "entities": [{"text": "GEO 7.6 13.7 6.9", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.8388284146785736}]}, {"text": "y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all flights from dallas before 10am y : (lambda $0 e (and (flight $0) (from $0 dallas:ci) (< (departure time $0) 1000:ti))) a : (lambda#2 (and flight@1 from@2 (< departure time@1 ? ) ) )  with previous systems, despite employing relatively simple sequence decoders.", "labels": [], "entities": [{"text": "ATIS 11.1 21.1 9.2", "start_pos": 135, "end_pos": 153, "type": "DATASET", "confidence": 0.937121108174324}]}, {"text": "We present results on the three semantic parsing tasks discussed in Section 4.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7785907983779907}]}, {"text": "Our implementation and pretrained models are available at https:// github.com/donglixp/coarse2fine.", "labels": [], "entities": []}, {"text": "Preprocessing For GEO and ATIS, we used the preprocessed versions provided by, where natural language expressions are lowercased and stemmed with NLTK (, and entity mentions are replaced by numbered markers.", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8643688559532166}, {"text": "GEO", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.9302368760108948}, {"text": "ATIS", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.5383673906326294}]}, {"text": "We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact.", "labels": [], "entities": []}, {"text": "We employed the preprocessed DJANGO data provided by, where input expressions are tokenized by NLTK, and quoted strings in the input are replaced with place holders.", "labels": [], "entities": [{"text": "DJANGO data", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.8286168873310089}]}, {"text": "WIK-ISQL was preprocessed by the script provided by, where inputs were lowercased and tokenized by Stanford CoreNLP).", "labels": [], "entities": [{"text": "WIK-ISQL", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8976637125015259}, {"text": "Stanford CoreNLP", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.892142653465271}]}, {"text": "Configuration Model hyperparameters were cross-validated on the training set for GEO, and were validated on the development split for the other datasets.", "labels": [], "entities": [{"text": "GEO", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9512471556663513}]}, {"text": "Dimensions of hidden vectors and word embeddings were selected from {250, 300} and {150, 200, 250, 300}, respectively.", "labels": [], "entities": []}, {"text": "The dropout rate was selected from {0.3, 0.5}.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.8901117742061615}]}, {"text": "Label smoothing () was employed for GEO and ATIS.", "labels": [], "entities": [{"text": "Label smoothing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.741653174161911}, {"text": "GEO", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9719803333282471}, {"text": "ATIS", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8804498910903931}]}, {"text": "The smoothing parameter was set to 0.1.", "labels": [], "entities": []}, {"text": "For WIKISQL, the hidden size of \u03c3(\u00b7)", "labels": [], "entities": [{"text": "WIKISQL", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9013391733169556}]}], "tableCaptions": [{"text": " Table schema: \ud97b\udf59Pianist\ud97b\udf59Conductor\ud97b\udf59Record Company\ud97b\udf59Year of Recording\ud97b\udf59Format\ud97b\udf59  x : What record company did conductor Mikhail Snitko record for after 1996?  y : SELECT Record Company WHERE (Year of Recording > 1996) AND (Conductor = Mikhail Snitko)  a : WHERE > AND =", "labels": [], "entities": [{"text": "Pianist\ud97b\udf59Conductor\ud97b\udf59Record Company\ud97b\udf59Year of Recording\ud97b\udf59Format", "start_pos": 16, "end_pos": 73, "type": "DATASET", "confidence": 0.661085935930411}, {"text": "SELECT", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9662295579910278}, {"text": "WHERE", "start_pos": 250, "end_pos": 255, "type": "METRIC", "confidence": 0.6500577330589294}]}, {"text": " Table 2: Accuracies on GEO and ATIS.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9975302815437317}, {"text": "GEO", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.768768846988678}, {"text": "ATIS", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.6894745826721191}]}, {"text": " Table 3: DJANGO results. Accuracies in the first  and second block are taken from Ling et al.", "labels": [], "entities": [{"text": "DJANGO", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7627227902412415}, {"text": "Accuracies", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9988396763801575}]}, {"text": " Table 4: Evaluation results on WIKISQL. Accu- racies in the first block are taken from Zhong et al.  (2017) and Xu et al. (2017).", "labels": [], "entities": [{"text": "WIKISQL", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9311711192131042}, {"text": "Accu- racies", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9577844341595968}]}, {"text": " Table 5: Sketch accuracy. For ONESTAGE,  sketches are extracted from the meaning represen- tations it generates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9926713705062866}, {"text": "ONESTAGE", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.7658433318138123}]}]}