{"title": [], "abstractContent": [{"text": "We introduce a novel architecture for dependency parsing: stack-pointer networks (STACKPTR).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8158471584320068}]}, {"text": "Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion.", "labels": [], "entities": []}, {"text": "The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step.", "labels": [], "entities": []}, {"text": "The STACKPTR parser benefits from the information of the whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers.", "labels": [], "entities": [{"text": "STACKPTR parser", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.61085245013237}]}, {"text": "Yet, the number of steps for building any (including non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n 2) time complexity.", "labels": [], "entities": []}, {"text": "We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performance on 21 of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8196366727352142}, {"text": "deep language understanding", "start_pos": 132, "end_pos": 159, "type": "TASK", "confidence": 0.6162641048431396}]}, {"text": "Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution, * Work done while at Carnegie Mellon University.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.9712241888046265}]}, {"text": "2016), sentiment analysis (, machine translation (, information extraction, word sense disambiguation (, and low-resource languages processing).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9838125109672546}, {"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8358966708183289}, {"text": "information extraction", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8113910853862762}, {"text": "word sense disambiguation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.6898584763209025}]}, {"text": "There are two dominant approaches to dependency parsing (: local and greedy transitionbased algorithms (, and the globally optimized graph-based algorithms.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8214732110500336}]}, {"text": "Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions.", "labels": [], "entities": [{"text": "Transition-based dependency parsers read words sequentially", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7409778386354446}]}, {"text": "The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence.", "labels": [], "entities": []}, {"text": "The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies ().", "labels": [], "entities": []}, {"text": "Previous studies have explored solutions to address this challenge.", "labels": [], "entities": []}, {"text": "Stack LSTMs ( ) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser's state.", "labels": [], "entities": []}, {"text": "proposed a globally normalized transition model to replace the locally normalized classifier.", "labels": [], "entities": []}, {"text": "However, the parsing accuracy is still behind state-of-the-art graph-based parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9783788919448853}, {"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9716196060180664}]}, {"text": "Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search overall possible trees fora sentence to find the globally highest scoring tree.", "labels": [], "entities": [{"text": "Graph-based dependency parsers", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6972772081693014}]}, {"text": "Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers have achieved the state-of-the-art accuracies on a number of treebanks in different languages.", "labels": [], "entities": []}, {"text": "Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n 3 ) time complexity for first-order models,b) and higher polynomials for higherorder models).", "labels": [], "entities": [{"text": "O(n 3 ) time complexity", "start_pos": 79, "end_pos": 102, "type": "METRIC", "confidence": 0.9031435166086469}]}, {"text": "In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (STACKPTR).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8470229804515839}]}, {"text": "STACKPTR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy.", "labels": [], "entities": [{"text": "STACKPTR", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7029548287391663}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9727646708488464}]}, {"text": "Our STACKPTR parser has a pointer network ( as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures.", "labels": [], "entities": []}, {"text": "The STACKPTR parser performs parsing in an incremental, topdown, depth-first fashion; at each step, it generates an arc by assigning a child for the headword at the top of the internal stack.", "labels": [], "entities": [{"text": "STACKPTR parser", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.5524766445159912}]}, {"text": "This architecture makes it possible to capture information from the whole sentence and all the previously derived subtrees, while maintaining a number of parsing steps linear in the sentence length.", "labels": [], "entities": []}, {"text": "We evaluate our parser on 29 treebanks across 20 languages and different dependency annotation schemas, and achieve state-of-the-art performance on 21 of them.", "labels": [], "entities": []}, {"text": "The contributions of this work are summarized as follows: (i) We propose a neural network architecture for dependency parsing that is simple, effective, and efficient.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8863143920898438}]}, {"text": "(ii) Empirical evaluations on benchmark datasets over 20 languages show that our method achieves state-of-the-art performance on 21 different treebanks 1 . (iii) Comprehensive error analysis is conducted to compare the proposed method to a strong graph-based baseline using biaffine attention).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: UAS and LAS of four versions of our model on test sets for three languages, together with top- performing parsing systems. \"T\" and \"G\" indicate transition-and graph-based models, respectively. For  BIAF, we provide the original results reported in", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6620927453041077}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.927169144153595}, {"text": "BIAF", "start_pos": 208, "end_pos": 212, "type": "DATASET", "confidence": 0.537126898765564}]}, {"text": " Table 2: Parsing performance on the test data of  PTB with different versions of POS tags.", "labels": [], "entities": [{"text": "PTB", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9079226851463318}]}, {"text": " Table 4: UAS and LAS on both the development and test datasets of 12 treebanks from UD Treebanks,  together with BIAF for comparison.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.6110185980796814}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9861184358596802}, {"text": "UD Treebanks", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9453373849391937}, {"text": "BIAF", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9188814163208008}]}]}