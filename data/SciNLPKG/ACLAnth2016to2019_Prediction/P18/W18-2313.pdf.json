{"title": [{"text": "Phrase2VecGLM: Neural generalized language model-based semantic tagging for complex query reformulation in medical IR", "labels": [], "entities": [{"text": "Neural generalized language model-based semantic tagging", "start_pos": 15, "end_pos": 71, "type": "TASK", "confidence": 0.6880782047907511}, {"text": "complex query reformulation", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.7038794954617819}, {"text": "IR", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.6723870635032654}]}], "abstractContent": [{"text": "In fact-based information retrieval, state-of-the-art performance is traditionally achieved by knowledge graphs driven by knowledge bases, as they can represent facts about and capture relationships between entities very well.", "labels": [], "entities": [{"text": "fact-based information retrieval", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.6603761613368988}]}, {"text": "However, in domains such as medical information retrieval , where addressing specific information needs of complex queries may require understanding query intent by capturing novel associations between potentially latent concepts, these systems can fall short.", "labels": [], "entities": [{"text": "medical information retrieval", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.6531525552272797}]}, {"text": "In this work, we develop a novel, completely unsupervised, neural language model-based ranking approach for semantic tagging of documents, using the document to be tagged as a query into the model to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text.", "labels": [], "entities": [{"text": "semantic tagging of documents", "start_pos": 108, "end_pos": 137, "type": "TASK", "confidence": 0.8050302863121033}]}, {"text": "For this we extend the word embedding-based generalized language model (GLM) due to (Ganguly et al., 2015), to employ phrasal embeddings, and use the semantic tags thus obtained for downstream query expansion, both directly and in feedback loop settings.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 193, "end_pos": 208, "type": "TASK", "confidence": 0.6960976868867874}]}, {"text": "Our method, evaluated using the TREC 2016 clinical decision support challenge dataset, shows statistically significant improvement not only over various baselines that use standard MeSH terms and UMLS concepts for query expansion, but also over baselines using human expert-assigned concept tags for the queries, on top of a standard Okapi BM25-based document retrieval system.", "labels": [], "entities": [{"text": "TREC 2016 clinical decision support challenge dataset", "start_pos": 32, "end_pos": 85, "type": "DATASET", "confidence": 0.842206997530801}, {"text": "query expansion", "start_pos": 214, "end_pos": 229, "type": "TASK", "confidence": 0.7693903744220734}]}], "introductionContent": [{"text": "Existing state-of-the-art information retrieval (IR) systems such as knowledge graphs (, or information extraction techniques centered around entity relationships (, that often rely on some form of weak supervision from ontological or knowledgebase (KB) sources, tend to perform quite reliably on fact-based information retrieval and factoid question answering tasks.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8714204072952271}, {"text": "information extraction", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7421829402446747}, {"text": "information retrieval", "start_pos": 308, "end_pos": 329, "type": "TASK", "confidence": 0.7262965142726898}, {"text": "factoid question answering", "start_pos": 334, "end_pos": 360, "type": "TASK", "confidence": 0.6646887560685476}]}, {"text": "However, such systems maybe limited in their ability to address the complex information needs of specific types of queries () in domains such as clinical decision support ( or guided product search (, due to: 1) complex and subjective, or lengthy nature of the query containing multiple topics, 2) vocabulary mismatch between the query expression and knowledge representations in the document collection, and 3) lack of sufficiently complete knowledge bases of \"related concepts\", covering all possible relations between candidate concepts that may exist in a collection, essential for effectively addressing these types of queries ().", "labels": [], "entities": []}, {"text": "We hypothesize, that similar to human experts who can determine the aboutness of an unseen document by recalling meaningful concepts gleaned from similar past experiences via shared contexts, a completely unsupervised machine learning model could be trained to associate documents within a large collection with meaningful concepts discovered by fully leveraging shared contexts within and between documents, thus surfacing \"related\" concepts specific to the current context (.", "labels": [], "entities": []}, {"text": "As a trivial example, ordinarily unrelated concepts (noun phrases, in this work) such as \"scarlet macaw\" and \"raccoon\" occurring in separate documents d 1 and d 2 may become related by a novel context such as \"exotic pets\" that may occur as terms in a query or as a phrase in a document d p which could be related to both d 1 and d 2 . If by some means, documents d 1 and d 2 were semantically tagged with the phrase \"exotic pets\" via d p , those documents would surface in the event of such a query (.", "labels": [], "entities": []}, {"text": "This could thus help to better close the vocabulary gap between potential user queries and the documents.", "labels": [], "entities": []}, {"text": "To our knowledge, ours is the first work that employs word and phrase-level embeddings for local context analysis in a pseudorelevance feedback setting, using a language model-based document ranking framework, to semantically tag documents with appropriate concepts for use in downstream retrieval tasks ().", "labels": [], "entities": [{"text": "local context analysis", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.6995099981625875}]}, {"text": "The main contributions of our work, are as follows: 1) We present a novel use fora neural language modeling approach that leverages shared context between documents within a collection via phrase-based embeddings (1, 2, and 3-grams), finding the right trade-off between the local context around each term versus its global context within the collection, incorporating a local context analysis-based pseudo-relevance feedback mechanism) for concept extraction.", "labels": [], "entities": [{"text": "concept extraction", "start_pos": 440, "end_pos": 458, "type": "TASK", "confidence": 0.7711860239505768}]}, {"text": "2) Our method is fully unsupervised, i.e. it includes no outside sources of knowledge in the training, leveraging instead the shared contexts within the document collection itself, via word and phrasal embeddings, mimicking a human that potentially reads through the documents in the collection and uses the seen information to make relevant concept tag judgments on unseen documents.", "labels": [], "entities": []}, {"text": "3) Our method presents a black-box approach for tagging any corpus of documents with meaningful concepts, treating it as a closed system.", "labels": [], "entities": []}, {"text": "Thus the concept associations can be pre-computed offline or periodically, as new documents are added to the collection and can reside outside of the document retrieval system, allowing for it to be plugged into any such system, or for the underlying retrieval system to be changed.", "labels": [], "entities": []}, {"text": "It is also in contrast to previous approaches to document categorization for retrieval, such as those based on clustering, e.g. clustering by committee () or semantic class induction as in), LDA-based topic modeling () and supervised or active learning approaches () for concept extraction in information retrieval.", "labels": [], "entities": [{"text": "LDA-based topic modeling", "start_pos": 191, "end_pos": 215, "type": "TASK", "confidence": 0.608482817808787}, {"text": "concept extraction", "start_pos": 271, "end_pos": 289, "type": "TASK", "confidence": 0.7471233904361725}]}], "datasetContent": [{"text": "The TREC Clinical Decision Support (CDS) task track investigates techniques to evaluate biomedical literature retrieval systems for providing answers to generic clinical questions about patient cases (, with a goal toward making relevant biomedical information more discoverable for clinicians.", "labels": [], "entities": [{"text": "TREC Clinical Decision Support (CDS) task track", "start_pos": 4, "end_pos": 51, "type": "TASK", "confidence": 0.7409242656495836}, {"text": "biomedical literature retrieval", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.66115074356397}]}, {"text": "For the 2016 TREC CDS challenge, actual electronic health records (EHR) of patients, in the form of case reports, typically describing a challenging medical case, as shown in are used.", "labels": [], "entities": [{"text": "TREC CDS challenge", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6522260705629984}]}, {"text": "A case report is, for our purposes a complex query having a specific information need.", "labels": [], "entities": []}, {"text": "There are 30 queries in the challenge dataset, corresponding to such case reports, divided into 3 topic types, at 3 levels of granularity Note, Description and Summary text.", "labels": [], "entities": [{"text": "Summary", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.9379948377609253}]}, {"text": "The target document collection is the Open Access Subset of PubMed Central (PMC), containing 1.25 million articles consisting of title, keywords, abstract and body sections.", "labels": [], "entities": [{"text": "Open Access Subset of PubMed Central (PMC)", "start_pos": 38, "end_pos": 80, "type": "DATASET", "confidence": 0.6957557267612882}]}, {"text": "In our work, we develop our query expansion method as a blackbox system using only a subset of 100K documents of the entire collection for which human judgments are made available by TREC.", "labels": [], "entities": [{"text": "TREC", "start_pos": 183, "end_pos": 187, "type": "DATASET", "confidence": 0.8747126460075378}]}, {"text": "This allows us to derive \"inferred measures\" for Normalized Discounted Cumulative Gain (NDCG) and Precision at 10 (P@10) scores for our evaluation).", "labels": [], "entities": [{"text": "Precision at 10 (P@10) scores", "start_pos": 98, "end_pos": 127, "type": "METRIC", "confidence": 0.9231479830212064}]}, {"text": "However, we evaluate our method on the entire collection of 1.25 million PMC articles on a separate search engine setup using an ElasticSearch () instance, that indexes this entire set of articles on all available fields.", "labels": [], "entities": []}, {"text": "Our unsupervised document tagging method as outlined in Section 4 employs only the abstract field of the 100K PMC articles, for developing the Phrase2VecGLM language model-based document ranking subsequently used in query expansion.", "labels": [], "entities": [{"text": "document tagging", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.705432191491127}, {"text": "PMC articles", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.7859986126422882}, {"text": "query expansion", "start_pos": 216, "end_pos": 231, "type": "TASK", "confidence": 0.7436282932758331}]}, {"text": "We run two different sets of experiments: (1) Direct query expansion of the 30 queries in the TREC dataset, using UMLS concepts for our augmented baselines, and, (2) Feedback loop-based query expansion where we use the concept tags fora subset of the top returned articles for the Summary Text-based queries ran against an ElasticSearch index, as query expansion terms, (here MeSH terms-based QE) is an augmented baseline), and evaluate both types of runs against our ElasticSearch (ES) index setup described in Section 6.2.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.9410027265548706}, {"text": "Feedback loop-based query expansion", "start_pos": 166, "end_pos": 201, "type": "TASK", "confidence": 0.5982012897729874}]}, {"text": "For the search engine-based evaluation of our proposed method, we replicated an ElasticSearch (ES) instance setup with similar settings used in a 2016 challenge submission).", "labels": [], "entities": []}, {"text": "Among the different algorithms available, BM25 (with parameters k1=3 and b=0.75) was selected as the ranking algorithm in our setup due to slightly better performance observed than others, with a logical OR querying model implemented, and the minimum percentage match criterion in ES, for search queries, set at 15% of the keywords matched fora document.", "labels": [], "entities": [{"text": "BM25", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.5100452303886414}, {"text": "minimum percentage match criterion", "start_pos": 243, "end_pos": 277, "type": "METRIC", "confidence": 0.6492880508303642}]}, {"text": "Since our GLM outlined in Section 4.2 uses the abstract field of the article for query expansion, we boosted the abstract field 4 times and the title field 2 times in our ES search index setup.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7978146374225616}]}, {"text": "outlines our results obtained with the various experimental runs described in Section 6.", "labels": [], "entities": []}, {"text": "The hyper-parameters for our best performing models were empirically determined and set to beat (\u03bb, \u03b1, \u03b2) = (0.2, 0.3, 0.2) for the word embeddingbased GLM and (\u03bb, \u03b1, \u03b2) = (0.2, 0.4, 0.2) for the phrasal embedding-based GLM, similar to those reported by.", "labels": [], "entities": []}, {"text": "All models were evaluated for statistical significance against the respective baselines using a two-sided Wilcoxon signed rank test, for p << 0.01, indicated by boldface value, if found to be significant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for IR after Query Expansion (QE) by different methods using unigram and phrasal  GLM-generated QE terms, in direct and feedback loop settings. Bold face values indicate statistical  significance at p << 0.01 over the previous result or baseline. Single asterisks indicate our best perform- ing models. Double asterisks indicate inferred measures", "labels": [], "entities": [{"text": "IR after Query Expansion (QE)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.8485150422368731}]}]}