{"title": [{"text": "Using pseudo-senses for improving the extraction of synonyms from word embeddings", "labels": [], "entities": [{"text": "extraction of synonyms from word embeddings", "start_pos": 38, "end_pos": 81, "type": "TASK", "confidence": 0.721461703379949}]}], "abstractContent": [{"text": "The methods proposed recently for specializing word embeddings according to a particular perspective generally rely on external knowledge.", "labels": [], "entities": []}, {"text": "In this article, we propose Pseudofit, anew method for specializing word embeddings according to semantic similarity without any external knowledge.", "labels": [], "entities": [{"text": "Pseudofit", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.942191481590271}]}, {"text": "Pseudofit exploits the notion of pseudo-sense for building several representations for each word and uses these representations for making the initial embeddings more generic.", "labels": [], "entities": []}, {"text": "We illustrate the interest of Pseudofit for acquiring synonyms and study several variants of Pseudofit according to this perspective.", "labels": [], "entities": []}], "introductionContent": [{"text": "The interest aroused byword embeddings in Natural Language Processing, especially for neural models, has led to propose methods for creating them from texts () but also for specializing them according to a particular viewpoint.", "labels": [], "entities": []}, {"text": "This viewpoint generally comes in the form of set of lexical relations.", "labels": [], "entities": []}, {"text": "For instance, specialize word embeddings towards semantic similarity or relatedness by relying either on synonyms or free lexical associations.", "labels": [], "entities": []}, {"text": "Methods such as Retrofitting (, Counterfitting or PARAGRAM () fall within the same framework.", "labels": [], "entities": [{"text": "Retrofitting", "start_pos": 16, "end_pos": 28, "type": "METRIC", "confidence": 0.4986473619937897}, {"text": "PARAGRAM", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9587871432304382}]}, {"text": "The specialization of word embeddings can also come from the way they are built.", "labels": [], "entities": []}, {"text": "For instance, bring word embeddings towards similarity rather than relatedness by using dependency-based distributional contexts rather than linear bag-of-word contexts.", "labels": [], "entities": []}, {"text": "Finally, some methods aim at improving word embeddings but without a clearly defined orientation, such as the All-but-the-Top method, which focuses on dimensionality reduction, or , which exploits morphological relations.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.6916417628526688}]}, {"text": "In this article, we propose Pseudofit, a method that improves word embeddings without external knowledge and focuses on semantic similarity and synonym extraction.", "labels": [], "entities": [{"text": "synonym extraction", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8727091252803802}]}, {"text": "The principle of Pseudofit is to exploit the notion of pseudo-sense coming from word sense disambiguation for building representations accounting for distributional variability and to create better word embeddings by bringing these representations closer together.", "labels": [], "entities": []}, {"text": "We show the interest of Pseudofit and its variants through both intrinsic and extrinsic evaluations.", "labels": [], "entities": []}], "datasetContent": [{"text": "For implementing Pseudofit, we randomly select at the level of sentences a 1 billion word subpart of the Annotated English Gigaword corpus ().", "labels": [], "entities": [{"text": "Annotated English Gigaword corpus", "start_pos": 105, "end_pos": 138, "type": "DATASET", "confidence": 0.8651943951845169}]}, {"text": "This corpus is made of news articles in English processed by the Stanford CoreNLP toolkit ( ).", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9265257120132446}]}, {"text": "We use this corpus under its lemmatized form.", "labels": [], "entities": []}, {"text": "The building of the embeddings are performed with word2vecf, the adaptation of word2vec from (, with the best parameter values from ( ): minimal count=5, vector size=300, window size=5, 10 negative examples and 10 \u22125 for the subsampling probability of the most frequent words.", "labels": [], "entities": []}, {"text": "For PARAGRAM, we adopt most of the parameter values from  Our first evaluation of Pseudofit at word level is a classical intrinsic evaluation consisting in measuring fora set of word pairs the Spearman's rank correlation between human judgments and the similarity of these words computed from their embeddings by the Cosine measure.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.4426926076412201}]}, {"text": "This evaluation is performed for the nouns of three large enough reference datasets: SimLex-999 (    Our second evaluation, which is our main focus, is a more extrinsic task consisting in extracting synonyms . This extraction is performed by ranking a set of candidate synonyms for each target word according to the similarity, computed hereby the Cosine measure, of their embeddings.", "labels": [], "entities": []}, {"text": "We evaluate the relevance of this ranking as in Information Retrieval with R-precision (R prec.", "labels": [], "entities": [{"text": "relevance", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.96345055103302}, {"text": "Information Retrieval", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7328563630580902}]}, {"text": "), MAP (Mean Average Precision) and precisions at various ranks (P@r).", "labels": [], "entities": [{"text": "MAP", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.9961349964141846}, {"text": "Mean Average Precision)", "start_pos": 8, "end_pos": 31, "type": "METRIC", "confidence": 0.9231197983026505}, {"text": "precisions", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9987565279006958}, {"text": "P@r)", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.8957809656858444}]}, {"text": "Our reference is made up of the synonyms of WordNet) while both our target words and candidate synonyms are made up of the nouns with more than ten occurrences in each half of our corpus, which represents 20,813 nouns.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9591966271400452}]}, {"text": "gives the result of this second evaluation for 11,481 nouns with synonyms in WordNet among our 20,813 targets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9776790142059326}]}, {"text": "As in the first evaluation, Pseudofit significantly 4 outperforms the initial embeddings.", "labels": [], "entities": [{"text": "Pseudofit", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9642975926399231}]}, {"text": "Moreover, replacing PARAGRAM with Retrofitting or Counter-fitting leads to a systematic decrease of results, which emphasizes the importance of the repelling term of PARAGRAM.", "labels": [], "entities": [{"text": "PARAGRAM", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.696399986743927}, {"text": "Retrofitting", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9311317801475525}, {"text": "PARAGRAM", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.5937259197235107}]}, {"text": "This term probably prevents the representation of a word from being changed too much by its pseudosenses, which are interesting variants in terms of representations but were built from half of the corpus only.", "labels": [], "entities": []}, {"text": "Figure 1: Gain brought by Pseudofit for MAP according to the ambiguity of the target word Finally, we performed a finer analysis of these results according to the frequency and the degree of ambiguity of the target words.", "labels": [], "entities": []}, {"text": "Concerning frequency, shows that Pseudofit is particularly efficient for the lower half of the target words in terms of frequency, with a large increase of 5.3 points for R-precision, 6.7 points for MAP, 7.0 points for P@1 and 5.2 points for P@2 while the largest increase for the higher half of the target words is equal to 1.1 points for MAP.", "labels": [], "entities": [{"text": "frequency", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9546631574630737}, {"text": "MAP", "start_pos": 199, "end_pos": 202, "type": "METRIC", "confidence": 0.7745330333709717}]}, {"text": "One possible explanation of this gap between high and low frequency words is linked to the degree of ambiguity of words: high frequency words are more likely to be polysemous and Pseudofit does not take into account the polysemy of words.", "labels": [], "entities": []}, {"text": "tends to confirm this hypothesis by showing that the improvement brought by Pseudofit fora word is inversely proportional to its ambiguity as estimated by its number of senses in WordNet 5 .", "labels": [], "entities": [{"text": "WordNet 5", "start_pos": 179, "end_pos": 188, "type": "DATASET", "confidence": 0.9402982592582703}]}], "tableCaptions": [{"text": " Table 1: Intrinsic evaluation of Pseudofit (\u00d7100)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9537325501441956}, {"text": "Pseudofit", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.8809642195701599}]}, {"text": " Table 2: Evaluation of Pseudofit for synonym ex- traction (differences / INITIAL, \u00d7100)", "labels": [], "entities": [{"text": "Pseudofit", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9865173101425171}, {"text": "INITIAL", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9921339750289917}]}, {"text": " Table 3: Evaluation of Pseudofit for synonym ex- traction according to the frequency (high or low)  of the target words (differences / INITIAL, \u00d7100)", "labels": [], "entities": [{"text": "INITIAL", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9919352531433105}]}, {"text": " Table 5: Evaluation of Pseudofit for identifying  sentence similarity", "labels": [], "entities": [{"text": "identifying  sentence similarity", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.8416455984115601}]}]}