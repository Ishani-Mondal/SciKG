{"title": [{"text": "Document Context Neural Machine Translation with Memory Networks", "labels": [], "entities": [{"text": "Document Context Neural Machine Translation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.878613007068634}]}], "abstractContent": [{"text": "We present a document-level neural machine translation model which takes both source and target document context into account using memory networks.", "labels": [], "entities": [{"text": "document-level neural machine translation", "start_pos": 13, "end_pos": 54, "type": "TASK", "confidence": 0.6618668586015701}]}, {"text": "We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unob-served target translations in the document.", "labels": [], "entities": []}, {"text": "The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components , one each for the source and target side, to capture the documental inter-dependencies.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.6996553540229797}]}, {"text": "We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent.", "labels": [], "entities": []}, {"text": "Experimental results of English translations from French, German, and Es-tonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.9987055063247681}, {"text": "METEOR", "start_pos": 261, "end_pos": 267, "type": "METRIC", "confidence": 0.874857485294342}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has proven to be powerful (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7893549104531606}]}, {"text": "It is on-par, and in some cases, even surpasses the traditional statistical MT () while enjoying more flexibility and significantly less manual effort for feature engineering.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.7209420204162598}]}, {"text": "Despite their flexibility, most neural MT models translate sentences independently.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9363850951194763}]}, {"text": "Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a few previous sentences, are neglected in sentencebased translation (.", "labels": [], "entities": [{"text": "sentencebased translation", "start_pos": 172, "end_pos": 197, "type": "TASK", "confidence": 0.6790912449359894}]}, {"text": "There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps.;; propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements.", "labels": [], "entities": [{"text": "document-wide machine translation", "start_pos": 40, "end_pos": 73, "type": "TASK", "confidence": 0.6139693558216095}, {"text": "document translation", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.7660036385059357}]}, {"text": "More recently, there have been a few attempts to incorporate source side context into neural MT (; however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.8383917212486267}]}, {"text": "The latter two report deteriorated performance when using the target-side context.", "labels": [], "entities": []}, {"text": "In this paper, we present a document-level machine translation model which combines sentencebased NMT () with memory networks.", "labels": [], "entities": [{"text": "document-level machine translation", "start_pos": 28, "end_pos": 62, "type": "TASK", "confidence": 0.6124962468942007}]}, {"text": "We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated.", "labels": [], "entities": []}, {"text": "We conduct experiments on three language pairs: French-English, German-English and Estonian-English.", "labels": [], "entities": []}, {"text": "The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9977496266365051}, {"text": "METEOR", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.7491512298583984}]}], "datasetContent": [{"text": "We conducted experiments on three language pairs: French-English, German-English and Estonian-English.", "labels": [], "entities": []}, {"text": "shows the statistics of the datasets used in our experiments.", "labels": [], "entities": []}, {"text": "The French-English dataset is based on the TED Talks corpus 1 () where each talk is considered a document.", "labels": [], "entities": [{"text": "French-English dataset", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.745512455701828}, {"text": "TED Talks corpus 1", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.7963958084583282}]}, {"text": "The EstonianEnglish data comes from the Europarl v7 corpus 2 (: Training/dev/test corpora statistics: number of documents (\u00d7100) and sentences (\u00d71000), average document length (in sentences) and source/target vocabulary size (\u00d71000).", "labels": [], "entities": [{"text": "EstonianEnglish data", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9054601788520813}, {"text": "Europarl v7 corpus 2", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.9575616717338562}]}, {"text": "For DeEn, we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets.", "labels": [], "entities": [{"text": "DeEn", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9076497554779053}]}, {"text": "The news-commentary corpus has document boundaries already provided.", "labels": [], "entities": []}, {"text": "We pre-processed all corpora to remove very short documents and those with missing translations.", "labels": [], "entities": []}, {"text": "Out-of-vocabulary and rare words (frequency less than 5) are replaced by the <UNK> token, following.", "labels": [], "entities": [{"text": "UNK> token", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.8282727003097534}]}, {"text": "Evaluation Measures We use BLEU () and METEOR ( scores to measure the quality of the generated translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9989233613014221}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9954127669334412}]}, {"text": "We use bootstrap resampling) to measure statistical significance, p < 0.05, comparing to the baselines.", "labels": [], "entities": []}, {"text": "Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (), on top of the basic sentence-level NMT implementation in mantis (.", "labels": [], "entities": [{"text": "document-level neural machine translation", "start_pos": 52, "end_pos": 93, "type": "TASK", "confidence": 0.5824119970202446}]}, {"text": "For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively.", "labels": [], "entities": [{"text": "GRU", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.8683522343635559}]}, {"text": "The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder.", "labels": [], "entities": []}, {"text": "GRUs are used instead of LSTMs to reduce the number of parameters in the main model.", "labels": [], "entities": [{"text": "GRUs", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8074520230293274}]}, {"text": "The RNN hidden dimensions and word embedding sizes are set to 512 in the translation and memory components, and the alignment dimension is set to 256 in the translation model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training/dev/test corpora statistics: num- ber of documents (\u00d7100) and sentences (\u00d71000),  average document length (in sentences) and  source/target vocabulary size (\u00d71000). For De- En, we report statistics of the two test sets  news-test2011 and news-test2016.", "labels": [], "entities": []}, {"text": " Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Docu- ment NMT model. bold: Best performance,  \u2020: Statistically significantly better than the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991538524627686}, {"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9958190321922302}]}, {"text": " Table 3: Number of model parameters (millions).", "labels": [], "entities": []}, {"text": " Table 5: Unigram BLEU for our  Memory-to-Context Document NMT  models vs. S-NMT and Source con- text NMT baselines. bold: Best per- formance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8838945031166077}]}, {"text": " Table 6: Analysis of target context model.", "labels": [], "entities": []}]}