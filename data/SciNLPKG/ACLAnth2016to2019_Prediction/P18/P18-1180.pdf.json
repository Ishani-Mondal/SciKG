{"title": [{"text": "A Distributional and Orthographic Aggregation Model for English Derivational Morphology", "labels": [], "entities": [{"text": "Distributional and Orthographic Aggregation", "start_pos": 2, "end_pos": 45, "type": "TASK", "confidence": 0.6693078577518463}]}], "abstractContent": [{"text": "Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering.", "labels": [], "entities": [{"text": "text generation", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.728546068072319}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7944017350673676}, {"text": "abstractive question answering", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.6341854731241862}]}, {"text": "In this work, we tackle the task of derived word generation.", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.6377553145090739}]}, {"text": "That is, given the word \"run,\" we attempt to generate the word \"runner\" for \"someone who runs.\"", "labels": [], "entities": []}, {"text": "We identify two key problems in generating derived words from root words and transformations: suffix ambiguity and orthographic irregularity.", "labels": [], "entities": []}, {"text": "We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space.", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6701135834058126}]}, {"text": "Our best open-vocabulary model, which can generate novel words, and our best closed-vocabulary model, show 22% and 37% relative error reductions over current state-of-the-art systems on the same dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "The explicit modeling of morphology has been shown to improve a number of tasks (Seeker and C \u00b8 etinoglu, 2015;.", "labels": [], "entities": []}, {"text": "Ina large number of the world's languages, many words are composed through morphological operations on subword units.", "labels": [], "entities": []}, {"text": "Some languages are rich in inflectional morphology, characterized by syntactic transformations like pluralization.", "labels": [], "entities": []}, {"text": "Similarly, languages like English are rich in derivational morphology, where the semantics of words are composed from * These authors contributed equally; listed alphabetically.", "labels": [], "entities": []}, {"text": "The AGENT derivational transformation, for example, answers the question, what is the word for 'someone who runs'? with the answer, a runner.", "labels": [], "entities": [{"text": "AGENT derivational transformation", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7089886168638865}]}, {"text": "1 Here, AGENT is spelled out as suffixing -ner onto the root verb run.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9906371235847473}]}, {"text": "We tackle the task of derived word generation.", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6151715815067291}]}, {"text": "In this task, a root word x and a derivational transformation tare given to the learner.", "labels": [], "entities": []}, {"text": "The learner's job is to produce the result of the transformation on the root word, called the derived wordy.", "labels": [], "entities": []}, {"text": "gives examples of these transformations.", "labels": [], "entities": []}, {"text": "Previous approaches to derived word generation model the task as a character-level sequenceto-sequence (seq2seq) problem ().", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6322226425011953}]}, {"text": "The letters from the root word and some encoding of the transformation are given as input to a neural encoder, and the decoder is trained to produce the derived word, one letter at a time.", "labels": [], "entities": []}, {"text": "We identify the following problems with these approaches: First, because these models are unconstrained, they can generate sequences of characters that do: The goal of derived word generation is to produce the derived word, y, given both the root word, x, and the transformation t, as demonstrated herewith examples from the dataset.", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 168, "end_pos": 191, "type": "TASK", "confidence": 0.6328360239664713}]}, {"text": "We argue that requiring the model to generate a known word is a reasonable constraint in the special case of English derivational morphology, and doing so avoids a large number of common errors.", "labels": [], "entities": [{"text": "English derivational morphology", "start_pos": 109, "end_pos": 140, "type": "TASK", "confidence": 0.5881285568078359}]}, {"text": "Second, sequence-based models can only generalize string manipulations (such as \"add -ment\") if they appear frequently in the training data.", "labels": [], "entities": []}, {"text": "Because of this, they are unable to generate derived words that do not follow typical patterns, such as generating truth as the nominative derivation of true.", "labels": [], "entities": []}, {"text": "We propose to learn a function for each transformation in a low dimensional vector space that corresponds to mapping from representations of the root word to the derived word.", "labels": [], "entities": []}, {"text": "This eliminates the reliance on orthographic information, unlike related approaches to distributional semantics, which operate at the suffix level (.", "labels": [], "entities": []}, {"text": "We contribute an aggregation model of derived word generation that produces hypotheses independently from two separate learned models: one from a seq2seq model with only orthographic information, and one from a feed-forward network using only distributional semantic information in the form of pretrained word vectors.", "labels": [], "entities": [{"text": "derived word generation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6573994755744934}]}, {"text": "The model learns to choose between the hypotheses according to the relative confidence of each.", "labels": [], "entities": []}, {"text": "This system can be interpreted as learning to decide between positing an orthographically regular form or a semantically salient word.", "labels": [], "entities": []}, {"text": "See fora diagram of our model.", "labels": [], "entities": []}, {"text": "We show that this model helps with two open problems with current state-of-the-art seq2seq derived word generation systems, suffix ambiguity and orthographic irregularity (Section 2).", "labels": [], "entities": [{"text": "word generation", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.7091832906007767}]}, {"text": "We also improve the accuracy of seq2seq-only derived word systems by adding external information through constrained decoding and hypothesis rescoring.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9989101886749268}]}, {"text": "These methods provide orthogonal gains to our main contribution.", "labels": [], "entities": []}, {"text": "We evaluate models in two categories: open vocabulary models that can generate novel words unattested in a preset vocabulary, and closedvocabulary models, which cannot.", "labels": [], "entities": []}, {"text": "Our best openvocabulary and closed-vocabulary models demonstrate 22% and 37% relative error reductions over the current state of the art.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the derivational morphology dataset used in our experiments and how we collected the dictionary and token frequencies used in the dictionary constraint and rescorer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The average accuracies and number of states explored  in the search graph of 30 runs of the SEQ model with various  search procedures. The BEAM models use a beam size of 10.", "labels": [], "entities": []}, {"text": " Table 4: The accuracies and edit distances of our best  open-vocabulary and closed-vocabulary models, AGGR and  AGGR+FREQ+DICT compared to prior work, evaluated at the  transformation level. For edit distance, lower is better.", "labels": [], "entities": [{"text": "AGGR", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.7518133521080017}, {"text": "FREQ+DICT", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.8446414470672607}]}, {"text": " Table 5: The accuracies of the top-10 best outputs for the SEQ,  SEQ+DICT, and prior work demonstrate that the dictionary  constraint significantly improves the overall candidate quality.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9673580527305603}, {"text": "DICT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.5654910206794739}]}]}