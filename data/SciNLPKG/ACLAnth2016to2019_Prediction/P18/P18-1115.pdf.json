{"title": [{"text": "A Stochastic Decoder for Neural Machine Translation *", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7496398289998373}]}], "abstractContent": [{"text": "The process of translation is ambiguous, in that there are typically many valid translations fora given sentence.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9783942103385925}]}, {"text": "This gives rise to significant variation in parallel corpora , however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7663190364837646}]}, {"text": "To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.797197550535202}]}, {"text": "We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models.", "labels": [], "entities": []}, {"text": "Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural architectures have taken the field of machine translation by storm and are in the process of replacing phrase-based systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7403625249862671}]}, {"text": "Based on the encoder-decoder framework ) increasingly complex neural systems are being developed at the moment.", "labels": [], "entities": []}, {"text": "These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions) or stacked self-attention layers (.", "labels": [], "entities": []}, {"text": "These architectural changes have led to great performance improvements over classical RNN-based neural translation systems ().", "labels": [], "entities": [{"text": "RNN-based neural translation", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.6421417792638143}]}, {"text": "Surprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures.", "labels": [], "entities": []}, {"text": "A notable exception is the work of who introduce a sentence-level latent In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of.", "labels": [], "entities": []}, {"text": "Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.).", "labels": [], "entities": []}, {"text": "Neural machine translation (NMT) models are incapable of capturing this variation, however.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7510272612174352}]}, {"text": "This is because their likelihood function incorporates the statistical assumption that there is one (and only one) output 1 fora given source sentence, i.e., Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data.", "labels": [], "entities": []}, {"text": "The noise sources are modelled as Gaussian random variables.", "labels": [], "entities": []}, {"text": "The contributions of this work are: \u2022 The introduction of an NMT system that is capable of capturing word-level variation in translation data.", "labels": [], "entities": []}, {"text": "\u2022 A thorough discussions of issues encountered when training this model.", "labels": [], "entities": []}, {"text": "In particular, we motivate the use of KL scaling as introduced by theoretically.", "labels": [], "entities": []}, {"text": "Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence.", "labels": [], "entities": []}, {"text": "The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.", "labels": [], "entities": []}, {"text": "\u2022 An empirical demonstration of the improvements achievable with the proposed model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations.", "labels": [], "entities": [{"text": "IWSLT 2016 data set", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9736185371875763}]}, {"text": "We trained models to translate from English into Arabic, Czech, French and German.", "labels": [], "entities": []}, {"text": "The number of sentences for each language after preprocessing is shown in.", "labels": [], "entities": []}, {"text": "The vocabulary was split into 50,000 subword units using Google's sentence piece 3 software in its standard settings.", "labels": [], "entities": []}, {"text": "As our baseline NMT systems we use Sockeye . Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2.", "labels": [], "entities": []}, {"text": "We report baselines with and without dropout ().", "labels": [], "entities": []}, {"text": "For dropout a retention probability of 0.5 was used.", "labels": [], "entities": [{"text": "retention probability", "start_pos": 14, "end_pos": 35, "type": "METRIC", "confidence": 0.9886049032211304}]}, {"text": "As a second baseline we use our own implementation of the model of which contains a single sentence-level Gaussian latent variable (SENT).", "labels": [], "entities": [{"text": "Gaussian latent variable (SENT)", "start_pos": 106, "end_pos": 137, "type": "METRIC", "confidence": 0.6710701733827591}]}, {"text": "Our implementation differs from theirs in three aspects.", "labels": [], "entities": []}, {"text": "First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the inference network ( use the average of all states).", "labels": [], "entities": []}, {"text": "Second, the latent variable is smaller in size than the one used by.", "labels": [], "entities": []}, {"text": "5 This was done to make their model and the stochastic decoder proposed here as similar as possible.", "labels": [], "entities": []}, {"text": "Finally, their implementation was based on groundhog whereas ours builds on Sockeye.", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9783998131752014}]}, {"text": "Our stochastic decoder model (SDEC) is also built on top of the basic Sockeye model.", "labels": [], "entities": []}, {"text": "It adds the components described in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "Recall that the functions that compute the means and standard deviations are implemented by neural nets with a single hidden layer with tanh activation.", "labels": [], "entities": []}, {"text": "The width of that layer is twice the size of the latent variable.", "labels": [], "entities": []}, {"text": "In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1).", "labels": [], "entities": []}, {"text": "The scale started from 0 and was increased by 1 /20,000 after each mini-batch.", "labels": [], "entities": [{"text": "scale", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.8060942888259888}]}, {"text": "Thus, at iteration t the scale is min( t /20,000, 1).", "labels": [], "entities": []}, {"text": "All models use 1028 units for the LSTM hid-den state (or 512 for each direction in the bidirectional LSTMs) and 256 for the attention mechansim.", "labels": [], "entities": []}, {"text": "Training is done with Adam ().", "labels": [], "entities": []}, {"text": "In decoding we use abeam of size 5 and output the most likely word at each position.", "labels": [], "entities": []}, {"text": "We deterministically set all latent variables to their mean values during decoding.", "labels": [], "entities": []}, {"text": "Monte Carlo decoding () is difficult to apply to our setting as it would require sampling entire translations.", "labels": [], "entities": []}, {"text": "Results We show the BLEU scores for all models that we tested on the IWSLT data set in Table 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9996565580368042}, {"text": "IWSLT data set", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9664955337842306}]}, {"text": "The stochastic decoder dominates the Sockeye baseline across all 4 languages, and outperforms SENT on most languages.", "labels": [], "entities": [{"text": "Sockeye baseline", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.9320173859596252}, {"text": "SENT", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9003403782844543}]}, {"text": "Except on German, there is a trend towards smaller latent variable sizes being more helpful.", "labels": [], "entities": [{"text": "German", "start_pos": 10, "end_pos": 16, "type": "DATASET", "confidence": 0.9767736196517944}]}, {"text": "This is inline with findings by and who also used relatively small latent variables.", "labels": [], "entities": []}, {"text": "This observation also implies that our model does not improve simply because it has more parameters than the baseline.", "labels": [], "entities": []}, {"text": "That the margin between the SDEC and SENT models is not large was to be expected for two reasons.", "labels": [], "entities": []}, {"text": "First, and have shown that stochastic RNNs lead to enormous improvements in modelling continuous sequences but only modest increases in performance for discrete sequences (such as natural language).", "labels": [], "entities": []}, {"text": "Second, translation performance is measured in BLEU score.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9714122414588928}, {"text": "BLEU score", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9765913188457489}]}, {"text": "We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.", "labels": [], "entities": [{"text": "SDEC", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.8579385876655579}, {"text": "ELBO", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9979721903800964}, {"text": "SENT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9468424916267395}]}, {"text": "How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future research.", "labels": [], "entities": []}, {"text": "Qualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation.", "labels": [], "entities": [{"text": "Qualitative Analysis", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7923440337181091}]}, {"text": "To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled instead of fixed.", "labels": [], "entities": [{"text": "IWSLT test set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9341203769048055}]}, {"text": "Contrary to the BLEU-based evaluation, beam search was not used in this evaluation in order to avoid interaction between different latent variable samples.", "labels": [], "entities": [{"text": "BLEU-based", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9841246604919434}, {"text": "beam search", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.816925436258316}]}, {"text": "See for examples of syntactic and lexical variation.", "labels": [], "entities": []}, {"text": "It is important to note that we do not sample from the categorical output distribution.", "labels": [], "entities": []}, {"text": "For each target position we pick the most likely word.", "labels": [], "entities": []}, {"text": "A non-stochastic NMT system would always yield the same translation in this scenario.", "labels": [], "entities": []}, {"text": "Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system.", "labels": [], "entities": []}, {"text": "This supports our initial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address.", "labels": [], "entities": [{"text": "SENT", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.8115357160568237}]}, {"text": "Like the model of, SENT presumably tends to ignore the latent variable.", "labels": [], "entities": [{"text": "SENT", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.7584185600280762}]}], "tableCaptions": [{"text": " Table 1: Number of parallel sentence pairs for each  language paired with English for IWSLT data.", "labels": [], "entities": [{"text": "IWSLT data", "start_pos": 87, "end_pos": 97, "type": "DATASET", "confidence": 0.8682967722415924}]}, {"text": " Table 2: BLEU scores for different models on the IWSLT data for translation into English. Recall that  all SDEC and SENT models used KL scaling during training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995736479759216}, {"text": "IWSLT data", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.9589135944843292}, {"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9731491804122925}]}]}