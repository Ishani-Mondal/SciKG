{"title": [{"text": "Stochastic Answer Networks for Machine Reading Comprehension", "labels": [], "entities": [{"text": "Machine Reading Comprehension", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8025749524434408}]}], "abstractContent": [{"text": "We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension.", "labels": [], "entities": []}, {"text": "Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neu-ral network during the training.", "labels": [], "entities": []}, {"text": "We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adver-sarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 115, "end_pos": 158, "type": "DATASET", "confidence": 0.8700244682175773}, {"text": "Microsoft MAchine Reading COmprehension Dataset (MS MARCO)", "start_pos": 192, "end_pos": 250, "type": "DATASET", "confidence": 0.7198987801869711}]}], "introductionContent": [{"text": "Machine reading comprehension (MRC) is a challenging task: the goal is to have machines read a text passage and then answer any question about the passage.", "labels": [], "entities": [{"text": "Machine reading comprehension (MRC)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8769954741001129}]}, {"text": "This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversational agents and customer service support.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.6572042306264242}]}, {"text": "It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning.", "labels": [], "entities": [{"text": "MRC", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9849408268928528}]}, {"text": "For instance, the following example from the MRC dataset SQuAD ( illustrates the need for synthesis of information across sentences and multiple steps of reasoning: To infer the answer (the underlined portion of the passage P ), the model needs to first perform coreference resolution so that it knows \"They\" refers \"V&A Theator\", then extract the subspan in the direct object corresponding to the answer.", "labels": [], "entities": [{"text": "MRC dataset SQuAD", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.8974930644035339}, {"text": "coreference resolution", "start_pos": 262, "end_pos": 284, "type": "TASK", "confidence": 0.882874071598053}]}, {"text": "This kind of iterative process can be viewed as a form of multi-step reasoning.", "labels": [], "entities": []}, {"text": "Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process.", "labels": [], "entities": [{"text": "MRC", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9652469754219055}]}, {"text": "The first models employed a predetermined fixed number of steps ().", "labels": [], "entities": []}, {"text": "Later, proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question.", "labels": [], "entities": []}, {"text": "Further,  empirically showed that dynamic multi-step reasoning outperforms fixed multi-step reasoning, which in turn outperforms single-step reasoning on two distinct MRC datasets (SQuAD and MS MARCO).", "labels": [], "entities": [{"text": "MRC datasets", "start_pos": 167, "end_pos": 179, "type": "DATASET", "confidence": 0.83499476313591}, {"text": "MS MARCO", "start_pos": 191, "end_pos": 199, "type": "DATASET", "confidence": 0.7842777967453003}]}, {"text": "In this work, we derive an alternative multi-step reasoning neural network for MRC.", "labels": [], "entities": [{"text": "MRC", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9245462417602539}]}, {"text": "During training, we fix the number of reasoning steps, but perform stochastic dropout on the answer module (final layer predictions).", "labels": [], "entities": []}, {"text": "During decoding, we generate answers based on the average of predictions in all steps, rather than the final step.", "labels": [], "entities": []}, {"text": "We call this a stochastic answer network (SAN) because the stochastic dropout is applied to the answer module; albeit simple, this technique significantly improves the robustness and overall accuracy of the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9986611604690552}]}, {"text": "Intuitively this works because while the model successively refines its prediction over multiple steps, each step is still trained to generate the same answer; we are performing a kind of stochastic ensemble over the model's successive predic- x: Illustration of \"stochastic prediction dropout\" in the answer module during training.", "labels": [], "entities": []}, {"text": "At each reasoning step t, the model combines memory (bottom row) with hidden states s t\u22121 to generate a prediction (multinomial distribution).", "labels": [], "entities": []}, {"text": "Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of the remaining distributions.", "labels": [], "entities": []}, {"text": "Stochastic prediction dropout is illustrated in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset: We evaluate on the Stanford Question Answering Dataset (SQuAD) (.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 28, "end_pos": 71, "type": "DATASET", "confidence": 0.8632147652762276}]}, {"text": "This contains about 23K passages and 100K questions.", "labels": [], "entities": []}, {"text": "The passages come from approximately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing.", "labels": [], "entities": []}, {"text": "The crowdsourced workers are asked to read a passage (a paragraph), come up with questions, then mark the answer span.", "labels": [], "entities": []}, {"text": "All results are on the official development set, unless otherwise noted.", "labels": [], "entities": []}, {"text": "Two evaluation metrics are used: Exact Match (EM), which measures the percentage of span predictions that matched anyone of the ground truth answer exactly, and Macro-averaged F1 score, which measures the average overlap between the prediction and the ground truth answer.", "labels": [], "entities": [{"text": "Exact Match (EM)", "start_pos": 33, "end_pos": 49, "type": "METRIC", "confidence": 0.9747350692749024}, {"text": "F1 score", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9462352693080902}]}, {"text": "Implementation details: The spaCy tool 2 is used to tokenize the both passages and questions, and generate lemma, part-of-speech and named entity tags.", "labels": [], "entities": []}, {"text": "We use 2-layer BiLSTM with d = 128 hidden units for both passage and question encoding.", "labels": [], "entities": [{"text": "question encoding", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.6612188071012497}]}, {"text": "The mini-batch size is set to) is used as our optimizer.", "labels": [], "entities": []}, {"text": "The learning rate is set to 0.002 at first and decreased by half after every 10 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.975355714559555}]}, {"text": "We set the dropout rate for all the hidden units of LSTM, and the answer module output layer to 0.4.", "labels": [], "entities": []}, {"text": "To prevent degenerate output, we ensure that at least one step in the answer module is active during training.", "labels": [], "entities": []}, {"text": "The results in show that SAN outperforms V-Net ( and becomes the new state of the art 6 .", "labels": [], "entities": [{"text": "V-Net", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.897552490234375}]}], "tableCaptions": [{"text": " Table 1: Main results-Comparison of different answer module architectures. Note that SAN performs  best in both Exact Match and F1 metrics.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 113, "end_pos": 124, "type": "METRIC", "confidence": 0.8013954758644104}, {"text": "F1", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.9531408548355103}]}, {"text": " Table 2: Test performance on SQuAD. Results are sorted by Test F1.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8774678707122803}, {"text": "F1", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.7578441500663757}]}, {"text": " Table 3: Robustness of SAN (5-step) on differ- ent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.", "labels": [], "entities": [{"text": "initialization", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.9767961502075195}]}, {"text": " Table 4: Effect of number of steps: best and  worst results are boldfaced.", "labels": [], "entities": []}, {"text": " Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7559438943862915}, {"text": "F1 score", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9658072590827942}]}, {"text": " Table 6: Prediction on different steps T . Note  that the SAN model is trained using 5 steps.", "labels": [], "entities": [{"text": "Prediction", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.984188437461853}]}, {"text": " Table 7: MS MARCO devset results.", "labels": [], "entities": [{"text": "MS MARCO devset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7228312393029531}]}]}