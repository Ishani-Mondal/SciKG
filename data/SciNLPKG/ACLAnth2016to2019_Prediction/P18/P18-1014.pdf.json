{"title": [{"text": "Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8545102477073669}]}], "abstractContent": [{"text": "We present anew neural sequence-to-sequence model for extractive summa-rization called SWAP-NET (Sentences and Words from Alternating Pointer Networks).", "labels": [], "entities": []}, {"text": "Extractive summaries comprising a salient subset of input sentences, often also contain important key words.", "labels": [], "entities": []}, {"text": "Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using anew two-level pointer network based architecture.", "labels": [], "entities": []}, {"text": "SWAP-NET identifies both salient sentences and key words in an input document , and then combines them to form the extractive summary.", "labels": [], "entities": []}, {"text": "Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic summarization aims to shorten a text document while maintaining the salient information of the original text.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5674657225608826}]}, {"text": "The practical need for such systems is growing with the rapid and continuous increase in textual information sources in multiple domains.", "labels": [], "entities": []}, {"text": "Summarization tools can be broadly classified into two categories: extractive and abstractive.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9551180601119995}]}, {"text": "Extractive summarization selects parts of the input document to create its summary while abstractive summarization generates summaries that may have words or phrases not present in the input document.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7898250818252563}]}, {"text": "Abstractive summarization is clearly harder as methods have to address factual and grammatical errors that maybe introduced and problems in utilizing external knowledge sources to obtain paraphrasing or generalization.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7051017582416534}]}, {"text": "Extractive summarizers obviate the need to solve these problems by selecting the most salient textual units (usually sentences) from the input documents.", "labels": [], "entities": []}, {"text": "As a result, they generate summaries that are grammatically and semantically more accurate than those from abstractive methods.", "labels": [], "entities": []}, {"text": "While they may have problems like incorrect or unclear referring expressions or lack of coherence, they are computationally simpler and more efficient to generate.", "labels": [], "entities": []}, {"text": "Indeed, state-of-the-art extractive summarizers are comparable or often better in performance to competitive abstractive summarizers (see) fora recent empirical comparison).", "labels": [], "entities": []}, {"text": "Classical approaches to extractive summarization have relied on human-engineered features from the text that are used to score sentences in the input document and select the highestscoring sentences.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7774070203304291}]}, {"text": "These include graph or constraint-optimization based approaches as well as classifier-based methods.", "labels": [], "entities": []}, {"text": "A review of these approaches can be found in.", "labels": [], "entities": []}, {"text": "Some of these methods generate summaries from multiple documents.", "labels": [], "entities": [{"text": "summaries from multiple documents", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8096406608819962}]}, {"text": "In this paper, we focus on single document summarization.", "labels": [], "entities": [{"text": "single document summarization", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.5689129928747813}]}, {"text": "Modern approaches that show the best performance are based on end-to-end deep learning models that do not require human-crafted features.", "labels": [], "entities": []}, {"text": "Neural models have tremendously improved performance in several difficult problems in NLP such as machine translation) and question-answering (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7762354016304016}]}, {"text": "Deep models with thousands of parameters require large, labeled datasets and for summarization this hurdle of labeled data was surmounted by, through the creation of a labeled dataset of news stories from CNN and Daily Mail consisting of around 280,000 documents and human-generated summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.9936700463294983}]}, {"text": "Recurrent neural networks with encoderdecoder architecture () have been successful in a variety of NLP tasks where an encoder obtains representations of input sequences and a decoder generates target sequences.", "labels": [], "entities": []}, {"text": "Attention mechanisms () are used to model the effects of different loci in the input sequence during decoding.", "labels": [], "entities": []}, {"text": "Pointer networks ( use this mechanism to obtain target sequences wherein each decoding step is used to point to elements of the input sequence.", "labels": [], "entities": []}, {"text": "This pointing ability has been effectively utilized by state-of-the-art extractive and abstractive summarizers.", "labels": [], "entities": []}, {"text": "In this work, we design SWAP-NET anew deep learning model for extractive summarization.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6739598214626312}]}, {"text": "Similar to previous models, we use an encoderdecoder architecture with attention mechanism to select important sentences.", "labels": [], "entities": []}, {"text": "Our key contribution is to design an architecture that utilizes key words in the selection process.", "labels": [], "entities": []}, {"text": "Salient sentences of a document, that are useful in summaries, often contain key words and, to our knowledge, none of the previous models have explicitly modeled this interaction.", "labels": [], "entities": [{"text": "summaries", "start_pos": 52, "end_pos": 61, "type": "TASK", "confidence": 0.9810882806777954}]}, {"text": "We model this interaction through a two-level encoder and decoder, one for words and the other for sentences.", "labels": [], "entities": []}, {"text": "An attention-based mechanism, similar to that of Pointer Networks, is used to learn important words and sentences from labeled data.", "labels": [], "entities": []}, {"text": "A switch mechanism is used to select between words and sentences during decoding and the final summary is generated using a combination of selected sentences and words.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of our model on the CNN/Daily Mail corpus where it outperforms state-of-the-art extractive summarizers.", "labels": [], "entities": [{"text": "CNN/Daily Mail corpus", "start_pos": 48, "end_pos": 69, "type": "DATASET", "confidence": 0.9268560647964478}]}, {"text": "Our experiments also suggest that the semantic redundancy in SWAP-NET generated summaries is comparable to that of human-generated summaries.", "labels": [], "entities": [{"text": "SWAP-NET generated summaries", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.7706313927968343}]}], "datasetContent": [{"text": "In our experiments the maximum number of words per document is limited to 800, and the maximum number of sentences per document to 50 (padding is used to maintain the length of word sequences).", "labels": [], "entities": []}, {"text": "We also use the symbols <GO> and <EOS> to indicate start and end of prediction by decoders.", "labels": [], "entities": [{"text": "GO", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9678603410720825}, {"text": "EOS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9572986364364624}]}, {"text": "The total vocabulary size is 150,000 words.", "labels": [], "entities": []}, {"text": "We use word embeddings of dimension 100 pretrained using word2vec () on the training dataset.", "labels": [], "entities": []}, {"text": "We fix the LSTM hidden state size at 200.", "labels": [], "entities": []}, {"text": "We use a batch size of 16 and the ADAM optimizer () with parameters: learning rate = 0.001, \u03b2 1 = 0.9, \u03b2 2 = 0.999 to train SWAP-NET.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.919500321149826}]}, {"text": "We employ gradient clipping to regularize our model and an early stopping criterion based on the validation loss.", "labels": [], "entities": [{"text": "early stopping criterion", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.9081571698188782}]}, {"text": "During training we find that SWAP-NET learns to predict important sentences faster than to predict words.", "labels": [], "entities": []}, {"text": "To speedup learning of word probabilities, we add the term \u2212 log \u03b1 w ij to our loss function l j in the final iterations of training.", "labels": [], "entities": []}, {"text": "It is possible to get the same sentence or word in multiple (usually consecutive) decoding steps.", "labels": [], "entities": []}, {"text": "In that case, in Eq.", "labels": [], "entities": []}, {"text": "3 we consider the maximum value of alpha obtained across these steps and calculate maximum scores of distinct sentences and words.", "labels": [], "entities": []}, {"text": "We select 3 top scoring sentences for the summary, as there are 3.11 sentences on average in the gold summary of the training set (similar to settings used by others, e.g.,).", "labels": [], "entities": []}, {"text": "For our experiments, we use the CNN/DailyMail corpus (.", "labels": [], "entities": [{"text": "CNN/DailyMail corpus", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9320395439863205}]}, {"text": "We use the anonymized version of this dataset, from Cheng and Lapata (2016), which has labels for important sentences, that are used for training.", "labels": [], "entities": []}, {"text": "To obtain labels for words, we extract keywords from each gold summary using RAKE, an unsupervised keyword extraction method ().", "labels": [], "entities": [{"text": "RAKE", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.7155145406723022}]}, {"text": "These keywords are used to label words in the corresponding input document during training.", "labels": [], "entities": []}, {"text": "We replace numerical values in the documents by zeros to limit the vocabulary size.", "labels": [], "entities": []}, {"text": "We have 193,986 training documents, 12,147 validation documents and 10,346 test documents from the DailyMail corpus and 83,568 training documents, 1,220 validation documents and 1,093 test documents from CNN subset with labels for sentences and words.", "labels": [], "entities": [{"text": "DailyMail corpus", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.9759179055690765}, {"text": "CNN subset", "start_pos": 204, "end_pos": 214, "type": "DATASET", "confidence": 0.959764689207077}]}, {"text": "We use the ROUGE toolkit ( for evaluation of the generated summaries in comparison to the gold summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8792819976806641}]}, {"text": "We use three variants of this metric: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) that are computed by matching unigrams, bigrams and longest common subsequences respectively between the two summaries.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 52, "end_pos": 59, "type": "METRIC", "confidence": 0.7804136872291565}, {"text": "ROUGE-L (RL)", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.8566498905420303}]}, {"text": "To compare with shows the performance of SWAP-NET, state-of-the-art baselines NN and SummaRuNNer and other baselines, using ROUGE recall with summary length of 75 bytes, on the entire Daily Mail test set.", "labels": [], "entities": [{"text": "ROUGE recall", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.8385055959224701}, {"text": "Daily Mail test set", "start_pos": 184, "end_pos": 203, "type": "DATASET", "confidence": 0.9724642485380173}]}, {"text": "The performance of SWAP-NET is comparable to that of SummaRuNNer and better than NN and other baselines.", "labels": [], "entities": []}, {"text": "the previous best reported F-score by SummaRuNNer, as seen in table 3, with a consistent improvement of over 2 ROUGE points in all three metrics.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9975917339324951}, {"text": "SummaRuNNer", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.6850120425224304}, {"text": "ROUGE", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9984723925590515}]}], "tableCaptions": [{"text": " Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.", "labels": [], "entities": [{"text": "Daily-Mail test set", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9923280080159506}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.7180603742599487}]}, {"text": " Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.", "labels": [], "entities": [{"text": "Daily-Mail test set", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.993039627869924}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.7147374153137207}]}]}