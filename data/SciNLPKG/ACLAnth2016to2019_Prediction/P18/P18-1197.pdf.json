{"title": [{"text": "To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness", "labels": [], "entities": [{"text": "To Attend or not to Attend", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7817380626996359}, {"text": "Semantic Relatedness", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.7900387048721313}]}], "abstractContent": [{"text": "With the recent success of Recurrent Neu-ral Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8632740616798401}]}, {"text": "The purpose of this paper is twofold ; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM.", "labels": [], "entities": []}, {"text": "Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs.", "labels": [], "entities": []}, {"text": "Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 160, "end_pos": 180, "type": "TASK", "confidence": 0.786439836025238}]}], "introductionContent": [{"text": "Recurrent Neural Networks (RNNs), in particular Long Short-Term Memory Networks (LSTMs), have demonstrated remarkable accomplishments in Natural Language Processing (NLP) in recent years.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 137, "end_pos": 170, "type": "TASK", "confidence": 0.7048197289307913}]}, {"text": "Several tasks such as information extraction, question answering, and machine translation have benefited from them.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8580256402492523}, {"text": "question answering", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9340088963508606}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7860808968544006}]}, {"text": "However, in their vanilla forms, these networks are constrained by the sequential order of tokens in a sentence.", "labels": [], "entities": []}, {"text": "To mitigate this limitation, structural (dependency or constituency) information in a sentence was exploited and witnessed partial success in various tasks.", "labels": [], "entities": []}, {"text": "On the other hand, alignment techniques () and attention mechanisms () act as a catalyst to augment the performance of classical Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) models, respectively.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT) and Neural Machine Translation (NMT)", "start_pos": 129, "end_pos": 203, "type": "TASK", "confidence": 0.8107356016452496}]}, {"text": "In short, both approaches focus on sub-strings of source sentence which are significant for predicting target words while translating.", "labels": [], "entities": []}, {"text": "Currently, the combination of linear RNNs/LSTMs and attention mechanisms has become a de facto standard architecture for many NLP tasks.", "labels": [], "entities": []}, {"text": "At the intersection of sentence encoding and attention models, some interesting questions emerge: Can attention mechanisms be employed on tree structures, such as Tree-LSTMs (?", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7302837669849396}]}, {"text": "If yes, what are the possible tree-based attention models?", "labels": [], "entities": []}, {"text": "Do different tree structures (in particular constituency vs. dependency) have different behaviors in such models?", "labels": [], "entities": []}, {"text": "With these questions in mind, we present our investigation and findings in the context of semantic relatedness tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our models on two tasks: (1) semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and (2) paraphrase detection for question pairs).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 145, "end_pos": 165, "type": "TASK", "confidence": 0.7383669316768646}]}], "tableCaptions": [{"text": " Table 1: Results on test dataset for SICK and MSRpar semantic relatedness task. Mean scores are  presented based on 5 runs (standard deviation in parenthesis). Categories of results: (1) Previous models  (2) Dependency structure (3) Constituency structure (4) Linear structure", "labels": [], "entities": [{"text": "MSRpar semantic relatedness task", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6018883213400841}]}, {"text": " Table 3: Effect of the progressive attention model", "labels": [], "entities": []}]}