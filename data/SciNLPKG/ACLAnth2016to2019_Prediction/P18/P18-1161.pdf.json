{"title": [{"text": "Denoising Distantly Supervised Open-Domain Question Answering", "labels": [], "entities": [{"text": "Denoising Distantly Supervised Open-Domain Question Answering", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.6900658309459686}]}], "abstractContent": [{"text": "Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text.", "labels": [], "entities": [{"text": "Distantly supervised open-domain question answering (DS-QA)", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.670204795897007}]}, {"text": "Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph.", "labels": [], "entities": []}, {"text": "They ignore the rich information contained in other paragraphs.", "labels": [], "entities": []}, {"text": "Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA.", "labels": [], "entities": []}, {"text": "To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs.", "labels": [], "entities": []}, {"text": "Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.", "labels": [], "entities": []}, {"text": "The source code and data of this paper can be obtained from https: //github.com/thunlp/OpenQA", "labels": [], "entities": [{"text": "OpenQA", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9145156741142273}]}], "introductionContent": [{"text": "Reading comprehension, which aims to answer questions about a document, has recently become a major focus of NLP research.", "labels": [], "entities": []}, {"text": "Many reading comprehension systems) have been proposed and achieved promising results since their multilayer architectures and attention mechanisms allow them to reason for the question.", "labels": [], "entities": []}, {"text": "To some ex- * Corresponding author: Zhiyuan Liu tent, reading comprehension has shown the ability of recent neural models for reading, processing, and comprehending natural language text.", "labels": [], "entities": [{"text": "comprehending natural language text", "start_pos": 151, "end_pos": 186, "type": "TASK", "confidence": 0.6296011209487915}]}, {"text": "Despite their success, existing reading comprehension systems rely on pre-identified relevant texts, which do not always exist in real-world question answering (QA) scenarios.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.8596121430397033}]}, {"text": "Hence, reading comprehension technique cannot be directly applied to the task of open domain QA.", "labels": [], "entities": []}, {"text": "In recent years, researchers attempt to answer opendomain questions with a large-scale unlabeled corpus.", "labels": [], "entities": []}, {"text": "propose a distantly supervised open-domain question answering (DS-QA) system which uses information retrieval technique to obtain relevant text from Wikipedia, and then applies reading comprehension technique to extract the answer.", "labels": [], "entities": [{"text": "distantly supervised open-domain question answering (DS-QA)", "start_pos": 10, "end_pos": 69, "type": "TASK", "confidence": 0.7466474957764149}]}, {"text": "Although DS-QA proposes an effective strategy to collect relevant texts automatically, it always suffers from the noise issue.", "labels": [], "entities": []}, {"text": "For example, for the question \"Which country's capital is Dublin?\", we may encounter that: (1) The retrieved paragraph \"Dublin is the largest city of Ireland ...\" does not actually answer the question; (2) The second \"Dublin\" in the retrieved paragraph 'Dublin is the capital of Ireland.", "labels": [], "entities": []}, {"text": "Besides, Dublin is one of the famous tourist cities in Ireland and ...\" is not the correct token of the answer.", "labels": [], "entities": [{"text": "Dublin", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.9726440906524658}]}, {"text": "These noisy paragraphs and tokens are regarded as valid instances in DS-QA.", "labels": [], "entities": []}, {"text": "To address this issue,  separate the answer generation in DS-QA into two modules including selecting a target paragraph in document and extracting the correct answer from the target paragraph by reading comprehension.", "labels": [], "entities": [{"text": "answer generation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.764490157365799}]}, {"text": "Further, use reinforcement learning to train target paragraph selection and answer extraction jointly.", "labels": [], "entities": [{"text": "target paragraph selection", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.5924203097820282}, {"text": "answer extraction", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8777852356433868}]}, {"text": "These methods only extract the answer according to the most related paragraph, which will lose a large amount of rich information contained in Paragraph Selector", "labels": [], "entities": []}], "datasetContent": [{"text": "For Quasar-T, SearchQA and TriviaQA datasets, we use the retrieved paragraphs provided by (: Statistics of the dataset.", "labels": [], "entities": [{"text": "TriviaQA datasets", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.921961635351181}]}, {"text": "Following , we adopt two metrics including ExactMatch (EM) and F1 scores to evaluate our model.", "labels": [], "entities": [{"text": "ExactMatch (EM)", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.9625546038150787}, {"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9973287582397461}]}, {"text": "EM measures the percentage of predictions that match one of the ground truth answers exactly and F1 score is a metric that loosely measures the average overlap between the prediction and ground truth answer.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5427465438842773}, {"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9914550185203552}]}, {"text": "In this paper, we tune our model on the development set and use a grid search to determine the optimal parameters.", "labels": [], "entities": []}, {"text": "We select the hidden size of LSTM n \u2208 {32, 64, 128, \u00b7 \u00b7 \u00b7 , 512}, the number of LSTM layers for document and question encoder among {1, 2, 3, 4}, regularization weight \u03b1 among {0.1, 0.5, 1.0, 2.0} and the batch size among {4, 8, 16, 32, 64, 128}.", "labels": [], "entities": []}, {"text": "The optimal parameters are highlighted with bold faces.", "labels": [], "entities": []}, {"text": "For other parameters, since they have little effect on the results, we simply follow the settings used in . For training, our Our+FULL model is first initialized by pre-training using Our+AVG model, and we set the iteration number overall the training data as 10.", "labels": [], "entities": [{"text": "FULL", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9923729300498962}]}, {"text": "For pre-trained word embeddings, we use the 300-dimensional GloVe) word embeddings learned from 840B Web crawl data.", "labels": [], "entities": [{"text": "840B Web crawl data", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.7532933354377747}]}, {"text": "From, we can see that: (1) There is a clear gap between top-3/5 and top-1 DS-QA performance (10-20%).", "labels": [], "entities": []}, {"text": "It indicates that our DS-QA model is far from the upper performance and still has a high probability to be improved by answer re-ranking.", "labels": [], "entities": []}, {"text": "(2) The Our+FULL model outperforms R 3 model in top-1, top-3 and top-5 on both Quasar-T and SearchQA datasets by 5% to 7%.", "labels": [], "entities": [{"text": "FULL", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.983302891254425}, {"text": "SearchQA datasets", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.8528664410114288}]}, {"text": "It indicates that aggregating the information from all informative paragraphs can effectively enhance our model in DS-QA, which is more potential using answer re-ranking.", "labels": [], "entities": []}, {"text": "shows two examples of our models, which illustrates that our model can make full use of informative paragraphs.", "labels": [], "entities": []}, {"text": "From the table we find that:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the dataset.  Following", "labels": [], "entities": []}, {"text": " Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).", "labels": [], "entities": []}, {"text": " Table 3: Effect of Different Paragraph Selector on  the Quasar-T and SearchQA development set.", "labels": [], "entities": [{"text": "SearchQA development set", "start_pos": 70, "end_pos": 94, "type": "DATASET", "confidence": 0.9283969203631083}]}, {"text": " Table 4: Effect of Different Paragraph Reader on  the Quasar-T and SearchQA development set. The  paragraph selector used in Our+FULL is RNN.", "labels": [], "entities": [{"text": "SearchQA development set", "start_pos": 68, "end_pos": 92, "type": "DATASET", "confidence": 0.9117084542910258}, {"text": "FULL", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.716982901096344}, {"text": "RNN", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.8503913283348083}]}, {"text": " Table 5: Comparison of our paragraph selector and traditional information retrieval model in para- graph selection. The Our+AVG and Our+FULL model used in WebQuestions dataset is pre-trained  with Quasart-T dataset", "labels": [], "entities": [{"text": "para- graph selection", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.6105132102966309}, {"text": "FULL", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.982993483543396}, {"text": "WebQuestions dataset", "start_pos": 156, "end_pos": 176, "type": "DATASET", "confidence": 0.9296053647994995}, {"text": "Quasart-T dataset", "start_pos": 198, "end_pos": 215, "type": "DATASET", "confidence": 0.7679928243160248}]}, {"text": " Table 6: The examples of the answers to the given questions extracted by our model. The token in bold  are the extracted answers in each paragraph. The paragraphs are sorted according to the probabilities  output by our paragraph selector.", "labels": [], "entities": []}, {"text": " Table 7: Potential improvement on DS-QA per- formance by answer re-ranking. The performance  is based on the Quasar-T and SearchQA develop- ment dataset.", "labels": [], "entities": [{"text": "SearchQA develop- ment dataset", "start_pos": 123, "end_pos": 153, "type": "DATASET", "confidence": 0.7901986241340637}]}]}