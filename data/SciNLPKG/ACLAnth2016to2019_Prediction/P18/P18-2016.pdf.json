{"title": [{"text": "Automatic Extraction of Commonsense LocatedNear Knowledge", "labels": [], "entities": [{"text": "Automatic Extraction of Commonsense", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6995286494493484}, {"text": "LocatedNear Knowledge", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.736993670463562}]}], "abstractContent": [{"text": "LOCATEDNEAR relation is a kind of com-monsense knowledge describing two physical objects that are typically found near each other in real life.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.8422020673751831}]}, {"text": "In this paper, we study how to automatically extract such relationship through a sentence-level relation classifier and aggregating the scores of entity pairs from a large corpus.", "labels": [], "entities": []}, {"text": "Also, we release two benchmark datasets for evaluation and future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Artificial intelligence systems can benefit from incorporating commonsense knowledge as background, such as ice is cold (HASPROPERTY), chewing is a sub-event of eating (HASSUBEVENT), chair and table are typically found near each other (LOCATEDNEAR), etc.", "labels": [], "entities": []}, {"text": "These kinds of commonsense facts have been used in many downstream tasks, such as textual entailment and visual recognition tasks ().", "labels": [], "entities": [{"text": "textual entailment and visual recognition tasks", "start_pos": 82, "end_pos": 129, "type": "TASK", "confidence": 0.7280870974063873}]}, {"text": "The commonsense knowledge is often represented as relation triples in commonsense knowledge bases, such as ConceptNet (, one of the largest commonsense knowledge graphs available today.", "labels": [], "entities": []}, {"text": "However, most commonsense knowledge bases are manually curated or crowd-sourced by community efforts and thus do not scale well.", "labels": [], "entities": []}, {"text": "This paper aims to automatically extract the commonsense LOCATEDNEAR relation between physical objects from textual corpora.", "labels": [], "entities": []}, {"text": "LOCATEDNEAR is defined as the relationship between two objects typically found near each other in real life.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.882366955280304}]}, {"text": "We focus on LOCATEDNEAR relation for these reasons: 1.", "labels": [], "entities": [{"text": "LOCATEDNEAR relation", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7216494083404541}]}, {"text": "LOCATEDNEAR facts provide helpful prior knowledge to object detection tasks in com- * Both authors contributed equally.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9490855932235718}, {"text": "object detection tasks", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.8575315078099569}]}, {"text": "Figure 1: LOCATEDNEAR facts assist the detection of vague objects: if a set of knife, fork and plate is on the table, one may believe there is a glass beside based on the commonsense, even though these objects are hardly visible due to low light.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9877678751945496}]}, {"text": "2. This commonsense knowledge can benefit reasoning related to spatial facts and physical scenes in reading comprehension, question answering, etc.", "labels": [], "entities": [{"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.9066804051399231}]}, {"text": "(Li et al., 2016) 3.", "labels": [], "entities": []}, {"text": "Existing knowledge bases have very few facts for this relation (ConceptNet 5.5 has only 49 triples of LOCATEDNEAR relation).", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9066160917282104}]}, {"text": "We propose two novel tasks in extracting LO-CATEDNEAR relation from textual corpora.", "labels": [], "entities": [{"text": "extracting LO-CATEDNEAR relation from textual corpora", "start_pos": 30, "end_pos": 83, "type": "TASK", "confidence": 0.8111565758784612}]}, {"text": "One is a sentence-level relation classification problem which judges whether or not a sentence describes two objects (mentioned in the sentence) being physically close by.", "labels": [], "entities": [{"text": "sentence-level relation classification", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.6253447731335958}]}, {"text": "The other task is to produce a ranked list of LOCATEDNEAR facts with the given classified results of large number of sentences.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9381449818611145}]}, {"text": "We believe both two tasks can be used to automatically populate and complete existing commonsense knowledge bases.", "labels": [], "entities": []}, {"text": "Additionally, we create two benchmark datasets for evaluating LOCATEDNEAR relation extraction systems on the two tasks: one is 5,000 sentences each describing a scene of two physical objects and with a label indicating if the two objects are co-located in the scene; the other consists of 500 pairs of objects with human-annotated scores indicating confidences that a certain pair of objects are commonly located near in real life.", "labels": [], "entities": [{"text": "LOCATEDNEAR relation extraction", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7160762747128805}]}, {"text": "We propose several methods to solve the tasks including feature-based models and LSTM-based neural architectures.", "labels": [], "entities": []}, {"text": "The proposed neural architecture compares favorably with the current state-ofthe-art method for general-purpose relation classification problem.", "labels": [], "entities": [{"text": "general-purpose relation classification", "start_pos": 96, "end_pos": 135, "type": "TASK", "confidence": 0.6274306873480479}]}, {"text": "From our relatively smaller proposed datasets, we extract in total 2,067 new LOCATEDNEAR triples that are not in ConceptNet.", "labels": [], "entities": [{"text": "ConceptNet", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.9239269495010376}]}], "datasetContent": [{"text": "Our proposed vocabulary of single-word physical objects is constructed by the intersection of all ConceptNet concepts and all entities that belong to \"physical object\" class in Wikidata).", "labels": [], "entities": []}, {"text": "We manually filter out some words that have the meaning of an abstract concept, which results in 1,169 physical objects in total.", "labels": [], "entities": []}, {"text": "Afterwards, we utilize a cleaned subset of the Project Gutenberg corpus, which contains 3,036 English books written by 142 authors.", "labels": [], "entities": []}, {"text": "An assumption here is that sentences in fictions are more likely to describe real life scenes.", "labels": [], "entities": []}, {"text": "We sample and investigate the density of LOCAT-EDNEAR relations in Gutenberg with other widely used corpora, namely Wikipedia, used by and New York Times corpus ().", "labels": [], "entities": [{"text": "LOCAT-EDNEAR", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8902989625930786}, {"text": "New York Times corpus", "start_pos": 139, "end_pos": 160, "type": "DATASET", "confidence": 0.7049851566553116}]}, {"text": "In the English Wikipedia dump, out of all sentences which mentions at least two physical objects, 32.4% turnout to be positive.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.9524027705192566}]}, {"text": "In the New York Times corpus, the percentage of positive sentences is only 25.1%.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 7, "end_pos": 28, "type": "DATASET", "confidence": 0.8436969816684723}]}, {"text": "In contrast, that percentage in the Gutenberg corpus is 55.1%, much higher than the other two corpora, making it a good choice for LOCATEDNEAR relation extraction.", "labels": [], "entities": [{"text": "LOCATEDNEAR relation extraction", "start_pos": 131, "end_pos": 162, "type": "TASK", "confidence": 0.7438065608342489}]}, {"text": "From this corpus, we identify 15,193 pairs that co-occur in more than 10 sentences.", "labels": [], "entities": []}, {"text": "Among these pairs, we randomly select 500 object pairs and 10 sentences with respect to each pair for annotators to label their commonsense LOCATEDNEAR.", "labels": [], "entities": [{"text": "LOCATEDNEAR", "start_pos": 140, "end_pos": 151, "type": "METRIC", "confidence": 0.8466334939002991}]}, {"text": "Each instance is labeled by at least three annotators who are college students and proficient with English.", "labels": [], "entities": []}, {"text": "The final truth labels are decided by majority voting.", "labels": [], "entities": [{"text": "truth labels", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.690451979637146}]}, {"text": "The Cohen's Kappa among the three annotators is 0.711 which suggests substantial agreement (.", "labels": [], "entities": [{"text": "Cohen's Kappa", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.5401319960753123}]}, {"text": "This dataset has almost double the size of those most popular relations in the SemEval task, and the sentences in our data set tend to be longer.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.901474118232727}]}, {"text": "We randomly choose 4,000 instances as the training set and 1,000 as the test set for evaluating the sentence-level relation classification task.", "labels": [], "entities": [{"text": "sentence-level relation classification task", "start_pos": 100, "end_pos": 143, "type": "TASK", "confidence": 0.7194538936018944}]}, {"text": "For the second task, we further ask the annotators to label whether each pair of objects are likely to locate near each other in the real world.", "labels": [], "entities": []}, {"text": "Majority votes determine the final truth labels.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement here is 0.703 (substantial agreement).", "labels": [], "entities": []}, {"text": "In this section, we first present our evaluation of our proposed methods and the state-of-the-art general relation classification model on the first task.", "labels": [], "entities": [{"text": "general relation classification", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.5992868940035502}]}, {"text": "Then, we evaluate the quality of the new LOCAT-EDNEAR triples we extracted.", "labels": [], "entities": [{"text": "LOCAT-EDNEAR", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9277502298355103}]}], "tableCaptions": [{"text": " Table 1: Examples of four types of tokens during  sentence normalization. (#s stands for subjects and  #o for objects)", "labels": [], "entities": [{"text": "sentence normalization", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7158723622560501}]}, {"text": " Table 2: Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy,  P=Precision, R=Recall, \"-\" means without certain feature)", "labels": [], "entities": [{"text": "co-location classification task", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7597524225711823}, {"text": "Acc.", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9988623857498169}, {"text": "Accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.873839795589447}, {"text": "Precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9100530743598938}, {"text": "Recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.6701872944831848}]}, {"text": " Table 3: Ranking results of scoring functions.", "labels": [], "entities": []}]}