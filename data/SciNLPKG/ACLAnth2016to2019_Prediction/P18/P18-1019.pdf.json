{"title": [{"text": "A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature", "labels": [], "entities": [{"text": "Medical Literature", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6050184518098831}]}], "abstractContent": [{"text": "We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials.", "labels": [], "entities": []}, {"text": "Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the 'PICO' elements).", "labels": [], "entities": []}, {"text": "These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary.", "labels": [], "entities": []}, {"text": "We acquired annotations from a diverse set of workers with varying levels of expertise and cost.", "labels": [], "entities": []}, {"text": "We describe our data collection process and the corpus itself in detail.", "labels": [], "entities": []}, {"text": "We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2015 alone, about 100 manuscripts describing randomized controlled trials (RCTs) for medical interventions were published everyday.", "labels": [], "entities": []}, {"text": "It is thus practically impossible for physicians to know which is the best medical intervention fora given patient group and condition.", "labels": [], "entities": []}, {"text": "This inability to easily search and organize the published literature impedes the aims of evidence based medicine (EBM), which aspires to inform patient care using the totality of relevant evidence.", "labels": [], "entities": [{"text": "evidence based medicine (EBM)", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.695654292901357}]}, {"text": "* * now at Google Inc.", "labels": [], "entities": []}, {"text": "Computational methods could expedite biomedical evidence synthesis) and natural language processing (NLP) in particular can play a key role in the task.", "labels": [], "entities": [{"text": "biomedical evidence synthesis", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6773587465286255}, {"text": "natural language processing (NLP)", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.7923981746037801}]}, {"text": "Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis.", "labels": [], "entities": [{"text": "biomedical evidence extraction and synthesis", "start_pos": 59, "end_pos": 103, "type": "TASK", "confidence": 0.6563534379005432}]}, {"text": "But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models.", "labels": [], "entities": []}, {"text": "Here we address this gap by introducing EBM-NLP, anew corpus to power NLP models in support of EBM.", "labels": [], "entities": []}, {"text": "The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.", "labels": [], "entities": []}, {"text": "EBM-NLP comprises \u223c5,000 medical abstracts describing clinical trials, multiply annotated in detail with respect to characteristics of the underlying trial Populations (e.g., diabetics), Interventions (insulin), Comparators (placebo) and Outcomes (blood glucose levels).", "labels": [], "entities": [{"text": "EBM-NLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9288747906684875}, {"text": "Outcomes", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9916002750396729}]}, {"text": "Collectively, these key informational pieces are referred to as PICO elements; they form the basis for wellformed clinical questions ().", "labels": [], "entities": []}, {"text": "We adopt a hybrid crowdsourced labeling strategy using heterogeneous annotators with varying expertise and cost, from laypersons to MDs.", "labels": [], "entities": []}, {"text": "Annotators were first tasked with marking text spans that described the respective PICO elements.", "labels": [], "entities": []}, {"text": "Identified spans were subsequently anno-tated in greater detail: this entailed finer-grained labeling of PICO elements and mapping these onto a normalized vocabulary, and indicating redundancy in the mentions of PICO elements.", "labels": [], "entities": []}, {"text": "In addition, we outline several NLP tasks that would directly support the practice of EBM and that maybe explored using the introduced resource.", "labels": [], "entities": []}, {"text": "We present baseline models and associated results for these tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Partial example annotation for Participants, Interventions, and Outcomes. The full annotation  includes multiple top-level spans for each PIO element as well as labels for repetition.", "labels": [], "entities": []}, {"text": " Table 2: Cohen's \u03ba between medical students for  the 200 reference documents.", "labels": [], "entities": []}, {"text": " Table 3: Precision, recall and F-1 for aggregated  AMT spans evaluated against the union of expert  span labels, for all three P, I, and O elements.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984782338142395}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9995476603507996}, {"text": "F-1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9985684156417847}]}, {"text": " Table 4: Token-wise statistics for individual AMT  annotations evaluated against the aggregated ver- sions.", "labels": [], "entities": [{"text": "AMT  annotations", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.8960736095905304}]}, {"text": " Table 5: Average per-document frequency of dif- ferent token labels.", "labels": [], "entities": []}, {"text": " Table 6: Average pair-wise Cohen's \u03ba between  three medical experts for the 200 reference doc- uments.", "labels": [], "entities": [{"text": "pair-wise Cohen's \u03ba", "start_pos": 18, "end_pos": 37, "type": "METRIC", "confidence": 0.7354271709918976}]}, {"text": " Table 7: Precision, recall, and F-1 for AMT la- bels against expert labels using different aggrega- tion strategies.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.998555600643158}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993011951446533}, {"text": "F-1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9985947012901306}, {"text": "AMT la- bels", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.6884218454360962}]}, {"text": " Table 8: Statistics for individual AMT annotations  evaluated against the aggregated versions, macro- averaged over different labels.", "labels": [], "entities": [{"text": "AMT annotations", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.8746294379234314}]}, {"text": " Table 9: Average per-document frequency of dif- ferent label types.", "labels": [], "entities": []}, {"text": " Table 10: Comparison against the majority vote  for span-level repetition labels.", "labels": [], "entities": [{"text": "span-level repetition labels", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.5965706010659536}]}, {"text": " Table 11: The number of common MeSH terms  (out of 135) that were assigned to a span of text in  at least 10%, 25%, and 50% of the possible docu- ments.", "labels": [], "entities": []}, {"text": " Table 12: Baseline models (on the test set) for the  PIO span tagging task.", "labels": [], "entities": [{"text": "PIO span tagging task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7987426370382309}]}, {"text": " Table 13: Baseline models for the token-level, de- tailed labeling task.", "labels": [], "entities": []}, {"text": " Table 14: Baseline model for predicting whether  pairs of spans contain redundant information.", "labels": [], "entities": []}]}