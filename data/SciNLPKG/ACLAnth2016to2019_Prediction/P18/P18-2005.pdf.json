{"title": [{"text": "Towards Robust and Privacy-preserving Text Representations", "labels": [], "entities": [{"text": "Privacy-preserving Text Representations", "start_pos": 19, "end_pos": 58, "type": "TASK", "confidence": 0.5660498837629954}]}], "abstractContent": [{"text": "Written text often provides sufficient clues to identify the author, their gender, age, and other important attributes.", "labels": [], "entities": []}, {"text": "Consequently , the authorship of training and evaluation corpora can have unforeseen impacts, including differing model performance for different user groups, as well as privacy implications.", "labels": [], "entities": []}, {"text": "In this paper, we propose an approach to explicitly obscure important author characteristics at training time, such that representations learned are invariant to these attributes.", "labels": [], "entities": []}, {"text": "Evaluating on two tasks, we show that this leads to increased privacy in the learned representations , as well as more robust models to varying evaluation conditions, including out-of-domain corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language is highly diverse, and differs according to author, their background, and personal attributes such as gender, age, education and nationality.", "labels": [], "entities": []}, {"text": "This variation can have a substantial effect on NLP models learned from text ( , leading to significant variation in inferences across different types of corpora, such as the author's native language, gender and age.", "labels": [], "entities": []}, {"text": "Training corpora are never truly representative, and therefore models fit to these datasets are biased in the sense that they are much more effective for texts from certain groups of user, e.g., middle-aged white men, and considerably poorer for other parts of the population.", "labels": [], "entities": []}, {"text": "Moreover, models fit to language corpora often fixate on author attributes which correlate with the target variable, e.g., gender correlating with class skews (, or translation choices (.", "labels": [], "entities": []}, {"text": "This signal, however, is rarely fundamental to the task of modelling language, and is better considered as a confounding influence.", "labels": [], "entities": []}, {"text": "These auxiliary learning signals can mean the models do not adequately capture the core linguistic problem.", "labels": [], "entities": []}, {"text": "In such situations, removing these confounds should give better generalisation, especially for out-of-domain evaluation, a similar motivation to research in domain adaptation based on selection biases over text domains.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 157, "end_pos": 174, "type": "TASK", "confidence": 0.710591122508049}]}, {"text": "Another related problem is privacy: texts convey information about their author, often inadvertently, and many individuals may wish to keep this information private.", "labels": [], "entities": []}, {"text": "Consider the case of the AOL search data leak, in which AOL released detailed search logs of many of their users in August 2006).", "labels": [], "entities": [{"text": "AOL search data leak", "start_pos": 25, "end_pos": 45, "type": "DATASET", "confidence": 0.91068135201931}]}, {"text": "Although they deidentified users in the data, the log itself contained sufficient personally identifiable information that allowed many of these individuals to be identifed (.", "labels": [], "entities": []}, {"text": "Other sources of user text, such as emails, SMS messages and social media posts, would likely pose similar privacy issues.", "labels": [], "entities": []}, {"text": "This raises the question of how the corpora, or models built thereupon, can be distributed without exposing this sensitive data.", "labels": [], "entities": []}, {"text": "This is the problem of differential privacy, which is more typically applied to structured data, and often involves data masking, addition or noise, or other forms of corruption, such that formal bounds can be stated in terms of the likelihood of reconstructing the protected components of the dataset.", "labels": [], "entities": [{"text": "differential privacy", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7996059656143188}, {"text": "data masking", "start_pos": 116, "end_pos": 128, "type": "TASK", "confidence": 0.7640740871429443}]}, {"text": "This often comes at the cost of an accuracy reduction for models trained on the corrupted data (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992988109588623}]}, {"text": "Another related setting is where latent representations of the data are shared, rather than the text itself, which might occur when sending data from a phone to the cloud for processing, or trusting a third party with sensitive emails for NLP processing, such as grammar correction or translation.", "labels": [], "entities": [{"text": "NLP processing", "start_pos": 239, "end_pos": 253, "type": "TASK", "confidence": 0.8543237149715424}, {"text": "grammar correction or translation", "start_pos": 263, "end_pos": 296, "type": "TASK", "confidence": 0.6380431652069092}]}, {"text": "The transfered representations may still contain sensitive information, however, especially if an adversary has preliminary knowledge of the training model, in which case they can readily reverse engineer the input, for example, by a GAN attack algorithm (.", "labels": [], "entities": []}, {"text": "This is true even when differential privacy mechanisms have been applied.", "labels": [], "entities": []}, {"text": "Inspired by the above works, and recent successes of adversarial learning (, we propose a novel approach for privacy-preserving learning of unbiased representations.", "labels": [], "entities": []}, {"text": "1 Specially, we employ Ganin et al.'s approach to training deep models with adversarial learning, to explicitly obscure individuals' private information.", "labels": [], "entities": []}, {"text": "Thereby the learned (hidden) representations of the data can be transferred without compromising the authors' privacy, while still supporting high-quality NLP inference.", "labels": [], "entities": []}, {"text": "We evaluate on the tasks of POS-tagging and sentiment analysis, protecting several demographic attributes -gender, age, and location -and show empirically that doing so does not hurt accuracy, but instead can lead to substantial gains, most notably in out-of-domain evaluation.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9045913219451904}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9973560571670532}]}, {"text": "Compared to differential privacy, we report gains rather than loss in performance, but note that we provide only empirical improvements in privacy, without any formal guarantees.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report experimental results for our methods with two very different language tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: POS prediction accuracy [%] using the  Trustpilot test set, stratified by SEX and AGE  (higher is better), and the absolute difference (\u2206)  within each bias group (smaller is better). The best  result is indicated in bold.", "labels": [], "entities": [{"text": "POS prediction", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6704303324222565}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8963475227355957}, {"text": "Trustpilot test set", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.7981682221094767}, {"text": "SEX", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9945683479309082}, {"text": "AGE", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9945060610771179}]}, {"text": " Table 2: POS predictive accuracy [%] over the  AAVE dataset, stratified over the three domains,  alongside the macro-average accuracy. The best  result is indicated in bold.", "labels": [], "entities": [{"text": "POS predictive", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.6487032622098923}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.543253481388092}, {"text": "AAVE dataset", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.9664722084999084}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.8538675904273987}]}, {"text": " Table 3: Sentiment F 1 -score [%] over the RAT- ING task, and accuracy [%] of all the discriminator  across three private attributes. The best score is in- dicated in bold. The majority class with respect to  each private attribute is also reported.", "labels": [], "entities": [{"text": "Sentiment F 1 -score", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8381144285202027}, {"text": "RAT- ING task", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6765253394842148}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9994805455207825}]}]}