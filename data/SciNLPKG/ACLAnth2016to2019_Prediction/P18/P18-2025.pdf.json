{"title": [{"text": "Automatic Article Commenting: the Task and Dataset", "labels": [], "entities": []}], "abstractContent": [{"text": "Comments of online articles provide extended views and improve user engagement.", "labels": [], "entities": []}, {"text": "Automatically making comments thus become a valuable functionality for online forums, intelligent chatbots, etc.", "labels": [], "entities": []}, {"text": "This paper proposes the new task of automatic article commenting, and introduces a large-scale Chinese dataset 1 with millions of real comments and a human-annotated subset characterizing the com-ments' varying quality.", "labels": [], "entities": [{"text": "article commenting", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7338830828666687}, {"text": "Chinese dataset", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.8058528304100037}]}, {"text": "Incorporating the human bias of comment quality, we further develop automatic metrics that generalize abroad set of popular reference-based metrics and exhibit greatly improved correlations with human evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Comments of online articles and posts provide extended information and rich personal views, which could attract reader attentions and improve interactions between readers and authors (.", "labels": [], "entities": []}, {"text": "In contrast, posts failing to receive comments can easily go unattended and buried.", "labels": [], "entities": []}, {"text": "With the prevalence of online posting, automatic article commenting thus becomes a highly desirable tool for online discussion forums and social media platforms to increase user engagement and foster online communities.", "labels": [], "entities": [{"text": "article commenting", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7129783481359482}]}, {"text": "Besides, commenting on articles is one of the increasingly demanded skills of intelligent chatbot) to enable in-depth, content-rich conversations with humans.", "labels": [], "entities": []}, {"text": "Article commenting poses new challenges for machines, as it involves multiple cognitive abil-\u21e4 Work done while Lianhui interned at Tencent AI Lab The dataset is available on http://ai.tencent.", "labels": [], "entities": []}, {"text": "com/upload/PapersUploads/article_ commenting.tgz ities: understanding the given article, formulating opinions and arguments, and organizing natural language for expression.", "labels": [], "entities": []}, {"text": "Compared to summarization), a comment does not necessarily coverall salient ideas of the article; instead it is often desirable fora comment to carry additional information not explicitly presented in the articles.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.978322446346283}]}, {"text": "Article commenting also differs from making product reviews (, as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore.", "labels": [], "entities": [{"text": "Article commenting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8476519286632538}]}, {"text": "In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation.", "labels": [], "entities": [{"text": "automatic article commenting", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.6396996080875397}]}, {"text": "We further develop a general approach of enhancing popular automatic metrics, such as BLEU () and METEOR (, to better fit the characteristics of the new task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9989789724349976}, {"text": "METEOR", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9908031225204468}]}, {"text": "In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments.", "labels": [], "entities": []}, {"text": "For example, identify constructive news comments; study human summaries of online comment conversations.", "labels": [], "entities": [{"text": "summaries of online comment conversations", "start_pos": 62, "end_pos": 103, "type": "TASK", "confidence": 0.8188687801361084}]}, {"text": "The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task.", "labels": [], "entities": [{"text": "article commenting", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7607745826244354}]}, {"text": "In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich metadata for article categories and user votes of comments.", "labels": [], "entities": []}, {"text": "Different from traditional text generation tasks such as machine translation () that has a relatively small set of gold targets, human comments on an article live in much larger space by involving diverse topics and personal views, and critically, are of vary-Title: \u02d8\u00fal\u00afiPhone 8 -\u21e4\u21e2\u00f6(9\ud97b\udf59>L (Apple's iPhone 8 event is happening in Sept.) Content: \u02d8\u00fal\u00afc\u270f\u2318\u00edS-\u21e4\u00c4\u02dc\u02dd\ud97b\udf59\u00a3\u21e4\u2303\u00e99\ud97b\udf5912\u00c2\u00cf\ud97b\udf59\u02d8\u00fa\u221e\u00a1 \u21e4\u00c4\u02dc\u02dd\ud97b\udf59\u00a3\u21e4\u2303\u00e99\ud97b\udf5912\u00c2\u00cf\ud97b\udf59\u02d8\u00fa\u221e\u00a1-\u21e4 \u21e2\ud97b\udf59\u00c2l\u00af\u2303-\u21e4\u21b5\u21b5\"iPhone\ud97b\udf59\u00e8K\u00d9\u221e \u00d1\u00ff \u02d8\u00faKh\ud97b\udf59\u02d8\u00faTV\ud97b\udf59\u00e5iOSo\u02c6\u21e5\u0178 !-\u21e4\u21e2\u2303&e >\u221eiPhones\u21e2&OLED> :O\u00e53D\u222b8koe\u00c4/\u00d1\u21b5\u21b5\"iPhone8\ud97b\udf59 /iPhone 7\ud97b\udf59iPhone 7Plus\u00d1\u00d9\u221eH\u21e5 (Apple has sent out invites for its next big event on September 12th, where the company is expected to reveal the next iPhone, along with updates to the Apple Watch, Apple TV, and iOS software.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.6730414927005768}, {"text": "O\u00e53D", "start_pos": 471, "end_pos": 475, "type": "METRIC", "confidence": 0.7251758575439453}]}, {"text": "Apple is expected to announce three new iPhones at the event: a next-generation iPhone 8 model with an OLED display and a 3D face-scanning camera; and updated versions of the iPhone 7 and 7 Plus.)", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the use of the dataset and metrics with simple retrieval and generation models, and show the enhanced metrics consistently improve correlations with human judgment.", "labels": [], "entities": []}, {"text": "Note that this paper does not aim to develop solutions for the article commenting task.", "labels": [], "entities": [{"text": "article commenting task", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8280269702275594}]}, {"text": "We leave the advanced modeling for future work.: Human correlation of metrics.", "labels": [], "entities": []}, {"text": "\"Human\" is the results from randomly dividing human scores into two groups.", "labels": [], "entities": []}, {"text": "Setup We briefly present key setup, and defer more details to the supplements.", "labels": [], "entities": []}, {"text": "Given an article to comment, the retrieval-based models first find a set of similar articles in the training set by TF-IDF, and return the comments most relevant to the target article with a CNN-based relevance predictor.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.8789181113243103}]}, {"text": "We use either the article title or full title/content for the article retrieval, and denote the two models with IR-T and IR-TC, respectively.", "labels": [], "entities": [{"text": "IR-T", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9720032811164856}, {"text": "IR-TC", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.7625554800033569}]}, {"text": "The generation models are based on simple sequence-tosequence network).", "labels": [], "entities": []}, {"text": "The models read articles using an encoder and generate comments using a decoder with or without attentions (), which are denoted as Seq2seq and Att if only article titles are read.", "labels": [], "entities": [{"text": "Att", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.978354275226593}]}, {"text": "We also setup an attentional sequence-tosequence model that reads full article title/content, and denote with Att-TC.", "labels": [], "entities": [{"text": "Att-TC", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9767182469367981}]}, {"text": "Again, these approaches are mainly for demonstration purpose and for evaluating the metrics, and are far from solving the difficult commenting task.", "labels": [], "entities": []}, {"text": "We discard comments with over 50 words and use a truncated vocabulary of size 30K.", "labels": [], "entities": []}, {"text": "Results We follow previous setting to evaluate the metrics, by conducting human evaluations and calculating the correlation between the scores assigned by humans and the metrics.", "labels": [], "entities": []}, {"text": "Specifically, for each article in the test set, we obtained six comments, five of which come from IR-T, IR-TC, Seq2seq, Att, and Att-TC, respectively, and one randomly drawn from real comments that are different from the reference comments.", "labels": [], "entities": []}, {"text": "The comments were then graded by human annotators following the same procedure of test set scoring (sec.3).", "labels": [], "entities": []}, {"text": "Meanwhile, we measure each comment with the vanilla and weighted automatic metrics based on the reference comments.", "labels": [], "entities": []}, {"text": "shows the Spearman and Pearson coefficients between the comment scores assigned by humans and the metrics.", "labels": [], "entities": []}, {"text": "The METEOR fam-ily correlates best with human judgments, and the enhanced weighted metrics improve over their vanilla versions inmost cases (including BLEU-2/3 as in the supplements).", "labels": [], "entities": [{"text": "METEOR fam-ily", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9013143181800842}, {"text": "BLEU-2/3", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9480251868565878}]}, {"text": "E.g., the Pearson of METEOR is substantially improved from 0.51 to 0.57, and the Spearman of ROUGE L from 0.19 to 0.26.", "labels": [], "entities": [{"text": "Pearson of", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9431804418563843}, {"text": "METEOR", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.49714016914367676}, {"text": "Spearman of ROUGE L", "start_pos": 81, "end_pos": 100, "type": "METRIC", "confidence": 0.7487313598394394}]}, {"text": "visualizes the human correlation of BLEU-1, METEOR, and W-METEOR, showing that the BLEU-1 scores vary a lot given any fixed human score, appearing to be random noise, while the METEOR family exhibit strong consistency with human scores.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9932403564453125}, {"text": "METEOR", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8962135314941406}, {"text": "BLEU-1", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9959486722946167}, {"text": "consistency", "start_pos": 206, "end_pos": 217, "type": "METRIC", "confidence": 0.95680171251297}]}, {"text": "Compared to W-METEOR, METEOR deviates from the regression line more frequently, esp. by assigning unexpectedly high scores to comments with low human grades.", "labels": [], "entities": []}, {"text": "Notably, the best automatic metric, W-METEOR, achieves 0.59 Spearman and 0.57 Pearson, which is higher or comparable to automatic metrics in other generation tasks, indicating a good supplement to human judgment for efficient evaluation and comparison.", "labels": [], "entities": [{"text": "Spearman", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9903746247291565}, {"text": "Pearson", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.996099591255188}]}, {"text": "We use the metrics to evaluate the above models in the supplements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A data example of an article (including title and content) paired with selected comments. We  also list a brief version of human judgment criteria (more details are in the supplement).", "labels": [], "entities": []}, {"text": " Table 3: Human correlation of metrics. \"Human\"  is the results from randomly dividing human  scores into two groups. All p-value < 0.01.", "labels": [], "entities": []}]}