{"title": [{"text": "A Purely End-to-end System for Multi-speaker Speech Recognition", "labels": [], "entities": [{"text": "Multi-speaker Speech Recognition", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6209274729092916}]}], "abstractContent": [{"text": "Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture.", "labels": [], "entities": [{"text": "multi-speaker speech recognition", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6478291749954224}]}, {"text": "Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner.", "labels": [], "entities": []}, {"text": "We further propose anew objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses.", "labels": [], "entities": []}, {"text": "Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1% relative improvement compared to a model trained without the proposed objective.", "labels": [], "entities": []}, {"text": "Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conventional automatic speech recognition (ASR) systems recognize a single utterance given a speech signal, in a one-to-one transformation.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.8470913072427114}]}, {"text": "However, restricting the use of ASR systems to situations with only a single speaker limits their applicability.", "labels": [], "entities": [{"text": "ASR", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9824147820472717}]}, {"text": "Recently, there has been growing inter- * This work was done while H.", "labels": [], "entities": []}, {"text": "Seki, Ph.D. candidate at Toyohashi University of Technology, Japan, was an intern at MERL.", "labels": [], "entities": [{"text": "MERL", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9154682755470276}]}, {"text": "est in single-channel multi-speaker speech recognition, which aims at generating multiple transcriptions from a single-channel mixture of multiple speakers' speech (.", "labels": [], "entities": [{"text": "multi-speaker speech recognition", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6125493844350179}]}, {"text": "To achieve this goal, several previous works have considered a two-step procedure in which the mixed speech is first separated, and recognition is then performed on each separated speech signal (.", "labels": [], "entities": []}, {"text": "Dramatic advances have recently been made in speech separation, via the deep clustering framework (, hereafter referred to as DPCL.", "labels": [], "entities": [{"text": "speech separation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7472049295902252}]}, {"text": "DPCL trains a deep neural network to map each time-frequency (T-F) unit to a high-dimensional embedding vector such that the embeddings for the T-F unit pairs dominated by the same speaker are close to each other, while those for pairs dominated by different speakers are farther away.", "labels": [], "entities": [{"text": "DPCL", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8641234636306763}]}, {"text": "The speaker assignment of each T-F unit can thus be inferred from the embeddings by simple clustering algorithms, to produce masks that isolate each speaker.", "labels": [], "entities": []}, {"text": "The original method using k-means clustering ( ) was extended to allow end-to-end training by unfolding the clustering steps using a permutation-free mask inference objective).", "labels": [], "entities": []}, {"text": "An alternative approach is to perform direct mask inference using the permutation-free objective function with networks that directly estimate the labels fora fixed number of sources.", "labels": [], "entities": [{"text": "direct mask inference", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.6397376557191213}]}, {"text": "Direct mask inference was first used in  as a baseline method, but without showing good performance.", "labels": [], "entities": [{"text": "Direct mask inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7101770838101705}]}, {"text": "This approach was revisited in  and under the name permutationinvariant training (PIT).", "labels": [], "entities": [{"text": "permutationinvariant training (PIT)", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.6152097880840302}]}, {"text": "Combination of such single-channel speaker-independent multi-speaker speech separation systems with ASR was first considered in using a conventional Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) system.", "labels": [], "entities": [{"text": "speaker-independent multi-speaker speech separation", "start_pos": 35, "end_pos": 86, "type": "TASK", "confidence": 0.6780862808227539}]}, {"text": "Combination with an endto-end ASR system was recently proposed in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8831576108932495}]}, {"text": "Both these approaches either trained or pre-trained the source separation and ASR networks separately, making use of mixtures and their corresponding isolated clean source references.", "labels": [], "entities": [{"text": "ASR", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9352511763572693}]}, {"text": "While the latter approach could in principle be trained without references for the isolated speech signals, the authors found it difficult to train from scratch in that case.", "labels": [], "entities": []}, {"text": "This ability can nonetheless be used when adapting a pre-trained network to new data without such references.", "labels": [], "entities": []}, {"text": "In contrast with this two-stage approach, considered direct optimization of a deep-learning-based ASR recognizer without an explicit separation module.", "labels": [], "entities": [{"text": "ASR recognizer", "start_pos": 98, "end_pos": 112, "type": "TASK", "confidence": 0.9354531466960907}]}, {"text": "The network is optimized based on a permutation-free objective defined using the cross-entropy between the system's hypotheses and reference labels.", "labels": [], "entities": []}, {"text": "The best permutation between hypotheses and reference labels in terms of cross-entropy is selected and used for backpropagation.", "labels": [], "entities": []}, {"text": "However, this method still requires reference labels in the form of senone alignments, which have to be obtained on the clean isolated sources using a single-speaker ASR system.", "labels": [], "entities": []}, {"text": "As a result, this approach still requires the original separated sources.", "labels": [], "entities": []}, {"text": "As a general caveat, generation of multiple hypotheses in such a system requires the number of speakers handled by the neural network architecture to be determined before training.", "labels": [], "entities": []}, {"text": "However, reported that the recognition of two-speaker mixtures using a model trained for three-speaker mixtures showed almost identical performance with that of a model trained on two-speaker mixtures.", "labels": [], "entities": []}, {"text": "Therefore, it maybe possible in practice to determine an upper bound on the number of speakers.", "labels": [], "entities": []}, {"text": "proposed a progressive training procedure fora hybrid system with explicit separation motivated by curriculum learning.", "labels": [], "entities": []}, {"text": "They also proposed self-transfer learning and multi-output sequence discriminative training methods for fully exploiting pairwise speech and preventing competing hypotheses, respectively.", "labels": [], "entities": []}, {"text": "In this paper, we propose to circumvent the need for the corresponding isolated speech sources when training on a set of mixtures, by using an end-to-end multi-speaker speech recognition without an explicit speech separation stage.", "labels": [], "entities": [{"text": "multi-speaker speech recognition", "start_pos": 154, "end_pos": 186, "type": "TASK", "confidence": 0.6986467242240906}]}, {"text": "In separation based systems, the spectrogram is segmented into complementary regions according to sources, which generally ensures that different utterances are recognized for each speaker.", "labels": [], "entities": []}, {"text": "Without this complementarity constraint, our direct multispeaker recognition system could be susceptible to redundant recognition of the same utterance.", "labels": [], "entities": []}, {"text": "In order to prevent degenerate solutions in which the generated hypotheses are similar to each other, we introduce anew objective function that enhances contrast between the network's representations of each source.", "labels": [], "entities": []}, {"text": "We also propose a training procedure to provide permutation invariance with low computational cost, by taking advantage of the joint CTC/attention-based encoder-decoder network architecture proposed in (.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed model is able to directly convert an input speech mixture into multiple label sequences without requiring any explicit intermediate representations.", "labels": [], "entities": []}, {"text": "In particular no frame-level training labels, such as phonetic alignments or corresponding unmixed speech, are required.", "labels": [], "entities": []}, {"text": "We evaluate our model on spontaneous English and Japanese tasks and obtain comparable results to the DPCL based method with explicit separation ().", "labels": [], "entities": []}, {"text": ", and the past label history.", "labels": [], "entities": []}, {"text": "The probability of the n-th label y n is computed by conditioning on the past history y 1:n\u22121 : The model is composed of two main sub-modules, an encoder network and a decoder network.", "labels": [], "entities": []}, {"text": "The encoder network transforms the input feature vector sequence into a high-level representation H = (h l \u2208 RC |l = 1, . .", "labels": [], "entities": []}, {"text": "The decoder network emits labels based on the label history y and a context vector c calculated using an attention mechanism which weights and sums the Cdimensional sequence of representation H with attention weight a.", "labels": [], "entities": []}, {"text": "A hidden state e of the decoder is updated based on the previous state, the previous context vector, and the emitted label.", "labels": [], "entities": []}, {"text": "This mechanism is summarized as follows: y n \u223c Decoder(c n , y n\u22121 ), (3) c n , an = Attention(a n\u22121 , en , H), (4) en = Update(e n\u22121 , c n\u22121 , y n\u22121 ).", "labels": [], "entities": []}, {"text": "At inference time, the previously emitted labels are used.", "labels": [], "entities": []}, {"text": "At training time, they are replaced by the reference label sequence R = (r 1 , . .", "labels": [], "entities": []}, {"text": ", r N ) in a teacher-forcing fashion, leading to conditional probability p att (Y R |O), where Y R denotes the output label sequence variable in this condition.", "labels": [], "entities": [{"text": "conditional probability p att (Y R |O)", "start_pos": 49, "end_pos": 87, "type": "METRIC", "confidence": 0.8077967703342438}]}, {"text": "The detailed definitions of Attention and Update are described in Section A of the supplementary material.", "labels": [], "entities": []}, {"text": "The encoder and decoder networks are trained to maximize the conditional probability of the reference label sequence R using backpropagation: where Loss att is the cross-entropy loss function.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used English and Japanese speech corpora, WSJ (Wall street journal) (Consortium, 1994; and CSJ (Corpus of spontaneous Japanese).", "labels": [], "entities": [{"text": "WSJ (Wall street journal)", "start_pos": 45, "end_pos": 70, "type": "DATASET", "confidence": 0.8978899518648783}]}, {"text": "To show the effectiveness of the proposed models, we generated mixed speech signals from these corpora to simulate single-channel overlapped multi-speaker recording, and evaluated the recognition performance using the mixed speech data.", "labels": [], "entities": []}, {"text": "For WSJ, we used WSJ1 SI284 for training, Dev93 for development, and Eval92 for evaluation.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5031284689903259}, {"text": "WSJ1 SI284", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8174770474433899}, {"text": "Eval92", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.8245440721511841}]}, {"text": "For CSJ, we followed the Kaldi recipe ( and used the full set of academic and simulated presentations for training, and the standard test sets 1, 2, and 3 for evaluation.", "labels": [], "entities": []}, {"text": "We created new corpora by mixing two utterances with different speakers sampled from existing corpora.", "labels": [], "entities": []}, {"text": "The detailed algorithm is presented in Section B of the supplementary material.", "labels": [], "entities": []}, {"text": "The sampled pairs of two utterances are mixed at various signal-to-noise ratios (SNR) between 0 dB and 5 dB with a random starting point for the overlap.", "labels": [], "entities": []}, {"text": "Duration of original unmixed and generated mixed corpora are summarized in.", "labels": [], "entities": []}, {"text": "First, we examined the performance of the baseline joint CTC/attention-based encoder-decoder network with the original unmixed speech data.", "labels": [], "entities": []}, {"text": "shows the character error rates, where the baseline model showed 2.6% on WSJ and 7.8% on CSJ.", "labels": [], "entities": [{"text": "error", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.511056125164032}, {"text": "WSJ", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8723498582839966}]}, {"text": "Since the model was trained and evaluated with unmixed speech data, these CERs are considered lower bounds for the CERs in the succeeding experiments with mixed speech data.", "labels": [], "entities": [{"text": "CERs", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9572036266326904}]}, {"text": "shows the CERs of the generated mixed speech from the WSJ corpus.", "labels": [], "entities": [{"text": "CERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9946233034133911}, {"text": "WSJ corpus", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9683669209480286}]}, {"text": "The first column indicates the position of split as mentioned in Section 3.5.", "labels": [], "entities": [{"text": "split", "start_pos": 43, "end_pos": 48, "type": "TASK", "confidence": 0.9799020290374756}]}, {"text": "The second, third and forth columns indicate CERs of the high energy speaker (HIGH E. SPK.), the low energy speaker (LOW E. SPK.), and the average (AVG.), respectively.", "labels": [], "entities": [{"text": "CERs", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9990367889404297}, {"text": "HIGH E. SPK.)", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9004718959331512}, {"text": "LOW E. SPK.)", "start_pos": 117, "end_pos": 129, "type": "METRIC", "confidence": 0.8606106042861938}, {"text": "AVG.", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.8698671460151672}]}, {"text": "The baseline model has very high CERs because  it was trained as a single-speaker speech recognizer without permutation-free training, and it can only output one hypothesis for each mixed speech.", "labels": [], "entities": [{"text": "CERs", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9975858926773071}]}, {"text": "In this case, the CERs were calculated by duplicating the generated hypothesis and comparing the duplicated hypotheses with the corresponding references.", "labels": [], "entities": [{"text": "CERs", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8083863258361816}]}, {"text": "The proposed models, i.e., splitby-VGG and split-by-BLSTM networks, obtained significantly lower CERs than the baseline CERs, the split-by-BLSTM model in particular achieving 14.0% CER.", "labels": [], "entities": [{"text": "CERs", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9911661744117737}, {"text": "CER", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.993536114692688}]}, {"text": "This is an 83.1% relative reduction from the baseline model.", "labels": [], "entities": []}, {"text": "The CER was further reduced to 13.7% by retraining the split-by-BLSTM model with the negative KL loss, a 2.1% relative reduction from the network without retraining.", "labels": [], "entities": [{"text": "CER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9360651969909668}]}, {"text": "This result implies that the proposed negative KL loss provides better separation by actively improving the contrast between the hidden vectors of each speaker.", "labels": [], "entities": []}, {"text": "Examples of recognition results are shown in Section C of the supplementary material.", "labels": [], "entities": [{"text": "recognition", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9433991312980652}]}, {"text": "Finally, we profiled the computation time for the permutations based on the decoder network and on CTC.", "labels": [], "entities": [{"text": "CTC", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9522993564605713}]}, {"text": "Permutation based on CTC was 16.3 times faster than that based on the decoder network, in terms of the time required to determine the best match permutation given the encoder network's output in Eq.", "labels": [], "entities": [{"text": "Permutation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.939978301525116}, {"text": "Eq", "start_pos": 195, "end_pos": 197, "type": "DATASET", "confidence": 0.919340193271637}]}, {"text": "shows the CERs for the mixed speech from the CSJ corpus.", "labels": [], "entities": [{"text": "CERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9423111081123352}, {"text": "CSJ corpus", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.896783858537674}]}, {"text": "Similarly to the WSJ experiments, our proposed model significantly reduced the CER from the baseline, where the average CER was 14.9% and the reduction ratio from the baseline was 83.9%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.8493766188621521}, {"text": "CER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9968053102493286}, {"text": "CER", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9953567385673523}]}], "tableCaptions": [{"text": " Table 1: Duration (hours) of unmixed and mixed  corpora. The mixed corpora are generated by Al- gorithm 1 in Section B of the supplementary ma- terial, using the training, development, and evalu- ation set respectively.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of unmixed speech without  multi-speaker training.", "labels": [], "entities": []}, {"text": " Table 4: CER (%) of mixed speech for WSJ.", "labels": [], "entities": [{"text": "CER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9985452890396118}, {"text": "WSJ", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8664942383766174}]}, {"text": " Table 5: CER (%) of mixed speech for CSJ.", "labels": [], "entities": [{"text": "CER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9981866478919983}]}, {"text": " Table 6: Comparison with conventional ap- proaches", "labels": [], "entities": []}]}