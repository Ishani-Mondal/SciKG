{"title": [{"text": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties", "labels": [], "entities": []}], "abstractContent": [{"text": "Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing.", "labels": [], "entities": []}, {"text": "\"Downstream\" tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.7371481209993362}]}, {"text": "The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations.", "labels": [], "entities": []}, {"text": "We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite Ray Mooney's quip that you cannot cram the meaning of a whole %&!$# sentence into a single $&!#* vector, sentence embedding methods have achieved impressive results in tasks ranging from machine translation () to entailment detection (, spurring the quest for \"universal embeddings\" trained once and used in a variety of applications (e.g.,.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.8042146861553192}, {"text": "entailment detection", "start_pos": 221, "end_pos": 241, "type": "TASK", "confidence": 0.8977414965629578}]}, {"text": "Positive results on concrete problems suggest that embeddings capture important linguistic properties of sentences.", "labels": [], "entities": []}, {"text": "However, real-life \"downstream\" tasks require complex forms of inference, making it difficult to pinpoint the information a model is relying upon.", "labels": [], "entities": []}, {"text": "Impressive as it might be that a system can tell that the sentence \"A movie that doesn't aim too high, but it doesn't need to\") expresses a subjective viewpoint, it is hard to tell how the system (or even a human) comes to this conclusion.", "labels": [], "entities": []}, {"text": "Complex tasks can also carry hidden biases that models might lock onto.", "labels": [], "entities": []}, {"text": "For example, show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9991546869277954}, {"text": "SICK sentence entailment task", "start_pos": 114, "end_pos": 143, "type": "TASK", "confidence": 0.9063376784324646}]}, {"text": "Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5).", "labels": [], "entities": [{"text": "Model introspection", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6449335664510727}]}, {"text": "However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods. and introduced a more general approach, relying on the notion of what we will call probing tasks.", "labels": [], "entities": []}, {"text": "A probing task is a classification problem that focuses on simple linguistic properties of sentences.", "labels": [], "entities": []}, {"text": "For example, one such task might require to categorize sentences by the tense of their main verb.", "labels": [], "entities": []}, {"text": "Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tuning).", "labels": [], "entities": [{"text": "machine translation)", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7712324659029642}]}, {"text": "If the classifier succeeds, it means that the pre-trained encoder is storing readable tense information into the embeddings it creates.", "labels": [], "entities": []}, {"text": "Note that: (i) The probing task asks a simple question, minimizing interpretability problems.", "labels": [], "entities": []}, {"text": "(ii) Because of their simplicity, it is easier to control for biases in probing tasks than in downstream tasks.", "labels": [], "entities": []}, {"text": "(iii) The probing task methodology is agnostic with respect to the encoder architecture, as long as it produces a vector representation of sentences.", "labels": [], "entities": []}, {"text": "We greatly extend earlier work on probing tasks as follows.", "labels": [], "entities": [{"text": "probing tasks", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9038877487182617}]}, {"text": "First, we introduce a larger set of probing tasks (10 in total), organized by the type of linguistic properties they probe.", "labels": [], "entities": []}, {"text": "Second, we systematize the probing task methodology, controlling fora number of possible nuisance factors, and framing all tasks so that they only require single sentence representations as input, for maximum generality and to ease result interpretation.", "labels": [], "entities": [{"text": "result interpretation", "start_pos": 232, "end_pos": 253, "type": "TASK", "confidence": 0.7323268055915833}]}, {"text": "Third, we use our probing tasks to explore a wide range of state-of-the-art encoding architectures and training methods, and further relate probing and downstream task performance.", "labels": [], "entities": []}, {"text": "Finally, we are publicly releasing our probing data sets and tools, hoping they will become a standard way to study the linguistic properties of sentence embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Baselines Baseline and human-bound performance are reported in the top block of.", "labels": [], "entities": [{"text": "Baselines", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9417924880981445}, {"text": "Baseline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5705310106277466}]}, {"text": "Length is a linear classifier with sentence length as sole feature.", "labels": [], "entities": []}, {"text": "NB-uni-tfidf is a Naive Bayes classifier using words' tfidf scores as features, NBbi-tfidf its extension to bigrams.", "labels": [], "entities": [{"text": "NB-uni-tfidf", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9215454459190369}]}, {"text": "Finally, BoVfastText derives sentence representations by averaging the fastText embeddings of the words they contain (same embeddings used as input to the encoders).", "labels": [], "entities": []}, {"text": "3 Except, trivially, for Length on SentLen and the NB baselines on WC, there is a healthy gap between top baseline performance and human upper bounds.", "labels": [], "entities": [{"text": "Length", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9932888150215149}]}, {"text": "NB-uni-tfidf evaluates to what extent our tasks can be addressed solely based on knowledge about the distribution of words in the training sentences.", "labels": [], "entities": []}, {"text": "Words are of course to some extent informative for most tasks, leading to relatively high performance in Tense, SubjNum and ObjNum.", "labels": [], "entities": []}, {"text": "Recall that the words containing the probed features are disjoint between train and test partitions, so we are not observing a confound here, but rather the effect of the redundancies one expects in natural language data.", "labels": [], "entities": []}, {"text": "For example, for Tense, since sentences often contain more than one verb in the same tense, NB-uni-tfidf can exploit nontarget verbs as cues: the NB features most associated to the past class are verbs in the past tense (e.g \"sensed\", \"lied\", \"announced\"), and similarly for present (e.g \"uses\", \"chuckles\", \"frowns\").", "labels": [], "entities": [{"text": "Tense", "start_pos": 17, "end_pos": 22, "type": "TASK", "confidence": 0.9523115754127502}]}, {"text": "Using bigram features (NB-bi-tfidf) brings in general little or no improvement with respect to the unigram baseline, except, trivially, for the BShift Similar results are obtained summing embeddings, and using GloVe embeddings ().", "labels": [], "entities": [{"text": "BShift", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.9711338877677917}, {"text": "summing embeddings", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.8749177157878876}]}, {"text": "task, where NB-bi-tfidf can easily detect unlikely bigrams.", "labels": [], "entities": []}, {"text": "NB-bi-tfidf has below-random performance on SOMO, confirming that the semantic intruder is not given away by superficial bigram cues.", "labels": [], "entities": []}, {"text": "Our first striking result is the good overall performance of Bag-of-Vectors, confirming early insights that aggregated word embeddings capture surprising amounts of sentence information).", "labels": [], "entities": []}, {"text": "BoV's good WC and SentLen performance was already established by.", "labels": [], "entities": [{"text": "BoV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9589416980743408}, {"text": "WC", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9838082194328308}]}, {"text": "Not surprisingly, word-order-unaware BoV performs randomly in BShift and in the more sophisticated semantic tasks SOMO and CoordInv.", "labels": [], "entities": [{"text": "BShift", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.8201839923858643}]}, {"text": "More interestingly, BoV is very good at the Tense, SubjNum, ObjNum, and TopConst tasks (much better than the word-based baselines), and well above chance in TreeDepth.", "labels": [], "entities": []}, {"text": "The good performance on Tense, SubjNum and ObjNum has a straightforward explanation we have already hinted at above.", "labels": [], "entities": [{"text": "ObjNum", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9200626015663147}]}, {"text": "Many sentences are naturally \"redundant\", in the sense that most tensed verbs in a sentence are in the same tense, and similarly for number in nouns.", "labels": [], "entities": []}, {"text": "In 95.2% Tense, 75.9% SubjNum and 78.7% ObjNum test sentences, the target tense/number feature is also the majority one for the whole sentence.", "labels": [], "entities": []}, {"text": "Word embeddings capture features such as number and tense  by the embeddings (such as the part of speech of a word) might signal different syntactic structures.", "labels": [], "entities": []}, {"text": "For example, sentences in the \"WHNP SQ .\" top constituent class (e.g., \"How long before you leave us again?\") must contain a wh word, and will often feature an auxiliary or modal verb.", "labels": [], "entities": [{"text": "WHNP SQ .\" top constituent class", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.91889821489652}]}, {"text": "BoV can rely on this information to noisily predict the correct class.", "labels": [], "entities": [{"text": "BoV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8919990658760071}]}, {"text": "Encoding architectures Comfortingly, proper encoding architectures clearly outperform BoV.", "labels": [], "entities": []}, {"text": "An interesting observation in gests that the prior imposed by the encoder architecture strongly preconditions the nature of the embeddings.", "labels": [], "entities": []}, {"text": "Complementing recent evidence that convolutional architectures are on a par with recurrent ones in seq2seq tasks, we find that Gated ConvNet's overall probing task performance is comparable to that of the best LSTM architecture (although, as shown in Appendix, the LSTM has a slight edge on downstream tasks).", "labels": [], "entities": []}, {"text": "We also replicate the finding of that BiLSTM-max outperforms BiLSTM-last both in the downstream tasks (see Appendix) and in the probing tasks.", "labels": [], "entities": []}, {"text": "Interestingly, the latter only outperforms the former in SentLen, a task that captures a superficial aspect of sentences (how many words they contain), that could get in the way of inducing more useful linguistic knowledge.", "labels": [], "entities": []}, {"text": "Training tasks We focus next on how different training tasks affect BiLSTM-max, but the patterns are generally representative across architectures.", "labels": [], "entities": []}, {"text": "NMT training leads to encoders that are more linguistically aware than those trained on the NLI data set, despite the fact that we confirm the finding of Conneau and colleagues that NLI is best for downstream tasks (Appendix).", "labels": [], "entities": [{"text": "NLI data set", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.8256776134173075}]}, {"text": "Perhaps, NMT captures richer linguistic features useful for the probing tasks, whereas shallower or more adhoc features might help more in our current downstream tasks.", "labels": [], "entities": []}, {"text": "Suggestively, the one task where NLI clearly outperforms NMT is WC.", "labels": [], "entities": []}, {"text": "Thus, NLI training is better at preserving shallower word features that might be more useful in downstream tasks (cf. and discussion there).", "labels": [], "entities": []}, {"text": "Unsupervised training (SkipThought and AutoEncoder) is not on a par with supervised tasks, but still effective.", "labels": [], "entities": []}, {"text": "AutoEncoder training leads, unsurprisingly, to a model excelling at SentLen, but it attains low performance in the WC prediction task.", "labels": [], "entities": [{"text": "WC prediction task", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.9318464199701945}]}, {"text": "This curious result might indicate that the latter information is stored in the embeddings in a complex way, not easily readable by our MLP.", "labels": [], "entities": []}, {"text": "At the other end, Seq2Tree is trained to predict annotation from the same parser we used to create some of the probing tasks.", "labels": [], "entities": []}, {"text": "Thus, its high performance on TopConst, Tense, SubjNum, ObjNum and TreeDepth is probably an artifact.", "labels": [], "entities": [{"text": "ObjNum", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8916359543800354}]}, {"text": "Indeed, for most of these tasks, Seq2Tree performance is above the human bound, that is, Seq2Tree learned to mimic the parser errors in our benchmarks.", "labels": [], "entities": []}, {"text": "For the more challenging SOMO and CoordInv tasks, that only indirectly rely on tagging/parsing information, Seq2Tree is comparable to NMT, that does not use explicit syntactic information.", "labels": [], "entities": [{"text": "tagging/parsing information", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.8528525680303574}]}, {"text": "Perhaps most interestingly, BiLSTM-max already achieves very good performance without any training (Untrained row in).", "labels": [], "entities": [{"text": "BiLSTM-max", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.6437495350837708}]}, {"text": "Untrained BiLSTM-max also performs quite well in the downstream tasks (Appendix).", "labels": [], "entities": [{"text": "Appendix", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9700294137001038}]}, {"text": "This architecture must encode priors that are intrinsically good for sentence representations.", "labels": [], "entities": []}, {"text": "Untrained BiLSTM-max exploits the input fastText embeddings, and multiplying the latter by a random recurrent matrix provides a form of positional encoding.", "labels": [], "entities": [{"text": "positional encoding", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7741198539733887}]}, {"text": "However, good performance in a task such as SOMO, where BoV fails and positional information alone should not help (the intruder is randomly distributed across the sentence), suggests that other architectural biases are at work.", "labels": [], "entities": []}, {"text": "Intriguingly, a preliminary comparison of untrained BiLSTM-max and human subjects on the SOMO sentences evaluated by both reveals that, whereas humans have a bias towards finding sentences acceptable (62% sentences are rated as untampered with, vs. 48% ground-truth proportion), the model has a strong bias in the opposite direction (it rates 83% of the sentences as modified).", "labels": [], "entities": [{"text": "SOMO sentences", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.7662740349769592}]}, {"text": "A cursory look at contrasting errors confirms, unsurprisingly, that those made by humans are perfectly justified, while model errors are opaque.", "labels": [], "entities": []}, {"text": "For example, the sentence \"I didn't come hereto reunite (orig.", "labels": [], "entities": []}, {"text": "undermine) you\" seems perfectly acceptable in its modified form, and indeed subjects judged it as such, whereas untrained BiLSTM-max \"correctly\" rated it as a modified item.", "labels": [], "entities": []}, {"text": "Conversely, it is difficult to see any clear reason for the latter tendency to rate perfectly acceptable originals as modified.", "labels": [], "entities": []}, {"text": "We leave a more thorough investigation to further work.", "labels": [], "entities": []}, {"text": "See similar observations on the effectiveness of untrained ConvNets in vision by.", "labels": [], "entities": []}, {"text": "Probing task comparison A good encoder, such as NMT-trained BiLSTM-max, shows generally good performance across probing tasks.", "labels": [], "entities": [{"text": "NMT-trained BiLSTM-max", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.7161490023136139}]}, {"text": "At one extreme, performance is not particularly high on the surface tasks, which might bean indirect sign of the encoder extracting \"deeper\" linguistic properties.", "labels": [], "entities": []}, {"text": "At the other end, performance is still far from the human bounds on TreeDepth, BShift, SOMO and CoordInv.", "labels": [], "entities": [{"text": "TreeDepth", "start_pos": 68, "end_pos": 77, "type": "DATASET", "confidence": 0.9517087936401367}, {"text": "BShift", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.8863961696624756}]}, {"text": "The last 3 tasks ask if a sentence is syntactically or semantically anomalous.", "labels": [], "entities": []}, {"text": "This is a daunting job for an encoder that has not been explicitly trained on acceptability, and it is interesting that the best models are, at least to a certain extent, able to produce reasonable anomaly judgments.", "labels": [], "entities": []}, {"text": "The asymmetry between the difficult TreeDepth and easier TopConst is also interesting.", "labels": [], "entities": []}, {"text": "Intuitively, TreeDepth requires more nuanced syntactic information (down to the deepest leaf of the tree) than TopConst, that only requires identifying broad chunks.", "labels": [], "entities": []}, {"text": "reports how probing task accuracy changes in function of encoder training epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9821469187736511}]}, {"text": "The figure shows that NMT probing performance is largely independent of target language, with strikingly similar development patterns across French, German and Finnish.", "labels": [], "entities": [{"text": "NMT probing", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9090123474597931}]}, {"text": "Note in particular the similar probing accuracy curves in French and Finnish, while the corresponding BLEU scores (in lavender) are consistently higher in the former lan- guage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.7643605470657349}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9991958737373352}]}, {"text": "For both NMT and SkipThought, WC performance keeps increasing with epochs.", "labels": [], "entities": [{"text": "NMT", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.7445507645606995}, {"text": "SkipThought", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8415887355804443}, {"text": "WC", "start_pos": 30, "end_pos": 32, "type": "TASK", "confidence": 0.8492590188980103}]}, {"text": "For the other tasks, we observe instead an early flattening of the NMT probing curves, while BLEU performance keeps increasing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9992700219154358}]}, {"text": "Most strikingly, SentLen performance is actually decreasing, suggesting again that, as a model captures deeper linguistic properties, it will tend to forget about this superficial feature.", "labels": [], "entities": []}, {"text": "Finally, for the challenging SOMO task, the curves are mostly flat, suggesting that what BiLSTM-max is able to capture about this task is already encoded in its architecture, and further training doesn't help much.", "labels": [], "entities": [{"text": "SOMO task", "start_pos": 29, "end_pos": 38, "type": "TASK", "confidence": 0.8866328597068787}]}, {"text": "Probing vs. downstream tasks reports correlation between performance on our probing tasks and the downstream tasks available in the SentEval 5 suite, which consists of classification (MR, CR, SUBJ, MPQA, SST2, SST5, TREC), natural language inference (SICK-E), semantic relatedness (SICK-R, STSB), paraphrase detection (MRPC) and semantic textual similarity (STS 2012 to 2017) tasks.", "labels": [], "entities": [{"text": "TREC", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.950272262096405}, {"text": "paraphrase detection", "start_pos": 297, "end_pos": 317, "type": "TASK", "confidence": 0.8288099467754364}]}, {"text": "Strikingly, WC is significantly positively correlated with all downstream tasks.", "labels": [], "entities": [{"text": "WC", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.8139467239379883}]}, {"text": "This suggests that, at least for current models, the latter do not require extracting particularly abstract knowledge from the data.", "labels": [], "entities": []}, {"text": "Just relying on the words contained in the input sentences can get you along way.", "labels": [], "entities": []}, {"text": "Conversely, there is a significant negative correlation between SentLen and most downstream tasks.", "labels": [], "entities": []}, {"text": "The number of words in a sentence is not informative about its linguistic contents.", "labels": [], "entities": []}, {"text": "The more models abstract away from such information, the more likely it is they will use their capacity to capture more interesting features, as the decrease of the SentLen curve along training (see) also suggests.", "labels": [], "entities": []}, {"text": "CoordInv and, especially, SOMO, the tasks requiring the most sophisticated semantic knowledge, are those that positively correlate with the largest number of downstream tasks after WC.", "labels": [], "entities": []}, {"text": "We observe intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions.", "labels": [], "entities": []}, {"text": "Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments.", "labels": [], "entities": [{"text": "logical entailment", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7671859860420227}]}, {"text": "TopConst and the number tasks negatively correlate with various similarity and sentiment data sets (SST, STS, SICK-R).", "labels": [], "entities": []}, {"text": "This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (.", "labels": [], "entities": []}, {"text": "It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar).", "labels": [], "entities": []}, {"text": "Remarkably, TREC question type classification is the downstream task correlating with most probing tasks.", "labels": [], "entities": [{"text": "TREC question type classification", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8676590770483017}]}, {"text": "Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate). are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders.", "labels": [], "entities": [{"text": "Question classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.804548442363739}]}, {"text": "Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena.", "labels": [], "entities": []}, {"text": "Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g.,.", "labels": [], "entities": []}, {"text": "These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Probing task accuracies. Classification performed by a MLP with sigmoid nonlinearity, taking  pre-learned sentence embeddings as input (see Appendix for details and logistic regression results).", "labels": [], "entities": []}]}