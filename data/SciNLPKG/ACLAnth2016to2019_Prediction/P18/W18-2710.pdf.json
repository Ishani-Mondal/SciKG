{"title": [{"text": "Bi-Directional Neural Machine Translation with Synthetic Parallel Data", "labels": [], "entities": [{"text": "Bi-Directional Neural Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6531875878572464}]}], "abstractContent": [{"text": "Despite impressive progress in high-resource settings, Neural Machine Translation (NMT) still struggles in low-resource and out-of-domain scenarios, often failing to match the quality of phrase-based translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.800794651110967}, {"text": "phrase-based translation", "start_pos": 187, "end_pos": 211, "type": "TASK", "confidence": 0.727683961391449}]}, {"text": "We propose a novel technique that combines back-translation and multilingual NMT to improve performance in these difficult cases.", "labels": [], "entities": []}, {"text": "Our technique trains a single model for both directions of a language pair, allowing us to back-translate source or target mono-lingual data without requiring an auxiliary model.", "labels": [], "entities": []}, {"text": "We then continue training on the augmented parallel data, enabling a cycle of improvement fora single model that can incorporate any source, target, or parallel data to improve both translation directions.", "labels": [], "entities": []}, {"text": "As a byproduct, these models can reduce training and deployment costs significantly compared to uni-directional models.", "labels": [], "entities": []}, {"text": "Extensive experiments show that our technique outperforms standard back-translation in low-resource scenarios, improves quality on cross-domain tasks, and effectively reduces costs across the board.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has been rapidly adopted in industry as it consistently outperforms previous methods across domains and language pairs.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8181169728438059}]}, {"text": "However, NMT systems still struggle compared to Phrase-based Statistical Machine Translation (SMT) in low-resource or out-of-domain scenarios (.", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation (SMT)", "start_pos": 48, "end_pos": 98, "type": "TASK", "confidence": 0.7076955267361232}]}, {"text": "This performance gap is a significant roadblock to full adoption of NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7435598373413086}]}, {"text": "In many low-resource scenarios, parallel data is prohibitively expensive or otherwise impractical to collect, whereas monolingual data maybe more abundant.", "labels": [], "entities": []}, {"text": "SMT systems have the advantage of a dedicated language model that can incorporate all available target-side monolingual data to significantly improve translation quality (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9835461378097534}]}, {"text": "By contrast, NMT systems consist of one large neural network that performs full sequence-to-sequence translation (.", "labels": [], "entities": []}, {"text": "Trained end-to-end on parallel data, these models lack a direct avenue for incorporating monolingual data.", "labels": [], "entities": []}, {"text": "overcome this challenge by back-translating target monolingual data to produce synthetic parallel data that can be added to the training pool.", "labels": [], "entities": []}, {"text": "While effective, backtranslation introduces the significant cost of first building a reverse system.", "labels": [], "entities": []}, {"text": "Another technique for overcoming alack of data is multitask learning, in which domain knowledge can be transferred between related tasks.", "labels": [], "entities": []}, {"text": "apply the idea to multilingual NMT by concatenating parallel data of various language pairs and marking the source with the desired output language.", "labels": [], "entities": []}, {"text": "The authors report promising results for translation between languages that have zero parallel data.", "labels": [], "entities": [{"text": "translation between languages", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.8780272603034973}]}, {"text": "This approach also dramatically reduces the complexity of deployment by packing multiple language pairs into a single model.", "labels": [], "entities": []}, {"text": "We propose a novel combination of backtranslation and multilingual NMT that trains both directions of a language pair jointly in a single model.", "labels": [], "entities": []}, {"text": "Specifically, we initialize a bi-directional model on parallel data and then use it to translate select source and target monolingual data.", "labels": [], "entities": []}, {"text": "Training is then continued on the augmented parallel data, leading to a cycle of improvement.", "labels": [], "entities": []}, {"text": "This approach has several advantages: \u2022 A single NMT model with standard architecture that performs all forward and backward translation during training.", "labels": [], "entities": []}, {"text": "\u2022 Training costs reduced significantly compared to uni-directional systems.", "labels": [], "entities": [{"text": "Training", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.8389896750450134}]}, {"text": "\u2022 Improvements in translating quality for lowresource languages, even over uni-directional systems with back-translation.", "labels": [], "entities": [{"text": "translating", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9653970003128052}]}, {"text": "\u2022 Effectiveness in domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7163654118776321}]}, {"text": "Via comprehensive experiments, we also contribute to best practices in selecting most suitable combinations of synthetic parallel data and choosing appropriate amount of monolingual data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe data, settings, and experimental methodology.", "labels": [], "entities": []}, {"text": "We then present the results of comprehensive experiments designed to answer the following questions: (1) How can synthetic data be most effectively used to improve translation quality?", "labels": [], "entities": []}, {"text": "(2) Does the reduction in training time for bi-directional NMT come at the cost of lower translation quality?", "labels": [], "entities": []}, {"text": "(3) Can we further improve training speed and translation quality training with incremental training and redecoding?", "labels": [], "entities": []}, {"text": "(4) How can we effectively choose monolingual training data?", "labels": [], "entities": []}, {"text": "(5) How well does bidirectional NMT perform on domain adaptation?", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8073024153709412}]}], "tableCaptions": [{"text": " Table 1: Data sizes of training, development, test,  sample and monolingual sets. Sample data serves  as the in-domain seed for data selection.", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores for uni-directional models (U-*) and bi-directional NMT models (B-*) trained  on different combinations of real and synthetic parallel data. Models in B-5* are fine-tuned from base  models in B-1. Best models in B-6* are fine-tuned from precedent models in B-5* and underscored  synthetic data is re-decoded using precedent models. Scores with largest improvement within each zone  are highlighted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998245358467102}]}, {"text": " Table 3: Number of checkpoints (= |updates|/1000 for TL/SW\u2194EN or |updates|/10,000 for  DE\u2194EN) used by various NMT models. Bi-directional models reduce the training time by 15-30% (com- paring 'TOTAL' rows). Fine-tuning bi-directional baseline models on synthetic parallel data reduces the  training time by 20-40% (comparing 'Synthetic' rows).", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores for bi-directional NMT models on Bible data. Models in A-2 are fine-tuned from  baseline models in A-1. Highlighted best models in A-3 are fine-tuned from precedent models in A-2  and underscored synthetic data is re-decoded using precedent models. Baseline models are significantly  improved in terms of BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993201494216919}, {"text": "Bible data", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8500249683856964}, {"text": "BLEU", "start_pos": 327, "end_pos": 331, "type": "METRIC", "confidence": 0.9994317889213562}]}]}