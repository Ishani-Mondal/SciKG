{"title": [{"text": "RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition", "labels": [], "entities": [{"text": "RETURNN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.782680332660675}, {"text": "Speech Recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6748964786529541}]}], "abstractContent": [{"text": "We compare the fast training and decoding speed of RETURNN of attention models for translation, due to fast CUDA LSTM kernels, and a fast pure Tensor-Flow beam search decoder.", "labels": [], "entities": [{"text": "translation", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9867019057273865}]}, {"text": "We show that a layer-wise pretraining scheme for recurrent attention models gives over 1% BLEU improvement absolute and it allows to train deeper recurrent encoder networks.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9995624423027039}]}, {"text": "Promising preliminary results on max.", "labels": [], "entities": []}, {"text": "expected BLEU training are presented.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9967003464698792}]}, {"text": "We obtain state-of-the-art models trained on the WMT 2017 German\u2194English translation task.", "labels": [], "entities": [{"text": "WMT 2017 German\u2194English translation task", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.7738315973963056}]}, {"text": "We also present end-to-end model results for speech recognition on the Switchboard task.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8056274056434631}]}, {"text": "The flexibility of RETURNN allows a fast research feedback loop to experiment with alternative architectures, and its generality allows to use it on a wide range of applications.", "labels": [], "entities": [{"text": "RETURNN", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.785631537437439}]}], "introductionContent": [{"text": "RETURNN, the RWTH extensible training framework for universal recurrent neural networks, was introduced in () for its computation.", "labels": [], "entities": [{"text": "RETURNN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9667815566062927}]}, {"text": "Since it was introduced, it got extended by comprehensive TensorFlow support.", "labels": [], "entities": [{"text": "TensorFlow", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.9805037975311279}]}, {"text": "A generic recurrent layer allows fora wide range of encoder-decoder-attention or other recurrent structures.", "labels": [], "entities": []}, {"text": "An automatic optimization logic can optimize the computation graph depending on training, scheduled sampling, sequence training, or beam search decoding.", "labels": [], "entities": []}, {"text": "The automatic optimization together with our fast native CUDA implemented LSTM kernels allows for very fast train-1 https://github.com/rwth-i6/returnn ing and decoding.", "labels": [], "entities": []}, {"text": "We will show in speed comparisons with Sockeye () that we are at least as fast or usually faster in both training and decoding.", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9442691802978516}]}, {"text": "Additionally, we show in experiments that we can train very competitive models for machine translation and speech recognition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8110013008117676}, {"text": "speech recognition", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7901851832866669}]}, {"text": "This flexibility together with the speed is the biggest strength of RETURNN.", "labels": [], "entities": [{"text": "RETURNN", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.5866879820823669}]}, {"text": "Our focus will be on recurrent attention models.", "labels": [], "entities": []}, {"text": "We introduce a layer-wise pretraining scheme for attention models and show its significant effect on deep recurrent encoder models.", "labels": [], "entities": []}, {"text": "We show promising preliminary results on expected maximum BLEU training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9976016879081726}]}, {"text": "The configuration files of all the experiments are publicly available 2 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Training speed and memory consumption  on WMT 2017 German\u2192English. Train time is  for seeing the full train dataset once. Batch size is  in words, such that it almost maximizes the GPU  memory consumption. The BLEU score is for  the converged models, reported for newstest2015  (dev) and newstest2017. The encoder has one  bidirectional LSTM layer and either 3 or 5 uni- directional LSTM layers.", "labels": [], "entities": [{"text": "WMT 2017 German", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9566529393196106}, {"text": "Batch size", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9014084339141846}, {"text": "BLEU", "start_pos": 220, "end_pos": 224, "type": "METRIC", "confidence": 0.9994693398475647}, {"text": "newstest2017", "start_pos": 298, "end_pos": 310, "type": "DATASET", "confidence": 0.951290488243103}]}, {"text": " Table 2. We see that RETURNN is the  fastest. We report results for the batch size that  yields the best speed. The slow speed of Sockeye  is due to frequent cross-device communication.", "labels": [], "entities": [{"text": "RETURNN", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9970763921737671}, {"text": "Sockeye", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.926497757434845}]}, {"text": " Table 2: Decoding speed and memory consump- tion on WMT 2017 German\u2192English. Time is  for decoding the whole dataset, reported for new- stest2015 (dev) and newstest2017, with beam size  12. Batch size is the number of sequences, such  that it optimizes the decoding speed. This does not  mean that it uses the whole GPU memory. These  are the same models as in", "labels": [], "entities": [{"text": "memory consump- tion", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.7932410165667534}, {"text": "WMT 2017 German", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9509164094924927}, {"text": "Time", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9874154925346375}]}, {"text": " Table 3. We obtain  the best results with Sockeye using a Transformer  network model (Vaswani et al., 2017), where we  achieve 32.0% BLEU on newstest2017. So far,  RETURNN does not support this architecture; see  Section 7 for details.  toolkit  BLEU [%]  2015 2017  RETURNN 31.2 31.3  Sockeye  29.7 30.2  Table 3: Comparison on German\u2192English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9990092515945435}, {"text": "newstest2017", "start_pos": 142, "end_pos": 154, "type": "DATASET", "confidence": 0.9622661471366882}, {"text": "BLEU", "start_pos": 247, "end_pos": 251, "type": "METRIC", "confidence": 0.9946115016937256}]}, {"text": " Table 4. We observe that our toolkit outper- forms all other toolkits. The best result obtained  by other toolkits is using Marian (25.5% BLEU).  In comparison, RETURNN achieves 26.1%. We  also compare RETURNN to the best performing  single systems of WMT 2017. In comparison to  the fine-tuned evaluation systems that also include  back-translated data, our model performs worse by  only 0.3 to 0.9 BLEU. We did not run experiments  with back-translated data, which can potentially  boost the performance by several BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9984647035598755}, {"text": "RETURNN", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.9942823648452759}, {"text": "WMT 2017", "start_pos": 253, "end_pos": 261, "type": "DATASET", "confidence": 0.8850767314434052}, {"text": "BLEU", "start_pos": 401, "end_pos": 405, "type": "METRIC", "confidence": 0.9926618337631226}, {"text": "BLEU", "start_pos": 518, "end_pos": 522, "type": "METRIC", "confidence": 0.9980096220970154}]}, {"text": " Table 4: Performance comparison on WMT 2017  English\u2192German. The baseline systems (upper  half) are trained on the parallel data of the WMT  Enlgish\u2192German 2017 task. We downloaded the  hypotheses from here.", "labels": [], "entities": [{"text": "WMT 2017  English\u2192German", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9199069142341614}, {"text": "WMT  Enlgish\u2192German 2017 task", "start_pos": 137, "end_pos": 166, "type": "DATASET", "confidence": 0.8359689911206564}]}, {"text": " Table 5: Performance comparison on Switch- board, trained on 300h. hybrid 1 is the IBM 2017  ResNet model (Saon et al., 2017). hybrid 2 trained  with Lattice-free MMI (Hadian et al., 2018).  CTC 3 is the Baidu 2014 DeepSpeech model (Han- nun et al., 2014). Our attention model does not use  any language model.", "labels": [], "entities": []}, {"text": " Table 6. The observations very  clearly match our expectations, that we can both  greatly improve the overall performance, and we  are able to train deeper models. A minor benefit is  faster training speed of the initial pretrain epochs.", "labels": [], "entities": []}]}