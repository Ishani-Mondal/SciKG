{"title": [{"text": "Unsupervised Neural Machine Translation with Weight Sharing", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6695124407609304}, {"text": "Weight Sharing", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.6525091379880905}]}], "abstractContent": [{"text": "Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data.", "labels": [], "entities": [{"text": "Unsupervised neural machine translation (NMT)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7407213577202388}, {"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7679092884063721}]}, {"text": "The models proposed for unsuper-vised NMT often use only one shared en-coder to map the pairs of sentences from different languages to a shared-latent space , which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure.", "labels": [], "entities": []}, {"text": "To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences.", "labels": [], "entities": []}, {"text": "Besides , two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation.", "labels": [], "entities": [{"text": "cross-language translation", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.786963552236557}]}, {"text": "With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.7497744758923849}]}], "introductionContent": [{"text": "Neural machine translation), directly applying a single neural network to transform the source sentence into the target sentence, has now reached impressive performance.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7872535387674967}]}, {"text": "The NMT typically consists of two sub neural networks.", "labels": [], "entities": []}, {"text": "The encoder network reads and encodes the source sentence into a Feng Wang is the corresponding author of this paper context vector, and the decoder network generates the target sentence iteratively based on the context vector.", "labels": [], "entities": []}, {"text": "NMT can be studied in supervised and unsupervised learning settings.", "labels": [], "entities": []}, {"text": "In the supervised setting, bilingual corpora is available for training the NMT model.", "labels": [], "entities": []}, {"text": "In the unsupervised setting, we only have two independent monolingual corpora with one for each language and there is no bilingual training example to provide alignment information for the two languages.", "labels": [], "entities": []}, {"text": "Due to lack of alignment information, the unsupervised NMT is considered more challenging.", "labels": [], "entities": [{"text": "alignment", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9580298662185669}]}, {"text": "However, this task is very promising, since the monolingual corpora is usually easy to be collected.", "labels": [], "entities": []}, {"text": "Motivated by recent success in unsupervised cross-lingual embeddings (), the models proposed for unsupervised NMT often assume that a pair of sentences from two different languages can be mapped to a same latent representation in a shared-latent space (.", "labels": [], "entities": []}, {"text": "Following this assumption,  use a single encoder and a single decoder for both the source and target languages.", "labels": [], "entities": []}, {"text": "The encoder and decoder, acting as a standard auto-encoder (AE), are trained to reconstruct the inputs.", "labels": [], "entities": []}, {"text": "(2017b) utilize a shared encoder but two independent decoders.", "labels": [], "entities": []}, {"text": "With some good performance, they share a glaring defect, i.e., only one encoder is shared by the source and target languages.", "labels": [], "entities": []}, {"text": "Although the shared encoder is vital for mapping sentences from different languages into the shared-latent space, it is weak in keeping the uniqueness and internal characteristics of each language, such as the style, terminology and sentence structure.", "labels": [], "entities": []}, {"text": "Since each language has its own characteristics, the source and target languages should be encoded and learned independently.", "labels": [], "entities": []}, {"text": "Therefore, we conjecture that the shared encoder maybe a factor limit-ing the potential translation performance.", "labels": [], "entities": []}, {"text": "In order to address this issue, we extend the encoder-shared model, i.e., the model with one shared encoder, by leveraging two independent encoders with each for one language.", "labels": [], "entities": []}, {"text": "Similarly, two independent decoders are utilized.", "labels": [], "entities": []}, {"text": "For each language, the encoder and its corresponding decoder perform an AE, where the encoder generates the latent representations from the perturbed input sentences and the decoder reconstructs the sentences from the latent representations.", "labels": [], "entities": [{"text": "AE", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9804520010948181}]}, {"text": "To map the latent representations from different languages to a shared-latent space, we propose the weightsharing constraint to the two AEs.", "labels": [], "entities": []}, {"text": "Specifically, we share the weights of the last few layers of two encoders that are responsible for extracting highlevel representations of input sentences.", "labels": [], "entities": []}, {"text": "Similarly, we share the weights of the first few layers of two decoders.", "labels": [], "entities": []}, {"text": "To enforce the shared-latent space, the word embeddings are used as a reinforced encoding component in our encoders.", "labels": [], "entities": []}, {"text": "For cross-language translation, we utilize the backtranslation following . Additionally, two different generative adversarial networks (GAN) ( , namely the local and global GAN, are proposed to further improve the cross-language translation.", "labels": [], "entities": [{"text": "cross-language translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.8626857399940491}, {"text": "cross-language translation", "start_pos": 214, "end_pos": 240, "type": "TASK", "confidence": 0.8135846853256226}]}, {"text": "We utilize the local GAN to constrain the source and target latent representations to have the same distribution, whereby the encoder tries to fool a local discriminator which is simultaneously trained to distinguish the language of a given latent representation.", "labels": [], "entities": []}, {"text": "We apply the global GAN to finetune the corresponding generator, i.e., the composition of the encoder and decoder of the other language, where a global discriminator is leveraged to guide the training of the generator by assessing how far the generated sentence is from the true data distribution . In summary, we mainly make the following contributions: \u2022 We propose the weight-sharing constraint to unsupervised NMT, enabling the model to utilize an independent encoder for each language.", "labels": [], "entities": []}, {"text": "To enforce the shared-latent space, we also propose the embedding-reinforced encoders and two different GANs for our model.", "labels": [], "entities": []}, {"text": "\u2022 We conduct extensive experiments on The code that we utilized to train and evaluate our models can be found at https://github.com/ZhenYangIACAS/unsupervised-NMT English-German, English-French and Chinese-to-English translation tasks.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed approach consistently achieves great success.", "labels": [], "entities": []}, {"text": "\u2022 Last but not least, we introduce the directional self-attention to model temporal order information for the proposed model.", "labels": [], "entities": []}, {"text": "Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self-attention layers of NMT.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed approach on EnglishGerman, English-French and Chinese-to-English translation tasks . We firstly describe the datasets, pre-processing and model hyper-parameters we used, then we introduce the baseline systems, and finally we present our experimental results.", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 71, "end_pos": 107, "type": "TASK", "confidence": 0.7834979097048441}]}, {"text": "Following the base model in (, we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.9365293085575104}]}, {"text": "We use beam search with abeam size of 4 and length penalty \u03b1 = 0.6.", "labels": [], "entities": [{"text": "beam search", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8238836824893951}, {"text": "length penalty \u03b1", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9798505107561747}]}, {"text": "The model is implemented in TensorFlow ( and trained on up to four K80 GPUs synchronously in a multi-GPU setup on a single machine.", "labels": [], "entities": []}, {"text": "For model selection, we stop training when the model achieves no improvement for the tenth evaluation on the development set, which is comprised of 3000 source and target sentences extracted randomly from the monolingual training corpora.", "labels": [], "entities": [{"text": "model selection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7393459379673004}]}, {"text": "Following ( , we translate the source sentences to the target language, and then translate the resulting sentences back to the source language.", "labels": [], "entities": []}, {"text": "The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstructions via this two-step translation process.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9790300130844116}]}, {"text": "The performance is finally averaged over two directions, i.e., from source to target and from target to source.", "labels": [], "entities": []}, {"text": "BLEU () is utilized as the evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9813264608383179}]}, {"text": "For Chinese-to-English, we apply the script mteval-v11b.pl to evaluate the translation performance.", "labels": [], "entities": []}, {"text": "For English-German and English-French, we evaluate the translation performance with the script multi-belu.pl 9 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The translation performance on English-German, English-French and Chinese-to-English test  sets. The results of (Lample et al., 2017) are copied directly from their paper. We do not present the  results of (Artetxe et al., 2017b) since we use different training sets.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9598298072814941}]}, {"text": " Table 3: Ablation study on English-German, English-French and Chinese-to-English translation tasks.  Without weight sharing means no layers are shared in the two AEs.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9858337044715881}, {"text": "Chinese-to-English translation tasks", "start_pos": 63, "end_pos": 99, "type": "TASK", "confidence": 0.7859331568082174}]}]}