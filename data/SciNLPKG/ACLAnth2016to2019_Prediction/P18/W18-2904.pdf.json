{"title": [{"text": "Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for SRL?", "labels": [], "entities": [{"text": "SRL", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9645354151725769}]}], "abstractContent": [{"text": "Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)?", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 172, "end_pos": 200, "type": "TASK", "confidence": 0.7875390698512396}]}, {"text": "We address this question by incorporating the massively successful ELMo em-beddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 213, "end_pos": 216, "type": "TASK", "confidence": 0.9868729114532471}]}, {"text": "In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntactically-informed models still out-perform syntax-free models when both use ELMo, especially on out-of-domain data.", "labels": [], "entities": [{"text": "F1", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.979748547077179}]}, {"text": "Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many state-of-the-art NLP models are now \"endto-end\" deep neural network architectures which eschew explicit linguistic structures as input in favor of operating directly on raw text.", "labels": [], "entities": []}, {"text": "Recently, proposed a method for unsupervised learning of rich, contextuallyencoded token representations which, when supplied as input word representations in end-to-end models, further increased these models' performance by up to 25% across many NLP tasks.", "labels": [], "entities": []}, {"text": "The immense success of these linguistically-agnostic models brings into question whether linguistic structures such as syntactic parse trees still provide any additional benefits in a deep neural network architecture for e.g. semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 226, "end_pos": 248, "type": "TASK", "confidence": 0.6119410395622253}]}, {"text": "In this work, we aim to begin to answer this question by experimenting with incorporating the ELMo embeddings of into LISA (, a \"linguistically-informed\" deep neural network architecture for SRL which, when given weaker GloVe embeddings as inputs (, has been shown to leverage syntax to outperform a state-of-the-art, linguistically-agnostic end-to-end SRL model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9862456321716309}]}, {"text": "In experiments on the CoNLL-2005 English SRL shared task, we find that, while the ELMo representations out-perform GloVe and begin to close the performance gap between LISA with predicted and gold syntactic parses, syntacticallyinformed models still out-perform syntax-free models, especially on out-of-domain data.", "labels": [], "entities": [{"text": "CoNLL-2005 English SRL shared task", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.7379954695701599}]}, {"text": "Our results suggest that with the right modeling, incorporating linguistic structures can indeed further improve strong neural network models for NLP.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we assess the impact of replacing GLoVe embeddings (+GloVe) with ELMo embeddings (+ELMo) in strong, end-to-end neural network models for SRL: one which incorporates syntax (LISA) and one which does not (SA).", "labels": [], "entities": [{"text": "SRL", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9735490679740906}]}, {"text": "The two models are identical except that the latter does not have an attention head trained to predict syntactic heads.", "labels": [], "entities": []}, {"text": "Since the LISA model can both predict its own parses as well as consume parses from another model, as in we experiment with providing syntactic parses from a high-quality dependency parser (+D&M), as well as providing the gold parses (+Gold) as an upper bound on the gains that can be attained by providing more accurate parses.", "labels": [], "entities": []}, {"text": "We compare LISA models to two baseline models: The deep bi-LSTM model of  and the deep self-attention model of.", "labels": [], "entities": []}, {"text": "Though both also report ensemble scores, we compare to the single-model scores of both works.", "labels": [], "entities": []}, {"text": "We note that is not directly comparable because they use gold predicates attest time.", "labels": [], "entities": []}, {"text": "Despite this handicap, our best models obtain higher scores than", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dependency parse accuracy (UAS) on  CoNLL-2005.", "labels": [], "entities": [{"text": "Dependency parse accuracy (UAS)", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.7599689811468124}, {"text": "CoNLL-2005", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.9414280652999878}]}, {"text": " Table 2: Precision, recall and F1 on CoNLL-2005 with predicted predicates.  \u2020 denotes that models were  evaluated on gold predicates.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9953976273536682}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993114471435547}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9995317459106445}, {"text": "CoNLL-2005", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.9456915855407715}]}, {"text": " Table 3: Predicate detection precision, recall and  F1 on", "labels": [], "entities": [{"text": "Predicate detection", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8604970574378967}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.7979317903518677}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9997593760490417}, {"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9997033476829529}]}]}