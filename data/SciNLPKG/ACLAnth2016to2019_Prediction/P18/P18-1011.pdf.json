{"title": [{"text": "Improving Knowledge Graph Embedding Using Simple Constraints", "labels": [], "entities": [{"text": "Improving Knowledge Graph Embedding", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9283832758665085}]}], "abstractContent": [{"text": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research.", "labels": [], "entities": [{"text": "Embedding knowledge graphs (KGs)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8165029187997183}]}, {"text": "Early works performed this task via simple models developed over KG triples.", "labels": [], "entities": []}, {"text": "Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples.", "labels": [], "entities": []}, {"text": "This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding.", "labels": [], "entities": []}, {"text": "We examine non-negativity constraints on entity representations and approximate en-tailment constraints on relation representations.", "labels": [], "entities": []}, {"text": "The former help to learn compact and interpretable representations for entities.", "labels": [], "entities": []}, {"text": "The latter further encode regularities of logical entailment between relations into their distributed representations.", "labels": [], "entities": []}, {"text": "These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalabil-ity.", "labels": [], "entities": []}, {"text": "Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9751893281936646}, {"text": "Freebase", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.9004368185997009}, {"text": "DBpedia", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.9161679744720459}]}, {"text": "The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space.", "labels": [], "entities": []}, {"text": "Code and data are available at https://github.com/i ieir-km/ComplEx-NNE_AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.6811955571174622}]}], "introductionContent": [{"text": "The past decade has witnessed great achievements in building web-scale knowledge graphs (KGs), e.g.,), DBpedia (, and Google's Knowledge * Corresponding author: Quan Wang.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.815019428730011}]}, {"text": "A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity).", "labels": [], "entities": []}, {"text": "Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.884977400302887}]}, {"text": "Recently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic.", "labels": [], "entities": []}, {"text": "The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG.", "labels": [], "entities": []}, {"text": "Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples).", "labels": [], "entities": []}, {"text": "Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond KG triples (.", "labels": [], "entities": []}, {"text": "See () fora thorough review.", "labels": [], "entities": []}, {"text": "This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task.", "labels": [], "entities": []}, {"text": "Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations.", "labels": [], "entities": []}, {"text": "By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability.", "labels": [], "entities": []}, {"text": "By using the latter, we further encode regularities of logical entailment between relations into their distributed representations, which might be advantageous to downstream tasks like link prediction and relation extraction).", "labels": [], "entities": [{"text": "link prediction", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.7867428660392761}, {"text": "relation extraction", "start_pos": 205, "end_pos": 224, "type": "TASK", "confidence": 0.7676031589508057}]}, {"text": "These constraints impose prior beliefs upon the structure of the embedding space, and will help us to learn more predictive embeddings, without significantly increasing the space or time complexity.", "labels": [], "entities": []}, {"text": "Our work has some similarities to those which integrate logical background knowledge into KG embedding).", "labels": [], "entities": []}, {"text": "Most of such works, however, need grounding of first-order logic rules.", "labels": [], "entities": []}, {"text": "The grounding process could be time and space inefficient especially for complicated rules.", "labels": [], "entities": []}, {"text": "To avoid grounding, tried to model rules using only relation representations.", "labels": [], "entities": []}, {"text": "But their work creates vector representations for entity pairs rather than individual entities, and hence fails to handle unpaired entities.", "labels": [], "entities": []}, {"text": "Moreover, it can only incorporate strict, hard rules which usually require extensive manual effort to create.", "labels": [], "entities": []}, {"text": "proposed adversarial training which can integrate first-order logic rules without grounding.", "labels": [], "entities": []}, {"text": "But their work, again, focuses on strict, hard rules.", "labels": [], "entities": []}, {"text": "tried to handle uncertainty of rules.", "labels": [], "entities": [{"text": "handle uncertainty of rules", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.7310844212770462}]}, {"text": "But their work assigns to different rules a same confidence level, and considers only equivalence and inversion of relations, which might not always be available in a given KG.", "labels": [], "entities": []}, {"text": "Our approach differs from the aforementioned works in that: (i) it imposes constraints directly on entity and relation representations without grounding, and can easily scale up to large KGs; (ii) the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no manual effort and applicable to almost all KGs; (iii) it learns an individual representation for each entity, and can successfully make predictions between unpaired entities.", "labels": [], "entities": []}, {"text": "We evaluate our approach on publicly available KGs of WordNet, Freebase, and DBpedia as well.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9191979765892029}, {"text": "Freebase", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8564456701278687}, {"text": "DBpedia", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9409776926040649}]}, {"text": "Experimental results indicate that our approach is simple yet surprisingly effective, achieving significant and consistent improvements over competitive baselines, but without negative impacts on efficiency or scalability.", "labels": [], "entities": []}, {"text": "The non-negativity and approximate entailment constraints indeed improve model interpretability, resulting in a substantially increased structuring of the embedding space.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first review related work in Section 2, and then detail our approach in Section 3.", "labels": [], "entities": []}, {"text": "Experiments and results are reported in Section 4, followed by concluding remarks in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents our experiments and results.", "labels": [], "entities": []}, {"text": "We first introduce the datasets used in our experiments ( \u00a7 4.1).", "labels": [], "entities": []}, {"text": "Then we empirically evaluate our approach in the link prediction task ( \u00a7 4.2).", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.8648181557655334}]}, {"text": "After that, we conduct extensive analysis on both entity representations ( \u00a7 4.3) and relation representations ( \u00a7 4.4) to show the interpretability of our model.", "labels": [], "entities": []}, {"text": "There will beat most 2s entities contained in s triples.", "labels": [], "entities": []}, {"text": "Code and data used in the experiments are available at https://github.com/iieir-km/ ComplEx-NNE_AER.", "labels": [], "entities": [{"text": "AER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.6391607522964478}]}, {"text": "The first two datasets we used are WN18 and F-B15K, released by.", "labels": [], "entities": [{"text": "WN18", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9310011267662048}, {"text": "F-B15K", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.49076300859451294}]}, {"text": "WN18 is a subset of WordNet containing 18 relations and 40,943 entities, and FB15K a subset of Freebase containing 1,345 relations and 14,951 entities.", "labels": [], "entities": [{"text": "WN18", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9641543030738831}, {"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9670694470405579}, {"text": "FB15K", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.798602283000946}, {"text": "Freebase", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.973055899143219}]}, {"text": "We create our third dataset from the mapping-based objects of core DBpedia.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.8610084056854248}]}, {"text": "We eliminate relations not included within the DBpedia ontology such as HomePage and Logo, and discard entities appearing less than 20 times.", "labels": [], "entities": [{"text": "HomePage", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9766020178794861}]}, {"text": "The final dataset, referred to as DB100K, is composed of 470 relations and 99,604 entities.", "labels": [], "entities": [{"text": "DB100K", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9607824683189392}]}, {"text": "Triples on each datasets are further divided into training, validation, and test sets, used for model training, hyperparameter tuning, and evaluation respectively.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.6680557429790497}]}, {"text": "We follow the original split for WN18 and FB15K, and draw a split of 597,572/ 50,000/50,000 triples for DB100K.", "labels": [], "entities": [{"text": "WN18", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9339858293533325}, {"text": "FB15K", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9483020901679993}, {"text": "DB100K", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.9791216254234314}]}, {"text": "We further use AMIE+ (Gal\u00e1rraga et al., 2015) 4 to extract approximate entailments automatically from the training set of each dataset.", "labels": [], "entities": [{"text": "AMIE+ (Gal\u00e1rraga et al., 2015) 4", "start_pos": 15, "end_pos": 47, "type": "DATASET", "confidence": 0.8109496146440506}]}, {"text": "As suggested by, we consider entailments with PCA confidence higher than 0.8.", "labels": [], "entities": [{"text": "PCA confidence", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.7909627854824066}]}, {"text": "As such, we extract 17 approximate entailments from WN18, 535 from FB15K, and 56 from DB100K.", "labels": [], "entities": [{"text": "WN18", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9765363335609436}, {"text": "FB15K", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9694194197654724}, {"text": "DB100K", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.9917610883712769}]}, {"text": "gives some examples of these approximate entailments, along with their confidence levels.", "labels": [], "entities": [{"text": "confidence", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9564675688743591}]}, {"text": "further summarizes the statistics of the datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Approximate entailments extracted from  WN18 (top), FB15K (middle), and DB100K (bot- tom), where r \u22121 means the inverse of relation r.", "labels": [], "entities": [{"text": "Approximate", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9831051230430603}, {"text": "WN18", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9796422719955444}, {"text": "FB15K", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.6658310890197754}, {"text": "DB100K", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.8842515349388123}]}, {"text": " Table 2: Statistics of datasets, where the columns  respectively indicate the number of entities, rela- tions, training/validation/test triples, and approxi- mate entailments.", "labels": [], "entities": []}, {"text": " Table 3: Link prediction results on the test sets of WN18 and FB15K. Results for TransE and DistMult  are taken from (Trouillon et al., 2016). Results for the other baselines are taken from the original papers.  Missing scores not reported in the literature are indicated by \"-\". Best scores are highlighted in bold,  and \" * \" indicates statistically significant improvements over ComplEx.", "labels": [], "entities": [{"text": "WN18", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9626467823982239}, {"text": "FB15K", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9047704339027405}, {"text": "TransE", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.9132770895957947}]}, {"text": " Table 4: Link prediction results on the test set of  DB100K, with best scores highlighted in bold, sta- tistically significant improvements marked by \" * \".", "labels": [], "entities": [{"text": "Link", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9401988983154297}, {"text": "test set of  DB100K", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.7513040006160736}]}]}