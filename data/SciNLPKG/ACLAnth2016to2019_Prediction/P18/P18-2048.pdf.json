{"title": [{"text": "Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation", "labels": [], "entities": [{"text": "Dynamic Sentence Sampling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6265228688716888}, {"text": "Neural Machine Translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6863564054171244}]}], "abstractContent": [{"text": "Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.7998317082722982}]}, {"text": "In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time.", "labels": [], "entities": []}, {"text": "Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 100, "end_pos": 112, "type": "TASK", "confidence": 0.7963054180145264}]}, {"text": "In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations.", "labels": [], "entities": []}, {"text": "Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights.", "labels": [], "entities": []}, {"text": "Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.", "labels": [], "entities": [{"text": "NIST", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.9756340384483337}, {"text": "WMT English-to-German tasks", "start_pos": 63, "end_pos": 90, "type": "DATASET", "confidence": 0.7952469189961752}, {"text": "NMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9591518640518188}, {"text": "NMT", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.9556829333305359}]}], "introductionContent": [{"text": "Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.8062282701333364}]}, {"text": "However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure.", "labels": [], "entities": [{"text": "phrasebased statistical machine translation (PBSMT)", "start_pos": 58, "end_pos": 109, "type": "TASK", "confidence": 0.69553690297263}]}, {"text": "To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model ( and implementing parallelism in the data or in the model (.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9276174008846283}]}, {"text": "Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations.", "labels": [], "entities": []}, {"text": "Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9523828625679016}]}, {"text": "Specifically, empirically investigated curriculum learning based on the sentence length and word rank.", "labels": [], "entities": []}, {"text": "proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7582768499851227}, {"text": "NMT", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.9055637121200562}]}, {"text": "They also proposed a sentence weighting method with dynamic weight adjustment ().", "labels": [], "entities": [{"text": "sentence weighting", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7848405539989471}]}, {"text": "used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner.", "labels": [], "entities": []}, {"text": "All of these criteria ( are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.7796294689178467}]}, {"text": "adopted the sentence-level training cost as a dynamic criterion to gradually fine-tune the NMT training.", "labels": [], "entities": []}, {"text": "This approach was developed based on the idea that the training cost is a useful measure to determine the translation quality of a sentence.", "labels": [], "entities": [{"text": "translation quality of a sentence", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.786965835094452}]}, {"text": "However, some of the sentences that can be potentially improved by training maybe deleted using this method.", "labels": [], "entities": []}, {"text": "In addition, all of the above works primarily focused on NMT translation performance, instead of training efficiency.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.948852926492691}]}, {"text": "In this study, we propose a method of dynamic sentence sampling (DSS) to improve the NMT training efficiency.", "labels": [], "entities": [{"text": "dynamic sentence sampling (DSS)", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.7750790069500605}, {"text": "NMT training", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.8623723089694977}]}, {"text": "First, the differences between the training costs of two iterations, which is a measure of whether the translation quality of a sentence can be potentially improved, is measured to be the criterion.", "labels": [], "entities": []}, {"text": "We further proposed two sentence resampling strategies, i.e., weighted sampling and review mechanism to help NMT focus on the not well-learned sentences as well as remember the knowledge from the well-learned sentences.", "labels": [], "entities": [{"text": "sentence resampling", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7131549715995789}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce the dynamic sentence sampling method.", "labels": [], "entities": [{"text": "dynamic sentence sampling", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6175220012664795}]}, {"text": "Experiments are described and analyzed in Section 3.", "labels": [], "entities": []}, {"text": "We discussed some other effects of the proposed methods in Section 4.", "labels": [], "entities": []}, {"text": "We conclude our paper in the last section.", "labels": [], "entities": []}, {"text": "2 Dynamic Sentence Sampling (DSS)", "labels": [], "entities": [{"text": "Dynamic Sentence Sampling (DSS", "start_pos": 2, "end_pos": 32, "type": "TASK", "confidence": 0.6629083216190338}]}], "datasetContent": [{"text": "The proposed methods were applied to perform 1) the NIST Chinese (ZH) to English (EN) translation task that contained a training dataset of 1.42 million bilingual sentence pairs from LDC 1 Zhang et al.", "labels": [], "entities": [{"text": "NIST Chinese (ZH) to English (EN) translation task", "start_pos": 52, "end_pos": 102, "type": "TASK", "confidence": 0.8210377196470896}]}, {"text": "(2017) adopted 80% as the selection threshold and we follow their settings for fair comparison.", "labels": [], "entities": [{"text": "selection threshold", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9219465255737305}]}, {"text": "Due to limited space, we will empirically investigate the effect of the thresholds as our future work.", "labels": [], "entities": []}, {"text": "For those 20% sentences who are not selected, their criterion i+1 x,y = criterion i x,y . corpora . The NIST02 and NIST03-08 datasets were used as the development and test datasets, respectively.", "labels": [], "entities": [{"text": "NIST02", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.9854062795639038}, {"text": "NIST03-08 datasets", "start_pos": 115, "end_pos": 133, "type": "DATASET", "confidence": 0.9719370603561401}]}, {"text": "2) the WMT English to German (DE) translation task for which 4.43 million bilingual sentence pairs from the WMT-14 dataset 4 was used as the training data.", "labels": [], "entities": [{"text": "WMT English to German (DE) translation task", "start_pos": 7, "end_pos": 50, "type": "TASK", "confidence": 0.6186194154951308}, {"text": "WMT-14 dataset", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.9659788608551025}]}, {"text": "The newstest2012 and newstest2013-2015 datasets were used as development and test datasets, respectively.", "labels": [], "entities": [{"text": "newstest2013-2015 datasets", "start_pos": 21, "end_pos": 47, "type": "DATASET", "confidence": 0.9347957968711853}]}], "tableCaptions": [{"text": " Table 1: Results from the NIST ZH-to-EN translation task.", "labels": [], "entities": [{"text": "NIST ZH-to-EN translation task", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.7426280677318573}]}, {"text": " Table 2: Results from the WMT EN-to-DE translation task.", "labels": [], "entities": [{"text": "WMT EN-to-DE translation task", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.874568298459053}]}]}