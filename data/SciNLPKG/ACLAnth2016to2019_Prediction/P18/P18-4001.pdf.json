{"title": [{"text": "Platforms for Non-Speakers Annotating Names in Any Language", "labels": [], "entities": [{"text": "Annotating Names in Any Language", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7566894888877869}]}], "abstractContent": [{"text": "We demonstrate two annotation platforms that allow an English speaker to annotate names for any language without knowing the language.", "labels": [], "entities": []}, {"text": "These platforms provided high-quality \"silver standard\" annotations for low-resource language name taggers (Zhang et al., 2017) that achieved state-of-the-art performance on two surprise languages (Oromo and Tigrinya) at LoreHLT2017 1 and ten languages at TAC-KBP EDL2017 (Ji et al., 2017).", "labels": [], "entities": [{"text": "TAC-KBP EDL2017", "start_pos": 256, "end_pos": 271, "type": "DATASET", "confidence": 0.8036672174930573}]}, {"text": "We discuss strengths and limitations and compare other methods of creating silver-and gold-standard annotations using native speakers.", "labels": [], "entities": []}, {"text": "We will make our tools publicly available for research use.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although researchers have been working on unsupervised and semi-supervised approaches to alleviate the demand for training data, most state-ofthe-art models for name tagging, especially neural network-based models ) still rely on a large amount of training data to achieve good performance.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.783625602722168}]}, {"text": "When applied to low-resource languages, these models suffer from data sparsity.", "labels": [], "entities": []}, {"text": "Traditionally, native speakers of a language have been asked to annotate a corpus in that language.", "labels": [], "entities": []}, {"text": "This approach is uneconomical for several reasons.", "labels": [], "entities": []}, {"text": "First, for some languages We thank Kevin Blissett and Tongtao Zhang from RPI for their contributions to the annotations used for the experiments.", "labels": [], "entities": [{"text": "RPI", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.9119515419006348}]}, {"text": "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contracts No. HR0011-15-C-0115 and No. HR0011-16-C-0102.", "labels": [], "entities": []}, {"text": "The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.", "labels": [], "entities": []}, {"text": "1 https://www.nist.gov/itl/iad/mig/lorehlt-evaluations with extremely low resources, it's not easy to access native speakers for annotation.", "labels": [], "entities": []}, {"text": "For example, Chechen is only spoken by 1.4 million people and Rejiang is spoken by 200,000 people.", "labels": [], "entities": []}, {"text": "Second, it is costly in both time and money to write an annotation guideline fora low-resource language and to train native speakers (who are usually not linguists) to learn the guidelines and qualify for annotation tasks.", "labels": [], "entities": []}, {"text": "Third, we observed poor annotation quality and low inter-annotator agreement among newly trained native speakers in spite of high language proficiency.", "labels": [], "entities": [{"text": "agreement", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.8000860810279846}]}, {"text": "For example, under DARPA LORELEI, 2 the performance of two native Uighur speakers on name tagging was only 69% and 73% F 1 -score respectively.", "labels": [], "entities": [{"text": "LORELEI", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.6889726519584656}, {"text": "name tagging", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.7831064164638519}, {"text": "F 1 -score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.986370787024498}]}, {"text": "Previous efforts to generate \"silver-standard\" annotations used Web search, parallel data(), Wikipedia markups (, and crowdsourcing (.", "labels": [], "entities": []}, {"text": "Annotations produced by these methods are usually noisy and specific to a particular writing style (e.g., Wikipedia articles), yielding unsatisfactory results and poor portability.", "labels": [], "entities": []}, {"text": "It is even more expensive to teach Englishspeaking annotators new languages.", "labels": [], "entities": []}, {"text": "But can we annotate names in a language we don't know?", "labels": [], "entities": []}, {"text": "Let's examine a Somali sentence: \"Sida uu saxaafadda u sheegay Dr Jaamac Warsame Cali oo fadhigiisu yahay magaalada Baardheere hadda waxaa shuban caloolaha la yaalla xarumaha caafimaadka 15-cunug oo lagu arkay fuuq bax joogto ah, wuxuu xusay dhakhtarku in ay wadaan dadaallo ay wax kaga qabanayaan xaaladdan\" Without knowing anything about Somali, an English speaker can guess that \"Jaamac Warsame Cali\" is a person name because it's capitalized, the word on its left, \"Dr,\" is similar to \"Dr.\" in English, and its spelling looks similar to the English \"Jamac Warsame Ali.\"", "labels": [], "entities": []}, {"text": "Similarly, we can identify \"Baardheere\" as a location name if we know that \"magaalada\" in English is \"town\" from a common word dictionary, and its spelling is similar to the English name \"Bardhere.\"", "labels": [], "entities": [{"text": "Bardhere", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9505854845046997}]}, {"text": "What about languages that are not written in Roman (Latin) script?", "labels": [], "entities": []}, {"text": "Fortunately language universal romanization () or transliteration 3 tools are available for most living languages.", "labels": [], "entities": [{"text": "language universal romanization", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.6315957804520925}]}, {"text": "For example, the following is a Tigrinya sentence and its romanized form: An English speaker can guess that \"\u12d3\u1265\u12f0\u120d\u1348\u1273\u1215 \u12a0\u120d-\u1232\u1232\" is a person name because its romanized form \"aabedalefataahhe 'ale-sisi\" sounds similar to the English name \"Abdel-Fattah el-Sissi,\" and the romanized form of the word on its left, \"\u1355\u1228\u12dd\u12f0\u1295\u1275,\" (perazedanete) sounds similar to the English word \"president.\"", "labels": [], "entities": []}, {"text": "Moreover, annotators (may) acquire languagespecific patterns and rules gradually during annotation; e.g., a capitalized word preceded by \"magaalaa\" is likely to be a city name in Oromo, such as \"magaalaa Adaamaa\" (Adama city).", "labels": [], "entities": []}, {"text": "Synchronizing such knowledge among annotators both improves annotation quality and boosts productivity.", "labels": [], "entities": []}, {"text": "The Information Sciences Institute (ISI) developed a \"Chinese Room\" interface 4 to allow a nonnative speaker to translate foreign language text into English, based on a small set of parallel sentences that include overlapped words.", "labels": [], "entities": []}, {"text": "Inspired by this, RPI and JHU developed two collaborative annotation platforms that exploit linguistic intuitions and resources to allow non-native speakers to perform name tagging efficiently and effectively.", "labels": [], "entities": [{"text": "JHU", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9034591317176819}, {"text": "name tagging", "start_pos": 168, "end_pos": 180, "type": "TASK", "confidence": 0.7312838435173035}]}, {"text": "Presentation of text in a familiar alphabet makes it easier to see similarities and differences between text segments, to learn aspects of the target language morphology, and to remember sequences previously seen.", "labels": [], "entities": []}, {"text": "Because named entities often are transliterated into another language, access to the sound of the words is particularly important for annotating names.", "labels": [], "entities": []}, {"text": "Sounds can be exposed either through a formal expression language such as IPA, or by transliteration into the appropriate letters of the annotator's native language.", "labels": [], "entities": []}, {"text": "The better the annotator understands the full meaning of the text being annotated, the easier it will be both to identify which named entities are likely to be mentioned in the text and what the boundaries of those mentions are.", "labels": [], "entities": []}, {"text": "Meaning can be conveyed in a variety of ways: dictionary lookup to provide fixed meanings for individual words and phrases; description of the position of a word or phrase in a semantic space (e.g., Brown clusters or embedding space) to define words that are not found in a dictionary; and full sentence translation.", "labels": [], "entities": [{"text": "full sentence translation", "start_pos": 290, "end_pos": 315, "type": "TASK", "confidence": 0.6906482179959615}]}, {"text": "Understanding how a word is used in a given instance can benefit greatly from understanding how that word is used broadly, either across the document being annotated, or across a larger corpus of monolingual text.", "labels": [], "entities": []}, {"text": "For example, knowing that a word frequently appears adjacent to a known person name suggests it might be a surname, even if the adjacent word in the current context is not known to be a name.", "labels": [], "entities": []}, {"text": "Knowledge of some of the entities, relations, and events referred to in the text allows the annotator to form a stronger model of what the text as a whole might be saying (e.g., a document about disease outbreak is likely to include organizations like Red Cross), leading to better judgments about components of the text.", "labels": [], "entities": []}, {"text": "Annotations previously applied to a use of a word form a strong prior on how anew instance of the word should be tagged.", "labels": [], "entities": []}, {"text": "While some of this knowledge is held by the annotator, it is difficult to maintain such knowledge overtime.", "labels": [], "entities": []}, {"text": "Programmatic support for capturing prior conclusions (linguistic patterns, word translations, possible annotations fora mention along with their frequency) and making them available to the annotator is essential for large collaborative annotation efforts.", "labels": [], "entities": [{"text": "word translations", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.6875424981117249}]}, {"text": "Disagreements among annotators can indicate cases that require closer examination.", "labels": [], "entities": []}, {"text": "An adjudication interface is beneficial to enhance precision (see Section 4).", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9992879033088684}]}, {"text": "The next section discusses how we embody these requirements in two annotation platforms.", "labels": [], "entities": []}], "datasetContent": [{"text": "We asked ten non-speakers to annotate names using our annotation platforms on documents in various low-resource languages released by the DARPA LORELEI program and the NIST TAC-KBP2017 EDL Pilot ( ).", "labels": [], "entities": [{"text": "DARPA", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.645226001739502}, {"text": "LORELEI", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.5750993490219116}, {"text": "NIST TAC-KBP2017 EDL Pilot", "start_pos": 168, "end_pos": 194, "type": "DATASET", "confidence": 0.9085778146982193}]}, {"text": "The genres of these documents include newswire, discussion forum and tweets.", "labels": [], "entities": []}, {"text": "Using non-speaker annotations as \"silver-standard\" training data, we trained).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data Statistics and Performance on  KBP2017 EDL Pilot", "labels": [], "entities": [{"text": "KBP2017 EDL", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.8995205163955688}]}, {"text": " Table 3: # Sentences in Oromo and Tigrinya Data.", "labels": [], "entities": [{"text": "Sentences in Oromo and Tigrinya", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.7495061993598938}]}, {"text": " Table 4: Comparison of Silver Standard Creation  Methods (F-score %).", "labels": [], "entities": [{"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.991640567779541}]}]}