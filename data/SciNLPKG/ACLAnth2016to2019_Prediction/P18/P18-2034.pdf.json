{"title": [{"text": "Transfer Learning for Context-Aware Question Matching in Information-seeking Conversations in E-commerce", "labels": [], "entities": [{"text": "Transfer Learning for Context-Aware Question Matching", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8604418238004049}]}], "abstractContent": [{"text": "Building multi-turn information-seeking conversation systems is an important and challenging research topic.", "labels": [], "entities": []}, {"text": "Although several advanced neural text matching models have been proposed for this task, they are generally not efficient for industrial applications.", "labels": [], "entities": [{"text": "neural text matching", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7384816805521647}]}, {"text": "Furthermore, they rely on a large amount of labeled data, which may not be available in real-world applications.", "labels": [], "entities": []}, {"text": "To alleviate these problems, we study transfer learning for multi-turn information seeking conversations in this paper.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9467540085315704}, {"text": "multi-turn information seeking conversations", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.6842934191226959}]}, {"text": "We first propose an efficient and effective multi-turn conversation model based on convo-lutional neural networks.", "labels": [], "entities": []}, {"text": "After that, we extend our model to adapt the knowledge learned from a resource-rich domain to enhance the performance.", "labels": [], "entities": []}, {"text": "Finally, we deployed our model in an industrial chatbot called AliMe Assist 1 and observed a significant improvement over the existing on-line model.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the popularity of online shopping, there is an increasing number of customers seeking information regarding their concerned items.", "labels": [], "entities": []}, {"text": "To efficiently handle customer questions, a common approach is to build a conversational customer service system ().", "labels": [], "entities": []}, {"text": "In the E-commerce environment, the informationseeking conversation system can serve millions of customer questions per day.", "labels": [], "entities": []}, {"text": "According to the statistics from areal e-commerce website ( , the majority of customer questions (nearly 90%) are business-related or seeking information about logistics, coupons etc.", "labels": [], "entities": []}, {"text": "Among these conversation sessions, 75% of them are more than one turn 2 . Hence it is important to handle multiturn conversations or context information in these conversation systems.", "labels": [], "entities": []}, {"text": "Recent researches in this area have focused on deep learning and reinforcement learning.", "labels": [], "entities": []}, {"text": "One of these methods is Sequential Matching Network(, which matches a response with each utterance in the context at multiple levels of granularity and leads to state-of-the-art performance on two multi-turn conversation corpora.", "labels": [], "entities": [{"text": "Sequential Matching Network", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8609248201052347}]}, {"text": "However, such methods suffer from at least two problems: they may not be efficient enough for industrial applications, and they rely on a large amount of labeled data which may not be available in reality.", "labels": [], "entities": []}, {"text": "To address the problem of efficiency, we made three major modifications to SMN to boost the efficiency of the model while preserving its effectiveness.", "labels": [], "entities": [{"text": "SMN", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.8151644468307495}]}, {"text": "First, we remove the RNN layers of inputs from the model; Second, SMN uses a Sentence Interaction based (SI-based) Pyramid model ( ) to model each utterance and response pair.", "labels": [], "entities": [{"text": "SMN", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9842877388000488}]}, {"text": "In practice, a Sentence Encoding based (SE-based) model like BCNN) is complementary to the SI-based model.", "labels": [], "entities": [{"text": "BCNN", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.795963704586029}]}, {"text": "Therefore, we extend the component to incorporate an SE-based BCNN model, resulting in a hybrid CNN (hCNN) (; Third, instead of using a RNN to model the output representations, we consider a CNN model followed by a fully-connected layer to further boost the efficiency of our model.", "labels": [], "entities": []}, {"text": "As shown in our experiments, our final model yields comparable results but with higher efficiency than SMN.", "labels": [], "entities": [{"text": "SMN", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.8267801403999329}]}, {"text": "To address the second problem of insufficient labeled data, we study transfer learning (TL) to utilize a source domain with adequate labeling to help the target domain.", "labels": [], "entities": [{"text": "transfer learning (TL)", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7533106684684754}]}, {"text": "A typical TL approach is to use a shared NN () and domain-specific NNs to derive shared and domain-specific features respectively.", "labels": [], "entities": [{"text": "TL", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9798088669776917}]}, {"text": "Recent studies ( consider adversarial networks to learn more robust shared features across domains.", "labels": [], "entities": []}, {"text": "Inspired by these studies, we extended our method with a Transfer Learning module to leverage information from a resource-rich domain.", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.9255857765674591}]}, {"text": "Similarly, our TL module consists of a shared NN and two domainspecific NNs for source and target domains.", "labels": [], "entities": []}, {"text": "The output of the shared NN is further linked to an adversarial network as used in ( to help learn domain invariant features.", "labels": [], "entities": []}, {"text": "Meanwhile, we also use domain discriminators on both source and target features derived by domainspecific NNs to help learn domain-specific features.", "labels": [], "entities": []}, {"text": "Experiments show that our TL method can further improve the model performance on a target domain with limited data.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our work is the first to study transfer learning for context-aware question matching in conversations.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8753198981285095}, {"text": "context-aware question matching in conversations", "start_pos": 83, "end_pos": 131, "type": "TASK", "confidence": 0.7423528730869293}]}, {"text": "Experiments on both benchmark and commercial data sets show that our proposed model outperforms several baselines including the state-of-the-art SMN model.", "labels": [], "entities": [{"text": "SMN", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9707189798355103}]}, {"text": "We have also deployed our model in an industrial bot called AliMe Assist and observed a significant improvement over the existing online model.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the efficiency and effectiveness of our base model, the transferability of the model, and the online evaluation in an industrial chatbot.", "labels": [], "entities": []}, {"text": ", in which numbers, urls and paths are replaced by special placeholders.", "labels": [], "entities": []}, {"text": "It is also used in several previous related works (.", "labels": [], "entities": []}, {"text": "It consists of 1 million context-response pairs for training, 0.5 million pairs for validation and 0.5 million pairs for testing.", "labels": [], "entities": []}, {"text": "AliMe Data: We collect the chat logs between customers and a chatbot called AliMe from \"2017-10-01\" to \"2017-10-20\" in Alibaba . The chatbot is built based on a question-to-question matching system ( , where for each query, it finds the most similar candidate question in a QA database and return its answer as the reply.", "labels": [], "entities": [{"text": "AliMe Data", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8954827189445496}, {"text": "Alibaba", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9730230569839478}]}, {"text": "It indexes all the questions in our QA database using Lucence . For each given query, it uses TF-IDF ranking algorithm to callback candidates.", "labels": [], "entities": [{"text": "QA database", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.7858648598194122}, {"text": "Lucence", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.8640483021736145}, {"text": "TF-IDF ranking algorithm", "start_pos": 94, "end_pos": 118, "type": "METRIC", "confidence": 0.8341337243715922}]}, {"text": "To form our data set, we concatenated utterances within three turns to form a query, and used the chatbot system to callback top 15 most similar candidate questions as candidate \"responses\".", "labels": [], "entities": []}, {"text": "We then asked a business analyst to annotate the candidate responses, where a \"response\" is labeled as positive if it matches the query, otherwise negative.", "labels": [], "entities": []}, {"text": "In all, we have annotated 63,000 context-response pairs.", "labels": [], "entities": []}, {"text": "This dataset is used as our Target data.", "labels": [], "entities": []}, {"text": "Furthermore, we build our Source data as follows.", "labels": [], "entities": []}, {"text": "In the AliMe chatbot, if the confidence score of answering a given user query is low, i.e. the matching score is below a given threshold 8 , we prompt top three related questions for users to choose.", "labels": [], "entities": [{"text": "AliMe chatbot", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.8971064686775208}]}, {"text": "We collected the user click logs as our source data, where we treat the clicked question as positive and the others as negative.", "labels": [], "entities": []}, {"text": "We collected 510,000 query-question pairs from the click logs in total as the source.", "labels": [], "entities": []}, {"text": "For the source and target datasets, we use 80% for training, 10% for validation, and 10% for testing.", "labels": [], "entities": []}, {"text": "Compared Methods: We compared our multiturn model (MT-hCNN) with two CNN based models ARC-I and ARC-II (, and several advanced neural matching models: MV-LSTM (  Settings: We use the same parameter settings of hCNN in ().", "labels": [], "entities": []}, {"text": "For the CNN 3 in our model, we set window size of convolution layer as 2, ReLU as the activation function, and the stride Around 85% of conversations are within 3 turns.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9957439303398132}, {"text": "stride", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9704111814498901}]}, {"text": "A \"response\" here is a question in our system.", "labels": [], "entities": []}, {"text": "The threshold is determined by a business analyst The results are based on the TensorFlow code from authors, and with no oversampling of negative training data. of max-pooling layer as 2.", "labels": [], "entities": []}, {"text": "The hidden node size of the Fully-Connected layer is set as 128.", "labels": [], "entities": []}, {"text": "AdaDelta is used to train our model with an initial learning rate of 0.08.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9181632995605469}]}, {"text": "We use MAP, Recall@5, Recall@2, and Recall@1 as evaluation metrics.", "labels": [], "entities": [{"text": "MAP", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.6234443187713623}]}, {"text": "We set \u03bb 1 = \u03bb 2 = \u03bb 3 = 0.05, and \u03bb 4 = 0.005.", "labels": [], "entities": []}, {"text": "We deployed our model online in AliMe Assist Bot.", "labels": [], "entities": [{"text": "AliMe Assist Bot.", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9387438893318176}]}, {"text": "For each query, the bot uses the TF-IDF model in Lucene to return a set of candidates, then uses our model to rerank all the candidates and returns the top.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.7826759815216064}]}, {"text": "We set the candidate size as 15 and context length as 3.", "labels": [], "entities": []}, {"text": "To accelerate the computation, we bundle the 15 candidates into a mini-batch to feed into our model.", "labels": [], "entities": []}, {"text": "We compare our method with the online model -a degenerated version of our model that only uses the current query to retrieve candidate, i.e. context length is 1.", "labels": [], "entities": []}, {"text": "We have run 3-day A/B testing on the Click-Through-Rate (CTR) of the models.", "labels": [], "entities": [{"text": "A/B", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8882186015446981}, {"text": "Click-Through-Rate (CTR)", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.8272166699171066}]}, {"text": "As shown in, our method consistently outperforms the online model, yielding 5% \u223c 10% improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of base models on Ubuntu Dialog Corpus (UDC) and an E-commerce data (AliMe).", "labels": [], "entities": [{"text": "Ubuntu Dialog Corpus (UDC)", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.8636639018853506}]}, {"text": " Table 2: Transferablity of our model.", "labels": [], "entities": [{"text": "Transferablity", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9879606366157532}]}, {"text": " Table 3: Comparison with the online model.", "labels": [], "entities": []}]}