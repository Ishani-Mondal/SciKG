{"title": [{"text": "The Influence of Context on Sentence Acceptability Judgements", "labels": [], "entities": [{"text": "Sentence Acceptability Judgements", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7891012827555338}]}], "abstractContent": [{"text": "We investigate the influence that document context exerts on human acceptability judgements for English sentences, via two sets of experiments.", "labels": [], "entities": []}, {"text": "The first compares ratings for sentences presented on their own with ratings for the same set of sentences given in their document contexts.", "labels": [], "entities": []}, {"text": "The second assesses the accuracy with which two types of neural models-one that incorporates context during training and one that does not-predict these judgements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9996241331100464}]}, {"text": "Our results indicate that: (1) context improves acceptability ratings for ill-formed sentences, but also reduces them for well-formed sentences; and (2) context helps unsupervised systems to model acceptability.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence acceptability is defined as the extent to which a sentence is well formed or natural to native speakers of a language.", "labels": [], "entities": [{"text": "Sentence acceptability", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9147628545761108}]}, {"text": "It encompasses semantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory limitation.", "labels": [], "entities": []}, {"text": "Grammaticality, by contrast, is the syntactic well-formedness of a sentence.", "labels": [], "entities": []}, {"text": "Grammaticality as characterised by formal linguists is a theoretical concept that is difficult to elicit from non-expert assessors.", "labels": [], "entities": []}, {"text": "In the research presented here we are interested in predicting acceptability judgements.", "labels": [], "entities": [{"text": "predicting acceptability judgements", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.8735558191935221}]}, {"text": "Lau et al.) present unsupervised probabilistic methods to predict sentence acceptability, where sentences were judged independently of context.", "labels": [], "entities": []}, {"text": "In this paper we extend this Annotated data (with acceptability ratings) is available at: https://github.com/GU-CLASP/BLL2018.", "labels": [], "entities": [{"text": "Annotated data", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.8277824521064758}, {"text": "BLL2018", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.8275254368782043}]}, {"text": "See fora detailed discussion of the relationship between acceptability and grammaticality.", "labels": [], "entities": []}, {"text": "They provide motivation for measuring acceptability rather than grammaticality in their crowd source surveys and modelling experiments.", "labels": [], "entities": []}, {"text": "research to investigate the impact of context on human acceptability judgements, where context is defined as the full document environment surrounding a sentence.", "labels": [], "entities": []}, {"text": "We also test the accuracy of more sophisticated language models -one which incorporates document context during trainingto predict human acceptability judgements.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9989423155784607}]}, {"text": "We believe that understanding how context influences acceptability is crucial to success in modelling human acceptability judgements.", "labels": [], "entities": []}, {"text": "It has implications for tasks such as style/coherence assessment and language generation.", "labels": [], "entities": [{"text": "style/coherence assessment", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6119823008775711}, {"text": "language generation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7623960673809052}]}, {"text": "Showing a strong correlation between unsupervised language model sentence probability and acceptability supports the view that linguistic knowledge can be represented as a probabilistic system.", "labels": [], "entities": []}, {"text": "This result addresses foundational questions concerning the nature of grammatical knowledge (.", "labels": [], "entities": []}, {"text": "Our work is guided by 3 hypotheses: H 1 : Document context boosts sentence acceptability judgements.", "labels": [], "entities": [{"text": "Document context", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.9063299298286438}]}, {"text": "H 2 : Document context helps language models to model acceptability.", "labels": [], "entities": []}, {"text": "H 3 : A language model predicts acceptability more accurately when it is tested on sentences within document context than when it is tested on the sentences alone.", "labels": [], "entities": []}, {"text": "We sample sentences and their document contexts from English Wikipedia articles.", "labels": [], "entities": []}, {"text": "We perform round-trip machine translation to generate sentences of varying degrees of well-formedness and ask crowdsourced workers to judge the acceptability of these sentences, presenting the sentences with and without their document environments.", "labels": [], "entities": []}, {"text": "We describe this experiment and address H 1 in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we experiment with two types of language models to predict acceptability: a standard language model and a topically-driven model.", "labels": [], "entities": []}, {"text": "The latter extends the language model by incorporating document context as a conditioning variable.", "labels": [], "entities": []}, {"text": "The model comparison allows us to understand the impact of incorporating context during training for acceptability prediction.", "labels": [], "entities": [{"text": "acceptability prediction", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.9169694781303406}]}, {"text": "We also experiment with adding context as input attest time for both models.", "labels": [], "entities": []}, {"text": "These experiments collectively address H 2 , by investigating the impact of using context during training and testing for modelling acceptability.", "labels": [], "entities": [{"text": "H 2", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6539064049720764}]}, {"text": "We evaluate the models against crowd-sourced annotated sentences judged both in context and out of context.", "labels": [], "entities": []}, {"text": "This tests H 3 . In Section 4 we briefly consider related work.", "labels": [], "entities": []}, {"text": "We indicate the issues to be addressed in future research and summarise our conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: A sample of sentences with their without-context (h \u2212 ) and with-context (h + ) ratings. The  \"Language\" column denotes the intermediate translation language. The original English sentence is  marked with \"-\".", "labels": [], "entities": []}, {"text": " Table 3: Pearson's r of acceptability measures and  human ratings. \"Rtg\" = \"Rating\", \"LP\" = Log- Prob, \"Mean\" = Mean LP, \"NrmD\" = Norm LP  (Div) and \"NrmS\" = Norm LP (Sub). Boldface  indicates optimal performance in each row.", "labels": [], "entities": [{"text": "Rtg", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9544626474380493}]}]}