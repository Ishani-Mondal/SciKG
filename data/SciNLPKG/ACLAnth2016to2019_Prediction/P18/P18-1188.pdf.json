{"title": [{"text": "Document Modeling with External Attention for Sentence Extraction", "labels": [], "entities": [{"text": "Document Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.881128340959549}, {"text": "Sentence Extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9188120365142822}]}], "abstractContent": [{"text": "Document modeling is essential to a variety of natural language understanding tasks.", "labels": [], "entities": [{"text": "Document modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9106732308864594}, {"text": "natural language understanding", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6448536018530527}]}, {"text": "We propose to use external information to improve document modeling for problems that can be framed as sentence extraction.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7130128294229507}, {"text": "sentence extraction", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7315558195114136}]}, {"text": "We develop a framework composed of a hierarchical document encoder and an attention-based ex-tractor with attention over external information.", "labels": [], "entities": []}, {"text": "We evaluate our model on extrac-tive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question).", "labels": [], "entities": [{"text": "extrac-tive document summarization", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.6657782196998596}, {"text": "answer selection", "start_pos": 145, "end_pos": 161, "type": "TASK", "confidence": 0.809555172920227}]}, {"text": "We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.", "labels": [], "entities": [{"text": "CNN document summarization", "start_pos": 116, "end_pos": 142, "type": "TASK", "confidence": 0.6980626384417216}, {"text": "answer selection", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.906004399061203}, {"text": "NewsQA", "start_pos": 217, "end_pos": 223, "type": "DATASET", "confidence": 0.8648605942726135}]}], "introductionContent": [{"text": "Recurrent neural networks have become one of the most widely used models in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.8009157677491506}]}, {"text": "A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; and Gated Recurrent Unit networks (GRU; have been designed to model text capturing long-term dependencies in problems such as language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 202, "end_pos": 219, "type": "TASK", "confidence": 0.6812457144260406}]}, {"text": "However, document modeling, a key to many natural language * The first three authors made equal contributions to this paper.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.8288766145706177}]}, {"text": "The work was done when the second author was visiting Edinburgh.", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.9734562039375305}]}, {"text": "Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information.", "labels": [], "entities": []}, {"text": "understanding tasks, is still an open challenge.", "labels": [], "entities": []}, {"text": "Recently, some neural network architectures were proposed to capture large context for modeling text. and proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity.", "labels": [], "entities": [{"text": "sentence-level modeling", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.7054855078458786}]}, {"text": "further proposed a contextual language model that considers information at interdocument level.", "labels": [], "entities": []}, {"text": "It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization and machine reading comprehension (.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.5918795764446259}]}, {"text": "In this paper, we formalize the use of external information to further guide document modeling for end goals.", "labels": [], "entities": []}, {"text": "We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with \"external attention.\"", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7820424735546112}]}, {"text": "Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor.", "labels": [], "entities": [{"text": "hierarchical attention-based sentence extractor", "start_pos": 86, "end_pos": 133, "type": "TASK", "confidence": 0.6550846919417381}]}, {"text": "Our hierarchical document encoder resembles the architectures proposed by and in that it derives the document meaning representation from its sentences and their constituent words.", "labels": [], "entities": []}, {"text": "Our novel sentence extractor combines this document meaning representation with an attention mechanism () over the external information to label sentences from the input document.", "labels": [], "entities": []}, {"text": "Our model explicitly biases the extractor with external cues and implicitly biases the encoder through training.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.738762155175209}]}, {"text": "These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document.", "labels": [], "entities": [{"text": "extractive document summarization", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.7664065559705099}, {"text": "answer selection", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.8275361061096191}]}, {"text": "Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query.", "labels": [], "entities": [{"text": "Extractive document summarization", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7290138403574625}]}, {"text": "For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.5527559220790863}]}, {"text": "For answer selection, we use word overlap features, such as the inverse sentence frequency) and the inverse document frequency (IDF) together with the query, all formulated as external cues.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9326016008853912}, {"text": "inverse document frequency (IDF)", "start_pos": 100, "end_pos": 132, "type": "METRIC", "confidence": 0.7949498196442922}]}, {"text": "Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7278636991977692}]}, {"text": "Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features), our model is the first to exploit such information in deep learning-based summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9897873401641846}]}, {"text": "We evaluate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9968835711479187}, {"text": "CNN news highlights dataset", "start_pos": 71, "end_pos": 98, "type": "DATASET", "confidence": 0.9590126872062683}]}, {"text": "Experimental results show that our summarizer, informed with title and image captions, consistently outperforms summarizers that do not use this information.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.949631929397583}]}, {"text": "We also conduct a human evaluation to judge which type of summary participants prefer.", "labels": [], "entities": []}, {"text": "Our results overwhelmingly show that human subjects find our summaries more informative and complete.", "labels": [], "entities": []}, {"text": "Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be \"read\" to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner.", "labels": [], "entities": []}, {"text": "Our model with ISF and IDF scores as external features achieves competitive results for answer selection.", "labels": [], "entities": [{"text": "IDF", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9925198554992676}, {"text": "answer selection", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.9409705400466919}]}, {"text": "Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA ( ) and NewsQA (, and it obtains comparable results to the state of the art for SQuAD ().", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 185, "end_pos": 191, "type": "DATASET", "confidence": 0.528374969959259}, {"text": "NewsQA", "start_pos": 200, "end_pos": 206, "type": "DATASET", "confidence": 0.9166013598442078}]}, {"text": "We also evaluate our approach on the MSMarco dataset ( and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.", "labels": [], "entities": [{"text": "MSMarco dataset", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9634815156459808}]}], "datasetContent": [{"text": "This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7682051956653595}]}, {"text": "In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.", "labels": [], "entities": [{"text": "document representation", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.7111220508813858}]}], "tableCaptions": [{"text": " Table 1: Ablation results on the validation set.  We report R1, R2, R3, R4, RL and their aver- age (Avg.). The first block of the table presents  LEAD and POINTERNET which do not use any ex- ternal information. LEAD is the baseline system  selecting first three sentences. POINTERNET is  the sentence extraction system of Cheng and La- pata. XNET is our model. The second and third  blocks of the table present different variants of  XNET. We experimented with three types of ex- ternal information: title (TITLE), image captions  (CAPTION) and the first sentence (FS) of the docu- ment. The bottom block of the table presents mod- els with more than one type of external informa- tion. The best performing model (highlighted in  boldface) is used on the test set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9827153086662292}, {"text": "aver- age (Avg.)", "start_pos": 90, "end_pos": 106, "type": "METRIC", "confidence": 0.7961876293023428}, {"text": "sentence extraction", "start_pos": 293, "end_pos": 312, "type": "TASK", "confidence": 0.7078998982906342}, {"text": "title (TITLE), image captions  (CAPTION)", "start_pos": 501, "end_pos": 541, "type": "METRIC", "confidence": 0.7341018855571747}, {"text": "FS", "start_pos": 566, "end_pos": 568, "type": "METRIC", "confidence": 0.9095831513404846}]}, {"text": " Table 2: Final results on the test set. POINTER- NET is the sentence extraction system of Cheng  and Lapata. XNET is our best model from Table  1. Best ROUGE score in each block and each col- umn is highlighted in boldface.", "labels": [], "entities": [{"text": "POINTER- NET", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.8600082596143087}, {"text": "sentence extraction", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7589941620826721}, {"text": "ROUGE score", "start_pos": 153, "end_pos": 164, "type": "METRIC", "confidence": 0.9766008257865906}]}, {"text": " Table 3: Human evaluations: Ranking of various  systems. Rank 1st is best and rank 4th, worst.  Numbers show the percentage of times a system  gets ranked at a certain position.", "labels": [], "entities": []}, {"text": " Table 4: Results (in percentage) for answer selection comparing our approaches (bottom part) to base- lines (top): AP-CNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (", "labels": [], "entities": [{"text": "answer selection", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8910804688930511}, {"text": "AP-CNN", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9257659912109375}, {"text": "ABCNN", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.9450096487998962}, {"text": "L.D.C", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.8218199014663696}]}]}