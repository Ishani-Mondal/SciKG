{"title": [{"text": "Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment", "labels": [], "entities": [{"text": "Multimodal Affective Analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7599854667981466}, {"text": "Word-Level Alignment", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.6363797783851624}]}], "abstractContent": [{"text": "Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still challenging because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities.", "labels": [], "entities": []}, {"text": "Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data.", "labels": [], "entities": [{"text": "classify utterance-level sentiment and emotion from text and audio", "start_pos": 116, "end_pos": 182, "type": "TASK", "confidence": 0.7705545822779337}]}, {"text": "Our introduced model outper-forms state-of-the-art approaches on published datasets, and we demonstrate that our model's synchronized attention over modalities offers visual interpretability.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the recent rapid advancements in social media technology, affective computing is now a popular task in human-computer interaction.", "labels": [], "entities": []}, {"text": "Sentiment analysis and emotion recognition, both of which require applying subjective human concepts for detection, can be treated as two affective computing subtasks on different levels (.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.935905247926712}, {"text": "emotion recognition", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7743629515171051}]}, {"text": "A variety of data sources, including voice, facial expression, gesture, and linguistic content have been employed in sentiment analysis and emotion recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.9701687097549438}, {"text": "emotion recognition", "start_pos": 140, "end_pos": 159, "type": "TASK", "confidence": 0.7088491320610046}]}, {"text": "In this paper, we focus on a multimodal structure to leverage the advantages of each data source.", "labels": [], "entities": []}, {"text": "Specifically, given an utterance, we consider the linguistic content and acoustic characteristics together to recognize the opinion or emotion.", "labels": [], "entities": []}, {"text": "Our work is important and useful * Equally Contribution because speech is the most basic and commonly used form of human expression.", "labels": [], "entities": []}, {"text": "A basic challenge in sentiment analysis and emotion recognition is filling the gap between extracted features and the actual affective states ( . The lack of high-level feature associations is a limitation of traditional approaches using low-level handcrafted features as representations (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9711634516716003}, {"text": "emotion recognition", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.750983327627182}]}, {"text": "Recently, deep learning structures such as CNNs and LSTMs have been used to extract high-level features from text and audio).", "labels": [], "entities": []}, {"text": "However, not all parts of the text and vocal signals contribute equally to the predictions.", "labels": [], "entities": []}, {"text": "A specific word may change the entire sentimental state of text; a different vocal delivery may indicate inverse emotions despite having the same linguistic content.", "labels": [], "entities": []}, {"text": "Recent approaches introduce attention mechanisms to focus the models on informative words) and attentive audio frames) for each individual modality.", "labels": [], "entities": []}, {"text": "However, to our knowledge, there is no common multimodal structure with attention for utterancelevel sentiment and emotion classification.", "labels": [], "entities": [{"text": "emotion classification", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7361748218536377}]}, {"text": "To address such issue, we design a deep hierarchical multimodal architecture with an attention mechanism to classify utterance-level sentiments and emotions.", "labels": [], "entities": []}, {"text": "It extracts high-level informative textual and acoustic features through individual bidirectional gated recurrent units (GRU) and uses a multi-level attention mechanism to select the informative features in both the text and audio module.", "labels": [], "entities": []}, {"text": "Another challenge is the fusion of cues from heterogeneous data.", "labels": [], "entities": []}, {"text": "Most previous works focused on combining multimodal information at a holistic level, such as integrating independent predictions of each modality via algebraic rules or fusing the extracted modality-specific features from entire utterances).", "labels": [], "entities": []}, {"text": "They extract word-level features in a text branch, but process audio at the frame-level or utterance-level.", "labels": [], "entities": []}, {"text": "These methods fail to properly learn the time-dependent interactions across modalities and restrict feature integration at timestamps due to the different time scales and formats of features of diverse modalities).", "labels": [], "entities": []}, {"text": "However, to determine human meaning, it is critical to consider both the linguistic content of the word and how it is uttered.", "labels": [], "entities": []}, {"text": "A loud pitch on different words may convey inverse emotions, such as the emphasis on \"hell\" for anger but indicating happy on \"great\".", "labels": [], "entities": []}, {"text": "Synchronized attentive information across text and audio would then intuitively help recognize the sentiments and emotions.", "labels": [], "entities": []}, {"text": "Therefore, we compute a forced alignment between text and audio for each word and propose three fusion approaches (horizontal, vertical, and fine-tuning attention fusion) to integrate both the feature representations and attention at the word-level.", "labels": [], "entities": []}, {"text": "We evaluated our model on four published sentiment and emotion datasets.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed architecture outperforms state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "Our methods also allow for attention visualization, which can be used for interpreting the internal attention distribution for both single-and multi-modal systems.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: (i) a hierarchical multimodal structure with attention mechanism to learn informative features and high-level associations from both text and audio; (ii) three wordlevel fusion strategies to combine features and learn correlations in a common timescale across different modalities; (iii) word-level attention visualization to help human interpretation.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: We list related work in section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed structure in detail.", "labels": [], "entities": []}, {"text": "We present the experiments in section 4 and provide the result analysis in section 5.", "labels": [], "entities": []}, {"text": "We discuss the limitations in section 6 and conclude with section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our model on four published datasets: two multimodal sentiment datasets (MOSI and YouTube) and two multimodal emotion recognition datasets (IEMOCAP and EmotiW).", "labels": [], "entities": [{"text": "MOSI", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.822821319103241}]}, {"text": "MOSI dataset is a multimodal sentiment intensity and subjectivity dataset consisting of 93 reviews with 2199 utterance segments (.", "labels": [], "entities": [{"text": "MOSI dataset", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9777377545833588}]}, {"text": "Each segment was labeled by five individual annotators between -3 (strong negative) to +3 (strong positive).", "labels": [], "entities": []}, {"text": "We used binary labels based on the sign of the annotations' average.", "labels": [], "entities": []}, {"text": "YouTube dataset is an English multimodal dataset that contains 262 positive, 212 negative, and 133 neutral utterance-level clips provided by).", "labels": [], "entities": [{"text": "YouTube dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9807478487491608}]}, {"text": "We only consider the positive and negative labels during our experiments.", "labels": [], "entities": []}, {"text": "IEMOCAP is a multimodal emotion dataset including visual, audio, and text data (.", "labels": [], "entities": [{"text": "IEMOCAP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5999146103858948}]}, {"text": "For each sentence, we used the label agreed on by the majority (at least two of the three annotators).", "labels": [], "entities": []}, {"text": "In this study, we evaluate both the 4-catgeory (happy+excited, sad, anger, and neutral) and 5-catgeory(happy+excited, sad, anger, neutral, and frustration) emotion classification problems.", "labels": [], "entities": []}, {"text": "The final dataset consists of 586 happy, 1005 excited, 1054 sad, 1076 anger, 1677 neutral, and 1806 frustration.", "labels": [], "entities": []}, {"text": "EmotiW 2 is an audio-visual multimodal utterance-level emotion recognition dataset consist of video clips.", "labels": [], "entities": [{"text": "multimodal utterance-level emotion recognition", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.6363888531923294}]}, {"text": "To keep the consistency with the IEMOCAP dataset, we used four emotion categories as the final dataset including 150 happy, 117 sad, 133 anger, and 144 neutral.", "labels": [], "entities": [{"text": "consistency", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9907497763633728}, {"text": "IEMOCAP dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.8373745083808899}]}, {"text": "We used IBM Watson 3 speech to text software to transcribe the audio data into text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of models. WA = weighted accuracy. UA = unweighted accuracy. * denotes that  we duplicated the method from cited research with the corresponding dataset in our experiment.", "labels": [], "entities": [{"text": "WA", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9939146637916565}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7967022657394409}, {"text": "UA", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9946231842041016}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.8011980056762695}]}, {"text": " Table 2: Accuracy (%) and F1 score on text only  (T), audio only (A), and multi-modality using FAF  (T+A).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995800852775574}, {"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9782181978225708}, {"text": "FAF", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.969184160232544}]}, {"text": " Table 3: Accuracy (%) and F1 score for general- ization testing.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996449947357178}, {"text": "F1 score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9825324416160583}]}]}