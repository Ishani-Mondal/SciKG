{"title": [{"text": "Incorporating Chinese Characters of Words for Lexical Sememe Prediction", "labels": [], "entities": [{"text": "Lexical Sememe Prediction", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6509112517038981}]}], "abstractContent": [{"text": "Sememes are minimum semantic units of concepts inhuman languages, such that each word sense is composed of one or multiple sememes.", "labels": [], "entities": []}, {"text": "Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks.", "labels": [], "entities": []}, {"text": "Recently, the lexical se-meme prediction task has been introduced.", "labels": [], "entities": [{"text": "lexical se-meme prediction task", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6823973879218102}]}, {"text": "It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9682977199554443}]}, {"text": "However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8166426718235016}]}, {"text": "To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words.", "labels": [], "entities": []}, {"text": "We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words. i", "labels": [], "entities": [{"text": "HowNet", "start_pos": 17, "end_pos": 23, "type": "DATASET", "confidence": 0.9536868929862976}]}], "introductionContent": [{"text": "A sememe is an indivisible semantic unit for human languages defined by linguists.", "labels": [], "entities": []}, {"text": "The semantic meanings of concepts (e.g., words) can be composed by a finite number of sememes.", "labels": [], "entities": []}, {"text": "However, the sememe set of a word is * Work done while doing internship at Tsinghua University.", "labels": [], "entities": []}, {"text": "Huiming Jin proposed the overall idea, designed the first experiment, conducted both experiments, and wrote the paper; Hao Zhu made suggestions on ensembling, proposed the second experiment, and spent a lot of time on proofreading the paper and making revisions.", "labels": [], "entities": []}, {"text": "All authors helped shape the research, analysis and manuscript.", "labels": [], "entities": []}, {"text": "\u2021 Corresponding author: Z.", "labels": [], "entities": []}, {"text": "Liu (liuzy@tsinghua.edu.cn) i Code is available at https://github.com/thunlp/Character-enhanced-Sememe-Prediction Figure 1: Sememes of the word \"\u94c1 \u5320\" (ironsmith) in HowNet, where occupation, human and industrial can be inferred by both external (contexts) and internal (characters) information, while metal is well-captured only by the internal information within the character \" \u94c1\" (iron).", "labels": [], "entities": [{"text": "HowNet", "start_pos": 165, "end_pos": 171, "type": "DATASET", "confidence": 0.9789156317710876}]}, {"text": "not explicit, which is why linguists build knowledge bases (KBs) to annotate words with sememes manually.", "labels": [], "entities": []}, {"text": "HowNet is a classical widely-used sememe KB ().", "labels": [], "entities": [{"text": "HowNet", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9642177820205688}]}, {"text": "In HowNet, linguists manually define approximately 2, 000 sememes, and annotate more than 100, 000 common words in Chinese and English with their relevant sememes in hierarchical structures.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.922640860080719}]}, {"text": "HowNet is well developed and has a wide range of applications in many NLP tasks, such as word sense disambiguation (, sentiment analysis () and cross-lingual word similarity).", "labels": [], "entities": [{"text": "HowNet", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9255185127258301}, {"text": "word sense disambiguation", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7278142968813578}, {"text": "sentiment analysis", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.9146915376186371}, {"text": "cross-lingual word similarity", "start_pos": 144, "end_pos": 173, "type": "TASK", "confidence": 0.662988523642222}]}, {"text": "Since new words and phrases are emerging everyday and the semantic meanings of existing concepts keep changing, it is time-consuming and work-intensive for human experts to annotate new concepts and maintain consistency for large-scale sememe KBs.", "labels": [], "entities": []}, {"text": "To address this issue,  propose an automatic sememe prediction framework to assist linguist annotation.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7457818686962128}]}, {"text": "They assumed that words which have similar semantic meanings are likely to share similar sememes.", "labels": [], "entities": []}, {"text": "Thus, they propose to represent word meanings as embeddings () learned from a large-scale text corpus, and they adopt collaborative filtering) and matrix factorization () for sememe prediction, which are concluded as Sememe Prediction with Word Embeddings (SPWE) and Sememe Prediction with Sememe Embeddings (SPSE) respectively.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 175, "end_pos": 192, "type": "TASK", "confidence": 0.8589565455913544}]}, {"text": "However, those methods ignore the internal information within words (e.g., the characters in Chinese words), which is also significant for word understanding, especially for words which are of lowfrequency or do not appear in the corpus at all.", "labels": [], "entities": [{"text": "word understanding", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.8003233373165131}]}, {"text": "In this paper, we take Chinese as an example and explore methods of taking full advantage of both external and internal information of words for sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 145, "end_pos": 162, "type": "TASK", "confidence": 0.9354170262813568}]}, {"text": "In Chinese, words are composed of one or multiple characters, and most characters have corresponding semantic meanings.", "labels": [], "entities": []}, {"text": "As shown by, more than 90% of Chinese characters in modern Chinese corpora are morphemes.", "labels": [], "entities": []}, {"text": "Chinese words can be divided into single-morpheme words and compound words, where compound words account fora dominant proportion.", "labels": [], "entities": []}, {"text": "The meanings of compound words are closely related to their internal characters as shown in.", "labels": [], "entities": []}, {"text": "Taking a compound word \"\u94c1\u5320\" (ironsmith) for instance, it consists of two Chinese characters: \"\u94c1\" (iron) and \"\u5320\" (craftsman), and the semantic meaning of \"\u94c1\u5320\" can be inferred from the combination of its two characters (iron + craftsman \u2192 ironsmith).", "labels": [], "entities": []}, {"text": "Even for some single-morpheme words, their semantic meanings may also be deduced from their characters.", "labels": [], "entities": []}, {"text": "For example, both characters of the single-morpheme word \"\u5f98\u5f8a\" (hover) represent the meaning of \"hover\" or \"linger\".", "labels": [], "entities": []}, {"text": "Therefore, it is intuitive to take the internal character information into consideration for sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.9037281274795532}]}, {"text": "In this paper, we propose a novel framework for Character-enhanced Sememe Prediction (CSP), which leverages both internal character information and external context for sememe prediction.", "labels": [], "entities": [{"text": "Sememe Prediction (CSP)", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7824797034263611}, {"text": "sememe prediction", "start_pos": 169, "end_pos": 186, "type": "TASK", "confidence": 0.875008225440979}]}, {"text": "CSP predicts the sememe candidates fora target word from its word embedding and the corresponding character embeddings.", "labels": [], "entities": []}, {"text": "Specifically, we follow SPWE and SPSE as introduced by  to model external information and propose Sememe Prediction with Word-to-Character Filtering (SPWCF) and Sememe Prediction with Character and Sememe Embeddings (SPCSE) to model internal character information.", "labels": [], "entities": [{"text": "SPWE", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.7982562184333801}]}, {"text": "In our experiments, we evaluate our models on the task of sememe prediction using HowNet.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.8540048003196716}, {"text": "HowNet", "start_pos": 82, "end_pos": 88, "type": "DATASET", "confidence": 0.9659287333488464}]}, {"text": "The results show that CSP achieves state-of-the-art performance and stays robust for low-frequency words.", "labels": [], "entities": []}, {"text": "To summarize, the key contributions of this work are as follows: (1) To the best of our knowledge, this work is the first to consider the internal information of characters for sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 177, "end_pos": 194, "type": "TASK", "confidence": 0.9201149642467499}]}, {"text": "(2) We propose a sememe prediction framework considering both external and internal information, and show the effectiveness and robustness of our models on a real-world dataset.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8172653317451477}]}], "datasetContent": [{"text": "In this section, we evaluate our models on the task of sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8848339319229126}]}, {"text": "Additionally, we analyze the performance of different methods for various word frequencies.", "labels": [], "entities": []}, {"text": "We also execute an elaborate case study to demonstrate the mechanism of our methods and the advantages of using internal information.", "labels": [], "entities": []}, {"text": "We use the human-annotated sememe KB HowNet for sememe prediction.", "labels": [], "entities": [{"text": "sememe KB HowNet", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.5372227927049001}, {"text": "sememe prediction", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8299528360366821}]}, {"text": "In HowNet, 103, 843 words are annotated with 212, 539 senses, and each sense is defined as a hierarchical structure of sememes.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9502688050270081}]}, {"text": "There are about 2, 000 sememes in HowNet.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.9847822785377502}]}, {"text": "However, the frequencies of some sememes in HowNet are very low, such that we consider them unimportant and remove them.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9593207836151123}]}, {"text": "Our final dataset contains 1, 400 sememes.", "labels": [], "entities": []}, {"text": "For learning the word and character embeddings, we use the Sogou-T corpus ii (, which contains 2.7 billion words.", "labels": [], "entities": [{"text": "Sogou-T corpus ii", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.9283727804819742}]}, {"text": "In our experiments, we evaluate SPWCF, SPCSE, and SPWCF + SPCSE which only use internal information, and the ensemble framework CSP which uses both internal and external information for sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 186, "end_pos": 203, "type": "TASK", "confidence": 0.8827696144580841}]}, {"text": "We use the stateof-the-art models from  as our baselines.", "labels": [], "entities": []}, {"text": "Additionally, we use the SPWE model with word embeddings learned by fastText) that considers both internal and external information as a baseline.", "labels": [], "entities": []}, {"text": "For the convenience of comparison, we select 60, 000 high-frequency words in Sogou-T corpus from HowNet.", "labels": [], "entities": [{"text": "Sogou-T corpus", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.938494086265564}, {"text": "HowNet", "start_pos": 97, "end_pos": 103, "type": "DATASET", "confidence": 0.7436671853065491}]}, {"text": "We divide the 60, 000 words into train, dev, and test sets of size 48, 000, 6, 000, and 6, 000, respectively, and we keep them fixed throughout all experiments except for Section 5.4.", "labels": [], "entities": []}, {"text": "In Section 5.4, we utilize the same train and dev sets, but use other words from HowNet as the test set to analyze the performance of our methods for different word frequency scenarios.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 81, "end_pos": 87, "type": "DATASET", "confidence": 0.952099084854126}]}, {"text": "We select the hyper-parameters on the dev set for all models including the baselines and report the evaluation results on the test set.", "labels": [], "entities": []}, {"text": "We set the dimensions of the word, sememe, and character embeddings to be 200.", "labels": [], "entities": []}, {"text": "The word embeddings are learned by).", "labels": [], "entities": []}, {"text": "For the baselines, in SPWE, the hyper-parameter c is set to 0.8, and the model considers no more than K = 100 nearest words.", "labels": [], "entities": [{"text": "SPWE", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.5888433456420898}]}, {"text": "We set the probability of decomposing zero elements in the word-sememe matrix in SPSE to be 0.5%.", "labels": [], "entities": [{"text": "SPSE", "start_pos": 81, "end_pos": 85, "type": "TASK", "confidence": 0.6241263151168823}]}, {"text": "(2) is 0.5.", "labels": [], "entities": []}, {"text": "The model is trained for 20 epochs, and the initial learning rate is 0.01, which decreases through iterations.", "labels": [], "entities": []}, {"text": "For fastText, we use skip-gram with hierarchical softmax to learn word embeddings, and we set the minimum length of character n-grams to be 1 and the maximum length ii Sogou-T corpus is provided by Sogou Inc., a Chinese commercial search engine company.", "labels": [], "entities": []}, {"text": "sogou.com/labs/resource/t.php of character n-grams to be 2.", "labels": [], "entities": []}, {"text": "For model ensembling, we use \u03bb SPWE \u03bb SPSE = 2.1 as the addition weight.", "labels": [], "entities": [{"text": "SPWE \u03bb SPSE", "start_pos": 31, "end_pos": 42, "type": "METRIC", "confidence": 0.7938387393951416}]}, {"text": "For SPCSE, we use Cluster-based Character Embeddings ( ) to learn pretrained character embeddings, and we set Ne to be 3.", "labels": [], "entities": [{"text": "SPCSE", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.8969476819038391}]}, {"text": "We set \u03bb in Eq. to be 0.1, and the model is trained for 20 epochs.", "labels": [], "entities": []}, {"text": "The initial learning rate is 0.01 and decreases during training as well.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7546886603037516}]}, {"text": "Since generally each character can relate to about 15 -20 sememes, we set the probability of decomposing zero elements in the word-sememe matrix in SPCSE to be 2.5%.", "labels": [], "entities": []}, {"text": "The task of sememe prediction aims to recommend appropriate sememes for unlabelled words.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.858369767665863}]}, {"text": "We cast this as a multi-label classification task, and adopt mean average precision (MAP) as the evaluation metric.", "labels": [], "entities": [{"text": "multi-label classification task", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.7509627342224121}, {"text": "mean average precision (MAP)", "start_pos": 61, "end_pos": 89, "type": "METRIC", "confidence": 0.9533129632472992}]}, {"text": "For each unlabelled word in the test set, we rank all sememe candidates with the scores given by our models as well as baselines, and we report the MAP results.", "labels": [], "entities": [{"text": "MAP", "start_pos": 148, "end_pos": 151, "type": "METRIC", "confidence": 0.7704473733901978}]}, {"text": "The results are reported on the test set, and the hyper-parameters are tuned on the dev set.", "labels": [], "entities": []}, {"text": "The evaluation results are shown in (1) Considerable improvements are obtained via model ensembling, and the CSP model achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "CSP combines the internal character information with the external context information, which significantly and consistently improves performance on sememe prediction.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.9162873327732086}]}, {"text": "Our results confirm the effectiveness of a combination of internal and external information for sememe prediction; since different models focus on different features of the inputs, the ensemble model can absorb the advantages of both methods.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.9446643888950348}]}, {"text": "(2) The performance of SPWCF + SPCSE is better than that of SPSE, which means using only internal information could already give good results for sememe prediction as well.", "labels": [], "entities": [{"text": "sememe prediction", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.8940407037734985}]}, {"text": "Moreover, in internal models, SPWCF performs much better than SPCSE, which also implies the strong power of collaborative filtering.", "labels": [], "entities": []}, {"text": "(3) The performance of SPWCF + SPCSE is worse than SPWE + SPSE.", "labels": [], "entities": [{"text": "SPWCF + SPCSE", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.4786569873491923}]}, {"text": "This indicates that it is still difficult to figure out the semantic meanings of a word without contextual information, due to the ambiguity and meaning vagueness of internal characters.", "labels": [], "entities": []}, {"text": "Moreover, some words are not compound words (e.g., single-morpheme words or transliterated words), whose meanings can hardly be inferred directly by their characters.", "labels": [], "entities": []}, {"text": "In Chinese, internal character information is just partial knowledge.", "labels": [], "entities": []}, {"text": "We present the results of SPWCF and SPCSE merely to show the capability to use the internal information in isolation.", "labels": [], "entities": []}, {"text": "In our case study, we will demonstrate that internal models are powerful for low-frequency words, and can be used to predict senses that do not appear in the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We  can observe that:", "labels": [], "entities": []}, {"text": " Table 2: MAP scores on sememe prediction with different word frequencies.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7857219576835632}, {"text": "sememe prediction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8896716237068176}]}]}