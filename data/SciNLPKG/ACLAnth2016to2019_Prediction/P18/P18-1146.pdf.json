{"title": [{"text": "Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation", "labels": [], "entities": [{"text": "Relation Schema Induction", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.7103725473086039}, {"text": "Back-off", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.971118152141571}, {"text": "Aggregation", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9188878536224365}]}], "abstractContent": [{"text": "Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text.", "labels": [], "entities": [{"text": "Relation Schema Induction (RSI)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.864394853512446}, {"text": "identifying type signatures of arguments of relations from unlabeled text", "start_pos": 50, "end_pos": 123, "type": "TASK", "confidence": 0.7357751309871674}]}, {"text": "Most of the previous work in this area have focused only on binary RSI, i.e., inducing only the subject and object type signatures per relation.", "labels": [], "entities": []}, {"text": "However, in practice, many relations are high-order, i.e., they have more than two arguments and inducing type signatures of all arguments is necessary.", "labels": [], "entities": []}, {"text": "For example, in the sports domain, inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is more informative than inducing just win(WinningPlayer, OpponentPlayer).", "labels": [], "entities": [{"text": "WinningPlayer", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9219704270362854}]}, {"text": "We refer to this problem as Higher-order Relation Schema Induction (HRSI).", "labels": [], "entities": [{"text": "Higher-order Relation Schema Induction (HRSI)", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7669476440974644}]}, {"text": "In this paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a novel framework for the HRSI problem.", "labels": [], "entities": [{"text": "Back-off and Aggregation (TFBA)", "start_pos": 52, "end_pos": 83, "type": "METRIC", "confidence": 0.7924624880154928}, {"text": "HRSI problem", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.9257070124149323}]}, {"text": "To the best of our knowledge, this is the first attempt at inducing higher-order relation schemata from unlabeled text.", "labels": [], "entities": []}, {"text": "Using the experimental analysis on three real world datasets, we show how TFBA helps in dealing with sparsity and induce higher order schemata.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building Knowledge Graphs (KGs) out of unstructured data is an area of active research.", "labels": [], "entities": []}, {"text": "Research in this has resulted in the construction of several large scale KGs, such as NELL (, Google Knowledge Vault () and YAGO (.", "labels": [], "entities": [{"text": "NELL", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.6676560044288635}, {"text": "YAGO", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.621600866317749}]}, {"text": "These KGs consist of millions of entities and beliefs involving those entities.", "labels": [], "entities": []}, {"text": "Such KG construction methods are schema-guided as they require the list of input relations and their schemata (e.g., playerPlaysSport).", "labels": [], "entities": [{"text": "KG construction", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.8574525117874146}]}, {"text": "In other words, knowledge of schemata is an important first step towards building such KGs.", "labels": [], "entities": []}, {"text": "While beliefs in such KGs are usually binary (i.e., involving two entities), many beliefs of interest go beyond two entities.", "labels": [], "entities": []}, {"text": "For example, in the sports domain, one maybe interested in beliefs of the form win(Roger Federer, Nadal, Wimbledon, London), which is an instance of the high-order (or n-ary) relation win whose schema is given by win(WinningPlayer, OpponentPlayer, Tournament, Location).", "labels": [], "entities": []}, {"text": "We refer to the problem of inducing such relation schemata involving multiple arguments as Higher-order Relation Schema Induction (HRSI).", "labels": [], "entities": [{"text": "Higher-order Relation Schema Induction (HRSI)", "start_pos": 91, "end_pos": 136, "type": "TASK", "confidence": 0.71051538842065}]}, {"text": "In spite of its importance, HRSI is mostly unexplored.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.6311569809913635}]}, {"text": "Recently, tensor factorization-based methods have been proposed for binary relation schema induction (, with gains in both speed and accuracy over previously proposed generative models.", "labels": [], "entities": [{"text": "binary relation schema induction", "start_pos": 68, "end_pos": 100, "type": "TASK", "confidence": 0.7565795630216599}, {"text": "speed", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9840528964996338}, {"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9938595294952393}]}, {"text": "To the best of our knowledge, tensor factorization methods have not been used for HRSI.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 82, "end_pos": 86, "type": "TASK", "confidence": 0.8922584652900696}]}, {"text": "We address this gap in this paper.", "labels": [], "entities": []}, {"text": "Due to data sparsity, straightforward adaptation of tensor factorization from () to HRSI is not feasible, as we shall see in Section 3.1.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.8059704899787903}]}, {"text": "We overcome this challenge in this paper, and make the following contributions.", "labels": [], "entities": []}, {"text": "\u2022 We propose Tensor Factorization with Backoff and Aggregation (TFBA), a novel tensor factorization-based method for Higher-order RSI (HRSI).", "labels": [], "entities": [{"text": "Backoff and Aggregation (TFBA)", "start_pos": 39, "end_pos": 69, "type": "METRIC", "confidence": 0.8257951786120733}]}, {"text": "In order to overcome data sparsity, TFBA backs-off and jointly factorizes multiple lower-order tensors derived from an extremely sparse higher-order tensor.", "labels": [], "entities": []}, {"text": "\u2022 As an aggregation step, we propose a constrained clique mining step which constructs the higher-order schemata from multiple binary schemata.", "labels": [], "entities": []}, {"text": "\u2022 Through experiments on multiple real-world datasets, we show the effectiveness of TFBA for HRSI.", "labels": [], "entities": []}, {"text": "Source code of TFBA is available at https: //github.com/madhavcsa/TFBA.", "labels": [], "entities": [{"text": "TFBA", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.7285757064819336}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3.1, we first motivate why a back-off strategy is needed for HRSI, rather than factorizing the higher-order tensor.", "labels": [], "entities": []}, {"text": "Further, we discuss the proposed TFBA framework in Section 3.2.", "labels": [], "entities": []}, {"text": "In Section 4, we demonstrate the effectiveness of the proposed approach using multiple real world datasets.", "labels": [], "entities": []}, {"text": "We conclude with a brief summary in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the performance of TFBA for the task of HRSI.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 69, "end_pos": 73, "type": "TASK", "confidence": 0.6238412857055664}]}, {"text": "We also propose a baseline model for HRSI called HardClust.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.8154189586639404}]}, {"text": "HardClust: We propose a baseline model called the Hard Clustering Baseline (HardClust) for the task of higher order relation schema induction.", "labels": [], "entities": [{"text": "higher order relation schema induction", "start_pos": 103, "end_pos": 141, "type": "TASK", "confidence": 0.6253439486026764}]}, {"text": "This model induces schemata by grouping perrelation NP arguments from OpenIE extractions.", "labels": [], "entities": []}, {"text": "In other words, for each relation, all the Noun Phrases (NPs) in first argument form a cluster that represents the subject of the relation, all the NPs in the second argument form a cluster that represents object and soon.", "labels": [], "entities": []}, {"text": "Then from each cluster, the topmost frequent NPs are chosen as the representative NPs for the argument type.", "labels": [], "entities": []}, {"text": "We note that this method is only able to induce one schema per relation.", "labels": [], "entities": []}, {"text": "Datasets: We run our experiments on three datasets.", "labels": [], "entities": []}, {"text": "The first dataset (Shootings) is a collection of 1,335 documents constructed from a publicly available database of mass shootings in the United States.", "labels": [], "entities": []}, {"text": "The second is New York Times Sports (NYT Sports) dataset which is a collection of 20,940 sports documents from the period 2005 and 2007.", "labels": [], "entities": [{"text": "New York Times Sports (NYT Sports) dataset", "start_pos": 14, "end_pos": 56, "type": "DATASET", "confidence": 0.8766107161839803}]}, {"text": "And the third dataset (MUC) is a set of 1300 Latin American newswire documents about terrorism events.", "labels": [], "entities": []}, {"text": "After performing the processing steps described in Section 3, we obtained 357,914 unique OpenIE extractions from the NYT Sports dataset, 10,847 from Shootings dataset, and 8,318 from the MUC dataset.", "labels": [], "entities": [{"text": "NYT Sports dataset", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.9535178144772848}, {"text": "Shootings dataset", "start_pos": 149, "end_pos": 166, "type": "DATASET", "confidence": 0.9599584341049194}, {"text": "MUC dataset", "start_pos": 187, "end_pos": 198, "type": "DATASET", "confidence": 0.9928215444087982}]}, {"text": "However, in order to properly analyze and evaluate the model, we consider only the 50 most frequent relations in the datasets and their corresponding OpenIE extractions.", "labels": [], "entities": [{"text": "OpenIE extractions", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.6985713243484497}]}, {"text": "This is done to avoid noisy OpenIE extractions to yield better data quality and to aid subsequent manual evaluation of the data.", "labels": [], "entities": [{"text": "OpenIE extractions", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7734779715538025}]}, {"text": "We construct input tensors following the procedure described in Section 3.2.", "labels": [], "entities": []}, {"text": "Details on the dimensions of tensors obtained are given in.", "labels": [], "entities": []}, {"text": "Model Selection: In order to select appropriate TFBA parameters, we perform a grid search over the space of hyper-parameters, and select the set of hyper-parameters that give best Average FIT score (AvgFIT).", "labels": [], "entities": [{"text": "Average FIT score (AvgFIT)", "start_pos": 180, "end_pos": 206, "type": "METRIC", "confidence": 0.8500128388404846}]}, {"text": "where, We perform a grid search for the rank parameters between 5 and 20, for the regularization weights we perform a grid search over 0 and 1.", "labels": [], "entities": []}, {"text": "provides the details of hyper-parameters set for different datasets.", "labels": [], "entities": []}, {"text": "Evaluation Protocol: For TFBA, we follow the protocol mentioned in Section 3.2.2 for constructing higher order schemata.", "labels": [], "entities": []}, {"text": "For every relation, we consider top 5 binary schemata from the factorization of each tensor.", "labels": [], "entities": []}, {"text": "We construct a tripartite graph, as explained in Section 3.2.2, and mine constrained maximal cliques from the tripartite graphs for schemata.", "labels": [], "entities": []}, {"text": "provides some qualitative examples of higher-order schemata induced by TFBA.", "labels": [], "entities": []}, {"text": "Accuracy of the schemata induced by the model is evaluated by human evaluators.", "labels": [], "entities": []}, {"text": "In our experiments, we use human judgments from three evaluators.", "labels": [], "entities": []}, {"text": "For every relation, the first and second columns given in are presented to the evaluators and they are asked to validate the schema.", "labels": [], "entities": []}, {"text": "We present top 50 schemata based on the score of the constrained maximal clique induced by TFBA to the evaluators.", "labels": [], "entities": []}, {"text": "This evaluation protocol was also used in (Movshovitz-Attias and Cohen, 2015) for evaluating ontology induction.", "labels": [], "entities": [{"text": "ontology induction", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8189765214920044}]}, {"text": "All evaluations were blind, i.e., the evaluators were not aware of the model they were evaluating.", "labels": [], "entities": []}, {"text": "Difficulty with Computing Recall: Even though recall is a desirable measure, due to the lack of availability of gold higher-order schema annotated corpus, it is not possible to compute recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9977846741676331}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9706335067749023}]}, {"text": "Although the MUC dataset has gold annotations for some predefined list of events, it does not have annotations for the relations.", "labels": [], "entities": [{"text": "MUC dataset", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9789497554302216}]}, {"text": "Experimental results comparing performance of various models for the task of HRSI are given in.", "labels": [], "entities": [{"text": "HRSI", "start_pos": 77, "end_pos": 81, "type": "TASK", "confidence": 0.7348482012748718}]}, {"text": "We present evaluation results from three evaluators represented as E1, E2 and E3.", "labels": [], "entities": []}, {"text": "As can be observed from, TFBA achieves better results than HardClust for the Shootings and NYT Sports datasets, however HardClust achieves better results for the MUC dataset.", "labels": [], "entities": [{"text": "TFBA", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.49355465173721313}, {"text": "NYT Sports datasets", "start_pos": 91, "end_pos": 110, "type": "DATASET", "confidence": 0.8834416667620341}, {"text": "MUC dataset", "start_pos": 162, "end_pos": 173, "type": "DATASET", "confidence": 0.98284912109375}]}, {"text": "Percentage agreement of the evaluators for TFBA is 72%, 70% and 60% for Shootings, NYT Sports and MUC datasets respectively.", "labels": [], "entities": [{"text": "TFBA", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8292035460472107}, {"text": "NYT Sports", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.8865261673927307}, {"text": "MUC datasets", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8609997928142548}]}, {"text": "HardClust Limitations: Even though HardClust gives better induction for MUC corpus, this approach has some serious drawbacks.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 72, "end_pos": 82, "type": "TASK", "confidence": 0.6445679068565369}]}, {"text": "HardClust can only induce one schema per relation.", "labels": [], "entities": []}, {"text": "This is a restrictive constraint as multiple senses can exist fora relation.", "labels": [], "entities": []}, {"text": "For example, consider the schemata induced for the relation shoot as shown in.", "labels": [], "entities": []}, {"text": "TFBA induces two senses for the relation, but HardClust can induce only one schema.", "labels": [], "entities": []}, {"text": "For a set of 4-tuples, HardClust can only induce ternary schemata; the dimensionality of the schemata cannot be varied.", "labels": [], "entities": []}, {"text": "Since the latent factors induced by HardClust are entirely based on frequency, the latent categories induced by HardClust are dominated by only a fixed set of noun phrases.", "labels": [], "entities": []}, {"text": "For example, in NYT Sports dataset, subject category induced by HardClust for all the relations is team, yankees, mets.", "labels": [], "entities": [{"text": "NYT Sports dataset", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.9503571192423502}]}, {"text": "In addition to inducing only one schema per relation, most of the times HardClust only induces a fixed set of categories.", "labels": [], "entities": []}, {"text": "Whereas for TFBA, the number of categories depends on the rank of factorization, which is a user provided parameter, thus providing more flexibility to choose the latent categories.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Details of dimensions of tensors constructed for each dataset used in the experiments.", "labels": [], "entities": []}, {"text": " Table 4: Examples of schemata induced by TFBA. Please note that some of them are 3-ary while others  are 4-ary. For details about schema induction, please refer to Section 3.2.", "labels": [], "entities": [{"text": "schema induction", "start_pos": 131, "end_pos": 147, "type": "TASK", "confidence": 0.8075447380542755}]}, {"text": " Table 5: Higher-order RSI accuracies of various methods on the three datasets. Induced schemata for  each dataset and method are evaluated by three human evaluators, E1, E2, and E3. TFBA performs  better than HardClust for Shootings and NYT Sports datasets. Even though HardClust achieves better  accuracy on MUC dataset, it has several limitations, see Section 4 for more details. Chambers-13 solves  a slightly different problem called event schema induction, for more details about the comparison with  Chambers-13 see Section 4.1.", "labels": [], "entities": [{"text": "NYT Sports datasets", "start_pos": 238, "end_pos": 257, "type": "DATASET", "confidence": 0.925737738609314}, {"text": "accuracy", "start_pos": 298, "end_pos": 306, "type": "METRIC", "confidence": 0.9963704347610474}, {"text": "MUC dataset", "start_pos": 310, "end_pos": 321, "type": "DATASET", "confidence": 0.8620450794696808}, {"text": "Chambers-13", "start_pos": 383, "end_pos": 394, "type": "DATASET", "confidence": 0.8692773580551147}, {"text": "event schema induction", "start_pos": 439, "end_pos": 461, "type": "TASK", "confidence": 0.8027868270874023}, {"text": "Chambers-13", "start_pos": 507, "end_pos": 518, "type": "DATASET", "confidence": 0.9739000201225281}]}]}