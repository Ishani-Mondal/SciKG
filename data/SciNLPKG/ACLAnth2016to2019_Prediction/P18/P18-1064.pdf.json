{"title": [{"text": "Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation", "labels": [], "entities": [{"text": "Soft Layer-Specific Multi-Task Summarization", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.4466676861047745}, {"text": "Question Generation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7121202349662781}]}], "abstractContent": [{"text": "An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document.", "labels": [], "entities": []}, {"text": "We improve these important aspects of abstractive summa-rization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details , and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document.", "labels": [], "entities": [{"text": "question generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7520771026611328}, {"text": "entailment generation", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.7037891447544098}]}, {"text": "We also propose novel multi-task architectures with high-level (seman-tic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution).", "labels": [], "entities": []}, {"text": "Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup.", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.8991801540056864}, {"text": "Gigaword datasets", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.7964640259742737}, {"text": "DUC-2002 transfer setup", "start_pos": 152, "end_pos": 175, "type": "DATASET", "confidence": 0.8821471134821574}]}, {"text": "We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abstractive summarization is the challenging NLG task of compressing and rewriting a document into a short, relevant, salient, and coherent summary.", "labels": [], "entities": [{"text": "Abstractive summarization", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6698325872421265}, {"text": "NLG task of compressing and rewriting a document into a short, relevant, salient, and coherent summary", "start_pos": 45, "end_pos": 147, "type": "Description", "confidence": 0.7621581962234095}]}, {"text": "It has numerous applications such as summarizing storylines, event understanding, etc.", "labels": [], "entities": [{"text": "summarizing storylines", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.961079329252243}, {"text": "event understanding", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8607926368713379}]}, {"text": "As compared to extractive or compressive summarization; Knight and * Equal contribution., abstractive summaries are based on rewriting as opposed to selecting.", "labels": [], "entities": [{"text": "summarization", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.8605666756629944}]}, {"text": "Recent end-to-end, neural sequence-tosequence models and larger datasets have allowed substantial progress on the abstractive task, with ideas ranging from copy-pointer mechanism and redundancy coverage, to metric reward based reinforcement learning.", "labels": [], "entities": []}, {"text": "Despite these strong recent advancements, there is still a lot of scope for improving the summary quality generated by these models.", "labels": [], "entities": []}, {"text": "A good rewritten summary is one that contains all the salient information from the document, is logically followed (entailed) by it, and avoids redundant information.", "labels": [], "entities": []}, {"text": "The redundancy aspect was addressed by coverage models), but we still need to teach these models about how to better detect salient information from the input document, as well as about better logicallydirected natural language inference skills.", "labels": [], "entities": []}, {"text": "In this work, we improve abstractive text summarization via soft, high-level (semantic) layerspecific multi-task learning with two relevant auxiliary tasks.", "labels": [], "entities": [{"text": "abstractive text summarization", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.5860680341720581}]}, {"text": "The first is that of document-toquestion generation, which teaches the summarization model about what are the right questions to ask, which in turn is directly related to what the salient information in the input document is.", "labels": [], "entities": [{"text": "document-toquestion generation", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.6987229734659195}]}, {"text": "The second auxiliary task is a premise-to-entailment generation task to teach it how to rewrite a summary which is a directed-logical subset of (i.e., logically follows from) the input document, and contains no contradictory or unrelated information.", "labels": [], "entities": [{"text": "premise-to-entailment generation task", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7936007579167684}]}, {"text": "For the question generation task, we use the SQuAD dataset (, where we learn to generate a question given a sentence containing the answer, similar to the recent work by.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.8235905766487122}, {"text": "SQuAD dataset", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.8177286982536316}]}, {"text": "Our entailment generation task is based on the recent SNLI classification dataset and task), converted to a generation task ( . Further, we also present novel multi-task learning architectures based on multi-layered encoder and decoder models, where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks, while keeping the lower-level (lexico-syntactic) layers unshared.", "labels": [], "entities": [{"text": "SNLI classification dataset", "start_pos": 54, "end_pos": 81, "type": "DATASET", "confidence": 0.720587412516276}]}, {"text": "We also explore different ways to optimize the shared parameters and show that 'soft' parameter sharing achieves higher performance than hard sharing.", "labels": [], "entities": []}, {"text": "Empirically, our soft, layer-specific sharing model with the question and entailment generation auxiliary tasks achieves statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets.", "labels": [], "entities": [{"text": "CNN/DailyMail", "start_pos": 198, "end_pos": 211, "type": "DATASET", "confidence": 0.8906139532725016}, {"text": "Gigaword datasets", "start_pos": 216, "end_pos": 233, "type": "DATASET", "confidence": 0.8191789388656616}]}, {"text": "It also performs significantly better on the DUC-2002 transfer setup, demonstrating its strong generalizability as well as the importance of auxiliary knowledge in low-resource scenarios.", "labels": [], "entities": [{"text": "DUC-2002 transfer setup", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.8197498520215353}]}, {"text": "We also report improvements on our auxiliary question and entailment generation tasks over their respective previous state-of-the-art.", "labels": [], "entities": [{"text": "question and entailment generation", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.6291778683662415}]}, {"text": "Moreover, we significantly decrease the training time of the multitask models by initializing the individual tasks from their pretrained baseline models.", "labels": [], "entities": []}, {"text": "Finally, we present human evaluation studies as well as detailed quantitative and qualitative analysis studies of the improved saliency detection and logical inference skills learned by our multi-task model.", "labels": [], "entities": [{"text": "saliency detection", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7311006486415863}]}], "datasetContent": [{"text": "Datasets  proval rate greater than 95%, and had at least 10,000 approved HITs.", "labels": [], "entities": []}, {"text": "For the pairwise model comparisons discussed in Sec.", "labels": [], "entities": []}, {"text": "6.2, we showed the annotators the input article, the ground truth summary, and the two model summaries (randomly shuffled to anonymize model identities) -we then asked them to choose the better among the two model summaries or choose 'Not-Distinguishable' if both summaries are equally good/bad.", "labels": [], "entities": []}, {"text": "Instructions for relevance were defined based on the summary containing salient/important information from the given article, being correct (i.e., avoiding contradictory/unrelated information), and avoiding redundancy.", "labels": [], "entities": []}, {"text": "Instructions for readability were based on the summary's fluency, grammaticality, and coherence.", "labels": [], "entities": []}, {"text": "Training Details All our soft/hard and layerspecific sharing decisions were made on the validation/development set.", "labels": [], "entities": []}, {"text": "Details of RNN hidden state sizes, Adam optimizer, mixing ratios, etc. are provided in the supplementary for reproducibility.", "labels": [], "entities": []}, {"text": "We also conducted a blind human evaluation on Amazon MTurk for relevance and readability, based on 100 samples, for both CNN/DailyMail and Gigaword (see instructions in Sec.", "labels": [], "entities": [{"text": "Amazon MTurk", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8862748444080353}, {"text": "relevance", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9128997325897217}, {"text": "CNN/DailyMail", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.9014839132626852}, {"text": "Gigaword", "start_pos": 139, "end_pos": 147, "type": "DATASET", "confidence": 0.8338320255279541}]}, {"text": "5).: ROUGE F1 scores on readability scores (and is higher in terms of total aggregate scores).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9876567125320435}, {"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.8096892237663269}]}, {"text": "One potential reason for this lower readability score is that our entailment generation auxiliary task encourages our summarization model to rewrite more and to be more abstractive than -see abstractiveness results in.", "labels": [], "entities": []}, {"text": "We also show human evaluation results on the Gigaword dataset in (again based on pairwise comparisons for 100 samples), where we see that our MTL model is better than our state-of-theart baseline on both relevance and readability.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9701191484928131}]}], "tableCaptions": [{"text": " Table 1: CNN/DailyMail summarization results. ROUGE scores are full length F-1 (as previous work).  All the multi-task improvements are statistically significant over the state-of-the-art baseline.", "labels": [], "entities": [{"text": "CNN/DailyMail summarization", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8986518979072571}, {"text": "ROUGE", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9934666752815247}, {"text": "F-1", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9906253814697266}]}, {"text": " Table 2: Summarization results on Gigaword.  ROUGE scores are full length F-1.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9918662905693054}, {"text": "F-1", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9868550300598145}]}, {"text": " Table 3: CNN/DM Human Evaluation: pairwise  comparison between our 3-way multi-task (MTL)  model w.r.t. our baseline and See et al. (2017).", "labels": [], "entities": [{"text": "CNN/DM Human Evaluation", "start_pos": 10, "end_pos": 33, "type": "DATASET", "confidence": 0.8982975959777832}]}, {"text": " Table 4: Gigaword Human Evaluation: pairwise  comparison between our 3-way multi-task (MTL)  model w.r.t. our baseline.", "labels": [], "entities": [{"text": "Gigaword Human Evaluation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.5279257893562317}]}, {"text": " Table 5: ROUGE F1 scores on", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9868000745773315}, {"text": "F1", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.7680570483207703}]}, {"text": " Table 6: Performance of our pointer-based entail- ment generation (EG) models compared with pre- vious SotA work. M, C, R, B are short for Meteor,  CIDEr-D, ROUGE-L, and BLEU-4, resp.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9303286671638489}, {"text": "BLEU-4", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9978085160255432}]}, {"text": " Table 7: Performance of our pointer-based ques- tion generation (QG) model w.r.t. previous work.", "labels": [], "entities": [{"text": "pointer-based ques- tion generation (QG)", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.697002001106739}]}, {"text": " Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.", "labels": [], "entities": [{"text": "Attn", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9718897938728333}]}, {"text": " Table 10: Saliency classification results of our  baseline vs. QG-multi-task model (p < 0.01).", "labels": [], "entities": [{"text": "Saliency classification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.914665699005127}]}, {"text": " Table 11: Abstractiveness: novel n-gram percent.", "labels": [], "entities": [{"text": "Abstractiveness", "start_pos": 11, "end_pos": 26, "type": "METRIC", "confidence": 0.9881334900856018}]}]}