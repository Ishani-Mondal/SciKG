{"title": [{"text": "Deep Reinforcement Learning for Chinese Zero pronoun Resolution", "labels": [], "entities": [{"text": "Chinese Zero pronoun Resolution", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.745845228433609}]}], "abstractContent": [{"text": "Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be shortsighted they often make local decisions.", "labels": [], "entities": [{"text": "Chinese zero pronoun resolution", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6124298498034477}]}, {"text": "They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions.", "labels": [], "entities": []}, {"text": "Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs.", "labels": [], "entities": []}, {"text": "In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models.", "labels": [], "entities": []}, {"text": "With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner , where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions.", "labels": [], "entities": []}, {"text": "Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.", "labels": [], "entities": [{"text": "OntoNotes 5.0 dataset", "start_pos": 24, "end_pos": 45, "type": "DATASET", "confidence": 0.8626006642977396}]}], "introductionContent": [{"text": "Zero pronoun, as a special linguistic phenomenon in pro-dropped languages, is pervasive in Chinese documents (.", "labels": [], "entities": []}, {"text": "A zero pronoun is a gap in the sentence, which refers to the component that is omitted because of the coherence of language.", "labels": [], "entities": []}, {"text": "Following shows an example of zero pronoun in Chinese document, where zero pronouns are represented as \"\ud97b\udf59\".", "labels": [], "entities": []}, {"text": "[S\u00e3\u222b N\u00f6\ud97b\udf59] d\u00dc h: \ud97b\udf591 #6 \u2022\u25ca F \ud97b\udf592 _ \ud97b\udf59\ud97b\udf59 \u02dd\u2202 \u00c5 \u222b \ud97b\udf59#\u21e5 \u21e4 Corresponding author.", "labels": [], "entities": [{"text": "F \ud97b\udf592 _ \ud97b\udf59\ud97b\udf59 \u02dd\u2202 \u00c5", "start_pos": 25, "end_pos": 39, "type": "METRIC", "confidence": 0.9236188530921936}]}, {"text": "( not only shows \ud97b\udf591 willing of acception, but also \ud97b\udf592 hopes that there should be someone in charge of it.)", "labels": [], "entities": []}, {"text": "A zero pronoun can bean anaphoric zero pronoun if it coreferes to one or more mentions in the associated text, or unanaphoric, if there are no such mentions.", "labels": [], "entities": []}, {"text": "In this example, the second zero pronoun \"\ud97b\udf59 2 \" is anaphoric and corefers to the mention \"S \u00e3\u222bN \u00f6 \ud97b\udf59/Litigant Li Yading\" while the zero pronoun \"\ud97b\udf59 1 \" is unanaphoric.", "labels": [], "entities": []}, {"text": "These mentions that contain the important information for interpreting the zero pronoun are called the antecedents.", "labels": [], "entities": [{"text": "interpreting the zero pronoun", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.8426331579685211}]}, {"text": "In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated.", "labels": [], "entities": [{"text": "Chinese zero pronoun resolution", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.6605376973748207}]}, {"text": "These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction.", "labels": [], "entities": [{"text": "anaphoric zero pronoun resolution", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.670866072177887}, {"text": "zero pronouncandidate antecedent prediction", "start_pos": 109, "end_pos": 152, "type": "TASK", "confidence": 0.6834559291601181}]}, {"text": "Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents, and substantially surpass classic models (, obtaining stateof-the-art results on the benchmark dataset.", "labels": [], "entities": []}, {"text": "However, these models are heavily making local coreference decisions.", "labels": [], "entities": []}, {"text": "They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future decisions.", "labels": [], "entities": []}, {"text": "Intuitively, antecedents provide key linguistic cues for explaining the zero pronoun, it is therefore reasonable to leverage useful information provided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate antecedent pairs.", "labels": [], "entities": []}, {"text": "For instance, given a sentence \"I have confidence that \ud97b\udf59 can do it.\" with its candidate mentions \"he\", \"the boy\" and \"I\", it is challenging to infer whether mention \"I\" is pos-sible to be the antecedent if it is considered separately.", "labels": [], "entities": []}, {"text": "In that case, the resolver may incorrectly predict \"I\" to be the antecedent since \"I\" is the nearest mention.", "labels": [], "entities": [{"text": "resolver", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.9477409720420837}]}, {"text": "Nevertheless, if we know that \"he\" and \"the boy\" have already been predicted to be the antecedents, it is uncomplicated to infer the \ud97b\udf59-\"I\" pair as \"non-coreference\" because \"I\" corefers to the disparate entity that is refered by \"he\".", "labels": [], "entities": []}, {"text": "Hence, a desirable resolver should be able to 1) take advantage of cues of previously predicted antecedents, which could be incorporated to help classify later candidate antecedents and 2) model the long-term influence of the single coreference decision in a sequential manner.", "labels": [], "entities": []}, {"text": "To achieve these goals, we propose a deep reinforcement learning model for anaphoric zero pronoun resolution.", "labels": [], "entities": [{"text": "anaphoric zero pronoun resolution", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.6486912816762924}]}, {"text": "On top of the neural network models (, two main innovations are introduced that are capable of efficaciously leveraging effective information provided by potential antecedents, and making long-term decisions from a global perspective.", "labels": [], "entities": []}, {"text": "First, when dealing with a specific zero pronoun-candidate antecedent pair, our system encodes all its preceding candidate antecedents that are predicted to be the antecedents in the vector space.", "labels": [], "entities": []}, {"text": "Consequently, this representative vector is regarded as the antecedent information, which can be utilized to measure the coreference probability of the zero pronoun-candidate antecedent pair.", "labels": [], "entities": []}, {"text": "In addition, the policy-based deep reinforcement learning algorithm is applied to learn the policy of making coreference decisions for zero pronoun-candidate antecedent pairs.", "labels": [], "entities": []}, {"text": "The innovative idea behind our reinforcement learning model is to model the antecedent determination as a sequential decision process, where our model learns to link the zero pronoun to its potential antecedents incrementally.", "labels": [], "entities": []}, {"text": "By encoding the antecedents predicted in previous states, our model is capable of exploring the longterm influence of independent decisions, producing more accurate results than models that simply consider the limited information in one single state.", "labels": [], "entities": []}, {"text": "Our strategy is favorable in the following aspects.", "labels": [], "entities": []}, {"text": "First, the proposed model learns to make decisions by linguistic cues of previously predicted antecedents.", "labels": [], "entities": []}, {"text": "Instead of simply making local decisions, our technique allows the model to learn which action (predict to bean antecedent) available from the current state can eventually lead to a high-scoring overall performance.", "labels": [], "entities": []}, {"text": "Second, instead of requiring supervised signals at each time step, deep reinforcement learning model optimizes its policy based on an overall reward signal.", "labels": [], "entities": []}, {"text": "In other words, it learns to directly optimize the overall evaluation metrics, which is more effective than models that learn with loss functions that heuristically define the goodness of a particular single decision.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on the OntoNotes dataset.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9519877135753632}]}, {"text": "Comparing to baseline systems, our model obtains significant improvements, achieving the state-of-the-art performance for zero pronoun resolution.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.6872199177742004}]}, {"text": "The major contributions of this paper are three-fold.", "labels": [], "entities": []}, {"text": "\u2022 We are the first to consider reinforcement learning models for zero pronoun resolution in Chinese documents; \u2022 The proposed deep reinforcement learning model leverages linguistic cues provided by the antecedents predicted in earlier states when classifying later candidate antecedents; \u2022 We evaluate our reinforcement learning model on a benchmark dataset, where a considerable improvement is gained over the state-of-the-art systems.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.8004851937294006}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section describes our deep reinforcement learning model for anaphoric zero pronoun resolution.", "labels": [], "entities": [{"text": "anaphoric zero pronoun resolution", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.6461377739906311}]}, {"text": "Section 3 presents our experiments, including the dataset description, evaluation metrics, experiment results, and analysis.", "labels": [], "entities": []}, {"text": "We outline related work in Section 4.", "labels": [], "entities": []}, {"text": "The Section 5 is about the conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Same to recent work on Chinese zero pronoun, the proposed model is evaluated on the Chinese portion of the OntoNotes 5.0 dataset 1 that was used in the Conll-2012 Shared Task.", "labels": [], "entities": [{"text": "OntoNotes 5.0 dataset 1", "start_pos": 107, "end_pos": 130, "type": "DATASET", "confidence": 0.8628437668085098}, {"text": "Conll-2012 Shared Task", "start_pos": 152, "end_pos": 174, "type": "DATASET", "confidence": 0.8280625939369202}]}, {"text": "Documents in this dataset are from six different sources, namely, Broadcast News (BN ), Newswires (NW ), Broadcast Conversations (BC), Telephone Conversations (T C), Web Blogs (W B) and Magazines (MZ).", "labels": [], "entities": []}, {"text": "Since zero pronoun coreference annotations exist in only the training and development set (Chen and Ng, 2016), we utilize the training dataset for training purposes and test our model on the development set.", "labels": [], "entities": []}, {"text": "The statistics of our dataset are reported in.", "labels": [], "entities": []}, {"text": "To make equal comparison, we adopt the strategy as utilized in the existing work, where 20% of the training dataset are randomly selected and reserved as a development dataset for tuning the model.", "labels": [], "entities": []}, {"text": "Following previous work on zero pronoun resolution (, metrics employed to evaluate our model are: recall, precision, and F-score (F).", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6521497865517935}, {"text": "recall", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9996688365936279}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9994206428527832}, {"text": "F-score (F)", "start_pos": 121, "end_pos": 132, "type": "METRIC", "confidence": 0.9378226846456528}]}, {"text": "We report the performance for each source in addition to the overall result.", "labels": [], "entities": []}, {"text": "Five recent zero pronoun resolution systems are employed as our baselines, namely,,,, and.", "labels": [], "entities": [{"text": "zero pronoun resolution", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.7439639369646708}]}, {"text": "The first of them is machine learning-based, the second is the unsupervised and the other ones are all deep learning models.", "labels": [], "entities": []}, {"text": "Since we concentrate on the anaphoric zero pronoun resolution process, we run experiments by employing the experiment setting with ground truth parse results and ground truth anaphoric zero pronoun, all of which are from the original dataset.", "labels": [], "entities": [{"text": "anaphoric zero pronoun resolution", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7781615406274796}]}, {"text": "Moreover, to illustrate the effectiveness of our reinforcement learning model, we run a set of ablation experiments by using different pretraining iterations and report the perfor-1 http://catalog.ldc.upenn.edu/ LDC2013T19 mance of our model with different iterations.", "labels": [], "entities": []}, {"text": "Besides, to explore the randomness of the reinforcement learning technique, we report the performance variation of our model with different random seeds.", "labels": [], "entities": []}, {"text": "In, we compare the results of our model with baselines in the test dataset.", "labels": [], "entities": []}, {"text": "Our reinforcement learning model surpasses all previous baselines.", "labels": [], "entities": []}, {"text": "More specifically, for the \"Overall\" results, our model obtains a considerable improvement by 2.3% in F-score over the best baseline ().", "labels": [], "entities": [{"text": "F-score", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9995296001434326}]}, {"text": "Moreover, we run experiments in different sources of documents and report the results for each source.", "labels": [], "entities": []}, {"text": "The number following a source's name indicates the amount of anaphoric zero pronoun in that source.", "labels": [], "entities": []}, {"text": "Our model beats the best baseline in four of six sources, demonstrating the efficiency of our reinforcement learning model.", "labels": [], "entities": []}, {"text": "The improvement gained over the best baseline in source \"BC\" is 4.3% in F-score, which is encouraging since it contains the most anaphoric zero pronoun.", "labels": [], "entities": [{"text": "BC", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9615985751152039}, {"text": "F-score", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9988453388214111}]}, {"text": "In all words, all these suggest that our model surpasses existed baselines, which demonstrates the efficiency of the proposed technique.", "labels": [], "entities": []}, {"text": "Ideally, our model learns useful information  gathered from candidates that have been predicted to be the antecedents in previous states, which brings a global-view instead of simply making partial decisions.", "labels": [], "entities": []}, {"text": "By applying the reinforcement learning, our model learns to directly optimize the overall performance in expectation, guiding benefit in making decisions in a sequential manner.", "labels": [], "entities": []}, {"text": "Consequently, they bring benefit to predict accurate antecedents, leading to the better performance.", "labels": [], "entities": []}, {"text": "Moreover, on purpose of better illustrating the effectiveness of the proposed reinforcement learning model, we run a set of experiments with different settings.", "labels": [], "entities": []}, {"text": "In particular, we compare the model with and without the proposed reinforcement learning process using different pre-training iterations.", "labels": [], "entities": []}, {"text": "For each time, we report the performance of our model on both the test and development set.", "labels": [], "entities": []}, {"text": "For all these experiments, we retain the rest of the model unchanged.: Experiment results of different models, where \"RL\" represents the reinforcement learning algorithm and \"Pre\" presents the model without reinforcement learning.", "labels": [], "entities": []}, {"text": "\"dev\" shows the performance of our reinforcement learning model on the development dataset.", "labels": [], "entities": []}, {"text": "shows the performance of our model with and without reinforcement learning.", "labels": [], "entities": []}, {"text": "We can see from the table that our model with reinforcement learning achieves better performance than the model without this all across the board.", "labels": [], "entities": []}, {"text": "With the help of reinforcement learning, our model learns to choose effective actions in sequential decisions.", "labels": [], "entities": []}, {"text": "It empowers the model to directly optimize the overall evaluation metrics, which brings a more effective and natural way of dealing with the task.", "labels": [], "entities": []}, {"text": "Moreover, by seeing that the performance on development dataset stops increasing with iterations bigger than 70, we therefore set the pretraining iterations to 70.", "labels": [], "entities": []}, {"text": "Following, to illustrate the impact of randomness in our reinforcement learning model, we run our model with different random seed values.", "labels": [], "entities": []}, {"text": "shows the performance of our model with different random seeds on the test dataset.", "labels": [], "entities": []}, {"text": "We report the minimum, the maximum, the median F-scores results and the standard deviation \ud97b\udf59 of F-scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9836109280586243}, {"text": "F-scores", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9716864228248596}]}, {"text": "We run Min F Median F Max F \ud97b\udf59 56.5 57.1 57.5 0.00253: Performance of our model with different random seeds.", "labels": [], "entities": [{"text": "Median F Max F \ud97b\udf59 56.5 57.1 57.5 0.00253", "start_pos": 13, "end_pos": 52, "type": "DATASET", "confidence": 0.6416331364048852}]}, {"text": "the model with 38 different random seeds.", "labels": [], "entities": []}, {"text": "The maximum F-score is 57.5% and the minimum one is 56.5%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9995274543762207}]}, {"text": "Based on this observation, we can draw the conclusion that our proposed reinforcement learning model generally beats the baselines and achieves the state-of-the-art performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the training and test dataset.", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters for the pre-training  (Pre) and reinforcement learning (RL).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 58, "end_pos": 85, "type": "METRIC", "confidence": 0.7410712778568268}]}, {"text": " Table 3: Experiment results on the test data. The first six columns show the results on different source  of documents and the last column is the overall results.", "labels": [], "entities": []}]}