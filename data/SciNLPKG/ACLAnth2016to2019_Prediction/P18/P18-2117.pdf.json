{"title": [{"text": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition", "labels": [], "entities": [{"text": "Language Recognition", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.7382786273956299}]}], "abstractContent": [{"text": "While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time.", "labels": [], "entities": [{"text": "Recurrent Neural Networks (RNNs)", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.7090733846028646}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9851621985435486}]}, {"text": "We consider the case of RNNs with finite precision whose computation time is linear in the input length.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9822378754615784}]}, {"text": "Under these limitations, we show that different RNN variants have different computational power.", "labels": [], "entities": []}, {"text": "In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9382367730140686}, {"text": "GRU", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.7016890048980713}]}, {"text": "This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior.", "labels": [], "entities": []}, {"text": "We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent Neural Network (RNNs) emerge as very strong learners of sequential data.", "labels": [], "entities": []}, {"text": "A famous result by, and its extension in, demonstrates that an Elman-RNN) with a sigmoid activation function, rational weights and infinite precision states can simulate a Turing-machine in real-time, making RNNs Turing-complete.", "labels": [], "entities": []}, {"text": "Recently, extended the result to the ReLU activation function.", "labels": [], "entities": [{"text": "ReLU activation function", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.7465969522794088}]}, {"text": "However, these constructions (a) assume reading the entire input into the RNN state and only then performing the computation, using unbounded time; and (b) rely on having infinite precision in the network states.", "labels": [], "entities": [{"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.983055830001831}]}, {"text": "As argued by, this is not the model of RNN computation used in NLP applications.", "labels": [], "entities": []}, {"text": "Instead, RNNs are often used by feeding an input sequence into the RNN one item at a time, each immediately returning a statevector that corresponds to a prefix of the sequence and which can be passed as input fora subsequent feed-forward prediction network operating inconstant time.", "labels": [], "entities": []}, {"text": "The amount of tape used by a Turing machine under this restriction is linear in the input length, reducing its power to recognition of context-sensitive language.", "labels": [], "entities": []}, {"text": "More importantly, computation is often performed on GPUs with 32bit floating point computation, and there is increasing evidence that competitive performance can be achieved also for quantized networks with 4-bit weights or fixed-point arithmetics.", "labels": [], "entities": [{"text": "computation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9669142961502075}]}, {"text": "The construction of) implements pushing 0 into a binary stack by the operation g \u2190 g/4 + 1/4.", "labels": [], "entities": []}, {"text": "This allows pushing roughly 15 zeros before reaching the limit of the 32bit floating point precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.8064519166946411}]}, {"text": "Finally, RNN solutions that rely on carefully orchestrated mathematical constructions are unlikely to be found using backpropagation-based training.", "labels": [], "entities": []}, {"text": "In this work we restrict ourselves to inputbound recurrent neural networks with finiteprecision states (IBFP-RNN), trained using backpropagation.", "labels": [], "entities": [{"text": "IBFP-RNN", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.6648194193840027}]}, {"text": "This class of networks is likely to coincide with the networks one can expect to obtain when training RNNs for NLP applications.", "labels": [], "entities": []}, {"text": "An IBFP Elman-RNN is finite state.", "labels": [], "entities": [{"text": "IBFP Elman-RNN", "start_pos": 3, "end_pos": 17, "type": "DATASET", "confidence": 0.6903080642223358}]}, {"text": "But what about other RNN variants?", "labels": [], "entities": []}, {"text": "In particular, we consider the Elman RNN (SRNN)) with squashing and with ReLU activations, the Long ShortTerm Memory (LSTM)) and the Gated Recurrent Unit (GRU) ().", "labels": [], "entities": []}, {"text": "The common wisdom is that the LSTM and GRU introduce additional gating components that handle the vanishing gradients problem of training SRNNs, thus stabilizing training and making it more robust.", "labels": [], "entities": [{"text": "GRU", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.821340799331665}]}, {"text": "The LSTM and GRU are often considered as almost equivalent variants of each other.", "labels": [], "entities": [{"text": "GRU", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.5632801651954651}]}, {"text": "(a) an b n -LSTM on a 1000 b (b) an b n c n -LSTM on a 100 b 100 c 100 (c) an b n -GRU on a 1000 b 1000 (d) an b n c n -GRU on a 100 b 100 c 100 Figure 1: Activations -c for LSTM and h for GRU -for networks trained on an b n and an b n c n . The LSTM has clearly learned to use an explicit counting mechanism, in contrast with the GRU.", "labels": [], "entities": []}, {"text": "We show that in the input-bound, finiteprecision case, there is areal difference between the computational capacities of the LSTM and the GRU: the LSTM can easily perform unbounded counting, while the GRU (and the SRNN) cannot.", "labels": [], "entities": []}, {"text": "This makes the LSTM a variant of a k-counter machine, while the GRU remains finite-state.", "labels": [], "entities": []}, {"text": "Interestingly, the SRNN with ReLU activation followed by an MLP classifier also has power similar to a k-counter machine.", "labels": [], "entities": []}, {"text": "These results suggest there is a class of formal languages that can be recognized by LSTMs but not by GRUs.", "labels": [], "entities": []}, {"text": "In section 5, we demonstrate that for at least two such languages, the LSTM manages to learn the desired concept classes using backpropagation, while using the hypothesized control structure.", "labels": [], "entities": []}, {"text": "shows the activations of 10-d LSTM and GRU trained to recognize the languages an b n and an b n c n . It is clear that the LSTM learned to dedicate specific dimensions for counting, in contrast to the GRU.", "labels": [], "entities": []}, {"text": "1 1 Is the ability to perform unbounded counting relevant to \"real world\" NLP tasks?", "labels": [], "entities": []}, {"text": "In some cases it might be.", "labels": [], "entities": []}, {"text": "For example, processing linearized parse trees ( requires counting brackets and nesting levels.", "labels": [], "entities": []}, {"text": "Indeed, previous works that process linearized parse trees report using LSTMs", "labels": [], "entities": [{"text": "LSTMs", "start_pos": 72, "end_pos": 77, "type": "TASK", "confidence": 0.3352317810058594}]}], "datasetContent": [{"text": "Can the LSTM indeed learn to behave as a kcounter machine when trained using backpropagation?", "labels": [], "entities": []}, {"text": "We show empirically that: 1.", "labels": [], "entities": []}, {"text": "LSTMs can be trained to recognize an b n and an b n c n . 2. These LSTMs generalize to much higher n than seen in the training set (though not infinitely so).", "labels": [], "entities": []}, {"text": "3. The trained LSTM learn to use the perdimension counting mechanism.", "labels": [], "entities": []}, {"text": "4. The GRU can also be trained to recognize an b n and an b n c n , but they do not have clear One such mechanism could be to divide a given dimension by k > 1 at each symbol encounter, by setting zt = 1/k and\u02dchtand\u02dc and\u02dcht = 0.", "labels": [], "entities": [{"text": "GRU", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.808565080165863}]}, {"text": "Note that the inverse operation would not be implementable, and counting down would have to be realized with a second counter.", "labels": [], "entities": []}, {"text": "One can argue that other counting mechanismsinvolving several dimensions-are also possible.", "labels": [], "entities": []}, {"text": "Intuitively, such mechanisms cannot be trained to perform unbounded counting based on a finite sample as the model has no means of generalizing the counting behavior to dimensions beyond those seen in training.", "labels": [], "entities": []}, {"text": "We discuss this more in depth in the supplementary material, where we also prove that an SRNN cannot represent a binary counter.", "labels": [], "entities": []}, {"text": "counting dimensions, and they generalize to much smaller n than the LSTMs, often failing to generalize correctly even for n within their training domain.", "labels": [], "entities": []}], "tableCaptions": []}