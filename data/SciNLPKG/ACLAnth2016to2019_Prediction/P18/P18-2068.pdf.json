{"title": [{"text": "Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention", "labels": [], "entities": [{"text": "Improving Slot Filling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8435589273770651}, {"text": "Spoken Language Understanding", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.8341131011644999}, {"text": "Joint Pointer and Attention", "start_pos": 61, "end_pos": 88, "type": "METRIC", "confidence": 0.7288957983255386}]}], "abstractContent": [{"text": "We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.824782520532608}]}, {"text": "This model predicts slot values by jointly learning to copy a word which maybe out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model.", "labels": [], "entities": []}, {"text": "Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9088961780071259}, {"text": "OOV problem", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.7863948047161102}]}, {"text": "Additionally , we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Slot filling is a key component in spoken language understanding (SLU), which is usually treated as a sequence labeling problem and solved using methods such as conditional random fields (CRFs) or recurrent neural networks (RNNs) (.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9265598356723785}, {"text": "spoken language understanding (SLU)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.8312558829784393}]}, {"text": "Although these models have achieved good results, they are learned on the datasets with wordlevel annotations, e.g., with the BIO tagging schema as in ATIS ().", "labels": [], "entities": []}, {"text": "Manual annotations at word level require big effort and some corpora has only sentence-level annotations available, e.g., the utterance \"...", "labels": [], "entities": []}, {"text": "moderately priced restaurant\" has a slot-value pair annotation of \"pricerange=moderate\".", "labels": [], "entities": [{"text": "slot-value pair annotation", "start_pos": 36, "end_pos": 62, "type": "METRIC", "confidence": 0.9302175442377726}, {"text": "pricerange", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9585063457489014}]}, {"text": "As such datasets lack explicit alignment between the annotations and the input words, some systems rely on handcrafted rules to find the alignments in order to automatically create word-level labels to learn the sequence model (, but finding such alignments is non-trivial.", "labels": [], "entities": []}, {"text": "For example, it was shown in) that when applying the manually created word aliases to the speech recognition hypotheses, only around 73% of alignments can be found due to the noise, and a CRF model trained on such noisy data performs particularly worse than some other methods.", "labels": [], "entities": [{"text": "speech recognition hypotheses", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.7858801384766897}]}, {"text": "In addition it is time-consuming to adapt the manual rules or aliases to new domains.", "labels": [], "entities": []}, {"text": "Some other work avoids this issue by regarding slot filling as a classification task, where an utterance is classified into one or more slot-value pairs.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.8340623080730438}]}, {"text": "This, however, brings other challenges.", "labels": [], "entities": []}, {"text": "One is that some types of slots may have a large or even unlimited number of possible values, so the classifiers may suffer from the data sparsity problem when the training data is limited.", "labels": [], "entities": []}, {"text": "Another is the OOV problem caused by unknown slot values (e.g., restaurant name, street name), which is impossible to predefine and is very common in real-world spoken dialogue applications.", "labels": [], "entities": [{"text": "OOV", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.7983720302581787}]}, {"text": "To address these challenges, we present a neural generative model for slot filling on unaligned dialog data, specifically for slot value prediction as it has more challenges caused by OOV.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 70, "end_pos": 82, "type": "TASK", "confidence": 0.8678198754787445}, {"text": "slot value prediction", "start_pos": 126, "end_pos": 147, "type": "TASK", "confidence": 0.7571345965067545}, {"text": "OOV", "start_pos": 184, "end_pos": 187, "type": "DATASET", "confidence": 0.7495908737182617}]}, {"text": "The model uses Seq2Seq learning to predict a sequence of slot values from an utterance.", "labels": [], "entities": []}, {"text": "Inspired by the ability of pointer network (Ptr-Net) ( at addressing OOV problems, we incorporate Ptr-Net into a standard Seq2Seq attentional model to handle OOV slots.", "labels": [], "entities": []}, {"text": "It can predict slot values by either generating one from a fixed vocabulary or selecting a word from the utterance.", "labels": [], "entities": []}, {"text": "The final model is a weighted combination of the two operations.", "labels": [], "entities": []}, {"text": "To summarize, our main contributions are: Figure 1: Our model for slot value prediction based on Seq2Seq learning with attention and Ptr-Net.", "labels": [], "entities": [{"text": "slot value prediction", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7157674034436544}]}, {"text": "\u2022 We use a neural generative model for slot filling on the data without word-level annotations which has received less attention.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.8107022643089294}]}, {"text": "\u2022 We adopt the pointer network to handle the OOV problem in slot value prediction, which achieves good performance without any manually-designed rules or features.", "labels": [], "entities": [{"text": "OOV", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7325242161750793}, {"text": "slot value prediction", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7345554629961649}]}], "datasetContent": [{"text": "In this section, we present our experimental results on DSTC2 (Dialog State Tracking Challenge) (), including the results of slot value prediction solely and a complete SLU system.", "labels": [], "entities": [{"text": "Dialog State Tracking Challenge)", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7641689062118531}, {"text": "slot value prediction", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7918940782546997}]}, {"text": "Our models are implemented using Keras 1 with TensorFlow as backend.", "labels": [], "entities": []}, {"text": "In all the experiments, the dimension of hidden states is 128, dimension of word embeddings is 100, dropout rate is 0.5, and batch size is 32.", "labels": [], "entities": []}, {"text": "Word embeddings are not pre-trained but learned from scratch during training.", "labels": [], "entities": []}, {"text": "Teacher forcing is used during training, with Adam optimizer ().", "labels": [], "entities": []}, {"text": "All training consists of 10 epochs with early stopping on the development set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of slot value prediction.", "labels": [], "entities": [{"text": "slot value prediction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8902904987335205}]}, {"text": " Table 2: Results of slot value prediction with vary- ing training size and OOV ratio.", "labels": [], "entities": [{"text": "slot value prediction", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8336649338404337}, {"text": "vary- ing training size", "start_pos": 48, "end_pos": 71, "type": "METRIC", "confidence": 0.9163346767425538}, {"text": "OOV ratio", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9907233417034149}]}, {"text": " Table 3: Overall SLU performance.", "labels": [], "entities": [{"text": "SLU", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8543928265571594}]}, {"text": " Table 4: SLU results with varying training size.", "labels": [], "entities": [{"text": "SLU", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7780578136444092}]}]}