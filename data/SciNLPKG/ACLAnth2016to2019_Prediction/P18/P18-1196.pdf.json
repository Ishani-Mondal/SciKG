{"title": [{"text": "Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers", "labels": [], "entities": [{"text": "Evaluating and Improving their Ability to Predict Numbers", "start_pos": 30, "end_pos": 87, "type": "TASK", "confidence": 0.6929049082100391}]}], "abstractContent": [{"text": "Numeracy is the ability to understand and work with numbers.", "labels": [], "entities": []}, {"text": "It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains.", "labels": [], "entities": []}, {"text": "In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary.", "labels": [], "entities": [{"text": "digit-by-digit composition", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.7310076653957367}]}, {"text": "Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models.", "labels": [], "entities": []}, {"text": "A combination of strategies can further improve perplexity.", "labels": [], "entities": []}, {"text": "Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.", "labels": [], "entities": [{"text": "mean absolute percentage errors", "start_pos": 58, "end_pos": 89, "type": "METRIC", "confidence": 0.8444227874279022}]}], "introductionContent": [{"text": "Language models (LMs) are statistical models that assign a probability over sequences of words.", "labels": [], "entities": []}, {"text": "Language models can often help with other tasks, such as speech recognition, machine translation (), text summarisation (, question answering (), semantic error detection (, and fact checking (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7602420747280121}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7904068231582642}, {"text": "text summarisation", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7678829729557037}, {"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.8472196161746979}, {"text": "semantic error detection", "start_pos": 146, "end_pos": 170, "type": "TASK", "confidence": 0.6856481035550436}, {"text": "fact checking", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.8783353865146637}]}, {"text": "Numeracy and literacy refer to the ability to comprehend, use, and attach meaning to numbers and words, respectively.", "labels": [], "entities": []}, {"text": "Language models exhibit literacy by being able to assign higher probabilities to sentences that Figure 1: Modelling numerals with a categorical distribution over a fixed vocabulary maps all out-ofvocabulary numerals to the same type, e.g. UNK, and does not reflect the smoothness of the underlying continuous distribution of certain attributes. are both grammatical and realistic, as in this example: 'I eat an apple' (grammatical and realistic) 'An apple eats me' (unrealistic) 'I eats an apple' (ungrammatical) Likewise, a numerate language model should be able to rank numerical claims based on plausibility: 'John's height is 1.75 metres' (realistic) 'John's height is 999.999 metres' (unrealistic) Existing approaches to language modelling treat numerals similarly to other words, typically using categorical distributions over a fixed vocabulary.", "labels": [], "entities": [{"text": "literacy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9629375338554382}, {"text": "UNK", "start_pos": 239, "end_pos": 242, "type": "DATASET", "confidence": 0.8252971768379211}]}, {"text": "However, this maps all unseen numerals to the same unknown type and ignores the smoothness of continuous attributes, as shown in.", "labels": [], "entities": []}, {"text": "In that respect, existing work on language modelling does not explicitly evaluate or optimise for numeracy.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7185405194759369}]}, {"text": "Numerals are often neglected and low-resourced, e.g. they are often masked, and there are only 15,164 (3.79%) numerals among GloVe's 400,000 embeddings pretrained on 6 billion tokens ().", "labels": [], "entities": []}, {"text": "Yet, numbers appear ubiquitously, from children's magazines to clinical reports (, and grant objectivity to sciences.", "labels": [], "entities": []}, {"text": "Previous work finds that numerals have higher out-of-vocabulary rates than other words and proposes solutions for representing unseen numerals as inputs to language models, e.g. using numerical magnitudes as features (.", "labels": [], "entities": []}, {"text": "Such work identifies that the perplexity of language models on the subset of numerals can be very high, but does not directly address the issue.", "labels": [], "entities": []}, {"text": "This paper focuses on evaluating and improving the ability of language models to predict numerals.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are as follows: 1.", "labels": [], "entities": []}, {"text": "We explore different strategies for modelling numerals, such as memorisation and digit-bydigit composition, and propose a novel neural architecture based on continuous probability density functions.", "labels": [], "entities": [{"text": "digit-bydigit composition", "start_pos": 81, "end_pos": 106, "type": "TASK", "confidence": 0.7208998799324036}]}, {"text": "2. We propose the use of evaluations that adjust for the high out-of-vocabulary rate of numerals and account for their numerical value (magnitude).", "labels": [], "entities": []}, {"text": "3. We evaluate on a clinical and a scientific corpus and provide a qualitative analysis of learnt representations and model predictions.", "labels": [], "entities": []}, {"text": "We find that modelling numerals separately from other words can drastically improve the perplexity of LMs, that different strategies for modelling numerals are suitable for different textual contexts, and that continuous probability density functions can improve the LM's prediction accuracy for numbers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9830190539360046}]}], "datasetContent": [{"text": "We set the vocabularies to the 1,000 and 5,000 most frequent token types for the clinical and scientific datasets, respectively.", "labels": [], "entities": []}, {"text": "We use gated token-character embeddings for the input of numerals and token embeddings for the input and output of words, since the scope of our paper is numeracy.", "labels": [], "entities": []}, {"text": "We set the models' hidden dimensions to D = 50 and initialise all token embeddings to pretrained GloVe (   is scale independent and allows for comparison across data, whereas root mean square and mean absolute errors (RMSE, MAE) are scale dependent.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9574730396270752}, {"text": "root mean square and mean absolute errors (RMSE", "start_pos": 175, "end_pos": 222, "type": "METRIC", "confidence": 0.6707883907688988}, {"text": "MAE", "start_pos": 224, "end_pos": 227, "type": "METRIC", "confidence": 0.7290775775909424}]}, {"text": "Medians (MdAE, MdAPE) are informative of the distribution of errors.", "labels": [], "entities": []}, {"text": "RNNs are LSTMs) with the biases of LSTM forget gate were initialised to 1.0 (.", "labels": [], "entities": []}, {"text": "We train using mini-batch gradient decent with the Adam optimiser () and regularise with early stopping and 0.1 dropout rate in the input and output of the token-based RNN.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.9327815771102905}, {"text": "RNN", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.7660210132598877}]}, {"text": "For the mixture of Gaussians, we select the mean and variances to summarise the data at different granularities by fitting 7 separate mixture of Gaussian models on all numbers, each with twice as many components as the previous, fora total of 2 7+1 \u2212 1 = 256 components.", "labels": [], "entities": []}, {"text": "These models are initialised at percentile points from the data and trained with the expectation-minimisation algorithm.", "labels": [], "entities": []}, {"text": "The means and variances are then fixed and not updated when we train the language model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical description of the clinical and sci- entific datasets: Number of instances (i.e. paragraphs),  maximum and average lengths, proportions of words  and numerals, descriptive statistics of numbers.", "labels": [], "entities": []}, {"text": " Table 2: Test set perplexities for the clinical and scientific data. Adjusted perplexities (APP) are directly  comparable across all data and models, but perplexities (PP) are sensitive to the varying out-of-vocabulary rates.", "labels": [], "entities": [{"text": "Adjusted perplexities (APP)", "start_pos": 70, "end_pos": 97, "type": "METRIC", "confidence": 0.8107881903648376}]}, {"text": " Table 3: Test set regression evaluation for the clinical and scientific data. Mean absolute percentage error (MAPE)", "labels": [], "entities": [{"text": "Mean absolute percentage error (MAPE)", "start_pos": 79, "end_pos": 116, "type": "METRIC", "confidence": 0.8869608300072807}]}, {"text": " Table 4: Examples of numerals with highest probability in each strategy of the combination model.", "labels": [], "entities": []}]}