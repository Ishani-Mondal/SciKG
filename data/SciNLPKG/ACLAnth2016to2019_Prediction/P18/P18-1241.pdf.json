{"title": [{"text": "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning", "labels": [], "entities": [{"text": "Attacking Visual Language Grounding", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7928288727998734}, {"text": "Neural Image Captioning", "start_pos": 79, "end_pos": 102, "type": "TASK", "confidence": 0.6875800887743632}]}], "abstractContent": [{"text": "Visual language grounding is widely studied in modern neural image caption-ing systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolu-tional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation.", "labels": [], "entities": [{"text": "Visual language grounding", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7433568040529887}, {"text": "image caption-ing", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7622533440589905}, {"text": "image feature extraction", "start_pos": 222, "end_pos": 246, "type": "TASK", "confidence": 0.6717955072720846}, {"text": "language caption generation", "start_pos": 288, "end_pos": 315, "type": "TASK", "confidence": 0.8142609496911367}]}, {"text": "To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 190, "end_pos": 213, "type": "TASK", "confidence": 0.7364092270533243}]}, {"text": "The proposed algorithm provides two evaluation approaches, which check whether neural image captioning systems can be mislead to output some randomly chosen captions or keywords.", "labels": [], "entities": []}, {"text": "Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords , and the adversarial examples can be made highly transferable to other image captioning systems.", "labels": [], "entities": []}, {"text": "Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.6933907270431519}, {"text": "visual language grounding", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.6133709649244944}]}], "introductionContent": [{"text": "In recent years, language understanding grounded in machine vision and perception has made remarkable progress in natural language processing (NLP) and artificial intelligence (AI), such as image captioning and visual question answering.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7544190883636475}, {"text": "image captioning", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7226301580667496}, {"text": "question answering", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.689261868596077}]}, {"text": "Image captioning is a multimodal learning task and has been used to study the interaction between language and vision models.", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8107320666313171}]}, {"text": "It takes an image as an input and generates a language caption that best describes its visual contents, and has many important applications such as developing image search engines with complex natural language queries, building AI agents that can see and talk, and promoting equal web access for people who are blind or visually impaired.", "labels": [], "entities": []}, {"text": "Modern image captioning systems typically adopt an encoder-decoder framework composed of two principal modules: a convolutional neural network (CNN) as an encoder for image feature extraction and a recurrent neural network (RNN) as a decoder for caption generation.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7241677045822144}, {"text": "image feature extraction", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.6546681125958761}, {"text": "caption generation", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.8904913365840912}]}, {"text": "This CNN+RNN architecture includes popular image captioning models such as Show-and-Tell (, Show-Attend-and-Tell ( and NeuralTalk (.", "labels": [], "entities": []}, {"text": "Recent studies have highlighted the vulnerability of CNN-based image classifiers to adversarial examples: adversarial perturbations to benign images can be easily crafted to mislead a well-trained classifier, leading to visually indistinguishable adversarial examples to human.", "labels": [], "entities": []}, {"text": "In this study, we investigate a more challenging problem in visual language grounding domain that evaluates the robustness of multimodal RNN in the form of a CNN+RNN architecture, and use neural image captioning as a case study.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.7072412570317587}]}, {"text": "Note that crafting adversarial examples in image captioning tasks is strictly harder than in well-studied image classification tasks, due to the following reasons: (i) class attack v.s. caption attack: unlike classification tasks where the class labels are well defined, the output of image captioning is a set of top-ranked captions.", "labels": [], "entities": [{"text": "image captioning tasks", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.746821771065394}, {"text": "image classification tasks", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.8266463478406271}]}, {"text": "Simply treating different captions as distinct classes will result in an enormous number of classes that can even precede the number of training images.", "labels": [], "entities": []}, {"text": "In addition, semantically similar In this paper, we tackle the aforementioned challenges by proposing a novel algorithm called Show-and-Fool.", "labels": [], "entities": []}, {"text": "We formulate the process of crafting adversarial examples in neural image captioning systems as optimization problems with novel objective functions designed to adopt the CNN+RNN architecture.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.7746885418891907}]}, {"text": "Specifically, our objective function is a linear combination of the distortion between benign and adversarial examples as well as some carefully designed loss functions.", "labels": [], "entities": []}, {"text": "The proposed Show-and-Fool algorithm provides two approaches to craft adversarial examples in neural image captioning under different scenarios: 1.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 94, "end_pos": 117, "type": "TASK", "confidence": 0.7042672236760458}]}, {"text": "Targeted caption method: Given a targeted caption, craft adversarial perturbations to any image such that its generated caption matches the targeted caption.", "labels": [], "entities": [{"text": "Targeted caption", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7110601365566254}]}, {"text": "2. Targeted keyword method: Given a set of keywords, craft adversarial perturbations to any image such that its generated caption contains the specified keywords.", "labels": [], "entities": []}, {"text": "The captioning model has the freedom to make sentences with target keywords in any order.", "labels": [], "entities": []}, {"text": "As an illustration, shows an adversarial example crafted by Show-and-Fool using the targeted caption method.", "labels": [], "entities": [{"text": "Show-and-Fool", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9269697666168213}]}, {"text": "The adversarial perturbations are visually imperceptible while can successfully mislead Show-and-Tell to generate the targeted captions.", "labels": [], "entities": []}, {"text": "Interestingly and perhaps surprisingly, our results pinpoint the Achilles heel of the language and vision models used in the tested image captioning systems.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 132, "end_pos": 148, "type": "TASK", "confidence": 0.7516611516475677}]}, {"text": "Moreover, the adversarial examples in neural image captioning highlight the inconsistency in visual language grounding between humans and machines, suggesting a possible weakness of current machine vision and perception machinery.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7648634711901346}]}, {"text": "Below we highlight our major contributions: \u2022 We propose Show-and-Fool, a novel optimization based approach to crafting adversarial examples in image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7132199108600616}]}, {"text": "We provide two types of adversarial examples, targeted caption and targeted keyword, to analyze the robustness of neural image captioners.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the very first work on crafting adversarial examples for image captioning.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 95, "end_pos": 111, "type": "TASK", "confidence": 0.7527030110359192}]}, {"text": "\u2022 We propose powerful and generic loss functions that can craft adversarial examples and evaluate the robustness of the encoder-decoder pipelines in the form of a CNN+RNN architecture.", "labels": [], "entities": []}, {"text": "In particular, our loss designed for targeted keyword attack only requires the adversarial caption to contain a few specified keywords; and we allow the neural network to make meaningful sentences with these keywords on its own.", "labels": [], "entities": []}, {"text": "\u2022 We conduct extensive experiments on the MSCOCO dataset.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9324881434440613}]}, {"text": "Experimental results show that our targeted caption method attains a 95.8% attack success rate when crafting adversarial examples with randomly assigned captions.", "labels": [], "entities": []}, {"text": "In addition, our targeted keyword attack yields an even higher success rate.", "labels": [], "entities": []}, {"text": "We also show that attacking CNN+RNN models is inherently different and more challenging than only attacking CNN models.", "labels": [], "entities": []}, {"text": "\u2022 We also show that Show-and-Fool can produce highly transferable adversarial examples: an adversarial image generated for fooling Showand-Tell can also fool other image captioning models, leading to new robustness implications of neural image captioning systems.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 231, "end_pos": 254, "type": "TASK", "confidence": 0.7442377010981241}]}], "datasetContent": [{"text": "We performed extensive experiments to test the effectiveness of our Show-and-Fool algorithm and study the robustness of image captioning systems under different problem settings.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 120, "end_pos": 136, "type": "TASK", "confidence": 0.7318431437015533}]}, {"text": "In our experiments 1 , we use the pre-trained TensorFlow implementation 2 of Show-and-Tell ( with Inception-v3 as the CNN for visual feature extraction.", "labels": [], "entities": [{"text": "visual feature extraction", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.6261265774567922}]}, {"text": "Our testbed is Microsoft COCO () (MSCOCO) data set.", "labels": [], "entities": [{"text": "Microsoft COCO () (MSCOCO) data set", "start_pos": 15, "end_pos": 50, "type": "DATASET", "confidence": 0.9011643379926682}]}, {"text": "Although some more recent neural image captioning systems can achieve better performance than Show-and-Tell, they share a similar framework that uses CNN for feature extraction and RNN for caption generation, and Show-and-Tell is the vanilla version of this CNN+RNN architecture.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7699937025705973}, {"text": "feature extraction", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.7319674491882324}, {"text": "caption generation", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8655735552310944}]}, {"text": "Indeed, we find that the adversarial examples on Show-and-Tell are transferable to other image captioning models such as Show-Attend-and-Tell ( and NeuralTalk2 3 , suggesting that the attention mechanism and the choice of CNN and RNN architectures do not significantly affect the robustness.", "labels": [], "entities": []}, {"text": "We also note that since Show-and-Fool is the first work on crafting adversarial examples for neural image captioning, to the best of our knowledge, there is no other method for comparison.", "labels": [], "entities": [{"text": "neural image captioning", "start_pos": 93, "end_pos": 116, "type": "TASK", "confidence": 0.681291272242864}]}, {"text": "We use ADAM to minimize our loss functions and set the learning rate to 0.005.", "labels": [], "entities": []}, {"text": "The number of iterations is set to 1, 000.", "labels": [], "entities": []}, {"text": "All the experiments are performed on a single Nvidia GTX 1080 Ti GPU.", "labels": [], "entities": [{"text": "Nvidia GTX 1080 Ti GPU", "start_pos": 46, "end_pos": 68, "type": "DATASET", "confidence": 0.9536349654197693}]}, {"text": "For targeted caption and targeted keyword methods, we perform a binary search for 5 times to find the best c: initially c = 1, and c will be increased by 10 times until a successful adversarial example is found.", "labels": [], "entities": []}, {"text": "Then, we choose anew c to be the average of the largest c where an adversarial example can be found and the smallest c where an adversarial example cannot be found.", "labels": [], "entities": []}, {"text": "We fix = 1 except for transferability experiments.", "labels": [], "entities": []}, {"text": "For each experiment, we randomly select 1,000 images from the MSCOCO validation set.", "labels": [], "entities": [{"text": "MSCOCO validation set", "start_pos": 62, "end_pos": 83, "type": "DATASET", "confidence": 0.863613506158193}]}, {"text": "We use BLEU-1 (), BLEU-2, BLEU-3, BLEU-4, ROUGE) and METEOR) scores to evaluate the correlations between the inferred captions and the targeted captions.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.9981415271759033}, {"text": "BLEU-2", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9970523118972778}, {"text": "BLEU-3", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9936374425888062}, {"text": "BLEU-4", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9941805601119995}, {"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9824033975601196}, {"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9955034852027893}]}, {"text": "These scores are widely used in NLP community and are adopted by image captioning systems for quality assessment.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7074527144432068}]}, {"text": "Throughout this section, we use the logits loss (7)(9).", "labels": [], "entities": []}, {"text": "The results of using the log-prob loss (5) are similar and are reported in the supplementary material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of targeted caption method  (Section 3.2) and targeted keyword method (Sec- tion 3.3) using logits loss. The 2 distortion of  adversarial noise \u03b4 2 is averaged over success- ful adversarial examples. For comparison, we also  include CNN based attack methods (Section 4.5).", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the 4.2% failed adversarial  examples using the targeted caption method and  logits loss (7). All correlation scores are computed  using the top-5 inferred captions of an adversar- ial image and the targeted caption (higher score  means better targeted attack performance).", "labels": [], "entities": []}, {"text": " Table 3: Percentage of partial success with differ- ent c in the 4.0% failed images that do not contain  all the 3 targeted keywords.", "labels": [], "entities": []}, {"text": " Table 4: Transferability of adversarial examples from Show-and-Tell to Show-Attend-and-Tell, using  different and c. ori indicates the scores between the generated captions of the original images and the  transferred adversarial images on Show-Attend-and-Tell. tgt indicates the scores between the targeted  captions on Show-and-Tell and the generated captions of transferred adversarial images on Show-Attend- and-Tell. A smaller ori or a larger tgt value indicates better transferability. mis measures the differences  between captions generated by the two models given the same benign image (model mismatch). When  C = 1000, = 10, tgt is close to mis, indicating the discrepancy between adversarial captions on the two  models is mostly bounded by model mismatch, and the adversarial perturbation is highly transferable.", "labels": [], "entities": []}]}