{"title": [{"text": "CMMC-BDRC Solution to the NLP-TEA-2018 Chinese Grammatical Error Diagnosis Task", "labels": [], "entities": [{"text": "CMMC-BDRC", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.952565610408783}, {"text": "Chinese Grammatical Error Diagnosis", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.7059969007968903}]}], "abstractContent": [{"text": "Chinese grammatical error diagnosis is an important natural language processing (NLP) task, which is also an important application using artificial intelligence technology in language education.", "labels": [], "entities": [{"text": "Chinese grammatical error diagnosis", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7543224692344666}, {"text": "natural language processing (NLP) task", "start_pos": 52, "end_pos": 90, "type": "TASK", "confidence": 0.8163878577096122}]}, {"text": "This paper introduces a system developed by the Chinese Multilingual & Multimodal Corpus and Big Data Research Center for the NLP-TEA shared task, named Chinese Grammar Error Diagnosis (CGED).", "labels": [], "entities": [{"text": "Chinese Multilingual & Multimodal Corpus and Big Data Research Center", "start_pos": 48, "end_pos": 117, "type": "DATASET", "confidence": 0.7192002326250077}, {"text": "Chinese Grammar Error Diagnosis (CGED)", "start_pos": 153, "end_pos": 191, "type": "TASK", "confidence": 0.6980118496077401}]}, {"text": "This system regards diagnosing errors task as a sequence tagging problem, while takes correction task as a text classification problem.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 48, "end_pos": 64, "type": "TASK", "confidence": 0.6731313318014145}, {"text": "correction", "start_pos": 86, "end_pos": 96, "type": "METRIC", "confidence": 0.9302806854248047}, {"text": "text classification", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.7733127474784851}]}, {"text": "Finally, in the 12 teams, this system gets the highest F1 score in the detection task and the second highest F1 score in mean in the identification task, position task and the correction task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9869435727596283}, {"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9871760010719299}, {"text": "mean", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.8770046234130859}]}], "introductionContent": [{"text": "With the development of Chinese economy and the growing popularity of Chinese culture, more and more foreigners begin to learn Chinese.", "labels": [], "entities": []}, {"text": "However, Chinese and English are different.", "labels": [], "entities": []}, {"text": "For instance, Chinese grammar is more flexible and more complex than English grammar and there are few morphological changes in Chinese.", "labels": [], "entities": []}, {"text": "Consequently, it is quite difficult for the second language (L2) learners to master.", "labels": [], "entities": []}, {"text": "In addition, the huge number of Chinese characters and no space between word and word cause the difficulty in Chinese natural language processing.", "labels": [], "entities": [{"text": "Chinese natural language processing", "start_pos": 110, "end_pos": 145, "type": "TASK", "confidence": 0.6184514909982681}]}, {"text": "In short, regarding how to use artificial intelligence to correct L2 learners, Chinese writing meets both opportunities and challenges.", "labels": [], "entities": [{"text": "Chinese writing", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.661609560251236}]}, {"text": "In order to promote the development of automatic detection of syntactic errors in Chinese writing, the Natural Language Processing Techniques for Educational Applications (NLP-TEA) have taken CGED as one of the shared tasks since 2014.", "labels": [], "entities": [{"text": "automatic detection of syntactic errors in Chinese writing", "start_pos": 39, "end_pos": 97, "type": "TASK", "confidence": 0.7867946997284889}]}, {"text": "Thanks to the CGED task, some research achievements have been made in Chinese grammar error detection.", "labels": [], "entities": [{"text": "Chinese grammar error detection", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7303636223077774}]}, {"text": "Based on those previous research results, this paper puts forward anew thinking direction of enriching training dataset for the CGED task.", "labels": [], "entities": []}, {"text": "The structure of this article is as follows: Section 2 briefly introduces the CGED shared task.", "labels": [], "entities": []}, {"text": "Section 3 introduces some related work.", "labels": [], "entities": []}, {"text": "Section 4 talks about the methodology.", "labels": [], "entities": []}, {"text": "Section 5 presents the data augmentation method used in the system, and section 6 shows the experiment result.", "labels": [], "entities": []}, {"text": "Finally, conclusion and future work are drawn in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The steps of extracting rules from the training dataset of CGED are indicated as follows: (1) Count the number of sentences in each training document that contains the original error text and correct text, and discard documents that are not equal in number and cannot be corrected manually.", "labels": [], "entities": [{"text": "CGED", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.5752355456352234}]}, {"text": "(2) Split the original error text and correct text of each document into sentences by LPT toolkit.", "labels": [], "entities": [{"text": "LPT toolkit", "start_pos": 86, "end_pos": 97, "type": "DATASET", "confidence": 0.8829125761985779}]}, {"text": "(3) Each error of the sentence can generate an error rule.", "labels": [], "entities": []}, {"text": "The components of an error rule can be calculated based on the sentence original error text, correct text, and error interval.", "labels": [], "entities": [{"text": "error interval", "start_pos": 111, "end_pos": 125, "type": "METRIC", "confidence": 0.9458816647529602}]}, {"text": "The prefix and suffix can be a word or a character.", "labels": [], "entities": []}, {"text": "If it is a word, the left and right strings of the error word in the sentence need to do word segmentation respectively.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.6889889389276505}]}, {"text": "After the word segmentation, the prefix becomes the rightmost word of the left string, and the suffix is the leftmost word of the right string.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6990197449922562}]}, {"text": "For example, example 3 in contains a S type error.", "labels": [], "entities": []}, {"text": "Through the original text and error interval, we can know that '\u6559\u517b' is a bad word selection.", "labels": [], "entities": [{"text": "error interval", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9387691617012024}]}, {"text": "The content before '\u629a\u517b' in the correct text is the same as the content before '\u6559\u517b' in original text, and the content behind '\u629a\u517b' incorrect text is also the same as the content behind '\u6559\u517b' in original text.", "labels": [], "entities": []}, {"text": "This can be inferred that the correct writing of '\u6559\u517b' should be '\u629a\u517b' in this context.", "labels": [], "entities": []}, {"text": "Therefore, the rules 'S -\u6559\u517b-\u5b69\u5b50-\u629a\u517b-\u6210\u4eba' and 'S -\u6559\u517b-\u5b50-\u629a\u517b-\u6210' can be derived from the example 3.", "labels": [], "entities": []}, {"text": "Not all the correct form of an error word can be inferred.", "labels": [], "entities": []}, {"text": "It is difficult to infer the correct word if the following conditions occur: (1) Two errors have crossed position, or one error is contained in another.", "labels": [], "entities": []}, {"text": "(2) Two errors next to each other in position, but they area S type error and a M type error.", "labels": [], "entities": [{"text": "area S type error", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.7114177048206329}]}, {"text": "(3) Two errors next to each other in position, but one of them is a W type error.", "labels": [], "entities": []}, {"text": "We used the validation dataset to select the best hyper-parameters for both the LSTM-CRF model of DIP tasks and the classification model for correction task.", "labels": [], "entities": []}, {"text": "From the results of table 4, it has been found that the model with added trigram embeddings performs better than that with only character embedding and bigram embeddings when using the same dataset.", "labels": [], "entities": []}, {"text": "The model trained with increased new data is superior to the model that only trained with CGED dataset.", "labels": [], "entities": [{"text": "CGED dataset", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.9570498466491699}]}, {"text": "shows the results of the correction task.", "labels": [], "entities": []}, {"text": "MN refers to model N.", "labels": [], "entities": [{"text": "MN", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.613164484500885}]}, {"text": "For example, M2 refers to model 2.", "labels": [], "entities": []}, {"text": "N stands for the number of aforementioned prefixes and suffixes in section 5.1.", "labels": [], "entities": []}, {"text": "The smaller the N is, the more effective the model is.", "labels": [], "entities": []}, {"text": "In table 5, model 1 has the best predictive effect, while the other models can predict the correct suggestions rather than model 1.", "labels": [], "entities": []}, {"text": "Therefore, we take the results of model 1 as basis.", "labels": [], "entities": []}, {"text": "If three results of the other four models are inconsistent with those of model 1, they will betaken as the priority result.: Results on Validation Dataset of Correction task.", "labels": [], "entities": [{"text": "betaken", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9721901416778564}]}, {"text": "While testing on the final evaluation dataset, we merged all the training dataset and validation dataset, and added generated sentences to retrain our models.", "labels": [], "entities": []}, {"text": "show the final results of DIP tasks and correction task.", "labels": [], "entities": [{"text": "DIP tasks", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.7260520458221436}]}, {"text": "We used the same parameters for training 9 different models\uff0cbut obtained 9 different test results.", "labels": [], "entities": []}, {"text": "Hence, we selected the best performing model in detection task in evaluating dataset of 2017 as run 1, and the best performing model in position task in evaluating dataset of 2017 as run 2.", "labels": [], "entities": []}, {"text": "During this process, we didn't apply any model stacking.", "labels": [], "entities": []}, {"text": "Finally, 12 teams submitted 32 DIP task results.", "labels": [], "entities": []}, {"text": "The first run of our system (run1) achieved the highest F1 scores in the detection task.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9792451858520508}]}, {"text": "In the identification task, the F1 of run1 and run2 ranked the second and the third respectively.", "labels": [], "entities": [{"text": "identification task", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.9199345409870148}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9915359020233154}]}, {"text": "And in the position task, the F1 of run2 gained third place among 32 results.", "labels": [], "entities": [{"text": "F1", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9806118607521057}]}, {"text": "As for the correction task, the new task of this year, 9 teams submitted a total of 23 results.", "labels": [], "entities": [{"text": "correction task", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.9190450310707092}]}, {"text": "Run2 got better result than run1 in both top1 and top3 tasks.", "labels": [], "entities": [{"text": "Run2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8504677414894104}]}, {"text": "In top1 correction task, the F1 of run2 ranked 2/9 according to teams and 2/23 according to results, which is lower than the highest result by only 0.0001.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9970688223838806}]}, {"text": "In top3 correction task, the F1 of run2 ranked 2/9 according to teams and 3/23 according to results.", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9945573806762695}]}], "tableCaptions": [{"text": " Table 1: Two examples of training sentence of the CGED training dataset.", "labels": [], "entities": [{"text": "CGED training dataset", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.9353633721669515}]}, {"text": " Table 3: An example of training sentence generated from an error rule 'S-\u6559\u517b-\u5b50-\u629a\u517b-\u6210'.", "labels": [], "entities": []}, {"text": " Table 4: Results on Validation Dataset of  DIP tasks. CGED indicates that only CGED  training dataset is used. G stands for using  generated dataset, U stands for character em- bedding, B stands for bigram embeddings, and  T stands for trigram embeddings.", "labels": [], "entities": [{"text": "CGED  training dataset", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.7779554128646851}]}, {"text": " Table 5: Results on Validation Dataset of  Correction task.", "labels": [], "entities": []}, {"text": " Table 6: Results on Evaluation Dataset of  DIP Tasks.", "labels": [], "entities": []}]}