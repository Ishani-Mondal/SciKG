{"title": [{"text": "Learning from Chunk-based Feedback in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6534719268480936}]}], "abstractContent": [{"text": "We empirically investigate learning from partial feedback in neural machine translation (NMT), when partial feedback is collected by asking users to highlight a correct chunk of a translation.", "labels": [], "entities": [{"text": "learning from partial feedback in neural machine translation (NMT)", "start_pos": 27, "end_pos": 93, "type": "TASK", "confidence": 0.752033447677439}]}, {"text": "We propose a simple and effective way of utilizing such feedback in NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9379703402519226}]}, {"text": "We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7176762074232101}]}, {"text": "We conduct a series of simulation experiments to test the effectiveness of the proposed method.", "labels": [], "entities": []}, {"text": "Our results show that chunk-level feedback out-performs sentence based feedback by up to 2.61% BLEU absolute.", "labels": [], "entities": [{"text": "BLEU absolute", "start_pos": 95, "end_pos": 108, "type": "METRIC", "confidence": 0.9658368825912476}]}], "introductionContent": [{"text": "In recent years, machine translation (MT) quality improved rapidly, especially because of advances in neural machine translation (NMT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8884168863296509}, {"text": "neural machine translation (NMT)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.8307986756165823}]}, {"text": "Most of remaining MT errors arguably come from domain, style, or terminology mismatch between the data on which the MT was trained on and data which it has to translate.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9855658411979675}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9281026124954224}]}, {"text": "It is hard to alleviate this mismatch since usually only limited amounts of relevant training data are available.", "labels": [], "entities": []}, {"text": "Yet MT systems deployed on-line in e.g. e-commerce websites or social networks can benefit from user feedback for overcoming this mismatch.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9745666980743408}]}, {"text": "Whereas MT users are usually not bilingual, they likely have a good command of the target language and are able to spot severe MT errors in a given translated sentence, sometimes with the help of e.g. an accompanying image, video, or simply prior knowledge.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9769030809402466}, {"text": "MT errors in a given translated sentence", "start_pos": 127, "end_pos": 167, "type": "TASK", "confidence": 0.7458392637116569}]}, {"text": "A common approach to get user feedback for MT is explicit ratings of translations on an n-point Likert scale.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9944846034049988}]}, {"text": "The main problem of such methods is that users are not qualified enough to provide reliable feedback for the whole sentence.", "labels": [], "entities": []}, {"text": "Since different users do not adhere to a single set of guidelines, their ratings maybe influenced by various factors, such as user expectations, user knowledge, or user satisfaction with the platform.", "labels": [], "entities": []}, {"text": "In (, the authors investigate the reliability and validity of real user ratings by re-evaluating five-star ratings by three independent human annotators, however the inter-annotator agreement between experts was relatively low and no correlation to the averaged user rating was found.", "labels": [], "entities": []}, {"text": "Instead of providing a rating, a user might be asked to correct the generated translation, in a process called post-editing.", "labels": [], "entities": []}, {"text": "Using corrected sentences for training an NMT system brings larger improvements, but this method requires significant effort and expertise from the user.", "labels": [], "entities": []}, {"text": "Alternatively, feedback can be collected by asking users to mark correct parts (chunks) of the translation.", "labels": [], "entities": []}, {"text": "It can be seen as the middle ground between quick sentence level rating and more expensive post-editing.", "labels": [], "entities": [{"text": "sentence level rating", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.5857826968034109}]}, {"text": "We hypothesize that collecting feedback in this form implicitly forces guidelines on the user, making it less susceptible to various user-dependent factors.", "labels": [], "entities": []}, {"text": "We expect marking of correct chunks in a translation to be simple enough for non-experts to do quickly and precisely and also be more intuitive than providing a numerical rating.", "labels": [], "entities": [{"text": "marking of correct chunks in a translation", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.6652929101671491}]}, {"text": "In this paper, we investigate the empirical hypothesis that NMT is able to learn from the good chunks of a noisy sentence and describe a simple way of utilizing such chunk-level feedback in NMT training.", "labels": [], "entities": [{"text": "NMT training", "start_pos": 190, "end_pos": 202, "type": "TASK", "confidence": 0.8809649050235748}]}, {"text": "To the best of our knowledge, no dataset with human feedback recorded in this form is available, therefore we experiment with user feedback that was artificially created from parallel data.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we review related work.", "labels": [], "entities": []}, {"text": "We describe our partial feedback approach in Section 3.", "labels": [], "entities": []}, {"text": "Next we present our experimental results in Section 4, followed by the conclusion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conduct a series of experiments to study how well an NMT system is able to learn only from partial user feedback when this feedback is given for in-domain translations, whereas the baseline system is trained on out-of-domain data.", "labels": [], "entities": []}, {"text": "We report results on two datasets: WMT 2017 German to English news translation task () and an in-house English to Spanish dataset in the e-commerce domain.", "labels": [], "entities": [{"text": "WMT 2017 German to English news translation task", "start_pos": 35, "end_pos": 83, "type": "TASK", "confidence": 0.8441888093948364}]}, {"text": "On all data we apply byte-pair encoding () with 40,000 merge operations learned separately for each language.", "labels": [], "entities": []}, {"text": "For each dataset we separate the larger outof-domain and smaller in-domain training data.", "labels": [], "entities": []}, {"text": "For De-En we use 1.8M sentence pairs randomly sampled from available parallel corpora as outof-domain data and 800K sentence pairs sampled from back-translated monolingual and unused parallel corpora as in-domain data.", "labels": [], "entities": []}, {"text": "For En-Es we have 2.7M out-of-domain and 1.5M in-domain sentence pairs.", "labels": [], "entities": []}, {"text": "We evaluate our models on newstest2016 (2999 sentence pairs) for the De-En task and an in-house test set of 1000 sentence pairs for the En-Es task using case-insensitive BLEU) and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9685268998146057}, {"text": "TER", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.998002827167511}]}, {"text": "We have implemented our NMT model using TensorFlow () library.", "labels": [], "entities": []}, {"text": "Our encoder is a bidirectional LSTM with a layer size of 512; our decoder is an LSTM with 2 layers of the same size.", "labels": [], "entities": []}, {"text": "We also use embedding size of 512 and MLP attention layer.", "labels": [], "entities": []}, {"text": "We train our networks using SGD with a learning rate schedule that starts gradually decaying to 0.01 after the initial 4 epochs.", "labels": [], "entities": []}, {"text": "As regularization we use dropout on the RNN inputs with dropping probability of 0.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Chunk-level feedback compared to  sentence-level feedback. Self-training is equiva- lent to having no feedback or setting all w i = 1, \u2200i  in the training objective in Eq. (2). sent-sBLEU  and sent-binary are sentence-level methods with  sentence BLEU and binary weighting rules, de- fined as in Section 4.2. chunk-match and chunk- lcs-level feedback refers to assigning w i using  simple matching or LCS method described in Sec- tion 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 258, "end_pos": 262, "type": "METRIC", "confidence": 0.863627016544342}]}, {"text": " Table 2: Impact of user errors on the translation  performance. Under selection ratio% indicates  on average what percentage of words in a correct  chunk have not been selected in user simulation,  but all selected words are correct. Incorrect se- lection ratio% indicates what percentage of words  are incorrectly selected, here the total number of  marked words is the same as in chunk-level feed- back. In the last row, 10% of marked words are  actually incorrect and the total number of marked  words is 25% less compared to system in row 1.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9619847536087036}, {"text": "Incorrect se- lection ratio", "start_pos": 235, "end_pos": 262, "type": "METRIC", "confidence": 0.9211199879646301}]}]}