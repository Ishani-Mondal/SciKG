{"title": [{"text": "Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "Fine-tuning is a popular method to achieve better performance when only a small target corpus is available.", "labels": [], "entities": []}, {"text": "However, it requires tuning of a number of meta-parameters and thus it might carry risk of adverse effect when inappropriate meta-parameters are used.", "labels": [], "entities": []}, {"text": "Therefore, we investigate effective parameters for fine-tuning when only a small target corpus is available.", "labels": [], "entities": []}, {"text": "In the current study, we target at improving Japanese word embeddings created from a huge corpus.", "labels": [], "entities": []}, {"text": "First, we demonstrate that even the word embeddings created from the huge corpus are affected by domain shift.", "labels": [], "entities": []}, {"text": "After that, we investigate effective parameters for fine-tuning of the word embeddings using a small target corpus.", "labels": [], "entities": []}, {"text": "We used perplexity of a language model obtained from a Long Short-Term Memory network to assess the word em-beddings input into the network.", "labels": [], "entities": []}, {"text": "The experiments revealed that fine-tuning sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for fine-tuning.", "labels": [], "entities": []}, {"text": "In addition, we confirmed that effect of fine-tuning is higher when size of a target corpus was larger.", "labels": [], "entities": []}], "introductionContent": [{"text": "We investigate effective parameters for finetuning using nwjc2vec.", "labels": [], "entities": [{"text": "nwjc2vec", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.9430369734764099}]}, {"text": "Nwjc2vec is Japanese word2vec (the word embeddings proposed by) created from NINJAL Web Japanese Corpus (NWJC) ().", "labels": [], "entities": [{"text": "Nwjc2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9533625245094299}, {"text": "NINJAL Web Japanese Corpus (NWJC)", "start_pos": 77, "end_pos": 110, "type": "DATASET", "confidence": 0.8942492774554661}]}, {"text": "It contains 25.8 billion words as a whole.", "labels": [], "entities": []}, {"text": "Therefore, it is believed that nwjc2vec is high-quality.", "labels": [], "entities": [{"text": "nwjc2vec", "start_pos": 31, "end_pos": 39, "type": "DATASET", "confidence": 0.8989260792732239}]}, {"text": "In fact, some models used it showed better results () () ().", "labels": [], "entities": []}, {"text": "In addition, it is also believed that nwjc2vec is useful for various documents because it contains a number of documents described about various topics.", "labels": [], "entities": [{"text": "nwjc2vec", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9385876059532166}]}, {"text": "However, we show that a problem posed by domain shift occurs when nwjc2vec is used in the current study.", "labels": [], "entities": []}, {"text": "(See Section 4) The simplest and most effective approach to address the problem caused from domain shift of word embeddings is fine-tuning using a large target corpus.", "labels": [], "entities": []}, {"text": "However, in practice, we often face the situation where only a small corpus of the target domain is available.", "labels": [], "entities": []}, {"text": "It is possible to use other resources than a corpus, but they are not always available.", "labels": [], "entities": []}, {"text": "Therefore, in the current study, we investigate effective parameters for word2vec, which is a program to create word embeddings, when we fine-tune nwjc2vec using only a small target corpus.", "labels": [], "entities": []}, {"text": "(See Section 5) We evaluate the word embeddings via language models obtained from a LSTM (Long Short-Term Memory)) () ( (See Section 3).", "labels": [], "entities": []}, {"text": "First, we develop a language model using a LSTM.", "labels": [], "entities": []}, {"text": "Usually, word embeddings are learned from the same corpus as a training corpus fora language model.", "labels": [], "entities": []}, {"text": "In other words, when we have only a small target corpus, we use the word embeddings learned from the target corpus for the inputs for the LSTM that develops a language model.", "labels": [], "entities": []}, {"text": "However, we input nwjc2vec fine-tuned using the small corpus into the LSTM instead of the word embeddings directly learned from the corpus.", "labels": [], "entities": []}, {"text": "We evaluate the language model to assess the fine-tuned word embeddings assuming that the quality of the output language model is higher when the quality of the word embeddings used in the LSTM is higher.", "labels": [], "entities": []}, {"text": "The experiments revealed that the batch size is the most important parameter for word2vec to fine-tune nwjc2vec using a small corpus.", "labels": [], "entities": []}, {"text": "In addition, they also showed that fine-tuning using inappropriate parameters sometimes make performance worse.", "labels": [], "entities": []}, {"text": "Moreover, we confirmed that size of the corpus is crucial for fine-tuning.", "labels": [], "entities": []}, {"text": "(See Sections 6 and 7)", "labels": [], "entities": []}], "datasetContent": [{"text": "Embeddings Using a LSTM In the current study, we used a LSTM, which is an extended version of an a RNN to evaluate the word embeddings fora certain domain as ().", "labels": [], "entities": []}, {"text": "We developed a language model using a LSTM from a training corpus and calculated the perplexity of the language model fora test corpus.", "labels": [], "entities": []}, {"text": "Perplexity is given by the following equation.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9776545166969299}]}, {"text": "where H is entropy given by the following equation.", "labels": [], "entities": []}, {"text": "where D denotes a size of test data, M denotes a language model, and W i denotes i th word in the test data.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the word embeddings depending on the perplexity assuming that the quality of the output language model is higher when the quality of the word embeddings used in the LSTM is higher.", "labels": [], "entities": []}, {"text": "Usually, word embeddings are learned from the same corpus as the training corpus fora language model.", "labels": [], "entities": []}, {"text": "However, we used the word embeddings to be evaluated instead of the word embeddings learned together with the language model (cf. Figure1).", "labels": [], "entities": []}, {"text": "We believe that we can evaluate the quality of the word embeddings by evaluating the perplexity of the language model when they are used in a LSTM.", "labels": [], "entities": []}, {"text": "We developed the language models through the LSTMs.", "labels": [], "entities": []}, {"text": "We used the five fine-tuned word embeddings described above, base emb, win emb, batch20 emb, batch100 emb, and epch emb, and used 100,000 sentences randomly extracted from Mainichi Shimbun Newspaper in from 1993 to  1999 to train the LSTMs.", "labels": [], "entities": [{"text": "Mainichi Shimbun Newspaper in from 1993", "start_pos": 172, "end_pos": 211, "type": "DATASET", "confidence": 0.9502051572004954}]}, {"text": "We calculated perplexities of the language models obtained from the LSTMs at each epoch using the test data, which was 10,000 sentences from the same corpus as the training data.", "labels": [], "entities": []}, {"text": "These is no overlap among the data for the fine-tuning, the training, and the testing.", "labels": [], "entities": []}, {"text": "summarizes the number of sentences of each corpus. and show the results.", "labels": [], "entities": []}, {"text": "They include the perplexities of the language model obtained from the LSTMs when original nwjc2vec was used without the fine-tuning.", "labels": [], "entities": []}, {"text": "The asterisks in the table mean that the language model using the fine-tuned word embeddings was better than that using nwjc2vec.", "labels": [], "entities": []}, {"text": "These results show that the perplexities of the language model decrease only when batch100 emb is used.", "labels": [], "entities": []}, {"text": "It indicates that fine-tuning is only effective when the batch size parameter is changed into 100.", "labels": [], "entities": []}, {"text": "Other parameter changes made the results worse.", "labels": [], "entities": []}, {"text": "The experiments revealed that fine-tuning has an opposite effect when unsuitable parameters are used in the case where small corpora are used.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora Used for Word2vec and Training and Test Data for Language Model for Blogs and Q  & A Sites  Name of model Word2Vec corpus  Training data Test data  mai2vec-lm-1  Newspaper in from 1993 to 1999  Blogs  nwjc2vec-lm-1 NWJC (Web pages)  And  base-lm-1  Blogs and Q & A sites  Q & A sites", "labels": [], "entities": [{"text": "Word2Vec corpus  Training data Test data  mai2vec-lm-1  Newspaper in from 1993 to 1999  Blogs  nwjc2vec-lm-1 NWJC", "start_pos": 124, "end_pos": 237, "type": "DATASET", "confidence": 0.852528516203165}]}, {"text": " Table 2: Evaluation of Language Models Obtained  from Each Word Embeddings 1  base-lm-1  mai2vec-lm-1 nwjc2vec-lm-1  130.35  124.72  118.68", "labels": [], "entities": []}, {"text": " Table 3: Corpora Used for Word2vec and Training and Test Data for Language Model for Newspapers  Name of model Word2Vec corpora  Training data Test data  mai2vec-lm-2  Newspaper in from 1993 to 1999 Newspaper  Newspaper  nwjc2vec-lm-2 NWJC (Web pages)  In  In  base-lm-2  Newspaper in 2007  2007  2008", "labels": [], "entities": [{"text": "Word2Vec corpora  Training data Test data  mai2vec-lm-2  Newspaper in from 1993 to 1999 Newspaper  Newspaper  nwjc2vec-lm-2 NWJC", "start_pos": 112, "end_pos": 240, "type": "DATASET", "confidence": 0.8060980281409096}]}, {"text": " Table 4: Evaluation of Language Models Obtained  from Each Word Embeddings  base-lm-2  mai2vec-lm-2 nwjc2vec-lm-2  81.52  64.81  67.47", "labels": [], "entities": []}, {"text": " Table 5: Corpus Data for Domain Shift Experiments  Corpus  Type  Aim  Genre  Number of Sentences  NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec of base-lm-1  Blogs  7,226  Language model  And  BCCWJ  Test  For blogs and Q &A sites Q & A sites  104  Mainichi Shimbun 2007  Training Word2vec of base-lm-2  100,000  Language model  Newspaper  Mainichi Shimbun 2008  Test  For newspaper  10,000", "labels": [], "entities": [{"text": "Domain Shift Experiments  Corpus  Type  Aim  Genre", "start_pos": 26, "end_pos": 76, "type": "TASK", "confidence": 0.8278185384614127}, {"text": "NWJC  Training Nwjc2vec  Web pages  1,463,142,939  Mainichi Shimbun 1993-1999 Training Mai2vec  Newspaper  6,791,403  BCCWJ  Training Word2vec", "start_pos": 99, "end_pos": 241, "type": "DATASET", "confidence": 0.862405389547348}, {"text": "Mainichi Shimbun 2007  Training Word2vec", "start_pos": 347, "end_pos": 387, "type": "DATASET", "confidence": 0.9242715954780578}, {"text": "Mainichi Shimbun 2008  Test", "start_pos": 438, "end_pos": 465, "type": "DATASET", "confidence": 0.8636067509651184}]}, {"text": " Table 6: Standard Parameters for Word2vec  Model Name  base emb  Number of Units  200  Window Size  5  Batch Size  10  Epoch Number  10  Used Model  skip-gram", "labels": [], "entities": [{"text": "Word2vec  Model Name  base emb", "start_pos": 34, "end_pos": 64, "type": "DATASET", "confidence": 0.8383223652839661}]}, {"text": " Table 7: Standard Parameters for Word2vec  Model Names  win emb batch20 emb batch100 emb epoch emb  Number of Units  200  Window Size  8  5  5  5  Batch Size  10  20  100  10  Epoch Number  10  10  10  20  Used Model  skip-gram", "labels": [], "entities": []}, {"text": " Table 8: Corpus Data for Fine-tuning Experiments  Corpus  Type  Aim  Genre  Number of Sentences  Mainichi Shimbun Fine-tuning Word2vec  100,000  1993- Training  Language Newspaper  100,000  1999  Test  Model  10,000", "labels": [], "entities": [{"text": "Mainichi Shimbun Fine-tuning Word2vec  100,000  1993- Training  Language Newspaper  100,000  1999  Test  Model  10,000", "start_pos": 98, "end_pos": 216, "type": "DATASET", "confidence": 0.8537704666455587}]}, {"text": " Table 9: Perplexities of Various Settings  epoch nwjc2vec base emb win emb batch20 emb batch100 emb epch emb  1  91.03  93.70  95.36  91.51  89.69  95.06  2  73.20  75.21  75.71  73.43  72.36  75.89  3  68.65  70.21  70.52  68.69  67.54  70.30  4  67.43  68.85  69.33  67.56  66.23*  68.46  5  67.52  68.84  69.51  67.70  66.35*  68.17  6  68.17  69.55  70.20  68.37  67.13*  68.54  7  69.08  70.37  71.11  69.37  68.17  69.29  8  70.06  71.48  72.22  70.56  69.37  70.36  9  71.09  72.71  73.40  71.80  70.58  71.49  10  72.18  73.92  74.66  73.06  71.82  72.68", "labels": [], "entities": []}]}