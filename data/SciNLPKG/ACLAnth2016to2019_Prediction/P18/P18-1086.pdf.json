{"title": [{"text": "What Action Causes This? Towards Naive Physical Action-Effect Prediction", "labels": [], "entities": [{"text": "Naive Physical Action-Effect Prediction", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.9158139228820801}]}], "abstractContent": [{"text": "Despite recent advances in knowledge representation , automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7330549657344818}]}, {"text": "If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions.", "labels": [], "entities": []}, {"text": "Towards this goal, this paper introduces anew task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images.", "labels": [], "entities": [{"text": "physical action-effect prediction", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.8028975526491801}]}, {"text": "We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.8155132830142975}]}, {"text": "Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning.", "labels": [], "entities": []}, {"text": "This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "Causation in the physical world has long been a central discussion to philosophers who study casual reasoning and explanation, to mathematicians or computer scientists who apply computational approaches to model cause-effect prediction (, and to domain experts (e.g., medical doctors) who attempt to understand the underlying cause-effect relations (e.g., disease and symptoms) for their particular inquires.", "labels": [], "entities": [{"text": "casual reasoning and explanation", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.7026398926973343}, {"text": "cause-effect prediction", "start_pos": 212, "end_pos": 235, "type": "TASK", "confidence": 0.7498844265937805}]}, {"text": "Apart from this wide range of topics, this paper investigates a specific kind of causation, the very basic causal relations between a concrete action (expressed in the form of a verb-noun pair such as \"cut-cucumber\") and the change of the physical state caused by this action.", "labels": [], "entities": []}, {"text": "We call such relations naive physical action-effect relations.", "labels": [], "entities": []}, {"text": "For example, given an image as shown in, we would have no problem predicting what actions can cause the state of the world depicted in the image, e.g., slicing an apple will likely lead to the state.", "labels": [], "entities": []}, {"text": "On the other hand, given a statement \"slice an apple\", it would not be hard for us to imagine what state change may happen to the apple.", "labels": [], "entities": []}, {"text": "We can make such action-effect prediction because we have developed an understanding of this kind of basic action-effect relations at a very young age.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7580836713314056}]}, {"text": "Will artificial agents be able to make the same kind of predictions?", "labels": [], "entities": []}, {"text": "The answer is not yet.", "labels": [], "entities": []}, {"text": "Despite tremendous progress in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the understanding of naive causal relations regarding the physical world.", "labels": [], "entities": [{"text": "knowledge representation", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7165683954954147}]}, {"text": "This is one of the bottlenecks in machine intelligence.", "labels": [], "entities": []}, {"text": "If artificial agents ever become capable of working with humans as partners, they will need to have this kind of physical action-effect understanding to help them reason, learn, and perform actions.", "labels": [], "entities": []}, {"text": "To address this problem, this paper introduces anew task on naive physical action-effect prediction.", "labels": [], "entities": [{"text": "physical action-effect prediction", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.6542519827683767}]}, {"text": "This task supports both cause predic- tion: given an image which describes a state of the world, identify the most likely action (in the form of a verb-noun pair, from a set of candidates) that can result in that state; and effect prediction: given an action in the form of a verb-noun pair, identify images (from a set of candidates) that depicts the most likely effects on the state of the world caused by that action.", "labels": [], "entities": [{"text": "effect prediction", "start_pos": 224, "end_pos": 241, "type": "TASK", "confidence": 0.7213757485151291}]}, {"text": "Note that there could be different ways to formulate this problem, for example, both causes and effects are in the form of language or in the form of images/videos.", "labels": [], "entities": []}, {"text": "Here we intentionally frame the action as a language expression (i.e., a verb-noun pair) and the effect as depicted in an image in order to make a connection between language and perception.", "labels": [], "entities": []}, {"text": "This connection is important for physical agents that not only can perceive and act, but also can communicate with humans in language.", "labels": [], "entities": []}, {"text": "As a first step, we collected a dataset of 140 verb-noun pairs.", "labels": [], "entities": []}, {"text": "Each verb-noun pair is annotated with possible effects described in language and depicted in images (where language descriptions and image descriptions are collected separately).", "labels": [], "entities": []}, {"text": "We have developed an approach that applies distant supervision to harness web data for bootstrapping action-effect prediction models.", "labels": [], "entities": [{"text": "bootstrapping action-effect prediction", "start_pos": 87, "end_pos": 125, "type": "TASK", "confidence": 0.596978118022283}]}, {"text": "Our empirical results have shown that, using a simple bootstrapping strategy, our approach can combine the noisy web data with a small number of seed examples to improve action-effect prediction.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 170, "end_pos": 194, "type": "TASK", "confidence": 0.8241404294967651}]}, {"text": "In addition, fora new verb-noun pair, our approach can infer its effect descriptions and predict action-effect relations only based on 3 image examples.", "labels": [], "entities": []}, {"text": "The contributions of this paper are three folds.", "labels": [], "entities": []}, {"text": "First, it introduces anew task on physical actioneffect prediction, a first step towards an understanding of causal relations between physical actions and the state of the physical world.", "labels": [], "entities": [{"text": "physical actioneffect prediction", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7067152460416158}]}, {"text": "Such ability is central to robots which not only perceive from the environment, but also act to the environment through planning.", "labels": [], "entities": []}, {"text": "To our knowledge, there is no prior work that attempts to connect actions (in language) and effects (in images) in this nature.", "labels": [], "entities": []}, {"text": "Second, our approach harnesses the large amount of image data available on the web with minimum supervision.", "labels": [], "entities": []}, {"text": "It has shown that physical action-effect models can be learned through a combination of a few annotated examples and a large amount of un-annotated web data.", "labels": [], "entities": []}, {"text": "This opens up the possibility for humans to teach robots new tasks through language communication with a small number of examples.", "labels": [], "entities": []}, {"text": "Third, we have created a dataset for this task, which is available to the community 1 . Our bootstrapping approach can serve as a baseline for future work on this topic.", "labels": [], "entities": []}, {"text": "In the following sections, we first describe our data collection effort, then introduce the bootstrapping approach for action-effect prediction, and finally present results from our experiments.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.8465715944766998}]}], "datasetContent": [{"text": "We divided the 140 verb-noun pairs into 70% training set (98 verb-noun pairs), 10% development set (14) and 20% test set (28).", "labels": [], "entities": []}, {"text": "For the actioneffect embedding model, we use pre-trained GloVe word embeddings () as input to the LSTM.", "labels": [], "entities": []}, {"text": "The embedding model was trained using the language effect data corresponding to the training verb-noun pairs, and then it was applied to predict effect phrases for the unseen verb-noun pairs in the test set.", "labels": [], "entities": []}, {"text": "For each unseen verb-noun pair, we collected its top five predicted effect phrases.", "labels": [], "entities": []}, {"text": "Each predicted effect phrase was then used as query keywords to download web effect images.", "labels": [], "entities": []}, {"text": "This set of web images are referred to as pEff and will be used in training the actioneffect prediction model.", "labels": [], "entities": [{"text": "actioneffect prediction", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.7886142432689667}]}, {"text": "For each of the 28 test (i.e., new) verb-noun pairs, we use the same ratio 10% (about 3 examples) of the human annotated images as the seeding images, which were combined with downloaded web images to train the prediction model.", "labels": [], "entities": []}, {"text": "The remaining 30% and 60% are used as the development set, and the test set.", "labels": [], "entities": []}, {"text": "We compare the following different configurations: (1) BS+Seed+Act+pEff.", "labels": [], "entities": []}, {"text": "The bootstrapping approach trained on the seeding images, the action web images, and the web images downloaded using the predicted effect phrases.", "labels": [], "entities": []}, {"text": "(2) BS+Seed+Act+Eff.", "labels": [], "entities": [{"text": "BS+Seed+Act+Eff", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.6798399005617414}]}, {"text": "The bootstrapping approach trained on the seeding images, the action web images, and the effect web images (downloaded using ground-truth effect phrases).", "labels": [], "entities": []}, {"text": "(3) BS+Seed+Act.", "labels": [], "entities": [{"text": "BS+Seed+Act.", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7481025258700053}]}, {"text": "The bootstrapping approach trained on the seeding images and the action web show the results for the action-effect prediction task for unseen verbnoun pairs.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.7488269209861755}]}, {"text": "From the results we can see that BS+Seed+Act+pEff achieves close performance compared with BS+Seed+Act+Eff, which uses human annotated effect phrases.", "labels": [], "entities": [{"text": "BS+Seed+Act+Eff", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.7957491959844317}]}, {"text": "Although inmost cases, BS+Seed+Act+pEff outperforms the baseline, which seems to point to the possibility that semantic embedding space can be employed to extend effect knowledge to new verb-noun pairs.", "labels": [], "entities": []}, {"text": "However, the current results are not conclusive partly due to the small testing set.", "labels": [], "entities": []}, {"text": "More in-depth evaluation is needed in the future.", "labels": [], "entities": []}, {"text": "shows top predicted effect phrases for several new verb-noun pairs.", "labels": [], "entities": []}, {"text": "After analyzing the action-effect prediction results we notice that generalizing the effect knowledge to a verb-noun pair that contains an unseen verb tends to be more difficult than generalizing to a verb-noun pair that contains an unseen noun.", "labels": [], "entities": []}, {"text": "Among the 28 test verbnoun pairs, 12 of them contain unseen verbs and known nouns, 7 of them contain unseen nouns and known verbs.", "labels": [], "entities": []}, {"text": "For the task of ranking images given an action, the mean average precision is 0.447 for the unseen verb cases and 0.584 for the unseen noun cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9611698389053345}]}, {"text": "Although not conclusive, this might indicate that, verbs tend to capture more information about the effect states of the world than nouns.", "labels": [], "entities": []}, {"text": "We apply the trained classification model to all of the test images.", "labels": [], "entities": []}, {"text": "Based on the matrix of prediction scores, we can evaluate action-effect prediction from two angles: (1) given an action class, rank all the candidate images; (2) given an image, rank all the candidate action classes.", "labels": [], "entities": [{"text": "action-effect prediction", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8159281313419342}]}, {"text": "show the results for these two angels respectively.", "labels": [], "entities": []}, {"text": "We report both mean average precision (MAP) and top prediction accuracy.", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 15, "end_pos": 43, "type": "METRIC", "confidence": 0.9343628187974294}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.678593635559082}]}, {"text": "Overall, BS+Seed+Act+Eff gives the best performance.", "labels": [], "entities": [{"text": "BS+Seed+Act+Eff", "start_pos": 9, "end_pos": 24, "type": "METRIC", "confidence": 0.634699033839362}]}, {"text": "By comparing the bootstrap approach with baseline approaches (i.e., BS+Seed+Act+Eff", "labels": [], "entities": [{"text": "BS+Seed+Act+Eff", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.7549226028578622}]}], "tableCaptions": [{"text": " Table 3: Results for the action-effect prediction  task (given an action, rank all the candidate im- ages).", "labels": [], "entities": [{"text": "action-effect prediction  task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8485175569852194}]}, {"text": " Table 4: Results for the action-effect prediction  task (given an image, rank all the actions).", "labels": [], "entities": [{"text": "action-effect prediction  task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8485113581021627}]}, {"text": " Table 5: Results for the action-effect prediction  task (given an action, rank all the candidate im- ages).", "labels": [], "entities": [{"text": "action-effect prediction  task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8487070401509603}]}, {"text": " Table 6: Results for the action-effect prediction  task (given an image, rank all the actions).", "labels": [], "entities": [{"text": "action-effect prediction  task", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.848513106505076}]}]}