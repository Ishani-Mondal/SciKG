{"title": [{"text": "GNEG: Graph-Based Negative Sampling for word2vec", "labels": [], "entities": []}], "abstractContent": [{"text": "Negative sampling is an important component in word2vec for distributed word representation learning.", "labels": [], "entities": [{"text": "Negative sampling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7772957384586334}, {"text": "distributed word representation learning", "start_pos": 60, "end_pos": 100, "type": "TASK", "confidence": 0.7064457833766937}]}, {"text": "We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution.", "labels": [], "entities": []}, {"text": "In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk.", "labels": [], "entities": []}, {"text": "We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.7484819889068604}]}], "introductionContent": [{"text": "Negative sampling, as introduced by, is used as a standard component in both the CBOW and skip-gram models of word2vec.", "labels": [], "entities": [{"text": "Negative sampling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6821714043617249}, {"text": "CBOW", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9115152359008789}]}, {"text": "For practical reasons, instead of using a softmax function, earlier work explored different alternatives which approximate the softmax in a computationally efficient way.", "labels": [], "entities": []}, {"text": "These alternative methods can be roughly divided into two categories: softmax-based approaches (hierarchical softmax), differentiated softmax and CNNsoftmax () and sampling-based approaches (importance sampling (), target sampling (), noise contrastive estimation) and negative sampling).", "labels": [], "entities": []}, {"text": "Generally speaking, among all these methods, negative sampling is the best choice for distributed word representation learning.", "labels": [], "entities": [{"text": "distributed word representation learning", "start_pos": 86, "end_pos": 126, "type": "TASK", "confidence": 0.7022765874862671}]}, {"text": "Negative sampling replaces the softmax with binary classifiers.", "labels": [], "entities": []}, {"text": "For instance, in the skip-gram model, word representations are learned by predicting a training word's surrounding words given this training word.", "labels": [], "entities": []}, {"text": "When training, correct surrounding words provide positive examples in contrast to a set of sampled negative examples (noise).", "labels": [], "entities": []}, {"text": "To find these negative examples, a noise distribution is empirically defined as the unigram distribution of the words to the 3/4 th power: (1) Although this noise distribution is widely used and significantly improves the distributed word representation quality, we believe there is still room for improvement in the two following aspects: First, the unigram distribution only takes into account word frequency, and provides the same noise distribution when selecting negative examples for different target words.", "labels": [], "entities": []}, {"text": "already showed that a context-dependent noise distribution could be a better solution to learn a language model.", "labels": [], "entities": []}, {"text": "But they only use information on adjacent words.", "labels": [], "entities": []}, {"text": "Second, unlike the positive target words, the meaning of negative examples remain unclear: For a training word, we do not know what a good noise distribution should be, while we do know what a good target word is (one of its surrounding words).", "labels": [], "entities": []}, {"text": "Our contributions: To address these two problems, we propose anew graph-based method to calculate noise distribution for negative sampling.", "labels": [], "entities": []}, {"text": "Based on a word co-occurrence network, our noise distribution is targeted to training words.", "labels": [], "entities": []}, {"text": "Besides, through our empirical exploration of the noise distribution, we get a better understanding of the meaning of 'negative' and of the characteristics of good noise distributions.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 defines the word co-occurrence network concepts and introduces our graph-based negative sampling approach.", "labels": [], "entities": []}, {"text": "Section 3 shows the experimental settings and results, then discusses our understanding of the good noise distributions.", "labels": [], "entities": []}, {"text": "Finally, Section 4 draws conclusions and mentions future work directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the skip-gram negative sampling model with window size 5, vocabulary size 10000, vector dimension size 200, number of iterations 5 and negative examples 5 to compute baseline word embeddings.", "labels": [], "entities": []}, {"text": "Our three types of graph-based skipgram negative sampling models share the parameters of the baseline.", "labels": [], "entities": []}, {"text": "In addition to these common parameters, they have their own parameters: the maximum distance d max for co-occurrence networks generation, a Boolean replace zeros to control whether or not to replace zeros with the minimum non-zero values, a Boolean no self loops to control whether or not to remove the self-loops, the number of random walk steps t (Eq. 4) and the power p (Eq. 5).", "labels": [], "entities": []}, {"text": "All four models are trained on an English Wikipedia dump from April 2017 of three sizes: about 19M tokens, about 94M tokens (both are for detailed analyses and non-common parameters grid search in each of the three graph-based models) and around 2.19 billion tokens (for four models comparison).", "labels": [], "entities": [{"text": "English Wikipedia dump from April 2017", "start_pos": 34, "end_pos": 72, "type": "DATASET", "confidence": 0.9411756495634714}]}, {"text": "During corpus preprocessing, we use CoreNLP ( ) for sentence segmentation and word tokenization, then convert tokens to lowercase, replace all expressions with numbers by 0 and replace rare tokens with UNK s.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7301017940044403}, {"text": "word tokenization", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.697215348482132}]}, {"text": "We perform a grid search on the \u223c19M tokens corpus, with d max \u2208 {2, . .", "labels": [], "entities": []}, {"text": "10}, t \u2208 {1, . .", "labels": [], "entities": []}, {"text": "4}, p \u2208 {\u22122, \u22121, 0.01, 0.25, 0.75, 1, 2} and T rue, F alse for the two Boolean parameters.", "labels": [], "entities": [{"text": "T rue, F alse", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.8592498302459717}]}, {"text": "We retain the best parameters obtained by this grid search and perform a tighter grid search around them on the \u223c94M tokens corpus.", "labels": [], "entities": [{"text": "\u223c94M tokens corpus", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.641190879046917}]}, {"text": "Then based on the two grid search results, we select the final parameters for the entire Wikipedia dump test.", "labels": [], "entities": [{"text": "Wikipedia dump test", "start_pos": 89, "end_pos": 108, "type": "DATASET", "confidence": 0.9151546557744344}]}, {"text": "We evaluate the resulting word embeddings on word similarity tasks using) and SimLex-999 () (correlation with humans), and on the word analogy task of.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 130, "end_pos": 142, "type": "TASK", "confidence": 0.7516182363033295}]}, {"text": "Therefore, we use the correlation coefficients between model similarity judgments and human similarity judgments for WordSim-353 and SimLex-999 tasks and the accuracy of the model prediction with gold standard for the word analogy task (the metrics in) as objective functions for these parameter tuning processes.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 117, "end_pos": 128, "type": "DATASET", "confidence": 0.972236156463623}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9989387392997742}, {"text": "word analogy task", "start_pos": 218, "end_pos": 235, "type": "TASK", "confidence": 0.8079580863316854}]}], "tableCaptions": [{"text": " Table 1: Evaluation results on WordSim-353, SimLex-999 and the word analogy task for the plain  word2vec model and our three graph-based noise distributions on the entire English Wikipedia dump. A  dagger  \u2020 marks a statistically significant difference to the baseline word2vec.", "labels": [], "entities": [{"text": "WordSim-353", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.9785172343254089}, {"text": "word analogy", "start_pos": 64, "end_pos": 76, "type": "TASK", "confidence": 0.7675721943378448}, {"text": "English Wikipedia dump", "start_pos": 172, "end_pos": 194, "type": "DATASET", "confidence": 0.8394735256830851}]}]}