{"title": [{"text": "No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling", "labels": [], "entities": []}], "abstractContent": [{"text": "Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem.", "labels": [], "entities": [{"text": "visual captioning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7813971638679504}]}, {"text": "Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images.", "labels": [], "entities": []}, {"text": "Thus it poses challenges to behavioral cloning algorithms.", "labels": [], "entities": []}, {"text": "Furthermore, due to the limitations of automatic metrics on evaluating story quality , reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost.", "labels": [], "entities": []}, {"text": "Therefore, we propose an Adver-sarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function.", "labels": [], "entities": []}, {"text": "Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.", "labels": [], "entities": []}, {"text": "Code will be made available here 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, increasing attention has been focused on visual captioning, which aims at describing the content of an image or a video.", "labels": [], "entities": [{"text": "visual captioning", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7814243733882904}]}, {"text": "Though it has achieved impressive results, its capability of performing human-like understanding is still restrictive.", "labels": [], "entities": []}, {"text": "To further investigate machine's capa- * Equal contribution https://github.com/littlekobe/AREL Story #1: The brother and sister were ready for the first day of school.", "labels": [], "entities": [{"text": "AREL Story #1", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.8404552936553955}]}, {"text": "They were excited to go to their first day and meet new friends.", "labels": [], "entities": []}, {"text": "They told their mom how happy they were.", "labels": [], "entities": []}, {"text": "They said they were going to make a lot of new friends . Then they got up and got ready to get in the car . Story #2: The brother did not want to talk to his sister.", "labels": [], "entities": []}, {"text": "They started to talk and smile.", "labels": [], "entities": []}, {"text": "They were happy to see them.", "labels": [], "entities": []}, {"text": "bilities in understanding more complicated visual scenarios and composing more structured expressions, visual storytelling () has been proposed.", "labels": [], "entities": [{"text": "visual storytelling", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7613088488578796}]}, {"text": "Visual captioning is aimed at depicting the concrete content of the images, and its expression style is rather simple.", "labels": [], "entities": [{"text": "Visual captioning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7115069925785065}]}, {"text": "In contrast, visual storytelling goes one step further: it summarizes the idea of a photo stream and tells a story about it. shows an example of visual captioning and visual storytelling.", "labels": [], "entities": []}, {"text": "We have observed that stories contain rich emotions (excited, happy, not want) and imagination (siblings, parents, school, car).", "labels": [], "entities": []}, {"text": "It, therefore, requires the capability to associate with concepts that do not explicitly appear in the images.", "labels": [], "entities": []}, {"text": "Moreover, stories are more subjective, so there barely exists standard templates for storytelling.", "labels": [], "entities": []}, {"text": "As shown in, the same photo stream can be paired with diverse stories, different from each other.", "labels": [], "entities": []}, {"text": "This heavily increases the evaluation difficulty.", "labels": [], "entities": []}, {"text": "So far, prior work for visual storytelling) is mainly inspired by the success of visual captioning.", "labels": [], "entities": [{"text": "visual captioning", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7333722412586212}]}, {"text": "Nevertheless, because these methods are trained by maximizing the likelihood of the observed data pairs, they are restricted to generate simple and plain description with limited expressive patterns.", "labels": [], "entities": []}, {"text": "In order to cope with the challenges and produce more human-like descriptions, have proposed a reinforcement learning framework.", "labels": [], "entities": []}, {"text": "However, in the scenario of visual storytelling, the common reinforced captioning methods are facing great challenges since the hand-crafted rewards based on string matches are either too biased or too sparse to drive the policy search.", "labels": [], "entities": [{"text": "visual storytelling", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7409374415874481}]}, {"text": "For instance, we used the METEOR () score as the reward to reinforce our policy and found that though the METEOR score is significantly improved, the other scores are severely harmed.", "labels": [], "entities": [{"text": "METEOR () score", "start_pos": 26, "end_pos": 41, "type": "METRIC", "confidence": 0.9452424844106039}, {"text": "METEOR score", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9354631304740906}]}, {"text": "Here we showcase an adversarial example with an average METEOR score as high as 40.2: We had a great time to have a lot of the.", "labels": [], "entities": [{"text": "METEOR score", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.9813705086708069}]}, {"text": "They were to be a of the.", "labels": [], "entities": []}, {"text": "They were to be in the.", "labels": [], "entities": []}, {"text": "The and it were to be the.", "labels": [], "entities": []}, {"text": "The, and it were to be the.", "labels": [], "entities": []}, {"text": "Apparently, the machine is gaming the metrics.", "labels": [], "entities": []}, {"text": "Conversely, when using some other metrics (e.g. BLEU, CIDEr) to evaluate the stories, we observe an opposite behavior: many relevant and coherent stories are receiving a very low score (nearly zero).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9952196478843689}]}, {"text": "In order to resolve the strong bias brought by the hand-coded evaluation metrics in RL training and produce more human-like stories, we propose an Adversarial REward Learning (AREL) framework for visual storytelling.", "labels": [], "entities": [{"text": "RL training", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.904039591550827}]}, {"text": "We draw our inspiration from recent progress in inverse reinforcement learning and propose the AREL algorithm to learn a more intelligent reward function.", "labels": [], "entities": [{"text": "inverse reinforcement learning", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.756154199441274}, {"text": "AREL", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9576936364173889}]}, {"text": "Specifically, we first incorporate a Boltzmann distribution to associate reward learning with distribution approximation, then design the adversarial process with two models -a policy model and a reward model.", "labels": [], "entities": []}, {"text": "The policy model performs the primitive actions and produces the story sequence, while the reward model is responsible for learning the implicit reward function from human demonstrations.", "labels": [], "entities": []}, {"text": "The learned reward function would be employed to optimize the policy in return.", "labels": [], "entities": []}, {"text": "For evaluation, we conduct both automatic metrics and human evaluation but observe a poor correlation between them.", "labels": [], "entities": []}, {"text": "Particularly, our method gains slight performance boost over the baseline systems on automatic metrics; human evaluation, however, indicates significant performance boost.", "labels": [], "entities": []}, {"text": "Thus we further discuss the limitations of the metrics and validate the superiority of our AREL method in performing more intelligent understanding of the visual scenes and generating more human-like stories.", "labels": [], "entities": [{"text": "AREL", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.8607475161552429}]}, {"text": "Our main contributions are four-fold: \u2022 We propose an adversarial reward learning framework and apply it to boost visual story generation.", "labels": [], "entities": [{"text": "visual story generation", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.7403806646664938}]}, {"text": "\u2022 We evaluate our approach on the Visual Storytelling (VIST) dataset and achieve the state-of-the-art results on automatic metrics.", "labels": [], "entities": [{"text": "Visual Storytelling (VIST) dataset", "start_pos": 34, "end_pos": 68, "type": "DATASET", "confidence": 0.6460110048453013}]}, {"text": "\u2022 We empirically demonstrate that automatic metrics are not perfect for either training or evaluation.", "labels": [], "entities": []}, {"text": "\u2022 We design and perform a comprehensive human evaluation via Amazon Mechanical Turk, which demonstrates the superiority of the generated stories of our method on relevance, expressiveness, and concreteness.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.9257862567901611}]}], "datasetContent": [{"text": "In this section, we compare our AREL method with the state-of-the-art methods as well as standard reinforcement learning algorithms on automatic evaluation metrics.", "labels": [], "entities": [{"text": "AREL", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.8471570014953613}]}, {"text": "Then we further discuss the limitations of the hand-crafted metrics on evaluating human-like stories.", "labels": [], "entities": []}, {"text": "Automatic metrics cannot fully evaluate the capability of our AREL method.", "labels": [], "entities": [{"text": "AREL", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.5813917517662048}]}, {"text": "Therefore, we perform two different kinds of human evaluation studies on Amazon Mechanical Turk: Turing test and pairwise human evaluation.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.9247420032819113}]}, {"text": "For both tasks, we use 150 stories (750 images) sampled from the test set, each assigned to 5 workers to eliminate human variance.", "labels": [], "entities": []}, {"text": "We batch six items as one assignment and insert an additional assignment as a sanity check.", "labels": [], "entities": []}, {"text": "Besides, the order of the options within each item is shuffled to make a fair comparison.", "labels": [], "entities": []}, {"text": "Turing Test We first conduct five independent Turing tests for XE-ss, BLEU-RL, CIDEr-RL, GAN, and AREL models, during which the worker is given one human-annotated sample and one machine-generated sample, and needs to decide which is human-annotated.", "labels": [], "entities": [{"text": "BLEU-RL", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9956235289573669}, {"text": "AREL", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9890568852424622}]}, {"text": "As shown in Table 3, our AREL model significantly outperforms all the other baseline models in the Turing test: it has much more chances to fool AMT worker (the ratio is AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN = 45.8%:28.3%:32.1%:19.7%:39.5%), which confirms the superiority of our AREL framework in generating human-like stories.", "labels": [], "entities": [{"text": "AREL:XE-ss:BLEU-RL:CIDEr-RL:GAN", "start_pos": 170, "end_pos": 201, "type": "METRIC", "confidence": 0.7349676092465719}]}, {"text": "Unlike automatic metric evaluation, the Turing test has indicated a much larger margin between AREL and other competing algorithms.", "labels": [], "entities": [{"text": "AREL", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.7207580208778381}]}, {"text": "Thus, we empirically confirm that metrics are not perfect in evaluating many implicit semantic properties of natural language.", "labels": [], "entities": []}, {"text": "Besides, the Turing test of our AREL model reveals that nearly half of the workers are fooled by our machine generation, indicating a preliminary success toward generating human-like stories.", "labels": [], "entities": [{"text": "AREL", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.7555041313171387}]}, {"text": "Pairwise Comparison In order to have a clear comparison with competing algorithms with respect to different semantic features of the stories, we further perform four pairwise comparison tests: AREL vs XE-ss/BLEU-RL/CIDEr-RL/GAN.", "labels": [], "entities": [{"text": "AREL", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9939272403717041}, {"text": "BLEU-RL", "start_pos": 207, "end_pos": 214, "type": "METRIC", "confidence": 0.9071493744850159}]}, {"text": "For each photo stream, the worker is presented with two generated stories and asked to make decisions from the three aspects: relevance 5 , expressiveness 6 and concreteness . This head-tohead compete is designed to help us understand in what aspect our model outperforms the competing algorithms, which is displayed in.", "labels": [], "entities": []}, {"text": "Consistently on all the three comparisons, a large majority of the AREL stories trumps the competing systems with respect to their relevance,", "labels": [], "entities": [{"text": "AREL stories", "start_pos": 67, "end_pos": 79, "type": "DATASET", "confidence": 0.7412646114826202}]}], "tableCaptions": [{"text": " Table 1, our XE- ss model already outperforms the best-known re-Method  B-1 B-2 B-3 B-4 M  R  C", "labels": [], "entities": [{"text": "XE- ss", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.857650895913442}]}, {"text": " Table 2: Comparison with different RL mod- els with different metric scores as the rewards.", "labels": [], "entities": []}, {"text": " Table 3: Turing test results.", "labels": [], "entities": []}, {"text": " Table 4: Pairwise human comparisons. The results indicate the consistent superiority of our AREL model  in generating more human-like stories than the SOTA methods.", "labels": [], "entities": []}]}