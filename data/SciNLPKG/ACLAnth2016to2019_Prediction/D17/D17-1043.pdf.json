{"title": [{"text": "Piecewise Latent Variables for Neural Variational Text Processing", "labels": [], "entities": [{"text": "Neural Variational Text Processing", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.7067835330963135}]}], "abstractContent": [{"text": "Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as varia-tional autoencoders.", "labels": [], "entities": []}, {"text": "The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text.", "labels": [], "entities": []}, {"text": "However , current models often assume simplis-tic priors on the latent variables-such as the uni-modal Gaussian distribution-which are incapable of representing complex latent factors efficiently.", "labels": [], "entities": []}, {"text": "To overcome this restriction, we propose the simple , but highly flexible, piecewise constant distribution.", "labels": [], "entities": []}, {"text": "This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable.", "labels": [], "entities": []}, {"text": "Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7759514153003693}, {"text": "natural language generation", "start_pos": 193, "end_pos": 220, "type": "TASK", "confidence": 0.6491033335526785}]}], "introductionContent": [{"text": "The development of the variational autoencoder framework () has paved the way for learning largescale, directed latent variable models.", "labels": [], "entities": []}, {"text": "This has led to significant progress in a diverse set of machine learning applications, ranging from computer vision () to natural language processing tasks; * The first two authors contributed equally.).", "labels": [], "entities": []}, {"text": "It is hoped that this framework will enable the learning of generative processes of real-world data -including text, audio and images -by disentangling and representing the underlying latent factors in the data.", "labels": [], "entities": [{"text": "learning of generative processes of real-world data -including text, audio and images", "start_pos": 48, "end_pos": 133, "type": "TASK", "confidence": 0.7536763101816177}]}, {"text": "However, latent factors in real-world data are often highly complex.", "labels": [], "entities": []}, {"text": "For example, topics in newswire text and responses in conversational dialogue often posses latent factors that follow non-linear (non-smooth), multi-modal distributions (i.e. distributions with multiple local maxima).", "labels": [], "entities": []}, {"text": "Nevertheless, the majority of current models assume a simple prior in the form of a multivariate Gaussian distribution in order to maintain mathematical and computational tractability.", "labels": [], "entities": []}, {"text": "This is often a highly restrictive and unrealistic assumption to impose on the structure of the latent variables.", "labels": [], "entities": []}, {"text": "First, it imposes a strong uni-modal structure on the latent variable space; latent variable samples from the generating model (prior distribution) all cluster around a single mean.", "labels": [], "entities": []}, {"text": "Second, it forces the latent variables to follow a perfectly symmetric distribution with constant kurtosis; this makes it difficult to represent asymmetric or rarely occurring factors.", "labels": [], "entities": []}, {"text": "Such constraints on the latent variables increase pressure on the down-stream generative model, which in turn is forced to carefully partition the probability mass for each latent factor throughout its intermediate layers.", "labels": [], "entities": []}, {"text": "For complex, multi-modal distributions -such as the distribution over topics in a text corpus, or natural language responses in a dialogue system -the unimodal Gaussian prior inhibits the model's ability to extract and represent important latent structure in the data.", "labels": [], "entities": []}, {"text": "In order to learn more expressive latent variable models, we therefore need more flexible, yet tractable, priors.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a simple, flexible prior distribution based on the piecewise constant distribution.", "labels": [], "entities": []}, {"text": "We derive an analytical, tractable form that is applicable to the variational autoencoder framework and propose a differentiable parametrization for it.", "labels": [], "entities": []}, {"text": "We then evaluate the effectiveness of the distribution when utilized both as a prior and as approximate posterior across variational architectures in two natural language processing tasks: document modeling and natural language generation for dialogue.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 189, "end_pos": 206, "type": "TASK", "confidence": 0.7629754543304443}, {"text": "natural language generation", "start_pos": 211, "end_pos": 238, "type": "TASK", "confidence": 0.6470305820306143}]}, {"text": "We show that the piecewise constant distribution is able to capture elements of a target distribution that cannot be captured by simpler priors -such as the uni-modal Gaussian.", "labels": [], "entities": []}, {"text": "We demonstrate state-ofthe-art results on three document modeling tasks, and show improvements on a dialogue natural language generation.", "labels": [], "entities": [{"text": "document modeling tasks", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7647405564785004}, {"text": "dialogue natural language generation", "start_pos": 100, "end_pos": 136, "type": "TASK", "confidence": 0.6364824175834656}]}, {"text": "Finally, we illustrate qualitatively how the piecewise constant distribution represents multi-modal latent structure in the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed models on two types of natural language processing tasks: document modeling and dialogue natural language generation.", "labels": [], "entities": [{"text": "document modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.783055305480957}, {"text": "dialogue natural language generation", "start_pos": 105, "end_pos": 141, "type": "TASK", "confidence": 0.6904117316007614}]}, {"text": "All models are trained with back-propagation using the variational lower-bound on the loglikelihood or the exact log-likelihood.", "labels": [], "entities": []}, {"text": "We use the first-order gradient descent optimizer Adam (: Test perplexities on three document modeling tasks: 20-NewGroup (20-NG), Reuters corpus (RCV1) and CADE12 (CADE).", "labels": [], "entities": [{"text": "Reuters corpus (RCV1)", "start_pos": 131, "end_pos": 152, "type": "DATASET", "confidence": 0.854380750656128}]}, {"text": "Perplexities were calculated using 10 samples to estimate the variational lower-bound.", "labels": [], "entities": [{"text": "Perplexities", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9663733839988708}, {"text": "variational", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.9564515352249146}]}, {"text": "The H-NVDM models perform best across all three datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test perplexities on three document mod- eling tasks: 20-NewGroup (20-NG), Reuters cor- pus (RCV1) and CADE12 (CADE). Perplexities  were calculated using 10 samples to estimate the  variational lower-bound. The H-NVDM models  perform best across all three datasets.", "labels": [], "entities": []}, {"text": " Table 3: Ubuntu evaluation using F1 metrics w.r.t.  activities and entities. G-VHRED, P-VHRED and  H-VHRED all outperform the baseline HRED.  G-VHRED performs best w.r.t. activities and H- VHRED performs best w.r.t. entities.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9508317708969116}]}]}