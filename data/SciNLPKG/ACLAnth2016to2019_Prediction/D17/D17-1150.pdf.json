{"title": [{"text": "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation", "labels": [], "entities": [{"text": "Attention-Based Neural Machine Translation", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.6186420768499374}]}], "abstractContent": [{"text": "This paper proposes a hierarchical atten-tional neural translation model which fo-cuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder.", "labels": [], "entities": [{"text": "hierarchical atten-tional neural translation", "start_pos": 22, "end_pos": 66, "type": "TASK", "confidence": 0.6462696865200996}]}, {"text": "To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors.", "labels": [], "entities": []}, {"text": "Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem.", "labels": [], "entities": []}, {"text": "Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "labels": [], "entities": [{"text": "tree-based neural translation", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.7539265155792236}, {"text": "English-Chinese translation tasks", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.7202614545822144}]}], "introductionContent": [{"text": "Neural machine translation (NMT) automatically learns the abstract features of and semantic relationship between the source and target sentences, and has recently given state-of-the-art results for various translation tasks).", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7479785680770874}]}, {"text": "The most widely used model is the encoder-decoder framework, in which the source sentence is encoded into a dense representation, followed by a decoding process which generates the target translation.", "labels": [], "entities": []}, {"text": "By exploiting the attention mechanism (, the generation of target words is conditional on the source hidden states, rather than on the context vector alone.", "labels": [], "entities": []}, {"text": "From a model architecture perspective, prior studies of the attentive * Corresponding author encoder-decoder translation model are mainly divided into two types.", "labels": [], "entities": [{"text": "attentive * Corresponding author encoder-decoder translation", "start_pos": 60, "end_pos": 120, "type": "TASK", "confidence": 0.7123378912607828}]}, {"text": "The sequence-to-sequence model treats a sentence as a sequence of tokens.", "labels": [], "entities": []}, {"text": "The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words.", "labels": [], "entities": []}, {"text": "Although used a bidirectional recurrent neural network (RNN)) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (.", "labels": [], "entities": []}, {"text": "By incorporating additional features into a sequential model, and suggest that a greater amount of linguistic information can improve the translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.958903968334198}]}, {"text": "The tree-to-sequence model encodes a source sentence according to a given syntactic tree over the sentence.", "labels": [], "entities": []}, {"text": "The existing tree-based encoders () recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "labels": [], "entities": []}, {"text": "As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence.", "labels": [], "entities": []}, {"text": "As illustrated in, the phrases \"take up\" 1 and \"a position\" 2 have different meanings in different contexts.", "labels": [], "entities": []}, {"text": "However, in composing the representations h VP 3 and h NP 7 for phrases VP 3 and NP 7 , the current approaches do not account for the differences in meaning which arise as a result of ignoring the neighboring context as well as the remote context, i.e. h NP 7 \u2190 h PP 8 (sibling) and h VP 3 \u2190 h NP 7 (child of sibling).", "labels": [], "entities": []}, {"text": "More specifically, at the encoding step t, the generated phrase is based on the results at the previous time steps h t\u22121 and h t\u22122 , but has no information about the parent phrases ht fort > t.", "labels": [], "entities": []}, {"text": "To address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models ().", "labels": [], "entities": []}, {"text": "In contrast to the model of, we first use a bidirectional RNN at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.1).", "labels": [], "entities": []}, {"text": "Secondly, we propose a bidirectional tree-based encoder (described in Section 3.2), in which the original bottom-up encoding model is extended using an additional top-down encoding process.", "labels": [], "entities": []}, {"text": "In the bidirectional hierarchical model, the vector representations of the sentence, phrases as well as words, are therefore based on the global context rather than local information.", "labels": [], "entities": []}, {"text": "To effectively leverage hierarchical representations in generating the target words, we adopt a variant weighted tree-based attention mechanism (described in Section 3.4) in which a timedependent gating scalar is used to control the proportion of conditional information between the word and phrase vectors.", "labels": [], "entities": []}, {"text": "To alleviate the out-ofvocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level by integrating byte-pair encoding (BPE)  into the tree-based model (as described in Section 3.3).", "labels": [], "entities": []}, {"text": "Experimental results for the NIST English-to-Chinese translation task reveal that the proposed model significantly outperforms the vanilla tree-based ( and sequential NMT models () (Section 4.1).", "labels": [], "entities": [{"text": "NIST English-to-Chinese translation task", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.7625181823968887}]}], "datasetContent": [{"text": "As shown in, which gives the statistics of the token types, we limit the source and target vo-: The vocabulary size of the training set before and after applying the generalization and BPE segmentation.", "labels": [], "entities": [{"text": "BPE segmentation", "start_pos": 185, "end_pos": 201, "type": "TASK", "confidence": 0.7046410888433456}]}, {"text": "cabulary size to 40,000, in order to coverall the English and Chinese tokens.", "labels": [], "entities": [{"text": "English and Chinese tokens", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.8636458814144135}]}, {"text": "The dimensions of word embedding and hidden layer are respectively set as 620 and 1,000.", "labels": [], "entities": []}, {"text": "Due to the concatenation in the bidirectional leaf-node encoding, the dimensions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500.", "labels": [], "entities": []}, {"text": "In order to prevent over-fitting, the training data is shuffled following each epoch.", "labels": [], "entities": []}, {"text": "Moreover, the model parameters are optimized using AdaDelta, due to its capability for dynamically adapting the learning rate.", "labels": [], "entities": [{"text": "AdaDelta", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8980242609977722}]}, {"text": "We set the mini-batch size to 16 and the beam search size to 5.", "labels": [], "entities": []}, {"text": "The accuracy of the translation relative to a reference is assessed using the BLEU metric).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995623230934143}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9964728951454163}]}, {"text": "In order to give an equitable comparison, all the NMT models used for comparison are implemented or re-implemented using GRU in our code, based on dl4mt 4 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data used in the experiments.", "labels": [], "entities": []}, {"text": " Table 3: Translation results for the various models. The first column shows the models; the second  column indicates whether the corresponding experiment uses BPE data. The number of parameters (M  = millions) in each model is given in the third column. The remaining columns are the translation  accuracies for the test sets and development set, evaluated using BLEU scores (%). \"\u2191 / \u21d1\": indicates  that the hierarchical encoder is significantly better than the vanilla tree-based encoder (p < 0.05/p <  0.01).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9651669859886169}, {"text": "BPE data", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.7249488234519958}, {"text": "BLEU", "start_pos": 364, "end_pos": 368, "type": "METRIC", "confidence": 0.9994470477104187}]}, {"text": " Table 4: Translation results for the development  set. The last column indicates the average length  of translation sentences, and the average length of  reference sentences is 23.19.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.941817581653595}]}]}