{"title": [{"text": "How much progress have we made on RST discourse parsing? A replication study of recent results on the RST-DT", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.9250818689664205}, {"text": "RST-DT", "start_pos": 102, "end_pos": 108, "type": "TASK", "confidence": 0.7779152393341064}]}], "abstractContent": [{"text": "This article evaluates purported progress over the past years in RST discourse parsing.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 65, "end_pos": 86, "type": "TASK", "confidence": 0.9379989703496298}]}, {"text": "Several studies report a relative error reduction of 24 to 51% on all met-rics that authors attribute to the introduction of distributed representations of discourse units.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.8755814433097839}]}, {"text": "We replicate the standard evaluation of 9 parsers, 5 of which use distributed representations, from 8 studies published between 2013 and 2017, using their predictions on the test set of the RST-DT.", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 190, "end_pos": 196, "type": "DATASET", "confidence": 0.807283878326416}]}, {"text": "Our main finding is that most recently reported increases in RST discourse parser performance are an artefact of differences in implementations of the evaluation procedure.", "labels": [], "entities": [{"text": "RST discourse parser", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.923222561677297}]}, {"text": "We evaluate all these parsers with the standard Parseval procedure to provide a more accurate picture of the actual RST discourse parsers performance in standard evaluation settings.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.925959845383962}]}, {"text": "Under this more stringent procedure, the gains attributable to distributed representations represent at most a 16% relative error reduction on fully-labelled structures.", "labels": [], "entities": []}], "introductionContent": [{"text": "While several theories of discourse structure for text exist, discourse parsing work has largely concentrated on Rhetorical Structure Theory (RST) ( and the RST Discourse Treebank (RST-DT), which is the largest corpus of texts annotated with full discourse structures.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7104261815547943}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 113, "end_pos": 146, "type": "TASK", "confidence": 0.7658310731252035}, {"text": "RST Discourse Treebank (RST-DT)", "start_pos": 157, "end_pos": 188, "type": "DATASET", "confidence": 0.7686277081569036}]}, {"text": "The RST-DT, annotated in the style of RST, consists of 385 news articles from the Penn Treebank, split into a training and test sets of 347 and 38 documents.The standard evaluation procedure for RST discourse parsing, RST-Parseval, proposed by, adapts the Parseval procedure for syntactic parsing (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9927257597446442}, {"text": "RST discourse parsing", "start_pos": 195, "end_pos": 216, "type": "TASK", "confidence": 0.9205657641092936}, {"text": "syntactic parsing", "start_pos": 279, "end_pos": 296, "type": "TASK", "confidence": 0.7591663300991058}]}, {"text": "RST-Parseval computes scores on discourse structures with no label (S for Span) or labelled with nuclearity (N), relation (R) or both (F for Full).", "labels": [], "entities": []}, {"text": "The semantic nature of discourse relations makes discourse parsing a difficult task.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.680568054318428}]}, {"text": "However, the recent introduction of distributed representations of discourse units has seemingly led to significant improvements, with a claimed relative error reduction of 51% on fully labelled structures.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 145, "end_pos": 169, "type": "METRIC", "confidence": 0.7744655807813009}]}, {"text": "As part of a broader study of methods and evaluation metrics for discourse parsing, we collected predictions from nine RST discourse parsers and reimplemented RST-Parseval.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.698381319642067}, {"text": "RST discourse parsers", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.8609649141629537}]}, {"text": "In section 2, we present these RST parsers and report their published scores on RST-Parseval.", "labels": [], "entities": []}, {"text": "In section 3, we replicate their evaluation and show that most of the heterogeneity in performance across RST parsers arises from differences in their evaluation procedures.", "labels": [], "entities": [{"text": "RST parsers", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.884882003068924}]}, {"text": "In section 4, we replace RST-Parseval with the standard Parseval procedure and obtain a more accurate picture of the actual performance of RST parsers.", "labels": [], "entities": [{"text": "RST parsers", "start_pos": 139, "end_pos": 150, "type": "TASK", "confidence": 0.7980916500091553}]}], "datasetContent": [{"text": "We collected or reproduced predictions from each parser and replicated the evaluation procedure . The predictions came in various formats: bracketed strings as in the RST-DT, lists of span descriptions, trees or lists of attachment decisions.", "labels": [], "entities": []}, {"text": "We wrote custom functions to load and normalize the predictions from each parser into RST trees.", "labels": [], "entities": []}, {"text": "While we favor evaluating against the original, non binarized reference RST trees, we conformed in this replicative study to the de facto standard in the RST parsing literature: We transformed the reference RST trees into right-branching binary trees and used these binary trees as reference in all our evaluation procedures.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 154, "end_pos": 165, "type": "TASK", "confidence": 0.983644038438797}]}, {"text": "We also examined the source code from the evaluation procedures provided by the authors to determine whether the published scores corresponded to the R or F metric.", "labels": [], "entities": []}, {"text": "In so doing we noticed a potentially important discrepancy in the various implementations of the RST-Parseval procedure: the implementations used to evaluate the parsers in the first group compute micro-averaged F 1 scores, as is standard practice in the syntactic parsing community, whereas the implementations used to evaluate the parsers in the second group compute macroaveraged F 1 scores across documents.", "labels": [], "entities": []}, {"text": "The microaveraged F 1 score is computed globally over the predicted and reference spans from all documents ; the macro-averaged F 1 score across documents is the average of F 1 scores computed independently for each document.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8476883769035339}, {"text": "F 1 score", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.8943070570627848}, {"text": "F 1 scores", "start_pos": 173, "end_pos": 183, "type": "METRIC", "confidence": 0.9198373556137085}]}, {"text": "We implemented both strategies and report the corresponding scores in two separate tables.", "labels": [], "entities": []}, {"text": "Parsers originally evaluated with micro-averaging scores are in the top half of each table, parsers originally evaluated with macro-averaged scores in the bottom half.", "labels": [], "entities": []}, {"text": "An asterisk (*) marks parsers for which we reproduced predictions using code and material made available by the authors, although the experimental settings are not guaranteed to match exactly those from the original study.", "labels": [], "entities": []}, {"text": "A double asterisk (**) marks a parser for which we used predictions generated by the author using an improved, unpublished version of the parser posterior to the original study.", "labels": [], "entities": []}, {"text": "Lines with no asterisk in correspond to parsers whose authors sent us their original predictions.", "labels": [], "entities": []}, {"text": "Replicated scores expected to match scores in Table 1 are underlined.", "labels": [], "entities": []}, {"text": "contains the micro-averaged F 1 scores on each metric (S, N, R, F).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9298949440320333}]}, {"text": "As expected, parsers in the first group obtain micro-averaged scores equal or close to their published scores reported in Table 1.", "labels": [], "entities": []}, {"text": "More strikingly, the micro-averaged scores for the parsers in the second group are much lower than their published scores 2 and most of their claimed advantages over the parsers in the first The milder decrease of the DPLP scores, especially on S, is directly attributable to improvements in the latest, unpublished version of the parser.", "labels": [], "entities": [{"text": "DPLP", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.8583125472068787}]}, {"text": "contains the macro-averaged F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9297587474187216}]}, {"text": "Parsers in the first group obtain macro-averaged scores markedly higher than the micro-averaged scores from.", "labels": [], "entities": []}, {"text": "Parsers in the second group obtain macro-averaged scores that are equal or close to the published scores reported in, which confirms our analysis of the source code of their evaluation procedures.", "labels": [], "entities": []}, {"text": "The global picture on macro-averaged scores is consistent with that on micro-averaged scores: On Sand N, parsers in the second group do not improve over parsers the first group and the best parser brings an absolute improvement of 0.9 and 1.4 points on Rand F. On each metric, the two lowest scores are obtained by parsers from the second group.", "labels": [], "entities": []}, {"text": "To sum up, parsers in the first group have identical scores in, except for slight differences between our evaluation procedure and the authors', or between the predictions used in our evaluation compared to the original study.", "labels": [], "entities": []}, {"text": "The second group of parsers have identical scores in, modulo the same factors.", "labels": [], "entities": []}, {"text": "The (exactly or nearly) matching entries between, are evidence of the two averaging strategies (micro in, macro in) used by the authors in their publications.", "labels": [], "entities": []}, {"text": "A comparison between Tables 2 and 3 reveals that the averaging strategy similarly affects both groups of parsers.", "labels": [], "entities": []}, {"text": "As a result, the performance level among recent RST discourse parsers is much more homogeneous than the situation depicted in the literature.", "labels": [], "entities": [{"text": "RST discourse parsers", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.9448084036509196}]}, {"text": "The distributed representations of DUs computed and used in JE14 DPLP () and possibly BCS17 cross+dev ( plausibly capture semantic information that helps with predicting discourse relations and structure, but the current experimental results do not provide a similarly strong support for BPS16 (), LLC16 () and BCS17 mono ().", "labels": [], "entities": [{"text": "JE14 DPLP", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.8210764229297638}, {"text": "predicting discourse relations and structure", "start_pos": 159, "end_pos": 203, "type": "TASK", "confidence": 0.8597758531570434}, {"text": "BPS16", "start_pos": 288, "end_pos": 293, "type": "DATASET", "confidence": 0.8258791565895081}]}, {"text": "More generally, it is important that authors compute and report scores that accord with standard practice, unless duly motivated.", "labels": [], "entities": []}, {"text": "The standard practice in syntactic parsing is to report microaveraged scores for overall performance, often complemented with macro-averaged scores over classes to gain valuable insight into the average performance of parsers across labels, especially infrequent ones.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7337251901626587}]}, {"text": "Early work in RST discourse parsing follows this practice, reporting microaveraged scores for global performance, plus distinct scores for each relation class or macroaveraged scores overall relation classes.", "labels": [], "entities": [{"text": "RST discourse parsing", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.9386153817176819}]}, {"text": "The latter should not be confused with the scores published for BPS16, LLC16, BCS17 (mono, cross+dev) and JE14 DPLP, which are macro-averaged over documents.", "labels": [], "entities": [{"text": "BPS16", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.66142338514328}, {"text": "BCS17", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.5751864910125732}, {"text": "JE14 DPLP", "start_pos": 106, "end_pos": 115, "type": "DATASET", "confidence": 0.8386030495166779}]}, {"text": "RST-Parseval crucially relies on an encoding of RST trees into constituency trees such that the rhetorical relation names are placed on the children nodes, and the nuclei of mononuclear relations are conventionally labelled SPAN.", "labels": [], "entities": []}, {"text": "RSTParseval resembles the original Parseval, except it considers a larger set of nodes to collect all nuclearity and relation labels in this encoding: the root node (whose label and nuclearity are fixed by convention) is excluded and the leaves, the EDUs, are included.", "labels": [], "entities": [{"text": "RSTParseval", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7604576945304871}]}, {"text": "On the one hand, RST-Parseval can handle discourse units of arity greater than 2, in particular those consisting of a nucleus independently modified by two satellites through distinct mononuclear relations.", "labels": [], "entities": []}, {"text": "This avoids introducing discourse units that were not part of the original annotation, which a preliminary binarization of trees would have induced.", "labels": [], "entities": []}, {"text": "On the other hand, RST-Parseval considers approximately twice as many nodes as the original Parseval would on binarized trees (at most 2n \u2212 2 nodes for n EDUs, compared ton \u2212 1 attachments in a binary tree), and the relation labels of most nuclei are redundant with the nuclearity of anode and its sister (SPAN fora nucleus whose sisters are satellites, and the same label as its sisters fora nucleus whose sisters are nuclei).", "labels": [], "entities": []}, {"text": "Both aspects artificially raise the level of agreement between RST trees, especially when using manual EDU segmentation.", "labels": [], "entities": [{"text": "RST", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9453807473182678}]}, {"text": "However, all the parsers in our sample except predict binary trees over manually segmented EDUs and evaluate them against right-heavy binarized reference trees.", "labels": [], "entities": []}, {"text": "In this setting, Marcu's encoding of RST trees RST-Parseval are no longer motivated.", "labels": [], "entities": [{"text": "RST trees RST-Parseval", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.5492056310176849}]}, {"text": "We can thus revert to using the standard Parseval procedure on a representation of binary RST trees where each internal node is a labelled attachment decision to obtain a more accurate evaluation of RST parser performance.", "labels": [], "entities": [{"text": "RST parser", "start_pos": 199, "end_pos": 209, "type": "TASK", "confidence": 0.9137334823608398}]}, {"text": "represents (a) an original RST tree using Marcu's encoding, (b) its right-heavy binarized version, (c) the tree of labelled attachment decisions for the right-heavy binarized tree.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, we are the first to explicitly use an evaluation procedure for RST parsing closer to the original Parseval for syntax, although the trees of labelled attachment decisions we use directly correspond to the trees built by many RST parsers, eg. shift-reduce parsers.", "labels": [], "entities": [{"text": "RST parsing", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.9630558490753174}]}, {"text": "Parseval is more stringent than RST-Parseval, with the best system obtaining 46.3 on fully labelled structures (F).", "labels": [], "entities": [{"text": "Parseval", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7532694339752197}]}, {"text": "Parsers in the first group are competitive with parsers in the second group, outperforming them on Sand to a lesser extent on N.", "labels": [], "entities": [{"text": "Sand", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9719401001930237}]}, {"text": "Parsers in the second group reduce relative error by 8% on Rand 16% on F, much lower than the published figures in the literature.", "labels": [], "entities": [{"text": "Parsers", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9447547793388367}, {"text": "relative", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9317374229431152}, {"text": "error", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.5421013236045837}, {"text": "F", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.855180025100708}]}], "tableCaptions": [{"text": " Table 1: Published F 1 scores.", "labels": [], "entities": [{"text": "F 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8334382176399231}]}, {"text": " Table 2: Micro-averaged F 1 scores.", "labels": [], "entities": [{"text": "Micro-averaged F 1 scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8687535375356674}]}, {"text": " Table 3: Macro-averaged F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9216793974240621}]}, {"text": " Table 4: Micro-averaged F 1 scores on labelled  attachment decisions (original Parseval).", "labels": [], "entities": [{"text": "Micro-averaged F 1 scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8482883721590042}]}]}