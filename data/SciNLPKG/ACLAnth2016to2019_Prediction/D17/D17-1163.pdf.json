{"title": [{"text": "Identifying civilians killed by police with distantly supervised entity-event extraction", "labels": [], "entities": [{"text": "Identifying civilians killed by police", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8895717978477478}]}], "abstractContent": [{"text": "We propose anew, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6612778802712759}]}, {"text": "We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neu-ral network classifiers.", "labels": [], "entities": []}, {"text": "Our model out-performs two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases.", "labels": [], "entities": [{"text": "event extractor", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.731398731470108}]}], "introductionContent": [{"text": "The United States government does not keep systematic records of when police kill civilians, despite a clear need for this information to serve the public interest and support social scientific analysis.", "labels": [], "entities": []}, {"text": "Federal records rely on incomplete cooperation from local police departments, and human rights statisticians assess that they fail to document thousands of fatalities (.", "labels": [], "entities": []}, {"text": "News articles have emerged as a valuable alternative data source.", "labels": [], "entities": []}, {"text": "Organizations including The Guardian, The Washington Post, Mapping Police Violence, and Fatal Encounters have started to build such databases of U.S. police killings by manually reading millions of news articles 1 Fatal Encounters director D.", "labels": [], "entities": [{"text": "Mapping Police Violence", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.917184074719747}]}, {"text": "Brian Burghart estimates he and colleagues have read 2 million news headlines and ledes to assemble its fatality records that date back to; we find FE to be the most comprehensive publicly available database.", "labels": [], "entities": [{"text": "FE", "start_pos": 148, "end_pos": 150, "type": "DATASET", "confidence": 0.6794870495796204}]}], "datasetContent": [{"text": "On documents from the test period (Sept-Dec 2016), our models predict entity-level labels date of killing date of news report entity labels e2 = \"positive\" P (y e = 1 | x M(e) ) (Eq.", "labels": [], "entities": []}, {"text": "6), and we wish to evaluate whether retrieved entities are listed in Fatal Encounters as being killed during Sept-Dec 2016.", "labels": [], "entities": [{"text": "Fatal Encounters", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.664840817451477}]}, {"text": "We rank entities by predicted probabilities to construct a precision-recall curve,).", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 59, "end_pos": 75, "type": "METRIC", "confidence": 0.9903822541236877}]}, {"text": "Area under the precision-recall curve (AUPRC) is calculated with a trapezoidal rule; F1 scores are shown for convenient comparison to non-ranking approaches ( \u00a75).", "labels": [], "entities": [{"text": "precision-recall curve (AUPRC)", "start_pos": 15, "end_pos": 45, "type": "METRIC", "confidence": 0.94157475233078}, {"text": "F1", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.999154806137085}]}, {"text": "Excluding historical fatalities: Our model gives strong positive predictions for many people who were killed by police before the test period (i.e. before Sept 2016), when news articles contain discussion of historical police killings.", "labels": [], "entities": []}, {"text": "We exclude these entities from evaluation, since we want to simulate an update to a fatality database.", "labels": [], "entities": []}, {"text": "Our test dataset contains 1,148 such historical entities.", "labels": [], "entities": []}, {"text": "Data upper bound: Of the 452 gold entities in the FE database attest time, our news corpus only contained 258, hence the data up-: Area under precision-recall curve (AUPRC) and F1 (its maximum value from the PR curve) for entity prediction on the test set.", "labels": [], "entities": [{"text": "FE database attest time", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.9275117814540863}, {"text": "Area under precision-recall curve (AUPRC)", "start_pos": 131, "end_pos": 172, "type": "METRIC", "confidence": 0.8730291639055524}, {"text": "F1", "start_pos": 177, "end_pos": 179, "type": "METRIC", "confidence": 0.9980410933494568}, {"text": "entity prediction", "start_pos": 222, "end_pos": 239, "type": "TASK", "confidence": 0.7166903018951416}]}, {"text": "per bound of 0.57 recall, which also gives an upper bound of 0.57 on AUPRC.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9987519979476929}, {"text": "AUPRC", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.701586127281189}]}, {"text": "This is mostly a limitation of our news corpus; though we collect hundreds of thousands of news articles, it turns out Google News only accesses a subset of relevant web news, as opposed to more comprehensive data sources manually reviewed by Fatal Encounters' human experts.", "labels": [], "entities": [{"text": "Fatal Encounters' human", "start_pos": 243, "end_pos": 266, "type": "TASK", "confidence": 0.8161854147911072}]}, {"text": "We still believe our dataset is large enough to be realistic for developing better methods, and expect the same approaches could be applied to a more comprehensive news corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Data statistics for Fatal Encounters (FE)  and scraped news documents. M and E re- sult from NER processing, while E + results from  matching textual named entities against the gold- standard database (G).", "labels": [], "entities": [{"text": "Fatal Encounters (FE)", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.7612924218177796}]}, {"text": " Table 4: Feature templates for logistic regression  grouped into syntactic dependencies (D) and N- gram (N ) features.", "labels": [], "entities": []}, {"text": " Table 5: Area under precision-recall curve  (AUPRC) and F1 (its maximum value from the PR  curve) for entity prediction on the test set.", "labels": [], "entities": [{"text": "Area under precision-recall curve  (AUPRC)", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.8873493245669773}, {"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9991883635520935}, {"text": "entity prediction", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.7464437186717987}]}, {"text": " Table 6: Precision, recall, and F1 scores for test  data using event extractors SEMAFOR and RPI- JIE and rules R1-R3 described below.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9991382360458374}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9947217702865601}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.999745786190033}, {"text": "RPI- JIE", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.7384737730026245}]}]}