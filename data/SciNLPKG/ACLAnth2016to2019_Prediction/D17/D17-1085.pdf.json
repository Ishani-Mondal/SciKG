{"title": [{"text": "Structural Embedding of Syntactic Trees for Machine Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees.", "labels": [], "entities": []}, {"text": "In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension.", "labels": [], "entities": []}, {"text": "We evaluate our approach using a state-of-the-art neu-ral attention model on the SQuAD dataset.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9363635778427124}]}, {"text": "Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reading comprehension such as SQuAD () or NewsQA () requires identifying a span from a given context, which is an extension to the traditional question answering task, aiming at responding questions posed by human with natural language (.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9164245128631592}, {"text": "question answering task", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.788589616616567}]}, {"text": "Many works have been proposed to leverage deep neural networks for such question answering tasks, most of which involve learning the query-aware context representations (;.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8297386765480042}]}, {"text": "Although deep learning based methods demonstrated great potentials for question answering, none them take syntactic information of the sentences such as con- * Authors contributed equally to this work.", "labels": [], "entities": [{"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8932482600212097}]}, {"text": "stituency tree and dependency tree into consideration.", "labels": [], "entities": []}, {"text": "Such techniques have been proven to be useful in many natural language understanding tasks in the past and illustrated noticeable improvements such as the work by.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.7088602185249329}]}, {"text": "In this paper, we adopt similar ideas but apply them to a neural attention model for question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.871378481388092}]}, {"text": "The constituency tree () of a sentence defines internal nodes and terminal nodes to represent phrase structure grammars and the actual words.", "labels": [], "entities": []}, {"text": "illustrates the constituency tree of the sentence \"the architect or engineer acts as the project coordinator\".", "labels": [], "entities": []}, {"text": "Here, \"the architect or engineer\" and \"the project coordinator\" are labeled as noun phrases (\"NP\"), which is critical for answering the question below.", "labels": [], "entities": []}, {"text": "Here, the question asks for the name of certain occupation that can be best answered using an noun phrase.", "labels": [], "entities": []}, {"text": "Utilizing the knowledge of a constituency relations, we can reduce the size of the candidate space and help the algorithm to identify the correct answer.", "labels": [], "entities": []}, {"text": "Whose role is to design the works, prepare the specifications and produce construction drawings, administer the contract, tender the works, and manage the works from inception to completion?", "labels": [], "entities": []}, {"text": "On the other hand, a dependency tree () is constructed based on the dependency structure of a sentence.", "labels": [], "entities": []}, {"text": "displays the dependency tree for sentence The Annual Conference, roughly the equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of organization within the UMC.", "labels": [], "entities": []}, {"text": "\"The Annual Conference\" being the subject of \"the basic unit of organization within the UMC\" provides a critical clue for the model to skip over a large chunk of the text when answering the question \"What is the basic unit of organization within the UMC\".", "labels": [], "entities": []}, {"text": "As we show in the analysis section, adding dependency information dramatically helps identify dependency structures within the sentence, which is otherwise difficult to learn.", "labels": [], "entities": []}, {"text": "In this paper, we propose Structural Embedding of Syntactic Trees (SEST) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 196, "end_pos": 219, "type": "TASK", "confidence": 0.8137854933738708}]}, {"text": "Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers that are both succinct and grammatically coherent, which boosted the performance on both qualitative studies and numerical results.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7852820456027985}]}, {"text": "Our focus is to show adding structural embedding can improve baseline models, rather than directly compare to published SQuAD results.", "labels": [], "entities": []}, {"text": "Although the methods proposed in the paper are demonstrated using syntactic trees, we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted systematic experiments on the SQuAD dataset ().", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8542182147502899}]}, {"text": "We compared our methods against Bi-Directional Attention Flow (BiDAF), as well as the SEST models described in Section 3.", "labels": [], "entities": [{"text": "SEST", "start_pos": 86, "end_pos": 90, "type": "TASK", "confidence": 0.8300954699516296}]}, {"text": "We run our experiments on a machine that contains a single GTX 1080 GPU with 8GB VRAM.", "labels": [], "entities": []}, {"text": "All of the models being compared have the same settings on character embedding and word embedding.", "labels": [], "entities": []}, {"text": "As introduced in Section 2, we use a variable character embedding with a fixed pre-trained word embedding to serve as part of the input into the model.", "labels": [], "entities": []}, {"text": "The character embedding is implemented using CNN with a one-dimensional layer consists of 100 units with a channel size of 5.", "labels": [], "entities": [{"text": "CNN", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9301971197128296}]}, {"text": "It has an input depth of 8.", "labels": [], "entities": []}, {"text": "The max length of SQuAD is 16 which means there area maximum 16 words in a sentence.", "labels": [], "entities": [{"text": "max length of SQuAD", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.8109729215502739}]}, {"text": "The fixed word embedding has a dimension of 100, which is provided by the GloVe data set (Pennington et al., 2014a).", "labels": [], "entities": [{"text": "GloVe data set", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.9213287035624186}]}, {"text": "The settings for syntactic embedding are slightly different for each model.", "labels": [], "entities": []}, {"text": "The BiDAF model does not deal with syntactic information.", "labels": [], "entities": []}, {"text": "The POS model contains syntactic information with 39 different POS tags that serve as both input and output.", "labels": [], "entities": []}, {"text": "For SECT and SEDT the input of the model has a size of 8 with 30 units to be output.", "labels": [], "entities": [{"text": "SECT", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.702582597732544}, {"text": "SEDT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9463285207748413}]}, {"text": "Both of them has a maximum length size that is set to be 10 and 20 respectively, which values will be further discussed in Section 4.5.", "labels": [], "entities": []}, {"text": "They also have two different ways to encode the syntactic information as indi-cated in Section 3: LSTM and CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.8993699550628662}]}, {"text": "We apply the same sets of parameters when we experiment them with the two models.", "labels": [], "entities": []}, {"text": "We report the results on the SQuAD development set and the blind test set.", "labels": [], "entities": [{"text": "SQuAD development set", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.8230976859728495}]}], "tableCaptions": [{"text": " Table 1: Performance comparison on the development set. Each setting contains five runs trained  consecutively. Standard deviations across five runs are shown in the parenthesis for single models. Dots  indicate the level of significance.", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison on the official  blind test set. Ensemble models are trained over  the five single runs with the identical network and  hyper-parameters.", "labels": [], "entities": [{"text": "official  blind test set", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.7279513478279114}]}, {"text": " Table 3: Performance comparisons of models  with only syntactic information against their coun- terparts with randomly shuffled node sequences  and randomly generated tree nodes using the  SQuAD Dev set", "labels": [], "entities": [{"text": "SQuAD Dev set", "start_pos": 190, "end_pos": 203, "type": "DATASET", "confidence": 0.7409950792789459}]}, {"text": " Table 4: Performance means and standard devi- ations of different window sizes on the develop- ment set.", "labels": [], "entities": []}]}