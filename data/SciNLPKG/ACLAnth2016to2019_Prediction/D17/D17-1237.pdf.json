{"title": [{"text": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning", "labels": [], "entities": [{"text": "Composite Task-Completion Dialogue Policy Learning", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6664675414562226}]}], "abstractContent": [{"text": "Building a dialogue agent to fulfill complex tasks, such as travel planning, is challenging because the agent has to learn to collectively complete multiple subtasks.", "labels": [], "entities": [{"text": "travel planning", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7092510014772415}]}, {"text": "For example, the agent needs to reserve a hotel and book a flight so that there leaves enough time for commute between arrival and hotel check-in.", "labels": [], "entities": []}, {"text": "This paper addresses this challenge by formulating the task in the mathematical framework of options over Markov Decision Processes (MDPs), and proposing a hierarchical deep reinforcement learning approach to learning a dialogue manager that operates at different temporal scales.", "labels": [], "entities": []}, {"text": "The dialogue manager consists of: (1) a top-level dialogue policy that selects among subtasks or options , (2) a low-level dialogue policy that selects primitive actions to complete the subtask given by the top-level policy, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied.", "labels": [], "entities": []}, {"text": "Experiments on a travel planning task with simulated and real users show that our approach leads to significant improvements over three baselines, two based on hand-crafted rules and the other based on flat deep reinforcement learning.", "labels": [], "entities": [{"text": "travel planning task", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7686965962251028}]}], "introductionContent": [{"text": "There is a growing demand for intelligent personal assistants, mainly in the form of dialogue agents, that can help users accomplish tasks ranging from meeting scheduling to vacation planning.", "labels": [], "entities": [{"text": "meeting scheduling to vacation planning", "start_pos": 152, "end_pos": 191, "type": "TASK", "confidence": 0.658957451581955}]}, {"text": "However, most of the popular agents in today's market, such as Amazon Echo, Apple Siri, Google Home and Microsoft Cortana, can only handle very simple tasks, such as reporting weather and requesting songs.", "labels": [], "entities": []}, {"text": "Building a dialogue agent to fulfill complex tasks remains one of the most fundamental challenges for the NLP community and AI in general.", "labels": [], "entities": []}, {"text": "In this paper, we consider an important type of complex tasks, termed composite task, which consists of a set of subtasks that need to be fulfilled collectively.", "labels": [], "entities": []}, {"text": "For example, in order to make a travel plan, we need to book air tickets, reserve a hotel, rent a car, etc.", "labels": [], "entities": []}, {"text": "in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints.", "labels": [], "entities": []}, {"text": "Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight's arrival time, hotel check-out time maybe earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and soon.", "labels": [], "entities": []}, {"text": "It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see;;; and fora few recent examples.", "labels": [], "entities": []}, {"text": "Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (.", "labels": [], "entities": []}, {"text": "The first challenge is reward sparsity.", "labels": [], "entities": []}, {"text": "Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory.", "labels": [], "entities": [{"text": "Dialogue policy learning", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7766911586125692}]}, {"text": "Thus, the reward signals (usually provided by users at the end of a conversation) are delayed and sparse.", "labels": [], "entities": []}, {"text": "As we will show in this paper, typical flat RL methods such as DQN with naive \u270f-greedy exploration is rather inefficient.", "labels": [], "entities": []}, {"text": "The second challenge is to satisfy slot constraints across subtasks.", "labels": [], "entities": []}, {"text": "This requirement makes most of the existing methods of learning multidomain dialogue agents) inapplicable: these methods train a collection of policies, one for each domain, and there is no cross-domain constraints required to successfully complete a dialogue.", "labels": [], "entities": []}, {"text": "The third challenge is improved user experience: we find in our experiments that a flat RL agent tends to switch between different subtasks frequently when conversing with users.", "labels": [], "entities": []}, {"text": "Such incoherent conversations lead to poor user experience, and are one of the main reasons that cause a dialogue session to fail.", "labels": [], "entities": []}, {"text": "In this paper, we address the above mentioned challenges by formulating the task using the mathematical framework of options over), and proposing a method that combines deep reinforcement learning and hierarchical task decomposition to train a composite taskcompletion dialogue agent.", "labels": [], "entities": []}, {"text": "At the heart of the agent is a dialogue manager, which consists of (1) a top-level dialogue policy that selects subtasks (options), (2) a low-level dialogue policy that selects primitive actions to complete a given subtask, and (3) a global state tracker that helps ensure all cross-subtask constraints be satisfied.", "labels": [], "entities": []}, {"text": "Conceptually, our approach exploits the structural information of composite tasks for efficient exploration.", "labels": [], "entities": []}, {"text": "Specifically, in order to mitigate the reward sparsity issue, we equip our agent with an evaluation module (internal critic) that gives intrinsic reward signals, indicating how likely a particular subtask is completed based on its current state generated by the global state tracker.", "labels": [], "entities": []}, {"text": "Such intrinsic rewards can be viewed as heuristics that encourage the agent to focus on solving a subtask before moving onto another subtask.", "labels": [], "entities": []}, {"text": "Our experiments show that such intrinsic rewards can be used inside a hierarchical RL agent to make exploration more efficient, yielding a significantly reduced state-action space for decision making.", "labels": [], "entities": [{"text": "decision making", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.8617429137229919}]}, {"text": "Furthermore, it leads to a better user experience, as the resulting conversations switch between subtasks less frequently.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that strives to develop a composite taskcompletion dialogue agent.", "labels": [], "entities": []}, {"text": "Our main contributions are three-fold: \u2022 We formulate the problem in the mathematical framework of options over MDPs.", "labels": [], "entities": []}, {"text": "\u2022 We propose a hierarchical deep reinforcement learning approach to efficiently learning the dialogue manager that operates at different temporal scales.", "labels": [], "entities": []}, {"text": "\u2022 We validate the effectiveness of the proposed approach in a travel planning task on simulated as well as real users.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed method, we conduct experiments on the composite task-completion dialogue task of travel planning.", "labels": [], "entities": []}, {"text": "In the study, we made use of a human-human conversation data derived from a publicly available multi-domain dialogue corpus 1, which was collected using the Wizard-ofOz approach.", "labels": [], "entities": []}, {"text": "We made a few changes to the schema of the data set for the composite taskcompletion dialogue setting.", "labels": [], "entities": []}, {"text": "Specifically, we added inter-subtask constraints as well as user preferences (soft constraints).", "labels": [], "entities": []}, {"text": "The data was mainly used to create simulated users, as will be explained below shortly.", "labels": [], "entities": []}, {"text": "On the composite task-completion dialogue task, we compared the HRL agent with the baseline agents in terms of three metrics: success rate 3 , average rewards, and the average number of turns per dialogue session.", "labels": [], "entities": [{"text": "success rate 3", "start_pos": 126, "end_pos": 140, "type": "METRIC", "confidence": 0.9327055017153422}]}, {"text": "shows the learning curves of all four agents trained on different types of users.", "labels": [], "entities": []}, {"text": "Each learning curve was averaged over 10 runs.", "labels": [], "entities": []}, {"text": "shows the performance on test data.", "labels": [], "entities": []}, {"text": "For all types of users, the HRL-based agent yielded more robust dialogue policies outperforming the hand-crafted rule-based agents and flat RL-based agent measured on success rate.", "labels": [], "entities": []}, {"text": "It also needed fewer turns per dialogue session to accomplish a task than the rule-based agents and flat RL agent.", "labels": [], "entities": []}, {"text": "The results  across all three types of simulated users suggest the following conclusions.", "labels": [], "entities": []}, {"text": "First, he HRL agent significantly outperformed the RL agent.", "labels": [], "entities": [{"text": "RL agent", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.6366166323423386}]}, {"text": "This, to a large degree, was attributed to the use of the hierarchical structure of the proposed agent.", "labels": [], "entities": []}, {"text": "Specifically, the top-level dialogue policy selected a subtask for the agent to focus on, one at a time, thus dividing a complex task into a sequence of simpler subtasks.", "labels": [], "entities": []}, {"text": "The selected subtasks, combined with the use of intrinsic rewards, alleviated the sparse reward and longhorizon issues, and helped the agent explore more efficiently in the state-action space.", "labels": [], "entities": [{"text": "longhorizon", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.9878231287002563}]}, {"text": "As a result, as shown in and, the performance of the HRL agent on types B and C users (who may need to go back to revise some slots during the dialogue) does not drop much compared to type A users, despite the increased search space in the former.", "labels": [], "entities": []}, {"text": "Additionally, we observed a large drop in the performance of the RL Agent due to the increased complexity of the task, which required more dialogue turns and posed a challenge for temporal credit assignment.", "labels": [], "entities": [{"text": "temporal credit assignment", "start_pos": 180, "end_pos": 206, "type": "TASK", "confidence": 0.7877530455589294}]}, {"text": "Second, the HRL agent learned much faster than the RL agent.", "labels": [], "entities": []}, {"text": "The HRL agent could reach the same level of performance with a smaller number of simulation examples than the RL agent, demonstrating that the hierarchical dialogue policies were more sample-efficient than flat RL policy and could significantly reduce the sample complexity on complex tasks.", "labels": [], "entities": []}, {"text": "Finally, we also found that the Rule+ and flat RL agents had comparable success rates, as shown in.", "labels": [], "entities": []}, {"text": "However, a closer look at the correlation between success rate and the average number of turns in suggests that the Rule+ agent required more turns which adversely affects its success, whilst the flat RL agent achieves similar success with much less number of turns in all the user types.", "labels": [], "entities": []}, {"text": "It suffices to say that our hierarchical RL agent outperforms all in terms of success rate as depicted in.", "labels": [], "entities": []}, {"text": "We further evaluated the agents, which were trained on simulated users, against real human users, recruited from the authors' affiliation.", "labels": [], "entities": []}, {"text": "We conducted the study using the HRL and RL agents, each tested against two types of users: Type A users who had no preference for subtask, and Type B users who preferred to complete the book-flightticket subtask first.", "labels": [], "entities": []}, {"text": "Note that Type C users were symmetric to Type B ones, so were not included in the study.", "labels": [], "entities": []}, {"text": "We compared two (agent, user type) pairs: {RL A, HRL A} and {RL B, HRL B}; in other words, four agents were trained against their specific user types.", "labels": [], "entities": []}, {"text": "In each dialogue session, one of the agents was randomly picked to converse with a user.", "labels": [], "entities": []}, {"text": "The user was presented with a user goal sampled from our corpus, and was instructed to converse with the agent to complete the task.", "labels": [], "entities": []}, {"text": "If one of the slots in the goal had multiple values, the user had multiple choices for this slot and might revise the slot value when the agent replied with a message like \"No ticket is available\" during the conversation.", "labels": [], "entities": []}, {"text": "At the end of each session, the user was asked to give a rating on a scale from 1 to 5 based on the naturalness and coherence of the dialogue.", "labels": [], "entities": []}, {"text": "(1 is the worst rating, and 5 the best).", "labels": [], "entities": []}, {"text": "We collected a total of 225 dialogue sessions from 12 human users.", "labels": [], "entities": []}, {"text": "presents the performance of these agents against real users in terms of success rate.", "labels": [], "entities": []}, {"text": "shows the comparison in user rating.", "labels": [], "entities": []}, {"text": "For all the cases, the HRL agent was consistently better than the RL agent in terms of success rate and user rating.", "labels": [], "entities": []}, {"text": "shows a sample dialogue session.", "labels": [], "entities": []}, {"text": "We see that the HRL agent produced a more coherent conversation, as it switched among subtasks much less frequently than the flat RL agent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of three agents on different User Types. Tested on 2000 dialogues using the best  model during training. Succ.: success rate, Turn: average turns, Reward: average reward.", "labels": [], "entities": [{"text": "Succ.", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9969704151153564}, {"text": "Turn", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9623148441314697}, {"text": "Reward", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9707794189453125}]}]}