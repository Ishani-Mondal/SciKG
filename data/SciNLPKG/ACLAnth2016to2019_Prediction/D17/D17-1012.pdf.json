{"title": [{"text": "Neural Machine Translation with Source-Side Latent Graph Parsing", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7261486053466797}]}], "abstractContent": [{"text": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences.", "labels": [], "entities": []}, {"text": "Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neu-ral machine translation model, and thus the parser is optimized according to the translation objective.", "labels": [], "entities": [{"text": "attention-based neu-ral machine translation", "start_pos": 141, "end_pos": 184, "type": "TASK", "confidence": 0.6205881685018539}]}, {"text": "In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models.", "labels": [], "entities": []}, {"text": "We also show that the performance of our model can be further improved by pre-training it with a small amount of tree-bank annotations.", "labels": [], "entities": []}, {"text": "Our final ensemble model significantly outperforms the previous best models on the standard English-to-Japanese translation dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8576846718788147}]}, {"text": "Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.7817378640174866}]}, {"text": "The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks.", "labels": [], "entities": []}, {"text": "An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences  along with the target task.", "labels": [], "entities": []}, {"text": "Motivated by the promising results of recent joint learning approaches, we present a novel NMT model that can learn a task-specific latent graph structure for each source-side sentence.", "labels": [], "entities": []}, {"text": "The graph structure is similar to the dependency structure of the sentence, but it can have cycles and is learned specifically for the translation task.", "labels": [], "entities": []}, {"text": "Unlike the aforementioned approach of learning single syntactic trees, our latent graphs are composed of \"soft\" connections, i.e., the edges have realvalued weights).", "labels": [], "entities": []}, {"text": "Our model consists of two parts: one is a task-independent parsing component, which we calla latent graph parser, and the other is an attention-based NMT model.", "labels": [], "entities": []}, {"text": "The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.9009509682655334}]}, {"text": "In experiments, we demonstrate that our model can be effectively pre-trained by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model.", "labels": [], "entities": []}, {"text": "Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset.", "labels": [], "entities": [{"text": "WAT English-to-Japanese dataset", "start_pos": 88, "end_pos": 119, "type": "DATASET", "confidence": 0.858026385307312}]}], "datasetContent": [{"text": "We first show our translation results using the small and medium training datasets.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9687096476554871}]}, {"text": "We report averaged scores with standard deviations across five different runs of the model training.  and UNI, which shows that the small training dataset is not enough to learn useful latent graph structures from scratch.", "labels": [], "entities": [{"text": "UNI", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.5246227979660034}]}, {"text": "However, LGP-NMT+ (K = 10,000) outperforms SEQ and UNI, and the standard deviations are the smallest.", "labels": [], "entities": [{"text": "LGP-NMT+", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.8086122572422028}, {"text": "SEQ", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.4706917405128479}, {"text": "UNI", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.6946569085121155}]}, {"text": "Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model.", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9662502408027649}, {"text": "translation", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.9474763870239258}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9216147065162659}]}, {"text": "We can also see that DEP performs the worst.", "labels": [], "entities": []}, {"text": "This is not surprising because previous studies, e.g.,, have reported that using syntactic structures do not always outperform competitive sequential models in several NLP tasks.", "labels": [], "entities": []}, {"text": "Now that we have observed the effectiveness of pre-training our model, one question arises naturally: how many training samples for parsing and tagging are necessary for improving the translation accuracy?", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.6231245299180349}, {"text": "translation", "start_pos": 184, "end_pos": 195, "type": "TASK", "confidence": 0.9404734969139099}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.8359515070915222}]}, {"text": "shows the results of using different numbers of training samples for parsing and tagging.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9777798652648926}, {"text": "tagging", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.8486454486846924}]}, {"text": "The results of K= 0 and K= 10,000 correspond to those of LGP-NMT and LGP-NMT+ in Table 1, respectively.", "labels": [], "entities": [{"text": "K", "start_pos": 15, "end_pos": 16, "type": "METRIC", "confidence": 0.9641168117523193}, {"text": "LGP-NMT", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9022190570831299}]}, {"text": "We can see that using the small amount of the training samples performs better than using all the training samples.", "labels": [], "entities": []}, {"text": "One possible reason is that the domains of the translation dataset and the parsing (tagging) dataset are considerably different.", "labels": [], "entities": [{"text": "parsing (tagging)", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8723898679018021}]}, {"text": "The parsing and tagging datasets come from WSJ, whereas the translation dataset comes from abstract text of scientific papers in a wide range of domains, such as: Evaluation on the development data using the medium training dataset (100,000 pairs).", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.783555825551351}, {"text": "WSJ", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9494372010231018}]}, {"text": "These results suggest that our model can be improved by a small amount of parsing and tagging datasets in different domains.", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.6356531083583832}]}, {"text": "Considering the recent universal dependency project which covers more than 50 languages, our model has the potential of being applied to a variety of language pairs.", "labels": [], "entities": []}, {"text": "shows the results of using the medium training dataset.", "labels": [], "entities": []}, {"text": "In contrast with using the small training dataset, LGP-NMT is slightly better than SEQ.", "labels": [], "entities": []}, {"text": "LGP-NMT significantly outperforms UNI, which shows that our adaptive learning is more effective than using the uniform graph weights.", "labels": [], "entities": [{"text": "UNI", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7497332692146301}]}, {"text": "By pre-training our model, LGP-NMT+ significantly outperforms SEQ in terms of the BLEU score.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.866990327835083}, {"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9717094004154205}]}, {"text": "Again, DEP performs the worst among all the models.", "labels": [], "entities": [{"text": "DEP", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.6406708359718323}]}, {"text": "By using our beam search strategy, the Brevity Penalty (BP) values of our translation results are equal to or close to 1.0, which is important when evaluating the translation results using the BLEU scores.", "labels": [], "entities": [{"text": "Brevity Penalty (BP)", "start_pos": 39, "end_pos": 59, "type": "METRIC", "confidence": 0.986239755153656}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9973522424697876}]}, {"text": "A BP value ranges from 0.0 to 1.0, and larger values mean that the translated sentences have relevant lengths compared with the reference translations.", "labels": [], "entities": [{"text": "BP", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.9985440969467163}]}, {"text": "As a result, our BLEU evaluation results are affected only by the word n-gram precision scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9924545288085938}, {"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.6675562262535095}]}, {"text": "BLEU scores are sensitive to the BP values, and thus our beam search strategy leads to more solid evaluation for NMT models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9886820316314697}, {"text": "BP", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9816600680351257}]}, {"text": "Again, we see that the translation scores of our model can be further improved by pre-training the model.", "labels": [], "entities": [{"text": "translation", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.8453517556190491}]}, {"text": "shows our results on the test data, and the previous best results summarized in and the WAT website 5 are also shown.", "labels": [], "entities": [{"text": "WAT website 5", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8843017220497131}]}, {"text": "Our proposed models, LGP-NMT and LGP-NMT+, outperform not only SEQ but also all of the previous best results.", "labels": [], "entities": [{"text": "SEQ", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.4509977698326111}]}, {"text": "Notice also that our implementation of the sequential model (SEQ) provides a very strong baseline, the performance of which is already comparable to the previous state of the art, even without using ensemble techniques.", "labels": [], "entities": []}, {"text": "The confidence interval (p \u2264 0.05) of the RIBES score of LGP-NMT+ estimated by bootstrap resampling, and thus the RIBES score of LGP-NMT+ is significantly better than that of SEQ, which shows that our latent parser can be effectively pre-trained with the human-annotated treebank.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9542393684387207}, {"text": "RIBES", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.8339792490005493}]}, {"text": "The sequential NMT model in and the tree-to-sequence NMT model in rely on ensemble techniques while our results mentioned above are obtained using single models.", "labels": [], "entities": []}, {"text": "Moreover, our model is more compact 6 than the previous best NMT model in.", "labels": [], "entities": []}, {"text": "By applying the ensemble technique to LGP-NMT, LGP-NMT+, As a result , it was found that a path which crosses a sphere obliquely existed .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation on the development data using  the small training dataset (20,000 pairs).", "labels": [], "entities": []}, {"text": " Table 2: Effects of the size K of the training  datasets for POS tagging and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.8740878999233246}, {"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8080903887748718}]}, {"text": " Table 3: Evaluation on the development data using  the medium training dataset (100,000 pairs).", "labels": [], "entities": []}, {"text": " Table 4: BLEU (B.) and RIBES (R.) scores on the  development data using the large training dataset.", "labels": [], "entities": [{"text": "BLEU (B.)", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8090524598956108}, {"text": "RIBES (R.)", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.926036924123764}]}, {"text": " Table 5: BLEU and RIBES scores on the test data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992546439170837}, {"text": "RIBES", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9968998432159424}]}]}