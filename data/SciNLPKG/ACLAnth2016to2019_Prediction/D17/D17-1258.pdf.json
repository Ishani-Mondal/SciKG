{"title": [{"text": "Does syntax help discourse segmentation? Not so much", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7788233160972595}]}], "abstractContent": [{"text": "Discourse segmentation is the first step in building discourse parsers.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7971790134906769}]}, {"text": "Most work on discourse segmentation does not scale to real-world discourse parsing across languages , for two reasons: (i) models rely on constituent trees, and (ii) experiments have relied on gold standard identification of sentence and token boundaries.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7403693497180939}]}, {"text": "We therefore investigate to what extent constituents can be replaced with universal dependencies , or left out completely, as well as how state-of-the-art segmenters fare in the absence of sentence boundaries.", "labels": [], "entities": []}, {"text": "Our results show that dependency information is less useful than expected, but we provide a fully scalable, robust model that only relies on part-of-speech information, and show that it performs well across languages in the absence of any gold-standard annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse segmentation is the task of identifying, in a document, the minimal units of textcalled Elementary Discourse Units (EDU)) -that will be then linked by semantico-pragmatic relations -called discourse relations.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7947197258472443}]}, {"text": "Discourse segmentation is the first step when building a discourse parser, and has a large impact on the building of the final structurepredicted segmentation leads to a drop in performance of about 12-14% (.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7454691529273987}]}, {"text": "In this work, we focus on the Rhetorical Structure Theory (RST) ( in which discourse analysis is a tree covering an entire document.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7838889559110006}, {"text": "discourse analysis", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7216868698596954}]}, {"text": "Most of the recent discourse parsers have been developed within this framework, making crucial the development of robust RST discourse segmenters.", "labels": [], "entities": [{"text": "discourse parsers", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7282429933547974}, {"text": "RST discourse segmenters", "start_pos": 121, "end_pos": 145, "type": "TASK", "confidence": 0.9360814094543457}]}, {"text": "Many corpora have been annotated within this framework for several domains and languages -such as English with the RST Discourse Treebank (RST-DT) ), but also Spanish (da), Brazilian Portuguese () or German ().", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT)", "start_pos": 115, "end_pos": 146, "type": "DATASET", "confidence": 0.8937318921089172}]}, {"text": "State-of-the-art performance for discourse segmentation on the RST-DT is about 94% in F 1 (Xuan).", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7203050702810287}, {"text": "F 1 (Xuan)", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.6231066584587097}]}, {"text": "Most work on discourse parsing has focused on English and on the RST-DT (, and so discourse segmentation.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7137535065412521}, {"text": "discourse segmentation", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.7276023626327515}]}, {"text": "And while discourse parsing is a document level task, discourse segmentation is done at the sentence level, assuming that sentence boundaries are known.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.721373975276947}, {"text": "discourse segmentation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7327894568443298}]}, {"text": "This prevents from using discourse information fora wider range of downstream tasks.", "labels": [], "entities": []}, {"text": "Moreover, while discourse parsing is a semantic task involving a large range of information, the annotation guidelines reflect that segmentation is merely based on syntax: in practice, an EDU cannot overlap sentence boundaries -while some discourse trees can cross the sentence boundaries (van der Vliet and Redeker, 2011) -, and deciding whether a clause is an EDU in the RST-DT strongly depends on its syntactic functione.g.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7253618836402893}]}, {"text": "\"Clauses that are subjects or objects of a main verb are not treated as EDUs\" ).", "labels": [], "entities": []}, {"text": "Consequently, existing discourse segmenters heavily rely on information derived from constituent trees usually following the Penn Treebank (PTB) guidelines.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.9700684785842896}]}, {"text": "Nevertheless constituent trees are not easily available for any language.", "labels": [], "entities": []}, {"text": "Finally, even for English, using predicted trees leads to a large drop in per-formance for discourse segmentation.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7182958871126175}]}, {"text": "Recently, proposed the first cross-lingual and cross-domain experiments for discourse segmentation, relying only on words and Part-of-Speech (POS) tags (morpho-syntactic level).", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.7196560353040695}]}, {"text": "However, they focus on document-level discourse segmentation -preventing from a comparison with previous work -, and they did not include any syntactic information.", "labels": [], "entities": [{"text": "document-level discourse segmentation", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.5920132100582123}]}, {"text": "In this paper, we significantly extend their work by investigating the use of syntactic information, reporting results with various sets of features at the sentence level -varying the settings between gold and predicted, and fine-grained vs coarse grained information -, and studying the impact of tokenisation.", "labels": [], "entities": []}], "datasetContent": [{"text": "For English, on the En-DT, evaluation for discourse segmentation has been done under different conditions.", "labels": [], "entities": [{"text": "En-DT", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9673531651496887}, {"text": "discourse segmentation", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7223715782165527}]}, {"text": "First, all previous systems were evaluated on the same set of 38 documents that initially contains 991 sentences -and more precisely on each sentence of this set for intra-sentential results.", "labels": [], "entities": []}, {"text": "However, Soricut and Marcu (2003) do not consider sentences that are not exactly spanned by a discourse subtree (keeping only 941 sentences in the test set), and Sporleder and only keep the sentences that contain intra-sentential EDUs (608 sentences).", "labels": [], "entities": []}, {"text": "Since we want to give results at the document level, -with the sentence boundaries being predicted as the other EDU boundaries -, there is no reason to remove any sentences.", "labels": [], "entities": []}, {"text": "We thus keep all the 991 sentences in the test set as done in) at the sentence level, and in () at the document level.", "labels": [], "entities": []}, {"text": "For the other corpora (see Section 5), we either use the official test set (Es-DT, 84 documents) or build a test set containing 38 documents chosen randomly.", "labels": [], "entities": []}, {"text": "Second, since Soricut and Marcu (2003), the evaluation scores do not include the first boundary of a sentence.", "labels": [], "entities": []}, {"text": "Exceptions are), and some results in given to compare with the former.", "labels": [], "entities": []}, {"text": "For intra-sentential results, we also ignore the first boundary of each sentence when computing the final score.", "labels": [], "entities": []}, {"text": "At the document level, we ignore the first boundary of each document (thus keeping the first boundary of the sentences within the document).", "labels": [], "entities": []}, {"text": "The reported score is the F 1 over the boundaries (the 'B' labels), ignoring the non-boundary words ('I' labels).", "labels": [], "entities": [{"text": "F 1", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9893952012062073}]}], "tableCaptions": [{"text": " Table 1: Number of documents, EDUs, sentences  and words (according to UDPipe).", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 72, "end_pos": 78, "type": "DATASET", "confidence": 0.8550644516944885}]}, {"text": " Table 3:  Multi-domain and multi-lingual  document-level results. State-of-the-art (SOA)  results reported in (", "labels": [], "entities": []}]}