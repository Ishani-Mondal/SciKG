{"title": [{"text": "Identifying Where to Focus in Reading Comprehension for Neural Question Generation", "labels": [], "entities": [{"text": "Identifying", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9687141180038452}, {"text": "Neural Question Generation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.8143098751703898}]}], "abstractContent": [{"text": "A first step in the task of automatically generating questions for testing reading comprehension is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about.", "labels": [], "entities": []}, {"text": "We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored.", "labels": [], "entities": [{"text": "sentence-level sequence tagging", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.6629419525464376}, {"text": "question generation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7205986082553864}]}, {"text": "The approach is fully data-driven-with no sophisticated NLP pipelines or any hand-crafted rules/features-and compares favorably to a number of base-lines when evaluated on the SQuAD data set.", "labels": [], "entities": [{"text": "SQuAD data set", "start_pos": 176, "end_pos": 190, "type": "DATASET", "confidence": 0.884780764579773}]}, {"text": "When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for reading comprehension .", "labels": [], "entities": [{"text": "paragraph-level question generation", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.6356980800628662}]}], "introductionContent": [], "datasetContent": [{"text": "We use the SQuAD dataset () for training and evaluation for both important sentence selection and sentence-level NQG.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.8358310461044312}]}, {"text": "The dataset contains 536 curated Wikipedia articles with over 100k questions posed about the articles.", "labels": [], "entities": []}, {"text": "The authors employ Amazon Mechanical Turk crowd-workers to generate questions based on the article paragraphs and to annotate the corresponding answer spans in the text.", "labels": [], "entities": []}, {"text": "Later, to make the evaluation of the dataset more robust, other crowd-workers are employed to provide additional answers to the questions.", "labels": [], "entities": []}, {"text": "We split the public portion of the dataset into training (\u223c80%), validation (\u223c10%) and test (\u223c10%) sets at the paragraph level.", "labels": [], "entities": []}, {"text": "For the sentence selection task, we treat sentences that contain at least one answer span (question-worthy sentences) as positive examples (y = 1); all remaining sentences are considered negative (y = 0).", "labels": [], "entities": [{"text": "sentence selection task", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8083431323369344}]}, {"text": "Not surprisingly, the training set is unbalanced: 52332 (\u223c60%) sentences contain answers, while: Results for the full QG systems using BLEU 1-4, METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9944384098052979}, {"text": "METEOR", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9785739779472351}]}, {"text": "The first stage of the two pipeline systems are the feature-rich linear model (LREG) and our best performing selection model respectively.", "labels": [], "entities": []}, {"text": "ity of human choice in generating questions, it is the case that many sentences labeled as negative examples might actually contain concepts worth asking a question about.", "labels": [], "entities": []}, {"text": "For the related important sentence detection task in text summarization,  therefore propose a two-stage approach ( to augment the set of known summaryworthy sentences.", "labels": [], "entities": [{"text": "sentence detection", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7259217351675034}, {"text": "text summarization", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7211131453514099}]}, {"text": "In contrast, we adopt a conservative approach rather than predict too many sentences as being question-worthy: we pair up source sentences with their corresponding questions, and use just these sentence-question pairs to training the encoder-decoder model.", "labels": [], "entities": []}, {"text": "We use the glove.840B.300d pre-trained embeddings () for initialization of the embedding layer for our sentence selection model and the full NQG model.", "labels": [], "entities": []}, {"text": "glove.6B.100d embeddings are used for calculating sentence similarity feature of the baseline linear model (LREG).", "labels": [], "entities": []}, {"text": "Tokens outside the vocabulary list are replaced by the UNK symbol.", "labels": [], "entities": [{"text": "UNK symbol", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8868693113327026}]}, {"text": "Hyperparameters for all models are tuned on the validation set and results are reported on the test set.", "labels": [], "entities": []}, {"text": "To evaluate the full systems for paragraph-level QG, we introduce in the \"conservative\" and \"liberal\" evaluation strategies.", "labels": [], "entities": [{"text": "paragraph-level QG", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.5523266345262527}]}, {"text": "Given an input source sentence, there will be in total four possibilities: if both the gold standard data and prediction include the sentence, then we use its n-gram matching score (by BLEU () and METEOR); if neither the gold data nor prediction include the sentence, then the sentence is discarded from the evaluation; if the gold data includes the sentence while the prediction does not, we assign a score of 0 for it; and if gold data does not include the sentence while prediction does, the generated question gets a 0 for conservative, while it gets full Wikipedia paragraph: arnold schwarzenegger has been involved with the special olympics for many years after they were founded by his ex-mother-in-law , eunice kennedy shriver . after they were founded by his ex-mother-in-law , eunice kennedy shriver . in 2007 , schwarzenegger was the official spokesperson for the special olympics which were held in shanghai , china . schwarzenegger believes that quality school opportunities should be made available to children who might not normally be able to access them.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9990956783294678}, {"text": "METEOR", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9978004097938538}]}, {"text": "in 1995 , he founded the inner city games foundation -lrb-icg -rrb-which provides cultural , Our questions: Q1: who founded the special olympics ? Q2: who was the official adviser for the special olympics ? Q3: when was the inner city games foundation founded ? Q4: how many schools does icg have ? Gold questions: Q1: schwarzenegger was the spokesperson for the special olympic games held in what city in china ? Q2: what nonprofit did schwarzenegger found in 1995 ? Q3: about how many schools across the country is icg active in ? Figure 2: Sample output from our full NQG system, the four questions correspond to the four highlighted sentences in the paragraph in the same order.", "labels": [], "entities": [{"text": "NQG system", "start_pos": 571, "end_pos": 581, "type": "DATASET", "confidence": 0.9276582896709442}]}, {"text": "Darkness indicates sentence importance, the score for deciding the darkness is obtained from the softmax results.", "labels": [], "entities": []}, {"text": "Wave-lined sentences bear label y = 1, and 0 otherwise.", "labels": [], "entities": []}, {"text": "The three gold questions also correspond to the wave-lined sentences in the same order.", "labels": [], "entities": []}, {"text": "Please refer to the appendix for sample output on more Wikipedia articles.", "labels": [], "entities": [{"text": "appendix", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9460362792015076}]}, {"text": "shows that the QG system incorporating our best performing sentence extractor outperforms its LREG counterpart across metrics.", "labels": [], "entities": []}, {"text": "Note that to calculate the score for the matching case, similar to our earlier work (, we adapt the image captioning evaluation scripts of since there can be several gold standard questions fora single input sentence.", "labels": [], "entities": []}, {"text": "In, we provide questions generated by the full NQG system (Q1-4) and according to the gold standard (Q1-3) for the selected Wikipedia paragraph.", "labels": [], "entities": [{"text": "NQG", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8973205089569092}]}, {"text": "The sentences they were drawn from are shown with wavy lines (gold standard) and via highlighting (our system).", "labels": [], "entities": []}, {"text": "Darkness of the highlighting is proportional to the softmax score provided by the sentence extractor.", "labels": [], "entities": [{"text": "Darkness", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9165226221084595}]}], "tableCaptions": [{"text": " Table 1: Automatic evaluation results for important sentence selection. The best performing system in  each column is highlighted in boldface. Paragraph-level accuracies are calculated as the proportion of  paragraphs in which all of the sentences are predicted correctly. We show two-tailed t-test results on  F-measure for our best performing method compared to the other baselines. (Statistical significance is  indicated with  *", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7326768636703491}, {"text": "Paragraph-level accuracies", "start_pos": 144, "end_pos": 170, "type": "METRIC", "confidence": 0.7802064418792725}, {"text": "F-measure", "start_pos": 312, "end_pos": 321, "type": "METRIC", "confidence": 0.9809234142303467}, {"text": "Statistical significance", "start_pos": 387, "end_pos": 411, "type": "METRIC", "confidence": 0.9404173493385315}]}, {"text": " Table 2: Results for the full QG systems using BLEU 1-4, METEOR. The first stage of the two pipeline  systems are the feature-rich linear model (LREG) and our best performing selection model respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9941169023513794}, {"text": "METEOR", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9757593274116516}]}]}