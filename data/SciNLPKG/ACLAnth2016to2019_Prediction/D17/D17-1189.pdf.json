{"title": [{"text": "A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction", "labels": [], "entities": [{"text": "Noise-tolerant Distantly Supervised Relation Extraction", "start_pos": 24, "end_pos": 79, "type": "TASK", "confidence": 0.735568106174469}]}], "abstractContent": [{"text": "Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels rela-tional facts with knowledge bases.", "labels": [], "entities": [{"text": "Distant-supervised relation extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6165608763694763}]}, {"text": "Previous sentence level denoise models don't achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training.", "labels": [], "entities": []}, {"text": "To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training.", "labels": [], "entities": []}, {"text": "We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain anew label, namely a soft label, for certain entity pair.", "labels": [], "entities": []}, {"text": "During training, soft labels instead of hard labels serve as gold labels.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms the state-of-the-art systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation Extraction (RE) aims to obtain relational facts from plain text.", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.91692715883255}]}, {"text": "Traditional supervised RE systems suffer from lack of manually labeled data.", "labels": [], "entities": [{"text": "RE", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.96610426902771}]}, {"text": "proposes distant supervision, which exploits relational facts in knowledge bases.", "labels": [], "entities": []}, {"text": "Distant supervision automatically generates training examples by aligning entity mentions in plain text with those in KB and labeling entity pairs with their relations in KB.", "labels": [], "entities": []}, {"text": "If there's no relation link between certain entity pair in KB, it will be labeled as negative instance (NA).", "labels": [], "entities": []}, {"text": "However, the automatic labeling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled.", "labels": [], "entities": []}, {"text": "Multi-instances learning (MIL) is proposed by to combat the noise.", "labels": [], "entities": [{"text": "Multi-instances learning (MIL)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6972493171691895}]}, {"text": "The method divides the training set into multiple bags of entity pairs (shown in and labels the bags with the relations of entity pairs in the KB.", "labels": [], "entities": []}, {"text": "Each bag consists of sentences mentioning both head and tail entities.", "labels": [], "entities": []}, {"text": "Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption ( and attention mechanisms over instances ().", "labels": [], "entities": []}, {"text": "However, the sentence level denoise methods can't fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur-ing training, no matter whether they are corrector not.", "labels": [], "entities": []}, {"text": "As shown in due to the absence of (Jan Eliasson 1 , Sweden) from Nationality relation in the KB, the entity pair is mislabeled as NA.", "labels": [], "entities": [{"text": "Jan Eliasson 1 , Sweden)", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.7623074700435003}, {"text": "KB", "start_pos": 93, "end_pos": 95, "type": "DATASET", "confidence": 0.6088516712188721}]}, {"text": "However, we find the sentences in the bag of (Jan Eliasson, Sweden) share similar semantic pattern \"X of Y\" with correctly labeled instances (blue).", "labels": [], "entities": []}, {"text": "In the false positive instance, Sebastian Roch is indeed from France, but the syntactic pattern of the sentence in the bag differs greatly from those of correctly labeled instances.", "labels": [], "entities": []}, {"text": "Actually, the reliability of a distant-supervised (DS) label can be determined by the syntactic/semantic similarity between certain instance and the potential correctly labeled instances.", "labels": [], "entities": [{"text": "reliability", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9589704275131226}]}, {"text": "Soft-label method intends to utilize corresponding similarities to correct wrong DS labels in the training stage dynamically, which means the same bag may have different soft labels in different epochs of training.", "labels": [], "entities": []}, {"text": "The basis of soft-label method is the dominance of correctly labeled instances.", "labels": [], "entities": []}, {"text": "Fortunately, proves that correctly labeled instances account for 94.4% (including true negatives) in the distant-supervised New York Times corpus (benchmark dataset).", "labels": [], "entities": [{"text": "New York Times corpus (benchmark dataset", "start_pos": 124, "end_pos": 164, "type": "DATASET", "confidence": 0.8434029221534729}]}, {"text": "To this end, we introduce a soft-label method to correct wrong labels at entity-pair level during training by exploiting semantic/syntactic information from correctly labeled instances.", "labels": [], "entities": []}, {"text": "In our model, the representation of certain entity pair is a weighted combination of related sentences which are encoded by piecewise convolutional neural network (PCNN) ().", "labels": [], "entities": []}, {"text": "Besides, we propose a joint score function to obtain soft labels during training by taking both the confidence of DS labels and the entity-pair representations into consideration.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: \u2022 To the best of our knowledge, we first propose an entity-pair level noise-tolerant method while previous works only focused on sentence level noise.", "labels": [], "entities": []}, {"text": "\u2022 We propose a simple but effective method called soft-label method to dynamically correct wrong labels during training.", "labels": [], "entities": []}, {"text": "Case study shows our corrections are of high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9989556074142456}]}, {"text": "\u2022 We evaluate our model on the benchmark dataset and achieve substantial improvement compared with the state-of-the-art systems.", "labels": [], "entities": []}, {"text": "1 Jan Eliasson is a Swedish diplomat.", "labels": [], "entities": [{"text": "Swedish diplomat", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7080011963844299}]}], "datasetContent": [{"text": "In this section, we first introduce the dataset and evaluation metrics in our experiments.", "labels": [], "entities": []}, {"text": "Then, we demonstrate the parameter settings in our experiments.", "labels": [], "entities": []}, {"text": "Besides, we compare the performance of our method with state-of-the-art feature-based and neural network baselines.", "labels": [], "entities": []}, {"text": "Case study shows our soft-label corrections are of high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9987866282463074}]}, {"text": "We evaluate our model on the benchmark dataset proposed by, which has also been used by: Top-N precision (P@N) for relation extraction in the entity pairs with different number of sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.519528329372406}, {"text": "relation extraction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7657566666603088}]}, {"text": "Following (, One, Two and All test settings random select one/two/all sentences on the bags of entity pairs from the testing set which have more than one sentence to predict relation.", "labels": [], "entities": []}, {"text": "We use cross-validation to determine the parameters in our model.", "labels": [], "entities": []}, {"text": "Soft-label method uses PCNN-ONE/PCNN-ATT to represent the bags of entity pairs, and we don't tune on the parameters of PCNN-ONE/PCNN-ATT for fair comparsion.", "labels": [], "entities": []}, {"text": "So we use the same pre-trained word embeddings and parameters of CNN encoder as those of.", "labels": [], "entities": [{"text": "CNN encoder", "start_pos": 65, "end_pos": 76, "type": "DATASET", "confidence": 0.8917487561702728}]}, {"text": "Detailed parameter settings are shown in.", "labels": [], "entities": []}, {"text": "Moreover, we use Adam optimizer.", "labels": [], "entities": []}, {"text": "Besides, to avoid negative effects of dominant NA instances in the begining of training, soft-label method is adopted after 3000 steps of parameter updates.", "labels": [], "entities": []}, {"text": "The confidence vector A is heuristically set as [0.9, 0.7, \u00b7 \u00b7 \u00b7 , 0.7] (the confidence of NA is 0.9 while confidence of other relations are all 0.7).", "labels": [], "entities": [{"text": "confidence vector A", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9235853552818298}]}], "tableCaptions": [{"text": " Table 1: Parameter settings of our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Top-N precision (P@N) for relation extraction in the entity pairs with different number of sen- tences. Following (", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.5713716745376587}, {"text": "relation extraction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7705026566982269}]}]}