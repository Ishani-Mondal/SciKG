{"title": [], "abstractContent": [{"text": "We explore how to detect people's perspectives that occupy a certain proposition.", "labels": [], "entities": []}, {"text": "We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as latent variables.", "labels": [], "entities": []}, {"text": "Words associated with topics or perspectives follow different generative routes.", "labels": [], "entities": []}, {"text": "Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document.", "labels": [], "entities": []}, {"text": "The model is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.", "labels": [], "entities": [{"text": "UK Parliament", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.9057377278804779}]}], "introductionContent": [{"text": "Stance classification is binary classification to detect whether people is supporting or against a topic.", "labels": [], "entities": [{"text": "Stance classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8538043200969696}]}, {"text": "Existing approaches largely rely on labelled data collected under specific topics for learning supervised classifiers for stance classification).", "labels": [], "entities": [{"text": "stance classification", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.9243094027042389}]}, {"text": "At most time, apart from detecting one's stance, we are interested in finding out the arguments behind the person's position.", "labels": [], "entities": []}, {"text": "Perspectives, that state people's ideas or the facts known to one, can be contrastive, i.e. to be in favour of or against something (e.g. Brexit vs Bremain), or non-contrastive, i.e. independent discussions that share a common topic (e.g. unemployment and migration in the context of economy).", "labels": [], "entities": []}, {"text": "Recent years have seen increasing interests in argumentation mining which involves the automatic identification of argumentative structures, e.g., the claims and premises, and detection of argumentative relations between claims and premises or evidences.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9111413657665253}, {"text": "detection of argumentative relations between claims and premises or evidences", "start_pos": 176, "end_pos": 253, "type": "TASK", "confidence": 0.7142355084419251}]}, {"text": "However, learning models for argumentation mining often require text labelled with components within argumentative structures and detailed indication of argumentative relations among them.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.9547332525253296}]}, {"text": "Such labelled data is expensive to obtain in practice and it is also difficult to port models trained on one domain to another.", "labels": [], "entities": []}, {"text": "We are particularly interested in detecting different perspectives in political debates.", "labels": [], "entities": []}, {"text": "Essentially, we would like to achieve somewhere in between stance classification and argumentation mining.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9312616288661957}, {"text": "argumentation mining", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.908213198184967}]}, {"text": "Given a text document, we want to identify a speaker's key arguments, without the use of any labelled data.", "labels": [], "entities": []}, {"text": "For example, in debates about 'Education', we want to automatically extract sentences summarising the key perspectives and their arguments, e.g. 'our education system needs to promote excellence in stem subjects', 'teenagers need to be taught with sexual and health education' or 'grammar schools promote inequality'.", "labels": [], "entities": []}, {"text": "Similarly, if 'Brexit' is being discussed in terms of leaving or remaining, we want to cluster arguments into those two viewpoints.", "labels": [], "entities": []}, {"text": "To do this, we introduce a Latent Argument Model (LAM) which assumes that words can be separated as topic words and argument words and follow different generative routes.", "labels": [], "entities": []}, {"text": "While topic words only involve a sampling of topics, argument words involve a joint sampling of both topics and arguments.", "labels": [], "entities": []}, {"text": "The model does not rely on labelled data as opposed to most existing approaches to stance classification or argument recognition.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.9171402752399445}, {"text": "argument recognition", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7844637632369995}]}, {"text": "It is also different from cross-perspective topic models which assume the perspectives are observed).", "labels": [], "entities": []}, {"text": "Quantitative and qualitative evaluations on debates from the House of Commons of United Kingdom show the utility of the approach and provide a comparison against related models.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates LAM and its variants qualitatively and quantitatively (averaged over 5 runs).", "labels": [], "entities": [{"text": "LAM", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9590868353843689}]}, {"text": "The models for comparison are listed below: \u2022 Both POS tags and a subjective lexicon are used to initialise the Dirichlet prior \u03b3 for the word type switch variable as described in \u00a73.2.", "labels": [], "entities": []}, {"text": "Results are evaluated in terms of both topic coherence and the quality of the extracted perspectives.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example sentences, belonging to speeches that were assigned in Hansard different major topics  labels, were clustered together by LAM (and it is sensible to do so as they are both about \"farmers\").", "labels": [], "entities": [{"text": "Hansard", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9760265946388245}, {"text": "LAM", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.8251270651817322}]}, {"text": " Table 2: Ratio of topics where x or more than x  out of top 10 topic sentences (\u2265 x) belong to the  same major topic.", "labels": [], "entities": []}, {"text": " Table 3: Averaged LA measure across all topic- perspectives for different models.", "labels": [], "entities": [{"text": "LA measure", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.87764972448349}]}, {"text": " Table 4: Accuracy on detecting perspectives ac- cording to the human outputs. In 1&2 a 'yes' an- swer is only valid if marked by both annotators.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.982986330986023}]}]}