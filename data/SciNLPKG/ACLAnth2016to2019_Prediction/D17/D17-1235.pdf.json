{"title": [{"text": "Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response.", "labels": [], "entities": [{"text": "conversation response generation", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8051550388336182}]}, {"text": "Unlike translation, conversation responding is inherently creative.", "labels": [], "entities": [{"text": "conversation responding", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7861505448818207}]}, {"text": "The generation of long, informative, coherent, and diverse responses remains a hard task.", "labels": [], "entities": []}, {"text": "In this work, we focus on the single turn setting.", "labels": [], "entities": []}, {"text": "We add self-attention to the de-coder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets.", "labels": [], "entities": []}, {"text": "We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process.", "labels": [], "entities": []}, {"text": "We trained on a combined data set of over 2.3B conversation messages mined from the web.", "labels": [], "entities": []}, {"text": "In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion.", "labels": [], "entities": [{"text": "length", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9834461808204651}]}, {"text": "A back-off strategy produces better responses overall, in the full spectrum of lengths.", "labels": [], "entities": []}], "introductionContent": [{"text": "Building computer systems capable of generalpurpose conversation is a challenging problem.", "labels": [], "entities": []}, {"text": "However, it is a necessary step toward building intelligent agents that can interact with humans via * Both authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "\u2020 Work done as a member of the Google Brain Residency program (g.co/brainresidency).", "labels": [], "entities": [{"text": "Google Brain Residency", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.6800363858540853}]}, {"text": "natural language, and for eventually passing the Turing test.", "labels": [], "entities": []}, {"text": "The sequence-to-sequence (seq2seq) model has proven very popular as a purely datadriven approach in domains that can be cast as learning to map to and from variable-length sequences, with state-of-the art results in many domains, including machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.8179830014705658}]}, {"text": "Neural conversation models are the latest development in the domain of conversation modeling, with the promise of training computers to converse in an end-to-end fashion (.", "labels": [], "entities": [{"text": "Neural conversation models", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7272918820381165}, {"text": "conversation modeling", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.8711748421192169}]}, {"text": "Despite promising results, there are still many challenges with this approach.", "labels": [], "entities": []}, {"text": "In particular, these models produce short, generic responses that lack diversity (.", "labels": [], "entities": []}, {"text": "Even when longer responses are explicitly encouraged (e.g. via length normalization), they tend to be incoherent (\"The sun is in the center of the sun.\"), redundant (\"i like cake and cake\"), or contradictory (\"I don't own a gun, but I do own a gun.\").", "labels": [], "entities": []}, {"text": "In this paper, we provide two methods to address these issues with minimal modifications to the standard seq2seq model.", "labels": [], "entities": []}, {"text": "First, we present a glimpse model that only trains on fixed-length segments of the target-side at a time, allowing us to scale up training to larger data sets.", "labels": [], "entities": []}, {"text": "Second, we introduce a segment-based stochastic decoding technique which injects diversity earlier in the generated responses.", "labels": [], "entities": []}, {"text": "Together, we find that these two methods lead to both longer responses and higher ratings, compared to a baseline seq2seq model with explicit length and diversitypromoting heuristics integrated into the generation procedure (see for examples generated using our model).", "labels": [], "entities": []}, {"text": "In Section 2, we present a high-level overview of these two techniques.", "labels": [], "entities": []}, {"text": "We then discuss each technique in more detail in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "Finally, we report small and large-scale experimental evaluations of the proposed techniques in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experimental results for evaluating the target-glimpse model and the stochastic decoding method that we presented.", "labels": [], "entities": []}, {"text": "We train the model using the Google neural machine translation model (GNMT, (), on a data set that combines multiple sources mined from the Web: 1.", "labels": [], "entities": [{"text": "Google neural machine translation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.542009599506855}]}, {"text": "The full Reddit data 3 that contains 1.7 billion messages (221 million conversations).", "labels": [], "entities": [{"text": "Reddit data 3", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.9085413813591003}]}, {"text": "2. The 2009 Open Subtitles data (0.5 million conversations,).", "labels": [], "entities": [{"text": "2009 Open Subtitles data", "start_pos": 7, "end_pos": 31, "type": "DATASET", "confidence": 0.7654268592596054}]}, {"text": "3. The Stack Exchange data (0.8 million conversations).", "labels": [], "entities": [{"text": "The Stack Exchange data", "start_pos": 3, "end_pos": 26, "type": "DATASET", "confidence": 0.6142930462956429}]}, {"text": "4. Dialogue-like texts that we recognized and extracted from the web (17 million conversations).", "labels": [], "entities": []}, {"text": "For all these data sets, we extract pairs of messages where one can be considered as a response to the other.", "labels": [], "entities": []}, {"text": "For example, in the Reddit data set, the messages belonging to the same post are organized as a tree.", "labels": [], "entities": [{"text": "Reddit data set", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.9216827154159546}]}, {"text": "A child node is a message that replies to its parent.", "labels": [], "entities": []}, {"text": "This may not necessarily be true as people maybe replying to other messages that are also visually close.", "labels": [], "entities": []}, {"text": "However, for our current single-turn experiments, we treat these as a single exchange.", "labels": [], "entities": []}, {"text": "In this setting, the GNMT model trained on prompt-to-response pairs works surprisingly well without modification when generating short responses with beam search.", "labels": [], "entities": []}, {"text": "Similar to previous work on neural conversation models, we find that the generated responses are almost always grammatical, and sometimes even interesting.", "labels": [], "entities": []}, {"text": "They are also usually on topic.", "labels": [], "entities": []}, {"text": "In addition, we found that even greedy sampling from the 8-layer GNMT model produces grammatical responses most of the time, although these responses are more likely to be semantically-broken than responses generated using standard beam search.", "labels": [], "entities": []}, {"text": "We would like to leverage the benefits of greedy sampling, because the induced variation generates more surprises and may potentially help improve user-engagement, and we found that our proposed segment-based beam sampling procedure accomplishes this to some extent.", "labels": [], "entities": [{"text": "segment-based beam sampling", "start_pos": 195, "end_pos": 222, "type": "TASK", "confidence": 0.5980601012706757}]}, {"text": "It is difficult to come up with an objective evaluation metric for conversation response generation that can be computed automatically.", "labels": [], "entities": [{"text": "conversation response generation", "start_pos": 67, "end_pos": 99, "type": "TASK", "confidence": 0.8304765025774637}]}, {"text": "The conditional distribution P (y|x) is supposed to have high entropy in order to be interesting (many possible valid responses to a given prompt).", "labels": [], "entities": []}, {"text": "Therefore BLEU scores used in translation are not a good fit (also see ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976783394813538}, {"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9802325963973999}]}, {"text": "Other than looking at the evaluation set perplexity, we use two metrics, the N-choose-1 accuracy and 5-scale sideby-side human evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.8946400880813599}]}, {"text": "In the N-choose-K metric, we use the model as a retriever.", "labels": [], "entities": []}, {"text": "Given a prompt, we ask the model to rank N candidate responses, where one is the ground truth and the other N \u2212 1 are random responses from the same data set.", "labels": [], "entities": []}, {"text": "We then calculate the N-choose-K accuracy as the proportion of trials where the true response is in the top K.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9452233910560608}]}, {"text": "The prompts used for evaluation are selected randomly from the same data set.", "labels": [], "entities": []}, {"text": "This metric isn't necessarily correlated well with the true response quality, but provides a useful first diagnostic for faster experimental iteration.", "labels": [], "entities": []}, {"text": "It takes about a day to train a small model on a single GPU that reaches 2-choose-1 accuracies of around 70% or 80%, but it is much harder to make progress on the 50-choose-1 accuracy.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9770616888999939}, {"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.988544762134552}]}, {"text": "As a reference, human performance on the 10-choose-1 task is around 45% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9986817240715027}]}, {"text": "In the 5-scale human evaluation, we use a collection of 200 context-free prompts . These prompts are collected from the following sources, and filtered to prompts that are context-free (i.e. do not depend on previous turns in the conversation), general enough, and by eliminating near duplicates: 1.", "labels": [], "entities": []}, {"text": "The questions and statements that users asked an internal testing bot.", "labels": [], "entities": []}, {"text": "2. The Fisher corpus ().", "labels": [], "entities": [{"text": "Fisher corpus", "start_pos": 7, "end_pos": 20, "type": "DATASET", "confidence": 0.7987082898616791}]}, {"text": "3. User inputs to the Jabberwacky chatbot 5 . These can be either generic or specific.", "labels": [], "entities": [{"text": "Jabberwacky chatbot", "start_pos": 22, "end_pos": 41, "type": "DATASET", "confidence": 0.8832673728466034}]}, {"text": "Some example prompts from this collection are shown in.", "labels": [], "entities": []}, {"text": "These prompts are open-domain (not about any specific topic), and include a wide range of topics.", "labels": [], "entities": []}, {"text": "Many require some creativity for answering, such as \"Tell me a story about a bear.\"", "labels": [], "entities": [{"text": "answering", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.9717511534690857}]}, {"text": "Our evaluation set is therefore not from the same distribution as our training set.", "labels": [], "entities": []}, {"text": "However, since our goal is to produce good general conversation responses, we found it to be a good general purpose evaluation set.", "labels": [], "entities": []}, {"text": "The evaluation itself is done by human raters.", "labels": [], "entities": []}, {"text": "They are well-trained for the purpose of ensuring rating quality, and they are native English speakers.", "labels": [], "entities": []}, {"text": "The A 5-scale rating is produced for each prompt-response pair: Excellent, Good, Acceptable, Mediocre, and Bad.", "labels": [], "entities": [{"text": "A 5-scale rating", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.980327844619751}, {"text": "Excellent", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9721721410751343}, {"text": "Acceptable", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9979380965232849}, {"text": "Mediocre", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9653210639953613}]}, {"text": "For example, the instructions for rating Excellent is \"On topic, interesting, shows understanding, moves the conversation forward.", "labels": [], "entities": [{"text": "Excellent", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9880422353744507}]}, {"text": "It answers the question.\"", "labels": [], "entities": []}, {"text": "The instruction for Acceptable is \"On topic but with flaws that make it seem like it didnt come from a human.", "labels": [], "entities": []}, {"text": "It implies an answer.\"", "labels": [], "entities": []}, {"text": "The instruction for Bad is \"A completely off-topic statement or question, nonsensical, or grammatically broken.", "labels": [], "entities": []}, {"text": "It does not provide an answer.\"", "labels": [], "entities": []}, {"text": "In our experiments, we perform the evaluations side-by-side, each time using responses generated from two methods.", "labels": [], "entities": []}, {"text": "Every prompt-response pair is rated by three raters.", "labels": [], "entities": []}, {"text": "We rate 200 pairs in total for every method, garnering 600 ratings overall.", "labels": [], "entities": []}, {"text": "After the evaluation, we report aggregated results from each method individually.", "labels": [], "entities": []}, {"text": "To see whether generating long responses is indeed a challenging problem, we trained the plain seq2seq with the GNMT model where the encoder holds the source sequence and the decoder holds the target sequence.", "labels": [], "entities": []}, {"text": "We experimented with the standard beam search and the beam search with length normalization \u03b1 = 0.8 similar to (.", "labels": [], "entities": [{"text": "length normalization \u03b1", "start_pos": 71, "end_pos": 93, "type": "METRIC", "confidence": 0.9056082367897034}]}, {"text": "With this length normalization the generated responses are indeed longer.", "labels": [], "entities": []}, {"text": "However, they are more often semantically incoherent.", "labels": [], "entities": []}, {"text": "It produces \"I have no idea what you are talking about.\" more often, similarly observed in ().", "labels": [], "entities": []}, {"text": "The human evaluation results are summarized in(b).", "labels": [], "entities": []}, {"text": "Methods that generate longer responses have more Bad and less Excellent / Good ratings.", "labels": [], "entities": [{"text": "Bad", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9947751760482788}, {"text": "Excellent / Good ratings", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.8580856770277023}]}, {"text": "We also performed the N-choose-1 evaluation on the baseline model using different normalization schemes.", "labels": [], "entities": []}, {"text": "The results are shown in Table 2(a).", "labels": [], "entities": []}, {"text": "No Normalization means that we use P (y|x) for scoring, Normalize by Marginal uses P (y|x)/P (y), as suggested in (, and Normalize by Random Prompts is our scoring objective described in Section 4.", "labels": [], "entities": []}, {"text": "The significant boost when using both normalization schemes indicates that the conditional log probability predicted by the model maybe biased towards the language model probability of P (y).", "labels": [], "entities": []}, {"text": "After adding the normalization, the score maybe closer to the true conditional log probability.", "labels": [], "entities": []}, {"text": "Overall, this reranking evaluation indicates that our heuristic is preferred to scoring using the marginal.", "labels": [], "entities": []}, {"text": "However, it is unfortunately hard to directly make use of this score during beam search decoding (i.e., generation), since the resulting sequences are usually ungrammatical, as also observed by . This is the motivation for using a segment-by-segment reranking procedure, as described in Section 4.", "labels": [], "entities": [{"text": "beam search decoding", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7376477917035421}]}, {"text": "For our large-scale experiments, we train our target-glimpse model on the full combined data set.", "labels": [], "entities": []}, {"text": "shows the training progress curve.", "labels": [], "entities": [{"text": "training progress curve", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.658927450577418}]}, {"text": "In this we also include the curve for K = 1, that is, the glimpse model with decoder-length 1.", "labels": [], "entities": []}, {"text": "It is clear enough that this model progresses much slower, so we terminated it early.", "labels": [], "entities": []}, {"text": "However, it is surprising that the glimpse model with K = 10 progresses faster than the baseline model with only source-side attention, because the model is trained on examples with decoder-length fixed at 10, while the average response length is 38 in our data set.", "labels": [], "entities": []}, {"text": "This means it takes on average 3.8x training steps for the glimpse model to train on the same number of raw training-pairs as the baseline model.", "labels": [], "entities": []}, {"text": "Despite this, the faster progress indicates that target-side attention indeed helps the model generalize better.", "labels": [], "entities": []}, {"text": "The human evaluation results shown in compare our proposed method with the baseline seq2seq model.", "labels": [], "entities": []}, {"text": "For this, we trained a length-10 target-glimpse model and decoded with stochastic beam-search using segment-by-segment reranking.", "labels": [], "entities": []}, {"text": "In our experiments, we were unable to generate better long, coherent responses using the whole-sequence level reranking method from () compared to using standard beam search with length-normalization . We therefore choose the latter as our baseline, because it is the only method which generates responses that are long enough that we can compare to. shows that our proposed method generates more long responses overall.", "labels": [], "entities": []}, {"text": "One third of all responses are longer than 100 characters, while the baseline model produces only a negligible fraction.", "labels": [], "entities": []}, {"text": "Although we do not employ any length-promoting objectives in our method, length-normalization is used for the baseline.", "labels": [], "entities": [{"text": "length-normalization", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.9415614008903503}]}, {"text": "For responses generated by our method, the proportion of Acceptable and Excellent responses remains constant or even increases as the responses grow longer.", "labels": [], "entities": [{"text": "Acceptable", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9920808672904968}]}, {"text": "Conversely, human ratings decline sharply with length for the baseline model.", "labels": [], "entities": [{"text": "length", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9925923347473145}]}, {"text": "The percentage of test cases with major agreement is high for both methods.", "labels": [], "entities": []}, {"text": "We consider a test to have major agreement if two ratings out of the three are the same.", "labels": [], "entities": []}, {"text": "For the baseline method, 80% of the responses have major agreements, and for our method it is 70%.", "labels": [], "entities": []}, {"text": "However, shorter responses have a much smaller search space, and we find that standard beam search tends to generate better (\"safer\") short responses.", "labels": [], "entities": []}, {"text": "To maximize cumulative response quality, we therefore implemented a backoff strategy that combines the strengths of the two methods.", "labels": [], "entities": []}, {"text": "We choose to fallback to the baseline model without length normalization when the latter produces a response shorter than 40 characters, otherwise we use the response from our method.", "labels": [], "entities": []}, {"text": "This corresponds to the white histogram in.", "labels": [], "entities": []}, {"text": "Compared to the other methods in the fig-ure, the combined strategy results in more ratings of Excellent, Good, Acceptable, and Mediocre, and fewer Bad ratings.", "labels": [], "entities": [{"text": "Excellent", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9993459582328796}, {"text": "Acceptable", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9973646998405457}, {"text": "Mediocre", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9895665645599365}, {"text": "Bad ratings", "start_pos": 148, "end_pos": 159, "type": "METRIC", "confidence": 0.975195974111557}]}, {"text": "With this strategy, among the responses generated for the same 200 prompts, 133 were from the standard beam search and 67 were from our model.", "labels": [], "entities": []}, {"text": "Out of the 67 long responses, two thirds were longer than 60 characters and half were longer than 75 characters.", "labels": [], "entities": []}, {"text": "To compare the combined model's performance with the baseline, we generated responses from both models using the same 200 prompts.", "labels": [], "entities": []}, {"text": "For 20 of the response pairs, human raters had no preference, but for the remaining 180, human raters preferred the combined model's response in 103 cases and the baseline's in only 77, indicating a significant win.", "labels": [], "entities": []}], "tableCaptions": []}