{"title": [{"text": "Deep Recurrent Generative Decoder for Abstractive Text Summarization *", "labels": [], "entities": [{"text": "Abstractive Text Summarization", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.5336240728696188}]}], "abstractContent": [{"text": "We propose anew framework for ab-stractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN).", "labels": [], "entities": [{"text": "ab-stractive text summarization", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.5774945418039957}]}, {"text": "Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality.", "labels": [], "entities": [{"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9682737588882446}]}, {"text": "Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables.", "labels": [], "entities": [{"text": "Neural variational inference", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7044177850087484}]}, {"text": "Abstractive summaries are generated based on both the generative latent variables and the dis-criminative deterministic states.", "labels": [], "entities": []}, {"text": "Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.", "labels": [], "entities": [{"text": "DRGN", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.778777003288269}]}], "introductionContent": [{"text": "Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5511591732501984}]}, {"text": "Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate ().", "labels": [], "entities": []}, {"text": "* The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).", "labels": [], "entities": [{"text": "Hong Kong Special Administrative Region", "start_pos": 89, "end_pos": 128, "type": "DATASET", "confidence": 0.7639657258987427}]}], "datasetContent": [{"text": "We use ROUGE score) as our evaluation metric with standard options.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 7, "end_pos": 18, "type": "METRIC", "confidence": 0.9795774221420288}]}, {"text": "The basic idea of ROUGE is to count the number of overlapping units between generated summaries and the reference summaries, such as overlapped n-grams, word sequences, and word pairs.", "labels": [], "entities": []}, {"text": "F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) and ROUGE-SU4 (R-SU4) are reported.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9641650319099426}, {"text": "ROUGE-1", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8805592656135559}]}, {"text": "For the experiments on the English dataset Gigawords, we set the dimension of word embeddings to 300, and the dimension of hidden states and latent variables to 500.", "labels": [], "entities": [{"text": "English dataset Gigawords", "start_pos": 27, "end_pos": 52, "type": "DATASET", "confidence": 0.8716676632563273}]}, {"text": "The maximum length of documents and summaries is 100 and 50 respectively.", "labels": [], "entities": []}, {"text": "The batch size of mini-batch training is 256.", "labels": [], "entities": []}, {"text": "For DUC-2004, the maximum length of summaries is 75 bytes.", "labels": [], "entities": [{"text": "DUC-2004", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.918182909488678}]}, {"text": "For the dataset of LCSTS, the dimension of word embeddings is 350.", "labels": [], "entities": []}, {"text": "We also set the dimension of hidden states and latent variables to 500.", "labels": [], "entities": []}, {"text": "The maximum length of documents and summaries is 120 and 25 respectively, and the batch size is also 256.", "labels": [], "entities": []}, {"text": "The beam size of the decoder was set to be 10.", "labels": [], "entities": []}, {"text": "Adadelta (Schmidhuber, 2015) with hyperparameter \u03c1 = 0.95 and = 1e \u2212 6 is used for gradient based optimization.", "labels": [], "entities": []}, {"text": "Our neural network based framework is implemented using Theano (Theano Development Team, 2016).", "labels": [], "entities": [{"text": "Theano (Theano Development Team, 2016)", "start_pos": 56, "end_pos": 94, "type": "DATASET", "confidence": 0.9126086309552193}]}, {"text": "We first depict the performance of our model DRGD by comparing to the standard decoders (StanD) of our own implementation.", "labels": [], "entities": []}, {"text": "The comparison results on the validation datasets of Gigawords and LCSTS are shown in.", "labels": [], "entities": [{"text": "Gigawords", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.8267484307289124}]}, {"text": "From the results we can see that our proposed generative decoders DRGD can obtain obvious improvements on abstractive summarization than the standard decoders.", "labels": [], "entities": [{"text": "generative decoders DRGD", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.7138893902301788}, {"text": "abstractive summarization", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.5266855955123901}]}, {"text": "Actually, the performance of the standard   The results on the English datasets of Gigawords and DUC-2004 are shown in and respectively.", "labels": [], "entities": [{"text": "English datasets of Gigawords", "start_pos": 63, "end_pos": 92, "type": "DATASET", "confidence": 0.7535072714090347}, {"text": "DUC-2004", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.6874340176582336}]}, {"text": "Our model DRGD achieves the best summarization performance on all the ROUGE metrics.", "labels": [], "entities": [{"text": "summarization", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9373037219047546}]}, {"text": "Although ASC+FSC 1 also uses a generative method to model the latent summary variables, the representation ability is limited and it cannot bring in noticeable improvements.", "labels": [], "entities": [{"text": "ASC+FSC", "start_pos": 9, "end_pos": 16, "type": "TASK", "confidence": 0.6446566979090372}]}, {"text": "It is worth noting that the methods lvt2k-1sent and lvt5k-1sent () utilize linguistic features such as parts-of-speech tags, namedentity tags, and TF and IDF statistics of the words as part of the document representation.", "labels": [], "entities": [{"text": "IDF", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9206745028495789}]}, {"text": "In fact, extracting all such features is a time consuming work, especially on large-scale datasets such as Gigawords.", "labels": [], "entities": [{"text": "Gigawords", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.9408603310585022}]}, {"text": "lvt2k and lvt5k are not end-to-end style models and are more complicated than our model in practical applications.", "labels": [], "entities": []}, {"text": "The results on the Chinese dataset LCSTS are shown in.", "labels": [], "entities": [{"text": "Chinese dataset LCSTS", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.9455870588620504}]}, {"text": "Our model DRGD also achieves the best performance.", "labels": [], "entities": [{"text": "DRGD", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8957583904266357}]}, {"text": "Although CopyNet employs a copying mechanism to improve the summary quality and RNN-distract considers attention information diversity in their decoders, our model is still better than those two methods demonstrating that the latent structure information learned from target summaries indeed plays a role in abstractive summarization.", "labels": [], "entities": []}, {"text": "We also believe that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance.", "labels": [], "entities": [{"text": "copying", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9757139682769775}, {"text": "summarization", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.9786125421524048}]}], "tableCaptions": [{"text": " Table 1: ROUGE-F1 on validation sets", "labels": [], "entities": [{"text": "ROUGE-F1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9682208299636841}]}, {"text": " Table 2: ROUGE-F1 on Gigawords", "labels": [], "entities": [{"text": "ROUGE-F1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9548711776733398}, {"text": "Gigawords", "start_pos": 22, "end_pos": 31, "type": "DATASET", "confidence": 0.6802721619606018}]}, {"text": " Table 3: ROUGE-Recall on DUC2004", "labels": [], "entities": [{"text": "ROUGE-Recall", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9745151400566101}, {"text": "DUC2004", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.7450652122497559}]}, {"text": " Table 4: ROUGE-F1 on LCSTS", "labels": [], "entities": [{"text": "ROUGE-F1", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9745312333106995}, {"text": "LCSTS", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8217623829841614}]}]}