{"title": [{"text": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "labels": [], "entities": [{"text": "Weakly-Supervised Relation Extraction", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7115608950455984}]}], "abstractContent": [{"text": "Deep residual learning (ResNet) (He et al., 2016) is anew method for training very deep neural networks using identity mapping for shortcut connections.", "labels": [], "entities": []}, {"text": "ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art performances in many computer vision tasks.", "labels": [], "entities": [{"text": "ImageNet ILSVRC 2015 classification task", "start_pos": 19, "end_pos": 59, "type": "DATASET", "confidence": 0.7472565054893494}]}, {"text": "However, the effect of residual learning on noisy natural language processing tasks is still not well understood.", "labels": [], "entities": []}, {"text": "In this paper, we design a novel convolu-tional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction.", "labels": [], "entities": [{"text": "distantly supervised noisy relation extraction", "start_pos": 136, "end_pos": 182, "type": "TASK", "confidence": 0.6558384776115418}]}, {"text": "In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.731577530503273}]}], "introductionContent": [{"text": "Relation extraction is the task of predicting attributes and relations for entities in a sentence).", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9462482631206512}, {"text": "predicting attributes and relations for entities in a sentence", "start_pos": 35, "end_pos": 97, "type": "TASK", "confidence": 0.8198729819721646}]}, {"text": "For example, given a sentence \"Barack Obama was born in Honolulu, Hawaii.\", a relation classifier aims at predicting the relation of \"bornInCity\".", "labels": [], "entities": []}, {"text": "Relation extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8965567350387573}, {"text": "sentiment analysis", "start_pos": 186, "end_pos": 204, "type": "TASK", "confidence": 0.9543005228042603}, {"text": "question answering", "start_pos": 206, "end_pos": 224, "type": "TASK", "confidence": 0.9161162376403809}, {"text": "summarization", "start_pos": 230, "end_pos": 243, "type": "TASK", "confidence": 0.989798903465271}]}, {"text": "A major issue for relation extraction is the lack of labeled training data.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.9751284420490265}]}, {"text": "In recent years, distant supervision () emerges as the most popular method for relation extraction-it uses knowledge base facts to select a set of noisy instances from unlabeled data.", "labels": [], "entities": [{"text": "relation extraction-it", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.9520358145236969}]}, {"text": "Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model () achieved the state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Following their success, proposed a piece-wise max-pooling strategy to improve the CNNs.", "labels": [], "entities": []}, {"text": "Various attention strategies ( for CNNs are also proposed, obtaining impressive results.", "labels": [], "entities": []}, {"text": "However, most of these neural relation extraction models are relatively shallow CNNs-typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task.", "labels": [], "entities": [{"text": "neural relation extraction", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.709293524424235}]}, {"text": "In this paper, we investigate the effects of training deeper CNNs for distantly-supervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7570180296897888}]}, {"text": "More specifically, we designed a convolutional neural network based on residual learning (-we show how one can incorporate word embeddings and position embeddings into a deep residual network, while feeding identity feedback to convolutional layers for this noisy relation prediction task.", "labels": [], "entities": [{"text": "relation prediction task", "start_pos": 264, "end_pos": 288, "type": "TASK", "confidence": 0.7940224210421244}]}, {"text": "Empirically, we evaluate on the NYT-Freebase dataset (, and demonstrate the state-of-the-art performance using deep CNNs with identify mapping and shortcuts.", "labels": [], "entities": [{"text": "NYT-Freebase dataset", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.9899144768714905}]}, {"text": "In contrast to popular beliefs in vision that deep residual network only works for very deep CNNs, we show that even with a moderately deep CNNs, there are substantial improvements over vanilla CNNs for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.895252913236618}]}, {"text": "Our contributions are three-fold: \u2022 We are the first to consider deeper convolutional neural networks for weakly-supervised relation extraction using residual learning; \u2022 We show that our deep residual network model outperforms CNNs by a large margin empirically, obtaining state-of-the-art performances; \u2022 Our identity mapping with shortcut feedback approach can be easily applicable to any variants of CNNs for relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7520702183246613}, {"text": "relation extraction", "start_pos": 413, "end_pos": 432, "type": "TASK", "confidence": 0.8562809824943542}]}], "datasetContent": [{"text": "In this paper, we use the word embeddings released by) which are trained on the NYT-Freebase corpus ().", "labels": [], "entities": [{"text": "NYT-Freebase corpus", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.987072765827179}]}, {"text": "We fine tune our model using validation on the training data.", "labels": [], "entities": []}, {"text": "The word embedding is of size 50.", "labels": [], "entities": []}, {"text": "The input text is padded to a fixed size of 100.", "labels": [], "entities": []}, {"text": "Training is performed with tensorflow adam optimizer, using a mini-batch of size 64, an initial learning rate of 0.001.", "labels": [], "entities": []}, {"text": "We initialize our convolutional layers following.", "labels": [], "entities": []}, {"text": "The implementation is done using Tensorflow 0.11.", "labels": [], "entities": []}, {"text": "All experiments are performed on a single NVidia Titan X (Pascal) GPU.", "labels": [], "entities": [{"text": "NVidia Titan X (Pascal) GPU", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9018985288483756}]}, {"text": "In we show all parameters used in the experiments.", "labels": [], "entities": []}, {"text": "We experiment with several state-of-the-art baselines and variants of our model.", "labels": [], "entities": []}, {"text": "\u2022 CNN-B: Our implementation of the CNN baseline () which contains one convolutional layer, and one fully connected layer.", "labels": [], "entities": [{"text": "CNN-B", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.8929522037506104}]}, {"text": "Window size h 3 Word dimension dw 50 Position dimension dp \u2022 CNN+ATT: CNN-B with attention over instance learning ().", "labels": [], "entities": [{"text": "ATT", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.40773579478263855}]}, {"text": "\u2022 PCNN+ATT: Piecewise CNN-B with attention over instance learning ().", "labels": [], "entities": [{"text": "ATT", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.5678525567054749}]}, {"text": "\u2022 CNN: Our CNN model which includes one convolutional layer and three fully connected layers.", "labels": [], "entities": []}, {"text": "\u2022 CNN-x: Deeper CNN model which has x convolutional layers.", "labels": [], "entities": []}, {"text": "For example, CNN-9 is a model constructed with 9 convolutional layers (1 + 4 residual cnn block without identity shortcut) and three fully connected layers.", "labels": [], "entities": [{"text": "CNN-9", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8895184993743896}]}, {"text": "\u2022 ResCNN-x: Our proposed CNN-x model with residual identity shortcuts.", "labels": [], "entities": []}, {"text": "We evaluate our models on the widely used NYT freebase larger dataset (.", "labels": [], "entities": [{"text": "NYT freebase larger dataset", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.9673862010240555}]}, {"text": "Note that ImageNet dataset used by the original ResNet paper (He et al., 2016) has 1.28 million training instances.", "labels": [], "entities": [{"text": "ImageNet dataset", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.9167666137218475}]}, {"text": "NYT freebase dataset includes 522K training sentences, which is the largest dataset in relation extraction, and it is the only suitable dataset to train deeper CNNs.", "labels": [], "entities": [{"text": "NYT freebase dataset", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9890726407368978}, {"text": "relation extraction", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8852877914905548}]}, {"text": "The advantage of this dataset is that there are 522,611 sentences in training data and 172,448 sentences in testing data and this size can support  us to train a deep network.", "labels": [], "entities": []}, {"text": "Similar to previous work (, we evaluate our model using the held-out evaluation.", "labels": [], "entities": []}, {"text": "We report both the aggregate curves precision/recall curves and Precision@N (P@N).", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9899467825889587}, {"text": "recall curves", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.9080566167831421}, {"text": "Precision@N (P@N)", "start_pos": 64, "end_pos": 81, "type": "METRIC", "confidence": 0.9327009692788124}]}, {"text": "In, we compare the proposed ResCNN model with various CNNs.", "labels": [], "entities": []}, {"text": "First, CNNs with multiple fully-connected layers obtained very good results, which is a novel finding.", "labels": [], "entities": [{"text": "CNNs", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.9586901664733887}]}, {"text": "Second, the results also suggest that deeper CNNs with residual learning help extracting signals from noisy distant supervision data.", "labels": [], "entities": [{"text": "extracting signals from noisy distant supervision", "start_pos": 78, "end_pos": 127, "type": "TASK", "confidence": 0.8231580555438995}]}, {"text": "We observe that overfitting happened when we try to add more layers and the performance of CNN-9 is much worse than CNN.", "labels": [], "entities": [{"text": "CNN-9", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9592726230621338}, {"text": "CNN", "start_pos": 116, "end_pos": 119, "type": "DATASET", "confidence": 0.9745825529098511}]}, {"text": "We find that ResNet can solve this problem and ResCNN-9 obtains better performance as compared to CNN-B and CNN and dominates the precision/recall curve overall.", "labels": [], "entities": [{"text": "CNN", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.8893208503723145}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9991607666015625}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.8899722695350647}]}, {"text": "We show the effect of depth in residual networks in.", "labels": [], "entities": []}, {"text": "We observe that ResCNN-5 is worse than CNN-5 because the ResNet does notwork well for shallow CNNs, and this is consis-  tent with the original ResNet paper.", "labels": [], "entities": [{"text": "CNN-5", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.930863618850708}]}, {"text": "As we increase the network depth, we see that CNN-9 does overfit to the training data.", "labels": [], "entities": [{"text": "CNN-9", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.9469083547592163}]}, {"text": "With residual learning, both ResCNN-9 and ResCNN-13 provide significant improvements over CNN-5 and ResCNN-5 models.", "labels": [], "entities": [{"text": "CNN-5", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.8890380859375}]}, {"text": "In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance learning in a noisy input setting.", "labels": [], "entities": []}, {"text": "The intuition of ResNet help this task in two aspect.", "labels": [], "entities": []}, {"text": "First, if the lower, middle, and higher levels learn hidden lexical, syntactic, and semantic representations respectively, sometimes it helps to bypass the syntax to connect lexical and semantic space directly.", "labels": [], "entities": []}, {"text": "Second, ResNet tackles the vanishing gradient problem which will decrease the effect of noise in distant supervision data.", "labels": [], "entities": []}, {"text": "In, we compare the performance of our models to state-of-the-art baselines.", "labels": [], "entities": []}, {"text": "We show that our ResCNN-9 outperforms all models that do not select training instances.", "labels": [], "entities": []}, {"text": "And even without piecewise max-pooling and instance-based attention, our model is on par with the PCNN+ATT model.", "labels": [], "entities": []}, {"text": "For the more practical evaluation, we compare the results for precision@N where N is small) in.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9991903901100159}]}, {"text": "We observe that our ResCNN-9 model dominates the performance when we predict the relation in the range of higher probability.", "labels": [], "entities": []}, {"text": "ResNet helps CNNs to focus on the highly possible candidate and mitigate the noise effect of distant supervision.", "labels": [], "entities": []}, {"text": "We believe that residual connections actually can be seen as a form of renormalizing the gradients, which prevents the model from overfitting to the noisy distant supervision data.", "labels": [], "entities": []}, {"text": "In our distant-supervised relation extraction experience, we have two important observations: (1) We get significant improvements with CNNs adding multiple fully-connected layers.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7861397862434387}]}], "tableCaptions": [{"text": " Table 2: P@N for relation extraction with different models.  Top: models that select training data. Bottom: models with- out selective attention.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.85179203748703}]}, {"text": " Table 3: P@N for relation extraction with different models  where N is small. We get the result of PCNN+ATT using  their public source code.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8844891488552094}, {"text": "PCNN+ATT", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.7736530105272929}]}]}