{"title": [{"text": "Learning Generic Sentence Representations Using Convolutional Neural Networks", "labels": [], "entities": [{"text": "Learning Generic Sentence Representations", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.5869777128100395}]}], "abstractContent": [{"text": "We propose anew encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes.", "labels": [], "entities": []}, {"text": "The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using along short-term memory recurrent neural network as a decoder.", "labels": [], "entities": []}, {"text": "Several tasks are considered, including sentence reconstruction and future sentence prediction.", "labels": [], "entities": [{"text": "sentence reconstruction", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.8232661783695221}, {"text": "future sentence prediction", "start_pos": 68, "end_pos": 94, "type": "TASK", "confidence": 0.6057359874248505}]}, {"text": "Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences.", "labels": [], "entities": []}, {"text": "By training our models on a large collection of novels, we obtain a highly generic con-volutional sentence encoder that performs well in practice.", "labels": [], "entities": []}, {"text": "Experimental results on several benchmark datasets, and across abroad range of applications, demonstrate the superiority of the proposed model over competing methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning sentence representations is central to many natural language modeling applications.", "labels": [], "entities": [{"text": "Learning sentence representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6080096065998077}, {"text": "natural language modeling", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6857691009839376}]}, {"text": "The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences.", "labels": [], "entities": []}, {"text": "Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (), recurrent neural networks (RNNs), convolutional neural networks (CNNs) (, and recursive neural networks ().", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8337943851947784}]}, {"text": "Most of these models are task-dependent: they are trained specifically fora certain task.", "labels": [], "entities": []}, {"text": "However, these methods may become inefficient when we need to repeatedly learn sentence representations fora large number of different tasks, because they may require retraining anew model for each individual task.", "labels": [], "entities": []}, {"text": "In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains.", "labels": [], "entities": []}, {"text": "Several approaches have been proposed for learning generic sentence embeddings.", "labels": [], "entities": []}, {"text": "The paragraphvector model of incorporates a global context vector into the log-linear neural language model () to learn the sentence representation; however, at prediction time, one needs to perform gradient descent to compute anew vector.", "labels": [], "entities": []}, {"text": "The sequence autoencoder of describes an encoder-decoder model to reconstruct the input sentence, while the skip-thought model of extends the encoder-decoder model to reconstruct the surrounding sentences of an input sentence.", "labels": [], "entities": []}, {"text": "Both the encoder and decoder of the methods above are modeled as RNNs.", "labels": [], "entities": []}, {"text": "CNNs have recently achieved excellent results in various task-dependent natural language applications as the sentence encoder ().", "labels": [], "entities": []}, {"text": "This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decoder models proposed by;.", "labels": [], "entities": []}, {"text": "Specifically, a CNN encoder performs convolution and pooling operations on an input sentence, then uses a fullyconnected layer to produce a fixed-length encoding of the sentence.", "labels": [], "entities": []}, {"text": "This encoding vector is then fed into along short-term memory (LSTM) recurrent network to produce a target sentence.", "labels": [], "entities": []}, {"text": "Depending on the task, we propose three models: (i) CNN-LSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the intra-sentence information; (ii) CNN-LSTM future  predictor: this model aims to predict a future sentence, by leveraging inter-sentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence.", "labels": [], "entities": [{"text": "CNN-LSTM future  predictor", "start_pos": 185, "end_pos": 211, "type": "TASK", "confidence": 0.6029649376869202}]}, {"text": "This composite model aims to learn a sentence encoder that captures both intra-and inter-sentence information.", "labels": [], "entities": []}, {"text": "The proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context.", "labels": [], "entities": [{"text": "CNN-LSTM future predictor", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6601080298423767}]}, {"text": "In order to capture longerterm dependencies between sentences, we further introduce a hierarchical encoder-decoder model.", "labels": [], "entities": []}, {"text": "This model abstracts the RNN language model of to the sentence level.", "labels": [], "entities": []}, {"text": "That is, instead of using the current word in a sentence to predict future words (sentence continuation), we encode a sentence to predict multiple future sentences (paragraph continuation).", "labels": [], "entities": []}, {"text": "This model is termed hierarchical CNN-LSTM model.", "labels": [], "entities": []}, {"text": "As in, we first train our proposed models on a large collection of novels.", "labels": [], "entities": []}, {"text": "We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.8422987461090088}, {"text": "image-sentence ranking", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.7075198292732239}]}, {"text": "In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN.", "labels": [], "entities": []}, {"text": "We show that our trained sentence encoder yields generic representations that perform as well as, or better, than those of;, in all the tasks considered.", "labels": [], "entities": []}, {"text": "Summarizing, the main contribution of this paper is anew class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for learning generic sentence representations.", "labels": [], "entities": [{"text": "learning generic sentence representations", "start_pos": 163, "end_pos": 204, "type": "TASK", "confidence": 0.6033220738172531}]}, {"text": "Inspired by the skip-thought model (), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context windows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for predicting multiple future sentences.", "labels": [], "entities": [{"text": "predicting multiple future sentences", "start_pos": 253, "end_pos": 289, "type": "TASK", "confidence": 0.8015685975551605}]}], "datasetContent": [{"text": "We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.8363313674926758}]}, {"text": "As in, we evaluate the capabilities of our encoder as a generic feature extractor.", "labels": [], "entities": []}, {"text": "To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trained sentence encoder on the 5 classification benchmarks.", "labels": [], "entities": []}, {"text": "All the CNN-LSTM models are trained using the BookCorpus dataset (, which consists of 70 million sentences from over 7000 books.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.9853345453739166}]}, {"text": "We train four models in total: (i) an autoencoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model.", "labels": [], "entities": []}, {"text": "For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector.", "labels": [], "entities": []}, {"text": "For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.", "labels": [], "entities": []}, {"text": "The CNN-LSTM models are trained with a vocabulary size of 22,154 words.", "labels": [], "entities": []}, {"text": "In order to learn a generic sentence encoder that can encode a large number of possible words, we use two methods of considering words not in the training set.", "labels": [], "entities": []}, {"text": "Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors (, in which all test words are assumed to reside.", "labels": [], "entities": []}, {"text": "The first method learns a linear mapping between the word2vec embedding space V and the learned word embedding space V cnn by solving a linear regression problem ().", "labels": [], "entities": []}, {"text": "Thus, any word from V can be mapped into V cnn for encoding sentences.", "labels": [], "entities": []}, {"text": "The second method fixes the word vectors in V cnn as the corresponding word vectors in V , and we do not update the word embedding parameters during training.", "labels": [], "entities": []}, {"text": "Thus, any word vector from V can be naturally used to encode sentences.", "labels": [], "entities": []}, {"text": "By doing this, our trained sentence encoder can successfully encode 931,331 words.", "labels": [], "entities": []}, {"text": "For training, all weights in the CNN and nonrecurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01].", "labels": [], "entities": [{"text": "LSTM", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8236353993415833}]}, {"text": "Orthogonal initialization is employed on the recurrent matrices in the LSTM.", "labels": [], "entities": []}, {"text": "All bias terms are initialized to zero.", "labels": [], "entities": []}, {"text": "The initial forget gate bias for LSTM is set to 3.", "labels": [], "entities": []}, {"text": "Gradients are clipped if the norm of the parameter vector exceeds 5 (  \" i brought sad news to mistress betty , \" he said quickly , taking back his hand . \" i really appreciate you taking care of lilly for me , \" he said sincerely , handing me the money . \" i am going to tell you a secret , \" she said quietly , and he leaned closer . \" you are very beautiful , \" he said , and he leaned in . she kept glancing out the window at every sound , hoping it was jackson coming back . i kept checking the time every few minutes , hoping it would be five oclock . i take tris 's hand and lead her to the other side of the car , so we can watch the city disappear behind us . i take emma 's hand and lead her to the first taxi , everyone else taking the two remaining cars .: Query-retrieval examples.", "labels": [], "entities": []}, {"text": "In each case (block of rows), the first sentence is a query, while the second sentence is the retrieved result from a random subset of 1 million sentences from the BookCorpus dataset.", "labels": [], "entities": [{"text": "BookCorpus dataset", "start_pos": 164, "end_pos": 182, "type": "DATASET", "confidence": 0.9850299060344696}]}, {"text": "in Theano (), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.", "labels": [], "entities": [{"text": "Theano", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.969841480255127}]}, {"text": "Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and), CR (Hu and Liu, 2004), SUBJ (), MPQA () and TREC ().", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.757540225982666}, {"text": "MR", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.962684690952301}, {"text": "CR", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9821313619613647}, {"text": "TREC", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.951968252658844}]}, {"text": "On all the datasets, we separately train a logistic regression model on top of the extracted sentence features.", "labels": [], "entities": []}, {"text": "We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison.", "labels": [], "entities": []}, {"text": "We also provide the state-of-the-art results using task-dependent learning methods for reference.", "labels": [], "entities": []}, {"text": "Our CNN encoder provides better results than the combine-skip model of on all the 5 datasets.", "labels": [], "entities": []}, {"text": "First, the autoencoder performs better than the future predictor, indicating that the intra-sentence information maybe more important for classification than the inter-sentence information.", "labels": [], "entities": []}, {"text": "Second, the hierarchi-: Classification accuracies on several standard benchmarks.", "labels": [], "entities": []}, {"text": "The last column shows results on the task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.990280419588089}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.784396767616272}, {"text": "F1 score", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9889196455478668}]}, {"text": "\u2020 The first and second block in our results were obtained using the first and second method of considering words not in the training set, respectively.", "labels": [], "entities": []}, {"text": "\u2021 \"combine\" means concatenating the feature vectors learned from both the hierarchical model and the composite model.", "labels": [], "entities": []}, {"text": "cal model performs better than the future predictor, demonstrating the importance of capturing longterm dependencies across multiple sentences.", "labels": [], "entities": []}, {"text": "Our combined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best.", "labels": [], "entities": []}, {"text": "This maybe due to that: (i) both intra-and long-term inter-sentence information are leveraged; (ii) it is easier to linearly separate the feature vectors in higher dimensional spaces.", "labels": [], "entities": []}, {"text": "Further, using (fixed) pre-trained word embeddings consistently provides better performance than using the learned word embeddings.", "labels": [], "entities": []}, {"text": "This maybe due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset (containing 100 billion words) ().", "labels": [], "entities": [{"text": "Google News dataset", "start_pos": 109, "end_pos": 128, "type": "DATASET", "confidence": 0.8141659299532572}]}, {"text": "To further demonstrate the advantage of the learned generic representations, we train a CNN classifier (i.e., a CNN encoder with a logistic regression model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM composite model.", "labels": [], "entities": []}, {"text": "Results are shown in  learning, with the autoencoder on all the data (labeled and unlabled), and the classifier only on the labeled data.", "labels": [], "entities": []}, {"text": "Paraphrase detection Now we consider paraphrase detection on the MSRP dataset ().", "labels": [], "entities": [{"text": "Paraphrase detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8389387130737305}, {"text": "paraphrase detection", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8260287344455719}, {"text": "MSRP dataset", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9089260697364807}]}, {"text": "On this task, one needs to predict whether or not two sentences are paraphrases.", "labels": [], "entities": []}, {"text": "The training set consists of 4076 sentence pairs, and the test set has 1725 pairs.", "labels": [], "entities": []}, {"text": "As in, given two sentence representations z x and z y , we first compute their element-wise product z x z y and their absolute difference |z x \u2212 z y |, and then concatenate them together.", "labels": [], "entities": []}, {"text": "A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases.", "labels": [], "entities": []}, {"text": "We present our results on the last column of.", "labels": [], "entities": []}, {"text": "Our best result is better than the other results that use task-independent methods.", "labels": [], "entities": []}, {"text": "Image-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other.", "labels": [], "entities": [{"text": "Image-sentence ranking", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6696201860904694}, {"text": "image-sentence ranking", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7075973153114319}]}, {"text": "We use the COCO dataset (), which contains 123,287 images each with 5 captions.", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 11, "end_pos": 23, "type": "DATASET", "confidence": 0.966096967458725}]}, {"text": "For development and testing we use the same splits as.", "labels": [], "entities": []}, {"text": "The development and test sets each contain 5000 images.", "labels": [], "entities": []}, {"text": "We further split them into 5 random sets of 1000 images, and report the average performance over the 5 splits.", "labels": [], "entities": []}, {"text": "Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results.", "labels": [], "entities": [{"text": "Recall@K", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8217475612958273}]}, {"text": "We also report the median rank of the closest ground truth result   in the ranked list.", "labels": [], "entities": []}, {"text": "We represent images using 4096-dimensional feature vectors from VggNet (.", "labels": [], "entities": []}, {"text": "Each caption is encoded using our trained CNN encoder.", "labels": [], "entities": []}, {"text": "The training objective is the same pairwise ranking loss as used in, which takes the form of max(0, \u03b1 \u2212 f (x n , y n ) + f (x n , y m )), where f (\u00b7, \u00b7) is the image-sentence score.", "labels": [], "entities": [{"text": "max", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9600701332092285}]}, {"text": "(x n , y n ) denotes the related image-sentence pair, and (x n , y m ) is the randomly sampled unrelated image-sentence pair with n = m.", "labels": [], "entities": []}, {"text": "For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7309905588626862}]}, {"text": "The objective is to force the matching score of the related pair (x n , y n ) to be greater than the unrelated pair (x n , y m ) by a margin \u03b1, which is set to 0.1 in our experiments.", "labels": [], "entities": [{"text": "matching score", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.9232246279716492}]}, {"text": "Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported.", "labels": [], "entities": []}, {"text": "As can be seen, we obtain the same median rank as in, indicating that our encoder is as competitive as the skip-thought vectors (.", "labels": [], "entities": []}, {"text": "The performance gain between our encoder and the combine-skip model of on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re-trieving the most correct item than the skip-thought vector.", "labels": [], "entities": [{"text": "R@1 score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.8563616126775742}]}, {"text": "Semantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset (), consisting of 9927 sentence pairs.", "labels": [], "entities": [{"text": "Semantic relatedness", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7807131707668304}, {"text": "SICK dataset", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.8347189426422119}]}, {"text": "Given two sentences, our goal is to produce a real-valued score between to indicate how semantically related two sentences are, based on human generated scores.", "labels": [], "entities": []}, {"text": "We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset.", "labels": [], "entities": [{"text": "MSRP dataset", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9549458026885986}]}, {"text": "We follow the method in, and use the crossentropy loss for training.", "labels": [], "entities": []}, {"text": "Our result is better than the combineskip model of.", "labels": [], "entities": []}, {"text": "This suggests that CNN also provides competitive performance at matching human relatedness judgements.", "labels": [], "entities": [{"text": "matching human relatedness judgements", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.837218701839447}]}], "tableCaptions": [{"text": " Table 3: Classification accuracies on several standard benchmarks. The last column shows results on the  task of paraphrase detection, where the evaluation metrics are classification accuracy and F1 score.  \u2020 The  first and second block in our results were obtained using the first and second method of considering words  not in the training set, respectively.  \u2021 \"combine\" means concatenating the feature vectors learned from  both the hierarchical model and the composite model.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.946504533290863}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.8270963430404663}, {"text": "F1 score", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9917483031749725}]}, {"text": " Table 4: Results for image-sentence ranking ex- periments on the COCO dataset. R@K denotes  Recall@K (higher is better) and Med r is the me- dian rank (lower is better). ( \u2020) taken from Kiros  et al. (2015). ( * ) taken from Karpathy and Fei-Fei  (2015). ( \u2021) taken from Mao et al. (2015).", "labels": [], "entities": [{"text": "COCO dataset", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.962194174528122}, {"text": "Recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9903642535209656}]}]}