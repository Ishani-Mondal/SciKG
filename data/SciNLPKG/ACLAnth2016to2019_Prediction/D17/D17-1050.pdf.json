{"title": [{"text": "Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal", "labels": [], "entities": [{"text": "Sarcasm Detection Timely", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7446641524632772}]}], "abstractContent": [{"text": "Sarcasm is a pervasive phenomenon in social media, permitting the concise communication of meaning, affect and attitude.", "labels": [], "entities": []}, {"text": "Concision requires wit to produce and wit to understand, which demands from each party knowledge of norms, context and a speaker's mindset.", "labels": [], "entities": []}, {"text": "Insight into a speaker's psychological profile at the time of production is a valuable source of context for sarcasm detection.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.9285619854927063}]}, {"text": "Using a neural architecture , we show significant gains in detection accuracy when knowledge of the speaker's mood at the time of production can be inferred.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9809380173683167}]}, {"text": "Our focus is on sarcasm detection on Twitter, and show that the mood exhibited by a speaker over tweets leading up to anew post is as useful a cue for sarcasm as the topical context of the post itself.", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7729580104351044}]}, {"text": "The work opens the door to an empirical exploration not just of sarcasm in text but of the sarcastic state of mind.", "labels": [], "entities": []}], "introductionContent": [{"text": "Oscar Wilde memorably described sarcasm as \"the lowest form of wit but the highest form of intelligence.\"", "labels": [], "entities": []}, {"text": "Though sarcasm lacks the sophistication of irony, and does little to conceal the speaker's disdain fora target, it is a figurative device that requires as much intelligence from its consumers as its producers.", "labels": [], "entities": []}, {"text": "The concision with which sarcasm and irony allow speakers to conflate propositional content and affective stance makes it a pervasive mode of communication in the 140-character tweets of Twitter.", "labels": [], "entities": []}, {"text": "By combining an overtly positive attitude with a meaning that is more deserving of scorn, sarcasm allows speakers to communicate disappointment about a state of affairs that bites (or etymologically \"cuts the flesh\") of an addressee.", "labels": [], "entities": []}, {"text": "It conveys the feeling the speaker would wish to experience (\"I love it when ...\") with the state of affairs that up-ends this feeling (\"... my friends forget my birthday\").", "labels": [], "entities": []}, {"text": "It often combines politeness with mockery to disguise the appearance of hostility while heightening its effect on a listener.", "labels": [], "entities": []}, {"text": "It establishes awry environment) that has its roots in social norms and the speaker's state of mind.", "labels": [], "entities": []}, {"text": "Psychological theories of irony, such as echoic reminder theory and implicit display theory) have yet to fully translate into text-analytic methods.", "labels": [], "entities": [{"text": "echoic reminder theory", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.972097376982371}]}, {"text": "Neuropsychology researchers who have sought patterns of brain activity to identify the neural correlates of sarcasm note that an understanding of sarcasm is highly dependent not just on the context of an utterance but on the state-of-mind and personality of the speaker, as well as on facial expressions and prosody).", "labels": [], "entities": []}, {"text": "Without the latter markers, purely textual detection must depend largely on the content and context of an utterance, though speaker personality and state-of-mind can also be approximated via text-analytic means.", "labels": [], "entities": [{"text": "textual detection", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7353276908397675}]}, {"text": "Probabilistic classification models that exploit textual cues -such as the juxtaposition of positive sentiment and negative situations (, discriminative words and punctuation marks ( , and emoticon usage () -have achieved good performance across domains, yet these models typically suffer from an absence of psychological insight into a speaker and topical insight into the context of utterance production.", "labels": [], "entities": []}, {"text": "argue that the likelihood of sarcasm is proportional to the amount of knowledge shared by speaker and audience, which includes knowledge of the world and knowledge of the speaker and audience.", "labels": [], "entities": []}, {"text": "Personality is defined by as the \"enduring characteristics of the individual\" though moodwhich is changeable -is perhaps just as useful if sampled in a timely fashion.", "labels": [], "entities": []}, {"text": "The difference between personality and mood can be likened to that between climate and weather.", "labels": [], "entities": []}, {"text": "have developed a Twitter-based mood analysis web service at AnalyzeWords.com which uses a variety of psycholinguistic criteria and the LIWC (Linguistic Inquiry and Word Count) resource 1 to quantify the recent mood -i.e. the recent weather -of a user along 11 dimensions ranging from Arrogance/Remoteness to Anger and Analyticity.", "labels": [], "entities": []}, {"text": "To exploit the stable personality of an online user, sought a correlation between Big Five personality traits and the LIWC-quantifiable dimensions found in re-tweets amongst Twitter users.", "labels": [], "entities": [{"text": "LIWC-quantifiable", "start_pos": 118, "end_pos": 135, "type": "METRIC", "confidence": 0.9681211113929749}]}, {"text": "() have also shown how relevant aspects of personality can be acquired from a speaker's past tweets.", "labels": [], "entities": []}, {"text": "Since personality and mood can each influence the detection process, they underpin our first research question: To what extent can the quantifiable dimensions of either lead to a better understanding of sarcasm?", "labels": [], "entities": []}, {"text": "Reliable detection depends as much on the context of an utterance -which provides the motivation for sarcasm -as its content.", "labels": [], "entities": [{"text": "Reliable detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7100325375795364}]}, {"text": "Consider e.g.: Speaker Utterance: @MSNBC of course all of those jobs will be in China In reply to @realDonaldTrump: I will be the greatest jobsproducing president that God ever created.", "labels": [], "entities": []}, {"text": "The speaker's sarcastic intent cannot be grasped without knowledge of the larger context.", "labels": [], "entities": []}, {"text": "This issue provides our second research question: How can we usefully incorporate utterance context into a neural network model of sarcasm detection?", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.9432215690612793}]}, {"text": "Sarcasm is ubiquitous but always in flux, relying on a changing swirl of socially relevant viewpoints.", "labels": [], "entities": []}, {"text": "The following tweet is sarcastic by virtue of its echoic mockery of a widely ventilated opinion: Time to get my Sunday dose of #fakenews from the failing @nytimes.", "labels": [], "entities": []}, {"text": "This begs the third research question that we explore in the following sections: How can we train our sarcasm detection model to exploit evolving social norms and public opinions?", "labels": [], "entities": [{"text": "sarcasm detection", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.9543875157833099}]}], "datasetContent": [{"text": "Tweets with sarcastic intent are often misclassified due to alack of shared context or knowledge between speaker and annotator.", "labels": [], "entities": []}, {"text": "Opposing social beliefs and a dearth of topical or personal knowledge can lead to serious misjudgments.", "labels": [], "entities": []}, {"text": "Relevant tweet sets can be harvested by searching sarcasm specific hashtags (e.g. #sarcasm).", "labels": [], "entities": []}, {"text": "This approach overlooks tweets that are not explicitly tagged as sarcastic by their authors.", "labels": [], "entities": []}, {"text": "Thus we have devised a feedback-based system that contacts tweet authors directly after-the fact to ask for their authoritative self-annotations fora potentially sarcastic tweet.", "labels": [], "entities": []}, {"text": "In addition to our own training and test sets, whose annotations come directly from tweet authors, we also used 5 Twitter datasets where tweet information, fetched by tweet identifier, contains identifier of context tweet, from which motivating contexts can be discerned for each.", "labels": [], "entities": []}, {"text": "(This contextual requirement prevents us from considering even more of the available sarcasm datasets.)", "labels": [], "entities": []}, {"text": "For the context tweets s j for each s i in these sets we collected the most recent linked tweets of s i . To obtain the 11 AnalyzeWords.com dimensions for tweet authors, we collect the 50 tweets of u i posted just prior to s i , and use the LIWC to estimate the 11 dimensions (Anger, Arrogance, etc.) from those tweets.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 241, "end_pos": 245, "type": "METRIC", "confidence": 0.5942369103431702}, {"text": "Arrogance", "start_pos": 284, "end_pos": 293, "type": "METRIC", "confidence": 0.8305205702781677}]}, {"text": "As AnalyzeWords.com does not provide retrospective analyses, and as its code is not public, we reverseengineered a substitute using the LIWC by following the creators' guidelines in).", "labels": [], "entities": []}, {"text": "For subsequent evaluations, the 5 external datasets were split into 3 parts each: 80% for training, 10% for development/tuning, and 10% for testing.", "labels": [], "entities": []}, {"text": "Success with a neural architecture requires apt input features and an equally apt selection of hyperparameters.", "labels": [], "entities": []}, {"text": "After performing a grid search over hyper-parameters, the best configuration of the CNN, LSTM and DNN layers places 1280 hidden memory units into each layer and uses a CNN filter width of 3.", "labels": [], "entities": []}, {"text": "A simple baseline will use only the textual content of a tweet s i without a context s j or an affective profile aw of the author u i . To appreciate the contribution of different input sources of information we trained the network on different combinations of these sources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the model when a specific  dimension aw i is omitted from training.", "labels": [], "entities": []}, {"text": " Table 2: Bootstrapping gains (August, 2016)", "labels": [], "entities": []}]}