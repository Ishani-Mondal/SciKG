{"title": [{"text": "Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation", "labels": [], "entities": []}], "abstractContent": [{"text": "Existing approaches to automatic VerbNet-style verb classification are heavily dependent on feature engineering and therefore limited to languages with mature NLP pipelines.", "labels": [], "entities": [{"text": "VerbNet-style verb classification", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.5892200767993927}]}, {"text": "In this work, we propose a novel cross-lingual transfer method for inducing VerbNets for multiple languages.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study which demonstrates how the architectures for learning word embeddings can be applied to this challenging syntactic-semantic task.", "labels": [], "entities": []}, {"text": "Our method uses cross-lingual translation pairs to tie each of the six target languages into a bilingual vector space with English, jointly specialising the representations to encode the relational information from English VerbNet.", "labels": [], "entities": []}, {"text": "A standard clustering algorithm is then run on top of the VerbNet-specialised representations, using vector dimensions as features for learning verb classes.", "labels": [], "entities": []}, {"text": "Our results show that the proposed cross-lingual transfer approach sets new state-of-the-art verb classification performance across all six target languages explored in this work.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7348434031009674}, {"text": "verb classification", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7448603808879852}]}], "introductionContent": [{"text": "Playing a key role in conveying the meaning of a sentence, verbs are famously complex.", "labels": [], "entities": [{"text": "conveying the meaning of a sentence", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.8220708469549814}]}, {"text": "They display a wide range of syntactic-semantic behaviour, expressing the semantics of an event as well as relational information among its participants.", "labels": [], "entities": []}, {"text": "Lexical resources which capture the variability of verbs are instrumental for many Natural Language Processing (NLP) applications.", "labels": [], "entities": []}, {"text": "One of the richest verb resources currently available for English is VerbNet (.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9500585198402405}]}, {"text": "Based on the work of, this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour.", "labels": [], "entities": []}, {"text": "Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (), semantic parsing), word sense disambiguation), discourse parsing, information extraction), text mining applications ( , research into human language acquisition, and other tasks.", "labels": [], "entities": [{"text": "semantic role labelling", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.6826731165250143}, {"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7424914985895157}, {"text": "word sense disambiguation", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.6663857201735178}, {"text": "discourse parsing", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7473519742488861}, {"text": "information extraction", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.7977174520492554}, {"text": "text mining", "start_pos": 215, "end_pos": 226, "type": "TASK", "confidence": 0.8169270157814026}, {"text": "human language acquisition", "start_pos": 258, "end_pos": 284, "type": "TASK", "confidence": 0.65420068303744}]}, {"text": "This benefit for English NLP has motivated the development of VerbNets for languages such as), Czech (, and Mandarin (.", "labels": [], "entities": []}, {"text": "However, end-to-end manual resource development using Levin's methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages ().", "labels": [], "entities": []}, {"text": "Approaches which aim to learn verb classes automatically offer an attractive alternative.", "labels": [], "entities": []}, {"text": "However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im;.", "labels": [], "entities": []}, {"text": "Such methods are limited to a small set of resource-rich languages.", "labels": [], "entities": []}, {"text": "It has been argued that VerbNet-style classification has a strong cross-lingual element.", "labels": [], "entities": [{"text": "VerbNet-style classification", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.6830617487430573}]}, {"text": "In support of this argument, have shown that English VerbNet has high translatability across different, even typologically diverse languages.", "labels": [], "entities": []}, {"text": "Based on this finding, we propose an automatic approach which exploits readily available annotations for English to facilitate efficient, large-scale development of VerbNets fora wide set of target languages.", "labels": [], "entities": []}, {"text": "Recently, unsupervised methods for inducing distributed word vector space representations or word embeddings () have been successfully applied to a plethora of NLP tasks, i.a.).", "labels": [], "entities": []}, {"text": "These methods offer an elegant way to learn directly from large corpora, bypassing the feature engineering step and the dependence on mature NLP pipelines (e.g., POS taggers, parsers, extraction of subcategorisation frames).", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 162, "end_pos": 173, "type": "TASK", "confidence": 0.7103271186351776}]}, {"text": "In this work, we demonstrate how these models can be used to support automatic verb class induction.", "labels": [], "entities": [{"text": "automatic verb class induction", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.6311341524124146}]}, {"text": "Moreover, we show that these models offer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages.", "labels": [], "entities": [{"text": "VerbNet-style classification", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.6764277368783951}]}, {"text": "To the best of our knowledge, this proposition has not been investigated in previous work.", "labels": [], "entities": []}, {"text": "There has been little work on assessing the suitability of embeddings for capturing rich syntacticsemantic phenomena.", "labels": [], "entities": []}, {"text": "One challenge is their reliance on the distributional hypothesis, which coalesces fine-grained syntacticsemantic relations between words into abroad relation of semantic relatedness (e.g., coffee:cup) (.", "labels": [], "entities": []}, {"text": "This property has an adverse effect when word embeddings are used in downstream tasks such as spoken language understanding ( or dialogue state tracking.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 94, "end_pos": 123, "type": "TASK", "confidence": 0.6599231660366058}, {"text": "dialogue state tracking", "start_pos": 129, "end_pos": 152, "type": "TASK", "confidence": 0.6712139149506887}]}, {"text": "It could have a similar effect on verb classification, which relies on the similarity in syntactic-semantic properties of verbs within a class.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7440490424633026}]}, {"text": "In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages?", "labels": [], "entities": []}, {"text": "(Q2) Can one boost verb classification for lowerresource languages by exploiting general-purpose cross-lingual resources such as BabelNet () or bilingual dictionaries such as PanLex () to construct better word vector spaces for these languages?", "labels": [], "entities": [{"text": "verb classification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7450448870658875}, {"text": "PanLex", "start_pos": 175, "end_pos": 181, "type": "DATASET", "confidence": 0.9371633529663086}]}, {"text": "(Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages?", "labels": [], "entities": [{"text": "VerbNet-style classification", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6856373846530914}]}, {"text": "In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language?", "labels": [], "entities": []}, {"text": "To investigate Q1, we induce standard distributional vector spaces () from large monolingual corpora in English and six target languages.", "labels": [], "entities": []}, {"text": "As expected, the results obtained with this straightforward approach show positive trends, but at the same time reveal its limitations for all the languages involved.", "labels": [], "entities": []}, {"text": "Therefore, the focus of our work shifts to Q2 and Q3.", "labels": [], "entities": []}, {"text": "The problem of inducing VerbNetoriented embeddings is framed as vector space specialisation using the available external resources: BabelNet or PanLex, and (English) VerbNet.", "labels": [], "entities": [{"text": "PanLex", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.9503015279769897}]}, {"text": "Formalised as an instance of post-processing semantic specialisation approaches (, our procedure is steered by two sets of linguistic constraints: 1) cross-lingual (translation) links between languages extracted from BabelNet (targeting Q2); and 2) the available VerbNet annotations fora resource-rich language.", "labels": [], "entities": []}, {"text": "The two sets of constraints jointly target Q3.", "labels": [], "entities": []}, {"text": "The main goal of vector space specialisation is to pull examples standing in desirable relations, as described by the constraints, closer together in the transformed vector space.", "labels": [], "entities": [{"text": "vector space specialisation", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7289780577023824}]}, {"text": "The specialisation process can capitalise on the knowledge of VerbNet relations in the source language (English) by using translation pairs to transfer that knowledge to each of the target languages.", "labels": [], "entities": []}, {"text": "By constructing shared bilingual vector spaces, our method facilitates the transfer of semantic relations derived from VerbNet to the vector spaces of resource-lean target languages.", "labels": [], "entities": []}, {"text": "This idea is illustrated by.", "labels": [], "entities": []}, {"text": "Our results indicate that cross-lingual connections yield improved verb classes across all six target languages (thus answering Q2).", "labels": [], "entities": []}, {"text": "Moreover, a consistent and significant boost in verb classification performance is achieved by propagating the VerbNet-style information from the source language (English) to any other target language (e.g., Italian, Croatian, Polish, Finnish) for which no VerbNet-style information is available during the and effectively transforms the initial distributional French vector subspace to also emphasise the VerbNet-style structure, facilitating the induction of verb classes in French.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7541501224040985}]}, {"text": "fine-tuning process (thus answering Q3).", "labels": [], "entities": []}, {"text": "We report state-of-the-art verb classification performance for all six languages in our experiments.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7129240185022354}]}, {"text": "For instance, we improve the state-of-the-art F-1 score from prior work from 0.55 to 0.79 for French, and from 0.43 to 0.74 for Brazilian Portuguese.", "labels": [], "entities": [{"text": "F-1 score", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9111091494560242}]}], "datasetContent": [{"text": "Languages We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).", "labels": [], "entities": []}, {"text": "All statistics regarding the source and size of training and test data, and linguistic constraints for each target language are summarised in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.9374117255210876}]}, {"text": "2. Automatic approaches to verb class induction have been tried out in prior work for FR and PT.", "labels": [], "entities": [{"text": "verb class induction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7410780191421509}, {"text": "FR", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.92622309923172}]}, {"text": "To the best of our knowledge, our cross-lingual study is the first aiming to generalise an automatic induction method to more languages using an underlying methodology which is language-pair independent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the experimental setup for each target language: training/test data and constraints.  Coverage refers to the percentage of test verbs represented in the target language vocabularies.", "labels": [], "entities": []}, {"text": " Table 3: The effect of multilingual vector space  specialisation. Results are reported for FR and IT  using: a) cross-lingual constraints only (XLing);  and b) the VerbNet transfer model (XLing+VN).", "labels": [], "entities": [{"text": "FR", "start_pos": 92, "end_pos": 94, "type": "DATASET", "confidence": 0.6207630634307861}]}, {"text": " Table 4: Comparison of verb classification (VC)  and verb semantic similarity (Sim) for English.  VC is measured on the EN test set of Sun et al.  (2008). Sim is measured on SimVerb-3500 (Gerz  et al., 2016). The scores are Spearman's \u03c1 corre- lation scores. EN-Dist is the initial distributional  English vector space: SGNS-BOW2; EN-VN is  the same space transformed using monolingual EN  VerbNet constraints only, an upper bound for the  specialisation-based approach in EN.", "labels": [], "entities": [{"text": "EN test set", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.8400318821271261}]}]}