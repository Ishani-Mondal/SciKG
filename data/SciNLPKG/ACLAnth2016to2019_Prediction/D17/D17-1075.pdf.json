{"title": [{"text": "A Sub-Character Architecture for Korean Language Processing", "labels": [], "entities": [{"text": "Korean Language Processing", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.6841291387875875}]}], "abstractContent": [{"text": "We introduce a novel sub-character architecture that exploits a unique com-positional structure of the Korean language.", "labels": [], "entities": []}, {"text": "Our method decomposes each character into a small set of primitive phonetic units called jamo letters from which character-and word-level representations are induced.", "labels": [], "entities": []}, {"text": "The jamo letters divulge syntactic and semantic information that is difficult to access with conventional character-level units.", "labels": [], "entities": []}, {"text": "They greatly alleviate the data sparsity problem, reducing the observation space to 1.6% of the original while increasing accuracy in our experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9992644190788269}]}, {"text": "We apply our architecture to dependency parsing and achieve dramatic improvement over strong lexical baselines.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8276161253452301}]}], "introductionContent": [{"text": "Korean is generally recognized as a language isolate: that is, it has no apparent genealogical relationship with other languages.", "labels": [], "entities": []}, {"text": "A unique feature of the language is that each character is composed of a small, fixed set of basic phonetic units called jamo letters.", "labels": [], "entities": []}, {"text": "Despite the important role jamo plays in encoding syntactic and semantic information of words, it has been neglected in existing modern Korean processing algorithms.", "labels": [], "entities": [{"text": "encoding syntactic and semantic information of words", "start_pos": 41, "end_pos": 93, "type": "TASK", "confidence": 0.8173379472323826}]}, {"text": "In this paper, we bridge this gap by introducing a novel compositional neural architecture that explicitly leverages the sub-character information.", "labels": [], "entities": []}, {"text": "Specifically, we perform Unicode decomposition on each Korean character to recover its underlying jamo letters and construct character-and word-level representations from these letters.", "labels": [], "entities": [{"text": "Unicode decomposition", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7370383739471436}]}, {"text": "See  The decomposition is deterministic; this is a crucial departure from previous work that uses language-specific sub-character information such as radical (a graphical component of a Chinese character).", "labels": [], "entities": []}, {"text": "The radical structure of a Chinese character does not follow any systematic process, requiring an incomplete dictionary mapping between characters and radicals to take advantage of this information.", "labels": [], "entities": []}, {"text": "In contrast, our Unicode decomposition does not need any supervision and can extract correct jamo letters for all possible Korean characters.", "labels": [], "entities": []}, {"text": "Our jamo architecture is fully general and can be plugged in any Korean processing network.", "labels": [], "entities": []}, {"text": "For a concrete demonstration of its utility, in this work we focus on dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8642448782920837}]}, {"text": "note that \"Korean emerges as a very clear outlier\" in their cross-lingual parsing experiments on the universal treebank, implying a need to tailor a model for this language isolate.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7006354331970215}]}, {"text": "Because of the compositional morphology, Korean suffers extreme data sparsity at the word level: 2,703 out of 4,698 word types (> 57%) in the held-out portion of our treebank are OOV.", "labels": [], "entities": []}, {"text": "This makes the language challenging for simple lexical parsers even when augmented with a large set of pre-trained word representations.", "labels": [], "entities": []}, {"text": "While such data sparsity can also be alleviated by incorporating more conventional characterlevel information, we show that incorporating jamo is an effective and economical new approach to combating the sparsity problem for Korean.", "labels": [], "entities": []}, {"text": "In experiments, we decisively improve the LAS of the lexical BiLSTM parser of from 82.77 to 91.46 while reducing the size of input space by 98.4% when we replace words with jamos.", "labels": [], "entities": [{"text": "LAS", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9974464178085327}]}, {"text": "As a point of reference, a strong feature-rich parser using gold POS tags obtains 88.61.", "labels": [], "entities": []}, {"text": "To summarize, we make the following contributions.", "labels": [], "entities": []}, {"text": "\u2022 To our knowledge, this is the first work that leverages jamo in end-to-end neural Korean processing.", "labels": [], "entities": [{"text": "end-to-end neural Korean processing", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.6244112849235535}]}, {"text": "To this end, we develop a novel sub-character architecture based on deterministic Unicode decomposition.", "labels": [], "entities": []}, {"text": "\u2022 We perform extensive experiments on dependency parsing to verify the utility of the approach.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8318026661872864}]}, {"text": "We show clear performance boost with a drastically smaller set of parameters.", "labels": [], "entities": []}, {"text": "Our final model outperforms strong baselines by a large margin.", "labels": [], "entities": []}, {"text": "\u2022 We release an implementation of our jamo architecture which can be plugged in any Korean processing network.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data We use the publicly available Korean treebank in the universal treebank version 2.0 (McDonald et al., 2013).", "labels": [], "entities": [{"text": "Korean treebank", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.953641951084137}]}, {"text": "The dataset comes with a train/development/test split; data statistics are shown in.", "labels": [], "entities": []}, {"text": "Since the test portion is significantly smaller than the dev portion, we report performance on both.", "labels": [], "entities": []}, {"text": "As expected, we observe severe data sparsity with words: 24,814 out of 31,060 elements in the vocabulary appear only once in the training data.", "labels": [], "entities": []}, {"text": "On the dev set, about 57% word types and 3% character types are OOV.", "labels": [], "entities": []}, {"text": "Upon Unicode decomposition, we obtain the following 48 jamo types: 2 https://github.com/ryanmcd/uni-dep-tb none of which is OOV in the dev set.", "labels": [], "entities": []}, {"text": "Implementation and baselines We implement our jamo architecture using the DyNet library ( and plug it into the BiLSTM parser of.", "labels": [], "entities": [{"text": "DyNet library", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.8813563287258148}]}, {"text": "For Korean syllable manipulation, we use the freely available toolkit by Joshua Dong.", "labels": [], "entities": [{"text": "Korean syllable manipulation", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.7161016960938772}]}, {"text": "We train the parser for 30 epochs and use the dev portion for model selection.", "labels": [], "entities": [{"text": "model selection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.610485315322876}]}, {"text": "We compare our approach to the following baselines: \u2022 McDonald13: A cross-lingual parser originally reported in.", "labels": [], "entities": [{"text": "McDonald13", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9744793772697449}]}, {"text": "\u2022 Yara: A beam-search transition-based parser of based on the rich non-local features in.", "labels": [], "entities": [{"text": "Yara", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.8378452658653259}]}, {"text": "We use beam width 64.", "labels": [], "entities": []}, {"text": "We use 5-fold jackknifing on the training portion to provide POS tag features.", "labels": [], "entities": [{"text": "POS tag", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.665550947189331}]}, {"text": "We also report on using gold POS tags.", "labels": [], "entities": []}, {"text": "\u2022 K&G16: The basic BiLSTM parser of Kiperwasser and Goldberg (2016) without the sublexical architecture introduced in this work.", "labels": [], "entities": []}, {"text": "\u2022 Stack LSTM: A greedy transition-based parser based on stack LSTM representations.", "labels": [], "entities": []}, {"text": "Dyer15 denotes the word-level variant (   We observe decisive improvement when we incorporate sub-lexical information into the parser of K&G16.", "labels": [], "entities": [{"text": "Dyer15", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7818319201469421}]}, {"text": "In fact, a strictly sub-lexical parser using only jamos or characters clearly outperforms its lexical counterpart despite the fact that the model is drastically smaller (e.g., 90.77 with 500\u00d7 100 jamo embeddings vs 82.77 with 298115\u00d7100 word embeddings).", "labels": [], "entities": []}, {"text": "Notably, jamos alone achieve 91.46 which is not far behind the best result 92.31 obtained by using word, character, and jamo units in conjunction.", "labels": [], "entities": []}, {"text": "This demonstrates that our compositional architecture learns to build effective representations of Korean characters and words for parsing from a minuscule set of jamo letters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Treebank statistics. Upper: Number of  trees in the split. Lower: Number of unit types  in the training portion. For simplicity, we include  non-Korean symbols (e.g., @, , a) as charac- ters/jamos.", "labels": [], "entities": []}, {"text": " Table 1. Since the test portion is sig- nificantly smaller than the dev portion, we report  performance on both.  As expected, we observe severe data sparsity  with words: 24,814 out of 31,060 elements in the  vocabulary appear only once in the training data.  On the dev set, about 57% word types and 3%  character types are OOV. Upon Unicode decom- position, we obtain the following 48 jamo types:", "labels": [], "entities": []}, {"text": " Table 2: Main result. Upper: Accuracy with baseline models. Lower: Accuracy with different configu- rations of our parser network (word-only is identical to K&G16).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.995824933052063}, {"text": "Accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9938200116157532}]}]}