{"title": [{"text": "Latent Space Embedding for Retrieval in Question-Answer Archives", "labels": [], "entities": [{"text": "Latent Space Embedding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.608106255531311}]}], "abstractContent": [{"text": "Community-driven Question Answering (CQA) systems such as Yahoo!", "labels": [], "entities": [{"text": "Question Answering (CQA)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.80859295129776}]}, {"text": "Answers have become valuable sources of reusable information.", "labels": [], "entities": []}, {"text": "CQA retrieval enables usage of historical CQA archives to solve new questions posed by users.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7776049077510834}]}, {"text": "This task has received much recent attention, with methods building upon literature from translation models, topic models, and deep learning.", "labels": [], "entities": []}, {"text": "In this paper, we devise a CQA retrieval technique, LASER-QA, that embeds question-answer pairs within a unified latent space preserving the local neighborhood structure of question and answer spaces.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9684568047523499}]}, {"text": "The idea is that such a space mirrors semantic similarity among questions as well as answers, thereby enabling high quality retrieval.", "labels": [], "entities": []}, {"text": "Through an empirical analysis on various real-world QA datasets, we illustrate the improved effectiveness of LASER-QA over state-of-the-art methods.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.5181990265846252}]}], "introductionContent": [{"text": "Community-based Question Answering (CQA) services such as Yahoo!", "labels": [], "entities": [{"text": "Question Answering (CQA)", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7988962292671203}]}, {"text": "Answers 1 , Quora 2 , StackOverflow 3 , and Baidu Zhidao have become a dependable source of knowledge to solve common user problems.", "labels": [], "entities": [{"text": "Baidu Zhidao", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.8677334785461426}]}, {"text": "These allow a user to post queries such as how and why questions that seek descriptive solutions and opinions as answers.", "labels": [], "entities": []}, {"text": "Over time, these services buildup a large archive of questionanswer knowledge that maybe leveraged to solve new user questions.", "labels": [], "entities": []}, {"text": "The CQA retrieval problem,  that has received much recent attention, is about addressing this opportunity.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.6603732407093048}]}, {"text": "CQA retrieval methods focus on finding historical archived knowledge (questions, answers or QA pairs) that are relevant to a newly posed user question.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6791833490133286}]}, {"text": "The central technical challenge that differentiates CQA retrieval from other general purpose IR tasks is that of the need to address the lexical gap (aka lexical chasm) in QA archives.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8072124719619751}]}, {"text": "Lexical chasm means that text fragments in questions (e.g., disk full) may lead to semantically correlated content in answers (e.g., format).", "labels": [], "entities": []}, {"text": "This QA-correlation is different from semantic relatedness such as synonymy and antonymy; in the above example, the correlation is due to disk full issues often leading to solution involving disk formating.", "labels": [], "entities": []}, {"text": "Explicit correlation modelling, using statistical translation models, have met with much success in CQA retrieval.", "labels": [], "entities": [{"text": "CQA retrieval", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.6529847979545593}]}, {"text": "In this paper, we take a neighborhood preserving learning approach, and learn a unified representation for QA pairs in an abstract latent space.", "labels": [], "entities": []}, {"text": "Consider two example CQA pairs from a technical support forum presented in; the intuitive causes listed alongside are external to the dataset.", "labels": [], "entities": []}, {"text": "Though the questions are reasonably similar lexically, they pertain to very different issues as illustrated by the wide disparity in the answers posed to them.", "labels": [], "entities": []}, {"text": "We model QA-pairs in a unified space that preserves the similarity neighborhood in question and answer spaces.", "labels": [], "entities": []}, {"text": "In this example, the wide divergence in answer-space similarity neighborhoods between the two QAs would pull them apart, so they live in different parts of the latent space, reflecting the dissimilarity between their causes.", "labels": [], "entities": []}, {"text": "Thus, our contribution in this paper is a neighborhood-preserving method for CQA retrieval, LASER-QA, expanding to LAtent-Space Embedding for Rretrieval in QA archives.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9519557952880859}]}], "datasetContent": [{"text": "In the datasets that we use, we have labels indicating which QAs are related/relevant to a particular question.", "labels": [], "entities": []}, {"text": "Thus, the quality of the scoring function can be evaluated using traditional information retrieval metrics such as Precision, MAP, MRR, and NDCG when measured against such labellings.", "labels": [], "entities": [{"text": "Precision", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.90278160572052}, {"text": "MAP", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.7534887194633484}, {"text": "MRR", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.8478327989578247}]}, {"text": "In addition, we will use one more metric, namely Success Rate, the fraction of questions for which at least one related question is ranked among the top-t, in evaluation.", "labels": [], "entities": [{"text": "Success Rate", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9884850382804871}]}, {"text": "Datasets: We use two recent datasets in our evaluation, and SemEval2016-Task3.", "labels": [], "entities": []}, {"text": "The former has a manually labelled set of related questions to every question, whereas the latter has relevance labels associated with answers (appearing as comments); these labellings make automated evaluation possible.", "labels": [], "entities": []}, {"text": "Among the 12 subsets in CQADupStack, owing to scalability issues of the AENN baseline, we choose the three smaller subsets from CQADupStack, namely webmasters (1299 QAs), android, and gis (3726) fora full comparative evaluation.", "labels": [], "entities": [{"text": "CQADupStack", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.934106707572937}, {"text": "AENN baseline", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.7450889647006989}]}, {"text": "Each of these are split into two halves, with one portion used for the training (that is, learning the statistical model such as LASER-QA, translation model, etc.) and the other one used for the testing (the 50:50 split ensures a sizeable test set).", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9079014658927917}]}, {"text": "The related labellings are used only for evaluation purposes; however, since only training pairs are retrieved within this setup, related labellings across QAs in the testing set would be missed, artificially lowering the recall of all the methods in our evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.9988983869552612}]}, {"text": "Ina recent analysis, CQADupStack authors quantify the incompleteness of labeling in the dataset.", "labels": [], "entities": [{"text": "CQADupStack", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.889866828918457}]}, {"text": "Such issues further artificially reduce retrieval accuracies as estimated from our automated evaluation.", "labels": [], "entities": []}, {"text": "The SemEval2016 dataset, on the other hand, has an implicit test-train split.", "labels": [], "entities": [{"text": "SemEval2016 dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8394869565963745}]}, {"text": "We use the subset of the data categorized under Qatar Living Lounge, the largest category (which is 27% of the full dataset), for our experiments.", "labels": [], "entities": [{"text": "Qatar Living Lounge", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.9768433173497518}]}, {"text": "All 'comments' that are labelled relevant to the associated question are paired together as QA-pairs to form a training set of 1366 pairs, with the test questions from the dataset used as is.", "labels": [], "entities": []}, {"text": "Baselines: As detailed in Section 2, we compare against three baselines (a) TBLM () (topic model approach), (b) Topic-TRLM () (topic+translation models), and (b) AENN () (deep learning).", "labels": [], "entities": [{"text": "TBLM", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8798632025718689}, {"text": "AENN", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9901829361915588}]}, {"text": "TBLM requires an answer quality signal that we set to unity.", "labels": [], "entities": [{"text": "TBLM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5857117176055908}]}, {"text": "We use authorrecommended parameter settings for TBLM and TopicTRLM.", "labels": [], "entities": []}, {"text": "Since AENN learns a latent space representation (though a separate one for questions and answers unlike LASER-QA), the evaluation w.r.t LASER-QA is a direct comparison of the quality of the respective latent spaces.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8035541772842407}]}, {"text": "The AENN method requires training triplets, i.e.,; we populate the other answer part using the answer of a related question.", "labels": [], "entities": [{"text": "AENN", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8960152864456177}]}, {"text": "This gives AENN an advantage as it uses relations among training pairs that are unavailable to other methods.", "labels": [], "entities": [{"text": "AENN", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.644574761390686}]}, {"text": "For AENN, quality measures peaked around 2000 (for webmasters and gis) and 3000 (for android and SemEval2016) for latent space dimensionality; our results are from such settings.", "labels": [], "entities": [{"text": "AENN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7385684251785278}]}, {"text": "LASER-QA Parameters: We set k = 15 and \u03b1 = 0.8, the latter ensuring that the question space is given more importance.", "labels": [], "entities": [{"text": "LASER-QA Parameters", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8763019144535065}]}, {"text": "We always set d to the number of eigen vectors in Z, equalling |D|.", "labels": [], "entities": []}, {"text": "We will separately study LASER-QA trends against parameter variations as well.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8833141326904297}]}, {"text": "Evaluation Metrics: We use Precision, Success Rate (SR) (Ref. Sec 3), MAP and NDCG) for our evaluation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9963697195053101}, {"text": "Success Rate (SR)", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.9711000084877014}, {"text": "MAP", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9704105257987976}]}, {"text": "Precision simply measures the fraction of related documents among the top-t that were retrieved.", "labels": [], "entities": []}, {"text": "Due to this rank-agnostic construction, precision is unable to incentivize for putting the relevant results at the top of the result instead of deeper down.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9993991851806641}]}, {"text": "In contrast, MAP and NDCG are rank-aware metrics.", "labels": [], "entities": [{"text": "MAP", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6407644152641296}, {"text": "NDCG", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8504665493965149}]}, {"text": "MAP 10 computes the average of precisions computed at rank positions where a relevant result is returned.", "labels": [], "entities": [{"text": "precisions", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9715098738670349}]}, {"text": "NDCG is another rank-aware metric 11 that discounts the appearance of the revelant result based on it's rank in the result set.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9621176719665527}]}, {"text": "We assess statistical significance using randomization tests (.", "labels": [], "entities": []}, {"text": "summarizes the comparative evaluation across varying t (best results boldfaced).", "labels": [], "entities": []}, {"text": "The following observations are notable:  \u2022 LASER-QA outperforms the other methods across datasets.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.972646951675415}]}, {"text": "This is followed by Topic-TRLM, TBLM and then AENN.", "labels": [], "entities": [{"text": "TBLM", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.8726674914360046}, {"text": "AENN", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9356493353843689}]}, {"text": "\u2022 LASER-QA's margin is highest at (small) values oft that are typical of scenarios involving human perusal of results.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9935479760169983}, {"text": "margin", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.5239179730415344}]}, {"text": "As t in-  \u2022 LASER-QA performance peaks on rankaware metrics such as MAP and NDCG (even at t = 50), indicating it's high effectiveness in producing relevant results at the top.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.989222526550293}, {"text": "MAP", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.8929233551025391}, {"text": "NDCG", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.6177234649658203}]}, {"text": "These observations underline the effectiveness of LASER-QA as a CQA retrieval method.", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.5162497758865356}]}, {"text": "It maybe noted that LASER-QA uses compact representations, as compared to vocabulary space representations that are typically \u2265 5000.", "labels": [], "entities": []}, {"text": "to illustrate the consistency in trends.", "labels": [], "entities": [{"text": "consistency", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9626750349998474}]}, {"text": "Boldfacing and statistical significance have the same semantics as earlier, with the comparison performed against only TopicTRLM and TBLM.", "labels": [], "entities": [{"text": "Boldfacing", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.8812984824180603}, {"text": "TBLM", "start_pos": 133, "end_pos": 137, "type": "DATASET", "confidence": 0.834966778755188}]}], "tableCaptions": [{"text": " Table 2: Retrieval Results (\u2022 & \u2022 denote statistical significance at p-value < 0.01 & < 0.05 respectively)", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7995508313179016}, {"text": "statistical significance", "start_pos": 42, "end_pos": 66, "type": "METRIC", "confidence": 0.791136771440506}]}, {"text": " Table 3: LASER-QA Results (Boldfacing and Statistical Sig- nificance indications from comparison with TopicTRLM and  TBLM) over Larger Categories in CQADupStack  Figure 1: NDCG (Y-axis) v/s. k", "labels": [], "entities": [{"text": "LASER-QA", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9903106689453125}, {"text": "Boldfacing", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9422096014022827}, {"text": "Statistical Sig- nificance indications", "start_pos": 43, "end_pos": 81, "type": "METRIC", "confidence": 0.877364432811737}]}]}