{"title": [{"text": "Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings in the setting of Wieting et al.", "labels": [], "entities": []}, {"text": "We use neural machine translation to generate sentential paraphrases via back-translation of bilingual sentence pairs.", "labels": [], "entities": []}, {"text": "We evaluate the paraphrase pairs by their ability to serve as training data for learning paraphrastic sentence embed-dings.", "labels": [], "entities": []}, {"text": "We find that the data quality is stronger than prior work based on bitext and on par with manually-written English paraphrase pairs, with the advantage that our approach can scale up to generate large training sets for many languages and domains.", "labels": [], "entities": []}, {"text": "We experiment with several language pairs and data sources, and develop a variety of data filtering techniques.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7334084808826447}]}, {"text": "In the process, we explore how neural machine translation output differs from human-written sentences, finding clear differences in length, the amount of repetition, and the use of rare words.", "labels": [], "entities": [{"text": "neural machine translation output", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7569742500782013}]}], "introductionContent": [{"text": "Pretrained word embeddings have received a great deal of attention from the research community, but there is much less work on developing pretrained embeddings for sentences.", "labels": [], "entities": [{"text": "Pretrained word embeddings", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5981037815411886}]}, {"text": "Here we target sentence embeddings that are \"paraphrastic\" in the sense that two sentences with similar meanings are close in the embedding space.", "labels": [], "entities": []}, {"text": "developed paraphrastic sentence embeddings that are useful for semantic textual similarity tasks and can also be used as initialization for supervised semantic tasks.", "labels": [], "entities": []}, {"text": "R: We understand that has already commenced, but there is along way to go.", "labels": [], "entities": []}, {"text": "T: This situation has already commenced, but much still needs to be done.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9250180721282959}]}, {"text": "R: The restaurant is closed on Sundays.", "labels": [], "entities": [{"text": "R", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9115244150161743}]}, {"text": "No breakfast is available on Sunday mornings.", "labels": [], "entities": []}, {"text": "T: The restaurant stays closed Sundays so no breakfast is served these days.", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9313499927520752}]}, {"text": "R: Improved central bank policy is another huge factor.", "labels": [], "entities": []}, {"text": "T: Another crucial factor is the improved policy of the central banks.: Illustrative examples of references (R) paired with back-translations (T).", "labels": [], "entities": [{"text": "T", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9645271897315979}]}, {"text": "To learn their sentence embeddings, Wieting et al. used the Paraphrase Database (PPDB) (.", "labels": [], "entities": [{"text": "Paraphrase Database (PPDB)", "start_pos": 60, "end_pos": 86, "type": "DATASET", "confidence": 0.7795440018177032}]}, {"text": "PPDB contains a large set of paraphrastic textual fragments extracted automatically from bilingual text (\"bitext\"), which is readily available for languages and domains.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8247705101966858}]}, {"text": "Versions of PPDB have been released for several languages ().", "labels": [], "entities": []}, {"text": "However, more recent work has shown that the fragmental nature of PPDB's pairs can be problematic, especially for recurrent networks (.", "labels": [], "entities": []}, {"text": "Better performance can be achieved with a smaller set of sentence pairs derived from aligning Simple English and standard English Wikipedia (.", "labels": [], "entities": []}, {"text": "While effective, this type of data is inherently limited in size and scope, and not available for languages other than English.", "labels": [], "entities": []}, {"text": "PPDB is appealing in that it only requires bitext.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8619415163993835}]}, {"text": "We would like to retain this property but develop a data resource with sentence pairs rather than phrase pairs.", "labels": [], "entities": []}, {"text": "We turn to neural machine translation (NMT) (), which has matured recently to yield strong performance especially in terms of producing grammatical outputs.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8063079218069712}]}, {"text": "In this paper, we build NMT systems for three language pairs, then use them to back-translate the non-English side of the training bitext.", "labels": [], "entities": []}, {"text": "The resulting data consists of sentence pairs containing an English reference and the output of an X-toEnglish NMT system.", "labels": [], "entities": []}, {"text": "We use this data for training paraphrastic sentence embeddings, yielding results that are much stronger than when using PPDB and competitive with the Simple English Wikipedia data.", "labels": [], "entities": [{"text": "Simple English Wikipedia data", "start_pos": 150, "end_pos": 179, "type": "DATASET", "confidence": 0.7384020537137985}]}, {"text": "Since bitext is abundant and available for many language pairs and domains, we also develop several methods of filtering the data, including based on sentence length, quality measures, and measures of difference between the reference and its back-translation.", "labels": [], "entities": []}, {"text": "We find length to bean effective filtering method, showing that very short length ranges-where the translation is 1 to 10 wordsare best for learning.", "labels": [], "entities": []}, {"text": "In studying quality measures for filtering, we train a classifier to predict if a sentence is a reference or a back-translation, then score sentences by the classifier score.", "labels": [], "entities": []}, {"text": "This investigation allows us to examine the kinds of phenomena that best distinguish NMT output from references in this controlled setting of translating the bitext training data.", "labels": [], "entities": []}, {"text": "NMT output has more repetitions of both words and longer n-grams, and uses fewer rare words than the references.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7410634160041809}]}, {"text": "We release our generated sentence pairs to the research community with the hope that the data can inspire others to develop additional filtering methods, to experiment with richer architectures for sentence embeddings, and to further analyze the differences between neural machine translations and references.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now investigate how best to use our generated paraphrase data for training universal paraphrastic sentence embeddings.", "labels": [], "entities": []}, {"text": "We consider 10 data sources: Common Crawl (CC), Europarl (EP), and News Commentary (News) from all 3 language pairs, as well as the 10 9 French-English data (Giga).", "labels": [], "entities": [{"text": "Europarl (EP)", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.8686495423316956}]}, {"text": "We extract 150,000 reference/backtranslation pairs from each data source.", "labels": [], "entities": []}, {"text": "We use 100,000 of these to mine for training data for our sentence embedding models, and the remaining 50,000 are used as train/validation/test data for the reference classification and language models described below.", "labels": [], "entities": []}, {"text": "We evaluate the quality of a paraphrase dataset by using the experimental setting of.", "labels": [], "entities": []}, {"text": "We use the paraphrases as training data to create paraphrastic sentence embeddings, using the cosine of the embeddings as the measure of semantic relatedness, then evaluate the embeddings on the SemEval semantic textual similarity (STS) tasks from 2012 to, the SemEval 2015 Twitter task (, and the SemEval 2014 SICK Semantic Relatedness task ().", "labels": [], "entities": [{"text": "SemEval semantic textual similarity (STS) tasks", "start_pos": 195, "end_pos": 242, "type": "TASK", "confidence": 0.7795002311468124}, {"text": "SemEval 2015 Twitter task", "start_pos": 261, "end_pos": 286, "type": "TASK", "confidence": 0.7103782743215561}, {"text": "SemEval 2014 SICK Semantic Relatedness task", "start_pos": 298, "end_pos": 341, "type": "TASK", "confidence": 0.6354701916376749}]}, {"text": "Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent.", "labels": [], "entities": [{"text": "STS tasks", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8731101155281067}]}, {"text": "As our test set, we report the average Pearson's rover these 22 sentence similarity tasks.", "labels": [], "entities": [{"text": "Pearson's rover", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.8209558924039205}]}, {"text": "As development data, we use the 2016 STS tasks (, where the tuning criterion is the average Pearson's rover its 5 datasets.", "labels": [], "entities": [{"text": "2016 STS tasks", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.5390142500400543}, {"text": "Pearson's rover its 5 datasets", "start_pos": 92, "end_pos": 122, "type": "DATASET", "confidence": 0.6642728199561437}]}, {"text": "For fair comparison among different datasets and dataset filtering methods described below, we use only 24,000 training examples for nearly all experiments.", "labels": [], "entities": []}, {"text": "Different filtering methods produce different amounts of training data, and using 24,000 examples allows us to keep the amount of training data constant across filtering methods.", "labels": [], "entities": []}, {"text": "It also allows us to complete these several thousand experiments in a reasonable amount of time.", "labels": [], "entities": []}, {"text": "In Section 5.8 below, we discuss experiments that scale up to larger amounts of training data.", "labels": [], "entities": []}, {"text": "We use embeddings ( to initialize the word embedding matrix (W w ) for both models.", "labels": [], "entities": []}, {"text": "For all experiments, we fix the mini-batch size to 100, \u03bb w to 0, \u03bb c to 0, and the margin \u03b4 to 0.4.", "labels": [], "entities": [{"text": "margin \u03b4", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9676450192928314}]}, {"text": "We train AVG for 20 epochs, and the GRAN for 3, since it converges much faster.", "labels": [], "entities": [{"text": "AVG", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.6338265538215637}, {"text": "GRAN", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.974837601184845}]}, {"text": "For optimization we use Adam () with a learning rate of 0.001.", "labels": [], "entities": []}, {"text": "We compare to two data resources used in previous work to learn paraphrastic sentence embeddings.", "labels": [], "entities": []}, {"text": "The first is phrase pairs from PPDB, used by.", "labels": [], "entities": []}, {"text": "We refer to this data source as \"SimpWiki\".", "labels": [], "entities": []}, {"text": "We refer to our back-translated data as \"NMT\".", "labels": [], "entities": []}, {"text": "We first compare datasets, randomly sampling 24,000 sentence pairs from each of PPDB, SimpWiki, and each of our NMT datasets.", "labels": [], "entities": [{"text": "PPDB", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.9580961465835571}, {"text": "NMT datasets", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9242484271526337}]}, {"text": "The only hyperparameter to tune for this experiment is the stopping epoch, which we tune based on our development set.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We find that the NMT datasets are all effective as training data, outperforming PPDB in all cases when using the GRAN.", "labels": [], "entities": [{"text": "NMT datasets", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.8878354430198669}, {"text": "GRAN", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.9219853281974792}]}, {"text": "There are exceptions when using AVG, for which PPDB is quite strong.", "labels": [], "entities": []}, {"text": "This is sensible because AVG is not sensitive to word order, so the fragments in PPDB do not cause problems.", "labels": [], "entities": []}, {"text": "However, when using the GRAN, which is sensitive to word order, the NMT data is consistently better than PPDB.", "labels": [], "entities": [{"text": "GRAN", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.7841262817382812}, {"text": "NMT data", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.6529582887887955}]}, {"text": "It often exceeds the performance of training on the SimpWiki data, which consists entirely of humanwritten sentences.", "labels": [], "entities": [{"text": "SimpWiki data", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8496669828891754}]}], "tableCaptions": [{"text": " Table 2: Dataset sizes (numbers of sentence pairs)  for data domains used for training NMT systems.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores on the WMT2015 test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992173910140991}, {"text": "WMT2015 test set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9822088281313578}]}, {"text": " Table 5: Test correlations for our models when  trained on sentences with particular length ranges  (averaged over languages and data sources for the  NMT rows). Results are on STS datasets (Pear- son's r \u00d7 100).", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 178, "end_pos": 190, "type": "DATASET", "confidence": 0.7454442530870438}, {"text": "Pear- son's r", "start_pos": 192, "end_pos": 205, "type": "DATASET", "confidence": 0.8062573313713074}]}, {"text": " Table 6: Length filtering test results after tuning  length ranges on development data (averaged over  languages and data sources for the NMT rows).  Results are on STS datasets (Pearson's r \u00d7 100).", "labels": [], "entities": [{"text": "Length filtering", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7155910730361938}, {"text": "STS datasets", "start_pos": 166, "end_pos": 178, "type": "DATASET", "confidence": 0.8827511966228485}]}, {"text": " Table 7: Quality filtering test results after tun- ing quality hyperparameters on development data  (averaged over languages and data sources for the  NMT rows). Results are on STS datasets (Pear- son's r \u00d7 100).", "labels": [], "entities": [{"text": "STS datasets", "start_pos": 178, "end_pos": 190, "type": "DATASET", "confidence": 0.9013366103172302}, {"text": "Pear- son's r", "start_pos": 192, "end_pos": 205, "type": "DATASET", "confidence": 0.8557151317596435}]}, {"text": " Table 8: Results of reference/translation clas- sification (accuracy\u00d7100). The highest score in  each column is in boldface. Final two columns  show accuracies of positive (reference) and nega- tive classes, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9988998174667358}]}, {"text": " Table 9: Differences in entropy and repetition of  unigrams/trigrams in references and translations.  Negative values indicate translations have a higher  value, so references show consistently higher en- tropies and lower repetition rates.", "labels": [], "entities": [{"text": "repetition", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9866334199905396}]}, {"text": " Table 12: Diversity filtering test results after tun- ing filtering hyperparameters on development data  (averaged over languages and data sources for the  NMT rows). Results are on STS datasets (Pear- son's r \u00d7 100).", "labels": [], "entities": [{"text": "Diversity filtering", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.8826327621936798}, {"text": "STS datasets", "start_pos": 183, "end_pos": 195, "type": "DATASET", "confidence": 0.9241269528865814}, {"text": "Pear- son's r", "start_pos": 197, "end_pos": 210, "type": "DATASET", "confidence": 0.8552024126052856}]}, {"text": " Table 13: Test results with more training data.  More data helps both AVG and GRAN to match or  surpass training on SimpWiki. Both comfortably  surpass PPDB. The number of training examples  used is in parentheses.", "labels": [], "entities": [{"text": "AVG", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8724974989891052}, {"text": "GRAN", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.5762780904769897}, {"text": "PPDB", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.9000437259674072}]}]}