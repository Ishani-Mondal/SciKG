{"title": [{"text": "When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)", "labels": [], "entities": [{"text": "Neural Text Generation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6426747043927511}]}], "abstractContent": [{"text": "In neural text generation such as neural machine translation, summarization, and image captioning, beam search is widely used to improve the output text quality.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.783248245716095}, {"text": "neural machine translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7279291749000549}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9726982712745667}, {"text": "image captioning", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.7181152701377869}]}, {"text": "However, in the neural generation setting , hypotheses can finish in different steps, which makes it difficult to decide when to end beam search to ensure op-timality.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.7492627203464508}]}, {"text": "We propose a provably optimal beam search algorithm that will always return the optimal-score complete hypothesis (modulo beam size), and finish as soon as the optimality is established (finishing no later than the baseline).", "labels": [], "entities": [{"text": "beam search", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.8764580190181732}]}, {"text": "To counter neural generation's tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8081934452056885}]}, {"text": "Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6873882214228312}, {"text": "beam search", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.6913219690322876}, {"text": "BLEU score", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9816975593566895}]}], "introductionContent": [{"text": "In recent years, neural text generation using recurrent networks have witnessed rapid progress, quickly becoming the state-of-the-art paradigms in machine translation), summarization (, and image captioning ().", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7448206742604574}, {"text": "machine translation", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.756016194820404}, {"text": "summarization", "start_pos": 169, "end_pos": 182, "type": "TASK", "confidence": 0.9907646179199219}, {"text": "image captioning", "start_pos": 190, "end_pos": 206, "type": "TASK", "confidence": 0.7533970773220062}]}, {"text": "In the decoder of neural generation, beam search is widely employed to boost the output text quality, often leading to substantial improvement over greedy search (equivalent to beam size 1) in metrics such as BLEU or \u2020 Current address: Google Inc., New York, NY, USA.", "labels": [], "entities": [{"text": "neural generation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7558712065219879}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9978455305099487}]}, {"text": "ROUGE; for example, reported +2.2 BLEU (on single reference) in translation and +3.5 ROUGE-2 in summarization, both using abeam of 10.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9725260734558105}, {"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9960993528366089}, {"text": "ROUGE-2", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9926224946975708}]}, {"text": "Our own experiments on machine translation (see Sec. 5) show +4.2 BLEU (on four references) using abeam of 5.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7290306687355042}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9988415837287903}]}, {"text": "However, unlike traditional beam search in phrase-based MT or shift-reduce parsing where all hypotheses finish in the same number of steps, herein neural generation, hypotheses can finish in vastly different numbers of steps.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.7430086731910706}, {"text": "neural generation", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7721300423145294}]}, {"text": "Once you find a completed hypothesis (by generating the </s> symbol), there are still other active hypotheses in the beam that can continue to grow, which might lead to better scores.", "labels": [], "entities": []}, {"text": "Therefore when can you end the beam search?", "labels": [], "entities": [{"text": "beam search", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.8314868807792664}]}, {"text": "How (and when) can you guarantee that the returned hypothesis has the optimal score modulo beam size?", "labels": [], "entities": []}, {"text": "There have not been satisfying answers to these questions, and existing beam search strategies are heuristic methods that do not guarantee optimality.", "labels": [], "entities": [{"text": "beam search", "start_pos": 72, "end_pos": 83, "type": "TASK", "confidence": 0.7224626243114471}]}, {"text": "For example, the widely influential RNNsearch () employs a \"shrinking beam\" method: once a completed hypothesis is found, beam size shrinks by 1, and beam search would finish if beam size shrinks to 0 or if the number of steps hits a hard limit.", "labels": [], "entities": [{"text": "RNNsearch", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8007960915565491}, {"text": "beam search", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.8345544040203094}]}, {"text": "The best scoring completed hypothesis among all completed ones encountered so far is returned.", "labels": [], "entities": []}, {"text": "On the other hand, OpenNMT (, whose PyTorch version will be the baseline in our experiments, uses a very different strategy: beam search terminates whenever the highest-ranking hypothesis in the current step is completed (which is also the one returned), without considering any other completed hypotheses.", "labels": [], "entities": [{"text": "OpenNMT", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9099781513214111}, {"text": "beam search", "start_pos": 125, "end_pos": 136, "type": "TASK", "confidence": 0.8491200804710388}]}, {"text": "Neither of these two methods guarantee optimality of the returned hypothesis.", "labels": [], "entities": []}, {"text": "We therefore propose a novel and simple beam search variant that will always return the optimalscore complete hypothesis (modulo beam size), and finish as soon as the optimality is established.", "labels": [], "entities": [{"text": "beam search", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.883017510175705}]}, {"text": "However, another well-known problem remains, that the generated sentences are often too short, compared to previous paradigms such as SMT (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9342966675758362}]}, {"text": "To alleviate this problem, previous efforts introduce length normalization (as a switch in RNNsearch) or length reward ) borrowed from SMT (.", "labels": [], "entities": [{"text": "length normalization", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8172840476036072}]}, {"text": "Unfortunately these changes will invalidate the optimal property of our proposed algorithm.", "labels": [], "entities": []}, {"text": "So we introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal.", "labels": [], "entities": [{"text": "beam search", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.7417163848876953}]}, {"text": "Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 15, "end_pos": 41, "type": "TASK", "confidence": 0.6873882214228312}, {"text": "beam search", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.6913219690322876}, {"text": "BLEU score", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9816975593566895}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Tuning length reward r (with beam size  b=1..20) for optimal bounded-reward beam search.", "labels": [], "entities": []}, {"text": " Table 3: Final BLEU scores on the test set (nist  08) using best settings from the dev set (nist 06).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9274855852127075}]}]}