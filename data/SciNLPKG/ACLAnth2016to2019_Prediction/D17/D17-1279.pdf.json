{"title": [{"text": "Scientific Information Extraction with Semi-supervised Neural Tagging", "labels": [], "entities": [{"text": "Scientific Information Extraction", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6026555697123209}, {"text": "Neural Tagging", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.7429020702838898}]}], "abstractContent": [{"text": "This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material.", "labels": [], "entities": []}, {"text": "We cast the problem as sequence tagging and introduce semi-supervised methods to a neu-ral tagging model, which builds on recent advances in named entity recognition.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7092826515436172}, {"text": "named entity recognition", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.6387675702571869}]}, {"text": "Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unanno-tated articles.", "labels": [], "entities": []}, {"text": "Both inductive and trans-ductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7244118005037308}, {"text": "SemEval Task 10 ScienceIE task", "start_pos": 144, "end_pos": 174, "type": "TASK", "confidence": 0.7662387013435363}]}], "introductionContent": [{"text": "As a research community grows, more and more papers are published each year.", "labels": [], "entities": []}, {"text": "As a result there is increasing demand for improved methods for finding relevant papers and automatically understanding the key ideas in those papers.", "labels": [], "entities": []}, {"text": "However, due to the large variety of domains and extremely limited annotated resources, there has been relatively little work on scientific information extraction.", "labels": [], "entities": [{"text": "scientific information extraction", "start_pos": 129, "end_pos": 162, "type": "TASK", "confidence": 0.6296448111534119}]}, {"text": "Previous research has focused on unsupervised approaches such as bootstrapping (, where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping.", "labels": [], "entities": []}, {"text": "Very recently anew challenge on Scientific Information Extraction (ScienceIE) ( ) 1 provides a dataset consisting of 500 scientific paragraphs with keyphrase annotations for three categories: TASK, PROCESS, MATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in.", "labels": [], "entities": [{"text": "Scientific Information Extraction (ScienceIE)", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.7656944791475931}, {"text": "TASK", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9888521432876587}, {"text": "MATERIAL", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9634103178977966}]}, {"text": "This dataset enables the use of more advanced approaches such as neural network (NN) models.", "labels": [], "entities": []}, {"text": "To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (.", "labels": [], "entities": [{"text": "keyphrase extraction task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8340169787406921}, {"text": "sequence tagging", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7118504643440247}, {"text": "information extraction task", "start_pos": 122, "end_pos": 149, "type": "TASK", "confidence": 0.7846319178740183}, {"text": "Named Entity Recognition (NER)", "start_pos": 151, "end_pos": 181, "type": "TASK", "confidence": 0.756965180238088}]}, {"text": "Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers \"use\" methods.", "labels": [], "entities": []}, {"text": "In addition, keyphrases can be associated with different categories in different contexts.", "labels": [], "entities": []}, {"text": "For example, 'semantic parsing' can be labeled as a TASK in one article and as a PROCESS in another.", "labels": [], "entities": [{"text": "semantic parsing'", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.8101825912793478}, {"text": "TASK", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9818612337112427}]}, {"text": "Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard \"words\" (equations, chemical compounds, references) can provide important cues.", "labels": [], "entities": []}, {"text": "Since the scale of the data is still small for supervised training of neural systems, we introduce semi-supervised methods to the neural tagging model in order to take advantage of the large quantity of unlabeled scientific articles.", "labels": [], "entities": [{"text": "neural tagging", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.6712290942668915}]}, {"text": "This is particularly important because of the differences in keyphrases across domains.", "labels": [], "entities": []}, {"text": "Our semi-supervised learning algorithm uses a graph-based label propagation scheme to estimate the posterior probabilities of unlabeled data.", "labels": [], "entities": []}, {"text": "It additionally extends the training objective to leverage the confidence of the estimated posteriors.", "labels": [], "entities": []}, {"text": "The new training treats low confidence tokens as missing labels and computes the sentence-level score by marginalizing over them.", "labels": [], "entities": []}, {"text": "Our experiments show that our neural tagging model achieves state-of-the-art results in the SemEval Science IE task.", "labels": [], "entities": [{"text": "SemEval Science IE task", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.9175723046064377}]}, {"text": "We further show that both inductive and transductive semi-supervised strategies significantly improve the performance.", "labels": [], "entities": []}, {"text": "Finally, we provide in-depth analysis of domain differences as well as analysis of failure cases.", "labels": [], "entities": []}, {"text": "The key contributions of our work include: i) achieving state of the art in scientific information extraction SEMEVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training.", "labels": [], "entities": [{"text": "scientific information extraction SEMEVAL Task 10", "start_pos": 76, "end_pos": 125, "type": "TASK", "confidence": 0.7171769688526789}]}], "datasetContent": [{"text": "Data The SemEval ScienceIE (SE) corpus consists of 500 journal articles; one paragraph of each   Comparisons We compare our system with two template matching baselines and the state-of-theart on the SemEval Science IE task.", "labels": [], "entities": [{"text": "SemEval ScienceIE (SE) corpus", "start_pos": 9, "end_pos": 38, "type": "DATASET", "confidence": 0.7093364497025808}, {"text": "Comparisons", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9451870918273926}, {"text": "SemEval Science IE task", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.8265132009983063}]}, {"text": "The first baseline () is an unsupervised method to extract keyphrases by initially using seed patterns in a dependency tree, and then adding to seed patterns through bootstrapping.", "labels": [], "entities": []}, {"text": "The second baseline) improves the work of by adding Named Entity Features and use different set of seed patterns.", "labels": [], "entities": []}, {"text": "Implementation details All parameters are tuned on the dev set performance, the best parameters are selected and fixed for model switching and semi-supervised systems.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.903751015663147}, {"text": "model switching", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.6931146830320358}]}, {"text": "The word embedding dimension is 250; the token-level hidden dimension is 100; the character-level hidden dimension is 25; and the optimization algorithm is SGD with a learning rate of 0.05.", "labels": [], "entities": []}, {"text": "For building the graph, the best pre-trained embeddings for the supervised system (Sec. 7.2) are used in each domain.", "labels": [], "entities": []}, {"text": "Two special tokens BOS and EOS are added when pre-training, indicating the begin and end of a sentence.", "labels": [], "entities": [{"text": "BOS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9969503283500671}, {"text": "EOS", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9838098287582397}]}, {"text": "The number of the graph vertices is 2M in tranductive setting and 1.4M in inductive setting.", "labels": [], "entities": []}, {"text": "The ULM parameter \u03b7 in Eq.", "labels": [], "entities": [{"text": "ULM parameter \u03b7", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7733447551727295}, {"text": "Eq", "start_pos": 23, "end_pos": 25, "type": "DATASET", "confidence": 0.9177613854408264}]}, {"text": "4 is tuned from 0.1 to 0.9, the best \u03b7 is 0.4.", "labels": [], "entities": []}, {"text": "The best parameters of label propagation are \u00b5 = 10 \u22126 and \u03bd = 10 \u22125 . The interpolation parameter \u03b1 in Eq.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7524052560329437}, {"text": "Eq", "start_pos": 104, "end_pos": 106, "type": "DATASET", "confidence": 0.9064816832542419}]}, {"text": "3 is tuned from 0.1 to 0.9, the best \u03b1 is 0.3.", "labels": [], "entities": []}, {"text": "We do iteration of semi-supervised learning until we obtain the best result on the dev set, which is mostly achieved in the second round.", "labels": [], "entities": []}, {"text": "We use Stanford CoreNLP ( ) tokenizer to tokenize words.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 7, "end_pos": 23, "type": "DATASET", "confidence": 0.8816742300987244}]}, {"text": "The tokenizer is augmented with a few hand-designed rules to handle equations (e.g. \"fs(B,t)=Spel(t)S\" is a single token) and other non-standard word phenomena (Cu40Zn, 20MW/m2) in scientific literature.", "labels": [], "entities": []}, {"text": "We use Approximate Nearest Neighbor Searching (ANN) to calculate the k-nearest neighbors.", "labels": [], "entities": [{"text": "Approximate Nearest Neighbor Searching (ANN", "start_pos": 7, "end_pos": 50, "type": "TASK", "confidence": 0.7439131687084833}]}, {"text": "For all experiments in this paper, k = 10.", "labels": [], "entities": []}, {"text": "Setup We evaluate our system in both inductive and transductive settings.", "labels": [], "entities": []}, {"text": "The systems with a * superscript in the table are transductive.", "labels": [], "entities": []}, {"text": "The inductive setting uses 400 full articles in ScienceIE training and dev sets, while the transductive setting uses 500 full articles including the test set.", "labels": [], "entities": [{"text": "ScienceIE", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.8468162417411804}]}, {"text": "In both settings parameters are tuned over the dev set.", "labels": [], "entities": []}, {"text": "We evaluate our NN-CRF model in both supervised and semi-supervised settings.", "labels": [], "entities": []}, {"text": "We also perform ablations and try different variants to best understand our model.", "labels": [], "entities": []}, {"text": "reports the results of our neural sequence tagging model NN-CRF in both supervised and semi-supervised learning (ULM and graph-based), and compares them with the baselines and the state-of-the-art (best SemEval System (Augenstein et al., 2017)).", "labels": [], "entities": [{"text": "neural sequence tagging", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6942183971405029}]}], "tableCaptions": [{"text": " Table 1: Overall span-level F1 results for keyphrase identification (SemEval Subtask A) and classification (SemEval Subtask", "labels": [], "entities": [{"text": "F1", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.7966865301132202}, {"text": "keyphrase identification", "start_pos": 44, "end_pos": 68, "type": "TASK", "confidence": 0.7391369342803955}]}, {"text": " Table 2: Ablation study showing impact of neural network", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9543056488037109}]}, {"text": " Table 3: F1 score on the dev and test sets for using different", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9781519770622253}]}, {"text": " Table 5: F1 score results on the test set for different cat-", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9774087965488434}]}]}