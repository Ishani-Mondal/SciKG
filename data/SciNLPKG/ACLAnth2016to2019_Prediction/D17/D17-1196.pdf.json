{"title": [], "abstractContent": [{"text": "This paper presents anew approach for building Language Models using the Quantum Probability Theory, a Quantum Language Model (QLM).", "labels": [], "entities": []}, {"text": "It mainly shows that relying on this probability calculus it is possible to build stochastic models able to benefit from quantum correlations due to interference and entanglement.", "labels": [], "entities": []}, {"text": "We extensively tested our approach showing its superior performances, both in terms of model perplexity and inserting it into an automatic speech recognition evaluation setting, when compared with state-of-the-art language modelling techniques.", "labels": [], "entities": [{"text": "speech recognition evaluation", "start_pos": 139, "end_pos": 168, "type": "TASK", "confidence": 0.7659244139989217}]}], "introductionContent": [{"text": "Quantum Mechanics Theory (QMT) is one of the most successful theories in modern science.", "labels": [], "entities": [{"text": "Quantum Mechanics Theory (QMT)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8951760431130728}]}, {"text": "Despite its effectiveness in the physics realm, the attempts to apply it in other domains remain quite limited, excluding, of course, the large quantity of studies regarding Quantum Information Processing on quantum computers.", "labels": [], "entities": []}, {"text": "Only in recent years some scholars tried to embody principles derived from QMT into their specific fields, for example, by the Information Retrieval community ( and in the domain of cognitive sciences and decision making).", "labels": [], "entities": [{"text": "Information Retrieval community", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.832275927066803}, {"text": "decision making", "start_pos": 205, "end_pos": 220, "type": "TASK", "confidence": 0.8249547481536865}]}, {"text": "In the machine learning field ( have used unitary evolution matrices for building deep neural networks obtaining interesting results, but we have to observe that their works do not adhere to QMT and use unitary evolution operators in away not allowed by QMT.", "labels": [], "entities": []}, {"text": "In recent years, also the Natural Language Processing (NLP) community started to look at QMT with interest and some studies using it have already been presented (.", "labels": [], "entities": []}, {"text": "Language models (LM) are basic tools in NLP used in various applications, such as Automatic Speech Recognition (ASR), machine translation, part-of-speech tagging, etc., and were traditionally modeled by using N-grams and various smoothing techniques.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.7942052036523819}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.812934935092926}, {"text": "part-of-speech tagging", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.7439325749874115}]}, {"text": "Among the dozen of tools for computing N-gram LM, we will refer to CMU-SLM (with Good-Turing smoothing)) and IRSTLM (with Linear Witten-Bell smoothing); the latter is the tool used in), one of the most powerful and used open-source ASR package that we will use for some of the experiments presented in the following sections.", "labels": [], "entities": [{"text": "IRSTLM", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.8375459909439087}]}, {"text": "In recent years new techniques from the Neural Networks (NN) domain have been introduced in order to enhance the performances of such models.", "labels": [], "entities": []}, {"text": "Elman recurrent NN, as used in the RNNLM tool), or Long Short-Term Memory NN, as in the tool LSTMLM, produce state-of-the-art performances for current language models.", "labels": [], "entities": []}, {"text": "This paper presents a different approach for building LM based on quantum probability theory.", "labels": [], "entities": [{"text": "LM", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9445309638977051}]}, {"text": "Actually, we present a QLM applicable only to problems defined on a small set of different tokens.", "labels": [], "entities": []}, {"text": "This is a \"proof-of-concept\" study and our main aim is to show the potentialities of such approach rather than building a complete application for solving this problem for any setting.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: we provide background on Quantum Probability Theory in Section 2 followed by the description of our proposed Quantum Language Model in Section 3.", "labels": [], "entities": [{"text": "Quantum Probability Theory", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6034601628780365}]}, {"text": "We then discuss some numerical issues mainly related to the optimisation procedure in Section 4, and in Section 5 we present the experiments we did to validate our approach.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss our results and draw some provisional conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the proposed model by setting up two different evaluations: the first is an intrinsic evaluation of LM performances in terms of global perplexity on the TIMIT testset; the second is an extrinsic evaluation in which we replace the LM tools provided with the Kaldi ASR toolkit () with our model in order to check the final system performances in a phone-recognition task and comparing them with the other state-ofthe-art LM techniques briefly introduced in Section 1.", "labels": [], "entities": [{"text": "TIMIT testset", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.9498980343341827}]}, {"text": "The first experiment consisted in an evaluation of models perplexity (PPL) on the TIMIT testset.", "labels": [], "entities": [{"text": "TIMIT testset", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.939459353685379}]}, {"text": "We compared the QLM model with two N-gram implementations, namely CMU-SLM) and IRSTLM, and two recurrent NN models able to produce state-of-the-art results in language modelling, the RNNLM () and the LSTMLM ( packages.", "labels": [], "entities": []}, {"text": "shows the results of the intrinsic evaluation.", "labels": [], "entities": []}, {"text": "With regard to RNNLM and LSTMLM results, only the best hyper-parameters combination after a lot of experiments, optimizing them on the validation set, has been inserted into the With regard to QLM, all the presented experiments are based on artificial word vectors produced randomly using values from the set {\u22121, 0, 1} instead of real word embeddings.", "labels": [], "entities": []}, {"text": "Every word vector is different from the others and we decided not to use real embeddings in order to test the core QMT method without adding the contex- tual information, contained in word embeddings, that could have helped our approach to obtain better performances, at least in principle.", "labels": [], "entities": []}, {"text": "The \"TIMIT recipe\" contained in the Kaldi distribution 2 reproduces exactly the same evaluation settings described in (Lopes and Perdigao, 2011) fora phone recognition task based on this corpus.", "labels": [], "entities": [{"text": "TIMIT", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9074428677558899}, {"text": "Kaldi distribution 2", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.7847366333007812}, {"text": "phone recognition task", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.8053946495056152}]}, {"text": "Moreover, Kaldi provides some n-best rescoring scripts that apply RNNLM hypothesis rescoring and interpolate the results with the standard N-gram model results used in the evaluation.", "labels": [], "entities": [{"text": "RNNLM hypothesis rescoring", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.5703192253907522}]}, {"text": "We slightly modified these scripts to work with LSTMLM and QLM in order to test different models using the same setting.", "labels": [], "entities": []}, {"text": "This allowed us to replace the LM used in Kaldi and experiment with all the systems evaluated in the previous section.", "labels": [], "entities": []}, {"text": "outlines the results we obtained replacing the LM technique into Kaldi ASR package w.r.t. the different ASR systems that the TIMIT recipe implements.", "labels": [], "entities": [{"text": "ASR", "start_pos": 104, "end_pos": 107, "type": "TASK", "confidence": 0.9268986582756042}]}, {"text": "These systems are built on top of MFCC, LDA, MLLT, fMLLR with CMN 3 features (see) for all acronyms references and a complete feature or recipe descriptions).", "labels": [], "entities": [{"text": "fMLLR", "start_pos": 51, "end_pos": 56, "type": "DATASET", "confidence": 0.7295262217521667}]}, {"text": "For this extrinsic evaluation we used the best models we obtained in the previous experiments interpolating their log-probability results for each utterance with the original bigram (or trigram) log-probability using a linear model with a ratio 0.25/0.75 between the original N-gram LM and the tested one as suggested in the standard Kaldi rescoring script.", "labels": [], "entities": []}, {"text": "For this test we rescored the 10,000-best hypothesis.", "labels": [], "entities": []}, {"text": "We have to say that in this experiment we were not trying to build the best possible phone recogniser, but simply to compare the relative performances of the analysed LM techniques showing the effectiveness of QLM when used in areal application.", "labels": [], "entities": [{"text": "phone recogniser", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.6941909343004227}]}, {"text": "Thus absolute Phone Error Rate is not so important here and it can be certainly possible to devise recognisers with better performances by applying more sophisticated techniques.", "labels": [], "entities": [{"text": "absolute Phone Error Rate", "start_pos": 5, "end_pos": 30, "type": "METRIC", "confidence": 0.6865350604057312}]}, {"text": "For example () presented a method for lattice rescoring in Kaldi that exhibits better performances than the n-best rescoring we used to interpolate between n-grams and the tested models, but modifying it in order to test LSTMLM and QLM presented a lot of problems and thus we decided to use the simpler n-best approach.", "labels": [], "entities": []}, {"text": "For completeness, the last column of outlines the results obtained using this lattice rescoring method with RNNLM as described in).", "labels": [], "entities": [{"text": "RNNLM", "start_pos": 108, "end_pos": 113, "type": "DATASET", "confidence": 0.8497220277786255}]}], "tableCaptions": [{"text": " Table 1: Perplexity (PPL) of the tested language- modelling techniques on the TIMIT testset. All  the QLM results in bold face are better than the  other systems we tested.", "labels": [], "entities": [{"text": "TIMIT testset", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9416311681270599}]}, {"text": " Table 2: Phone-recognition performances, in terms of Phone Error Rate, for the TIMIT dataset and  the different Kaldi ASR models, rescoring the 10,000-best solutions with the tested LM techniques in- terpolated with the IRSTLM bigrams and trigrams LM (the standard LM used in Kaldi). In boldface  the best performing system and in italics the second best. Kaldi ASR systems descriptions: tri1 = a  triphone model using 13 dim. MFCC+\u2206+\u2206\u2206; tri2 = tri1+LDA+MLLT; tri3 = tri2+SAT; SGMM2 =  Semi-supervised Gaussian Mixture Model", "labels": [], "entities": [{"text": "Phone Error Rate", "start_pos": 54, "end_pos": 70, "type": "METRIC", "confidence": 0.6725663145383199}, {"text": "TIMIT dataset", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.951639324426651}, {"text": "MLLT", "start_pos": 455, "end_pos": 459, "type": "METRIC", "confidence": 0.7314527630805969}]}]}