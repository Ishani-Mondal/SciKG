{"title": [{"text": "Incremental Skip-gram Model with Negative Sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives.", "labels": [], "entities": []}, {"text": "Existing methods of neural word embed-dings, including SGNS, are multi-pass algorithms and thus cannot perform incre-mental model update.", "labels": [], "entities": []}, {"text": "To address this problem, we present a simple incremen-tal extension of SGNS and provide a thorough theoretical analysis to demonstrate its validity.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 71, "end_pos": 75, "type": "TASK", "confidence": 0.8259591460227966}]}, {"text": "Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "labels": [], "entities": []}], "introductionContent": [{"text": "Existing methods of neural word embeddings are typically designed to go through the entire training data multiple times.", "labels": [], "entities": []}, {"text": "For example, negative sampling () needs to precompute the noise distribution from the entire training data before performing Stochastic Gradient Descent (SGD).", "labels": [], "entities": [{"text": "Stochastic Gradient Descent (SGD)", "start_pos": 125, "end_pos": 158, "type": "TASK", "confidence": 0.811140110095342}]}, {"text": "It thus needs to go through the training data at least twice.", "labels": [], "entities": []}, {"text": "Similarly, hierarchical soft-max () has to determine the tree structure and GloVe () has to count co-occurrence frequencies before performing SGD.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.827846884727478}, {"text": "SGD", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9154170155525208}]}, {"text": "The fact that those existing methods are multipass algorithms means that they cannot perform incremental model update when additional training data is provided.", "labels": [], "entities": []}, {"text": "Instead, they have to re-train the model on the old and new training data from scratch.", "labels": [], "entities": []}, {"text": "However, the re-training is obviously inefficient since it has to process the entire training data received thus far whenever new training data is provided.", "labels": [], "entities": []}, {"text": "This is especially problematic when the amount of the new training data is relatively smaller than the old one.", "labels": [], "entities": []}, {"text": "One such situation is that the embedding model is updated on a small amount of training data that includes newly emerged words for instantly adding them to the vocabulary set.", "labels": [], "entities": []}, {"text": "Another situation is that the word embeddings are learned from ever-evolving data such as news articles and microblogs) and the embedding model is periodically updated on newly generated data (e.g., once in a week or month).", "labels": [], "entities": []}, {"text": "This paper investigates an incremental training method of word embeddings with a focus on the skip-gram model with negative sampling (SGNS) () for its popularity.", "labels": [], "entities": []}, {"text": "We present a simple incremental extension of SGNS, referred to as incremental SGNS, and provide a thorough theoretical analysis to demonstrate its validity.", "labels": [], "entities": []}, {"text": "Our analysis reveals that, under a mild assumption, the optimal solution of incremental SGNS agrees with the original SGNS when the training data size is infinitely large.", "labels": [], "entities": []}, {"text": "See Section 4 for the formal and strict statement.", "labels": [], "entities": []}, {"text": "Additionally, we present techniques for the efficient implementation of incremental SGNS.", "labels": [], "entities": []}, {"text": "Three experiments were conducted to assess the correctness of the theoretical analysis as well as the practical usefulness of incremental SGNS.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 138, "end_pos": 142, "type": "TASK", "confidence": 0.9500694870948792}]}, {"text": "The first experiment empirically investigates the validity of the theoretical analysis result.", "labels": [], "entities": []}, {"text": "The second experiment compares the word embeddings learned by incremental SGNS and the original SGNS across five benchmark datasets, and demonstrates that those word embeddings are of comparable quality.", "labels": [], "entities": []}, {"text": "The last experiment explores the training time of incremental SGNS, demonstrating that it is able to save much training time by avoiding expensive re-training when additional training data is provided.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.9048795700073242}]}], "datasetContent": [{"text": "Three experiments were conducted to investigate the correctness of the theoretical analysis (Section 5.1) and the practical usefulness of incremental SGNS (Sections 5.2 and 5.3).", "labels": [], "entities": [{"text": "SGNS", "start_pos": 150, "end_pos": 154, "type": "TASK", "confidence": 0.9439350366592407}]}, {"text": "Details of the experimental settings that do not fit into the paper are presented in Appendix E.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example noise distribution q(w) for the  vocabulary set W = {a, b, c} (left) and the corre- sponding unigram table T of size 10 (right).", "labels": [], "entities": []}]}