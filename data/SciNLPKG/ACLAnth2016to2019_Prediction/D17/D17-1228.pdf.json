{"title": [{"text": "Steering Output Style and Topic in Neural Response Generation", "labels": [], "entities": [{"text": "Neural Response Generation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8531479636828104}]}], "abstractContent": [{"text": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation.", "labels": [], "entities": [{"text": "neural encoder-decoder based language generation", "start_pos": 103, "end_pos": 151, "type": "TASK", "confidence": 0.6434221267700195}]}, {"text": "This capability is desirable in a variety of applications , including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge.", "labels": [], "entities": []}, {"text": "We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling.", "labels": [], "entities": []}, {"text": "We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction.", "labels": [], "entities": []}, {"text": "Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (, image captioning (), summarization (, and conversation generation ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.6595710019270579}, {"text": "image captioning", "start_pos": 137, "end_pos": 153, "type": "TASK", "confidence": 0.7063425332307816}, {"text": "summarization", "start_pos": 158, "end_pos": 171, "type": "TASK", "confidence": 0.9892589449882507}, {"text": "conversation generation", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.782364696264267}]}, {"text": "These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions.", "labels": [], "entities": []}, {"text": "In order to create more human-like chat context Where are you?", "labels": [], "entities": []}, {"text": "Twitter bot i'm on my way to london now.", "labels": [], "entities": []}, {"text": "Star Wars bot we're heading to the dark side of jabba's palace.", "labels": [], "entities": []}, {"text": "Hillary bot i'm herein philadelphia, pennsylvania Trump bot i went to the white house.", "labels": [], "entities": []}, {"text": "Kennedy bot i am on the question of the united states.", "labels": [], "entities": [{"text": "Kennedy", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9307608604431152}]}], "datasetContent": [{"text": "In this section, we propose three simple yet effective methods of influencing the language style of the output in the neural encoder-decoder framework.", "labels": [], "entities": []}, {"text": "Our language style restricting setup assumes that there is a large open-domain parallel corpus that provides training for context-response relevance, and a smaller monologue speaker corpus that reflects the language characteristics of the target speaker.", "labels": [], "entities": [{"text": "language style restricting", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6953464945157369}, {"text": "context-response relevance", "start_pos": 122, "end_pos": 148, "type": "TASK", "confidence": 0.687271386384964}]}, {"text": "We will refer to this smaller set as a 'scenting' dataset, since it hints at, or insinuates, the characteristics of the target speaker.", "labels": [], "entities": []}, {"text": "We use the Comprehensive Questions and Answers dataset 2 to train and validate the performances of different decoding setups with ranking experiments described in section 7.3.", "labels": [], "entities": [{"text": "Comprehensive Questions and Answers dataset 2", "start_pos": 11, "end_pos": 56, "type": "DATASET", "confidence": 0.7272951106230418}]}, {"text": "This dataset contains 4.4 million Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8939570188522339}]}, {"text": "Answers questions and the user-selected best answers.", "labels": [], "entities": []}, {"text": "Unlike the conversational datasets, such as the Twitter dataset described below, it contains more relevant and specific responses for each question, which leads to less ambiguity in ranking.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8140820264816284}]}, {"text": "We trained our base encoder-decoder models on the Twitter Conversation Triple Dataset described in (, which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month period from June through August 2012.", "labels": [], "entities": [{"text": "Twitter Conversation Triple Dataset", "start_pos": 50, "end_pos": 85, "type": "DATASET", "confidence": 0.6256868615746498}]}, {"text": "For the purposes of our experiments, we split the triples into context-message and message-response pairs yielding 46M source-target pairs.", "labels": [], "entities": []}, {"text": "For tuning and evaluation, we used the development dataset of size 200K conversation pairs and the test dataset of 5K examples.", "labels": [], "entities": []}, {"text": "The corpus is preprocessed using a Twitter specific tokenizer (O'.", "labels": [], "entities": []}, {"text": "The vocabulary size is limited to 50,000 excluding the special boundary symbol and the unknown word tag.", "labels": [], "entities": []}, {"text": "A variety of persona characters have been trained and tested, including Hillary Clinton, Donald Trump, John F. Kennedy, Richard Nixon, singer-songwriters, stand-up comedians, and a generic Star Wars character.", "labels": [], "entities": []}, {"text": "In experiments, we evaluated on a diverse set of representative target speakers: JFK.", "labels": [], "entities": [{"text": "JFK", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9465566873550415}]}, {"text": "We mainly tested our models on John F. Kennedy's speeches collected from American Presidency Project 3 , which contains 6474 training and 719 validation sentences.", "labels": [], "entities": [{"text": "John F. Kennedy's speeches collected from American Presidency Project 3", "start_pos": 31, "end_pos": 102, "type": "DATASET", "confidence": 0.8417720794677734}]}, {"text": "Movie subtitles of three Star Wars movies are also tested 4 . They are extracted from Cornell Movie-Dialogs Corpus (DanescuNiculescu-Mizil and Lee, 2011), and have 495 training and 54 validation sentences.", "labels": [], "entities": [{"text": "Cornell Movie-Dialogs Corpus", "start_pos": 86, "end_pos": 114, "type": "DATASET", "confidence": 0.9546359181404114}]}, {"text": "We also evaluated our approach on a lyric corpus from a collective of singers: Coldplay, Linkin Park, and Green Day.", "labels": [], "entities": []}, {"text": "The lyric dataset is collected from mldb.org and has 9182 training and 1020 validation lines.", "labels": [], "entities": []}, {"text": "We designed testing questionnaires with 64 chat contexts spanning a range of topics in politic, science, and technology: the sort of questions we might ask in an entertaining political debate.", "labels": [], "entities": []}, {"text": "To test the model's ability to control output topic in section 7.4.3, we also created one hint per question.", "labels": [], "entities": []}, {"text": "Owing to the low consistency between automatic metrics and human perception on conversational tasks () and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 248, "end_pos": 276, "type": "DATASET", "confidence": 0.9022971789042155}]}, {"text": "Workers were selected based on their AMT prior approval rate (>95%).", "labels": [], "entities": [{"text": "AMT prior approval rate", "start_pos": 37, "end_pos": 60, "type": "METRIC", "confidence": 0.8393604010343552}]}, {"text": "Each questionnaire was presented to 3 different workers.", "labels": [], "entities": []}, {"text": "We evaluated our proposed models on the 64 debate chat contexts.", "labels": [], "entities": []}, {"text": "Each of the evaluated methods generated 3 samples for every chat context.", "labels": [], "entities": []}, {"text": "To ensure calibrated ratings between systems, we show the human judges all system outputs (randomly ordered) for each particular test case at the same time.", "labels": [], "entities": []}, {"text": "For each chat context, we conducted three kinds of assessments: Quality Assessment Workers were provided with the following guidelines: \"Given the chat: Results of quality assessments with 5-scale mean opinion scores (MOS) and JFK style assessments with binary ratings.", "labels": [], "entities": [{"text": "5-scale mean opinion scores (MOS)", "start_pos": 189, "end_pos": 222, "type": "METRIC", "confidence": 0.7125999672072274}]}, {"text": "Style results are statistically significant compared to the selectivesampling by paired t-tests (p < 0.5%).", "labels": [], "entities": [{"text": "selectivesampling", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.9176381826400757}]}, {"text": "context, a chat-bot needs to continue the conversation.", "labels": [], "entities": []}, {"text": "Rate the potential answers based on your own preference on a scale of 1 to 5 (the highest):\" \u2022 5-Excellent: \"Very appropriate response, and coherent with the chat context.\"", "labels": [], "entities": []}, {"text": "In this test, the outputs of all 10 systems evaluated are then provided to worker together fora total of 30 responses.", "labels": [], "entities": []}, {"text": "In total, we gathered 64 \u00b7 30 \u00b7 3 = 5760 ratings for quality assessments, and 47 different workers participated.", "labels": [], "entities": []}, {"text": "We provided following instructions: \"Which candidate responses are likely to have come from or are related to [Persona Name]?\".", "labels": [], "entities": []}, {"text": "Checkboxes were provided for the responses from style-influenced systems and from selective-sampling as a baseline.", "labels": [], "entities": []}, {"text": "The instruction was: \"Which candidate answers to the chat context above are similar or related to the following answer: '[a hint topic provided by us]'?\".", "labels": [], "entities": []}, {"text": "This was also a checkbox questionnaire.", "labels": [], "entities": []}, {"text": "Candidates are from both style-and topic-influenced systems (fine-tuned cg-topic), and from selective-sampling as a baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of quality assessments with 5- scale mean opinion scores (MOS) and JFK style  assessments with binary ratings. Style results are  statistically significant compared to the selective- sampling by paired t-tests (p < 0.5%).", "labels": [], "entities": [{"text": "5- scale mean opinion scores (MOS)", "start_pos": 46, "end_pos": 80, "type": "METRIC", "confidence": 0.775108277797699}]}]}