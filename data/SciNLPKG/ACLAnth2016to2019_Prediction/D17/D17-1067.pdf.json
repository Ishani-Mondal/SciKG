{"title": [{"text": "Filling the Blanks (hint: plural noun) for Mad Libs R Humor", "labels": [], "entities": [{"text": "R Humor", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.4539562463760376}]}], "abstractContent": [{"text": "Computerized generation of humor is a notoriously difficult AI problem.", "labels": [], "entities": [{"text": "Computerized generation of humor", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.830856941640377}]}, {"text": "We develop an algorithm called Libitum that helps humans generate humor in a Mad Lib R , which is a popular fill-in-the-blank game.", "labels": [], "entities": []}, {"text": "The algorithm is based on a machine learned classifier that determines whether a potential fill-in word is funny in the context of the Mad Lib story.", "labels": [], "entities": [{"text": "Mad Lib story", "start_pos": 135, "end_pos": 148, "type": "DATASET", "confidence": 0.8009673555692037}]}, {"text": "We use Amazon Mechanical Turk to create ground truth data and to judge humor for our classifier to mimic, and we make this data freely available.", "labels": [], "entities": []}, {"text": "Our testing shows that Libitum successfully aids humans in filling in Mad Libs that are usually judged funnier than those filled in by humans with no computerized help.", "labels": [], "entities": [{"text": "filling in Mad Libs", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7930667996406555}]}, {"text": "We goon to analyze why some words are better than others at making a Mad Lib funny.", "labels": [], "entities": []}], "introductionContent": [{"text": "As technologists attempt to build more natural human-computer interfaces, the inclusion of computer-generated humor becomes more important for creating personable interactions.", "labels": [], "entities": []}, {"text": "However, computational humor remains a long-standing challenge in AI.", "labels": [], "entities": [{"text": "computational humor", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7675438225269318}]}, {"text": "Despite decades devoted to theories and algorithms for humor, the best computerized humor is still mediocre compared to humans.", "labels": [], "entities": []}, {"text": "Humor requires creativity, sophistication of language, world knowledge, empathy and cognitive mechanisms, which are extremely difficult to model theoretically.", "labels": [], "entities": [{"text": "Humor", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9517691731452942}]}, {"text": "A more modest goal for computational humor is to build machines that help humans create humor rather than replace them.", "labels": [], "entities": []}, {"text": "We develop and test an algorithm, called Li-: Example of a Mad Lib sentence.", "labels": [], "entities": []}, {"text": "The original words for the blanks, in order, were \"apartment\", \"impatient\" and \"gallery\".", "labels": [], "entities": []}, {"text": "bitum , fora computer-aided approach to humor generation.", "labels": [], "entities": [{"text": "humor generation", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.8411051630973816}]}, {"text": "Its aim is to help a human player fill in the blanks of a Mad Lib R story to make it funny.", "labels": [], "entities": [{"text": "fill in the blanks of a Mad Lib R story", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.7201183915138245}]}, {"text": "The algorithm generates candidate words, and its core component is a machine-trained classifier that can assess whether a potential fill-inthe-blank word is funny, based on several features, including the blank's surrounding context.", "labels": [], "entities": []}, {"text": "We trained the classifier on Mad Lib stories that were filled in and judged by humans.", "labels": [], "entities": [{"text": "Mad Lib stories", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.7918966809908549}]}, {"text": "We note that in our work, we give players, both human and computer, access to the full Mad Lib story, including the sentences surrounding the blanks.", "labels": [], "entities": [{"text": "Mad Lib story", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.7565845847129822}]}, {"text": "In regular Mad Libs, players do not seethe surrounding sentences.", "labels": [], "entities": []}, {"text": "shows a sentence from atypical Mad Lib, completed by a human player.", "labels": [], "entities": []}, {"text": "The work presented here makes three contributions.", "labels": [], "entities": []}, {"text": "The first is the creation of a challenging benchmark for humor generation, a vital aspect of human communication which has received relatively little attention in the NLP community.", "labels": [], "entities": [{"text": "humor generation", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.8327287137508392}]}, {"text": "This benchmark, based on Mad Libs, (i) is challenging, but doable by both humans and machines, (ii) provides quantitative results so that progress can be measured, and (iii) cannot be gamed by trivial strategies, such as filling in random words.", "labels": [], "entities": [{"text": "filling in random words", "start_pos": 221, "end_pos": 244, "type": "TASK", "confidence": 0.8702395856380463}]}, {"text": "The benchmark dataset is annotated and judged us-ing Amazon Mechanical Turk, with several steps taken to reduce effects of human variation in humor taste.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9289520581563314}]}, {"text": "Our second contribution is that we create and demonstrate a computer-aided humor algorithm that, inmost cases, allows humans to generate funnier Mad Lib stories than they canon their own without computer assistance.", "labels": [], "entities": []}, {"text": "The third contribution is an analysis of our test data and algorithm that helps to quantitatively explain what makes our results humorous.", "labels": [], "entities": []}, {"text": "Our work goes beyond the modeling and generation of language to simply convey information.", "labels": [], "entities": []}, {"text": "Instead, we are trying to create a pleasurable feeling using humor.", "labels": [], "entities": []}, {"text": "This is analogous to the Story Cloze Test, where the task is to choose a satisfying ending to a story.", "labels": [], "entities": []}, {"text": "We note the previous work called \"Visual Madlibs\" (.", "labels": [], "entities": []}, {"text": "Although its title implies similarity to our work, it is about an image data set augmented with fill-in-the-blank questions, such as \"This place is a park.\"", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we define three approaches for generating humor in Mad Libs, and then we compare and analyze the results we obtain from them.", "labels": [], "entities": []}, {"text": "These approaches are: 1.", "labels": [], "entities": []}, {"text": "FreeText: players fill in the blanks without any restrictions.", "labels": [], "entities": [{"text": "FreeText", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9625537991523743}]}, {"text": "2. LMC (Language Model + Multiple Choice): for each blank, players choose a word from up to 20 candidate words generated by the language model and sorted by their joint probability score 4 . 3. Libitum: Similar to the LMC method, except here we rank all the words up to the top 500 words generated by the language model using our classifier, keeping up to the top 20 \"humorous\" candidates 4 . These methods are designed to study the outcome of humor generated by humans only vs. humans with machine help.", "labels": [], "entities": []}, {"text": "The purpose of the LMC model is to study whether the language model alone is a good aid to humans when creating humor, and the benefit of adding machine learning.", "labels": [], "entities": []}, {"text": "For evaluation, we used our ten test Fun Libs, and for each of our three approaches, each of the In rare cases, fora blank, the language model was only able to generate less than 20 candidate words that pass the candidate refinement step.", "labels": [], "entities": []}, {"text": "For such cases, the LMC candidate list was expanded to at least 10 words by adding words randomly from all possible words in WordNet that fit the blank's hint type, if necessary.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9655739665031433}]}, {"text": "For Libitum, WordNet was used to randomly add words fitting the blank's hint type to make a list of 50 words that pass the candidate refinement phase.", "labels": [], "entities": [{"text": "Libitum", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9090139269828796}]}, {"text": "These words were sorted using Libitum and used to ensure the final candidate list had at least 10 words.", "labels": [], "entities": [{"text": "Libitum", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.5859050750732422}]}, {"text": "ten stories was completed by three players.", "labels": [], "entities": []}, {"text": "shows the mean grade for these 30 stories.", "labels": [], "entities": [{"text": "mean grade", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9568401277065277}]}, {"text": "In the figure, the titles are sorted left to right based on the maximum mean story grade among the titles in the Libitum approach.", "labels": [], "entities": [{"text": "Libitum", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9285842180252075}]}, {"text": "The stories per title are also sorted left to right in descending mean grade.", "labels": [], "entities": [{"text": "descending mean grade", "start_pos": 55, "end_pos": 76, "type": "METRIC", "confidence": 0.7483061949412028}]}, {"text": "In only one story (ID = 21), the Libitum model receives a lower mean grade than the LMC model, suggesting that adding the machine learning to the language model helps generate significantly more humor than the language model alone.", "labels": [], "entities": [{"text": "ID", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.934276819229126}]}, {"text": "The FreeText model is fairly consistent in generating more humor than the LMC model, which beats the FreeText model in only 7 out of 30 instances.", "labels": [], "entities": []}, {"text": "However, the Libitum approach frequently outperforms the FreeText (human only) approach, and it achieves significant gain in generating humor in the stories with the titles \"Valentine's Day\" and \"Cats\".", "labels": [], "entities": [{"text": "Valentine's Day\" and \"Cats\"", "start_pos": 174, "end_pos": 201, "type": "TASK", "confidence": 0.6449693813920021}]}, {"text": "Interestingly, the two stories that received the highest mean grade (\"Batman\" and \"Ducks\") are from the FreeText format.", "labels": [], "entities": [{"text": "mean grade", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9819700121879578}, {"text": "FreeText format", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.954221785068512}]}, {"text": "This suggests that given more freedom, humans are capable of generating a stronger degree of humor than when they are restricted to a limited number of choices.", "labels": [], "entities": []}, {"text": "Excerpts from the best \"Batman\" story are shown in.", "labels": [], "entities": []}, {"text": "Here, the strategy employed by the FreeText player is to consistently portray Batman as an obese person obsessed with eating, exploiting the superiority theory of humor).", "labels": [], "entities": []}, {"text": "This is remarkable, because it shows how skilled humans are at finding and relating multiple coherent concepts, achieving meaningful and steady humor via continuity, something which is very difficult for machines to do.", "labels": [], "entities": []}, {"text": "Much of the humor generated by the Libitum approach here is via incongruity -the filled-in words are quite humorous mainly because they fit their contexts but do not match the expectation of the reader (e.g., Batman is an inefficient superhero).", "labels": [], "entities": []}, {"text": "At times, some of the filled-in words in the Libitum approach coherently generate humor, for instance, in the last sentence when Batman is described as wearing a veil to fight acne.", "labels": [], "entities": []}, {"text": "Since each human has a bias towards his/her own understanding of humor, we also studied how the stories appealed to the judges individually by counting the total number of judges for each grade in the three approaches.", "labels": [], "entities": []}, {"text": "shows the results, where the difference between the mean fun-   niness grades for each pair of approaches is statistically significant with p < 0.005 when a 2-sample t-test was performed.", "labels": [], "entities": [{"text": "fun-   niness grades", "start_pos": 57, "end_pos": 77, "type": "METRIC", "confidence": 0.7459074258804321}]}, {"text": "Using Krippendorff's Alpha, we also found positive agreements between judges for these and training set ratings.", "labels": [], "entities": []}, {"text": "As expected, the LMC model is the poorest in terms of generating humor.", "labels": [], "entities": []}, {"text": "Further, for each non-zero grade, the Libitum model received more votes than the FreeText model.", "labels": [], "entities": []}, {"text": "A possible reason is that the judges and players have different perceptions of humor.", "labels": [], "entities": []}, {"text": "In the FreeText approach, the common technique employed by players was to use words that are coherent, belonging to a specific topic or domain, and to guide the story towards one conclusion (e.g., the Batman story in).", "labels": [], "entities": []}, {"text": "When this technique worked well, the humor generated was very strong.", "labels": [], "entities": []}, {"text": "However, the players have their own biases towards what is humorous, and having more freedom in the FreeText format allowed them to explore their own concept of humor, which could be too narrow to appeal to a broader audience.", "labels": [], "entities": []}, {"text": "The Libitum approach, by restricting the players, prevented them from inserting words that they themselves thought were funny, but were not actually funny to people in general.", "labels": [], "entities": []}, {"text": "shows passages from stories containing filled-in words that received 9 funniness votes (the maximum possible) from the judges.", "labels": [], "entities": []}, {"text": "The story ID and the algorithm used are also provided.", "labels": [], "entities": []}, {"text": "Here, in the \"Ducks\" story, the FreeText player chose to generate humor by developing a steady mockery by satirizing ducks as politicians, whereas the LMC player chose the incongruity approach.", "labels": [], "entities": []}, {"text": "The \"Beauty Contest\" story shows the outstanding skills of humans in generating humor when two blanks are directly connected to each other (i.e., \"brawler\" and \"deadly\").", "labels": [], "entities": []}, {"text": "For the same segment, Libitum was also able to aid the players in generating a very funny word, however, the coherence between the blanks does not appear strong.", "labels": [], "entities": []}, {"text": "With the computer aided approaches, it is quite difficult to suggest candidates for pairs of (or more) blanks such that the choices are coherent.", "labels": [], "entities": []}, {"text": "shows correlations between different ratings by judges (coherence, topic change and incongruity) and the stories' funniness grades.", "labels": [], "entities": []}, {"text": "Incongruity had the strongest positive, and statistically significant (with p < 0.001), correlation with the graded humor of a story.", "labels": [], "entities": [{"text": "Incongruity", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9805740714073181}]}, {"text": "Coherence also appears very important for generating humor in all the datasets except LMC, where it is difficult to generate coherent words using a language model only.", "labels": [], "entities": []}, {"text": "Libitum is likely aiding players in achieving funniness by providing coherent words.", "labels": [], "entities": []}, {"text": "In LMC, most of the words generating humor are probably random, incongruous words since the change of topic strongly positively correlates with increasing the humor but coherence does not.", "labels": [], "entities": []}, {"text": "Lastly, the player's self grade of humor has no significant relationship with the judges' grade of humor, suggesting that each person has his/her own biases about what is humorous.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean hint types per story in the two  datasets.  *  The mean number of blanks in the Mad  Libs dataset was computed based on 14 hint types.", "labels": [], "entities": [{"text": "Mad  Libs dataset", "start_pos": 95, "end_pos": 112, "type": "DATASET", "confidence": 0.8479147354761759}]}, {"text": " Table 2: Filled-in word classification results.", "labels": [], "entities": [{"text": "Filled-in word classification", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6559780736764272}]}, {"text": " Table 4: Funniness grades for the 30 stories for the  three humor generation formats.", "labels": [], "entities": [{"text": "Funniness", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9223511815071106}]}, {"text": " Table 6: Correlation of different assessments of  stories with their funniness grade. The boldfaced  and underlined correlations are statistically signif- icant, respectively, with p < 0.001 and p < 0.05.", "labels": [], "entities": []}]}