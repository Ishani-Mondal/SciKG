{"title": [{"text": "Integrating Order Information and Event Relation for Script Event Prediction *", "labels": [], "entities": [{"text": "Event Relation", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7116471529006958}, {"text": "Script Event Prediction", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.6362209419409434}]}], "abstractContent": [{"text": "There has been a recent line of work automatically learning scripts from unstruc-tured texts, by modeling narrative event chains.", "labels": [], "entities": []}, {"text": "While the dominant approach group events using event pair relations, L-STMs have been used to encode full chains of narrative events.", "labels": [], "entities": []}, {"text": "The latter has the advantage of learning long-range temporal orders 1 , yet the former is more adap-tive to partial orders.", "labels": [], "entities": []}, {"text": "We propose a neu-ral model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling.", "labels": [], "entities": [{"text": "event pair modelling", "start_pos": 118, "end_pos": 138, "type": "TASK", "confidence": 0.6257530947526296}]}, {"text": "A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event.", "labels": [], "entities": []}, {"text": "Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far.", "labels": [], "entities": []}], "introductionContent": [{"text": "Frequently recurring sequences of events in prototypical scenarios, such as visiting a restaurant and driving to work, area useful source of world knowledge.", "labels": [], "entities": []}, {"text": "Two examples are shown in, which are different variations of the \"restaurant visiting\" scenario, where events are partially ordered and can be flexible.", "labels": [], "entities": []}, {"text": "Such knowledge is useful for natural language understanding because texts typically do not include event details when mentioning a scenario.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.6475765804449717}]}, {"text": "For example, the reader is expected to infer that the narrator could have been driving or cycling given the text \"I got flat tire\".", "labels": [], "entities": []}, {"text": "Another typical use of event chain knowledge is to help infer what is likely to happen next given a previous event sequence in a scenario.", "labels": [], "entities": []}, {"text": "We investigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly ordered than words.", "labels": [], "entities": []}, {"text": "Our work follows a recent line of NLP research on script learning.", "labels": [], "entities": [{"text": "script learning", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8461026549339294}]}, {"text": "Stereotypical knowledge about partially-ordered events, together with their participant roles such as \"customer\", \"waiter\", and \"table\", is conventionally referred to as scripts (.", "labels": [], "entities": []}, {"text": "NLP algorithms have been investigated for automatically inducing scripts from unstructured texts.", "labels": [], "entities": []}, {"text": "In particular, made a first attempt to learn scripts from test inducing event chains by grouping events based on their narrative coherence, calculated based on Pairwise Mutual Information (PMI).", "labels": [], "entities": []}, {"text": "showed that the method can be improved by calculating event relations using skip bi-gram probabilities, which explicitly model the temporal order of pairs event.'s model is adopted by a line of subsequent methods on inducing event chains from text ().", "labels": [], "entities": []}, {"text": "While the above methods are statistical, neural network models have recently been used for event sequence modeling.", "labels": [], "entities": [{"text": "event sequence modeling", "start_pos": 91, "end_pos": 114, "type": "TASK", "confidence": 0.6987449129422506}]}, {"text": "used a Siamese Network instead of PMI to calculate the coherence between two events.", "labels": [], "entities": []}, {"text": "extended the idea of Jans et al. by using a log-bilinear neural language model to calculate event probabilities.", "labels": [], "entities": []}, {"text": "By learning embeddings for reducing sparsity, the above models give much better results compared to the models of and.", "labels": [], "entities": []}, {"text": "Similar in spirit, predicted the probability of an event belonging to a certain event chain by modeling known events in the chain as a bag of vectors, showing that it outperforms discrete statistical methods.", "labels": [], "entities": []}, {"text": "These neural methods are consistent with the earlier statistical models in leveraging event-pair relations.", "labels": [], "entities": []}, {"text": "Pichotta and Mooney (2016) experimented with LSTM for script learning, using an existing sequence of events to predict the probability of a next event, which outperformed strong discrete baselines.", "labels": [], "entities": [{"text": "script learning", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.9121830761432648}]}, {"text": "One advantage of LSTMs is that they can encode unbounded time sequences without losing long-term historical information.", "labels": [], "entities": []}, {"text": "LSTMs capture significantly more order information compared to the methods of,, which model the temporal order of only pairs of events.", "labels": [], "entities": []}, {"text": "On the other hand, a strong-order LSTM model can also suffer the disadvantage of over-fitting, given the flexible order of event chains in a script, as demonstrated by the cases of Figure 1.", "labels": [], "entities": []}, {"text": "In this aspect, event-pair models are more adaptive for flexible orders.", "labels": [], "entities": []}, {"text": "However, no direct comparisons have been reported between LSTM and various existing neural network methods that model event-pairs.", "labels": [], "entities": []}, {"text": "We make such comparisons using the same benchmark, finding that the method of Pichotta and Mooney (2016) does not necessarily outperform event-pair models, such as.", "labels": [], "entities": []}, {"text": "LSTM temporal ordering and event pair modeling have their respective strength.", "labels": [], "entities": [{"text": "LSTM temporal ordering", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7696421345074972}, {"text": "event pair modeling", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.6876799066861471}]}, {"text": "To leverage the advantages of both methods, we propose to integrate chain temporal order information into event relation measuring.", "labels": [], "entities": [{"text": "event relation measuring", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.7144174575805664}]}, {"text": "In particular, we calculate event pair relations by representing events in a chain using LSTM hidden states, which encode temporal information.", "labels": [], "entities": []}, {"text": "The L-STM over-fitting issue is mitigated by using the temporal-order in a chain as a feature for event pair modeling, rather than the direct model output.", "labels": [], "entities": [{"text": "event pair modeling", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.6494204600652059}]}, {"text": "In addition, observing that the importance of existing events can vary for inferring a subsequent event, we use a dynamic memory network model to automatically induce event weights for each event for inferring the next event.", "labels": [], "entities": []}, {"text": "In contrast, previous methods give equal weights to existing events.", "labels": [], "entities": []}, {"text": "Results on a multi-choice narrative cloze benchmark show that our model significantly outperforms both and, improving the state-of-the-art accuracy from 49.57% to 55.12%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9982895255088806}]}, {"text": "Our contributions can be summarized as follows: \u2022 We make a systematic comparison of LSTM and pair-based event sequence learning methods using the same benchmarks.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel dynamic memory network model, which combines the advantages of both LSTM temporal order learning and traditional event pair coherence learning.", "labels": [], "entities": [{"text": "LSTM temporal order learning", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.691567674279213}]}, {"text": "\u2022 We obtain the best results in the standard multi-choice narrative cloze test.", "labels": [], "entities": []}, {"text": "Our code is released at https://github.", "labels": [], "entities": []}, {"text": "com/wangzq870305/event_chain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following Granroth-Wilding and Clark (2016), we extract events from the NYT portion of the Gigaword corpus ().", "labels": [], "entities": [{"text": "NYT portion of the Gigaword corpus", "start_pos": 72, "end_pos": 106, "type": "DATASET", "confidence": 0.8794296383857727}]}, {"text": "The C&C tools ( are used for POS tagging and dependency parsing, and OpenNLP 3 for phrase structure parsing and coreference resolution.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8060015439987183}, {"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7694563865661621}, {"text": "phrase structure parsing", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.8308332363764445}, {"text": "coreference resolution", "start_pos": 112, "end_pos": 134, "type": "TASK", "confidence": 0.9609878957271576}]}, {"text": "The training set consists of 1,500,000 event chains.", "labels": [], "entities": []}, {"text": "We follow and use 10,000 event chains as the test set, and 1,000 event chains for development.", "labels": [], "entities": []}, {"text": "There are 5 choices of output event for event input chain, which are given by.", "labels": [], "entities": []}, {"text": "This dataset is referred to as G&C16.", "labels": [], "entities": [{"text": "G&C16", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9299595753351847}]}, {"text": "We also adapt the Chambers and Jurafsky (2008)'s dataset to the multiple choice setting, and use this dataset as the second benchmark.", "labels": [], "entities": [{"text": "Chambers and Jurafsky (2008)'s dataset", "start_pos": 18, "end_pos": 56, "type": "DATASET", "confidence": 0.5990207456052303}]}, {"text": "The dataset contains 69 documents, with 346 multiple choice event chain samples.", "labels": [], "entities": []}, {"text": "We randomly sample 4 negative subsequent events for each event chain to make multiple-choice candidates.", "labels": [], "entities": []}, {"text": "This dataset is referred to as C&J08.", "labels": [], "entities": [{"text": "C&J08", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9368650118509928}]}, {"text": "For both datasets, accuracy (Acc.) of the chosen subsequent event is used to measure the performance of our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9995025396347046}, {"text": "Acc.)", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9548813104629517}]}, {"text": "We conduct a set of development experiments on the G&C16 development set to study the influence of event argument representations and network configurations of the proposed MemNet model.", "labels": [], "entities": [{"text": "G&C16 development set", "start_pos": 51, "end_pos": 72, "type": "DATASET", "confidence": 0.9305834889411926}]}], "tableCaptions": [{"text": " Table 2: Analysis of network structure.", "labels": [], "entities": []}]}