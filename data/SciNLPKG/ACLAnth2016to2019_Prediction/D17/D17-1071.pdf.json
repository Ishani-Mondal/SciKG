{"title": [{"text": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "labels": [], "entities": [{"text": "Determining Semantic Textual Similarity", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6771533638238907}]}], "abstractContent": [{"text": "Determining semantic textual similarity is a core research subject in natural language processing.", "labels": [], "entities": [{"text": "Determining semantic textual similarity", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8645045757293701}, {"text": "natural language processing", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.6454102694988251}]}, {"text": "Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7403596937656403}]}, {"text": "By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of tex-tual similarity.", "labels": [], "entities": []}, {"text": "We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs.", "labels": [], "entities": [{"text": "determining semantic textual similarity", "start_pos": 24, "end_pos": 63, "type": "TASK", "confidence": 0.7009895294904709}]}, {"text": "For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs.", "labels": [], "entities": []}, {"text": "Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Determining semantic textual similarity (STS) is one of the most critical tasks in information retrieval and natural language processing.", "labels": [], "entities": [{"text": "Determining semantic textual similarity (STS)", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8761379718780518}, {"text": "information retrieval", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7622381448745728}, {"text": "natural language processing", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6312347451845804}]}, {"text": "Vector-based sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores.", "labels": [], "entities": [{"text": "Vector-based sentence representation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5815620720386505}]}, {"text": "Recently, neural network-based sentence representation models) have been proposed for learning textual similarity.", "labels": [], "entities": [{"text": "learning textual similarity", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.7486258347829183}]}, {"text": "However, these vectorbased models often use shallow information, such as words and characters, and whether they can account for phenomena such as negation and quantification is not clear.", "labels": [], "entities": []}, {"text": "Consider the sentences: Tom did not meet some of the players and Tom did not meet any of the players.", "labels": [], "entities": []}, {"text": "If functional words such as some or any are ignored or represented as the same vector, then these sentences are to be represented by identical vectors.", "labels": [], "entities": []}, {"text": "However, the first sentence implies that there is a player who Tom did not meet, whereas the second sentence means that Tom did not meet anyone, so the sentences have different meanings.", "labels": [], "entities": []}, {"text": "Conversely, logic-based approaches have been successful in representing the meanings of complex sentences, having had a positive impact for applications such as recognizing textual entailment (.", "labels": [], "entities": [{"text": "representing the meanings of complex sentences", "start_pos": 59, "end_pos": 105, "type": "TASK", "confidence": 0.8531266947587332}, {"text": "recognizing textual entailment", "start_pos": 161, "end_pos": 191, "type": "TASK", "confidence": 0.842655877272288}]}, {"text": "However, purely logic-based approaches only assess entailment or contradiction relations between sentences and do not offer graded notions of semantic similarity.", "labels": [], "entities": []}, {"text": "In this paper, we propose to leverage logic cues to learn textual similarity.", "labels": [], "entities": []}, {"text": "Our hypothesis is that observing proof processes when testing the semantic relations is predictive of textual similarity.", "labels": [], "entities": []}, {"text": "We show that our approach can be more effective than systems that ignore these logic cues.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our system 3 using two datasets: the SemEval-2014 version of the SICK dataset () and the SemEval-2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) ().", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.8068476319313049}, {"text": "MSR-paraphrase video corpus dataset (MSR-vid)", "start_pos": 130, "end_pos": 175, "type": "DATASET", "confidence": 0.7614165076187679}]}, {"text": "The experimental conditions were as follows.", "labels": [], "entities": []}, {"text": "The SICK dataset is a dataset for studying STS as well as for recognizing textual entailment (RTE).", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7095331400632858}, {"text": "STS", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9041008949279785}, {"text": "recognizing textual entailment (RTE)", "start_pos": 62, "end_pos": 98, "type": "TASK", "confidence": 0.7964607129494349}]}, {"text": "It was originally developed for evaluating compositional distributional semantics, so it contains logically challenging expressions such as quantifiers, negations, conjunctions and disjunctions.", "labels": [], "entities": [{"text": "evaluating compositional distributional semantics", "start_pos": 32, "end_pos": 81, "type": "TASK", "confidence": 0.6435048207640648}]}, {"text": "The dataset contains 9927 sentence pairs with a 5000/4927 training/test split.", "labels": [], "entities": []}, {"text": "These sentence pairs are manually annotated with three types of labels yes (entailment), no (contradiction), or unknown (neutral) as well as a semantic relatedness scores in (see fora sample).", "labels": [], "entities": []}, {"text": "In this dataset, sentence pairs whose gold entailment labels are no tend to be scored a little more highly than the average, whereas those whose labels are unknown have a wide range of scores.", "labels": [], "entities": []}, {"text": "Thus, we set the baseline of the relatedness score to 5 when the gold entailment label was yes and to 3 when the label was no or unknown.", "labels": [], "entities": []}, {"text": "We compared our system with the following systems: the state-of-the-art neural network-based system); the best system () from SemEval-2014; and two of the logic-3 Available at https://github.com/mynlp/ccg2lambda.", "labels": [], "entities": []}, {"text": "based systems stated in Section 2: namely The Meaning Factory () and UTexas.", "labels": [], "entities": [{"text": "UTexas", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9484487175941467}]}, {"text": "The Pearson correlation coefficient \u03b3, Spearman's rank correlation coefficient \u03c1, and the MSE were used as the evaluation metrics.", "labels": [], "entities": [{"text": "Pearson correlation coefficient \u03b3", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.9010718613862991}, {"text": "Spearman's rank correlation coefficient \u03c1", "start_pos": 39, "end_pos": 80, "type": "METRIC", "confidence": 0.6623523732026418}, {"text": "MSE", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.6913930177688599}]}, {"text": "The MSR-vid dataset is our second dataset for the STS task and contains 1500 sentence pairs with a 750/750 training/test split.", "labels": [], "entities": [{"text": "MSR-vid dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8743123114109039}, {"text": "STS task", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.774055153131485}]}, {"text": "All sentence pairs are annotated with semantic relatedness scores in the range.", "labels": [], "entities": []}, {"text": "We used this dataset to compare our system with the best system from SemEval-2012) and the logic-based UTexas system ().", "labels": [], "entities": []}, {"text": "We used the Pearson correlation coefficient \u03b3 as the evaluation metric.", "labels": [], "entities": [{"text": "Pearson correlation coefficient \u03b3", "start_pos": 12, "end_pos": 45, "type": "METRIC", "confidence": 0.878938302397728}]}, {"text": "shows the results of our experiments with the SICK dataset.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.8456608057022095}]}, {"text": "Although the state-of-the-art neural network-based system yielded the best results overall, our system achieved higher scores than SemEval-2014 submissions, including the two logic-based systems (The Meaning Factory and UTexas), in terms of Pearson correlation and Spearman's correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 241, "end_pos": 260, "type": "METRIC", "confidence": 0.9225258529186249}, {"text": "Spearman's correlation", "start_pos": 265, "end_pos": 287, "type": "METRIC", "confidence": 0.6073307494322459}]}], "tableCaptions": [{"text": " Table 1: Examples in the SICK dataset with different entailment labels and similarity scores.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.7371355295181274}, {"text": "similarity scores", "start_pos": 76, "end_pos": 93, "type": "METRIC", "confidence": 0.957115113735199}]}, {"text": " Table 2: Results on the test split of SICK dataset.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.8373767137527466}]}, {"text": " Table 3: Results on the test split of MSR-vid.", "labels": [], "entities": [{"text": "MSR-vid", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.7557535171508789}]}, {"text": " Table 4: Results when training our regressor with  each feature group in isolation.", "labels": [], "entities": []}, {"text": " Table 5: Examples for which our regressor trained only with logic-based features performs better than  when using non-logic features. \"Gold\": correct score, \"Pred+logic\": prediction score only with logic- based features, \"Pred-logic\": prediction score only with non-logic-based features.", "labels": [], "entities": [{"text": "correct score", "start_pos": 143, "end_pos": 156, "type": "METRIC", "confidence": 0.9786428213119507}]}, {"text": " Table 6: Error examples when training the regressor only with logic-based features.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9767683744430542}]}]}