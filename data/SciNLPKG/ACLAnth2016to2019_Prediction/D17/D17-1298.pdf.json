{"title": [{"text": "Adapting Sequence Models for Sentence Correction", "labels": [], "entities": [{"text": "Adapting Sequence", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8763102293014526}]}], "abstractContent": [{"text": "Ina controlled experiment of sequence-to-sequence approaches for the task of sentence correction, we find that character-based models are generally more effective than word-based models and models that encode subword information via con-volutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches.", "labels": [], "entities": [{"text": "sentence correction", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7507437169551849}]}, {"text": "Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M 2 (0.5 GLEU) points.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 69, "end_pos": 113, "type": "TASK", "confidence": 0.5906287208199501}, {"text": "GLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.950547456741333}]}, {"text": "Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M 2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.", "labels": [], "entities": [{"text": "CoNLL-2014 setup", "start_pos": 54, "end_pos": 70, "type": "DATASET", "confidence": 0.9073955416679382}]}], "introductionContent": [{"text": "The task of sentence correction is to convert a natural language sentence that mayor may not have errors into a corrected version.", "labels": [], "entities": [{"text": "sentence correction", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7505925893783569}]}, {"text": "The task is envisioned as a component of a learning tool or writing-assistant, and has seen increased interest since 2011 driven by a series of shared tasks ().", "labels": [], "entities": []}, {"text": "Most recent work on language correction has focused on the data provided by the CoNLL-2014 shared task (), a set of corrected essays by second-language learners.", "labels": [], "entities": [{"text": "language correction", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7430911362171173}]}, {"text": "The CoNLL-2014 data consists of only around 60,000 sentences, and as such, competitive systems have made use of large amounts of corrected text without annotations, and in some cases lower-quality crowd-annotated data, in addition to the shared data.", "labels": [], "entities": [{"text": "CoNLL-2014 data", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9525337517261505}]}, {"text": "In this data environment, it has been suggested that statistical phrase-based machine translation (MT) with task-specific features is the state-of-the-art for the task, outperforming wordand character-based sequence-to-sequence models, phrase-based systems with neural features, re-ranking output from phrase-based systems, and combining phrase-based systems with classifiers trained for hand-picked subsets of errors.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation (MT)", "start_pos": 53, "end_pos": 102, "type": "TASK", "confidence": 0.7564668825694493}]}, {"text": "We revisit the comparison across translation approaches for the correction task in light of the Automated Evaluation of Scientific Writing (AESW) 2016 dataset, a correction dataset containing over 1 million sentences, holding constant the training data across approaches.", "labels": [], "entities": [{"text": "correction task", "start_pos": 64, "end_pos": 79, "type": "TASK", "confidence": 0.886728972196579}, {"text": "Automated Evaluation of Scientific Writing (AESW) 2016 dataset", "start_pos": 96, "end_pos": 158, "type": "DATASET", "confidence": 0.6602776437997818}]}, {"text": "The dataset was previously proposed for the distinct binary classification task of grammatical error identification.", "labels": [], "entities": [{"text": "grammatical error identification", "start_pos": 83, "end_pos": 115, "type": "TASK", "confidence": 0.6214262545108795}]}, {"text": "Experiments demonstrate that pure characterlevel sequence-to-sequence models are more effective on AESW than word-based models and models that encode subword information via convolutions over characters, and that representing the output data as a series of diffs significantly increases effectiveness on this task.", "labels": [], "entities": []}, {"text": "Our strongest character-level model achieves statistically significant improvements over our strongest phrasebased statistical machine translation model by 6 M 2 (0.5 GLEU) points, with additional gains when including domain information.", "labels": [], "entities": [{"text": "phrasebased statistical machine translation", "start_pos": 103, "end_pos": 146, "type": "TASK", "confidence": 0.5553184524178505}, {"text": "GLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9519705176353455}]}, {"text": "Furthermore, in the partially crowd-sourced data environment of the standard CoNLL-2014 setup in which there are comparatively few professionally annotated sentences, we find that tuning against the tags marking the diffs yields similar or superior effectiveness relative to existing sequence-to-sequence approaches despite using significantly less data, with or without using secondary models.", "labels": [], "entities": []}, {"text": "All code is available at https://github.", "labels": [], "entities": []}, {"text": "com/allenschmaltz/grammar.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data AESW consists of sentences taken from academic articles annotated with corrections by professional editors used for the AESW shared task.", "labels": [], "entities": []}, {"text": "The training set contains 1,182,491 sentences, of which 460,901 sentences have edits.", "labels": [], "entities": []}, {"text": "We set aside a 9,947 sentence sample from the original development set for tuning (of which 3,797 contain edits), and use the remaining 137,446 sentences as the dev set 3 (of which 53,502 contain edits).", "labels": [], "entities": []}, {"text": "The test set contains 146,478 sentences.", "labels": [], "entities": []}, {"text": "The primary focus of the present study is conducting controlled experiments on the AESW dataset, but we also investigate results on the CoNLL-2014 shared task data in light of recent neural results ( and to serve as a baseline of comparison against existing sequenceto-sequence approaches The dev set contains 13,562 unique deletion types, 29,952 insertion types, and 39,930 replacement types.", "labels": [], "entities": [{"text": "AESW dataset", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.9692249298095703}, {"text": "CoNLL-2014 shared task data", "start_pos": 136, "end_pos": 163, "type": "DATASET", "confidence": 0.8603847026824951}]}, {"text": "data ( Parameters All our models, implemented with OpenNMT (, are 2-layer LSTMs with 750 hidden units.", "labels": [], "entities": []}, {"text": "For the WORD model, the word embedding size is also set to 750, while for the CHARCNN and CHAR models we use a character embedding size of 25.", "labels": [], "entities": []}, {"text": "The CHARCNN model has a convolutional layer with 1000 filters of width 6 followed by max-pooling, which is fed into a 2-layer highway network.", "labels": [], "entities": []}, {"text": "Additional training details are provided in Appendix A. For AESW, the WORD+BI model contains around 144 million parameters, the CHARCNN+BI model around 79 million parameters, and the CHAR+BI model around 25 million parameters.", "labels": [], "entities": [{"text": "AESW", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.7882774472236633}, {"text": "WORD+BI", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.6177325447400411}]}, {"text": "Statistical Machine Translation As a baseline of comparison, we experiment with a phrase-based machine translation approach (SMT) shown to be state-of-the-art for the CoNLL-2014 shared task data in previous work, which adds task specific features and the M 2 metric as a scorer to the Moses statistical machine translation system.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6918749411900839}, {"text": "phrase-based machine translation", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6147074500719706}, {"text": "CoNLL-2014 shared task data", "start_pos": 167, "end_pos": 194, "type": "DATASET", "confidence": 0.836750715970993}, {"text": "statistical machine translation", "start_pos": 291, "end_pos": 322, "type": "TASK", "confidence": 0.6114049553871155}]}, {"text": "The SMT model follows the training, parameters, and dense and sparse task-specific features that generate state-of-the-art results for CoNLL-2014 shared task data, as implemented in publicly available code.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9867553114891052}, {"text": "CoNLL-2014 shared task data", "start_pos": 135, "end_pos": 162, "type": "DATASET", "confidence": 0.7472387105226517}]}, {"text": "However, to compare models against the same training data, we remove language model features associated with external data.", "labels": [], "entities": []}, {"text": "We exper-iment with tuning against M 2 (+M 2 ) and BLEU (+BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9986231327056885}, {"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.7638642191886902}]}, {"text": "Models trained with diffs were only tuned with BLEU, since the tuning pipeline from previous work is not designed to handle removing such annotation tags prior to M 2 scoring.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9992420673370361}, {"text": "M 2 scoring", "start_pos": 163, "end_pos": 174, "type": "TASK", "confidence": 0.5957109928131104}]}, {"text": "shows the full set of experimental results on the AESW development and test data.", "labels": [], "entities": [{"text": "AESW development and test data", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.881188714504242}]}], "tableCaptions": [{"text": " Table 1: AESW development/test set correction results.", "labels": [], "entities": [{"text": "AESW development/test set correction", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.5979155153036118}]}, {"text": " Table 2: Micro F0.5 scores on replacement errors on the dev set. Errors are grouped by 'Punctuation', 'Article', and 'Other'.", "labels": [], "entities": [{"text": "Micro", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.5559671521186829}, {"text": "F0.5", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.6377860307693481}, {"text": "Errors", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9564725756645203}]}, {"text": " Table 3: Micro F0.5 scores across error types", "labels": [], "entities": [{"text": "F0.5", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9912950992584229}]}, {"text": " Table 4: M 2 scores on the CoNLL-2013 set.", "labels": [], "entities": [{"text": "M", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9814522862434387}, {"text": "CoNLL-2013 set", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.9798407256603241}]}, {"text": " Table 5: M 2 scores on the CoNLL-2014 test set and data", "labels": [], "entities": [{"text": "M", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9524374604225159}, {"text": "CoNLL-2014 test set", "start_pos": 28, "end_pos": 47, "type": "DATASET", "confidence": 0.9854127367337545}]}]}