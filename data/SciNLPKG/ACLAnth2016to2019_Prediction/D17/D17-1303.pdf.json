{"title": [{"text": "Image Pivoting for Learning Multilingual Multimodal Representations", "labels": [], "entities": [{"text": "Image Pivoting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7110037356615067}, {"text": "Learning Multilingual Multimodal Representations", "start_pos": 19, "end_pos": 67, "type": "TASK", "confidence": 0.5753342285752296}]}], "abstractContent": [{"text": "In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding.", "labels": [], "entities": [{"text": "image understanding", "start_pos": 208, "end_pos": 227, "type": "TASK", "confidence": 0.7273452281951904}]}, {"text": "Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages.", "labels": [], "entities": []}, {"text": "We introduce anew pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities.", "labels": [], "entities": []}, {"text": "We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English.", "labels": [], "entities": []}, {"text": "In both cases we achieve state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images.", "labels": [], "entities": []}, {"text": "Examples include text-based image retrieval, image description and visual question answering.", "labels": [], "entities": [{"text": "text-based image retrieval", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6134823262691498}, {"text": "image description", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7897087037563324}, {"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7023391276597977}]}, {"text": "An increasing number of large image description datasets has become available () and various systems have been proposed to handle the image description task as a generation problem (.", "labels": [], "entities": [{"text": "image description task", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.8398903409639994}]}, {"text": "There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text).", "labels": [], "entities": [{"text": "sentence-based image search", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6033166944980621}]}, {"text": "Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets.", "labels": [], "entities": [{"text": "image description generation", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.8086345791816711}]}, {"text": "Recently there have been attempts to create image descriptions and models for other languages).", "labels": [], "entities": []}, {"text": "Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions.", "labels": [], "entities": []}, {"text": "Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function ( or by aligning image regions (objects) and segments of the description () in a common space.", "labels": [], "entities": []}, {"text": "Recently have leveraged visual question answering models to encode images and descriptions into the same space.", "labels": [], "entities": [{"text": "question answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7586337924003601}]}, {"text": "However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space.", "labels": [], "entities": []}, {"text": "The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation ( and to learn multilingual multimodal representations (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7743545770645142}]}, {"text": "propose a Two men playing soccer on field a zwei m\u00e4nner k\u00e4mpfen um einen fussball Joint space: Our multilingual multimodal model with image as pivot model to learn common representations between M views and assume there is parallel data available between a pivot view and the remaining M \u22121 views.", "labels": [], "entities": []}, {"text": "Their multimodal experiments are based on English as the pivot and use large parallel corpora available between languages to learn their representations.", "labels": [], "entities": []}, {"text": "Related to our work proposed a model for creating multilingual multimodal embeddings.", "labels": [], "entities": []}, {"text": "Our work is different from theirs in that we choose the image as the pivot and use a different similarity function.", "labels": [], "entities": []}, {"text": "We also propose a single model for learning representations of images and multiple languages, whereas their model is language-specific.", "labels": [], "entities": []}, {"text": "In this paper, we learn multimodal representations in multiple languages, i.e., our model yields a joint space for images and text in multiple languages using the image as a pivot between languages.", "labels": [], "entities": []}, {"text": "We propose anew objective function in a multitask learning setting and jointly optimize the mappings between images and text in two different languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with the Multi30k dataset, a multilingual extension of Flickr30k corpus) consisting of English and German image descriptions ).", "labels": [], "entities": [{"text": "Multi30k dataset", "start_pos": 23, "end_pos": 39, "type": "DATASET", "confidence": 0.8806963860988617}, {"text": "Flickr30k corpus", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9565203785896301}]}, {"text": "The Multi30K dataset has 29k, 1k and 1k images in the train, validation and test splits respectively, and contains two types of multilingual annotations: (i) a corpus of one English description per image and its translation into German; and (ii) a corpus of five independently collected English and German descriptions per image.", "labels": [], "entities": [{"text": "Multi30K dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9338065087795258}]}, {"text": "We use the independently collected English and German descriptions to train our models.", "labels": [], "entities": []}, {"text": "Note that these descriptions are not translations of each other, i.e., they are not parallel, although they describe the same image.", "labels": [], "entities": []}, {"text": "We test our model on the tasks of imagedescription ranking and semantic textual similarity.", "labels": [], "entities": [{"text": "imagedescription ranking", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.778559684753418}]}, {"text": "We work with each language separately.", "labels": [], "entities": []}, {"text": "Since we learn embeddings for images and languages in the same semantic space, our hope is that the training data for each modality or language acts complementary data for the another modality or language, and thus helps us learn better embeddings.", "labels": [], "entities": []}, {"text": "Experiment Setup We sampled minibatches of size 64 images and their descriptions, and drew all negative samples from the minibatch.", "labels": [], "entities": []}, {"text": "We trained using the Adam optimizer with learning rate 0.001, and early stopping on the validation set.", "labels": [], "entities": [{"text": "learning rate 0.001", "start_pos": 41, "end_pos": 60, "type": "METRIC", "confidence": 0.9550374348958334}, {"text": "validation", "start_pos": 88, "end_pos": 98, "type": "TASK", "confidence": 0.9576689004898071}]}, {"text": "Following we set the dimensionality of the embedding space and the GRU hidden layer N to 1024 for both English and German.", "labels": [], "entities": [{"text": "GRU hidden layer N", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.7747313529253006}]}, {"text": "We set the dimensionality of the learned word embeddings to 300 for both languages, and the margin \u03b1 to 0.05 and 0.2, respectively, to learn asymmetric and symmetric similarity-based embeddings.", "labels": [], "entities": []}, {"text": "We keep all hyperparameters constant across all models.", "labels": [], "entities": []}, {"text": "We used the L2 norm to mitigate over-fitting (.", "labels": [], "entities": []}, {"text": "We tokenize and truecase both English and German descriptions using the Moses Decoder scripts.", "labels": [], "entities": [{"text": "Moses Decoder scripts", "start_pos": 72, "end_pos": 93, "type": "DATASET", "confidence": 0.8488990664482117}]}, {"text": "To extract image features, we used a convolutional neural network model trained on 1.2M images of 1000 class ILSVRC 2012 object classification dataset, a subset of ImageNet (.", "labels": [], "entities": [{"text": "ILSVRC 2012 object classification dataset", "start_pos": 109, "end_pos": 150, "type": "DATASET", "confidence": 0.8214851140975952}]}, {"text": "Specifically, we used VGG 19-layer CNN architecture and extracted the activations of the penultimate fully connected layer to obtain features for all images in the dataset (: The rank of the gold-standard image when using each German and English descriptions as a query on models trained using asymmetric similarity.", "labels": [], "entities": [{"text": "VGG 19-layer CNN", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8205669124921163}]}, {"text": "use a publicly available implementation to train both VSE and OE.", "labels": [], "entities": [{"text": "VSE", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.6244156956672668}, {"text": "OE", "start_pos": 62, "end_pos": 64, "type": "DATASET", "confidence": 0.5461058616638184}]}], "tableCaptions": [{"text": " Table 1: Image-description ranking results of  English on Flickr30k test data.", "labels": [], "entities": [{"text": "Flickr30k test data", "start_pos": 59, "end_pos": 78, "type": "DATASET", "confidence": 0.9829941193262736}]}, {"text": " Table 2: Image-description ranking results of  German on Flickr30k test data.", "labels": [], "entities": [{"text": "Flickr30k test data", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9353620807329813}]}, {"text": " Table 3: The rank of the gold-standard image when using each German and English descriptions as a  query on models trained using asymmetric similarity.", "labels": [], "entities": []}, {"text": " Table 4: Results on Semantic Textual Similarity  Image datasets (Pearson's r \u00d7 100 ). Our systems  that performed better than best reported shared  task scores are in bold.", "labels": [], "entities": [{"text": "Semantic Textual Similarity  Image", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.6383816972374916}]}, {"text": " Table 5: Example sentences with gold-standard  semantic textual similarity score and the predicted  score using our best performing PARALLEL- ASYM model.", "labels": [], "entities": [{"text": "PARALLEL", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9162601232528687}]}]}