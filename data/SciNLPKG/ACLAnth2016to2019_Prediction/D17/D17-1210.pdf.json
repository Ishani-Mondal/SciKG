{"title": [{"text": "Trainable Greedy Decoding for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7602890332539877}]}], "abstractContent": [{"text": "Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-to-end learning algorithms.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6752155621846517}]}, {"text": "The problem of decoding, however, has received relatively little attention from the research community.", "labels": [], "entities": []}, {"text": "In this paper, we solely focus on the problem of decoding given a trained neu-ral machine translation model.", "labels": [], "entities": []}, {"text": "Instead of trying to build anew decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective.", "labels": [], "entities": []}, {"text": "More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient.", "labels": [], "entities": []}, {"text": "We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation has recently become a method of choice in machine translation research.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8228695789972941}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7799960076808929}]}, {"text": "Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (, neural machine translation has ventured into more sophisticated settings of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7409610152244568}, {"text": "neural machine translation", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.7227000792821249}, {"text": "machine translation", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.7706816792488098}]}, {"text": "For instance, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.669717808564504}]}, {"text": "Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (.", "labels": [], "entities": []}, {"text": "A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.671820749839147}]}, {"text": "That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus.", "labels": [], "entities": []}, {"text": "Once the model is trained in this way, it defines the conditional distribution overall possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns the highest conditional probability.", "labels": [], "entities": [{"text": "translation", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.9602230191230774}]}, {"text": "Since it is computationally intractable to do so exactly, it is a usual practice to resort to approximate search/decoding algorithms such as greedy decoding or beam search.", "labels": [], "entities": []}, {"text": "In this scenario, we have identified two points where improvements could be made.", "labels": [], "entities": []}, {"text": "They are (1) training (including the selection of a model architecture) and (2) decoding.", "labels": [], "entities": []}, {"text": "Much of the research on neural machine translation has focused solely on the former, that is, on improving the model architecture.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6749457021554311}]}, {"text": "Neural machine translation started with with a simple encoderdecoder architecture in which a source sentence is encoded into a single, fixed-size vector (.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8458481431007385}]}, {"text": "It soon evolved with the attention mechanism ( . A few variants of the attention mechanism, or its regularization, have been proposed recently to improve both the translation quality as well as the computational efficiency ().", "labels": [], "entities": []}, {"text": "More recently, convolutional net-works have been adopted either as a replacement of or a complement to a recurrent network in order to efficiently utilize parallel computing).", "labels": [], "entities": []}, {"text": "On the aspect of decoding, only a few research groups have tackled this problem by incorporating a target decoding algorithm into training. and proposed a learning algorithm tailored for beam search. and ( suggested to use a reinforcement learning algorithm by viewing a neural machine translation model as a policy function.", "labels": [], "entities": [{"text": "beam search.", "start_pos": 187, "end_pos": 199, "type": "TASK", "confidence": 0.8316946923732758}]}, {"text": "Investigation on decoding alone has, however, been limited.", "labels": [], "entities": []}, {"text": "showed the limitation of greedy decoding by simply injecting unstructured noise into the hidden state of the neural machine translation system.", "labels": [], "entities": []}, {"text": "similarly showed that the exactness of beam search does not correlate well with actual translation quality, and proposed to augment the learning cost function with reconstruction to alleviate this problem.", "labels": [], "entities": [{"text": "exactness", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9484922885894775}, {"text": "beam search", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.7988058030605316}]}, {"text": "proposed a modification to the existing beam search algorithm to improve its exploration of the translation space.", "labels": [], "entities": [{"text": "beam search", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.8061454892158508}]}, {"text": "In this paper, we tackle the problem of decoding in neural machine translation by introducing a concept of trainable greedy decoding.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.7006860971450806}]}, {"text": "Instead of manually designing anew decoding algorithm suitable for neural machine translation, we propose to learn a decoding algorithm with an arbitrary decoding objective.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7266121904055277}]}, {"text": "More specifically, we introduce a neural-network-based decoding algorithm that works on an already-trained neural machine translation system by observing and manipulating its hidden state.", "labels": [], "entities": []}, {"text": "We treat such a neural network as an agent with a deterministic, continuous action and train it with a variant of the deterministic policy gradient algorithm).", "labels": [], "entities": []}, {"text": "We extensively evaluate the proposed trainable greedy decoding on four language pairs (En-Cs, En-De, En-Ru and En-Fi; in both directions) with two different decoding objectives; sentence-level BLEU and negative perplexity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.973972737789154}]}, {"text": "By training such trainable greedy decoding using deterministic policy gradient with the proposed critic-aware actor learning, we observe that we can improve decoding performance with minimal computational overhead.", "labels": [], "entities": []}, {"text": "Furthermore, the trained actors are found to improve beam search as well, suggesting a future research direction in extending the proposed idea of trainable decoding for more sophisticated underlying decoding algorithms.", "labels": [], "entities": [{"text": "beam search", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.8276852369308472}]}], "datasetContent": [{"text": "We empirically evaluate the proposed trainable greedy decoding on four language pairs -EnDe, En-Ru, En-Cs and En-Fi -using a standard attention-based neural machine translation system ( ).", "labels": [], "entities": [{"text": "attention-based neural machine translation", "start_pos": 134, "end_pos": 176, "type": "TASK", "confidence": 0.6614395827054977}]}, {"text": "We train underlying neural translation systems using the parallel corpora made available from WMT'15.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.806412935256958}, {"text": "WMT'15", "start_pos": 94, "end_pos": 100, "type": "DATASET", "confidence": 0.9595762491226196}]}, {"text": "The same set of corpora are used for trainable greedy decoding as well.", "labels": [], "entities": []}, {"text": "All the corpora are tokenized and segmented into subword symbols using byte-pair encoding (BPE) (.", "labels": [], "entities": []}, {"text": "We use sentences of length up to 50 subword symbols for MLE training and 200 symbols for trainable decoding.", "labels": [], "entities": [{"text": "MLE training", "start_pos": 56, "end_pos": 68, "type": "TASK", "confidence": 0.9102171957492828}]}, {"text": "For validation and testing, we use newstest-2013 and newstest-2015, respectively.", "labels": [], "entities": [{"text": "newstest-2013", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8769792318344116}, {"text": "newstest-2015", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.8887037634849548}]}], "tableCaptions": []}