{"title": [{"text": "Unfolding and Shrinking Neural Machine Translation Ensembles", "labels": [], "entities": [{"text": "Shrinking Neural Machine Translation Ensembles", "start_pos": 14, "end_pos": 60, "type": "TASK", "confidence": 0.8560336589813232}]}], "abstractContent": [{"text": "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8381296396255493}]}, {"text": "Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models.", "labels": [], "entities": []}, {"text": "En-sembling often improves the quality of the generated translations drastically.", "labels": [], "entities": []}, {"text": "However , it is not suitable for production systems because it is cumbersome and slow.", "labels": [], "entities": []}, {"text": "This work aims to reduce the runtime to be on par with a single system without compromising the translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.9521499872207642}]}, {"text": "First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system.", "labels": [], "entities": []}, {"text": "We show that unfolding can already improve the runtime in practice since more work can be done on the GPU.", "labels": [], "entities": [{"text": "GPU", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.9854186773300171}]}, {"text": "We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers.", "labels": [], "entities": []}, {"text": "On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.", "labels": [], "entities": []}], "introductionContent": [{"text": "The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8544789751370748}]}, {"text": "Ensembling of neural networks is a simple yet very effective technique to improve the accuracy of NMT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9992803931236267}]}, {"text": "The decoder makes use of K NMT networks which are either trained independently or share some amount of training iterations ().", "labels": [], "entities": []}, {"text": "The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average) or the geometric average.", "labels": [], "entities": [{"text": "arithmetic average", "start_pos": 111, "end_pos": 129, "type": "METRIC", "confidence": 0.940691202878952}]}, {"text": "Ensembling consistently outperforms single NMT by a large margin.", "labels": [], "entities": []}, {"text": "However, the decoding speed is significantly worse since the decoder needs to apply K NMT models rather than only one.", "labels": [], "entities": []}, {"text": "Therefore, a recent line of research transfers the idea of knowledge distillation () to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher).", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.7073143720626831}]}, {"text": "This paper presents an alternative to knowledge distillation as we aim to speedup decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7495786249637604}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.8683809041976929}]}, {"text": "Ina first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology.", "labels": [], "entities": []}, {"text": "We will refer to this process as unfolding.", "labels": [], "entities": []}, {"text": "GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU.", "labels": [], "entities": []}, {"text": "Ina second step, we explore methods to reduce the size of the unfolded network.", "labels": [], "entities": []}, {"text": "This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (.", "labels": [], "entities": []}, {"text": "Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices.", "labels": [], "entities": []}, {"text": "More importantly, the  decoding speed on all platforms benefits greatly from the reduced number of neurons.", "labels": [], "entities": []}, {"text": "We find that the dimensionality of linear embedding layers in the NMT network can be reduced heavily by lowrank matrix approximation based on singular value decomposition (SVD).", "labels": [], "entities": [{"text": "NMT network", "start_pos": 66, "end_pos": 77, "type": "DATASET", "confidence": 0.8031363487243652}]}, {"text": "This suggest that high dimensional embedding layers maybe needed for training, but do not play an important role for decoding.", "labels": [], "entities": []}, {"text": "The NMT network, however, also consists of complex layers like gated recurrent units (, GRUs) and attention ( ).", "labels": [], "entities": []}, {"text": "Therefore, we introduce a novel algorithm based on linear combinations of neurons which can be applied either during training (data-bound) or directly on the weight matrices without using training data (data-free).", "labels": [], "entities": []}, {"text": "We report that with a mix of the presented shrinking methods we are able to reduce the size of the unfolded network to the size of the single NMT network while keeping the boost in BLEU score from the ensemble.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 181, "end_pos": 191, "type": "METRIC", "confidence": 0.9812920987606049}]}, {"text": "Depending on the aggressiveness of shrinking, we report either again of 2.2 BLEU at the same decoding speed, or a 3.4\u00d7 CPU decoding speedup with only a minor drop in BLEU compared to the original single NMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.997653067111969}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9993329644203186}]}, {"text": "Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints.", "labels": [], "entities": []}, {"text": "Unfolding and shrinking address these problems directly.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Shrinking layers of the unfolded network on Ja-En to their original size.", "labels": [], "entities": []}, {"text": " Table 2: Compensating for neuron removal in  the data-bound algorithm. Row (d) corresponds  to row (f) in Tab. 1.", "labels": [], "entities": [{"text": "neuron removal", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.7883273661136627}, {"text": "Row", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9522289037704468}, {"text": "Tab. 1", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.9426469206809998}]}, {"text": " Table 3: Time measurements on Ja-En. Layers are shrunk to their size in the original NMT model.", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9632377028465271}]}, {"text": " Table 4: Layer sizes of our setups for Ja-En.", "labels": [], "entities": []}, {"text": " Table 5: Our best models on Ja-En.", "labels": [], "entities": [{"text": "Ja-En", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9230462312698364}]}, {"text": " Table 6: Our best models on En-De.", "labels": [], "entities": [{"text": "En-De", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9563273787498474}]}]}