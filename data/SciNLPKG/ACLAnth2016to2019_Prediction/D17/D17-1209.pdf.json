{"title": [{"text": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation", "labels": [], "entities": [{"text": "Syntax-aware Neural Machine Translation", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.660750575363636}]}], "abstractContent": [{"text": "We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7828502058982849}]}, {"text": "We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data.", "labels": [], "entities": []}, {"text": "Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods.", "labels": [], "entities": []}, {"text": "GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidi-rectional RNNs or convolutional neural networks).", "labels": [], "entities": []}, {"text": "We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs ().", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8278170724709829}]}, {"text": "State-ofthe-art NMT systems rely on sequential encoderdecoders ( and lack any explicit modeling of syntax or any hierarchical structure of language.", "labels": [], "entities": []}, {"text": "One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs.", "labels": [], "entities": []}, {"text": "Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning () or maybe too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases ().", "labels": [], "entities": []}, {"text": "Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task.", "labels": [], "entities": [{"text": "MT", "start_pos": 138, "end_pos": 140, "type": "TASK", "confidence": 0.9906141757965088}]}, {"text": "This goal is inline with claims that rigid syntactic constraints typically hurt MT (, and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.938937783241272}, {"text": "MT", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.9505502581596375}]}, {"text": "Attention-based NMT systems () represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.", "labels": [], "entities": []}, {"text": "Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output.", "labels": [], "entities": []}, {"text": "Since vectors correspond to words, it is natural for us to use dependency syntax.", "labels": [], "entities": []}, {"text": "Dependency trees (see) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.", "labels": [], "entities": []}, {"text": "In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (.", "labels": [], "entities": []}, {"text": "GCNs can be regarded as computing a latent-feature representation of anode (i.e. a real-valued vector) based on its k- th order neighborhood (i.e. nodes at most k hops aways from the node) ().", "labels": [], "entities": []}, {"text": "They are generally simple and computationally inexpensive.", "labels": [], "entities": []}, {"text": "We use Syntactic GCNs, aversion of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.674818734327952}]}, {"text": "Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework.", "labels": [], "entities": []}, {"text": "As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task.", "labels": [], "entities": []}, {"text": "Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders, enriching their states with syntactic information.", "labels": [], "entities": []}, {"text": "A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (.", "labels": [], "entities": []}, {"text": "Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively.", "labels": [], "entities": [{"text": "BLEU point", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.972568541765213}]}, {"text": "In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (, AMR semantic graphs () and co-reference chains).", "labels": [], "entities": []}, {"text": "For example, unlike recursive neural networks), GCNs do not require the graphs to be trees.", "labels": [], "entities": []}, {"text": "However, in this work we solely focus on dependency syntax and leave more general investigation for future work.", "labels": [], "entities": [{"text": "dependency syntax", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.82606440782547}]}, {"text": "Our main contributions can be summarized as follows: \u2022 we introduce a method for incorporating structure into NMT using syntactic GCNs; \u2022 we show that GCNs can be used along with RNN and CNN encoders; \u2022 we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 258, "end_pos": 277, "type": "TASK", "confidence": 0.7720708549022675}]}], "datasetContent": [{"text": "Experiments are performed using the Neural Monkey toolkit 3 (Helcl and Libovick\u00b4y, which implements the model of in TensorFlow.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer () with a learning rate of 0.001 (0.0002 for CNN models).", "labels": [], "entities": []}, {"text": "The batch size is set to 80.", "labels": [], "entities": []}, {"text": "Between layers we apply dropout with a probability of 0.2, and in experiments with GCNs we use the same value for edge dropout.", "labels": [], "entities": []}, {"text": "We train for 45 epochs, evaluating the BLEU performance of the model every epoch on the validation set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9992437362670898}]}, {"text": "For testing, we select the model with the highest validation BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9537085890769958}]}, {"text": "L2 regularization is used with a value of 10 \u22128 . All the model selection (incl.", "labels": [], "entities": []}, {"text": "hyperparameter selections) was performed on the validation set.", "labels": [], "entities": []}, {"text": "In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.", "labels": [], "entities": []}, {"text": "We will describe an artificial experiment in \u00a74.1 and MT experiments in \u00a74.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 and vocabulary sizes in Table 2.", "labels": [], "entities": []}, {"text": " Table 3: Test results for English-German.", "labels": [], "entities": []}, {"text": " Table 4: Test results for English-Czech.", "labels": [], "entities": []}, {"text": " Table 5: Validation BLEU for English-German  and English-Czech for 1-and 2-layer GCNs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8954545259475708}]}]}