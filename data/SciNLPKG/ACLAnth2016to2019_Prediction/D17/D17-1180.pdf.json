{"title": [{"text": "TAG Parsing with Neural Networks and Vector Representations of Supertags", "labels": [], "entities": [{"text": "TAG Parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8773881494998932}]}], "abstractContent": [{"text": "We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (ele-mentary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores.", "labels": [], "entities": [{"text": "Tree Adjoining Grammar parsing", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.7399383783340454}]}, {"text": "The shift-reduce parsing model eschews lexical information entirely , and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is \"almost parsing.\"", "labels": [], "entities": []}, {"text": "We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distri-butional semantics.", "labels": [], "entities": []}, {"text": "This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.6266622096300125}]}], "introductionContent": [{"text": "Recent work has applied Combinatory Categorial Grammar) to the problem of broad-coverage parsing in order to derive grammatical representations that are sufficiently rich to support tasks requiring deeper representation of a sentence's meaning ().", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.5949221551418304}, {"text": "broad-coverage parsing", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7889032065868378}]}, {"text": "Yet CCG is only one of a number of mildly context-sensitive grammar formalisms that can provide such rich representations, and each has distinct advantages.", "labels": [], "entities": []}, {"text": "In this paper we explore the applicability of another formalism, Tree Adjoining Grammar (TAG,), to the task of broad-coverage parsing.", "labels": [], "entities": [{"text": "broad-coverage parsing", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8250789046287537}]}, {"text": "TAG and CCG share the property of lexicalization: words are associated with elementary units of grammatical structure which are composed during a derivation using one of a small set of operations to produce a parse tree.", "labels": [], "entities": []}, {"text": "The task of parsing involves the construction of a derivation tree that encodes the application of this set of actions to a set of elementary lexically-associated objects.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9689415097236633}]}, {"text": "TAG differs from CCG in having an even richer set of lexical units, so that the identification of these units in a derivation could be even more informative for subsequent tasks involving semantic interpretation, translation and the like, which have been the focus of CCG-based work.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.7271098792552948}]}, {"text": "The elementary units of CCG and TAG (categories for CCG, and elementary trees for TAG) determine a word's combinatory potential, in away that is not the case for the usual part-ofspeech tags used in parsing.", "labels": [], "entities": []}, {"text": "Indeed, the assignment of elementary objects to the words in a sentence almost determines the possible parse fora sentence.", "labels": [], "entities": []}, {"text": "The near uniqueness of a parse given a sequence of lexical units motivated to decompose the parsing problem into two phases: supertagging, where elementary objects, or supertags, are assigned to each word, and stapling, where these supertags are combined together.", "labels": [], "entities": []}, {"text": "They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is \"almost parsing.\"", "labels": [], "entities": []}, {"text": "This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (, CCG: ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.982040286064148}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9985846281051636}, {"text": "TAG", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9782311320304871}]}, {"text": "However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re-sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned.", "labels": [], "entities": []}, {"text": "This problem is even more severe for TAG parsing.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9572632014751434}]}, {"text": "TAG differs from CCG in having a smaller set of combinatory operations, but a more varied set of elementary objects: the TAG-annotated version of the Penn Treebank that we use) includes 4727 distinct supertags (2165 occur once) while the CCG-annotated version) includes 1286 distinct supertags (439 occur once).", "labels": [], "entities": [{"text": "TAG-annotated version of the Penn Treebank", "start_pos": 121, "end_pos": 163, "type": "DATASET", "confidence": 0.7361430128415426}]}, {"text": "As a result, building a robust, broad-coverage TAG parser has proven difficult.", "labels": [], "entities": [{"text": "TAG parser", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9003902971744537}]}, {"text": "In this work, we show that robust supertaggingbased parsing of TAG is indeed possible by using a dense representation of supertags that is induced using neural networks.", "labels": [], "entities": [{"text": "supertaggingbased parsing", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6956906318664551}]}, {"text": "In the first half of the paper, we present a neural network supertagger based on a bi-directional LSTM (BLSTM) architecture, inspired by the work of Xu (2015) and in CCG, and we make crucial use of synchronized dropout (.", "labels": [], "entities": [{"text": "CCG", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.9087532162666321}]}, {"text": "This supertagger achieves the stateof-the-art accuracy on the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9995083808898926}, {"text": "WSJ Penn Treebank", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9312298695246378}]}, {"text": "When combined with an existing TAG chart parser (, the LSTM-based supertagger already yields state-of-the-art unlabeled and labeled attachment scores.", "labels": [], "entities": []}, {"text": "In the second half of the work, we present a shift-reduce parsing model based on a feedforward neural network that makes use of dense supertag embeddings.", "labels": [], "entities": []}, {"text": "Although this approach has much in common with the approach to shiftreduce CCG parsing taken by, it differs in its additive structures in supertag embeddings.", "labels": [], "entities": [{"text": "shiftreduce CCG parsing", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.49551209807395935}]}, {"text": "When a CCG operation combines two supertags (categories), it yields a resulting category that is typically distinct from the two that are combined, and CCG shift-reduce parsers (e.g. Xu) make use of this result to guide subsequent actions.", "labels": [], "entities": []}, {"text": "When the resulting category is the same as some lexical category assignment (for example when function application over (S\\N P )/N P NP yields S\\N P , the same as an intransitive verb), the parser will benefit from sharing statistics across these contexts.", "labels": [], "entities": []}, {"text": "For TAG however, substitution or adjoining of one elementary tree into another does not change the nature of the elementary tree into which the operation has taken place.", "labels": [], "entities": [{"text": "TAG", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9055770039558411}]}, {"text": "Consequently, the results of partial derivations are not identified with atomic lexical entries, resulting in sparser data.", "labels": [], "entities": []}, {"text": "We propose a solution to this problem for TAG by introducing vector representations that are added to the supertag embedding when an operation has been applied to an elementary tree.", "labels": [], "entities": [{"text": "TAG", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.7735863327980042}]}, {"text": "Not only does this result in a TAG-parser with the best known performance over the WSJ Penn Treebank, but the resulting supertag embeddings turnout to contain linguistically sensible linear structure that we illustrate.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.9581788380940756}]}], "datasetContent": [{"text": "In order to ensure comparability with past work on TAG parsing, we follow the protocol of and, and use the grammar and the TAG-annotated WSJ Penn Tree Bank described in Section 2.", "labels": [], "entities": [{"text": "TAG parsing", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.9541910588741302}, {"text": "WSJ Penn Tree Bank", "start_pos": 137, "end_pos": 155, "type": "DATASET", "confidence": 0.8170690685510635}]}, {"text": "Following that work, we use Sections 01-22 as the training set, Section 00 as the development set, and Section 23 as the test set.", "labels": [], "entities": []}, {"text": "The training, development, and test sets comprise 39832, 1921, and 2415 sentences, respectively.", "labels": [], "entities": [{"text": "1921", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.900215744972229}]}, {"text": "The development set contains 177 sentences with at least one supertag that was absent from the training set.", "labels": [], "entities": []}, {"text": "We implement the networks in TensorFlow (.", "labels": [], "entities": []}, {"text": "During training, we shuffle the order of the sentences in the training set to form mini-batches.", "labels": [], "entities": []}, {"text": "Each mini-batch consists of 100 sentences, except the last which contains 32 sentences.", "labels": [], "entities": []}, {"text": "For supertagging, we first generate predicted POS tags for both the training set and the development set.", "labels": [], "entities": []}, {"text": "The POS-tagger architecture is similar to that of the supertagger shown in, except that, obviously, we do not feed it POS embeddings.", "labels": [], "entities": []}, {"text": "The BLSTMs each contain 128 units, and we do not apply dropout at this stage.", "labels": [], "entities": [{"text": "BLSTMs", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6201642155647278}]}, {"text": "To derive predicted POS tags for the supertagger training set, we perform 10-fold jackknife training over the training set.", "labels": [], "entities": [{"text": "POS", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9373733997344971}]}, {"text": "For the supertagger, each direction of LSTM computation involves two layers, and each LSTM contains 512 units.", "labels": [], "entities": []}, {"text": "The hidden units, layerto-layer, and input units dropout rates are 0.5, 0.5, and 0.2 respectively.", "labels": [], "entities": []}, {"text": "After each training epoch, we test the parser on the development set.", "labels": [], "entities": []}, {"text": "When classification accuracy does not improve on two consecutive epochs, we end the training.", "labels": [], "entities": [{"text": "classification", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.957793653011322}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9736836552619934}]}, {"text": "For the parser, we initialize the supertag embedding matrix E and the substitution memory embedding matrix M according to ).", "labels": [], "entities": []}, {"text": "For all of the experiments reported here, we fix the hyper-parameters as follows: the embedding dimensions d for the supertag and substitution memory embeddings are 50, the number of units is 200 on both of the two hidden layers, and the input dropout rate is 0.2 and the hidden dropout rate is 0.3.", "labels": [], "entities": []}, {"text": "We choose k = 3 or 5 for the stack/buffer scope.", "labels": [], "entities": []}, {"text": "After each training epoch, we test the parser on the development set, and when the greedy accuracy fails to improving on two consecutive epochs, we terminate the training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9678192138671875}]}], "tableCaptions": [{"text": " Table 1: 1-best Supertag Analysis on Section 00. # indi- cates the number of possible classes in a property. The Prec  and Rec columns show macro-averaging precision and recall.  The Acc columns indicate simple accuracy. For a complete  description of the properties, see Chung et al. (2016).", "labels": [], "entities": [{"text": "Supertag Analysis", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8621785044670105}, {"text": "indi- cates", "start_pos": 52, "end_pos": 63, "type": "METRIC", "confidence": 0.9270649353663126}, {"text": "Prec", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9280423521995544}, {"text": "precision", "start_pos": 157, "end_pos": 166, "type": "METRIC", "confidence": 0.9771301746368408}, {"text": "recall", "start_pos": 171, "end_pos": 177, "type": "METRIC", "confidence": 0.9993557333946228}, {"text": "Acc", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9857774376869202}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9966963529586792}]}, {"text": " Table 3: Parsing Results on Section 00. k is # of elements from stack and buffer used as input, B is the beam size. We show  mean and standard deviation over 5 trials with different initialization for each configuration. BLSTM+Chart shows results  obtained by feeding the 1-best supertag inputs from the MICA chart parser discussed in Section 4.3.", "labels": [], "entities": [{"text": "BLSTM+Chart", "start_pos": 222, "end_pos": 233, "type": "DATASET", "confidence": 0.7661784887313843}, {"text": "MICA chart parser", "start_pos": 305, "end_pos": 322, "type": "DATASET", "confidence": 0.7857748667399088}]}, {"text": " Table 4: Section 00 Performance Comparison with Prior Models. The P3 results are from Chung et al. (2016). P3 is based on  the model described in Nivre et al. (2004). * denotes the results with gold POS tags. For the NN parser, k=5 and B=16.", "labels": [], "entities": [{"text": "B", "start_pos": 237, "end_pos": 238, "type": "METRIC", "confidence": 0.9645587205886841}]}, {"text": " Table 5: Supertagging and Parsing Results on Section 23. For  the NN parser, k=5 and B=16 throughout. We trained Syn- taxnet (Andor et al., 2016) with global normalization beam  size 16 using the TensorFlow toolkit.", "labels": [], "entities": [{"text": "Supertagging", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9554190635681152}, {"text": "B", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9381117224693298}, {"text": "Syn- taxnet", "start_pos": 114, "end_pos": 125, "type": "DATASET", "confidence": 0.5916773478190104}]}, {"text": " Table 6: Analogy Task Results.", "labels": [], "entities": []}]}