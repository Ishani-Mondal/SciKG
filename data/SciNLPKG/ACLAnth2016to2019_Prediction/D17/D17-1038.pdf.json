{"title": [{"text": "Learning to select data for transfer learning with Bayesian Optimization", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8916186988353729}]}], "abstractContent": [{"text": "Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.9306804835796356}]}, {"text": "Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks.", "labels": [], "entities": []}, {"text": "Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, part-of-speech tagging, and parsing.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9547508656978607}, {"text": "part-of-speech tagging", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.715967670083046}, {"text": "parsing", "start_pos": 146, "end_pos": 153, "type": "TASK", "confidence": 0.9721500873565674}]}, {"text": "We show the importance of complementing similarity with diversity, and that learned measures are-to some degree-transferable across models, domains, and even tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Processing (NLP) models suffer considerably when applied in the wild.", "labels": [], "entities": []}, {"text": "The distribution of the test data is typically very different from the data used during training, causing a model's performance to deteriorate substantially.", "labels": [], "entities": []}, {"text": "Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far).", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7655231058597565}, {"text": "transfer learning", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9237772822380066}]}, {"text": "However, most work focused on one-toone scenarios.", "labels": [], "entities": []}, {"text": "Only recently research considered using multiple sources.", "labels": [], "entities": []}, {"text": "Such studies are rare and typically rely on specific model transfer approaches.", "labels": [], "entities": []}, {"text": "Inspired by work on curriculum learning, we instead propose-to the best of our knowledge-the first model-agnostic data selection approach to transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.9013417065143585}]}, {"text": "Contrary to curriculum learning that aims at speeding up learning (see \u00a76), we aim at learning to select the most relevant data from multiple sources using data metrics.", "labels": [], "entities": []}, {"text": "While several measures have been proposed in the past, prior work is limited in studying metrics mostly in isolation, using only the notion of similarity) and focusing on a single task (see \u00a76).", "labels": [], "entities": []}, {"text": "Our hypothesis is that different tasks or even different domains demand different notions of similarity.", "labels": [], "entities": []}, {"text": "In this paper we go beyond prior work by i) studying a range of similarity metrics, including diversity; and ii) testing the robustness of the learned weights across models (e.g., whether a more complex model can be approximated with a simpler surrogate), domains and tasks (to delimit the transferability of the learned weights).", "labels": [], "entities": []}, {"text": "The contributions of this work are threefold.", "labels": [], "entities": []}, {"text": "First, we present the first model-independent approach to learn a data selection measure for transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.9455045163631439}]}, {"text": "It outperforms baselines across three tasks and multiple domains and is competitive with state-of-the-art domain adaptation approaches.", "labels": [], "entities": []}, {"text": "Second, prior work on transfer learning mostly focused on similarity.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9441061317920685}]}, {"text": "We demonstrate empirically that diversity is as important asand complements-domain similarity for transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8950686156749725}]}, {"text": "Finally, we show-for the first timeto what degree learned measures transfer across models, domains and tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on three tasks: sentiment analysis, part-of speech (POS) tagging, and dependency parsing.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9735677540302277}, {"text": "part-of speech (POS) tagging", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.579055537780126}, {"text": "dependency parsing", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8397321701049805}]}, {"text": "We use then examples with the highest score as determined by the learned data selection measure for training our models.", "labels": [], "entities": []}, {"text": "We show statistics for all datasets in.", "labels": [], "entities": []}, {"text": "Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset).", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8236043155193329}, {"text": "sentiment analysis", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.9667186439037323}, {"text": "Amazon reviews dataset", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9337767163912455}]}, {"text": "We use tf-idf-weighted unigram and bigram features and a linear SVM classifier ( . We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set.", "labels": [], "entities": []}, {"text": "POS tagging For POS tagging and parsing, we evaluate on the coarse-grained POS data (12 universal POS) of the SANCL 2012 shared task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.760598748922348}, {"text": "POS tagging", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8710824251174927}, {"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.7388968467712402}, {"text": "SANCL 2012 shared task", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.7647532224655151}]}, {"text": "Each domainexcept for WSJ-contains around 2000-5000 labeled sentences and more than 100,000 unlabeled sentences.", "labels": [], "entities": [{"text": "WSJ-contains", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.8320693373680115}]}, {"text": "In the case of WSJ, we use its dev and test data as labeled samples and treat the remaining sections as unlabeled.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.8996303677558899}]}, {"text": "We set n = 2000 for POS tagging and parsing to retain enough examples for the most-similar-domain baseline.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8372624814510345}, {"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.8044609427452087}]}, {"text": "To evaluate the impact of model choice, we compare two models: a Structured Perceptron (inhouse implementation with commonly used features pertaining to tags, words, case, prefixes, as well as prefixes and suffixes) trained for 5 iterations with a learning rate of 0.2; and a state-of-theart Bi-LSTM tagger) with word and character embeddings as input.", "labels": [], "entities": []}, {"text": "We perform early stopping on the validation set with patience of 2 and use otherwise default hyperparameters 3 as provided by the authors.", "labels": [], "entities": [{"text": "patience", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9901190996170044}]}, {"text": "Parsing For parsing, we evaluate the state-ofthe-art Bi-LSTM parser by Kiperwasser and Goldberg (2016) with default hyperparameters.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9730871915817261}]}, {"text": "We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 shared task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.859711080789566}, {"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6813297867774963}, {"text": "SANCL 2012 shared task", "start_pos": 122, "end_pos": 144, "type": "DATASET", "confidence": 0.7688852995634079}]}], "tableCaptions": [{"text": " Table 1: Number of labeled and unlabeled sen- tences for each domain in the Amazon Reviews  dataset (Blitzer et al., 2006) (above) and the  SANCL 2012 dataset (Petrov and McDonald,  2012) for POS tagging and parsing (below).", "labels": [], "entities": [{"text": "Amazon Reviews  dataset", "start_pos": 77, "end_pos": 100, "type": "DATASET", "confidence": 0.9597081343332926}, {"text": "SANCL 2012 dataset", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.8926372130711874}, {"text": "POS tagging", "start_pos": 193, "end_pos": 204, "type": "TASK", "confidence": 0.8187650740146637}]}, {"text": " Table 3: Results for data selection for part-of-speech tagging and parsing domain adaptation on the  SANCL 2012 shared task dataset (Petrov and McDonald, 2012). POS: Part-of-speech tagging. Pars:  Parsing. POS tagging models: Structured Perceptron (P); Bi-LSTM tagger (B) (Plank et al., 2016). Pars- ing model: Bi-LSTM parser (BIST) (Kiperwasser and Goldberg, 2016). Evaluation metrics: Accuracy  (POS tagging); Labeled Attachment Score (parsing). Best: bold; second-best: underlined.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7016657590866089}, {"text": "parsing domain adaptation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.9184122284253439}, {"text": "SANCL 2012 shared task dataset", "start_pos": 102, "end_pos": 132, "type": "DATASET", "confidence": 0.8098449349403382}, {"text": "Part-of-speech tagging", "start_pos": 167, "end_pos": 189, "type": "TASK", "confidence": 0.7177191078662872}, {"text": "Accuracy", "start_pos": 388, "end_pos": 396, "type": "METRIC", "confidence": 0.9954029321670532}, {"text": "Labeled Attachment Score", "start_pos": 413, "end_pos": 437, "type": "METRIC", "confidence": 0.6763184666633606}]}, {"text": " Table 3. Using Bayesian Optimization, we are  able to outperform the baselines with almost all  feature sets, except for a few cases (e.g., diver-", "labels": [], "entities": []}, {"text": " Table 4: Accuracy scores for cross-model transfer of learned data selection weights for part-of-speech  tagging from Structured Perceptron (P proxy ) to Bi-LSTM tagger (B)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9898990988731384}, {"text": "part-of-speech  tagging", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6641406267881393}]}, {"text": " Table 5: Accuracy scores for cross-domain trans- fer of learned data selection weights on Amazon  reviews (Blitzer et al., 2006). D S : target domain  used for learning metric S. B: Book. D: DVD. E:  Electronics. K: Kitchen. Sim: term distribution- based similarity. Div: diversity. Best per feature  set: bold. In-domain results: gray. SDAMS (Wu  and Huang, 2016) listed as comparison.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9919278621673584}]}, {"text": " Table 6: Accuracy scores for cross-domain transfer of learned data selection weights for part-of-speech  tagging with the Structured Perceptron model on the SANCL 2012 shared task dataset (Petrov and Mc-", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9884097576141357}, {"text": "part-of-speech  tagging", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7156939804553986}, {"text": "SANCL 2012 shared task dataset", "start_pos": 158, "end_pos": 188, "type": "DATASET", "confidence": 0.6542293071746826}]}, {"text": " Table 7: Results of cross-task transfer of learned  data selection weights. T S : task used for learn- ing metric S. POS: Part-of-speech tagging. Pars:  Parsing. SA: sentiment analysis. Accuracy scores  for SA and POS; LAS Attachment Score for pars- ing. Models: Structured Perceptron (POS tag- ging); Bi-LSTM parser (Kiperwasser and Gold- berg, 2016) (Pars). Same features as in Table 5.  In-task results: gray. Better than base: underlined.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7386611700057983}, {"text": "Accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9942881464958191}, {"text": "LAS Attachment Score", "start_pos": 220, "end_pos": 240, "type": "METRIC", "confidence": 0.9117447137832642}]}]}