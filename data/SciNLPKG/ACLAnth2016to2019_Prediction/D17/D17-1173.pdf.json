{"title": [], "abstractContent": [{"text": "Very recently, some studies on neural dependency parsers have shown advantage over the traditional ones on a wide variety of languages.", "labels": [], "entities": [{"text": "neural dependency parsers", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6814093391100565}]}, {"text": "However, for graph-based neural dependency parsing systems, they either count on the long-term memory and attention mechanism to implicitly capture the high-order features or give up the global exhaustive inference algorithms in order to harness the features over a rich history of parsing decisions.", "labels": [], "entities": [{"text": "graph-based neural dependency parsing", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.6917559653520584}]}, {"text": "The former might miss out the important features for specific headword predictions without the help of the explicit structural information, and the latter may suffer from the error propagation as false early structural constraints are used to create features when making future predictions.", "labels": [], "entities": []}, {"text": "We explore the feasibility of explicitly taking high-order features into account while remaining the main advantage of global inference and learning for graph-based parsing.", "labels": [], "entities": []}, {"text": "The proposed parser first forms an initial parse tree by head-modifier predictions based on the first-order factorization.", "labels": [], "entities": []}, {"text": "High-order features (such as grandparent, sibling, and uncle) then can be defined over the initial tree, and used to refine the parse tree in an iterative fashion.", "labels": [], "entities": []}, {"text": "Experimental results showed that our model (called INDP) archived competitive performance to existing benchmark parsers on both English and Chinese datasets.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We conducted three sets of experiments.", "labels": [], "entities": []}, {"text": "The first one is to test several variants of the INDP on the development set, to gain some understanding of how the choice of hyper-parameters impacts upon the performance.", "labels": [], "entities": []}, {"text": "The goal of the second one is to see how well the incremental approach enhanced with the high-order features to improve the first-order results by analysing parsing errors relative to sentence length.", "labels": [], "entities": []}, {"text": "In the third set, we compared the performance of the INDP with existing state-of-the-art models on both English and Chinese datasets.", "labels": [], "entities": [{"text": "INDP", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.787632942199707}]}, {"text": "We report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) with punctuations being omitted from evaluation.", "labels": [], "entities": [{"text": "unlabeled attachment scores (UAS)", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.7800142268339793}, {"text": "labeled attachment scores (LAS)", "start_pos": 48, "end_pos": 79, "type": "METRIC", "confidence": 0.8289391746123632}]}, {"text": "We show test results for the proposed model on the English Penn Treebank (PTB), converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter, and the Chinese Penn Treebank (CTB).", "labels": [], "entities": [{"text": "English Penn Treebank (PTB)", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.9442599614461263}, {"text": "Chinese Penn Treebank (CTB)", "start_pos": 183, "end_pos": 210, "type": "DATASET", "confidence": 0.9183079699675242}]}, {"text": "We follow the standard splits of PTB, using section 2-21 for training, section 22 as development set and 23 as test set.", "labels": [], "entities": [{"text": "PTB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.9181492328643799}]}, {"text": "We use POS tags generated from the Stanford POS tagger (; for the Chinese PTB dataset, we use gold word segmentation and POS tags.", "labels": [], "entities": [{"text": "Chinese PTB dataset", "start_pos": 66, "end_pos": 85, "type": "DATASET", "confidence": 0.7929752469062805}]}], "tableCaptions": [{"text": " Table 1: Results on the English PTB dataset.", "labels": [], "entities": [{"text": "English PTB dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.904543399810791}]}, {"text": " Table 2: Results on the Chinese CTB dataset.", "labels": [], "entities": [{"text": "Chinese CTB dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9612782200177511}]}]}