{"title": [{"text": "Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embeddings have attracted much attention recently.", "labels": [], "entities": []}, {"text": "Different from alphabetic writing systems, Chinese characters are often composed of subcharacter components which are also semantically informative.", "labels": [], "entities": []}, {"text": "In this work, we propose an approach to jointly embed Chinese words as well as their characters and fine-grained subcharacter components.", "labels": [], "entities": []}, {"text": "We use three likelihoods to evaluate whether the context words, characters, and components can predict the current target word, and collected 13,253 subcharacter components to demonstrate the existing approaches of decomposing Chinese characters are not enough.", "labels": [], "entities": []}, {"text": "Evaluation on both word similarity and word analogy tasks demonstrates the superior performance of our model.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.7260270416736603}, {"text": "word analogy", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.7689990997314453}]}], "introductionContent": [{"text": "Distributed word representation represents a word as a vector in a continuous vector space and can better uncover both the semantic and syntactic information over traditional one-hot representations.", "labels": [], "entities": []}, {"text": "It has been successfully applied to many downstream natural language processing (NLP) tasks as input features, such as named entity recognition), text classification (, sentiment analysis (, and question answering ().", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 119, "end_pos": 143, "type": "TASK", "confidence": 0.6508539021015167}, {"text": "text classification", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7930707037448883}, {"text": "sentiment analysis", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.9512684643268585}, {"text": "question answering", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.8606601357460022}]}, {"text": "Among many embedding methods), CBOW and Skip-Gram models are very popular due to their simplicity and efficiency, making it feasible to learn good embeddings of words from large scale training corpora (.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.7973717451095581}]}, {"text": "Despite the success and popularity of word embeddings, most of the existing methods treat each word as the minimum unit, which ignores the morphological information of words.", "labels": [], "entities": []}, {"text": "Rare words cannot be well represented when optimizing a cost function related to a rare word and its contexts.", "labels": [], "entities": []}, {"text": "To address this issue, some recent studies ( have investigated how to exploit morphemes or character n-grams to learn better embeddings of English words.", "labels": [], "entities": []}, {"text": "Different from other alphabetic writing systems such as English, written Chinese is logosyllabic, i.e., a Chinese character can be a word on its own or part of a polysyllabic word 1 . The characters themselves are often composed of subcharacter components which are also semantically informative.", "labels": [], "entities": []}, {"text": "The subword items of Chinese words, including characters and subcharacter components, contain rich semantic information.", "labels": [], "entities": []}, {"text": "The characters composing a word can indicate the semantic meaning of the word and the subcharacter components, such as radicals and components themselves being a character, composing a character can indicate the semantic meaning of the character.", "labels": [], "entities": []}, {"text": "The components of characters can be roughly divided into two types: semantic component and phonetic component.", "labels": [], "entities": []}, {"text": "The semantic component indicates the meaning of a character while the phonetic component indicates the sound of a character.", "labels": [], "entities": []}, {"text": "For example, (water) is the semantic component of characters (lake) and (sea), (horse) is the phonetic component of characters (mother) and (scold) where both and are pronounced similar to . Leveraging the subword information such as characters and subcharacter components can enhance Chinese word embeddings with internal morphological semantics.", "labels": [], "entities": []}, {"text": "Some methods have been proposed to incorporate the subword infor-mation for Chinese word embeddings. and proposed methods to enhance Chinese character embeddings with radicals based on C&W model) and word2vec models (,b) respectively.", "labels": [], "entities": []}, {"text": "used Chinese characters to improve Chinese word embeddings and proposed the CWE model to jointly learn Chinese word and character embeddings.", "labels": [], "entities": []}, {"text": "extended the CWE model by exploiting the internal semantic similarity between a word and its characters in a cross-lingual manner.", "labels": [], "entities": []}, {"text": "To combine both the radical-character and character-word compositions, proposed a multi-granularity embedding (MGE) model based on the CWE model, which represents the context as a combination of surrounding words, surrounding characters, and the radicals of the target word.", "labels": [], "entities": []}, {"text": "Particularly, they developed a dictionary of 20,847 characters and 296 radicals.", "labels": [], "entities": []}, {"text": "However, all the above approaches still missed a lot of fine-grained components in Chinese characters.", "labels": [], "entities": []}, {"text": "Formally and historically, radicals are character components used to index Chinese characters in dictionaries.", "labels": [], "entities": []}, {"text": "Although many of the radicals are also semantic components, a character has only one radical, which cannot fully uncover the semantics and structure of the character.", "labels": [], "entities": []}, {"text": "Besides over 200 radicals, there are more than 10,000 components which are also semantically meaningful or phonetically useful.", "labels": [], "entities": []}, {"text": "For example, Chinese character (illuminate, reflect, mirror, picture) has one radical (the corresponding traditional Chinese radical is , meaning fire) and three other components, i.e., (sun), (knife), and (mouth).", "labels": [], "entities": []}, {"text": "proposed using WUBI input method to decompose the Chinese characters into components.", "labels": [], "entities": [{"text": "WUBI", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8762582540512085}]}, {"text": "However, WUBI input method uses rules to group Chinese characters into meaningless clusters which can fit the alphabet based keyboard.", "labels": [], "entities": [{"text": "WUBI", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.7943150401115417}]}, {"text": "The semantics of the components are not straightforwardly meaningful.", "labels": [], "entities": []}, {"text": "In this work, we present a model to jointly learn the embeddings of Chinese words, characters, and subcharacter components.", "labels": [], "entities": []}, {"text": "The learned Chinese word embeddings can leverage the external context co-occurrence information and incorporate rich internal subword semantic information.", "labels": [], "entities": []}, {"text": "Experiments on both word similarity and word analogy tasks demonstrate the effectiveness of our model over previous works.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7651378810405731}, {"text": "word analogy tasks", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.8163123528162638}]}, {"text": "The code and data are available at https://github.com/ HKUST-KnowComp/JWE.", "labels": [], "entities": [{"text": "HKUST-KnowComp/JWE", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7863107522328695}]}], "datasetContent": [{"text": "We quantitatively evaluate the quality of word embeddings learned by our model on word similarity evaluation and word analogy tasks.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7590764363606771}, {"text": "word analogy tasks", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8395843903223673}]}, {"text": "For all models, we used the same parameter settings.", "labels": [], "entities": []}, {"text": "We fixed the word vector dimension to be 200, the window size to be 5, the training iteration to be 100, the initial learning rate to be 0.025, and the subsampling parameter to be 10 \u22124 . Words with frequency less than 5 were ignored during training.", "labels": [], "entities": []}, {"text": "We used 10-word negative sampling for optimization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on word similarity evaluation.  For our JWE model, +c represents the components  feature and +r represents the radicals feature; +p  indicates which subcharacters are used to predict  the target word; +p1 indicates using the surround- ing words' subcharacter features; +p2 indicates us- ing the target word's subcharacter features; +p3  indicates using the subcharacter features of both  the surrounding words and the target word; -n in- dicates only using characters without either com- ponents or radicals.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8042410612106323}]}, {"text": " Table 2: Results on word analogy reasoning. The  configurations are the same of the ones used in", "labels": [], "entities": [{"text": "word analogy reasoning", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8619945645332336}]}]}