{"title": [{"text": "Guided Open Vocabulary Image Captioning with Constrained Beam Search", "labels": [], "entities": [{"text": "Open Vocabulary Image Captioning", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.5645487383008003}]}], "abstractContent": [{"text": "Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7805663347244263}]}, {"text": "This limitation severely hinders the use of these models in real world applications dealing with images in the wild.", "labels": [], "entities": []}, {"text": "We address this problem using a flexible approach that enables existing deep captioning ar-chitectures to take advantage of image tag-gers attest time, without retraining.", "labels": [], "entities": []}, {"text": "Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words.", "labels": [], "entities": []}, {"text": "Using this approach we achieve state of the art results for out-of-domain caption-ing on MSCOCO (and improved results for in-domain captioning).", "labels": [], "entities": []}, {"text": "Perhaps surprisingly , our results significantly outper-form approaches that incorporate the same tag predictions into the learning algorithm.", "labels": [], "entities": []}, {"text": "We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic image captioning is a fundamental task that couples visual and linguistic learning.", "labels": [], "entities": [{"text": "Automatic image captioning", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7156745195388794}]}, {"text": "Recently, models incorporating recurrent neural networks (RNNs) have demonstrated promising results on this challenging task (, leveraging new benchmark datasets such as the MSCOCO dataset ().", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 174, "end_pos": 188, "type": "DATASET", "confidence": 0.8730189502239227}]}, {"text": "However, these datasets are generally only concerned with a relatively small number of objects and interactions.", "labels": [], "entities": []}, {"text": "Unsur- prisingly, models trained on these datasets do not generalize well to out-of-domain images containing novel scenes or objects (.", "labels": [], "entities": []}, {"text": "This limitation severely hinders the use of these models in real world applications dealing with images in the wild.", "labels": [], "entities": []}, {"text": "Although available image-caption training data is limited, many image collections are augmented with ground-truth text fragments such as semantic attributes (i.e., image tags) or object annotations.", "labels": [], "entities": []}, {"text": "Even if these annotations do not exist, they can be generated using (potentially task specific) image taggers () or object detectors, which are easier to scale to new concepts.", "labels": [], "entities": []}, {"text": "In this paper our goal is to incorporate text fragments such as these during caption generation, to improve the quality of resulting captions.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.9534483551979065}]}, {"text": "This goal poses two key challenges.", "labels": [], "entities": []}, {"text": "First, RNNs are generally opaque, and difficult to influence attest time.", "labels": [], "entities": []}, {"text": "Second, text fragments may include words 936 that are not present in the RNN vocabulary.", "labels": [], "entities": [{"text": "RNN vocabulary", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8513548076152802}]}, {"text": "As illustrated in, we address the first challenge (guidance) by using constrained beam search to guarantee the inclusion of selected words or phrases in the output of an RNN, while leaving the model free to determine the syntax and additional details.", "labels": [], "entities": []}, {"text": "Constrained beam search is an approximate search algorithm capable of enforcing any constraints over resulting output sequences that can be expressed in a finite-state machine.", "labels": [], "entities": [{"text": "Constrained beam search", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6547716756661733}]}, {"text": "With regard to the second challenge (vocabulary), empirically we demonstrate that an RNN can successfully generalize from similar words if both the input and output layers are fixed with pretrained word embeddings and then expanded as required.", "labels": [], "entities": []}, {"text": "To evaluate our approach, we use a held-out version of the MSCOCO dataset.", "labels": [], "entities": [{"text": "MSCOCO dataset", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9769678711891174}]}, {"text": "Leveraging image tag predictions from an existing model as constraints, we demonstrate state of the art performance for out-of-domain image captioning, while simultaneously improving the performance of the base model on indomain data.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 134, "end_pos": 150, "type": "TASK", "confidence": 0.7720206379890442}]}, {"text": "Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm ( ).", "labels": [], "entities": []}, {"text": "Furthermore, we attempt the extremely challenging task of captioning the ImageNet classification dataset (.", "labels": [], "entities": [{"text": "ImageNet classification dataset", "start_pos": 73, "end_pos": 104, "type": "DATASET", "confidence": 0.9054921666781107}]}, {"text": "Human evaluations indicate that by leveraging ground truth image labels as constraints, the proportion of captions meeting or exceeding human quality increases from 11% to 22%.", "labels": [], "entities": []}, {"text": "To facilitate future research we release our code and data from the project page 1 .", "labels": [], "entities": []}], "datasetContent": [{"text": "The MSCOCO 2014 captions dataset () contains 123,293 images, split into a 82,783 image training set and a 40,504 image validation set.", "labels": [], "entities": [{"text": "MSCOCO 2014 captions dataset", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9290905743837357}]}, {"text": "Each image is labeled with five humanannotated captions.", "labels": [], "entities": []}, {"text": "In our experiments we follow standard practice and perform only minimal text pre-processing, converting all sentences to lowercase and tokenizing on white space.", "labels": [], "entities": []}, {"text": "It is common practice to filter vocabulary words that occur less than five times in the training set.", "labels": [], "entities": []}, {"text": "However, since our model does not learn word embeddings, vocabulary filtering is not necessary.", "labels": [], "entities": [{"text": "vocabulary filtering", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7492637932300568}]}, {"text": "Avoiding filtering increases our vocabulary from around 8,800 words to 21,689, allowing the model to potentially extract a useful training signal even from rare words and spelling mistakes (which are generally close to the correctly spelled word in embedding space).", "labels": [], "entities": []}, {"text": "In all experiments we use abeam size of 5, and we also enforce the constraint that a single word cannot be predicted twice in a row.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of captions generated using constrained beam search with 1 -4 predicted image  tags used as constraints (Base+T1 -4). Our approach significantly outperforms both the DCC and NOC  models, despite reusing the image tag predictions of the DCC model. Importantly, performance on in- domain data is not degraded but can also improve.", "labels": [], "entities": []}, {"text": " Table 2: F1 scores for mentions of objects not seen during caption training. Our approach (Base+T4)  reuses the top 4 image tag predictions from the DCC model but generates higher F1 scores by interpreting  tag predictions as constraints. All results based on use of the VGG-16 CNN.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.999297022819519}, {"text": "F1", "start_pos": 181, "end_pos": 183, "type": "METRIC", "confidence": 0.9892665147781372}, {"text": "VGG-16 CNN", "start_pos": 272, "end_pos": 282, "type": "DATASET", "confidence": 0.9798220694065094}]}, {"text": " Table 3: In human evaluations our approach lever- aging ground-truth synset labels (Base+Syn) im- proves significantly over the base model (Base)  in both direct comparison and in comparison to  human-generated captions.", "labels": [], "entities": []}]}