{"title": [{"text": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks.", "labels": [], "entities": [{"text": "Transfer", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9587632417678833}]}, {"text": "Ideally, the linguistic levels of morphology , syntax and semantics would benefit each other by being trained in a single model.", "labels": [], "entities": []}, {"text": "We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks.", "labels": [], "entities": []}, {"text": "Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies.", "labels": [], "entities": []}, {"text": "We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks.", "labels": [], "entities": []}, {"text": "Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing , relatedness, and entailment tasks.", "labels": [], "entities": [{"text": "parsing", "start_pos": 114, "end_pos": 121, "type": "TASK", "confidence": 0.6803956031799316}]}], "introductionContent": [{"text": "The potential for leveraging multiple levels of representation has been demonstrated in various ways in the field of Natural Language Processing (NLP).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 117, "end_pos": 150, "type": "TASK", "confidence": 0.6951211790243784}]}, {"text": "For example, Part-Of-Speech (POS) tags are used for syntactic parsers.", "labels": [], "entities": []}, {"text": "The parsers are used to improve higher-level tasks, such as natural language inference) and machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8356572091579437}]}, {"text": "These systems are often pipelines and not trained end-to-end.", "labels": [], "entities": []}, {"text": "Deep NLP models have yet shown benefits from predicting many increasingly complex tasks each at a successively deeper layer.", "labels": [], "entities": []}, {"text": "Existing models often ignore linguistic hierarchies by predicting: Overview of the joint many-task model predicting different linguistic outputs at successively deeper layers.", "labels": [], "entities": []}, {"text": "different tasks either entirely separately or at the same depth).", "labels": [], "entities": []}, {"text": "We introduce a Joint Many-Task (JMT) model, outlined in, which predicts increasingly complex NLP tasks at successively deeper layers.", "labels": [], "entities": []}, {"text": "Unlike traditional pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, by considering linguistic hierarchies.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.8481130301952362}, {"text": "dependency parsing", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7119202762842178}, {"text": "textual entailment", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.6996566355228424}]}, {"text": "We propose an adaptive training and regularization strategy to grow this model in its depth.", "labels": [], "entities": [{"text": "regularization", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9614521861076355}]}, {"text": "With the help of this strategy we avoid catastrophic interference between the tasks.", "labels": [], "entities": []}, {"text": "Our model is motivated by who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer).", "labels": [], "entities": []}, {"text": "Experimental results show that our single model achieves competitive results for all of the five different tasks, demonstrating that us-ing linguistic hierarchies is more important than handling different tasks in the same layer.", "labels": [], "entities": []}], "datasetContent": [{"text": "POS tagging: To train the POS tagging layer, we used the Wall Street Journal (WSJ) portion of Penn Treebank, and followed the standard split for the training (Section 0-18), development (Section 19-21), and test (Section 22-24) sets.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7323114275932312}, {"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8349553942680359}, {"text": "Wall Street Journal (WSJ) portion of Penn Treebank", "start_pos": 57, "end_pos": 107, "type": "DATASET", "confidence": 0.9423686802387238}]}, {"text": "The evaluation metric is the word-level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9229901432991028}]}, {"text": "Chunking: For chunking, we also used the WSJ corpus, and followed the standard split for the training (Section 15-18) and test (Section 20) sets as in the CoNLL 2000 shared task.", "labels": [], "entities": [{"text": "chunking", "start_pos": 14, "end_pos": 22, "type": "TASK", "confidence": 0.9798270463943481}, {"text": "WSJ corpus", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9720179736614227}, {"text": "CoNLL 2000 shared task", "start_pos": 155, "end_pos": 177, "type": "DATASET", "confidence": 0.9153591841459274}]}, {"text": "We used Section 19 as the development set and employed the IOBES tagging scheme.", "labels": [], "entities": [{"text": "Section 19", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9099297821521759}, {"text": "IOBES tagging", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.4766028821468353}]}, {"text": "The evaluation metric is the F1 score defined in the shared task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.98061203956604}]}, {"text": "Dependency parsing: We also used the WSJ corpus for dependency parsing, and followed the standard split for the training (Section 2-21), development (Section 22), and test (Section 23) sets.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8061016201972961}, {"text": "WSJ corpus", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.9786466956138611}, {"text": "dependency parsing", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.890815258026123}]}, {"text": "We obtained Stanford style dependencies using the version 3.3.0 of the Stanford converter.", "labels": [], "entities": []}, {"text": "The evaluation metrics are the Unlabeled Attachment Score (UAS) and the Labeled Attachment Score (LAS), and punctuations are excluded for the evaluation.", "labels": [], "entities": [{"text": "Unlabeled Attachment Score (UAS)", "start_pos": 31, "end_pos": 63, "type": "METRIC", "confidence": 0.810157522559166}, {"text": "Labeled Attachment Score (LAS)", "start_pos": 72, "end_pos": 102, "type": "METRIC", "confidence": 0.7987515578667322}]}, {"text": "Semantic relatedness: For the semantic relatedness task, we used the SICK dataset (Marelli et al.,), and followed the standard split for the training, development, and test sets.", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8048529326915741}]}, {"text": "The evaluation metric is the Mean Squared Error (MSE) between the gold and predicted scores.", "labels": [], "entities": [{"text": "Mean Squared Error (MSE)", "start_pos": 29, "end_pos": 53, "type": "METRIC", "confidence": 0.9647401670614878}]}, {"text": "Textual entailment: For textual entailment, we also used the SICK dataset and exactly the same data split as the semantic relatedness dataset.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7704619765281677}, {"text": "SICK dataset", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.8168549537658691}]}, {"text": "The evaluation metric is the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994019269943237}]}], "tableCaptions": [{"text": " Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.", "labels": [], "entities": []}, {"text": " Table 2: POS tagging results.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7504673600196838}]}, {"text": " Table 5: Semantic relatedness results.", "labels": [], "entities": []}, {"text": " Table 6: Textual entailment results.", "labels": [], "entities": [{"text": "Textual entailment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7979894876480103}]}, {"text": " Table 7: Effectiveness of the Shortcut Connections  (SC) and the Label Embeddings (LE).", "labels": [], "entities": []}, {"text": " Table 8: Effectiveness of using different layers for  different tasks.", "labels": [], "entities": []}, {"text": " Table 9: Effectiveness of the Successive Regular- ization (SR) and the Vertical Connections (VC).", "labels": [], "entities": [{"text": "Successive Regular- ization (SR)", "start_pos": 31, "end_pos": 63, "type": "METRIC", "confidence": 0.5789560462747302}]}, {"text": " Table 10: Effects of the order of training.", "labels": [], "entities": []}, {"text": " Table 11: Effects of depth for the single tasks.", "labels": [], "entities": []}, {"text": " Table 12: Effects of the character embeddings.", "labels": [], "entities": []}]}