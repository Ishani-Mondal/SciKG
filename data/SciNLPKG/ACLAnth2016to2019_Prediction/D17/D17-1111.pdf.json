{"title": [], "abstractContent": [{"text": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.8539463520050049}]}, {"text": "Existing neu-ral approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scala-bility.", "labels": [], "entities": []}, {"text": "We propose instead to cast extrac-tive QA as an iterative search problem: select the answer's sentence, start word, and end word.", "labels": [], "entities": []}, {"text": "This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths.", "labels": [], "entities": []}, {"text": "We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient.", "labels": [], "entities": []}, {"text": "We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow.", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset", "start_pos": 174, "end_pos": 209, "type": "DATASET", "confidence": 0.8063150197267532}, {"text": "F1 dev)", "start_pos": 226, "end_pos": 233, "type": "METRIC", "confidence": 0.9437434474627177}]}, {"text": "We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type.", "labels": [], "entities": []}, {"text": "This method improves the performance of all models considered in this work and is of independent interest fora variety of NLP tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question answering (QA) and information extraction systems have proven to be invaluable in wide variety of applications such as medical information collection on drugs and genes Who was first to recognize that the Analytical Engine had applications beyond pure calculation?", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8682321786880494}, {"text": "information extraction", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7779443562030792}, {"text": "medical information collection on drugs and genes", "start_pos": 128, "end_pos": 177, "type": "TASK", "confidence": 0.8059375371251788}]}, {"text": "Ada Lovelace was known for her work on Charles Babbage's Analytical Engine.", "labels": [], "entities": []}, {"text": "She was the first to recognize that the machine had applications beyond calculation.", "labels": [], "entities": []}, {"text": "Ada: GNR answering a question.", "labels": [], "entities": [{"text": "GNR answering a question", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.7713923454284668}]}, {"text": "It first picks a sentence, then start word, then end word.", "labels": [], "entities": []}, {"text": "Probabilities are global and normalized over the beam.", "labels": [], "entities": []}, {"text": "Model initially picks the wrong sentence, but global normalization lets it recover.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.725252777338028}]}, {"text": "Final prediction's probability (0.64) exceeds sentence pick (0.49), whereas with local normalization each probability is upper bounded by the previous step.", "labels": [], "entities": [{"text": "Final prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.62305648624897}]}, {"text": "Poon, 2016), large scale health impact studies, or educational material development (.", "labels": [], "entities": []}, {"text": "Recent progress in neural-network based extractive question answering models are quickly closing the gap with human performance on several benchmark QA tasks such as SQuAD (, MS MARCO (, or NewsQA ().", "labels": [], "entities": [{"text": "extractive question answering", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6229901909828186}, {"text": "MS MARCO", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.6429789662361145}]}, {"text": "However, current approaches to extractive question answering face several limitations: 1.", "labels": [], "entities": [{"text": "extractive question answering", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.7976027528444926}]}, {"text": "Computation is allocated equally to the entire document, regardless of answer location, with no ability to ignore or focus computation on specific parts.", "labels": [], "entities": []}, {"text": "This limits applicability to longer documents.", "labels": [], "entities": []}, {"text": "2. They rely extensively on expensive bidirectional attention mechanisms ( or must rank all possible answer spans ().", "labels": [], "entities": []}, {"text": "3. While data-augmentation for question answering have been proposed ( , current approaches still do not provide training data that can improve the performance of existing systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9197338819503784}]}, {"text": "In this paper we demonstrate a methodology for addressing these three limitations, and make the following claims: 1.", "labels": [], "entities": []}, {"text": "Extractive Question Answering can be cast as a nested search process, where sentences provide a powerful document decomposition and an easy to learn search step.", "labels": [], "entities": [{"text": "Extractive Question Answering", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.806189219156901}]}, {"text": "This factorization enables conditional computation to be allocated to sentences and spans likely to contain the right answer.", "labels": [], "entities": []}, {"text": "2. When cast as a search process, models without bi-directional attention mechanisms and without ranking all possible answer spans can achieve near state of the art results on extractive question answering.", "labels": [], "entities": [{"text": "extractive question answering", "start_pos": 176, "end_pos": 205, "type": "TASK", "confidence": 0.6128599643707275}]}, {"text": "3. Preserving narrative structure and explicitly incorporating type and question information into synthetic data generation is key to generating examples that actually improve the performance of question answering systems.", "labels": [], "entities": [{"text": "Preserving narrative structure", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.8936563928922018}, {"text": "question answering", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.7922265231609344}]}, {"text": "Our claims are supported by experiments on the SQuAD dataset where we show that the Globally Normalized Reader (GNR), a model that performs an iterative search process through a document (shown visually in), and has computation conditionally allocated based on the search process, achieves near state of the art Exact Match (EM) and F1 scores without resorting to more expensive attention or ranking of all possible spans.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.7655413150787354}, {"text": "Exact Match (EM)", "start_pos": 312, "end_pos": 328, "type": "METRIC", "confidence": 0.9003813743591309}, {"text": "F1 scores", "start_pos": 333, "end_pos": 342, "type": "METRIC", "confidence": 0.9757142663002014}]}, {"text": "Furthermore, we demonstrate that Type Swaps, a type-aware data augmentation strategy that aligns named entities with a knowledge base and swaps them out for new entities that share the same type, improves the performance of all models on extractive question answering.", "labels": [], "entities": [{"text": "extractive question answering", "start_pos": 238, "end_pos": 267, "type": "TASK", "confidence": 0.6876150270303091}]}, {"text": "We structure the paper as follows: in Section 2 we introduce the task and our model.", "labels": [], "entities": []}, {"text": "Section 3 describes our data-augmentation strategy.", "labels": [], "entities": []}, {"text": "Section 4 introduces our experiments and results.", "labels": [], "entities": []}, {"text": "In Section 5 we discuss our findings.", "labels": [], "entities": []}, {"text": "In Section 6 we relate our work to existing approaches.", "labels": [], "entities": []}, {"text": "Conclusions and directions for future work are given in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Top bigrams in K-means (K = 7) clusters of question after Bi-LSTM. We observe emergent  clustering according to question type: e.g. Where\u2192 Cluster 7, Who\u2192 Cluster 3. \"What\" granularity  only observable with more clusters.", "labels": [], "entities": [{"text": "Bi-LSTM", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.7651330232620239}]}, {"text": " Table 3: Impact of Beam Width B", "labels": [], "entities": [{"text": "Impact", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9485654234886169}, {"text": "Beam Width", "start_pos": 20, "end_pos": 30, "type": "TASK", "confidence": 0.8046373128890991}]}, {"text": " Table 4: Impact of Augmentation Sample Size T .", "labels": [], "entities": [{"text": "Augmentation Sample Size T", "start_pos": 20, "end_pos": 46, "type": "TASK", "confidence": 0.8166725039482117}]}, {"text": " Table 5: Impact of Type Swaps on the DCN+", "labels": [], "entities": [{"text": "DCN+", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.9249790906906128}]}]}