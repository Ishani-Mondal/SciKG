{"title": [{"text": "Dependency Grammar Induction with Neural Lexicalization and Big Training Data *", "labels": [], "entities": []}], "abstractContent": [{"text": "We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 132, "end_pos": 160, "type": "TASK", "confidence": 0.8565225601196289}]}, {"text": "We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Va-lence (Jiang et al., 2016).", "labels": [], "entities": []}, {"text": "We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora.", "labels": [], "entities": []}, {"text": "L-NDMV can benefit from big training data and lexicaliza-tion of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammar induction is the task of learning a grammar from a set of unannotated sentences.", "labels": [], "entities": [{"text": "Grammar induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7625637650489807}]}, {"text": "In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (.", "labels": [], "entities": [{"text": "WSJ10 corpus", "start_pos": 118, "end_pos": 130, "type": "DATASET", "confidence": 0.9795463681221008}, {"text": "Wall Street Journal corpus", "start_pos": 136, "end_pos": 162, "type": "DATASET", "confidence": 0.9062161296606064}]}, {"text": "Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy.", "labels": [], "entities": [{"text": "Lexicalized grammar induction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8507399559020996}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.984451413154602}]}, {"text": "The most straightforward approach to encoding lexical information is full lexicalization.", "labels": [], "entities": [{"text": "encoding lexical information", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.8513970176378886}]}, {"text": "A major problem with * This work was supported by the National Natural Science Foundation of China (61503248).", "labels": [], "entities": [{"text": "National Natural Science Foundation of China (61503248)", "start_pos": 54, "end_pos": 109, "type": "DATASET", "confidence": 0.7489129801591238}]}, {"text": "full lexicalization is that the grammar becomes much larger and thus learning is more data demanding.", "labels": [], "entities": []}, {"text": "To mitigate this problem, and used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags.", "labels": [], "entities": []}, {"text": "Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.9479512870311737}]}, {"text": "For example, used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar.", "labels": [], "entities": []}, {"text": "Finally, smoothing techniques can be used to reduce the negative impact of data scarcity.", "labels": [], "entities": []}, {"text": "One example is Neural DMV (NDMV) () which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities.", "labels": [], "entities": []}, {"text": "Inspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9930171966552734}, {"text": "grammar induction", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.7518498301506042}]}, {"text": "We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) () and our lexicalized extension of NDMV (L-NDMV).", "labels": [], "entities": []}, {"text": "We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora.", "labels": [], "entities": []}, {"text": "In comparison, L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when it is enhanced with good model initialization.", "labels": [], "entities": []}, {"text": "The performance of L-NDMV is competitive with the current state-of-the-art.", "labels": [], "entities": []}], "datasetContent": [{"text": "For English, we used the BLLIP corpus 1 in addition to the regular WSJ corpus in our experiments.", "labels": [], "entities": [{"text": "BLLIP corpus 1", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8801584839820862}, {"text": "WSJ corpus", "start_pos": 67, "end_pos": 77, "type": "DATASET", "confidence": 0.964979499578476}]}, {"text": "Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.7393213510513306}, {"text": "WSJ corpus", "start_pos": 81, "end_pos": 91, "type": "DATASET", "confidence": 0.9614973962306976}, {"text": "WSJ test set", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.9695242444674174}]}, {"text": "In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger () to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.6562312841415405}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.8699060678482056}, {"text": "BLLIP corpus", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.9526815116405487}]}, {"text": "We used this subset of BLLIP and section 2-21 of WSJ10 for training, section 22 of WSJ for validation and section 23 of WSJ for testing.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.6887508630752563}, {"text": "WSJ10", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.9736068248748779}, {"text": "WSJ", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9657230973243713}, {"text": "WSJ", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.954926073551178}]}, {"text": "We used training sets of four different sizes: WSJ10 only (5779 sentences) and 20k, 50k, and all sentences from the BLLIP subset.", "labels": [], "entities": [{"text": "WSJ10", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.6974719762802124}, {"text": "BLLIP subset", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.8402047753334045}]}, {"text": "For Chinese, we obtained 4762 sentences for training from Chinese Treebank 6.0 (CTB) after converting data to dependency structures via Penn2Malt) and then stripping off punctuations.", "labels": [], "entities": [{"text": "Chinese Treebank 6.0 (CTB)", "start_pos": 58, "end_pos": 84, "type": "DATASET", "confidence": 0.9558850427468618}, {"text": "Penn2Malt", "start_pos": 136, "end_pos": 145, "type": "DATASET", "confidence": 0.9745813012123108}]}, {"text": "We used the recommended validation and test data split described in the documentation.", "labels": [], "entities": []}, {"text": "We trained the models with different degrees of lexicalization.", "labels": [], "entities": []}, {"text": "We control the degree of lexicalization by replacing words that appear less than a cutoff number in the WSJ10 or CTB corpus with their POS tags.", "labels": [], "entities": [{"text": "WSJ10 or CTB corpus", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.8159457147121429}]}, {"text": "For each degree of lexicalization, we tuned the dimension of the hidden layer of the neural network on the validation dataset.", "labels": [], "entities": []}, {"text": "For English, we tested nine word cutoff numbers: 100000, 500, 200, 100, 80, 70, 60, 50, and 40, which resulted in vocabulary sizes of, and 390 respectively; for Chinese, the word cutoff numbers are 100000, 100, 70, 50, 40, 30, 20, 12, and 10.", "labels": [], "entities": []}, {"text": "Ideally, with higher degrees of lexicalization, the hidden layer dimension should be larger in order to accommodate the increased number of tokens.", "labels": [], "entities": []}, {"text": "For the neural network of L-NDMV, we initialized the word and tag vectors in the neu- ral network by learning a CBOW model using the Gensim package.", "labels": [], "entities": [{"text": "Gensim package", "start_pos": 133, "end_pos": 147, "type": "DATASET", "confidence": 0.858723521232605}]}, {"text": "We set the dimension of input and output word vectors to 100 and the dimension of input and output tag vectors to 20.", "labels": [], "entities": []}, {"text": "We trained the neural network with learning rate 0.03, mini-batch size 200 and momentum 0.9.", "labels": [], "entities": [{"text": "learning rate 0.03", "start_pos": 35, "end_pos": 53, "type": "METRIC", "confidence": 0.9584241112073263}, {"text": "momentum", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9766163229942322}]}, {"text": "Because some of the neural network weights are randomly initialized, the model converges to a different local minimum in each run of the learning algorithm.", "labels": [], "entities": []}, {"text": "Therefore, for each setup we ran our learning algorithm for three times and reported the average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9979448914527893}]}, {"text": "More detail of the experimental setup can be found in the supplementary material.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of recent grammar induction  systems.", "labels": [], "entities": []}]}