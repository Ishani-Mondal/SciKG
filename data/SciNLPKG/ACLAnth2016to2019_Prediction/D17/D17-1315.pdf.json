{"title": [{"text": "CharManteau: Character Embedding Models For Portmanteau Creation", "labels": [], "entities": []}], "abstractContent": [{"text": "Portmanteaus area word formation phenomenon where two words are combined to form anew word.", "labels": [], "entities": [{"text": "word formation", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7205145806074142}]}, {"text": "We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information.", "labels": [], "entities": [{"text": "portmanteau generation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.910052478313446}]}, {"text": "We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model.", "labels": [], "entities": []}, {"text": "This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task.", "labels": [], "entities": []}, {"text": "Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9590562582015991}]}], "introductionContent": [{"text": "Portmanteaus (or lexical blends) are novel words formed from parts of multiple root words in order to refer to anew concept which can't otherwise be expressed concisely.", "labels": [], "entities": []}, {"text": "Portmanteaus have become frequent in modern-day social media, news reports and advertising, one popular example being Brexit (Britain + Exit)..", "labels": [], "entities": [{"text": "Brexit (Britain + Exit).", "start_pos": 118, "end_pos": 142, "type": "DATASET", "confidence": 0.794457753499349}]}, {"text": "These are found not only in English but many other languages such as Bahasa Indonesia, Modern Hebrew BatEl; and.", "labels": [], "entities": []}, {"text": "Their short length makes them ideal for headlines and brandnames.", "labels": [], "entities": []}, {"text": "Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation * * denotes equal contribution: A sketch of our BACKWARD, noisychannel model.", "labels": [], "entities": [{"text": "BACKWARD", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.5548815727233887}]}, {"text": "The attentional S2S model with bidirectional encoder gives P (x|y) and nextcharacter model gives P (y), where y (spime) is the portmanteau and x = concat(x (1) , \";\", x ) are the concatenated root words (space and time). is difficult to capture using a set of rules.", "labels": [], "entities": []}, {"text": "For instance, state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head.", "labels": [], "entities": []}, {"text": "An existing work by aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches.", "labels": [], "entities": [{"text": "predicting portmanteau", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.8716932833194733}]}, {"text": "Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages.", "labels": [], "entities": []}, {"text": "Prior works, such as, have demonstrated the efficacy of neural approaches for morphological tasks such as inflection.", "labels": [], "entities": []}, {"text": "We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple FSTs used in the previous work, and (2) automatically capture features such as phonetic similarity through the use of character embeddings, removing the need for explicit grapheme-to-phoneme prediction.", "labels": [], "entities": [{"text": "grapheme-to-phoneme prediction", "start_pos": 292, "end_pos": 322, "type": "TASK", "confidence": 0.7456150949001312}]}, {"text": "To test these hypotheses, in this paper, we propose a neural S2S model to predict portmanteaus given the two root words, specifically making 3 major contributions: \u2022 We propose an S2S model that attends to the two input words to generate portmanteaus, and an additional improvement that leverages noisy-channel-style modelling to incorporate a language model over the vocabulary of words ( \u00a72).", "labels": [], "entities": []}, {"text": "\u2022 Instead of using the model to directly predict output character-by-character, we use the features of portmanteaus to exhaustively generate candidates, making scoring using the noisy channel model possible ( \u00a73).", "labels": [], "entities": []}, {"text": "\u2022 We curate and share anew and larger dataset of 1624 portmanteaus ( \u00a74).", "labels": [], "entities": []}, {"text": "In experiments ( \u00a75), our model performs better than the baseline Deri and Knight (2015) on both objective and subjective measures, demonstrating that such methods can be used effectively in a morphological task.", "labels": [], "entities": []}], "datasetContent": [{"text": "The existing dataset by  In this section, we show results comparing various configurations of our model to the baseline FST model of Deri and Knight (2015) (BASELINE).", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9562333822250366}]}, {"text": "Models are evaluated using exactmatches (Matches) and average Levenshtein editdistance (Distance) w.r.t ground truth.", "labels": [], "entities": [{"text": "exactmatches", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.995955228805542}, {"text": "Levenshtein editdistance (Distance) w.r.t ground truth", "start_pos": 62, "end_pos": 116, "type": "METRIC", "confidence": 0.8800294771790504}]}, {"text": "In Experiment 1, we follow the same setup as.", "labels": [], "entities": []}, {"text": "D Wiki is split into 10 folds.", "labels": [], "entities": [{"text": "D Wiki", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.939602255821228}]}, {"text": "Each fold model uses 8 folds for training, 1 for validation, and 1 for test.", "labels": [], "entities": []}, {"text": "The average (10 fold crossvalidation style approach) performance metrics on the test fold are then evaluated.", "labels": [], "entities": []}, {"text": "We believe this is because portmanteaus have high fidelity towards their root word characters and its critical that the model can observe all root sequence characters, which attention manages to do as shown in.", "labels": [], "entities": []}, {"text": "On inspecting outputs, we observed that often output from our system seemed good in spite of high edit distance from ground truth.", "labels": [], "entities": []}, {"text": "Such aspect of an output seeming good is not captured satisfactorily by measures like edit distance.", "labels": [], "entities": []}, {"text": "To compare the errors made by our model to the baseline, we designed and conducted a human evaluation task on AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7112452387809753}]}, {"text": "In the survey, we show human annotators outputs from our system and that of the baseline.", "labels": [], "entities": []}, {"text": "We ask them to judge which alternative is better overall based on following criteria: 1.", "labels": [], "entities": []}, {"text": "It is a good shorthand for two original words 2.", "labels": [], "entities": []}, {"text": "We requested annotation on a scale of 1-4.", "labels": [], "entities": []}, {"text": "To avoid ordering bias, we shuffled the order of two portmanteau between our system and that of baseline.", "labels": [], "entities": []}, {"text": "We restrict annotators to be from Anglophone countries, have HIT Approval Rate > 80% and pay 0.40$ per HIT (5 Questions per HIT).", "labels": [], "entities": [{"text": "HIT Approval Rate", "start_pos": 61, "end_pos": 78, "type": "METRIC", "confidence": 0.8008599082628886}]}, {"text": "As seen in, output from our system was labelled better by humans as compared to the baseline 58.12% of the time.", "labels": [], "entities": []}, {"text": "shows outputs from different models fora few examples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 10-Fold Cross-Validation results, D Wiki .  Attn, Ens, Init denote attention, ensembling, and  initializing character embeddings respectively.", "labels": [], "entities": []}, {"text": " Table 2: Results on D Blind (1223 Examples). In  general, BACKWARD architecture performs better  than FORWARD architecture.", "labels": [], "entities": [{"text": "BACKWARD", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.6711786389350891}, {"text": "FORWARD", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.8896505832672119}]}, {"text": " Table 3: Example outputs from different models  (Refer to appendix for more examples)", "labels": [], "entities": []}, {"text": " Table 4: AMT annotator judgements on whether  our system's proposed portmanteau is better or  worse compared to the baseline", "labels": [], "entities": [{"text": "AMT annotator", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7707778513431549}]}]}