{"title": [{"text": "Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging", "labels": [], "entities": [{"text": "Reporting Score Distributions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8503735860188802}, {"text": "Sequence Tagging", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.9218799769878387}]}], "abstractContent": [{"text": "In this paper we show that reporting a single performance score is insufficient to compare non-deterministic approaches.", "labels": [], "entities": []}, {"text": "We demonstrate for common sequence tagging tasks that the seed value for the random number generator can result in statistically significant (p < 10 \u22124) differences for state-of-the-art systems.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7096021473407745}]}, {"text": "For two recent systems for NER, we observe an absolute difference of one percentage point F 1-score depending on the selected seed value, making these systems perceived either as state-of-the-art or mediocre.", "labels": [], "entities": [{"text": "NER", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9726380109786987}]}, {"text": "Instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions.", "labels": [], "entities": []}, {"text": "Based on the evaluation of 50.000 LSTM-networks for five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7017323672771454}]}, {"text": "The full experimental results are published in (Reimers and Gurevych, 2017).", "labels": [], "entities": []}], "introductionContent": [{"text": "Large efforts are spent in our community on developing new state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "To document that those approaches are better, they are applied to unseen data and the obtained performance score is compared to previous approaches.", "labels": [], "entities": []}, {"text": "In order to make results comparable, a provided split between train, development and test data is often used, for example from a former shared task.", "labels": [], "entities": []}, {"text": "In recent years, deep neural networks were shown to achieve state-of-the-art performance fora wide range of NLP tasks, including many sequence tagging tasks, dependency parsing (, and machine translation ().", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7583528757095337}, {"text": "dependency parsing", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.8627561926841736}, {"text": "machine translation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.82403963804245}]}, {"text": "The training process for neural networks is highly non-deterministic as it usually depends on a random weight initialization, a random shuffling of the training data for each epoch, and repeatedly applying random dropout masks.", "labels": [], "entities": []}, {"text": "The error function of a neural network is a highly non-convex function of the parameters with the potential for many distinct local minima (.", "labels": [], "entities": []}, {"text": "Depending on the seed value for the pseudo-random number generator, the network will converge to a different local minimum.", "labels": [], "entities": []}, {"text": "Our experiments show that these different local minima have vastly different characteristics on unseen data.", "labels": [], "entities": []}, {"text": "For the recent NER system by we observed that, depending on the random seed value, the performance on unseen data varies between 89.99% and 91.00% F 1 -score.", "labels": [], "entities": [{"text": "NER", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.6710727214813232}, {"text": "F 1 -score", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9806659817695618}]}, {"text": "The difference between the best and worst performance is statistically significant (p < 10 \u22124 ) using a randomization test . In conclusion, whether this newly developed approach is perceived as state-of-the-art or as mediocre, largely depends on which random seed value is selected.", "labels": [], "entities": []}, {"text": "This issue is not limited to this specific approach, but potentially applies to all approaches with non-deterministic training processes.", "labels": [], "entities": []}, {"text": "This large dependence on the random seed value creates several challenges when evaluating new approaches: \u2022 Observing a (statistically significant) improvement through anew non-deterministic approach might not be the result of a superior approach, but the result of having a more favorable sequence of random numbers.", "labels": [], "entities": []}, {"text": "\u2022 Promising approaches might be rejected too early, as they fail to deliver an outperformance simply due to a less favorable sequence of random numbers.", "labels": [], "entities": []}, {"text": "\u2022 Reproducing results is difficult.", "labels": [], "entities": [{"text": "Reproducing", "start_pos": 2, "end_pos": 13, "type": "TASK", "confidence": 0.9595859050750732}]}, {"text": "To study the impact of the random seed value on the performance we will focus on five linguistic sequence tagging tasks: POS-tagging, Chunking, Named Entity Recognition, Entity Recognition 4 , and Event Detection.", "labels": [], "entities": [{"text": "linguistic sequence tagging", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6636171936988831}, {"text": "Entity Recognition", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7522805035114288}, {"text": "Event Detection", "start_pos": 197, "end_pos": 212, "type": "TASK", "confidence": 0.7963931858539581}]}, {"text": "Further we will focus on Long-Short-Term-Memory (LSTM) Networks), as those demonstrated state-of-the-art performance fora wide variety of sequence tagging tasks (.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.7391521334648132}]}, {"text": "Fixing the random seed value would solve the issue with the reproducibility, however, there is no justification for choosing one seed value over another seed value.", "labels": [], "entities": []}, {"text": "Hence, instead of reporting and comparing a single performance, we show that comparing score distributions can lead to new insights into the functioning of algorithms.", "labels": [], "entities": []}, {"text": "Our main contributions are: 1.", "labels": [], "entities": []}, {"text": "Showing the implications of non-deterministic approaches on the evaluation of approaches and the requirement to compare score distributions instead of single performance scores.", "labels": [], "entities": []}, {"text": "2. Comparison of two recent, state-of-the-art systems for NER and showing that reporting a single performance score can be misleading.", "labels": [], "entities": [{"text": "NER", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8899606466293335}]}, {"text": "3. In-depth analysis of different LSTMarchitectures for five sequence tagging tasks with respect to: superior performance, stability of results, and importance of tuning parameters.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7004608064889908}, {"text": "stability", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.968475341796875}]}, {"text": "Entity Recognition labels all tokens that refer to an entity in a sentence, also generic phrases like U.S. president.", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6551859825849533}]}], "datasetContent": [{"text": "Two recent, state-of-the-art systems for NER are proposed by and by . Lample et al. report an F 1 -score of 90.94% and Ma and Hovy report an F 1 -score of 91.21%.", "labels": [], "entities": [{"text": "NER", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9669597744941711}, {"text": "F 1 -score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9885808527469635}, {"text": "F 1 -score", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.987192839384079}]}, {"text": "Ma and Hovy draw the conclusion that their system achieves a significant improvement over the system by Lample et al.", "labels": [], "entities": []}, {"text": "We re-ran both implementations multiple times, each time only changing the seed value of the random number generator.", "labels": [], "entities": []}, {"text": "We ran the Ma and Hovy system 86 times and the Lample et al. system, due to its high computational requirement, for 41 times.", "labels": [], "entities": []}, {"text": "The score distribution is depicted as a violin plot in.", "labels": [], "entities": []}, {"text": "Using a Kolmogorov-Smirnov significance test, we observe a statistically significant difference between these two distributions (p < 0.01).", "labels": [], "entities": []}, {"text": "The plot reveals that the quartiles for the Lample et al. system are above those of the Ma and Hovy system.", "labels": [], "entities": [{"text": "Lample et al. system", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.8788293153047562}]}, {"text": "Further it reveals a smaller standard deviation \u03c3 of the F 1 -5 https://github.com/XuezheMax/ LasagneNLP 6 https://github.com/glample/tagger scores for the Lample et al. system.", "labels": [], "entities": [{"text": "standard deviation \u03c3", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9202274084091187}, {"text": "F 1 -5", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.7956441044807434}]}, {"text": "Using a BrownForsythe test, the standard deviations are different with p < 0.05.", "labels": [], "entities": [{"text": "BrownForsythe test", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8965019583702087}]}, {"text": "shows the minimum, the maximum, and the median performance for the test performances.", "labels": [], "entities": []}, {"text": "Based on this observation, we draw the conclusion that the system by Lample et al. outperforms the system by Ma and Hovy, as their implementation achieves a higher score distribution and shows a lower standard deviation.", "labels": [], "entities": []}, {"text": "Ina usual setup, approaches would be compared on a development set and the run with the highest development score would be used for unseen data, i.e. be used to report the test performance.", "labels": [], "entities": []}, {"text": "For the Lample et al. system we observe a Spearman's rank correlation between the development and the test score of \u03c1 = 0.229.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 42, "end_pos": 69, "type": "METRIC", "confidence": 0.9338159114122391}]}, {"text": "This indicates a weak correlation and that the performance on the development set is not a reliable indicator.", "labels": [], "entities": []}, {"text": "Using the run with the best development score (94.44%) would yield a test performance of mere 90.31%.", "labels": [], "entities": []}, {"text": "Using the second best run on development set (94.28%), would yield state-of-the-art performance with 91.00%.", "labels": [], "entities": []}, {"text": "This difference is statistically significant (p < 0.002).", "labels": [], "entities": []}, {"text": "In conclusion, a development set will not necessarily solve the issue with bad local minima.", "labels": [], "entities": []}, {"text": "The main difference between these two approaches is in the generation of character-based representations: Ma and Hovy uses a Convolutional Neural Network (CNN) (  In the next step, we evaluated the impact of the random seed value for the five sequence tagging tasks described in section 4.", "labels": [], "entities": [{"text": "sequence tagging tasks", "start_pos": 243, "end_pos": 265, "type": "TASK", "confidence": 0.757996141910553}]}, {"text": "We sampled randomly 1830 different configurations, for example different numbers of recurrent units, and ran the network twice, each time with a different seed value.", "labels": [], "entities": []}, {"text": "The results are depicted in.", "labels": [], "entities": []}, {"text": "The largest difference was observed for the ACE 2005 Entities dataset: Using one seed value, the network achieved an F 1 performance of 82.5% while using another seed value, the network achieved a performance of only 74.3%.", "labels": [], "entities": [{"text": "ACE 2005 Entities dataset", "start_pos": 44, "end_pos": 69, "type": "DATASET", "confidence": 0.9705900698900223}, {"text": "F 1 performance", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.9772698879241943}]}, {"text": "Even though this is a rare extreme case, the median difference between different weight initializations is still large.", "labels": [], "entities": []}, {"text": "For example for the CoNLL 2003 NER dataset, the median difference is at 0.38% and the 95th percentile is at 1.08%.", "labels": [], "entities": [{"text": "CoNLL 2003 NER dataset", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.932294949889183}, {"text": "median difference", "start_pos": 48, "end_pos": 65, "type": "METRIC", "confidence": 0.9528667628765106}]}, {"text": "In conclusion, if the fact of different local minima is not taken care of and single performance scores are compared, there is a high chance of drawing false conclusions and either rejecting promising approaches or selecting weaker approaches due to a more or less favorable sequence of random numbers.", "labels": [], "entities": []}, {"text": "In order to find LSTM-network architectures that perform robustly on different tasks, we selected five classical NLP tasks as benchmark tasks: Partof-Speech tagging (POS), Chunking, Named Entity Recognition (NER), Entity Recognition (Entities) and Event Detection (Events).", "labels": [], "entities": [{"text": "Partof-Speech tagging (POS)", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.8359554290771485}, {"text": "Chunking, Named Entity Recognition (NER)", "start_pos": 172, "end_pos": 212, "type": "TASK", "confidence": 0.7139673568308353}, {"text": "Entity Recognition", "start_pos": 214, "end_pos": 232, "type": "TASK", "confidence": 0.6866354048252106}]}, {"text": "For Part-of-Speech tagging, we use the benchmark setup described by.", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8619903326034546}]}, {"text": "Using the full training set for POS tagging would hinder our ability to detect design choices that are consistently better than others.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8765405118465424}]}, {"text": "The error rate for this dataset is approximately 3% (, making all improvements above 97% accuracy likely the result of chance.", "labels": [], "entities": [{"text": "error rate", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9886441826820374}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9968686699867249}]}, {"text": "A 97.24% accuracy was achieved by For the POS-task, we report accuracy and for the other tasks we report the F 1 -score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9989981055259705}, {"text": "POS-task", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.6321336030960083}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.991950511932373}, {"text": "F 1 -score", "start_pos": 109, "end_pos": 119, "type": "METRIC", "confidence": 0.9860595613718033}]}, {"text": "We have shown in section 3 that re-running nondeterministic approaches multiple times and comparing score distributions is essential to draw correct conclusions.", "labels": [], "entities": []}, {"text": "However, to truly understand the capabilities of an approach, it is interesting to test the approach with different sets of hyperparameters for the complete network.", "labels": [], "entities": []}, {"text": "Training and tuning a neural network can be time consuming, sometimes taking multiple days to train a single instance of a network.", "labels": [], "entities": []}, {"text": "A priori it is hard to know which hyperparameters will yield the best performance and the selection of the parameters often makes the difference between mediocre and state-of-the-art performance (.", "labels": [], "entities": []}, {"text": "If an approach yields good performance only fora narrow set of parameters, it might be difficult to adapt the approach to new tasks, new domains or new languages, as a large range of possible parameters must be evaluated, each time requiring a significant amount of training time.", "labels": [], "entities": []}, {"text": "Hence it is desirable, that the approach yields stable results fora wide range of parameters.", "labels": [], "entities": []}, {"text": "In order to find approaches that result in high performance and are robust against the remaining parameters, we decided to randomly sample several hundred network configurations from the set described in section 4.2.", "labels": [], "entities": []}, {"text": "For each sampled configuration, we compare different options, e.g. different options for the last layer of the network.", "labels": [], "entities": []}, {"text": "For example, we sampled in total 975 configurations and each configuration was trained with a Softmax classifier as well as with a CRF classifier, totaling to 1950 trained networks.", "labels": [], "entities": []}, {"text": "Our results are presented in.", "labels": [], "entities": []}, {"text": "The table shows that for the NER task 232 configurations were sampled randomly and for 210 of the 232 configurations (90.5%), the CRF setup achieved a better test performance than the setup with a Softmax classifier.", "labels": [], "entities": [{"text": "NER task 232", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.7885083357493082}]}, {"text": "To measure the difference between these two options, we compute the median of the absolute differences: Let Si be the test performance (F 1 -measure) for the Softmax setup for configuration i and Ci the test performance for the CRF setup.", "labels": [], "entities": [{"text": "F 1 -measure)", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.9734994292259216}]}, {"text": "We then compute \u2206F 1 = median(S 1 \u2212 C 1 , S 2 \u2212 C 2 , . .", "labels": [], "entities": [{"text": "median", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9603685736656189}]}, {"text": ", S 232 \u2212 C 232 ).", "labels": [], "entities": []}, {"text": "For the NER task, the median difference was \u2206F 1 = \u22120.66%, i.e. the setup with a Softmax classifier achieved on average an F 1 -score of 0.66 percentage points below that of the CRF setup.", "labels": [], "entities": [{"text": "NER task", "start_pos": 8, "end_pos": 16, "type": "TASK", "confidence": 0.9268791079521179}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9903128445148468}, {"text": "F 1 -score", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9327698051929474}]}, {"text": "We also evaluated the standard deviation of the F 1 -scores to detect approaches that are less dependent on the remaining hyperparameters and the random number generator.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9708407521247864}]}, {"text": "The standard deviation \u03c3 for the CRF-classifier is with 0.0060 significantly lower (p < 10 \u22123 using Brown-Forsythe test) than for the Softmax classifier with \u03c3 = 0.0082.", "labels": [], "entities": [{"text": "standard deviation \u03c3", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8500712712605795}]}], "tableCaptions": [{"text": " Table 1: The system by Ma and Hovy (2016) and Lample et al. (2016) were run multiple times with  different seed values.", "labels": [], "entities": []}, {"text": " Table 2: The table depicts the median, the 95th percentile and the maximum difference between networks  with the same hyperparameters but different random seed values.", "labels": [], "entities": []}, {"text": " Table 3: Percentages of configurations where Soft- max or CRF classifiers demonstrated a higher test  performance.", "labels": [], "entities": []}, {"text": " Table 4: Randomly sampled configurations were evaluated with 8 possible word embeddings. 108  configurations were sampled for POS, 97 for Chunking, 110 for NER, 119 for Entities, and 124 for Events.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of not using character-based  representations and using CNNs (Ma and Hovy,  2016) or LSTMs (Lample et al., 2016) to derive  character-based representations. 225 configura- tions were sampled for POS, 241 for Chunking,  217 for NER, 228 for Entities, and 219 for Events.", "labels": [], "entities": []}]}