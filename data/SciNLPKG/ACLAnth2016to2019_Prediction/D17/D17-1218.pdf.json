{"title": [{"text": "What is the Essence of a Claim? Cross-Domain Claim Identification", "labels": [], "entities": [{"text": "Cross-Domain Claim Identification", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.5161582926909128}]}], "abstractContent": [{"text": "Argument mining has become a popular research area in NLP.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8526999056339264}]}, {"text": "It typically includes the identification of argumentative components , e.g. claims, as the central component of an argument.", "labels": [], "entities": []}, {"text": "We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently.", "labels": [], "entities": []}, {"text": "To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion.", "labels": [], "entities": []}, {"text": "While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.", "labels": [], "entities": [{"text": "cross-domain classification", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7798988819122314}]}], "introductionContent": [{"text": "The key component of an argument is the claim.", "labels": [], "entities": []}, {"text": "This simple observation has not changed much since the early works on argumentation by Aristotle more than two thousand years ago, although argumentation scholars provide us with a plethora of often clashing theories and models).", "labels": [], "entities": []}, {"text": "Despite the lack of a precise definition in the contemporary argumentation theory, Toulmin's influential work on argumentation in the 1950's introduced a claim as an 'assertion that deserves our attention'; recent works describe a claim as 'a statement that is in dispute and that we are trying to support with reasons'.", "labels": [], "entities": []}, {"text": "Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8224219679832458}, {"text": "manual argumentation analysis", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.6961818933486938}]}, {"text": "'Mining' arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims-the main focus of this article.", "labels": [], "entities": [{"text": "parsing argument structures", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.872883935769399}]}, {"text": "Claim identification itself is an important prerequisite for applications such as fake checking), politics and legal affairs (, and science.", "labels": [], "entities": [{"text": "Claim identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8368694186210632}, {"text": "fake checking", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.7340600937604904}]}, {"text": "Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays, less homogeneous resources, for instance online discourse, pose challenges to current systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9936143755912781}]}, {"text": "Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents, microtexts, Wikipedia articles ( or student essays.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8603077828884125}]}, {"text": "The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored.", "labels": [], "entities": []}, {"text": "This situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.", "labels": [], "entities": []}, {"text": "We first review and qualitatively analyze six existing publicly available datasets for argument mining ( \u00a73), showing that the conceptualizations of claims in these datasets differ largely.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.8514294326305389}]}, {"text": "Ina next step, we analyze the influence of these differences for crossdomain claim identification.", "labels": [], "entities": [{"text": "crossdomain claim identification", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.829299787680308}]}, {"text": "We propose several computational models for claim identification, including systems using linguistically motivated features ( \u00a74.1) and recent deep neural networks ( \u00a74.2), and rigorously evaluate them on and across all datasets ( \u00a75).", "labels": [], "entities": [{"text": "claim identification", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8093778192996979}]}, {"text": "Finally, in order to better understand the factors influencing the performance in a cross-domain scenario, we perform an extensive quantitative analysis on the results ( \u00a76).", "labels": [], "entities": []}, {"text": "Our analysis reveals that despite obvious differences in conceptualizations of claims across datasets, there are some shared properties on the lexical level which can be useful for claim identification in heterogeneous or unknown domains.", "labels": [], "entities": [{"text": "claim identification", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7461583912372589}]}, {"text": "Furthermore, we found that the choice of the source (training) domain is crucial when the target domain is unknown.", "labels": [], "entities": []}, {"text": "We release our experimental framework to help other researchers build upon our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "The AraucariaDB corpus () includes various genres (VG) such as newspaper editorials, parliamentary records, or judicial summaries.", "labels": [], "entities": [{"text": "AraucariaDB corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.827407956123352}]}, {"text": "The annotation scheme structures arguments as trees and distinguishes between claims and premises at the clause level.", "labels": [], "entities": []}, {"text": "Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining ().", "labels": [], "entities": [{"text": "argument mining", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.8823074102401733}]}, {"text": "The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated with claims and premises as well as backings, rebuttals and refutations (\u03b1 U 0.48) inspired by Toulmin's model of argument.", "labels": [], "entities": []}, {"text": "The persuasive essay (PE) corpus (Stab and Gurevych, 2017) includes 402 student essays.", "labels": [], "entities": []}, {"text": "The scheme comprises major claims, claims and premises at the clause level (\u03b1 U 0.77).", "labels": [], "entities": []}, {"text": "The corpus has been extensively used in the argument mining community (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.780306875705719}]}, {"text": "Biran and Rambow (2011a) annotated claims and premises in online comments (OC) from blog threads of LiveJournal (\u03ba 0.69).", "labels": [], "entities": [{"text": "LiveJournal (\u03ba 0.69)", "start_pos": 100, "end_pos": 120, "type": "DATASET", "confidence": 0.8199271321296692}]}, {"text": "Ina subsequent work, Biran and Rambow (2011b) applied their annotation scheme to documents from Wikipedia talk pages (WTP) and annotated 118 threads.", "labels": [], "entities": []}, {"text": "For our experiments, we consider each user comment in both corpora as a document, which yields 2, 805 documents in the OC corpus and 1, 985 documents in the WTP corpus.", "labels": [], "entities": [{"text": "OC corpus", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.8905817866325378}, {"text": "WTP corpus", "start_pos": 157, "end_pos": 167, "type": "DATASET", "confidence": 0.9015043675899506}]}, {"text": "Peldszus and Stede (2016) created a corpus of German microtexts (MT) of controlled linguistic and rhetoric complexity.", "labels": [], "entities": []}, {"text": "Each document includes a single argument and does not exceed five argument components.", "labels": [], "entities": []}, {"text": "The scheme models the argument structure and distinguishes between premises and claims, among other properties (such as proponent/opponent or normal/example).", "labels": [], "entities": []}, {"text": "In the first annotation study, 26 untrained annotators annotated 23 microtexts in a classroom experiment (\u03ba 0.38)).", "labels": [], "entities": []}, {"text": "Ina subsequent work, the corpus was largely extended by expert annotators (\u03ba 0.83).", "labels": [], "entities": []}, {"text": "Recently, they translated the corpus to English, resulting in the first parallel corpus in computational argumentation; our experiments rely on the English version.", "labels": [], "entities": []}, {"text": "The performance of the learners is quite divergent across datasets, with Macro-F 1 scores 6 ranging from 60% (WTP) to 80% (MT), average 67% (see).", "labels": [], "entities": []}, {"text": "On all datasets, our best systems clearly outperform both baselines.", "labels": [], "entities": []}, {"text": "In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help inmost cases.", "labels": [], "entities": []}, {"text": "Discourse features only contribute significantly on MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9232826232910156}]}, {"text": "When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information.", "labels": [], "entities": []}, {"text": "The average performances of LR \u2212syntax and CNN:rand are virtually identical, both for Macro- Described as Fscore Min.", "labels": [], "entities": [{"text": "CNN:rand", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8625356952349345}]}, {"text": "F 1 and Claim-F 1 , with a slight advantage for the feature-based approach, but their difference is not statistically significant (p \u2264 0.05).", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9713592827320099}, {"text": "Claim-F 1", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9435793459415436}]}, {"text": "Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p \u2264 0.05).", "labels": [], "entities": []}, {"text": "The absence or the different nature of inter-annotator agreement measures for all datasets prevent us from searching for correlations between agreement and performance.", "labels": [], "entities": []}, {"text": "But we observed that the systems yield better results on PE and MT, both datasets with good inter-annotator agreement (\u03b1 u = 0.77 for PE and \u03ba = 0.83 for MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8787658214569092}, {"text": "MT", "start_pos": 154, "end_pos": 156, "type": "TASK", "confidence": 0.6910082101821899}]}, {"text": "For all six datasets, training on different sources resulted in a performance drop.", "labels": [], "entities": []}, {"text": "lists the results of the best feature-based (LR All features) and deep learning (CNN:rand) systems, as well as single feature groups (averages overall source domains, results for individual source domains can be found in the supplementary material to this article).", "labels": [], "entities": []}, {"text": "We note the biggest performance drops on the datasets which performed best in the indomain setting (MT and PE).", "labels": [], "entities": [{"text": "PE", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9192405939102173}]}, {"text": "For the lowest scoring datasets, OC and WTP, the differences are only marginal when trained on a suitable dataset (VG   and OC, respectively).", "labels": [], "entities": [{"text": "OC", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.7560293078422546}, {"text": "WTP", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.6674721240997314}]}, {"text": "The best feature-based approach outperforms the best deep learning approach inmost scenarios.", "labels": [], "entities": []}, {"text": "In particular, as opposed to the in-domain experiments, the difference of the Claim-F 1 measure between the feature-based approaches and the deep learning approaches is striking.", "labels": [], "entities": [{"text": "Claim-F 1 measure", "start_pos": 78, "end_pos": 95, "type": "METRIC", "confidence": 0.983498195807139}]}, {"text": "In the feature-based approaches, on average, a combination of all features yields the best results for both Macro-F 1 and Claim-F 1 . When comparing single features, lexical ones do the best job.", "labels": [], "entities": []}, {"text": "Looking at the best overall system (LR with all features), the average test results when training on different source datasets are between 54% Macro-F 1 resp.", "labels": [], "entities": []}, {"text": "23% Claim-F 1 (both MT) and 58% (VG) resp.", "labels": [], "entities": [{"text": "Claim-F 1", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9946311414241791}, {"text": "VG) resp", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.5882862011591593}]}, {"text": "34% (OC).", "labels": [], "entities": [{"text": "OC", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.7366948127746582}]}, {"text": "Depending on the goal that should be achieved, training on VG (highest average Macro-F 1 ) or OC (highest average Claim-F 1 ) seems to be the best choice when the domain of test data is unknown (we analyze this finding in more depth in \u00a76).", "labels": [], "entities": [{"text": "VG", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.776759922504425}, {"text": "OC (highest average Claim-F 1 )", "start_pos": 94, "end_pos": 125, "type": "METRIC", "confidence": 0.7798052643026624}]}, {"text": "MT clearly gives the best results as target domain, followed by PE and VG.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7696223855018616}, {"text": "PE", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.7936314940452576}]}, {"text": "We also performed experiments with mixed sources, the results are shown in.", "labels": [], "entities": []}, {"text": "We did this in a leave-one-domain-out fashion, in particular we trained on all but one datasets and tested on the remaining one.", "labels": [], "entities": []}, {"text": "In this scenario, the neural network systems seem to benefit from the increased amount of training data and thus gave the best results.", "labels": [], "entities": []}, {"text": "Overall, the mixed sources approach works better than many of the single-source crossdomain systems -yet, the differences were not found to be significant, but as good as training on suitable single sources (see above).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the employed corpora.", "labels": [], "entities": []}, {"text": " Table 2: In-domain experiments, best values per column are highlighted. For each dataset (column head)  we show two scores: Macro-F 1 score (left-hand column) and F 1 score for claims (right-hand column).", "labels": [], "entities": [{"text": "Macro-F 1 score", "start_pos": 125, "end_pos": 140, "type": "METRIC", "confidence": 0.7836288809776306}, {"text": "F 1 score", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9878937800725301}]}, {"text": " Table 3: Cross-domain experiments, best values per column are highlighted, in-domain results (for com- parison) in italics; results only for selected systems. For each source/target combination we show two  scores: Macro-F 1 score (left-hand column) and F 1 score for claims (right-hand column).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 255, "end_pos": 264, "type": "METRIC", "confidence": 0.9874297380447388}]}, {"text": " Table 4: Leave-one-domain-out experiments, best values per column are highlighted. For each test  dataset (column head) we show two scores: Macro-F 1 score (left-hand column) and F 1 score for claims  (right-hand column).", "labels": [], "entities": [{"text": "Macro-F 1 score", "start_pos": 141, "end_pos": 156, "type": "METRIC", "confidence": 0.7745082775751749}, {"text": "F 1 score", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9872411489486694}]}, {"text": " Table 5: Heatmap of Spearman correlations in  % based on most frequent 500 lemmas for each  dataset. Source domain: rows, target domain:  columns.", "labels": [], "entities": [{"text": "Heatmap", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8906338810920715}]}]}