{"title": [{"text": "Zero-Shot Activity Recognition with Verb Attribute Induction", "labels": [], "entities": [{"text": "Zero-Shot Activity Recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5327841341495514}, {"text": "Verb Attribute Induction", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.7938893040021261}]}], "abstractContent": [{"text": "In this paper, we investigate large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs.", "labels": [], "entities": [{"text": "zero-shot activity recognition", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.6071483294169108}]}, {"text": "For example, the verb \"salute\" has several properties, such as being alight movement, asocial act, and short in duration.", "labels": [], "entities": [{"text": "salute\"", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.8587051928043365}]}, {"text": "We use these attributes as the internal mapping between visual and textual representations to reason about a previously unseen action.", "labels": [], "entities": []}, {"text": "In contrast to much prior work that assumes access to gold standard attributes for zero-shot classes and focuses primarily on object attributes, our model uniquely learns to infer action attributes from dictionary definitions and distributed word representations.", "labels": [], "entities": []}, {"text": "Experimental results confirm that action attributes inferred from language can provide a predictive signal for zero-shot prediction of previously unseen activities.", "labels": [], "entities": [{"text": "zero-shot prediction of previously unseen activities", "start_pos": 111, "end_pos": 163, "type": "TASK", "confidence": 0.7834709286689758}]}], "introductionContent": [{"text": "We study the problem of inferring action verb attributes based on how the word is defined and used in context.", "labels": [], "entities": []}, {"text": "For example, given a verb such as \"swig\" shown in, we want to infer various properties of actions such as motion dynamics (moderate movement), social dynamics (solitary act), body parts involved (face, arms, hands), and duration (less than 1 minute) that are generally true for the range of actions that can be denoted by the verb \"swig\".", "labels": [], "entities": [{"text": "duration", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9909071922302246}]}, {"text": "Our ultimate goal is to improve zero-shot learning of activities in computer vision: predicting a previously unseen activity by integrating background knowledge about the conceptual properties of actions.", "labels": [], "entities": []}, {"text": "For example, a computer vision system may have seen images of \"drink\" activities during Figure 1: An overview of our task.", "labels": [], "entities": []}, {"text": "A: we seek to use use distributed word embeddings in tandem with dictionary definitions to obtain a high level understanding of verbs.", "labels": [], "entities": []}, {"text": "B: we seek to use these predicted attributes to allow a classifier to recognize a broader set of activities than what was seen in training time.", "labels": [], "entities": []}, {"text": "training, but not \"swig\".", "labels": [], "entities": []}, {"text": "Ideally, the system should infer the likely visual characteristics of \"swig\" using world knowledge implicitly available in dictionary definitions and word embeddings.", "labels": [], "entities": []}, {"text": "However, most existing literature on zero-shot learning has focused on object recognition with only a few notable exceptions (see Related Work in Section 8).", "labels": [], "entities": [{"text": "object recognition", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8461891114711761}]}, {"text": "There are two critical reasons: object attributes, such as color, shape, and texture, are conceptually straightforward to enumerate.", "labels": [], "entities": []}, {"text": "In addition, they have distinct visual patterns which are robust for current vision systems to recognize.", "labels": [], "entities": []}, {"text": "In contrast, activity attributes are more difficult to conceptualize as they involve varying levels of abstractness, which are also more challenging for computer vision as they have less distinct visual patterns.", "labels": [], "entities": []}, {"text": "Noting this difficulty, instead employ cartoon illustrations as intermediate mappings for zero-shot dyadic activity recognition.", "labels": [], "entities": [{"text": "zero-shot dyadic activity recognition", "start_pos": 90, "end_pos": 127, "type": "TASK", "confidence": 0.6164241805672646}]}, {"text": "We present a complementary approach: that of tackling the abstractness of verb attributes directly.", "labels": [], "entities": []}, {"text": "We develop and use a corpus of verb attributes, using linguistic theories on verb semantics (e.g., aspectual verb classes of) and also drawing inspiration from studies on linguistic categorization of verbs and their properties).", "labels": [], "entities": []}, {"text": "In sum, we present the first study aiming to recover general action attributes fora diverse collection of verbs, and probe their predictive power for zero-shot activity recognition on the recently introduced imSitu dataset (.", "labels": [], "entities": [{"text": "zero-shot activity recognition", "start_pos": 150, "end_pos": 180, "type": "TASK", "confidence": 0.603755384683609}, {"text": "imSitu dataset", "start_pos": 208, "end_pos": 222, "type": "DATASET", "confidence": 0.9793969690799713}]}, {"text": "Empirical results show that action attributes inferred from language can help classifying previously unseen activities and suggest several avenues for future research on this challenging task.", "labels": [], "entities": []}, {"text": "We publicly share our dataset and code for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our hypotheses on action attributes and zero-shot learning, we constructed a dataset using crowd-sourcing experiments.", "labels": [], "entities": []}, {"text": "The Actions and Attributes dataset consists of annotations for 1710 verb templates, each consisting of a verb and an optional particle (e.g. \"put\" or \"put up\").", "labels": [], "entities": []}, {"text": "We selected all verbs from the ImSitu corpus, consisting of images representing verbs from many categories (, then extended the set using the MPII movie visual description dataset and ScriptBase datasets, (.", "labels": [], "entities": [{"text": "ImSitu corpus", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.8492499887943268}, {"text": "MPII movie visual description dataset", "start_pos": 142, "end_pos": 179, "type": "DATASET", "confidence": 0.8398176789283752}, {"text": "ScriptBase datasets", "start_pos": 184, "end_pos": 203, "type": "DATASET", "confidence": 0.7103464305400848}]}, {"text": "We used the spaCy dependency parser ( to extract the verb template for each sentence, and collected annotations on Mechanical Turk to filter out nonliteral and abstract verbs.", "labels": [], "entities": []}, {"text": "Turkers annotated this filtered set of templates using the attributes described in Section 2.", "labels": [], "entities": []}, {"text": "In total, 1203 distinct verbs are included.", "labels": [], "entities": []}, {"text": "The templates are split randomly by verb; out of 1710 total templates, we save 1313 for training, 81 for validation, and 316 for testing.", "labels": [], "entities": []}, {"text": "To provide signal for classifying these verbs, we collected dictionary definitions for each verb using the Wordnik API, including only senses that are explicitly labeled \"verb.\"", "labels": [], "entities": [{"text": "Wordnik API", "start_pos": 107, "end_pos": 118, "type": "DATASET", "confidence": 0.9124399423599243}]}, {"text": "This leaves us with 23,636 definitions, an average of 13.8 per verb.", "labels": [], "entities": []}, {"text": "BGRU pretaining We pretrain the BGRU model on the Dictionary Challenge, a collection of 800,000 word-definition pairs obtained from Wordnik and Wikipedia articles (; the objective is to obtain a word's embedding given one of its definitions.", "labels": [], "entities": [{"text": "BGRU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7387539744377136}, {"text": "BGRU", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.5986194014549255}, {"text": "Wordnik", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9415686130523682}]}, {"text": "For the BGRU model, we use an internal dimension of 300, and embed the words to a size 300 representation.", "labels": [], "entities": [{"text": "BGRU", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7012205719947815}]}, {"text": "The vocabulary size is set to 30,000 (including all verbs for which we have definitions).", "labels": [], "entities": []}, {"text": "During pretraining, we keep the architecture the same, except a different 300-dimensional final layer is used to predict the GloVe embeddings.", "labels": [], "entities": []}, {"text": "Following, we use a ranking loss.", "labels": [], "entities": []}, {"text": "Let\u02c6wLet\u02c6 Let\u02c6w = W emb f (x) be the predicted word embeddings for each definition x of a word in the dictionary (not necessarily a verb).", "labels": [], "entities": []}, {"text": "Let w be the word's embedding, and w be the embedding of a random dictionary word.", "labels": [], "entities": []}, {"text": "The loss is then given by: After pretraining the model on the Dictionary Challenge, we fine-tune the attribute weights W (k) using the cross-entropy over Equation 1.", "labels": [], "entities": [{"text": "Dictionary Challenge", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.8486605584621429}]}, {"text": "Zero-shot with the imSitu dataset We build our image-to-verb model on the newly introduced imSitu dataset, which contains a diverse collection of images depicting one of 504 verbs.", "labels": [], "entities": [{"text": "imSitu dataset", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9875909686088562}, {"text": "imSitu dataset", "start_pos": 91, "end_pos": 105, "type": "DATASET", "confidence": 0.9809572994709015}]}, {"text": "The images represent a variety of different semantic role labels ().", "labels": [], "entities": []}, {"text": "shows examples from the dataset.", "labels": [], "entities": []}, {"text": "We apply our attribute split to the dataset and are left with 379 training classes, 29 validation classes, and 96 test classes.", "labels": [], "entities": []}, {"text": "Zero-shot activity recognition baselines We compare against several additional baseline models for learning from attributes and embeddings.", "labels": [], "entities": [{"text": "Zero-shot activity recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5928097267945608}]}, {"text": "Romera-Paredes and Torr: Results on the text-to-attributes task.", "labels": [], "entities": []}, {"text": "All values reported are accuracies (in %).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9938176274299622}]}, {"text": "For attributes where multiple labels can be selected, the accuracy is averaged overall instances (e.g., the accuracy of \"Body\" is given by the average of accuracies from correctly predicting Head, Torso, etc.).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993212223052979}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9992006421089172}, {"text": "predicting Head, Torso", "start_pos": 180, "end_pos": 202, "type": "TASK", "confidence": 0.48836150020360947}]}, {"text": "As such, we report two ways of averaging the results: microaveraging (where the accuracy is the average of accuracies on the underlying labels) and macroaveraging (where the accuracy is averaged together from the groups).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9965406060218811}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9934788942337036}]}], "tableCaptions": [{"text": " Table 1: Results on the text-to-attributes task. All values reported are accuracies (in %). For attributes  where multiple labels can be selected, the accuracy is averaged over all instances (e.g., the accuracy of  \"Body\" is given by the average of accuracies from correctly predicting Head, Torso, etc.). As such, we  report two ways of averaging the results: microaveraging (where the accuracy is the average of accuracies  on the underlying labels) and macroaveraging (where the accuracy is averaged together from the groups).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9966963529586792}, {"text": "accuracy", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9928233623504639}, {"text": "predicting Head, Torso", "start_pos": 276, "end_pos": 298, "type": "TASK", "confidence": 0.5405175909399986}, {"text": "accuracy", "start_pos": 388, "end_pos": 396, "type": "METRIC", "confidence": 0.9902597069740295}, {"text": "accuracy", "start_pos": 483, "end_pos": 491, "type": "METRIC", "confidence": 0.9866086840629578}]}, {"text": " Table 2: Results on the image-to-verb task.  atts(P) refers to attributes predicted from the  BGRU+GloVe model described in Section 3,  atts(G) to gold attributes, and GloVe to GloVe vec- tors. The accuracies reported are amongst the 96  unseen labels of V test .", "labels": [], "entities": [{"text": "BGRU", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.639020562171936}]}]}