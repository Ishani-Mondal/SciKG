{"title": [{"text": "Speaking, Seeing, Understanding: Correlating semantic models with conceptual representation in the brain", "labels": [], "entities": []}], "abstractContent": [{"text": "Research in computational semantics is increasingly guided by our understanding of human semantic processing.", "labels": [], "entities": [{"text": "human semantic processing", "start_pos": 83, "end_pos": 108, "type": "TASK", "confidence": 0.6441723803679148}]}, {"text": "However, semantic models are typically studied in the context of natural language processing system performance.", "labels": [], "entities": []}, {"text": "In this paper, we present a systematic evaluation and comparison of a range of widely-used, state-of-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain.", "labels": [], "entities": []}, {"text": "Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of to multi-modal meaning representations ().", "labels": [], "entities": [{"text": "data-driven semantic modelling", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.680379698673884}]}, {"text": "These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness), lexical entailment (), analogy () and metaphor (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9945876598358154}]}, {"text": "However, less is known about the extent to which such models correlate with and reflect human conceptual representation.", "labels": [], "entities": []}, {"text": "Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments.", "labels": [], "entities": []}, {"text": "In the computational linguistics community, the availability of such fMRI data provides researchers with a benchmark for evaluating semantic model performance in terms of their ability to represent human semantic memory. were the first to demonstrate that distributional semantic models encode some of the patterns found in the fMRI data.", "labels": [], "entities": [{"text": "fMRI data", "start_pos": 328, "end_pos": 337, "type": "DATASET", "confidence": 0.7713867425918579}]}, {"text": "Other researchers followed in their steps, evaluating traditional count-based distributional models), topic model-based semantic features (), psycholinguistic and behavioural features () and visual representations (.", "labels": [], "entities": []}, {"text": "While all of these studies report correlation between the investigated semantic models and patterns found in the brain imaging data, their focus on individual models and the use of different datasets and prediction methods make their results difficult to compare and to integrate into a coherent evaluation landscape.", "labels": [], "entities": []}, {"text": "The work of is an exception, in that the authors systematically compare several distributional models with a range of parameters on the same brain imaging dataset.", "labels": [], "entities": []}, {"text": "However, they focus on the traditional count-based distributional models only.", "labels": [], "entities": []}, {"text": "We take inspiration from the works of and; however, we conduct a more extensive study of the ability of different types of semantic models to predict the patterns of brain activity associated with conceptual representation.", "labels": [], "entities": []}, {"text": "We evaluate and compare several kinds of semantic models, using different modalities and data sources: (1) traditional countbased distributional models (with word windowbased and dependency-based contexts) learnt from text; (2) log-linear skip-gram models (with word window-based and dependency-based contexts); (3) behavioural models based on the free association task; (4) word representations learnt from visual data; and (5) multi-modal word representations combining linguistic and visual information.", "labels": [], "entities": []}, {"text": "Unlike previous studies, where evaluations were typically conducted using a single technique, we evaluate our models using several methods: ridge regression (, similarity-based encoding and similaritybased decoding.", "labels": [], "entities": []}, {"text": "Such an experimental setup allows fora comprehensive evaluation and comparison of the models.", "labels": [], "entities": []}, {"text": "To the best of our knowledge the dependencybased skip-gram model and the free associationbased model, as well as their multimodal counterparts, have not been previously evaluated on the brain activity prediction task.", "labels": [], "entities": [{"text": "brain activity prediction task", "start_pos": 186, "end_pos": 216, "type": "TASK", "confidence": 0.6941928938031197}]}, {"text": "Other models have been evaluated individually and have not yet been systematically compared within a single evaluation framework.", "labels": [], "entities": []}, {"text": "Providing such a comparison, our experiments and results demonstrate that (1) visual information is a stronger predictor of brain activity than the linguistic information for concrete nouns; (2) sparse text-based models, whether dependency-based or built using linear bag-of-words context, tend to predict neural activity more accurately than dense models; (3) cognitively-motivated association-based models perform on par with or better than other linguistic models, which suggests that they provide an interesting avenue in computational semantics research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Nine right-handed adults between the age of 18 and 32 (five female) participated in the study.", "labels": [], "entities": []}, {"text": "They were presented with line drawings and noun labels for 60 concrete nouns from 12 semantic classes -animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, tools, vegetables, vehicles and man-made objects -with five exemplars per class.", "labels": [], "entities": [{"text": "line drawings and noun labels for 60 concrete nouns from 12 semantic classes -animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, tools, vegetables, vehicles and man-made objects", "start_pos": 25, "end_pos": 244, "type": "Description", "confidence": 0.823884104083224}]}, {"text": "The task for the participants during the scanning was to think about the properties of the noun stimuli they were presented with.", "labels": [], "entities": []}, {"text": "The entire set of 60 stimulus words was presented six times to every participant, in a different order for each presentation.", "labels": [], "entities": []}, {"text": "The fMRI images were acquired on a Siemens Allegra 3.0T scanner.", "labels": [], "entities": [{"text": "Allegra 3.0T scanner", "start_pos": 43, "end_pos": 63, "type": "DATASET", "confidence": 0.8890224893887838}]}, {"text": "The initial data was corrected for slice timing, motion and linear trend; spatially normalised and resampled to 3x3x6mm 3 voxels.", "labels": [], "entities": [{"text": "timing", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.7390427589416504}, {"text": "linear", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9591736793518066}]}, {"text": "Only those voxels overlapping with the cortex were selected (approximately 20000 for every participant).", "labels": [], "entities": []}, {"text": "All semantic spaces presented in Section 4 have full coverage on the dataset.", "labels": [], "entities": []}, {"text": "All experiments detailed in this section were performed separately for every participant and evaluated using leave-two-out cross validation.", "labels": [], "entities": []}, {"text": "We repeatedly train a regression model to fit brain activation vectors for each of the semantic spaces described in Section 4, using only 58 of the 60 available concept representations (leave-two-out cross validation).", "labels": [], "entities": []}, {"text": "This resulted in 1770 crossvalidation folds.", "labels": [], "entities": []}, {"text": "The only hyperparameter in the regression is \u03bb, which controls the degree of regularisation.", "labels": [], "entities": []}, {"text": "The \u03bb hyperparameter was optimised when training each cross-validation fold, by choosing from the range 0.0001 to 100 through generalised cross validation (i.e. \u03bb was optimised by only looking at the training items during each cross-validation fold).", "labels": [], "entities": []}, {"text": "There are (60 choose 2) ways to choose two test items from the 60 concepts.", "labels": [], "entities": []}, {"text": "During each testing round, we used the learned mapping function to construct predicted brain activation vectors for the two held out words.We evaluated each of the semantic models by computing its accuracy of matching the two predicted brain activation vectors with the two observed ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9992006421089172}]}, {"text": "A matching score was computed by analysing the cosine similarity between the predicted and the observed brain activation vectors.", "labels": [], "entities": []}, {"text": "If the sum of similarities for the correct pairing was higher than the one for the incorrect pairing the matching accuracy was set to 1 for this cross-validation fold, and otherwise it was set to 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8245715498924255}]}, {"text": "If the model was choosing the match at random, the expected accuracy is 0.50.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996331930160522}]}, {"text": "The similarity between two brain activation vectors was computed by only taking into account the 500 most stable voxels (during each cross-validation fold) as detailed in Section 3.2.", "labels": [], "entities": []}, {"text": "The cross-validated accuracies for each of our semantic models are presented in Table 1, with selected results also shown in.", "labels": [], "entities": []}, {"text": "We only report results on two multi-modal models (VISUAL+MITCHELL and VISUAL+ASSOC), as there was no significant difference in performance between any pair of multi-modal models.", "labels": [], "entities": [{"text": "VISUAL+MITCHELL", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.6547302405039469}, {"text": "VISUAL+ASSOC", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.5746315817038218}]}, {"text": "All semantic models learn to predict neural activation patterns for unseen words significantly above chance level.", "labels": [], "entities": []}, {"text": "Association-based semantic models (ASSOC) significantly outperform all dense semantic representations (whether embedding-based or SVD-reduced), with p < 0.05.", "labels": [], "entities": []}, {"text": "Sparse text-based representations with linear context (DISTRIB and DEPS) significantly outperform some dense semantic representations.", "labels": [], "entities": []}, {"text": "However, no dense semantic models significantly outperform DISTRIB and DEPS.", "labels": [], "entities": [{"text": "DEPS", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.8844262361526489}]}, {"text": "There is no significant difference between the performance of AS-SOC, DISTRIB and DEPS.", "labels": [], "entities": [{"text": "DISTRIB", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.6650978326797485}, {"text": "DEPS", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.677923321723938}]}, {"text": "Contrary to the findings of Murphy et al., we do not find any advantage in predicting brain activation patterns from dependency-based text models.", "labels": [], "entities": []}, {"text": "Both VISUAL and multi-modal models significantly outperform text-based models overall (p < 0.05), excepting MITCHELL with p < 0.11 when comparing to VISUAL and p < 0.09 when comparing against multi-modal semantic models.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.8872815370559692}]}, {"text": "These results support previous findings regarding the importance of grounding semantic models in perceptual input.", "labels": [], "entities": []}, {"text": "These grounded semantic models per-: Regression results.", "labels": [], "entities": [{"text": "Regression", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9767540693283081}]}, {"text": "Cross-validated accuracies for models trained on participants P1 through P9, together with mean over participants.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9811113476753235}]}, {"text": "form as well as models that encode mental representations through associations (ASSOC).", "labels": [], "entities": []}, {"text": "There is no significant advantage for multi-modal models over VISUAL.", "labels": [], "entities": [{"text": "VISUAL", "start_pos": 62, "end_pos": 68, "type": "DATASET", "confidence": 0.6811450719833374}]}, {"text": "We also compare performance of the semantic models when the predicted brain activation vector is computed using the similarity based encoding method.", "labels": [], "entities": []}, {"text": "We use a leave-two-out cross validation strategy, to match previous work and our experiments detailed in Section 6.1.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6581369340419769}]}, {"text": "The similarity-based encoding approach does not require any mapping function to be learned, hence is a robust and fast way to obtain synthesised brain activation vectors for unseen words.", "labels": [], "entities": []}, {"text": "During each cross-validation fold, semantic model similarity codes of the two test words were computed using the procedure outlined in Section 5.1.", "labels": [], "entities": []}, {"text": "Predicted brain activation vectors were then synthesised for the two test words by weighting a superposition of brain activity vectors using their semantic model similarity codes.", "labels": [], "entities": []}, {"text": "The matching score for each of the cross-validation folds was computed in the same way as in the case of the regression model (Section 6.1).", "labels": [], "entities": [{"text": "matching", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9678992033004761}]}, {"text": "The only difference was that we measured the similarity between the two brain activation vectors using Pearson's correlation coefficient, following.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 103, "end_pos": 136, "type": "METRIC", "confidence": 0.7996160537004471}]}, {"text": "As in the previous experiment, the expected chance performance of this method is 0.5.", "labels": [], "entities": [{"text": "chance", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9650251865386963}]}, {"text": "The cross-validated accuracies for each of our semantic models are shown in, with selected results also shown in.", "labels": [], "entities": []}, {"text": "All semantic models perform significantly above chance level.", "labels": [], "entities": [{"text": "chance", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9772987961769104}]}, {"text": "As in the case of the regression experiments, there is a clear advantage in synthesising brain activation vectors for visually grounded models (VISUAL and multi-modal models) over the language-based ones (this time including MITCHELL), as well as ASSOC.", "labels": [], "entities": []}, {"text": "When looking at the performance of the text-based models in general, there is no difference in performance when comparing context-predicting models to count-based ones, or sparse semantic models to dense ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Regression results. Cross-validated accuracies for models trained on participants P1  through P9, together with mean over participants.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9822240471839905}]}, {"text": " Table 2: Similarity based encoding results: Cross-validated accuracies for models trained on  participants P1 through P9, together with mean over participants.", "labels": [], "entities": []}, {"text": " Table 3: Similarity based decoding results: Cross-validated accuracies for models trained on  participants P1 through P9, together with mean over participants.", "labels": [], "entities": []}]}