{"title": [{"text": "Measuring Thematic Fit with Distributional Feature Overlap", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce anew distri-butional method for modeling predicate-argument thematic fit judgments.", "labels": [], "entities": []}, {"text": "We use a syntax-based DSM to build a prototyp-ical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes.", "labels": [], "entities": []}, {"text": "Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems.", "labels": [], "entities": []}, {"text": "Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several psycholinguistic studies in the last two decades have brought extensive evidence that humans activate a rich array of event knowledge during sentence processing: verbs (e.g. arrest) activate expectations about their typical arguments (e.g. cop, thief ) (), and nouns activate other nouns typically co-occurring in the same events (.", "labels": [], "entities": []}, {"text": "Subjects are able to determine the plausibility of a noun fora given argument role and quickly use this knowledge to anticipate upcoming linguistic input.", "labels": [], "entities": []}, {"text": "This phenomenon is referred to in the literature as thematic fit.", "labels": [], "entities": []}, {"text": "Thematic fit estimation has been extensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.", "labels": [], "entities": [{"text": "Thematic fit estimation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6036761105060577}]}, {"text": "More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (.", "labels": [], "entities": []}, {"text": "Starting from the work of, several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (.", "labels": [], "entities": []}, {"text": "Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).", "labels": [], "entities": [{"text": "thematic fit estimation", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7813738783200582}]}, {"text": "Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance.", "labels": [], "entities": [{"text": "thematic fit estimation", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.6011167168617249}]}, {"text": "This is consistent with 's observation that \"thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space\".", "labels": [], "entities": []}, {"text": "The present work sets itself among the unsupervised approaches to thematic fit estimation.", "labels": [], "entities": []}, {"text": "By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-parsed corpora.", "labels": [], "entities": []}, {"text": "The key features of our proposal are a) prototypical representations of verb-specific thematic roles, based on feature weighting and filtering of second order contexts (i.e. contexts that are salient for many of the typical fillers of a given verb-specific thematic role), and b) a similarity measure which computes the Weighted Overlap (W O) between prototypes and candidate fillers. were, at the best of our knowledge, the first authors to measure the correlation between human-elicited thematic fit ratings and the scores assigned by a syntax-based Distributional Semantic Model (DSM).", "labels": [], "entities": [{"text": "Weighted Overlap (W O)", "start_pos": 320, "end_pos": 342, "type": "METRIC", "confidence": 0.8403369784355164}]}, {"text": "More specifically, their gold standard consisted of the human judgments collected by.", "labels": [], "entities": []}, {"text": "The plausibility of each verb-filler pair was computed as the similarity between new candidate nouns and previously attested exemplars for each specific verb-role pairing (as already proposed in).", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our method on three popular datasets for thematic fit estimation, namely, and.", "labels": [], "entities": [{"text": "thematic fit estimation", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.5970708827177683}]}, {"text": "All the datasets contain human plausibility judgments for verb-role-filler triples.", "labels": [], "entities": []}, {"text": "McRae and Pad\u00f3 include scores for agent and patient roles, whereas Ferretti includes instruments and locations (see for the coverage of each system for the datasets).", "labels": [], "entities": []}, {"text": "Performance is evaluated as the Spearman correlation between the scores of the systems and the human plausibility judgments.", "labels": [], "entities": []}, {"text": "In order to make our results more comparable with previous studies, the typical fillers for each verb role were extracted from the TypeDM tensor of the Distributional Memory framework (see Section 3.1).", "labels": [], "entities": []}, {"text": "Those were the same fillers used by   \u2022 the number of fillers used to build the prototype, with the most typical values in the literature ranging between 10 and 50.", "labels": [], "entities": []}, {"text": "We report the results for 10, 30 and 50 fillers \u2022 the types of the dependency relations used for calculating the overlap: we report results for the SO, PREP and ALL settings; \u2022 the value of N , that is the number of top contexts that we take into account when computing the weighted overlap.", "labels": [], "entities": []}, {"text": "reports the scores for our best setting, while the performances for other values of N are discussed in the Section 5.", "labels": [], "entities": []}, {"text": "Baseline and State of the Art.", "labels": [], "entities": []}, {"text": "As a baseline, we use the thematic fit model by, with no ranking of the features of the prototypes and with vector cosine as a similarity metric.", "labels": [], "entities": []}, {"text": "11 Results are reported for 10, 30 and 50 fillers.", "labels": [], "entities": []}, {"text": "For reference, we also report the results of state-of-the-art models, both the unsupervised () and the supervised ones ().", "labels": [], "entities": []}, {"text": "describes the performance of the best setting (weight: PPMI; N=2000).", "labels": [], "entities": [{"text": "weight: PPMI; N=2000)", "start_pos": 47, "end_pos": 68, "type": "METRIC", "confidence": 0.7967469170689583}]}, {"text": "In the first three rows, the table shows the scores obtained by our system varying the types of dependency contexts (i.e. ALL, SO, PREP) and the number of fillers considered for the prototype (i.e. 10, 30 and 50).", "labels": [], "entities": []}, {"text": "The other rows respectively show i) the scores obtained by calculating the vector cosine between the role prototype vector (i.e. the vector obtained by summing the most typical fillers, with no salience ranking of the dimensions) and the candidate filler vector and ii) the scores reported in the literature for the best unsupervised and supervised models.", "labels": [], "entities": []}, {"text": "At a glance, our best scores always outperform the reimplementation of Baroni and Lenci, being mostly competitive with the state of the art models.", "labels": [], "entities": []}, {"text": "More precisely, for agents and patients the performance is close to the reported scores for DM, when only predicates are used in the W O calculation, as hypothesized in Section 3.3.", "labels": [], "entities": [{"text": "DM", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.8157624006271362}, {"text": "W O calculation", "start_pos": 133, "end_pos": 148, "type": "METRIC", "confidence": 0.7534971237182617}]}, {"text": "The neural network of Tilk and colleagues retains a significant advantage on our models only for the McRae dataset.", "labels": [], "entities": [{"text": "McRae dataset", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9661500155925751}]}, {"text": "Our system, however, shows a remarkable improvements on the Ferretti's datasets, and specifically on Ferretti-Instruments, when only complements are used (see Section 3.3), outperforming even the supervised and more complex model by, which has access to semantic roles information.", "labels": [], "entities": [{"text": "Ferretti's datasets", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.811244527498881}]}, {"text": "Compared to the other unsupervised models, our system has a statistically significant advantage over on the locations dataset and over on the locations and on the instruments dataset (p < 0.05).", "labels": [], "entities": []}, {"text": "At the best of our knowledge, the result for the: Average gold values, number of items listed for both metrics, and distribution of syntactic and lexical forms among the 35 best and worst correlated items for every measure in the given datasets.", "labels": [], "entities": [{"text": "Average gold values", "start_pos": 50, "end_pos": 69, "type": "METRIC", "confidence": 0.7151364088058472}]}], "tableCaptions": [{"text": " Table 1: Typical fillers and top second order contexts for several verb-specific roles.", "labels": [], "entities": []}, {"text": " Table 2: Dataset coverage (%) for all systems.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.6895566582679749}]}, {"text": " Table 3: Results for Pad\u00f3, McRae and Ferretti, Instruments and Locations, with W O computed on PPMI  matrix, varying the number of fillers (i.e. 10, 30 and 50) and the types of dependency contexts (i.e. ALL,  SO and PREP). The best results of our system are in bold. A baseline reimplementing Baroni and Lenci  (2010) -with 10, 30 and 50 fillers -and state of the art results from previous literature are reported for  comparison.", "labels": [], "entities": [{"text": "W O", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.7870096862316132}]}, {"text": " Table 4: Average gold values, number of items listed for both metrics, and distribution of syntactic and  lexical forms among the 35 best and worst correlated items for every measure in the given datasets.", "labels": [], "entities": []}, {"text": " Table 5: Correlation between W O and vector co- sine in W O best settings for all datasets", "labels": [], "entities": []}]}