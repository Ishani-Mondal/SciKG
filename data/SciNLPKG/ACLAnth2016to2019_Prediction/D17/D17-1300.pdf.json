{"title": [{"text": "Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU", "labels": [], "entities": [{"text": "Sharp", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9187396764755249}, {"text": "Accurate", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9858887791633606}, {"text": "Neural Machine Translation Decoding", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.6745881363749504}]}], "abstractContent": [{"text": "Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8198581635951996}]}, {"text": "Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/through-put close to that of a phrasal decoder.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9994075298309326}, {"text": "neural machine translation (NMT)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.8527353405952454}]}, {"text": "We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search de-coder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output.", "labels": [], "entities": [{"text": "NMT beam search de-coder", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.548625260591507}]}, {"text": "Second , we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep.", "labels": [], "entities": []}, {"text": "This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9981712102890015}]}, {"text": "By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT English-French NewsTest2014, while decoding at 100 words/sec on single-threaded CPU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.999147891998291}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.993837296962738}, {"text": "WMT English-French NewsTest2014", "start_pos": 100, "end_pos": 131, "type": "DATASET", "confidence": 0.9253975550333658}]}, {"text": "We believe this is the best published ac-curacy/speed trade-off of an NMT system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Attentional sequence-to-sequence models have become the new standard for machine translation over the last two years, and with the unprecedented improvements in translation accuracy comes anew set of technical challenges.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.800625741481781}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.8828391432762146}]}, {"text": "One of the biggest challenges is the high training and decoding costs of these neural machine translation (NMT) system, which is often at least an order of magnitude higher than a phrase-based system trained on the same data.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.8273008167743683}]}, {"text": "For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs, while reported single-threaded decoding speeds of 8-10 words/sec on a shallow NMT system. was able to reach CPU decoding speeds of 100 words/sec fora deep model, but used 44 CPU cores to do so.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.7630842328071594}]}, {"text": "There has been recent work in speeding up decoding by reducing the search space, but little in computational improvements.", "labels": [], "entities": []}, {"text": "In this work, we consider a production scenario which requires low-latency, high-throughput NMT decoding.", "labels": [], "entities": []}, {"text": "We focus on CPU-based decoders, since GPU/FPGA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing.", "labels": [], "entities": []}, {"text": "Efficient CPU decoders can also be used for ondevice mobile translation.", "labels": [], "entities": [{"text": "ondevice mobile translation", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.6186048885186514}]}, {"text": "We focus on singlethreaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput.", "labels": [], "entities": []}, {"text": "We approach this problem from two angles: In Section 4, we describe a number of techniques for improving the speed of the decoder, and obtain a 4.4x speedup over a highly efficient baseline.", "labels": [], "entities": []}, {"text": "These speedups do not affect decoding results, so they can be applied universally.", "labels": [], "entities": []}, {"text": "In Section 5, we describe a simple but powerful network architecture which uses a single RNN (GRU/L-STM) layer at the bottom with a large number of fully-connected (FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Decoding speeds on an Intel E5-2660 CPU, pro-", "labels": [], "entities": []}, {"text": " Table 2: Results on WMT English-French NewsTest2014. Models (S1)-(S6) use a 3-layer 512-dim bidirectional GRU for", "labels": [], "entities": [{"text": "WMT English-French NewsTest2014", "start_pos": 21, "end_pos": 52, "type": "DATASET", "confidence": 0.8947784701983134}]}]}