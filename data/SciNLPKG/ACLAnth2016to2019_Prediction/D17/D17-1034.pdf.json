{"title": [{"text": "MUSE: Modularizing Unsupervised Sense Embeddings", "labels": [], "entities": [{"text": "MUSE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8355507850646973}]}], "abstractContent": [{"text": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts.", "labels": [], "entities": [{"text": "word sense ambiguity", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.6898432374000549}]}, {"text": "Prior work focused on designing a single model to deliver both mechanisms, and thus suffered from either coarse-grained representation learning or inefficient sense selection.", "labels": [], "entities": []}, {"text": "The proposed modular approach, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection.", "labels": [], "entities": []}, {"text": "We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness.", "labels": [], "entities": [{"text": "sense selection", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.7419419884681702}]}, {"text": "The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.8783614337444305}]}], "introductionContent": [{"text": "Recently, deep learning methodologies have dominated several research areas in natural language processing (NLP), such as machine translation, language understanding, and dialogue systems.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.8082595070203146}, {"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.8143819570541382}, {"text": "language understanding", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7133854031562805}]}, {"text": "However, most of applications usually utilize word-level embeddings to obtain semantics.", "labels": [], "entities": []}, {"text": "Considering that natural language is highly ambiguous, the standard word embeddings may suffer from polysemy issues.", "labels": [], "entities": []}, {"text": "pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of the distances between the word and its synonym in each sense would upper-bound the distance between the respective synonyms, which maybe mutually irrelevant, in embedding space . Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word.", "labels": [], "entities": []}, {"text": "This paper focuses on unsupervised learning from the unannotated corpus.", "labels": [], "entities": []}, {"text": "There are two key mechanisms fora multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense fora word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space.", "labels": [], "entities": []}, {"text": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (.", "labels": [], "entities": []}, {"text": "However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (, and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (.", "labels": [], "entities": []}, {"text": "Unlike the prior work, this paper proposes MUSE 2 -a novel modularization framework incorporating sense selection and representation learning models, which implements flexible modules to optimize distinct mechanisms.", "labels": [], "entities": []}, {"text": "Specifically, MUSE enables linear time sense identity decoding with a sense selection module and purely senselevel representation learning with a sense representation module.", "labels": [], "entities": []}, {"text": "With the modular design, we propose a novel joint learning algorithm on the modules by connecting to a reinforcement learning scenario, which achieves the following advantages.", "labels": [], "entities": []}, {"text": "First, the decision making process under reinforcement learning better captures the sense selection mechanism than probabilistic and clustering methods.", "labels": [], "entities": []}, {"text": "Second, our reinforcement learning algorithm realizes the first single objective function for modular unsupervised sense representation systems.", "labels": [], "entities": []}, {"text": "Finally, we introduce various exploration techniques under reinforcement learning on sense selection to enhance robustness.", "labels": [], "entities": []}, {"text": "In summary, our contributions are five-fold: \u2022 MUSE is the first system that maintains purely sense-level representation learning with linear-time sense decoding.", "labels": [], "entities": []}, {"text": "\u2022 We are among the first to leverage reinforcement learning to model the sense selection process in sense representations system.", "labels": [], "entities": []}, {"text": "\u2022 We are among the first to propose a single objective for modularized unsupervised sense embedding learning.", "labels": [], "entities": []}, {"text": "\u2022 We introduce a sense exploration mechanism for the sense selection module to achieve better flexibility and robustness.", "labels": [], "entities": []}, {"text": "\u2022 Our experimental results show the state-ofthe-art performance for synonym selection and contextual word similarities in terms of MaxSimC.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.9258012473583221}]}], "datasetContent": [{"text": "We evaluate our proposed MUSE model in both quantitative and qualitative experiments.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8234795928001404}]}, {"text": "Our model is trained on the April 2010 Wikipedia dump), which contains approximately 1 billion tokens.", "labels": [], "entities": [{"text": "April 2010 Wikipedia dump)", "start_pos": 28, "end_pos": 54, "type": "DATASET", "confidence": 0.7144670009613037}]}, {"text": "For fair comparison, we adopt the same vocabulary set as and.", "labels": [], "entities": []}, {"text": "For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer ( , and remove all sentences with less than 10 tokens.", "labels": [], "entities": []}, {"text": "The number of senses per word in Q is set to 3 as the prior work ().", "labels": [], "entities": []}, {"text": "In the experiments, the context window size is set to 5 (| \u00af Ct | = 11).", "labels": [], "entities": []}, {"text": "Subsampling technique introduced by) is applied to accelerate the training process.", "labels": [], "entities": []}, {"text": "The learning rate is set to 0.025.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.957282155752182}]}, {"text": "The embedding dimension is 300.", "labels": [], "entities": []}, {"text": "We initialize Q and V as zeros, and P and U from uniform distribution [\u2212 1/100, 1/100] such that each embedding has unit length in expectation (.", "labels": [], "entities": []}, {"text": "Our model uses 25 negative senses for negative sampling in (4).", "labels": [], "entities": []}, {"text": "We use = 5% for -Greedy sense selection strategy In optimization, we conduct mini-batch training with 2048 batch size using the following procedure: 1) select senses in the batch; 2) optimize U, V using stochastic training within the batch for efficiency; 3) optimize P, Q using mini-batch training for robustness.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford's Contextual Word Similarities (SCWS) dataset.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 72, "end_pos": 88, "type": "METRIC", "confidence": 0.9621323347091675}, {"text": "Stanford's Contextual Word Similarities (SCWS) dataset", "start_pos": 196, "end_pos": 250, "type": "DATASET", "confidence": 0.6123603913519118}]}, {"text": "Specifically, given a list of word pairs with corresponding contexts, S = {(w i , \u00af Ct , w j , \u00af Ct )}, we calculate the Spearman's rank correlation \u03c1 between human-judged similarity and model similarity estimations . Two major contextual similarity esti-  mations are introduced by: AvgSimC and MaxSimC.", "labels": [], "entities": [{"text": "Spearman's rank correlation \u03c1", "start_pos": 121, "end_pos": 150, "type": "METRIC", "confidence": 0.6890811562538147}, {"text": "MaxSimC", "start_pos": 296, "end_pos": 303, "type": "DATASET", "confidence": 0.8304660320281982}]}, {"text": "AvgSimC is a soft measurement that addresses the contextual information with a probability estimation: where d(z ik , z jl ) refers to the cosine similarity between U z ik and U z jl . AvgSimC weights the similarity measurement of each sense pair z ik and z jl by their probability estimations.", "labels": [], "entities": []}, {"text": "On the other hand, MaxSimC is a hard measurement that only considers the most probable senses: The baselines for comparison include classic clustering methods (), EM algorithms (, and Chinese Restaurant Process ( , where all approaches are trained on the same corpus except used more recent Wikipedia dumps.", "labels": [], "entities": []}, {"text": "The embedding sizes of all baselines are 300, except 50 in.", "labels": [], "entities": []}, {"text": "For every competitor with multiple settings, we report the best performance in each similarity measurement setting and show in  Our MUSE model achieves the state-of-the-art performance on MaxSimC, demonstrating superior quality on independent sense embeddings.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.6874539256095886}]}, {"text": "On the other hand, MUSE achieves comparable performance with the best competitor in terms of AvgSimC (68.7 vs. 69.3), while MUSE outperforms the same competitor significantly in terms of MaxSimC (67.9 vs. 60.1).", "labels": [], "entities": [{"text": "MUSE", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.6176896691322327}, {"text": "AvgSimC", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9690555334091187}, {"text": "MaxSimC", "start_pos": 187, "end_pos": 194, "type": "DATASET", "confidence": 0.8195171356201172}]}, {"text": "The results demonstrate not only the high quality of sense representations but also accurate sense selection.", "labels": [], "entities": []}, {"text": "From the application perspective, MaxSimC refers to atypical scenario using single embedding per word, while AvgSimC employs multiple sense vectors simultaneously per word, which not only brings computational overhead but changes existing neural architecture for NLP.", "labels": [], "entities": []}, {"text": "Hence, we argue that MaxSimC better characterize practical usage of a sense representation system than AvgSimC.", "labels": [], "entities": []}, {"text": "Among various learning methods for MUSE, policy gradient performs worst, echoing our argument in \u00a7 3.2.1.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.961024820804596}]}, {"text": "On the other hand, the superior performance of Boltzmann sampling andGreedy over Greedy selection demonstrates the effectiveness of exploration.", "labels": [], "entities": []}, {"text": "Finally, replacing \u00af L(\u00b7) with\u02c6Lwith\u02c6 with\u02c6L(\u00b7) as the reward signal yields 2.3 times speedup for MUSE--Greedy and 1.3 times speedup for MUSEBoltzmann to reach 67.0 in MaxSimC, which demonstrates the efficacy of proposed approximation\u02c6Ltion\u02c6 tion\u02c6L(\u00b7) over typical \u00af L(\u00b7) in terms of convergence.", "labels": [], "entities": []}, {"text": "We further evaluate our model on synonym selection using multi-sense word representations (.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9806454181671143}]}, {"text": "Three standard synonym selection datasets, ESL-50, RD-300 (), and TOEFL-80, are performed.", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.9097440838813782}, {"text": "ESL-50", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.5429916381835938}, {"text": "RD-300", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.5012176632881165}, {"text": "TOEFL-80", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.6563404202461243}]}, {"text": "In the datasets, each question consists of a question word w Q and four answer candidates {w A , w B , w C , w D }, and the goal is to select the most semantically synonymous choice among the four candidates.", "labels": [], "entities": []}, {"text": "For example, in the TOEFL-80 dataset, a question shows {(Q) enormously, (A) appropriately, (B) uniquely, (C) tremendously, (D) decidedly}, and the answer is (C).", "labels": [], "entities": [{"text": "TOEFL-80 dataset", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9589673578739166}]}, {"text": "For multi-sense representations system, it selects the synonym of the question word w Q using the maximum senselevel cosine similarity as a proxy of the semantic similarity (.", "labels": [], "entities": []}, {"text": "Our model is compared with the following baselines: 1) conventional word embeddings: global context vectors () and skipgram (); 2) applying supervised word sense disambiguation using the IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (, multi-sense skipgram (MSSG) (), Chinese restaurant process (CRP) (, and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 456, "end_pos": 463, "type": "DATASET", "confidence": 0.9487853646278381}]}, {"text": "Among unsupervised sense embedding approaches, CRP and MSSG refer to the baselines with highest MaxSimC and AvgSimC in respectively.", "labels": [], "entities": [{"text": "MaxSimC", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.7841627597808838}, {"text": "AvgSimC", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9736034274101257}]}, {"text": "Here we report the setting for baselines based on the best average performance in this task.", "labels": [], "entities": []}, {"text": "We also show the performance of supervised sense embeddings as an upperbound of unsupervised methods due to the usage of additional supervised information from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9709041714668274}]}, {"text": "The results are shown in, where our MUSE--Greedy and MUSE-Boltzmann significantly outperform all unsupervised sense embeddings methods, echoing the superior quality of our  sense vectors in last section.", "labels": [], "entities": [{"text": "MUSE--Greedy", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.8145677447319031}]}, {"text": "MUSE-Boltzmann also outperforms the supervised sense embeddings except 1 setting without any supervised signal during training.", "labels": [], "entities": [{"text": "MUSE-Boltzmann", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.471999853849411}]}, {"text": "Finally, the MUSE methods with proper exploration outperform all unsupervised baselines consistently, demonstrating the importance of exploration.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.3604249358177185}]}], "tableCaptions": [{"text": " Table 1: Spearman's rank correlation \u03c1 x100 on  the SCWS dataset.  \u2020 denotes superior performance  to all unsupervised competitors.", "labels": [], "entities": [{"text": "Spearman's rank correlation \u03c1", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.7256396889686585}, {"text": "SCWS dataset", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9726875722408295}]}, {"text": " Table 2: Accuracy on synonym selection.  \u2020 de- notes superior performance to all unsupervised  competitors.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9969028830528259}, {"text": "synonym selection", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9785081148147583}]}, {"text": " Table 3: Different word senses are selected by MUSE according to different contexts. The respective  k-NN (sorted by collocation likelihood) senses are shown to indicate respective semantic meanings.", "labels": [], "entities": []}]}