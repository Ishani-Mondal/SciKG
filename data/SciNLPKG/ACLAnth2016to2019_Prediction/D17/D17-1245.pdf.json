{"title": [{"text": "Argument Mining on Twitter: Arguments, Facts and Sources", "labels": [], "entities": [{"text": "Argument Mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7430229634046555}]}], "abstractContent": [{"text": "Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users maybe interested in.", "labels": [], "entities": []}, {"text": "Applying argument mining methods to such heterogeneous data sources is a challenging open research issue , in particular considering the peculiarities of the language used to write textual messages on social media.", "labels": [], "entities": [{"text": "Applying argument mining", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.76497483253479}]}, {"text": "In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for prove-nance verification.", "labels": [], "entities": [{"text": "prove-nance verification", "start_pos": 249, "end_pos": 273, "type": "TASK", "confidence": 0.6627216041088104}]}, {"text": "In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.7563635110855103}, {"text": "facts recognition", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.716469332575798}, {"text": "source identification", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7474274039268494}]}, {"text": "We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.", "labels": [], "entities": [{"text": "Grexit and Brexit news", "start_pos": 109, "end_pos": 131, "type": "DATASET", "confidence": 0.7086440250277519}]}], "introductionContent": [{"text": "Argument mining aims at automatically extracting natural language arguments and their relations from a variety of textual corpora, with the final goal of providing machine-processable structured data for computational models of arguments and reasoning engines.", "labels": [], "entities": [{"text": "Argument mining", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8597518503665924}]}, {"text": "Several approaches have been proposed so far to tackle the two main tasks identified in the field: i) arguments extraction, i.e., to detect arguments within the input natural language texts and the further detection of their boundaries, and ii) relations prediction, i.e., to predict what are the relations holding between the arguments identified in the first task . Social media platforms like Twitter 2 and newspapers blogs allow users to post their own viewpoints on a certain topic, or to disseminate news read on newspapers.", "labels": [], "entities": [{"text": "arguments extraction", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.7302765846252441}, {"text": "relations prediction", "start_pos": 245, "end_pos": 265, "type": "TASK", "confidence": 0.7269765585660934}]}, {"text": "Being these texts short, without standard spelling and with specific conventions (e.g., hashtags, emoticons), they represent an open challenge for standard argument mining approaches.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.7830202281475067}]}, {"text": "The nature and peculiarity of social media data rise also the need of defining new tasks in the argument mining domain.", "labels": [], "entities": [{"text": "argument mining domain", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.8371626138687134}]}, {"text": "In this paper, we tackle the first standard task in argument mining, addressing the research question: how to mine arguments from Twitter?", "labels": [], "entities": [{"text": "argument mining", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.8539035320281982}]}, {"text": "Going a step further, we address also the following subquestions that arise in the context of social media: i) how to distinguish factual arguments from opinions?", "labels": [], "entities": []}, {"text": "ii) how to automatically detect the source of factual arguments?", "labels": [], "entities": []}, {"text": "To answer these questions, we extend and annotate a dataset of tweets extracted from the streams about the Grexit and the Brexit news.", "labels": [], "entities": [{"text": "Grexit", "start_pos": 107, "end_pos": 113, "type": "DATASET", "confidence": 0.9424006938934326}, {"text": "Brexit news", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.9028204381465912}]}, {"text": "To address the first task of argument detection, we apply supervised classification to separate argument-tweets from non-argumentative ones.", "labels": [], "entities": [{"text": "argument detection", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7704277038574219}]}, {"text": "By considering only argument-tweets, in the second step we apply again a supervised classifier to recognize tweets reporting factual information from those containing opinions only.", "labels": [], "entities": []}, {"text": "Finally, we detect, for all those arguments recognized as factual in the previous step, what is the source of such information (e.g., the CNN), relying on the type of the Named Entities recognized in the tweets.", "labels": [], "entities": [{"text": "CNN", "start_pos": 138, "end_pos": 141, "type": "DATASET", "confidence": 0.9214959740638733}]}, {"text": "The last two steps represent new tasks in the argument We refer the reader interested in more details on argument mining to as survey papers, and to the proceedings of the Argument Mining workshop series (https:// argmining2017.wordpress.com/).", "labels": [], "entities": [{"text": "argument mining", "start_pos": 105, "end_pos": 120, "type": "TASK", "confidence": 0.7481559216976166}, {"text": "Argument Mining workshop series", "start_pos": 172, "end_pos": 203, "type": "TASK", "confidence": 0.668922908604145}]}, {"text": "2 www.twitter.com mining research field, of particular importance in social media applications.", "labels": [], "entities": []}], "datasetContent": [{"text": "The only available resource of annotated tweets for argument mining is DART ().", "labels": [], "entities": [{"text": "argument mining", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7733756005764008}]}, {"text": "From the highly heterogeneous topics contained in such resource (i.e. the letter to Iran written by 47 U.S. senators; the referendum for or against Greece leaving the EU; the release of Apple iWatch; the airing of the 4th episode of the 5th season of the TV series Game of Thrones), and considering the fact that tweets discussing apolitical topic generally have a more developed argumentative structure than tweets commenting on a product release, we decided to select for our experiments the subset of the DART dataset on the thread #Grexit (987 tweets).", "labels": [], "entities": [{"text": "DART dataset", "start_pos": 508, "end_pos": 520, "type": "DATASET", "confidence": 0.9262681305408478}, {"text": "Grexit", "start_pos": 536, "end_pos": 542, "type": "METRIC", "confidence": 0.8989287614822388}]}, {"text": "Then, following the same methodology described in (), we have extended such dataset collecting 900 tweets from the thread on #Brexit.", "labels": [], "entities": [{"text": "Brexit", "start_pos": 126, "end_pos": 132, "type": "DATASET", "confidence": 0.7160035371780396}]}, {"text": "From the original thread, we filtered away retweets, accounts with a bot probability >0., and almost identical tweets (Jaccard distance, empirically evaluated threshold).", "labels": [], "entities": [{"text": "Jaccard distance", "start_pos": 119, "end_pos": 135, "type": "METRIC", "confidence": 0.7939745187759399}]}, {"text": "Given that tweets in DART are already annotated for task 1 (argument/non-argument, see Section 2.2), two annotators carried out the same task on the newly extracted data.", "labels": [], "entities": []}, {"text": "Moreover, the same annotators annotated both datasets (Grexit/Brexit) for the other two tasks of our experiments, i.e. i) given the argument tweets, annotation of tweets as either containing factual information or opinions (see Section 2.3), and ii) given factual argument tweets, annotate their source when explicitly cited (see Section 2.4).", "labels": [], "entities": [{"text": "Grexit", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.7740976810455322}, {"text": "Brexit", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8473519682884216}]}, {"text": "contain statistical information on the datasets.", "labels": [], "entities": []}, {"text": "Inter annotator agreement (IAA) between the two annotators has been calculated for the three annotation tasks, resulting in \u03ba=0.767 on the first task (calculated on 100 tweets), \u03ba=0.727 on the second task (on 80 tweets), and Dice=0.84 on the third task (on the whole dataset).", "labels": [], "entities": [{"text": "Inter annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8409886161486307}, {"text": "Dice", "start_pos": 225, "end_pos": 229, "type": "METRIC", "confidence": 0.9959180951118469}]}, {"text": "More specifically, to compute IAA, we sampled the data applying the same strategy: for the first task, we randomly selected 10% of the tweets of the Grexit dataset (our training set); for task 2, again we randomly selected 10% of the tweets annotated as argument in the previous annotation step; for task 3, given the small size of the dataset, both annotators annotated the whole corpus.", "labels": [], "entities": [{"text": "IAA", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.8120282292366028}, {"text": "Grexit dataset", "start_pos": 149, "end_pos": 163, "type": "DATASET", "confidence": 0.9509164988994598}]}, {"text": "We tested Logistic Regression (LR) and Random Forest (RF) classification algorithms, relying on the scikit-learn tool suite . For the learning methods, we have used a Grid Search (exhaustive) through a set of predefined hyper-parameters to find the best performing ones (the goal of our work is not to optimize the classification performance but to provide a preliminary investigation on new tasks in argument mining over Twitter data).", "labels": [], "entities": [{"text": "Random Forest (RF) classification", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.5718386471271515}]}, {"text": "We extract argumentlevel features from the dataset of tweets (following (), that we group into the following categories: \u2022 Lexical (L): unigram, bigram, WordNet verb synsets; \u2022 Twitter-specific (T): punctuation, emoticons; \u2022 Syntactic/Semantic (S): we have two versions of dependency relations as features, one being the original form, the other generalizing a word to its POS tag in turn.", "labels": [], "entities": []}, {"text": "We also use the syntactic tree of the tweets as feature.", "labels": [], "entities": []}, {"text": "We apply the Stanford parser () to obtain parse trees and dependency relations; \u2022 Sentiment (SE): we extract the sentiment from the tweets with the Alchemy API 6 , the sentiment analysis feature of IBM's Semantic Text Analysis API.", "labels": [], "entities": [{"text": "Sentiment (SE)", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.674148440361023}, {"text": "Semantic Text Analysis API", "start_pos": 204, "end_pos": 230, "type": "TASK", "confidence": 0.7212440967559814}]}, {"text": "It returns a polarity label (positive, negative or neutral) and a polarity score between -1 (totally negative) and 1 (totally positive).", "labels": [], "entities": []}, {"text": "As baselines we consider both LR and RF algorithms with a set of basic features (i.e., lexical).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset for task 1: argument detection", "labels": [], "entities": [{"text": "argument detection", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7301069051027298}]}, {"text": " Table 2: Dataset for task 2: factual arguments vs  opinions classification", "labels": [], "entities": []}, {"text": " Table 3: Dataset for task 3: source identification", "labels": [], "entities": [{"text": "source identification", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8645311892032623}]}, {"text": " Table 4: Results obtained on the test set for the  argument detection task (L=lexical features)", "labels": [], "entities": [{"text": "argument detection task", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7998239596684774}]}, {"text": " Table 5: Results obtained by the best model on  each category of the test set for the argument de- tection task", "labels": [], "entities": []}, {"text": " Table 6: Results obtained on the test set for  the factual vs opinion argument classification task  (L=lexical features)", "labels": [], "entities": [{"text": "factual vs opinion argument classification task", "start_pos": 52, "end_pos": 99, "type": "TASK", "confidence": 0.6958765834569931}]}, {"text": " Table 7: Results obtained by the best model on  each category of the test set for the factual vs opin- ion argument classification task", "labels": [], "entities": [{"text": "factual vs opin- ion argument classification", "start_pos": 87, "end_pos": 131, "type": "TASK", "confidence": 0.6809436125414712}]}, {"text": " Table 8: Results obtained on the test set for the  source identification task", "labels": [], "entities": [{"text": "source identification", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8300897181034088}]}]}