{"title": [{"text": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "labels": [], "entities": [{"text": "Identifying Products", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8868495523929596}, {"text": "Fine-grained Domain Adaptation", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7017688155174255}]}], "abstractContent": [{"text": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data.", "labels": [], "entities": []}, {"text": "In this work, we study the task of identifying products being bought and sold in on-line cybercrime forums, which exhibits particularly challenging cross-domain effects.", "labels": [], "entities": []}, {"text": "We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums.", "labels": [], "entities": [{"text": "slot-filling information extraction", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.7892289757728577}, {"text": "named entity recognition", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.633664866288503}]}, {"text": "Each of these forums constitutes its own \"fine-grained domain\" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime.", "labels": [], "entities": []}, {"text": "We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9971476197242737}]}, {"text": "We release a dataset of 1,938 annotated posts from across the four forums.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP can be extremely useful for enabling scientific inquiry, helping us to quickly and efficiently understand large corpora, gather evidence, and test hypotheses ( (b) File 0-initiator10815 Figure 1: Example posts and annotations from Darkode, with annotated product tokens underlined.", "labels": [], "entities": []}, {"text": "The second example exhibits jargon (fud means \"fully undetectable\"), nouns that could be a product in other contexts (Exploit), and multiple lexically-distinct descriptions of a single service.", "labels": [], "entities": []}, {"text": "Note that these posts are much shorter than the average Darkode post (61.5 words).", "labels": [], "entities": [{"text": "Darkode post", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.8423545956611633}]}, {"text": "One domain for which automated analysis is particularly useful is Internet security: researchers obtain large amounts of text data pertinent to active threats or ongoing cybercriminal activity, for which the ability to rapidly characterize that text and draw conclusions can reap major benefits.", "labels": [], "entities": [{"text": "Internet security", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7063556760549545}]}, {"text": "However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models () and deep models (.", "labels": [], "entities": []}, {"text": "Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging.", "labels": [], "entities": []}, {"text": "In this work, we present the task of identifying products being bought and sold in the marketplace sections of these online cybercrime forums.", "labels": [], "entities": []}, {"text": "We define a token-level annotation task where, for each post, we annotate references to the product or products being bought or sold in that post.", "labels": [], "entities": []}, {"text": "Having the ability to automatically tag posts in this way lets us characterize the composition of a forum in terms of what products it deals with, identify trends overtime, associate users with particular activity profiles, and connect to price information to better understand the marketplace.", "labels": [], "entities": []}, {"text": "Some of these analyses only require post-level information (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possible.", "labels": [], "entities": []}, {"text": "Our dataset has already proven enabling for case studies on these particular forums (, including a study of marketplace activity on bulk hacked accounts versus users selling their own accounts.", "labels": [], "entities": []}, {"text": "Our task has similarities to both slot-filling information extraction (with provenance information) as well as standard named-entity recognition (NER).", "labels": [], "entities": [{"text": "slot-filling information extraction", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.776032567024231}, {"text": "named-entity recognition (NER)", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.8433597207069397}]}, {"text": "Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be mentioned.", "labels": [], "entities": []}, {"text": "Moreover, because we are operating over forums, the data is substantially messier than classical NER corpora like CoNLL.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 114, "end_pos": 119, "type": "DATASET", "confidence": 0.9094586968421936}]}, {"text": "While prior work has dealt with these messy characteristics for syntax () and for discourse), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.7245308458805084}]}, {"text": "Having annotated a dataset, we examine supervised and semi-supervised learning approaches to the product extraction problem.", "labels": [], "entities": [{"text": "product extraction problem", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.7983389993508657}]}, {"text": "Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a system trained on one forum is applied to a different forum: in this sense, even two different cybercrime forums seem to represent different \"finegrained domains.\"", "labels": [], "entities": [{"text": "Binary or CRF classification of tokens as products", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7287444099783897}]}, {"text": "Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation, token-level annotation, and semi-supervised approaches.", "labels": [], "entities": []}, {"text": "We find little improvement from these methods and discuss why they fail to have a larger impact.", "labels": [], "entities": []}, {"text": "Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data.", "labels": [], "entities": []}, {"text": "We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of finegrained domain differences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider several forums that vary in the nature of products being traded: \u2022 Darkode: Cybercriminal wares, including exploit kits, spam services, ransomware programs, and stealthy botnets.", "labels": [], "entities": []}, {"text": "\u2022 Hack Forums: A mixture of cyber-security and computer gaming blackhat and noncybercrime products.", "labels": [], "entities": []}, {"text": "\u2022 Blackhat: Blackhat Search Engine Optimization techniques.", "labels": [], "entities": []}, {"text": "\u2022 Nulled: Data stealing tools and services.", "labels": [], "entities": [{"text": "Data stealing", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7537571787834167}]}, {"text": "gives some statistics of these forums.", "labels": [], "entities": []}, {"text": "These are the same forums used to study product activity in.", "labels": [], "entities": []}, {"text": "We collected all available posts and annotated a subset of them.", "labels": [], "entities": []}, {"text": "In total, we annotated 130,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data.", "labels": [], "entities": []}, {"text": "shows two examples of posts from Darkode.", "labels": [], "entities": []}, {"text": "In addition to aspects of the annotation, which we describe below, we see that the text exhibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual formatting, particularly in thread titles.", "labels": [], "entities": []}, {"text": "Also, note how some words that are not products here might be in other contexts (e.g., Exploits).", "labels": [], "entities": []}, {"text": "In light of the various views on this task and its different requirements for different potential applications, we describe and motivate a few distinct evaluation metrics below.", "labels": [], "entities": []}, {"text": "The choice of metric will impact system design, as we discuss in the following sections.", "labels": [], "entities": []}, {"text": "Token-level accuracy We can follow the approach used in token-level tasks like NER and compute precision, recall, and F 1 over the set of tokens labeled as products.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9646040797233582}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9963815212249756}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9981173276901245}, {"text": "F 1", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9918315410614014}]}, {"text": "This most closely mimics our annotation process.", "labels": [], "entities": []}, {"text": "Type-level product extraction (per post) For many applications, the primary goal of the extraction task is more inline with KBP-style slot filling, where we care about the set of products extracted from a particular post.", "labels": [], "entities": [{"text": "Type-level product extraction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.639288196961085}, {"text": "KBP-style slot filling", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.6559801499048868}]}, {"text": "Without a domain-specific lexicon containing full synsets of products (e.g., something that could recognize that hack and access are synonymous), it is difficult to evaluate this in a fully satisfying way.", "labels": [], "entities": []}, {"text": "However, we approximate this evaluation by comparing the set of product types 5 in a post with the set of product types predicted by the system.", "labels": [], "entities": []}, {"text": "Again, we consider precision, recall, and F 1 over these two sets.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.999738872051239}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9997473359107971}, {"text": "F 1", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9893186986446381}]}, {"text": "This metric favors systems that consistently make correct post-level predictions even if they do not retrieve every token-level occurrence of the product.", "labels": [], "entities": []}, {"text": "Post-level accuracy Most posts contain only one product, but our type-level extraction will naturally be a conservative estimate of performance simply because there may seem to be multiple Two product tokens are considered the same type if after lowercasing and stemming they have a sufficiently small edit distance: 0 if the tokens are length 4 or less, 1 if the lengths are between 5 and 7, and 2 for lengths of 8 or more \"products\" that are actually just different ways of referring to one core product.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.6556590795516968}]}, {"text": "Roughly 60% of posts in the two forums contain multiple annotated tokens that are distinct beyond stemming and lowercasing.", "labels": [], "entities": []}, {"text": "However, we analyzed 100 of these multiple product posts across Darkode and Hack Forums, and found that only 6 of them were actually selling multiple products, indicating that posts selling multiple types of products are actually quite rare (roughly 3% of cases overall).", "labels": [], "entities": [{"text": "Darkode and Hack Forums", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.7543611377477646}]}, {"text": "In the rest of the cases, the variations were due to slightly different ways of describing the same product.", "labels": [], "entities": []}, {"text": "In light of this, we also might consider asking the system to extract some product reference from the post, rather than all of them.", "labels": [], "entities": []}, {"text": "Specifically, we compute accuracy on a post-level by checking whether the first product type extracted by the system is contained in the annotated set of product types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9987897276878357}]}, {"text": "Because most posts feature one product, this metric is sufficient to evaluate whether we understood what the core product of the post was.", "labels": [], "entities": []}, {"text": "Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs.", "labels": [], "entities": []}, {"text": "As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them.", "labels": [], "entities": []}, {"text": "In, for example, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product.", "labels": [], "entities": [{"text": "extracting Backconnect bot", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6415819724400839}]}, {"text": "We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser.", "labels": [], "entities": []}, {"text": "We used the parser of to parse all sentences of each post.", "labels": [], "entities": []}, {"text": "For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake.", "labels": [], "entities": []}, {"text": "In, the entire noun phrase Backconnect bot would be labeled as a product.", "labels": [], "entities": []}, {"text": "For products realized as verbs (e.g., hack), we leave the annotation as the single token.", "labels": [], "entities": []}, {"text": "Throughout the rest of this work, we will evaluate sometimes at the token-level and sometimes at the NP-level 7 (including for the product type evaluation and post-level accuracy); we will specify which evaluation is used where.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.8205991983413696}]}], "tableCaptions": [{"text": " Table 1: Forum statistics. The left columns (posts and words per post) are calculated over all data,  while the right columns are based on annotated data only. Note that products per post indicate product  mentions per post, not product types. Slashes indicate the train/development/test split for Darkode and  train/test split for Hack Forums. Agreement is measured using Fleiss' Kappa; the two columns cover  data where three annotators labeled each post and a subset labeled by all annotators.", "labels": [], "entities": []}, {"text": " Table 2: Development set results on Darkode.  Bolded F 1 values represent statistically-significant  improvements over all other system values in the  column with p < 0.05 according to a bootstrap re- sampling test. Our post-level system outperforms  our binary classifier at whole-post accuracy and  on type-level product extraction, even though it is  less good on the token-level metric. All systems  consistently identify product NPs better than they  identify product tokens. However, there is a sub- stantial gap between our systems and human per- formance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9213904738426208}, {"text": "type-level product extraction", "start_pos": 305, "end_pos": 334, "type": "TASK", "confidence": 0.606076180934906}]}, {"text": " Table 3: Test set results at the NP level in within-forum and cross-forum settings for a variety of different  systems. Using either Brown clusters or gazetteers gives mixed results on cross-forum performance: only  one of the improvements ( \u2020) is statistically significant with p < 0.05 according to a bootstrap resampling  test. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums.", "labels": [], "entities": []}, {"text": " Table 4: Test set results at the whole-post level  in within-forum and cross-forum settings for a va- riety of different systems. Brown clusters and  gazetteers give similarly mixed results as in the  token-level evaluation;  \u2020 indicates statistically sig- nificant gains over the post-level system with p <  0.05 according to a bootstrap resampling test.", "labels": [], "entities": []}, {"text": " Table 5: Product token out-of-vocabulary rates on development sets (test set for Blackhat and Nulled) of  various forums with respect to training on Darkode and Hack Forums. We also show the recall of an NP- level system on seen (R seen ) and OOV (R oov ) tokens. Darkode seems to be more \"general\" than Hack  Forums: the Darkode system generally has lower OOV rates and provides more consistent performance  on OOV tokens than the Hack Forums system.", "labels": [], "entities": [{"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9974715709686279}]}]}