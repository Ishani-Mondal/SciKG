{"title": [{"text": "Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks", "labels": [], "entities": [{"text": "Repeat before Forgetting", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8062355319658915}]}], "abstractContent": [{"text": "We present a novel approach for training artificial neural networks.", "labels": [], "entities": []}, {"text": "Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition).", "labels": [], "entities": []}, {"text": "We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models.", "labels": [], "entities": []}, {"text": "The core part of our algorithm is a cognitively-motivated sched-uler according to which training instances and their \"reviews\" are spaced overtime.", "labels": [], "entities": []}, {"text": "Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural models are known to be computationally expensive to train even with fast hardware (.", "labels": [], "entities": []}, {"text": "For example, it takes three weeks to train a deep neural machine translation system on 100 Graphics Processing Units (GPUs) (.", "labels": [], "entities": [{"text": "deep neural machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6027929335832596}]}, {"text": "Furthermore, a large amount of data is usually required to train effective neural models (. and developed training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier concepts and gradually proceeds with more difficult concepts.", "labels": [], "entities": []}, {"text": "Since these approaches are motivated by a \"starting small\" strategy they are called curriculum or self-paced learning.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel training paradigm which is inspired by the broad evidence in psychology that shows human ability to retain information improves with repeated exposure and exponentially decays with delay since last exposure ().", "labels": [], "entities": []}, {"text": "Spaced repetition was presented in psychology and forms the building block of many educational devices, including flashcards, in which small pieces of information are repeatedly presented to a learner on a schedule determined by a spaced repetition algorithm.", "labels": [], "entities": [{"text": "Spaced repetition", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6800017207860947}]}, {"text": "Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials.", "labels": [], "entities": []}, {"text": "We investigate the analogy between training neural models and findings in psychology about human memory model and develop a spaced repetition algorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models.", "labels": [], "entities": [{"text": "Repeat before Forgetting, RbF", "start_pos": 159, "end_pos": 188, "type": "METRIC", "confidence": 0.6757758259773254}]}, {"text": "The core part of our algorithm is a scheduler that ensures a given neural network spends more time working on difficult training instances and less time on easier ones.", "labels": [], "entities": []}, {"text": "Our scheduler is inspired by factors that affect human memory retention, namely, difficulty of learning materials, delay since their last review, and strength of memory.", "labels": [], "entities": []}, {"text": "The scheduler uses these factors to lengthen or shorten review intervals with respect to individual learners and training instances.", "labels": [], "entities": []}, {"text": "We evaluate schedulers based on their scheduling accuracy, i.e., accuracy in estimating network memory retention with respect to previously-seen instances, as well as their effect on the efficiency and effectiveness of downstream neural networks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9291906952857971}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9988480806350708}]}, {"text": "The contributions of this paper are: (1) we show that memory retention in neural networks is affected by the same (known) factors that affect memory retention in humans, (2) we present a novel training paradigm for neural networks based on spaced repetition, and (3) our approach can be applied without modification to any neural network.", "labels": [], "entities": []}, {"text": "Our best RbF algorithm uses 34-50% of training data per epoch while producing similar results to state-of-the-art systems on three tasks, namely sentiment classification, image categorization, and arithmetic addition.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 145, "end_pos": 169, "type": "TASK", "confidence": 0.9495354294776917}, {"text": "image categorization", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.7628810703754425}, {"text": "arithmetic addition", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.8154507875442505}]}, {"text": "3 It also runs 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "Table 1 describes the tasks, datasets, and models that we consider in our experiments.", "labels": [], "entities": []}, {"text": "It also reports the training epochs for which the models produce their best performance on validation data (based on rote training).", "labels": [], "entities": []}, {"text": "We note that the Addition dataset is randomly generated and contains numbers with at most 4 digits.", "labels": [], "entities": [{"text": "Addition dataset", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.7270525842905045}]}, {"text": "We consider three schedulers as baselines: a slightly modified version of the Leitner scheduler (Lit) developed in for human learners (see Footnote 5), curriculum learning (CL) in which training instances are scheduled with respect to their easiness (, and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch.", "labels": [], "entities": []}, {"text": "For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best performance of this scheduler across all datasets.", "labels": [], "entities": []}, {"text": "Curriculum learning starts training with easy instances and gradually introduces more complex instances for training.", "labels": [], "entities": []}, {"text": "Since easiness information is not readily available inmost datasets, previous approaches have used heuristic techniques () or optimization algorithms () to quantify easiness of training instances.", "labels": [], "entities": []}, {"text": "These approaches consider an instance as easy if its loss is smaller than a threshold (\u03bb).", "labels": [], "entities": []}, {"text": "We adopt this technique as follows: at each iteration e, we divide the entire training data into easy and hard sets using iteration-specific \u03bb e and the loss values of instances, obtained from the current partially-trained network.", "labels": [], "entities": []}, {"text": "All easy instances in conjunction with \u03b1 e \u2208 [0, 1] fraction of easiest hard instances (those with smallest loss values greater than \u03bb e ) are used for training at iteration e.", "labels": [], "entities": []}, {"text": "We set each \u03bb e to the average loss of training instances that are correctly classified by the current partiallytrained network.", "labels": [], "entities": []}, {"text": "Furthermore, at each iteration e, we set \u03b1 e = e/k to gradually introduce complex instances at every new iteration.", "labels": [], "entities": []}, {"text": "Note that we treat all instances as easy ate = 0.", "labels": [], "entities": []}, {"text": "Performance values reported in experiments are averaged over 10 runs of systems and the confidence parameter \u03b7 is always set to 0.5 unless otherwise stated.", "labels": [], "entities": [{"text": "confidence parameter \u03b7", "start_pos": 88, "end_pos": 110, "type": "METRIC", "confidence": 0.926642656326294}]}, {"text": "In these experiments, we evaluate memory schedulers with respect to their accuracy in predicting network retention for delayed instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9968165755271912}]}, {"text": "Since curriculum learning does not estimate delay for training instances, we only consider Leitner and RbF schedulers in these experiments.", "labels": [], "entities": []}, {"text": "For this evaluation, if a scheduler predicts a delay t fora training instance hat epoch e, we evaluate network retention with respect to hat epoch e + t.", "labels": [], "entities": []}, {"text": "If the network recalls (correctly classifies) the instance at epoch e + t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction.", "labels": [], "entities": []}, {"text": "We use this binary outcome to evaluate the accuracy of each scheduler.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9990488886833191}]}, {"text": "Note that the performance of schedulers on instances that have not been delayed is not a major concern.", "labels": [], "entities": []}, {"text": "Although failing to delay an item inversely affects efficiency, it makes the network stronger by providing more instances to train from.", "labels": [], "entities": []}, {"text": "Therefore, we consider a good scheduler as the one that accurately delays more items.", "labels": [], "entities": []}, {"text": "depicts the average accuracy of schedulers in predicting networks' retention versus the average fraction of training instances that they delayed per epoch.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9991863369941711}]}, {"text": "As the results show, all schedulers delay substantial amount of instances per epoch.", "labels": [], "entities": []}, {"text": "In particular, Cos and Qua outperform Lit in both predicting network retention and delaying items, delaying around 50% of training instances per epoch.", "labels": [], "entities": [{"text": "predicting network retention", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.7620883782704672}]}, {"text": "This is while Gau and Sec show comparable accuracy to Lit but delay more instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9994004964828491}]}, {"text": "On the other hand, Lap, which has been found effective in Psychology, and Lin are less accurate in predicting network retention.", "labels": [], "entities": [{"text": "predicting network retention", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.6709491709868113}]}, {"text": "This is because of the tradeoff between delaying more instances and creating stronger networks.", "labels": [], "entities": []}, {"text": "Since these schedulers are more flexible in delaying greater amount of instances, they might not provide networks with enough data to fully train.", "labels": [], "entities": []}, {"text": "shows the performance of RbF schedulers with respect to the recall confidence parameter \u03b7, see Equation (9).", "labels": [], "entities": [{"text": "recall confidence parameter \u03b7", "start_pos": 60, "end_pos": 89, "type": "METRIC", "confidence": 0.9834305346012115}, {"text": "Equation", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9877001047134399}]}, {"text": "As the results show, schedulers have poor performance with smaller values of \u03b7.", "labels": [], "entities": []}, {"text": "This is because smaller values of \u03b7 make schedulers very flexible in delaying instances.", "labels": [], "entities": []}, {"text": "However, the performance of schedulers are not dramatically low even with very small \u03b7s.", "labels": [], "entities": []}, {"text": "Our further analyses on the delay patterns show that although a smaller \u03b7 leads to more delayed instances, the delays are significantly shorter.", "labels": [], "entities": []}, {"text": "Therefore, most delayed instances will be \"reviewed\" shortly in next epochs.", "labels": [], "entities": []}, {"text": "These bulk reviews make the network stronger and help it to recall most delayed instance in future iterations.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9780047535896301}]}, {"text": "On the other hand, greater \u03b7s lead to more accurate schedulers at the cost of using more training data.", "labels": [], "entities": []}, {"text": "In fact, we found that larger \u03b7s do not delay most training instances in the first few iterations.", "labels": [], "entities": []}, {"text": "However, once the network obtains a reasonably high performance, schedulers start delaying instances for longer durations.", "labels": [], "entities": []}, {"text": "We will further study this effect in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Datasets, models, and tasks.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of schedulers in terms of aver- age network accuracy, average fraction of instances  used for training per epoch (TIPE), and the extent  to which a model runs faster than Rote training (X  Times Faster). Gain column indicates the Accuracy", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.783379077911377}, {"text": "TIPE)", "start_pos": 135, "end_pos": 140, "type": "METRIC", "confidence": 0.908967137336731}, {"text": "Accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9986802935600281}]}]}