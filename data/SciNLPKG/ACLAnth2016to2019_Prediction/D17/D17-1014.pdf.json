{"title": [{"text": "Towards Decoding as Continuous Optimisation in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7314741611480713}]}], "abstractContent": [{"text": "We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.8310194214185079}]}, {"text": "We refor-mulate decoding, a discrete optimization problem, into a continuous problem, such that optimization can make use of efficient gradient-based techniques.", "labels": [], "entities": []}, {"text": "Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models.", "labels": [], "entities": []}, {"text": "Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where greedy search and beam search are not feasible.", "labels": [], "entities": [{"text": "translations", "start_pos": 114, "end_pos": 126, "type": "TASK", "confidence": 0.9740146994590759}]}, {"text": "Finally, we show how the technique is highly competitive with, and complementary to, reranking.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence to sequence learning with neural networks) is typically associated with two phases: training and decoding (a.k.a. inference).", "labels": [], "entities": [{"text": "Sequence to sequence learning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8072320073843002}]}, {"text": "Model parameters are learned by optimising the training objective, in order that the model can produce good translations when decoding unseen sentences.", "labels": [], "entities": []}, {"text": "The majority of research has focused on the training paradigm or network architecture, however effective means of decoding have been under-investigated.", "labels": [], "entities": []}, {"text": "Conventional heuristicbased approaches for approximate inference include greedy, beam, and stochastic search.", "labels": [], "entities": [{"text": "approximate inference", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.8274523913860321}]}, {"text": "Greedy and beam search have been empirically proved to be adequate for many sequence to sequence tasks, and are the standard methods for NMT decoding.", "labels": [], "entities": [{"text": "beam search", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.7723536193370819}, {"text": "NMT decoding", "start_pos": 137, "end_pos": 149, "type": "TASK", "confidence": 0.8625625371932983}]}, {"text": "However, these inference approaches have several drawbacks.", "labels": [], "entities": []}, {"text": "Firstly, although NMT models use a left-to-right generation which would appear to facilitate efficient search, the models themselves use a recurrent architecture, and accordingly are non-Markov.", "labels": [], "entities": []}, {"text": "This prevents exact dynamic programming solutions, and moreover, limits the potential to incorporate additional global features or constraints.", "labels": [], "entities": []}, {"text": "Global factors can be highly useful in producing better and more diverse translations.", "labels": [], "entities": []}, {"text": "Secondly, the sequential decoding of symbols in the target sequence, the inter-dependencies among the target symbols are not fully exploited.", "labels": [], "entities": []}, {"text": "For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see who apply this idea in traditional statistical MT).", "labels": [], "entities": [{"text": "MT", "start_pos": 218, "end_pos": 220, "type": "TASK", "confidence": 0.9061213731765747}]}, {"text": "A natural way to capture this is to intersect leftto-right and right-to-left models, however the resulting model has no natural generation order, and thus standard decoding methods are unsuitable.", "labels": [], "entities": []}, {"text": "We introduce a novel decoding framework ( \u00a7 3) that relaxes this discrete optimisation problem into a continuous optimisation problem.", "labels": [], "entities": []}, {"text": "This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables, where the exact inference is NP-hard).", "labels": [], "entities": []}, {"text": "The resulting continuous optimisation problem is challenging due to the non-linearity and non-convexity of the relaxed decoding objective.", "labels": [], "entities": []}, {"text": "We make use of stochastic gradient descent (SGD) and exponentiated gradient (EG) algorithms for decoding based on our relaxation approach.", "labels": [], "entities": []}, {"text": "Our decoding framework is powerful and flexible, as it enables us to decode with global constraints involving intersection of multiple NMT models ( \u00a74).", "labels": [], "entities": []}, {"text": "We present experimental results on Chinese-English and German-English translation tasks, confirming the effectiveness of our relaxed optimisation method for decoding ( \u00a75).", "labels": [], "entities": [{"text": "German-English translation tasks", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6876219312349955}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. Our EG algorithm  initialised with the reranked output achieves the  best BLEU score. We also compare reranking  with EG algorithm initialised with the beam de- coder, where for direct comparison we filter out  sentences with length greater than that of the beam  output in the k-best lists. These results show that  the EG algorithm is capable of effectively exploit- ing the search space.  Beyond achieving similar or better translations  to re-ranking, note that EG is simpler in imple- mentation, as it does not require kbest lists, weight  tuning and so forth. Instead this is replaced with  iterative gradient descent. The run-time of the  two methods are comparable, when reranking uses  modest k, however EG can be considerably faster  when k is large, as is typically done to extract the  full benefit from re-ranking. This performance dif- ference is a consequence of GPU acceleration of  the dense vector operations in EG inference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9990770816802979}]}, {"text": " Table 3: The BLEU evaluation results across eval- uation datasets for EG algorithm variants against  the baselines; bold: statistically significantly bet- ter than the best greedy or beam baseline,  \u2020 : best  performance on dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9975841045379639}]}]}