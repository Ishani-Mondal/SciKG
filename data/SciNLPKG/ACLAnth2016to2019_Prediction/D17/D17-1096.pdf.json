{"title": [{"text": "Sound-Word2Vec: Learning Word Representations Grounded in Sounds", "labels": [], "entities": [{"text": "Learning Word Representations Grounded in Sounds", "start_pos": 16, "end_pos": 64, "type": "TASK", "confidence": 0.7585989733537039}]}], "abstractContent": [{"text": "To be able to interact better with humans, it is crucial for machines to understand sound-a primary modality of human perception.", "labels": [], "entities": []}, {"text": "Previous works have used sound to learn embeddings for improved generic semantic similarity assessment.", "labels": [], "entities": [{"text": "generic semantic similarity assessment", "start_pos": 64, "end_pos": 102, "type": "TASK", "confidence": 0.80705326795578}]}, {"text": "In this work, we treat sound as a first-class citizen , studying downstream 6textual tasks which require aural grounding.", "labels": [], "entities": []}, {"text": "To this end, we propose sound-word2vec-a new embedding scheme that learns specialized word embeddings grounded in sounds.", "labels": [], "entities": []}, {"text": "For example, we learn that two seemingly (se-mantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make.", "labels": [], "entities": []}, {"text": "Our embed-dings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects (used in movies).", "labels": [], "entities": [{"text": "text-based sound retrieval", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6183106700579325}]}, {"text": "Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.", "labels": [], "entities": [{"text": "AMEN", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.8288102149963379}]}], "introductionContent": [{"text": "Sound and vision are the dominant perceptual signals, while language helps us communicate complex experiences via rich abstractions.", "labels": [], "entities": []}, {"text": "For example, a novel can stimulate us to mentally construct the image of the scene despite having never physically perceived it.", "labels": [], "entities": []}, {"text": "Indeed, language has evolved to contain numerous constructs that help depict visual concepts.", "labels": [], "entities": []}, {"text": "For example, we can easily form the picture of a white, furry cat with blue eyes via.", "labels": [], "entities": []}, {"text": "a description of the cat in terms of its visual attributes ().", "labels": [], "entities": []}, {"text": "However, how would one describe the auditory instantiation of cats?", "labels": [], "entities": []}, {"text": "While a first thought might be to use audio descriptors like loud, shrill, husky etc.", "labels": [], "entities": []}, {"text": "as mid-level constructs or \"attributes\", arguably, it is difficult to precisely convey and comprehend sound through such language.", "labels": [], "entities": []}, {"text": "Indeed, find that humans first communicate sounds using \"onomatopoeia\" -words that are suggestive of the phonetics of sounds while having no explicit meaning e.g. meow, tic-toc.", "labels": [], "entities": []}, {"text": "When asked for further explanation of sounds, humans provide descriptions of potential sound sources or impressions created by the sound (pleasant, annoying, etc.)", "labels": [], "entities": []}, {"text": "Need for Grounding in Sound.", "labels": [], "entities": []}, {"text": "While onomatopoeic words exist for commonly found concepts, avast majority of concepts are not as perceptually striking or sufficiently frequent for us to come up with dedicated words describing their sounds.", "labels": [], "entities": []}, {"text": "Even worse, some sounds, say, musical instruments, might be difficult to mimic using speech.", "labels": [], "entities": []}, {"text": "Thus, fora large number of concepts there seems to be a gap between sound and its counterpart in language).", "labels": [], "entities": []}, {"text": "This becomes problematic in specific situations where we want to talk about the heavy tail of concepts and their sounds, or while describing a particular sound we want to create as an effect (say in movies).", "labels": [], "entities": []}, {"text": "To alleviate this, a common literary strategy is to provide metaphors to more relatable exemplars.", "labels": [], "entities": []}, {"text": "For example, when we say, \"He thundered angrily\", we compare the person's angry speech to the sound of thunder to convey the seriousness of the situation.", "labels": [], "entities": []}, {"text": "However, without this grounding in sound, thunder and anger both appear to be seemingly unrelated concepts in terms of semantics.", "labels": [], "entities": []}, {"text": "In this work, we learn embeddings to bridge the gap between sound and its counterpart in language.", "labels": [], "entities": []}, {"text": "We follow a retrofitting strategy, capturing similarity in sounds associated with words, while using distributional semantics (from word2vec) to provide smoothness to the embeddings.", "labels": [], "entities": []}, {"text": "Note that we are not interested in capturing phonetic similarity, but the grounding in sound of the concept associated with the word (say \"rustling\" of leaves and paper.)", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our embeddings on three downstream tasks that require reasoning about related aural cues: 1.", "labels": [], "entities": []}, {"text": "Text-based sound retrieval -Given a textual query describing the sound and a database containing sounds and associated textual tags, we retrieve sound samples by matching text (Sec. 5.1) 2.", "labels": [], "entities": [{"text": "Text-based sound retrieval", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5713825523853302}]}, {"text": "Foley Sound Discovery -Given a short phrase that outlines the technique of producing Foley sounds 1 , we discover other relevant words (objects or actions) which can produce similar sound effects (Sec. 5.2) 3.", "labels": [], "entities": [{"text": "Foley Sound Discovery", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7714047233263651}]}, {"text": "Aurally-relevant word relatedness assessment on AMEN and ASLex ( (Sec.", "labels": [], "entities": [{"text": "Aurally-relevant word relatedness", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6723873217900594}]}, {"text": "5.3) We also qualitatively compare with word2vec to highlight the unique notions of word relatedness captured by imposing auditory grounding.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the freesound database, also used in prior work ( to learn the proposed sound-word2vec embeddings.", "labels": [], "entities": []}, {"text": "Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse.", "labels": [], "entities": [{"text": "Freesound", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.898052990436554}]}, {"text": "All uploaded sounds have human descriptions in the form of tags and captions in natural language.", "labels": [], "entities": []}, {"text": "The tags contain abroad set of relevant topics fora sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality.", "labels": [], "entities": []}, {"text": "For the text-based sound retrieval task, we use a subset of 234,120 sounds from this database and divide it into training (80%), validation (10%) and testing splits (10%).", "labels": [], "entities": [{"text": "text-based sound retrieval task", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6768110319972038}]}, {"text": "Further, for foley sound discovery, we aggregate descriptions of foley sound production provided by sound engineers (epicsound, accessed 23-Jan-2017; Singer, accessed 23-Jan-2017) to create a list of 30 foley sound pairs, which forms our ground truth for the task.", "labels": [], "entities": [{"text": "foley sound discovery", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.806195855140686}]}, {"text": "For example, the description to produce a foley \"driving on gravel\" sound is to record the \"crunching sound of plastic or polyethene bags\".) are subsets of the standard MEN () and) word similarity datasets consisting of word-pairs that \"can be associated with a distinctive associated sound\".", "labels": [], "entities": []}, {"text": "We evaluate on this dataset for completeness to benchmark our approach against previous work.", "labels": [], "entities": []}, {"text": "However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that mayor may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Text-based sound retrieval (higher is better). We find  that our sound-word2vec model outperforms all baselines.", "labels": [], "entities": [{"text": "Text-based sound retrieval", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.615872859954834}]}, {"text": " Table 2: Comparison to state of the art AMEN and ASLex  datasets (", "labels": [], "entities": [{"text": "AMEN and ASLex  datasets", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.6747312098741531}]}]}