{"title": [{"text": "High-risk learning: acquiring new word vectors from tiny data", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional semantics models are known to struggle with small data.", "labels": [], "entities": []}, {"text": "It is generally accepted that in order to learn 'a good vector' fora word, a model must have sufficient examples of its usage.", "labels": [], "entities": []}, {"text": "This contradicts the fact that humans can guess the meaning of a word from a few occurrences only.", "labels": [], "entities": [{"text": "guess the meaning of a word", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.73734912276268}]}, {"text": "In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.957101583480835}]}, {"text": "We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.", "labels": [], "entities": [{"text": "word definitions", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.6901778280735016}]}], "introductionContent": [{"text": "Distributional models (DS:;;), and in particular neural network approaches (, do not farewell in the absence of large corpora.", "labels": [], "entities": []}, {"text": "That is, fora DS model to learn a word vector, it must have seen that word a sufficient number of times.", "labels": [], "entities": []}, {"text": "This is in sharp contrast with the human ability to perform fast mapping, i.e. the acquisition of anew concept from a single exposure to information.", "labels": [], "entities": []}, {"text": "There are at least two reasons for wanting to acquire vectors from very small data.", "labels": [], "entities": []}, {"text": "First, some words are simply rare in corpora, but potentially crucial to some applications (consider, for instance, the processing of text containing technical terminology).", "labels": [], "entities": []}, {"text": "Second, it seems that fast-mapping should be a prerequisite for any system pretending to cognitive plausibility: an intelligent agent with learning capabilities should be able to make educated guesses about new concepts it encounters.", "labels": [], "entities": []}, {"text": "One way to deal with data sparsity issues when learning word vectors is to use morphological structure as away to overcome the lack of primary data ().", "labels": [], "entities": []}, {"text": "Whilst such work has shown promising result, it is only applicable when there is transparent morphology to fallback on.", "labels": [], "entities": []}, {"text": "Another strand of research has been started by, who recently showed that by using simple summation over the (previously learnt) contexts of a nonce word, it is possible to obtain good correlation with human judgements in a similarity task.", "labels": [], "entities": []}, {"text": "It is important to note that both these strategies assume that rare words are special cases of the distributional semantics apparatus, and thus require separate approaches to model them.", "labels": [], "entities": []}, {"text": "Having different algorithms for modelling the same phenomenon means however that we need some meta-theory to know when to apply one or the other: it is for instance unclear at which frequency a rare word is not rare anymore.", "labels": [], "entities": []}, {"text": "Further, methods like summation are naturally selflimiting: they create frustratingly strong baselines but are too simplistic to be extended and improved in any meaningful way.", "labels": [], "entities": [{"text": "summation", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.9898235201835632}]}, {"text": "In this paper, our underlying assumption is thus that it would be desirable to build a single, all-purpose architecture to learn word representations from any amount of data.", "labels": [], "entities": []}, {"text": "The work we present views fast-mapping as a component of an incremental architecture: the rare word case is simply the first part of the concept learning process, regardless of how many times it will eventually be encountered.", "labels": [], "entities": []}, {"text": "With the aim of producing such an incremen-tal system, we demonstrate that the general architecture of neural language models like Word2Vec () is actually suited to modelling words from a few occurrences only, providing minor adjustments are made to the model itself and its parameters.", "labels": [], "entities": []}, {"text": "Our main conclusion is that the combination of a heightened learning rate and greedy processing results in very reasonable oneshot learning, but that some safeguards must be in place to mitigate the high risks associated with this strategy.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first tune N2V's initial parameters on the training part of the definitional dataset.", "labels": [], "entities": []}, {"text": "We experiment with a range of values for the learning rate), window size (), the number of negative samples (), the number of epochs () and the subsampling rate ().", "labels": [], "entities": []}, {"text": "Here, given the size of the data, the minimum frequency fora word to be considered is 1.", "labels": [], "entities": []}, {"text": "The best performance is obtained fora window of 15 words, 3 negative samples, a learning rate of 1, a subsampling rate of 10000, an exponential decay where \u03bb = 1 70 , and one single epoch (that is, the system truly implements fast-mapping).", "labels": [], "entities": [{"text": "learning rate", "start_pos": 80, "end_pos": 93, "type": "METRIC", "confidence": 0.9444617331027985}]}, {"text": "When applied to the test set, N2V shows a dramatic improvement in performance over the simple sum model, reaching MM R = 0.04907 (median rank 623).", "labels": [], "entities": [{"text": "MM R", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.969632089138031}]}, {"text": "On the training set of the Chimeras, we further tune the per-sentence decrease in window size and increase in subsampling.", "labels": [], "entities": []}, {"text": "For the window size, we experiment with a reduction of [1...6] words on either side of the target, not going under a window of \u00b13 words.", "labels": [], "entities": []}, {"text": "Further, we adjust each word's subsampling rate by a factor in the range [1.1, 1.2...1.9, 2.0].", "labels": [], "entities": []}, {"text": "Our results confirm that indeed, an appropriate change in those parameters is required: keeping them constant results in decreasing performance as more sentences are introduced.", "labels": [], "entities": []}, {"text": "On the training set, we obtain our best performance (averaged over the 2-, 4-and 6-sentences datasets) fora per-sentence window size decrease of 5 words on either side of the target, and adjusting subsampling by a factor of 1.9.", "labels": [], "entities": []}, {"text": "shows results on the three corresponding test sets using those parameters.", "labels": [], "entities": []}, {"text": "Unfortunately, on this dataset, N2V does not improve on addition.", "labels": [], "entities": []}, {"text": "The difference in performance between the definitional and the Chimeras datasets maybe explained in two ways.", "labels": [], "entities": [{"text": "Chimeras datasets", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.7408502697944641}]}, {"text": "First, the chimera sentences were randomly selected and thus, are not necessarily hugely informative about the nature of the nonce.", "labels": [], "entities": []}, {"text": "Second, the most informative sentences are not necessarily at the beginning of the fragment, so the system heightens its learning rate on the wrong data: the risk does not payoff.", "labels": [], "entities": []}, {"text": "This suggests that a truly intelligent system should adjust its parameters in a non-monotonic way, to take into account the quality of the information it is processing.", "labels": [], "entities": []}, {"text": "This point seems to bean important general requirement for any architecture that claims incrementality: our results indicate very strongly that a notion of informativeness must play a role in the learning decisions of the system.", "labels": [], "entities": []}, {"text": "This conclusion is inline with work in other domains, e.g. interactive word learning using dialogue, where performance is linked to the ability of the system to measure its own confidence in particular pieces of knowledge and ask questions with a high information gain (.", "labels": [], "entities": []}, {"text": "It also meets with general considerations on language acquisition, which accounts for the ability of young children to learn from limited 'primary linguistic data' by restricting explanatory models to those that provide such efficiency).", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7263380438089371}]}], "tableCaptions": [{"text": " Table 1: Results on definitional dataset", "labels": [], "entities": []}, {"text": " Table 2: Results on chimera dataset", "labels": [], "entities": []}]}