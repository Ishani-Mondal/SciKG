{"title": [{"text": "Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data.", "labels": [], "entities": []}, {"text": "It combines an incremental semantic grammar-Dynamic Syntax and Type Theory with Records (DS-TTR)-with Reinforcement Learning (RL), where language generation and dialogue management area joint decision problem.", "labels": [], "entities": [{"text": "language generation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7229525148868561}]}, {"text": "The systems thus produced are incremental: dialogues are processed word-byword , shown previously to be essential in supporting natural, spontaneous dialogue.", "labels": [], "entities": []}, {"text": "We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues.", "labels": [], "entities": []}, {"text": "Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues).", "labels": [], "entities": [{"text": "Facebook AI bAbI dataset", "start_pos": 59, "end_pos": 83, "type": "DATASET", "confidence": 0.9168292135000229}]}, {"text": "It can in addition process 65% of bAbI+, a corpus 1 we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI.", "labels": [], "entities": []}, {"text": "We compare our model with a state-of-the-art retrieval model, memn2n (Bordes et al., 2017).", "labels": [], "entities": []}, {"text": "We find that, in terms of semantic accuracy, memn2n shows very poor robust-ness to the bAbI+ transformations even when trained on the full bAbI dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9778863191604614}, {"text": "bAbI dataset", "start_pos": 139, "end_pos": 151, "type": "DATASET", "confidence": 0.7413400709629059}]}], "introductionContent": [{"text": "There are currently several key problems for the practical data-driven (rather than hand-crafted) development of task-oriented dialogue systems, Dataset available at https://bit.ly/babi_plus among them: (1) large amounts of dialogue data are needed, i.e. thousands of examples in a domain; (2) this data is usually required to be annotated with task-specific semantic/pragmatic information for the domain (e.g. various dialogue act schemes); and (3) the resulting systems are generally turn-based, and so do not support natural spontaneous dialogue which is processed incrementally, word-by-word, with many characteristic phenomena that arise from this incrementality.", "labels": [], "entities": []}, {"text": "In overcoming issue (2), a recent advance made in research on (non-task) chat dialogues has been the development of so-called \"end-to-end\" systems, in which all components are trained from textual dialogue examples, e.g. (. However, as argued, these end-to-end methods may not transfer well to task-based settings (where the user is trying to achieve a domain goal, such as booking a flight or finding a restaurant, resulting in an API call).", "labels": [], "entities": []}, {"text": "then presented an end-to-end method using Memory Networks (memn2ns), which achieves 100% performance on a test-set of 1000 dialogues, after being trained on 1000 dialogues.", "labels": [], "entities": []}, {"text": "This method processes dialogues turn-by-turn, and so does not have the advantages of more natural incremental systems; nor does it really perform language generation, rather it's based on a retrieval model that selects from a set of candidate system responses seen in the data.", "labels": [], "entities": [{"text": "language generation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7211839258670807}]}, {"text": "This paper investigates an approach to these challenges -dubbed babble -using an incremental, semantic parser and generator for dialogue, based around the Dynamic Syntax grammar formalism (DS,;).", "labels": [], "entities": []}, {"text": "Our advance in this paper, for end-to-end systems, is therefore twofold: (a) the babble method overcomes the requirement for large amounts of dialogue data (i.e. 1000s of dialogues in a domain); (b) resulting systems are word-by-word incremental, in both parsing, generation and dialogue management.", "labels": [], "entities": [{"text": "parsing, generation", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.8119456569353739}, {"text": "dialogue management", "start_pos": 279, "end_pos": 298, "type": "TASK", "confidence": 0.7657308578491211}]}, {"text": "We show that using only 5 example dialogues from the bAbI, Task 1 dataset (i.e. 0.13% of the training data used by) babble can automatically induce dialogue systems which process 74% of the bAbI testset in an incremental manner.", "labels": [], "entities": [{"text": "bAbI", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.905251681804657}]}, {"text": "We then introduce an extended incremental version of the bAbI dataset, which we call bAbI+ (see section 4.1), which adds some characteristic incremental phenomena -such as mid-utterance self-corrections -to the bAbI dialogues (this new dataset is freely available).", "labels": [], "entities": [{"text": "bAbI dataset", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8462395071983337}]}, {"text": "Using this, we demonstrate that the babble system can in addition generalise to, and process 65% of the bAbI+ dataset, still when trained only on 5 dialogues from bAbI.", "labels": [], "entities": [{"text": "bAbI+ dataset", "start_pos": 104, "end_pos": 117, "type": "DATASET", "confidence": 0.7864379088083903}]}, {"text": "We compare this method to's memn2n, which, in terms of semantic accuracy (reflected in how well api-calls are predicted at the end of bAbI Task 1), shows very poor robustness to the bAbI+ transformations, even when it is trained on the full bAbI dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9582164287567139}, {"text": "bAbI dataset", "start_pos": 241, "end_pos": 253, "type": "DATASET", "confidence": 0.8255539834499359}]}, {"text": "This overall method is portable to other taskbased domains.", "labels": [], "entities": []}, {"text": "Furthermore, as we use a semantic parser, the semantic/contextual representations of the dialogue can be used directly for large-scale inference, required in more complex tasks (e.g. interactive QA and search).", "labels": [], "entities": []}], "datasetContent": [{"text": "We have so far induced two prototype dialogue systems, one in an 'electronics shopping' domain (see and) and another in a 'restaurant-search' domain, showing that fully incremental dialogue systems can be automatically induced from small amounts of unannotated dialogue transcripts ( -in this case both systems were bootstrapped from a single successful example dialogue.", "labels": [], "entities": []}, {"text": "We are in the process of evaluating these systems with real users.", "labels": [], "entities": []}, {"text": "In this paper, however, our focus is not on building dialogue systems per se, but on: (1) studying and quantifying the interactional and structural generalisation power of the DS-TTR grammar formalism (see Section 2), and that of symbolic, grammar-based approaches to language processing more generally.", "labels": [], "entities": []}, {"text": "We focus hereon specific dialogue phenomena, such as mid-sentence self-corrections, hesitations, and restarts (see below); (2) doing the same for Bordes and Weston's (2017) state-of-the-art, bottom up response retrieval model, without use of linguistic knowledge of any form; and (3) comparing (1) and.", "labels": [], "entities": [{"text": "bottom up response retrieval", "start_pos": 191, "end_pos": 219, "type": "TASK", "confidence": 0.6098161041736603}]}, {"text": "In order to test and quantify the interactional and structural generalisation power/robustness of the two models, babble and memn2n, we need contrasting dialogue data-sets that control for interactional vs. lexical variations in the input dialogues.", "labels": [], "entities": []}, {"text": "Furthermore, to make our results comparable to the existing approach of Bordes and Weston (2017), we need to use the same dataset that they have used.", "labels": [], "entities": []}, {"text": "We therefore use Facebook AI Research's bAbI dialogue tasks dataset ().", "labels": [], "entities": [{"text": "Facebook AI Research's bAbI dialogue tasks dataset", "start_pos": 17, "end_pos": 67, "type": "DATASET", "confidence": 0.8480447307229042}]}, {"text": "These are goal-oriented dialogues in the domain of restaurant search.", "labels": [], "entities": []}, {"text": "Here we tackle Task 1, wherein each dialogue the system asks the user about their preferences for the properties of a restaurant, and each dialogue results in an API call which contains values of each slot obtained.", "labels": [], "entities": []}, {"text": "Other than the explicit API call notation, there are no annotations in the data whatsoever.", "labels": [], "entities": []}, {"text": "While containing some lexical variation, the original bAbI dialogues significantly lack interactional variation vital for natural real-life dialogue.", "labels": [], "entities": []}, {"text": "In order to obtain such variation while holding lexical variation constant, we created the bAbI+ dataset by systematically transforming the bAbI dialogues.", "labels": [], "entities": [{"text": "bAbI+ dataset", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.6547776460647583}]}, {"text": "bAbI+ is an extension of the original bAbI Task 1 dialogues with everyday incremental dialogue phenomena (hesitations, restarts, and corrections -see below).", "labels": [], "entities": []}, {"text": "While the original bAbI tasks 2-7 increase the user's goal complexity, modifications introduced in bAbI+ can bethought of as orthogonal to this: we instead increase the complexity of surface forms of dialogue utterances, while keeping every other aspect of the task fixed.", "labels": [], "entities": []}, {"text": "The variations introduced in bAbI+ are: 1.", "labels": [], "entities": []}, {"text": "Hesitations, e.g. as in \"we will be uhm eight\"; 2.", "labels": [], "entities": []}, {"text": "Restarts, e.g. \"can you make a restaurant uhm yeah can you make a restaurant reservation for four people with french cuisine in a moderate price range\"; and 3.", "labels": [], "entities": []}, {"text": "Corrections affecting task-specific information -both short-distance ones correcting one token, e.g. \"with french oh no spanish food\", and long-distance NP/PP-level corrections, e.g. \"with french food uhm sorry with spanish food\".", "labels": [], "entities": [{"text": "NP/PP-level corrections", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.4930015355348587}]}, {"text": "The phenomena above are mixed in probabilistically from the fixed sets of templates to the original data 2 . The modifications affect a total of 11336 2 See https://github.com/ishalyminov/babi_ tools utterances in the 3998 dialogues.", "labels": [], "entities": []}, {"text": "Around 21% of user turns contain corrections, 40% hesitations, and 5% restarts (they are not mutually exclusive, so that an utterance can contain up to 3 modifications).", "labels": [], "entities": [{"text": "corrections", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9462425708770752}, {"text": "hesitations", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9790823459625244}, {"text": "restarts", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9941881895065308}]}, {"text": "Our modifications, with respect to corrections in particular, are more conservative than those observed in real-world data: reports that self-corrections appear in 20% of all turns of natural conversations from the British National Corpus, and in 40% of turns in the Map Task, a corpus of human-human goal-oriented dialogues.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 215, "end_pos": 238, "type": "DATASET", "confidence": 0.9345263838768005}]}, {"text": "Here's part of an example dialogue in the bAbI+ corpus: sys: hello what can I help you with today?", "labels": [], "entities": [{"text": "bAbI+ corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8205112020174662}]}, {"text": "usr: I'd like to book a uhm yeah I'd like to book a table in a expensive price range sys: I'm on it.", "labels": [], "entities": []}, {"text": "Any preference on a type of cuisine?", "labels": [], "entities": []}, {"text": "usr: with indian food no sorry with spanish food please  We have now set out all we need to perform the first experiment.", "labels": [], "entities": []}, {"text": "Our aim here is to assess the generalisation power that results from the grammar and our state encoding method (section 3.1) -we dub our overall model babble -and compare this to the state of the art results of.", "labels": [], "entities": []}, {"text": "The method in is not generative, rather it is based on retrieval of system responses, based on the history of the dialogue up to that point.", "labels": [], "entities": []}, {"text": "Therefore, for direct comparison, and for simplicity of exposition, we do the same here: we apply the method described for creating a user simulation (section 3.2.1), this time for the system side, resulting in a 'system simulation'.", "labels": [], "entities": []}, {"text": "We then use this to predict a system response, by parsing and encoding the containing test dialogue up to the point immediately prior to the system turn.", "labels": [], "entities": []}, {"text": "This results in a triggering state, s trig , which is then used as the key to lookup the system's response from the rules constructed as per section 3.2.1.", "labels": [], "entities": []}, {"text": "The returned response is then parsed wordby-word as normal, and this same process continues for the rest of the dialogue.", "labels": [], "entities": []}, {"text": "This method uses the full machinery of DS-TTR & our stateencoding method -the babble model -and will thus reflect the generalisation properties that we are interested in.", "labels": [], "entities": []}, {"text": "Cross-Validation Since we are here interested in data efficiency and generalisation we use all the bAbI and bAbI+ data -the train, dev, and test sets -as follows: we train Bordes & Weston's memn2n and babble from 1-5 examples selected at random from the longest dialogues in bAbI -note bAbI+ data is never used for training in these experiments.", "labels": [], "entities": []}, {"text": "This process is repeated across 10 folds.", "labels": [], "entities": []}, {"text": "The models are then tested on sets of 1000 examples selected at random, in each fold.", "labels": [], "entities": []}, {"text": "Both the training and test sets constructed in this way are kept constant in each fold across the babble & memn2n models.", "labels": [], "entities": []}, {"text": "The test sets are selected either exclusively from bAbI or exclusively from bAbI+.", "labels": [], "entities": []}, {"text": "shows per utterance accuracies for the babble & memn2n models.", "labels": [], "entities": []}, {"text": "Per utterance accuracy is the percentage of all system turns in the test dialogues that were correctly predicted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9623326063156128}]}, {"text": "The table shows that babble can generalise to a remarkable 74% of bAbI and 65% of bAbI+ with only 5 input dialogues from bAbI.", "labels": [], "entities": []}, {"text": "It also shows that memn2ns can also generalise remarkably well.", "labels": [], "entities": []}, {"text": "Although as discussed below, this result is misleading on its own as the memn2ns are very poor at generating the final api-calls correctly on both the bAbI & bAbI+ data, and are thus making too many semantic mistakes.", "labels": [], "entities": []}, {"text": "The results from Experiment 1 on their own can be misleading, as correct prediction of system responses does not in general tell us enough about how well the models are interpreting the dialogues, or whether they are doing this with a sufficient level of granularity.", "labels": [], "entities": []}, {"text": "To assess this, in this second experiment, we measure the semantic accuracy of each model by looking exclusively at how accurately they predict the final api-calls in the bAbI & bAbI+ datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9662125110626221}]}, {"text": "For the memn2n model, we follow the same overall procedure as in the previous experiment: train on bAbI data, and test on bAbI+.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean per utterance accuracies (%) for memn2n & babble models across the bAbI & bAbI+  datasets (10 folds)", "labels": [], "entities": [{"text": "Mean per utterance accuracies", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.8734598159790039}]}]}