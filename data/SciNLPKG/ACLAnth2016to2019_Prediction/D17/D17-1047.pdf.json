{"title": [{"text": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis", "labels": [], "entities": [{"text": "Recurrent Attention", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9205954670906067}, {"text": "Aspect Sentiment Analysis", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8640560706456503}]}], "abstractContent": [{"text": "We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review.", "labels": [], "entities": []}, {"text": "Our framework adopts multiple-attention mechanism to capture sentiment features separated by along distance, so that it is more robust against irrelevant information.", "labels": [], "entities": []}, {"text": "The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications.", "labels": [], "entities": []}, {"text": "The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence.", "labels": [], "entities": []}, {"text": "We examine the merit of our model on four datasets: two are from Se-mEval2014, i.e. reviews of restaurants and laptops; atwitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity.", "labels": [], "entities": [{"text": "atwitter dataset", "start_pos": 120, "end_pos": 136, "type": "DATASET", "confidence": 0.8134184181690216}, {"text": "Chinese news comment dataset", "start_pos": 194, "end_pos": 222, "type": "DATASET", "confidence": 0.5234427452087402}]}, {"text": "The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of aspect sentiment analysis is to identify the sentiment polarity (i.e., negative, neutral, or positive) of a specific opinion target expressed in a comment/review by a reviewer.", "labels": [], "entities": [{"text": "aspect sentiment analysis", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.8266490499178568}]}, {"text": "For example, in \"I bought a mobile phone, its camera is wonderful but the battery life is short\", there are three opinion targets, \"camera\", \"battery life\", and \"mobile phone\".", "labels": [], "entities": []}, {"text": "The reviewer has a positive sentiment on the \"camera\", a negative sentiment on the * Corresponding author.", "labels": [], "entities": []}, {"text": "\"battery life\", and a mixed sentiment on the \"mobile phone\".", "labels": [], "entities": []}, {"text": "Sentence-oriented sentiment analysis methods are not capable to capture such fine-grained sentiments on opinion targets.", "labels": [], "entities": [{"text": "Sentence-oriented sentiment analysis", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8407754302024841}]}, {"text": "In order to identify the sentiment of an individual opinion target, one critical task is to model appropriate context features for the target in its original sentence.", "labels": [], "entities": []}, {"text": "In simple cases, the sentiment of a target is identifiable with a syntactically nearby opinion word, e.g. \"wonderful\" for \"camera\".", "labels": [], "entities": []}, {"text": "However, there are many cases in which opinion words are enclosed in more complicated contexts.", "labels": [], "entities": []}, {"text": "E.g., \"Its camera is not wonderful enough\" might express a neutral sentiment on \"camera\", but not negative.", "labels": [], "entities": []}, {"text": "Such complications usually hinder conventional approaches to aspect sentiment analysis.", "labels": [], "entities": [{"text": "aspect sentiment analysis", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.975503663221995}]}, {"text": "To model the sentiment of the above phraselike word sequence (i.e. \"not wonderful enough\"), LSTM-based methods are proposed, such as target dependent LSTM (TD-LSTM) (.", "labels": [], "entities": []}, {"text": "TD-LSTM might suffer from the problem that after it captures a sentiment feature far from the target, it needs to propagate the feature word byword to the target, in which case it's likely to lose this feature, such as the feature \"cost-effective\" for \"the phone\" in \"My overall feeling is that the phone, after using it for three months and considering its price, is really cost-effective\".", "labels": [], "entities": [{"text": "TD-LSTM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.913493275642395}]}, {"text": "1 Attention mechanism, which has been successfully used in machine translation ( , can enforce a model to pay more attention to the important part of a sentence.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7768266201019287}]}, {"text": "There are already some works using attention in sentiment analysis to exploit this advantage (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9403579533100128}]}, {"text": "Another observation is that some types of sentence structures are particularly challenging for target sentiment analysis.", "labels": [], "entities": [{"text": "target sentiment analysis", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.8248791694641113}]}, {"text": "For example, in \"Except Patrick, all other actors don't play well\", the word \"except\" and the phrase \"don't play well\" produce a positive sentiment on \"Patrick\".", "labels": [], "entities": []}, {"text": "It's hard to synthesize these features just by LSTM, since their positions are dispersed.", "labels": [], "entities": []}, {"text": "Single attention based methods (e.g. () are also not capable to overcome such difficulty, because attending multiple words with one attention may hide the characteristic of each attended word.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel framework to solve the above problems in target sentiment analysis.", "labels": [], "entities": [{"text": "target sentiment analysis", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7924514909585317}]}, {"text": "Specifically, our framework first adopts a bidirectional LSTM (BLSTM) to produce the memory (i.e. the states of time steps generated by LSTM) from the input, as bidirectional recurrent neural networks (RNNs) were found effective fora similar purpose in machine translation ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 253, "end_pos": 272, "type": "TASK", "confidence": 0.7886135280132294}]}, {"text": "The memory slices are then weighted according to their relative positions to the target, so that different targets from the same sentence have their own tailor-made memories.", "labels": [], "entities": []}, {"text": "After that, we pay multiple attentions on the position-weighted memory and nonlinearly combine the attention results with a recurrent network, i.e. GRUs.", "labels": [], "entities": []}, {"text": "Finally, we apply softmax on the output of the GRU network to predict the sentiment on the target.", "labels": [], "entities": [{"text": "GRU network", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.8574334979057312}]}, {"text": "Our framework introduces a novel way of applying multiple-attention mechanism to synthesize important features in difficult sentence structures.", "labels": [], "entities": []}, {"text": "It's sort of analogous to the cognition procedure of a person, who might first notice part of the important information at the beginning, then notices more as she reads through, and finally combines the information from multiple attentions to draw a conclusion.", "labels": [], "entities": []}, {"text": "For the above sentence, our model may attend the word \"except\" first, and then attends the phrase \"don't play well\", finally combines them to generate a positive feature for \"Patrick\".", "labels": [], "entities": []}, {"text": "also adopted the idea of multiple attentions, but they used the result of a previous attention to help the next attention attend more accurate information.", "labels": [], "entities": []}, {"text": "Their vector fed to softmax for classification is only from the final attention, which is essentially a linear combination of input embeddings (they did not have a memory component).", "labels": [], "entities": [{"text": "classification", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9640254378318787}]}, {"text": "Thus, the above limitation of single attention based methods also holds for (.", "labels": [], "entities": []}, {"text": "In contrast, our model combines the results of multiple attentions with a GRU network, which has different behaviors inherited from RNNs, such as forgetting, maintaining, and non-linearly transforming, and thus allows a better prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9551853537559509}]}, {"text": "We evaluate our approach on four datasets: the first two come from SemEval 2014 (, containing reviews of restaurant domain and laptop domain; the third one is a collection of tweets, collected by; to examine whether our framework is language-insensitive (since languages show differences in quite a few aspects in expressing sentiments), we prepared a dataset of Chinese news comments with people mentions as opinion targets.", "labels": [], "entities": []}, {"text": "The experimental results show that our model performs well for different types of data, and consistently outperforms the state-of-the-art methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on four datasets, as shown in.", "labels": [], "entities": []}, {"text": "The first two are from SemEval 2014 (), containing reviews of restaurant and laptop domains, which are widely used in previous works.", "labels": [], "entities": []}, {"text": "The third one is a collection of tweets, collected by ().", "labels": [], "entities": []}, {"text": "The last one is prepared by us for testing the language sensitivity of our model, which contains Chinese news comments and has politicians and entertainers as opinion targets.", "labels": [], "entities": []}, {"text": "We purposely add more negation, contrastive, and question comments to make it more challenging.", "labels": [], "entities": []}, {"text": "Each comment is annotated by at least two annotators, and only if they agree with each other, the comment will be added into our dataset.", "labels": [], "entities": []}, {"text": "Moreover, we replace each opinion target (i.e. word/phrase of pronoun or person name) with a placeholder, as did in ().", "labels": [], "entities": []}, {"text": "For the first two datasets, we removed a few examples having the \"conflict label\", e.g., \"Certainly not the best sushi in New York, however, it is always fresh\" ().", "labels": [], "entities": []}, {"text": "We use 300-dimension word vectors pre-trained by) (whose vocabulary size is 1.9M 2 ) for our experiments on the English datasets, as previous works did (.", "labels": [], "entities": [{"text": "English datasets", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.7690978050231934}]}, {"text": "Some works employed domain-specific training corpus to learn embeddings for better performance, such as TD-LSTM (", "labels": [], "entities": [{"text": "TD-LSTM", "start_pos": 104, "end_pos": 111, "type": "DATASET", "confidence": 0.9199289083480835}]}], "tableCaptions": [{"text": " Table 1: Details of the experimental datasets.", "labels": [], "entities": []}, {"text": " Table 2: Main results. The results with '*' are retrieved from the papers of compared methods, and those  with ' ' are retrieved from Rec-NN paper.", "labels": [], "entities": [{"text": "Rec-NN paper", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.947671115398407}]}, {"text": " Table 3: The impacts of attention layers. (Word embeddings are not tuned in the training stage.)", "labels": [], "entities": []}, {"text": " Table 4: The impact of different embedding tuning strategies.", "labels": [], "entities": []}]}