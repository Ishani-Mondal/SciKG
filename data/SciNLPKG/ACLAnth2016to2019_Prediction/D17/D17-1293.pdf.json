{"title": [{"text": "A Novel Cascade Model for Learning Latent Similarity from Heterogeneous Sequential Data of MOOC", "labels": [], "entities": [{"text": "MOOC", "start_pos": 91, "end_pos": 95, "type": "TASK", "confidence": 0.3971504271030426}]}], "abstractContent": [{"text": "Recent years have witnessed the proliferation of Massive Open Online Courses (MOOCs).", "labels": [], "entities": [{"text": "Massive Open Online Courses (MOOCs)", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.548099262373788}]}, {"text": "With massive learners being offered MOOCs, there is a demand that the forum contents within MOOCs need to be classified in order to facilitate both learners and instructors.", "labels": [], "entities": []}, {"text": "Therefore we investigate a significant application, which is to associate forum threads to subtitles of video clips.", "labels": [], "entities": []}, {"text": "This task can be regarded as a document ranking problem, and the key is how to learn a distinguish-able text representation from word sequences and learners' behavior sequences.", "labels": [], "entities": [{"text": "document ranking", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.7343886196613312}]}, {"text": "In this paper, we propose a novel cascade model, which can capture both the latent semantics and latent similarity by mod-eling MOOC data.", "labels": [], "entities": []}, {"text": "Experimental results on two real-world datasets demonstrate that our textual representation outperforms state-of-the-art unsupervised counterparts for the application.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the rapid development of Massive Open Online Courses (MOOCs), more and more learners participate in MOOCs ().", "labels": [], "entities": [{"text": "Massive Open Online Courses (MOOCs)", "start_pos": 30, "end_pos": 65, "type": "TASK", "confidence": 0.4502403608390263}]}, {"text": "Due to the lack of effective management, most of the discussion forums within MOOCs are overloaded and in chaos ().", "labels": [], "entities": []}, {"text": "Therefore, a key problem is how to manage the forum contents.", "labels": [], "entities": []}, {"text": "To manage the forum contents, threads of forums can be regarded as documents and be classified to groups.", "labels": [], "entities": []}, {"text": "There are several straightforward methods, such as defining sub-forums according to weeks and asking learners to tag threads.", "labels": [], "entities": []}, {"text": "However their effectiveness is limited, because learners have few incentives to tag threads.", "labels": [], "entities": []}, {"text": "Recently, machine learning solutions have been proposed, e.g., content-related thread identification (, confusion classification () and sentiment classification ().", "labels": [], "entities": [{"text": "content-related thread identification", "start_pos": 63, "end_pos": 100, "type": "TASK", "confidence": 0.5992376406987509}, {"text": "confusion classification", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.7346729040145874}, {"text": "sentiment classification", "start_pos": 136, "end_pos": 160, "type": "TASK", "confidence": 0.9342387914657593}]}, {"text": "However they are developed for specific research problems and cannot be applied to our problem.", "labels": [], "entities": []}, {"text": "Moreover, they require labeling data which needs domain experts to label data for different courses.", "labels": [], "entities": []}, {"text": "We observe that the video clips of a MOOC would have many well-formed subtitles composed by instructors.", "labels": [], "entities": [{"text": "MOOC", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.9697149991989136}]}, {"text": "Moreover, within MOOC settings, the course contents can be broken down to knowledge points, and each video clip just corresponds to a knowledge point.", "labels": [], "entities": [{"text": "MOOC", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.9199873208999634}]}, {"text": "Consequently, we propose to fulfill the application, which is to associate threads to subtitles of video clips, i.e., threadsubtitle matching.", "labels": [], "entities": [{"text": "threadsubtitle matching", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.697557270526886}]}, {"text": "By this way, the relevant videos to the threads can be recommended to learners, and the chaotic threads in discussion forums can also be well grouped.", "labels": [], "entities": []}, {"text": "However, it is challenging to identify the relevant video clips for threads without labeling data.", "labels": [], "entities": []}, {"text": "To address this issue, we regard it as a document ranking problem based on the calculation of similarity between documents.", "labels": [], "entities": []}, {"text": "The key problem of this task is to learn a textual representation, which can cluster similar documents and meanwhile distinguish irrelevant ones.", "labels": [], "entities": []}, {"text": "Intuitively, Bag-of-words model (BOW) can be utilized to calculate the similarity between threads and subtitles (.", "labels": [], "entities": [{"text": "BOW", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.5194263458251953}]}, {"text": "However, BOW cannot effectively capture semantics of words and documents.", "labels": [], "entities": []}, {"text": "In addition, recently-studied semantic word embeddings, e.g.,, can capture the semantics.", "labels": [], "entities": []}, {"text": "Para2Vec ( can capture the similarity to some degree, but not explicitly model the latent similarity of documents.", "labels": [], "entities": []}, {"text": "Since the latent similarity is crucial to determine whether a document can be associated to the right target, in our task, the document representation is expected to preserve both the latent semantics and similarity.", "labels": [], "entities": []}, {"text": "In this paper, we leverage two kinds of sequential information: 1) word sequence of subtitles and forum contents, and 2) clickstream log of learning behaviors.", "labels": [], "entities": []}, {"text": "Specifically, different from conventional representation learning tasks, e.g. Word2Vec and Para2Vec, we consider the clickstream data, which reflects the relationship between thread and video's subtitle.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9541954398155212}]}, {"text": "For instance, if a user watches a video and then clicks a thread in forums, the video would be relevant to the thread.", "labels": [], "entities": []}, {"text": "In order to learn representations from the two types of data, we propose a novel cascade model.", "labels": [], "entities": []}, {"text": "Our basic idea is to jointly model three components: 1) word-word coherence, 2) documentdocument coherence, and 3) word-document coherence.", "labels": [], "entities": []}, {"text": "The three components are cascaded for learning the low-dimensional word embeddings.", "labels": [], "entities": []}, {"text": "Then the learned embeddings are used to calculate similarities between threads and subtitles.", "labels": [], "entities": []}, {"text": "To summarize, our contributions include: \u2022 We study an application-oriented research problem, which is how to capture the latent similarity when learning text representation.", "labels": [], "entities": [{"text": "learning text representation", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.5931019385655721}]}, {"text": "\u2022 We propose a novel cascade model to learn the document representation from heterogeneous sequential data: 1) word sequence and 2) learners' clickstream.", "labels": [], "entities": []}, {"text": "\u2022 We collect two real-world MOOC datasets and conduct thorough experiments.", "labels": [], "entities": [{"text": "MOOC datasets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8938652873039246}]}, {"text": "The results demonstrate that our proposed model outperforms the state-of-the-art unsupervised counterparts on the application.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Sets We collect the sequential data of two MOOCs from Coursera 1 and China University MOOC 2 respectively.", "labels": [], "entities": [{"text": "Coursera", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.967210054397583}, {"text": "China University MOOC 2", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.9615413844585419}]}, {"text": "The former is an interdiscipline course called People and Network, and the second is called Introduction to MOOC.", "labels": [], "entities": [{"text": "Introduction to MOOC", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.4852639039357503}]}, {"text": "From both courses, we collect subtitles of video clips, forum contents and learners' log of clickstream.", "labels": [], "entities": []}, {"text": "shows the statistical information of the two MOOCs.", "labels": [], "entities": [{"text": "MOOCs", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.6429241299629211}]}, {"text": "For evaluation, we invite the teaching assistants (TAs) of respective courses to label test samples in advance.", "labels": [], "entities": []}, {"text": "Note that our model is unsupervised.", "labels": [], "entities": []}, {"text": "Therefore, labeled data (thread-subtitle matching pairs) are only used for evaluation, and we do not utilize dev dataset.", "labels": [], "entities": []}, {"text": "Experimental Setting We compare our embeddings with unsupervised rivals and the labels are only used for evaluation.", "labels": [], "entities": []}, {"text": "To ensure fair comparison, we represent documents with their averaged word embeddings.", "labels": [], "entities": []}, {"text": "Note that in the training phase, we represent each thread/subtitle with a vector, in order to make the words within a document clustered and close to each other.", "labels": [], "entities": []}, {"text": "We evaluate the following methods.", "labels": [], "entities": []}, {"text": "\u2022 Bag-of-words(BOW): the classical text representation.", "labels": [], "entities": [{"text": "Bag-of-words(BOW)", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.5420005619525909}]}, {"text": "\u2022 Word2Vec: word embeddings which leverages word-level coherence and we adopt the CBOW architecture.", "labels": [], "entities": []}, {"text": "\u2022 Para2Vec: paragraph embeddings which considers document-level context information.", "labels": [], "entities": []}, {"text": "We also adopt CBOW framework.", "labels": [], "entities": [{"text": "CBOW framework", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9241331517696381}]}, {"text": "\u2022 Hierarchical Document Vector(HDV): the latest word embeddings with a hierarchical architecture for modeling streaming documents and their contents.", "labels": [], "entities": [{"text": "Hierarchical Document Vector(HDV)", "start_pos": 2, "end_pos": 35, "type": "TASK", "confidence": 0.675638477007548}]}, {"text": "\u2022 Cascade Document Representation (CDR): our proposed model which captures both the latent semantics and latent similarity.", "labels": [], "entities": [{"text": "Cascade Document Representation (CDR)", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.7787833511829376}]}, {"text": "We use the hype-parameters recommended by previous literatures.", "labels": [], "entities": []}, {"text": "For all the evaluated baselines, we use the same parameter setting.", "labels": [], "entities": []}, {"text": "Thus it is fair to make comparison.", "labels": [], "entities": []}, {"text": "The window size set in all baselines is 5 by default.", "labels": [], "entities": []}, {"text": "The number of negative samples is empirically set as 5.", "labels": [], "entities": []}, {"text": "The size of hidden layer is set as 100 for all the methods.", "labels": [], "entities": []}, {"text": "We utilize the Precision@K (denoted by P@K) as metric.", "labels": [], "entities": [{"text": "Precision@K", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.953953226407369}]}, {"text": "If the retrieved top-K subtitles hit at least one ground-truth label, we regard it as true; otherwise, it is false.", "labels": [], "entities": []}, {"text": "In our experiments, we run 10 times and report the average result for each case.", "labels": [], "entities": []}, {"text": "Result Firstly we use all the data to learn word embeddings by models.", "labels": [], "entities": []}, {"text": "Then the learned word vectors are utilized to calculate the similarity between threads and subtitles, and rank the subtitles.", "labels": [], "entities": []}, {"text": "reports the results of thread-subtitle matching.", "labels": [], "entities": [{"text": "thread-subtitle matching", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6829570382833481}]}, {"text": "We can notice that there are some anomalies in P@3 and P@5 results.", "labels": [], "entities": []}, {"text": "It maybe for the reason of dataset.", "labels": [], "entities": []}, {"text": "In the first MOOC (people and network), video subtitles contain relatively less words, and therefore it is hard to get effective representations.", "labels": [], "entities": [{"text": "MOOC (people and network)", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.6603856931130091}]}, {"text": "Overall, the proposed models can achieve better performance than baselines, and we highlight the Precision@1 results.", "labels": [], "entities": [{"text": "Precision@1", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.860429048538208}]}, {"text": "Compared to HDV which also considers the streaming documents, our model is better at every task.", "labels": [], "entities": []}, {"text": "This indicates our model can effectively capture the latent similarity.", "labels": [], "entities": []}, {"text": "We investigate the effect of number of dimensions, i.e., the size of the neural network's hidden layer., we find that CDR can achieve better performance than baselines with various numbers of dimensions.", "labels": [], "entities": []}, {"text": "In addition, the optimal results can be obtained when the dimension is set as 100 or 200 in both datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of two MOOC datasets.", "labels": [], "entities": [{"text": "MOOC datasets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8296846449375153}]}, {"text": " Table 2: Result of thread-subtitle matching.", "labels": [], "entities": [{"text": "thread-subtitle matching", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6679593026638031}]}]}