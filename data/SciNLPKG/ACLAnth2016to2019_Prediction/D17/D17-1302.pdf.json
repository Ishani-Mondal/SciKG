{"title": [{"text": "Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources", "labels": [], "entities": [{"text": "Cross-Lingual Transfer", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7736901342868805}, {"text": "POS Tagging", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9145241975784302}]}], "abstractContent": [{"text": "Training a POS tagging model with cross-lingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.8324808180332184}]}, {"text": "In this paper , we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.8751177191734314}]}, {"text": "The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.", "labels": [], "entities": []}, {"text": "The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.", "labels": [], "entities": []}, {"text": "Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.", "labels": [], "entities": [{"text": "Universal Dependencies corpus", "start_pos": 52, "end_pos": 81, "type": "DATASET", "confidence": 0.6212557256221771}, {"text": "POS tagging", "start_pos": 146, "end_pos": 157, "type": "TASK", "confidence": 0.8408938944339752}]}], "introductionContent": [{"text": "Bidirectional Long Short-Term Memory (BLSTM) based models (), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (.", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagging", "start_pos": 153, "end_pos": 181, "type": "TASK", "confidence": 0.6575294733047485}]}, {"text": "Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.7612279057502747}, {"text": "POS tagging", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7024342119693756}]}, {"text": "This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora, morphological analyses (), dictionaries (, and gaze features (.", "labels": [], "entities": []}, {"text": "Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.", "labels": [], "entities": []}, {"text": "Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different since each language has its own vocabulary space.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.944943368434906}, {"text": "transductive transfer learning", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.7996351718902588}]}, {"text": "When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer, but that approach is not directly applicable when the input spaces differ.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7637767195701599}]}, {"text": "used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.", "labels": [], "entities": []}, {"text": "Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7093216776847839}, {"text": "character-level representation transfer", "start_pos": 96, "end_pos": 135, "type": "TASK", "confidence": 0.642013798157374}, {"text": "knowledge transfer between languages", "start_pos": 165, "end_pos": 201, "type": "TASK", "confidence": 0.8114156872034073}]}, {"text": "In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.7610785663127899}, {"text": "BLSTM", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.8177099227905273}]}, {"text": "Inspired by's multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-  lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.", "labels": [], "entities": []}, {"text": "The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training so that the language-general representations to be more compatible among different languages.", "labels": [], "entities": []}, {"text": "Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.", "labels": [], "entities": [{"text": "Universal Dependencies corpus 1.4", "start_pos": 105, "end_pos": 138, "type": "DATASET", "confidence": 0.6936549320816994}]}], "datasetContent": [{"text": "For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 ().", "labels": [], "entities": [{"text": "POS datasets", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8527466356754303}, {"text": "Universal Dependencies corpus 1.4", "start_pos": 76, "end_pos": 109, "type": "DATASET", "confidence": 0.7723101526498795}]}, {"text": "We used English as the source language, which is with 12,543 training sentences.", "labels": [], "entities": []}, {"text": "We chose datasets with 1k to 14k training sentences.", "labels": [], "entities": []}, {"text": "The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.", "labels": [], "entities": []}, {"text": "shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.723353236913681}]}, {"text": "The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.", "labels": [], "entities": [{"text": "bidirectional language modeling", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.7202563087145487}]}, {"text": "Training with only the train sets in the target languages (c) showed 91.61% on average.", "labels": [], "entities": []}, {"text": "When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.", "labels": [], "entities": [{"text": "bidirectional language modeling", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.6093114912509918}, {"text": "accuracies", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.99937504529953}]}, {"text": "Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.", "labels": [], "entities": []}, {"text": "With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.8994455635547638}]}, {"text": "They showed better average accuracies than target only cases, but they showed mixed results.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.8625426888465881}]}, {"text": "However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.", "labels": [], "entities": []}, {"text": "For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of  other transfer learning models.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 79, "end_pos": 89, "type": "METRIC", "confidence": 0.9970589876174927}]}, {"text": "For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9575238823890686}]}, {"text": "Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.", "labels": [], "entities": []}, {"text": "This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 181, "end_pos": 192, "type": "TASK", "confidence": 0.8598615527153015}]}, {"text": "shows the results when using 320 taglabeled training sentences.", "labels": [], "entities": []}, {"text": "In this case, transfer learning methods still show better accuracies than target-only approaches on average.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.9142056703567505}]}, {"text": "However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.", "labels": [], "entities": []}, {"text": "In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.", "labels": [], "entities": []}, {"text": "When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.8879269063472748}]}, {"text": "However, not using the common BLSTM in transfer learning models showed better performance than using it on average.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.7226874232292175}]}, {"text": "The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.7776534557342529}, {"text": "BLSTMs", "start_pos": 144, "end_pos": 150, "type": "DATASET", "confidence": 0.8785799145698547}]}, {"text": "In this case, just having private BLSTMs without the common BLSTM can show better performance.", "labels": [], "entities": []}, {"text": "We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.", "labels": [], "entities": []}, {"text": "For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9997138381004333}]}, {"text": "This is expected since transfer learning is effective when the target train set is small.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9653303325176239}]}, {"text": "An extension of this work is utilizing multiple languages as the source languages.", "labels": [], "entities": []}, {"text": "Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.", "labels": [], "entities": []}, {"text": "For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.", "labels": [], "entities": []}, {"text": "When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-  formance than the results of using English as a single source language.", "labels": [], "entities": []}, {"text": "Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.", "labels": [], "entities": []}, {"text": "We also tried to use Wasserstein distance ( for the adversarial training in the multi-source settings, but there were no significant differences on average.", "labels": [], "entities": [{"text": "Wasserstein distance", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.7412325143814087}]}, {"text": "4 Implementation Details All the models were optimized using ADAM ( with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9882715344429016}]}, {"text": "The dimensionalites of all the BLSTM related layers follow's model.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6739675402641296}]}, {"text": "Each word vector is 128 dimensional and each character vector is 100 dimensional.", "labels": [], "entities": []}, {"text": "They are randomly initialized with Xavier initialization.", "labels": [], "entities": []}, {"text": "For stable training, we use gradient clipping, where the threshold is set to 5.", "labels": [], "entities": []}, {"text": "The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 165, "end_pos": 170, "type": "METRIC", "confidence": 0.6003648638725281}]}, {"text": "Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328 The extended work in detail are shown in.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.74530029296875}]}, {"text": "5 learning rate=0.001, \u03b21 = 0.9, \u03b22 = 0.999, = 1e \u2212 8. dimensional.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.9633237421512604}]}, {"text": "The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 ().", "labels": [], "entities": [{"text": "BLSTMs", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.5807909965515137}]}, {"text": "For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (.", "labels": [], "entities": []}, {"text": "For all the BLSTMs, forget biases are initialized with 1 ( and the other biases are initialized with 0.", "labels": [], "entities": [{"text": "BLSTMs", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.5969837307929993}]}, {"text": "Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: POS tagging accuracies (%) when setting the numbers of the tag-labeled training examples of the target languages", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7013537585735321}, {"text": "accuracies", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.6633062362670898}]}, {"text": " Table 2: POS tagging accuracies (%) with 320 tag-labeled training examples for each target language. All the training", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6655987799167633}]}]}