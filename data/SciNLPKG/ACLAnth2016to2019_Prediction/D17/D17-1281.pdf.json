{"title": [{"text": "Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods", "labels": [], "entities": [{"text": "Reinforcement Learning-based Information Extraction Training", "start_pos": 12, "end_pos": 72, "type": "TASK", "confidence": 0.9059265494346619}]}], "abstractContent": [{"text": "RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process.", "labels": [], "entities": [{"text": "Reinforcement Learning-based Information Extraction (IE)", "start_pos": 32, "end_pos": 88, "type": "TASK", "confidence": 0.7802539042064122}]}, {"text": "RLIE-DQN trains a single agent sequentially, training on one instance at a time.", "labels": [], "entities": [{"text": "RLIE-DQN", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.686978280544281}]}, {"text": "This results in significant training slowdown which is undesirable.", "labels": [], "entities": [{"text": "training slowdown", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.7502716779708862}]}, {"text": "We leverage recent advances in parallel RL training using asynchronous methods and propose RLIE-A3C.", "labels": [], "entities": [{"text": "RL training", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9267440140247345}]}, {"text": "RLIE-A3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8447896838188171}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9837518930435181}]}], "introductionContent": [{"text": "Extracting information about an event (or entity) involves multiple decisions, as one first needs to identify documents relevant to the event, extract relevant information from those documents, and finally reconcile various values obtained for the same relation of the event from different sources).", "labels": [], "entities": [{"text": "Extracting information about an event (or entity)", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8869753016365899}]}, {"text": "Search based methods for Information Extraction have been increasingly investigated (;;;.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8220272958278656}]}, {"text": "() combine search and Information Extraction (IE) using Reinforcement Learning (RL), with the goal of selecting good actions while staying within resource constraints, but don't optimze for extraction accuracy.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8076665043830872}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9452380537986755}]}, {"text": "More recently, ( proposed a RL-based approach to model the IE process outlined above.", "labels": [], "entities": [{"text": "IE process", "start_pos": 59, "end_pos": 69, "type": "TASK", "confidence": 0.9158671796321869}]}, {"text": "We shall refer to this approach as RLIE-DQN in this paper.", "labels": [], "entities": [{"text": "RLIE-DQN", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.966759979724884}]}, {"text": "RLIE-DQN trains an RL * Research carried out during an internship at the Indian Institute of Science, Bangalore.", "labels": [], "entities": [{"text": "RLIE-DQN", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.44375115633010864}, {"text": "RL", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.8695905804634094}]}, {"text": "agent using Deep Q-Network (DQN) ( to select optimal actions to query for documents and also reconcile extracted values.", "labels": [], "entities": []}, {"text": "DQN trains a single agent sequentially, updating parameters based on one instance at a time.", "labels": [], "entities": []}, {"text": "Each such instance is sampled from the entire training data, also called the experience replay.", "labels": [], "entities": []}, {"text": "Such sequential experience replay-based training results in slow learning, while requiring high memory and computation resources.", "labels": [], "entities": []}, {"text": "In order to overcome this challenge, A3C (Asynchronous Advantage Actor-Critic), an asynchronous deep RL training algorithm, has been proposed recently).", "labels": [], "entities": [{"text": "A3C", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9367824792861938}]}, {"text": "A3C trains multiple RL agents in parallel, each of which estimates gradients locally, and asynchronously updates globally shared parameters.", "labels": [], "entities": []}, {"text": "Recent work has explored applications for A3C in varied domains (,.", "labels": [], "entities": []}, {"text": "In this paper, we propose RLIE-A3C which uses A3C-based parallel asynchronous agents for training.", "labels": [], "entities": []}, {"text": "This is in contrast to the sequential DQN training in RLIE-DQN.", "labels": [], "entities": [{"text": "RLIE-DQN", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.7786062359809875}]}, {"text": "Differences between the training regimes of the two methods are shown in.", "labels": [], "entities": []}, {"text": "Through experiments on multiple realworld datasets, we find that RLIE-A3C achieves upto 6x training speedup compared to RLIE-DQN, while suffering no loss inaccuracy.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first application of asynchronous deep RL methods in IE (and also in NLP), and we hope this paper will foster further adoption and research into such methods in the NLP community.", "labels": [], "entities": [{"text": "IE", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9475662708282471}]}, {"text": "RLIE-A3C code is available at https://github.com/adi-sharma/RLIE A3C: Left: DQN-based sequential learning framework used in RLIE-DQN (, as discussed in Section 2.", "labels": [], "entities": [{"text": "RLIE-A3C code", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8584554493427277}]}, {"text": "At each time step, the agent looks at a specific instance from the training data.", "labels": [], "entities": []}, {"text": "Right: A3C-based parallel learning framework in RLIE-A3C (proposed approach).", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.7543989419937134}]}, {"text": "The parallel agents look at different parts of the training data, estimate parameter update statistics locally, and then perform asynchronous updates on the globally shared parameters (\u03b8 t ) at time step t.", "labels": [], "entities": []}, {"text": "See Section 3 for details.", "labels": [], "entities": []}, {"text": "Due to the asynchronous parallel updates, RLIE-A3C achieves significant training speedup without loss inaccuracy, as we shall see in Section 4.", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.6881179809570312}]}], "datasetContent": [{"text": "Setup: We compare RLIE-A3C against RLIE-DQN using the same protocol and hyperparameters as reported in).", "labels": [], "entities": []}, {"text": "Also, we experiment with the same two datasets used in that paper: the Gun Violence Archive 1 and the Foodshield EMA database 2 . The train, dev and test datasets contain 372, 146 and 146 source articles respectively for the Shooting incident cases and 292, 42 and 148 source articles respectively for the food adulteration cases.", "labels": [], "entities": [{"text": "Gun Violence Archive 1", "start_pos": 71, "end_pos": 93, "type": "DATASET", "confidence": 0.8527328372001648}, {"text": "Foodshield EMA database", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.911811888217926}]}, {"text": "We used the implementation of RLIE-DQN provided by the authors of that system.", "labels": [], "entities": [{"text": "RLIE-DQN", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.5722001194953918}]}, {"text": "For more details on the dataset and other parameters, we refer the reader to (.", "labels": [], "entities": []}, {"text": "RLIE-A3C: This is our proposed method which is described in Section 3.", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5258302092552185}]}, {"text": "For the sake of fair comparison, the network, base classifier, and evaluation metrics are same as that of RLIE-DQN.", "labels": [], "entities": []}, {"text": "Each of the three deep networks in RLIE-A3C, one each for \u03c0(a d |s), \u03c0(a q |s) and V (s), is built using two linear layers with 20 hidden units, followed by ReLUs.", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 35, "end_pos": 43, "type": "DATASET", "confidence": 0.7654784321784973}, {"text": "ReLUs", "start_pos": 157, "end_pos": 162, "type": "METRIC", "confidence": 0.866188108921051}]}, {"text": "MaxEnt classifier is used as the base extractor, and the model is evaluated on the entire test set for 1.6 million steps.", "labels": [], "entities": []}, {"text": "The dev set is used to tune all hyperparameters, which can be found in the Appendix.", "labels": [], "entities": []}, {"text": "For RLIE-A3C, the evaluation is carried out 50 times after training and the average accuracy values are taken over the top 5 evaluations, as done in ().", "labels": [], "entities": [{"text": "RLIE-A3C", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.4849048852920532}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992457628250122}]}], "tableCaptions": []}