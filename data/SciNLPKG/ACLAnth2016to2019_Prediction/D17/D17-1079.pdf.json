{"title": [{"text": "Word-Context Character Embeddings for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.5707797408103943}]}], "abstractContent": [{"text": "Neural parsers have benefited from automatically labeled data via dependency-context word embeddings.", "labels": [], "entities": []}, {"text": "We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tri-training baselines for leveraging auto-segmented data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 165, "end_pos": 182, "type": "TASK", "confidence": 0.7611578404903412}]}], "introductionContent": [{"text": "Neural network Chinese word segmentation (CWS) models () appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features ().", "labels": [], "entities": [{"text": "Neural network Chinese word segmentation (CWS)", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.7206300310790539}]}, {"text": "We investigate leveraging automatically segmented texts for enhancing their accuracies.", "labels": [], "entities": []}, {"text": "Such semi-supervised methods can be divided into two main categories.", "labels": [], "entities": []}, {"text": "The first one is bootstrapping, which includes self-training and tritraining.", "labels": [], "entities": []}, {"text": "The idea is to generate more training instances by automatically labeling large-scale data.", "labels": [], "entities": []}, {"text": "Self-training; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training () uses two extra classifiers, taking the instances with the same labels for additional training data.", "labels": [], "entities": []}, {"text": "A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.738143116235733}]}, {"text": "* Equal contributions Tri-training has been used in neural parsing, giving considerable improvements for both of dependency () and constituent parsing (.", "labels": [], "entities": [{"text": "neural parsing", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7221162617206573}, {"text": "constituent parsing", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.6839841902256012}]}, {"text": "Knowledge from auto-labeled data has also been used for parsing (, where word embeddings are trained on automatic dependency tree context.", "labels": [], "entities": [{"text": "parsing", "start_pos": 56, "end_pos": 63, "type": "TASK", "confidence": 0.9852951765060425}]}, {"text": "Such knowledge has also been proved effective in conventional discrete CWS models, such as label distribution information ().", "labels": [], "entities": []}, {"text": "However, it has not been investigated for neural CWS.", "labels": [], "entities": [{"text": "neural CWS", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.5289183855056763}]}, {"text": "We propose word-context character embeddings (WCC), using segmentation label information in the pre-training of unigram and bigram character embeddings.", "labels": [], "entities": [{"text": "word-context character embeddings (WCC)", "start_pos": 11, "end_pos": 50, "type": "TASK", "confidence": 0.7733820378780365}]}, {"text": "The method packs the label distribution information into the embeddings, which could be regarded as away for knowledge parameterization.", "labels": [], "entities": []}, {"text": "Our idea follows, who use dependency contexts to train word embeddings.", "labels": [], "entities": []}, {"text": "Additionally, motivated by co-training, we propose multi-view wordcontext character embeddings for cross-domain segmentation, which pre-trains two types of embedding for in-domain and out-of-domain data, respectively.", "labels": [], "entities": [{"text": "cross-domain segmentation", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.7702909409999847}]}, {"text": "In-domain embeddings are used for solving data sparseness, and out-of-domain embeddings are used for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.748723715543747}]}, {"text": "Our proposed model is simple, efficient and effective, giving average 1% accuracy improvement on in-domain data and 3.5% on out-of-domain data, respectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9993271827697754}]}, {"text": "acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation).", "labels": [], "entities": []}, {"text": "B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively.", "labels": [], "entities": []}, {"text": "S represents that the current character is a single character word.", "labels": [], "entities": []}, {"text": "Following, a standard bi-LSTM model) is used to assign segmentation label for each character.", "labels": [], "entities": []}, {"text": "As shown in, our model consists of a representation layer and a scoring layer.", "labels": [], "entities": []}, {"text": "The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence.", "labels": [], "entities": []}, {"text": "Given a sentence {w 1 , w 2 , w 3 , \u00b7 \u00b7 \u00b7 , w N }, where w i is the i th character in the sentence, and N is the sentence length, we have a corresponding embedding e w i and e w i\u22121 w i for each character unigram w i and character bigram w i\u22121 w i , respectively.", "labels": [], "entities": []}, {"text": "A forward word representation e f i is calculated as follows:  Given the representation r i , we use a scoring unit to score for each potential segment label.", "labels": [], "entities": []}, {"text": "Given r i , the score of segment label M is: W M is the score matrix for label M, and e M is the label embedding for label M.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparisons between greedy and CRF  segmentation. Speed: tokens per millisecond.", "labels": [], "entities": [{"text": "CRF  segmentation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7761379778385162}, {"text": "Speed", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9889432191848755}]}, {"text": " Table 3: Results of self-training and tri-training on  CTB6 with varying scaled training data.", "labels": [], "entities": [{"text": "CTB6", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9668264985084534}]}, {"text": " Table 4: Comparison with other models.", "labels": [], "entities": []}, {"text": " Table 5: Results on the out-of-domain data. Mod- els with  \u2020 do not use large-scale data, models with   \u2021 use in-domain large-scale data, and models with  \u266f use both in-domain, and out-of-domain large-", "labels": [], "entities": []}]}