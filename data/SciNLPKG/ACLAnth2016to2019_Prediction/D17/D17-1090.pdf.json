{"title": [], "abstractContent": [{"text": "This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data.", "labels": [], "entities": []}, {"text": "The contribution of the paper is 2-fold: First, two types of question generation approaches are proposed, one is a retrieval-based method using convo-lution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN); Second, we show how to leverage the generated questions to improve existing question answering systems.", "labels": [], "entities": [{"text": "question generation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7486283779144287}, {"text": "question answering", "start_pos": 332, "end_pos": 350, "type": "TASK", "confidence": 0.716327890753746}]}, {"text": "We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA.", "labels": [], "entities": [{"text": "question generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7073438763618469}, {"text": "answer sentence selection task", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.7693530768156052}, {"text": "SQuAD", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.8620386719703674}, {"text": "MS", "start_pos": 128, "end_pos": 130, "type": "DATASET", "confidence": 0.9351017475128174}, {"text": "MARCO", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.4484303891658783}, {"text": "WikiQA", "start_pos": 142, "end_pos": 148, "type": "DATASET", "confidence": 0.9227665662765503}]}, {"text": "Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved.", "labels": [], "entities": [{"text": "QA", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.8715434074401855}]}], "introductionContent": [{"text": "Question Answering (or QA) is one of the core problems for AI, and consists of several typical tasks, i.e. community-based QA (, knowledge-based QA (), text-based QA (, and reading comprehension ().", "labels": [], "entities": [{"text": "Question Answering (or QA)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8636393944422404}]}, {"text": "Most of current QA systems, e.g., (,,, need labeled QA pairs as training data.", "labels": [], "entities": []}, {"text": "Although labeling efforts have been made, such as WebQuestions dataset and SimpleQuestions dataset () for knowledge-based QA, WikiQA dataset () for text-based QA, SQuAD dataset ( and MS MARCO dataset () for reading comprehension, these datasets are still with limited sizes, as labeling is very expensive.", "labels": [], "entities": [{"text": "WebQuestions dataset", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.9556643664836884}, {"text": "SimpleQuestions dataset", "start_pos": 75, "end_pos": 98, "type": "DATASET", "confidence": 0.8723725974559784}, {"text": "WikiQA dataset", "start_pos": 126, "end_pos": 140, "type": "DATASET", "confidence": 0.8922324776649475}, {"text": "SQuAD dataset", "start_pos": 163, "end_pos": 176, "type": "DATASET", "confidence": 0.7167958319187164}, {"text": "MS MARCO dataset", "start_pos": 183, "end_pos": 199, "type": "DATASET", "confidence": 0.808835506439209}]}, {"text": "Motivated by this, we explore how to generate questions from given passages using neural networks, with three expected goals: (1) the training data should need few or no human efforts and reflect commonly-asked question intentions; (2) the questions are generated based on natural language passages, and should have good quality; (3) the generated questions should be helpful to QA tasks.", "labels": [], "entities": [{"text": "QA tasks", "start_pos": 379, "end_pos": 387, "type": "TASK", "confidence": 0.864843487739563}]}, {"text": "To achieve the 1 st goal, we propose to acquire large scale high-quality training data from Community-QA (CQA) website.", "labels": [], "entities": [{"text": "Community-QA (CQA) website", "start_pos": 92, "end_pos": 118, "type": "DATASET", "confidence": 0.7291629135608673}]}, {"text": "The motivation of using CQA website for training data collection is that, such websites (e.g., YahooAnswers, Quora, etc.) contain large scale QA pairs generated by real users, and these questions reflect the most common user intentions, and therefore are useful to search, QA, and chatbot scenarios.", "labels": [], "entities": [{"text": "training data collection", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6834639012813568}]}, {"text": "To achieve the 2 nd goal, we explore two ways to generate questions fora given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN).", "labels": [], "entities": []}, {"text": "We evaluate the generation quality by BLEU score () and human annotations, and discuss their pros and cons in Section 9.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9789766371250153}]}, {"text": "To achieve the 3 rd goal, we integrate our question generation approach into an end-to-end QA task, i.e., answer sentence selection, and evaluate its impact on three popular benchmark datasets, SQuAD, MS MARCO, and WikiQA.", "labels": [], "entities": [{"text": "question generation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.737985298037529}, {"text": "answer sentence selection", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.836512287457784}, {"text": "MS MARCO", "start_pos": 201, "end_pos": 209, "type": "DATASET", "confidence": 0.6975303590297699}, {"text": "WikiQA", "start_pos": 215, "end_pos": 221, "type": "DATASET", "confidence": 0.935936689376831}]}, {"text": "Experimental results show that, the generated questions can improve the QA quality on all these three datasets.", "labels": [], "entities": [{"text": "QA", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.7437325716018677}]}], "datasetContent": [{"text": "As described in Section 4.1, we collect 1, 984, 401 < A, Q p , Q t > pairs from YahooAnswers, and use them as the training set of the question pattern prediction model.", "labels": [], "entities": [{"text": "question pattern prediction", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.76812215646108}]}, {"text": "We re-use the dev sets and test sets of SQuAD, MS MARCO, and WikiQA, to evaluate the quality of generated questions.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.8928338289260864}, {"text": "MS MARCO", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8118172883987427}]}, {"text": "The dataset statistics are in  Besides, an answer sentence selection model () is trained based on the 1,984,401 QA pairs from the training set as well, and used to compute the QA matching score for question ranking, as we described in Section 6.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.6633628904819489}, {"text": "question ranking", "start_pos": 198, "end_pos": 214, "type": "TASK", "confidence": 0.8050274550914764}]}, {"text": "Feature weights for question ranking are optimized on dev set.", "labels": [], "entities": [{"text": "question ranking", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8289965987205505}]}, {"text": "We first perform a vanilla sequence-to-sequence method () using the original training sets of these three datasets, and show QG results in: QG results using original training sets.", "labels": [], "entities": []}, {"text": "We then evaluate the quality of the generated questions based on auto-extracted training set.", "labels": [], "entities": []}, {"text": "For each passage in the test set, we generate two top-1 questions based on retrieval-based method and generation-based method respectively, and then compare them with labeled questions using BLEU 4 as the metric.", "labels": [], "entities": [{"text": "BLEU 4", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.969983696937561}]}, {"text": "Results are listed in: QG results using auto-extracted training set, where R-QG denotes results from Retrievalbased QG method, G-QG denotes results from Generation-based QG method.", "labels": [], "entities": []}, {"text": "From and 4 we can see two findings: (1) Comparing to QG results based on original labeled training sets, G-QG achieves comparable or better results.", "labels": [], "entities": []}, {"text": "We think this is due to two facts: first, the size of the automatically constructed training set is much larger than the labeled training sets, and second, as the QA pairs from CQA websites are generated by real users, the quality is good.", "labels": [], "entities": []}, {"text": "(2) Generation-based QG performs better than Retrieval-based QG.", "labels": [], "entities": [{"text": "Retrieval-based QG", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.6924667954444885}]}, {"text": "By analyzing outputs we find that, for question pattern prediction, both retrieval-based and generation-based methods perform similarly.", "labels": [], "entities": [{"text": "question pattern prediction", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8143025239308676}]}, {"text": "However, Generation-based QG performs better than Retrieval-based QG on question topic selection.", "labels": [], "entities": [{"text": "question topic selection", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6752868195374807}]}, {"text": "This could be caused by the fact that, in Generation-based QG, question topic selection is based on the attention mechanism, which is optimized together with question pattern prediction in an end-to-end way; while in Retrieval-based QG, question topic selection is a separate task, and based on the similarity between each question topic candidate and historical question topics of a given question pattern.", "labels": [], "entities": [{"text": "question topic selection", "start_pos": 63, "end_pos": 87, "type": "TASK", "confidence": 0.704082727432251}, {"text": "question pattern prediction", "start_pos": 158, "end_pos": 185, "type": "TASK", "confidence": 0.62057497104009}, {"text": "question topic selection", "start_pos": 237, "end_pos": 261, "type": "TASK", "confidence": 0.6463913321495056}]}, {"text": "The embedding of each question topic is pre-trained, which is not directly related to the question generation task.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.7632241149743398}]}, {"text": "So such method cannot handle unseen question topics very well.", "labels": [], "entities": []}, {"text": "Another disadvantage of Retrieval-based QG is that, each time, we have to compute the similarity between the input passage and each question pattern.", "labels": [], "entities": [{"text": "Retrieval-based QG", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8828315138816833}]}, {"text": "When question pattern size is large, the computation is very expensive.", "labels": [], "entities": []}, {"text": "In order to better understand the question generation quality, we manually check a set of sampled outputs, and list the main errors in: \u2022 Multi-Fact Error (40%).", "labels": [], "entities": [{"text": "question generation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7678211331367493}, {"text": "Multi-Fact Error", "start_pos": 138, "end_pos": 154, "type": "METRIC", "confidence": 0.9268347918987274}]}, {"text": "Most input passages include more than one fact.", "labels": [], "entities": []}, {"text": "For such a question, it is reasonable to generate different questions from different aspects, all of which can be answered by the input passage.", "labels": [], "entities": []}, {"text": "For each passage in QAGen, we only label one question as ground truth.", "labels": [], "entities": []}, {"text": "In the future, we will extend QAGen to be a more comprehensive dataset, by labeling multiple questions to each passage for more reasonable evaluation; \u2022 Paraphrase Error (30%).", "labels": [], "entities": [{"text": "Paraphrase Error", "start_pos": 153, "end_pos": 169, "type": "METRIC", "confidence": 0.9008390009403229}]}, {"text": "The same question can be expressed by different ways.", "labels": [], "entities": []}, {"text": "Labeling more paraphrased questions fora passage can alleviate this issue as well; \u2022 Question Topic Selection Error (15%).", "labels": [], "entities": [{"text": "Error", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.47773998975753784}]}, {"text": "This error is caused by selecting either a total- ly wrong question topic, or a partially right question topic.", "labels": [], "entities": []}, {"text": "In the future, we plan to develop an independent question topic selection model for the question generation task.", "labels": [], "entities": [{"text": "question generation task", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.8586589694023132}]}, {"text": "As described in Section 7, we combine question generation into QA system for answer sentence selection task, and do evaluation on SQuAD, M-S MARCO, and WikiQA.", "labels": [], "entities": [{"text": "question generation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7622260451316833}, {"text": "answer sentence selection task", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.7761495411396027}, {"text": "M-S", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.709465503692627}, {"text": "MARCO", "start_pos": 141, "end_pos": 146, "type": "METRIC", "confidence": 0.4129704236984253}, {"text": "WikiQA", "start_pos": 152, "end_pos": 158, "type": "DATASET", "confidence": 0.9085611701011658}]}, {"text": "Evaluation results are shown in: Impact of QG on MS MARCO.", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.6058565825223923}]}, {"text": "The improvement on MS MARCO dataset is most significant.", "labels": [], "entities": [{"text": "MS MARCO dataset", "start_pos": 19, "end_pos": 35, "type": "DATASET", "confidence": 0.8233009378115336}]}, {"text": "We think it due to the fact that, the questions from MS MARCO dataset are from Bing search log, which are generated naturally by real users.", "labels": [], "entities": [{"text": "MS MARCO dataset", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8862618009249369}, {"text": "Bing search log", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.8786242604255676}]}, {"text": "This is similar to the questions coming WikiQA MAP MRR ACC@1 QA 0.7703 0.7851 0.6540 QA+QG 0.7742 0.7893 0.6624: Impact of QG on WikiQA.", "labels": [], "entities": [{"text": "WikiQA MAP MRR ACC@1 QA 0.7703 0.7851 0.6540 QA+QG 0.7742", "start_pos": 40, "end_pos": 97, "type": "METRIC", "confidence": 0.7466216066053936}]}, {"text": "for CQA websites; while questions from the other datasets are labeled by crowd-sourcing.", "labels": [], "entities": []}, {"text": "In order to explain these improvements, two datasets, WikiQG+ and WikiQG-, are built from WikiQA test set: given each document and its labeled question, we pair the question with its COR-RECT answer sentence as a QA pair and add it to WikiQG+; we also pair the same question with a randomly selected WRONG answer sentence as a QA pair and add it to WikiQG-.", "labels": [], "entities": [{"text": "WikiQA test set", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.9080755710601807}]}, {"text": "Then, we generate questions for passages in WikiQG+ and WikiQGrespectively, and compare them with labeled questions.", "labels": [], "entities": [{"text": "WikiQGrespectively", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.937125027179718}]}, {"text": "The BLEU 4 score is 0.2031 on WikiQG+, and 0.1301 on WikiQG-, which indicates that the questions generated from correct answers are more likely to be similar to labeled questions than questions generated from wrong answers.", "labels": [], "entities": [{"text": "BLEU 4 score", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9773229757944742}, {"text": "WikiQG+", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.905510812997818}, {"text": "WikiQG", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9274968504905701}]}], "tableCaptions": [{"text": " Table 3: QG results using original training sets.", "labels": [], "entities": []}, {"text": " Table 4: QG results using auto-extracted training  set, where R-QG denotes results from Retrieval- based QG method, G-QG denotes results from  Generation-based QG method.", "labels": [], "entities": []}, {"text": " Table 5: Impact of QG on SQuAD.", "labels": [], "entities": [{"text": "SQuAD", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.5798030495643616}]}, {"text": " Table 6: Impact of QG on MS MARCO.", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.696132242679596}]}]}