{"title": [{"text": "SCDV : Sparse Composite Document Vectors using soft clustering over distributional representations", "labels": [], "entities": [{"text": "Sparse Composite Document Vectors", "start_pos": 7, "end_pos": 40, "type": "TASK", "confidence": 0.7449638992547989}]}], "abstractContent": [{"text": "We present a feature vector formation technique for documents-Sparse Composite Document Vector (SCDV)-which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation.", "labels": [], "entities": [{"text": "feature vector formation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7463844815889994}, {"text": "text representation", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7530496716499329}]}, {"text": "In SCDV, word em-beddings are clustered to capture multiple semantic contexts in which words occur.", "labels": [], "entities": []}, {"text": "They are then chained together to form document topic-vectors that can express complex, multi-topic documents.", "labels": [], "entities": []}, {"text": "Through extensive experiments on multi-class and multi-label classification tasks, we outper-form the previous state-of-the-art method, NTSG (Liu et al., 2015a).", "labels": [], "entities": [{"text": "multi-label classification tasks", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.7391232152779897}, {"text": "NTSG", "start_pos": 136, "end_pos": 140, "type": "DATASET", "confidence": 0.8380594253540039}]}, {"text": "We also show that SCDV embeddings perform well on heterogeneous tasks like Topic Coherence, context-sensitive Learning and Information Retrieval.", "labels": [], "entities": [{"text": "Topic Coherence", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.8963471949100494}, {"text": "Information Retrieval", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.8071912825107574}]}, {"text": "Moreover, we achieve significant reduction in training and prediction times compared to other representation methods.", "labels": [], "entities": []}, {"text": "SCDV achieves best of both worlds-better performance with lower time and space complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed word embeddings represent words as dense, low-dimensional and real-valued vectors that can capture their semantic and syntactic properties.", "labels": [], "entities": []}, {"text": "These embeddings are used abundantly by machine learning algorithms in tasks such as text classification and clustering.", "labels": [], "entities": [{"text": "text classification", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7969062924385071}]}, {"text": "Traditional bagof-word models that represent words as indices into a vocabulary don't account for word ordering and long-distance semantic relations.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.7098647803068161}]}, {"text": "Representations based on neural network language models *Represents equal contribution) can overcome these flaws and further reduce the dimensionality of the vectors.", "labels": [], "entities": []}, {"text": "The success of the method is recently mathematically explained using the random walk on discourses model ().", "labels": [], "entities": []}, {"text": "However, there is a need to extend word embeddings to entire paragraphs and documents for tasks such as document and short-text classification.", "labels": [], "entities": [{"text": "document and short-text classification", "start_pos": 104, "end_pos": 142, "type": "TASK", "confidence": 0.6130210533738136}]}, {"text": "Representing entire documents in a dense, lowdimensional space is a challenge.", "labels": [], "entities": [{"text": "Representing entire documents", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8605175018310547}]}, {"text": "A simple weighted average of the word embeddings in a large chunk of text ignores word ordering, while a parse tree based combination of embeddings ( can only extend to sentences.", "labels": [], "entities": [{"text": "word ordering", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.6981638222932816}]}, {"text": "() trains word and paragraph vectors to predict context but shares wordembeddings across paragraphs.", "labels": [], "entities": []}, {"text": "However, words can have different semantic meanings in different contexts.", "labels": [], "entities": []}, {"text": "Hence, vectors of two documents that contain the same word in two distinct senses need to account for this distinction for an accurate semantic representation of the documents.", "labels": [], "entities": []}, {"text": "(, () map word embeddings to a latent topic space to capture different senses in which words occur.", "labels": [], "entities": []}, {"text": "However, they represent complex documents in the same space as words, reducing their expressive power.", "labels": [], "entities": []}, {"text": "These methods are also computationally intensive.", "labels": [], "entities": []}, {"text": "In this work, we propose the Sparse Composite Document Vector(SCDV) representation learning technique to address these challenges and create efficient, accurate and robust semantic representations of large texts for document classification tasks.", "labels": [], "entities": [{"text": "Sparse Composite Document Vector(SCDV) representation learning", "start_pos": 29, "end_pos": 91, "type": "TASK", "confidence": 0.7850385871198442}, {"text": "document classification tasks", "start_pos": 216, "end_pos": 245, "type": "TASK", "confidence": 0.802055299282074}]}, {"text": "SCDV combines syntax and semantics learnt byword embedding models together with a latent topic model that can handle different senses of words, thus enhancing the expressive power of document vectors.", "labels": [], "entities": []}, {"text": "The topic space is learnt efficiently using a soft clustering technique over embeddings and the final document vectors are made sparse for reduced time and space complexity in tasks that consume these vectors.", "labels": [], "entities": []}, {"text": "The remaining part of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work in document representations.", "labels": [], "entities": [{"text": "document representations", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.7141183167695999}]}, {"text": "Section 3 introduces and explains SCDV in detail.", "labels": [], "entities": [{"text": "SCDV", "start_pos": 34, "end_pos": 38, "type": "TASK", "confidence": 0.8659891486167908}]}, {"text": "This is followed by extensive and rigorous experiments together with analysis in section 4 and 5 respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform multiple experiments to show the effectiveness of SCDV representations for multiclass and multi-label text classification.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.5927975078423818}]}, {"text": "For all experiments and baselines, we use Intel(R) Xeon(R) CPU E5-2670 v2 @ 2.50GHz, 40 working cores, 128GB RAM machine with Linux Ubuntu 14.4.", "labels": [], "entities": []}, {"text": "However, we utilize multiple cores only during Word2Vec training and when we run the one-vsrest classifier for Reuters.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8961465358734131}, {"text": "Reuters", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9297063946723938}]}], "tableCaptions": [{"text": " Table 1: Performance on multi-class classification  (Values in red show best performance, the SCDV  algorithm of this paper)", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7393139600753784}]}, {"text": " Table 2: Class-level results on the balanced  20newsgroup dataset.", "labels": [], "entities": [{"text": "20newsgroup dataset", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9094835519790649}]}, {"text": " Table 3: Performance on various metrics for multi-label classification for Reuters(Values in red show  best performance, the SCDV algorithm of this paper)", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7597484588623047}, {"text": "Reuters", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.7945255041122437}]}, {"text": " Table 6: Mean average precision (MAP) for IR on four IR datasets", "labels": [], "entities": [{"text": "Mean average precision (MAP)", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.9559495647748312}, {"text": "IR", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9861444234848022}]}, {"text": " Table 7: Time Comparison (20NewsGroup) (Val- ues in red show least time, the SCDV algorithm of  this paper)", "labels": [], "entities": []}]}