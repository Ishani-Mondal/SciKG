{"title": [], "abstractContent": [{"text": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records.", "labels": [], "entities": []}, {"text": "In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task.", "labels": [], "entities": [{"text": "data-to-text generation", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7421262860298157}]}, {"text": "In particular, we introduce anew, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods.", "labels": [], "entities": []}, {"text": "Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents.", "labels": [], "entities": []}, {"text": "Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy-and reconstruction-based extensions lead to noticeable improvements .", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past several years, neural text generation systems have shown impressive performance on tasks such as machine translation and summarization.", "labels": [], "entities": [{"text": "neural text generation", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.6724867026011149}, {"text": "machine translation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.8216297626495361}, {"text": "summarization", "start_pos": 135, "end_pos": 148, "type": "TASK", "confidence": 0.9136481881141663}]}, {"text": "As neural systems begin to move toward generating longer outputs in response to longer and more complicated inputs, however, the generated texts begin to display reference errors, intersentence incoherence, and alack of fidelity to the source material.", "labels": [], "entities": []}, {"text": "The goal of this paper is to suggest a particular, long-form generation task in which these challenges maybe fruitfully explored, to provide a publically available dataset for this task, to suggest some automatic evaluation metrics, and finally to establish how current, neural text generation methods perform on this task.", "labels": [], "entities": []}, {"text": "A classic problem in natural-language generation (NLG)) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output.", "labels": [], "entities": [{"text": "natural-language generation (NLG))", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.8756441593170166}]}, {"text": "Unlike machine translation, which aims fora complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.7459776103496552}]}, {"text": "Traditionally, these two challenges have been modularized and handled separately by generation systems.", "labels": [], "entities": []}, {"text": "However, neural generation systems, which are typically trained end-to-end as conditional language models (, blur this distinction.", "labels": [], "entities": []}, {"text": "In this context, we believe the problem of generating multi-sentence summaries of tables or database records to be a reasonable next-problem for neural techniques to tackle as they begin to consider more difficult NLG tasks.", "labels": [], "entities": []}, {"text": "In particular, we would like this generation task to have the following two properties: (1) it is relatively easy to obtain fairly clean summaries and their corresponding databases for dataset construction, and (2) the summaries should be primarily focused on conveying the information in the database.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 185, "end_pos": 205, "type": "TASK", "confidence": 0.7347002327442169}]}, {"text": "This latter property ensures that the task is somewhat congenial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database.", "labels": [], "entities": []}, {"text": "One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed along history of NLG work that generates sports game summaries.", "labels": [], "entities": [{"text": "summaries of sports games from associated box-score", "start_pos": 57, "end_pos": 108, "type": "TASK", "confidence": 0.8170198968478611}]}, {"text": "To this end, we make the following contributions: \u2022 We introduce anew large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables.", "labels": [], "entities": []}, {"text": "This dataset is sufficiently large that fully data-driven approaches might be sufficient.", "labels": [], "entities": []}, {"text": "\u2022 We introduce a series of extractive evaluation models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.754283219575882}]}, {"text": "\u2022 We apply a series of state-of-the-art neural methods, as well as a simple templated generation system, to our data-to-document generation task in order to establish baselines and study their generations.", "labels": [], "entities": []}, {"text": "Our experiments indicate that neural systems are quite good at producing fluent outputs and generally score well on standard word-match metrics, but perform quite poorly at content selection and at capturing long-term structure.", "labels": [], "entities": [{"text": "content selection", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7165066599845886}]}, {"text": "While the use of copy-based models and additional reconstruction terms in the training loss can lead to improvements in BLEU and in our proposed extractive evaluations, current models are still quite far from producing human-level output, and are significantly worse than templated systems in terms of content selection and realization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9905906319618225}]}, {"text": "Overall, we believe this problem of data-to-document generation highlights important remaining challenges in neural generation systems, and the use of extractive evaluation reveals significant issues hidden by standard automatic metrics.", "labels": [], "entities": [{"text": "data-to-document generation", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.7793039083480835}]}], "datasetContent": [{"text": "We consider the problem of generating descriptive text from database records.", "labels": [], "entities": []}, {"text": "Following the notation in, let s = {r j } J j=1 be a set of records, where for each r \u2208 s we define r.t \u2208 T to be the type of r, and we assume each r to be a binarized relation, where r.e and r.m area record's entity and value, respectively.", "labels": [], "entities": []}, {"text": "For example, a database recording statistics fora basketball game might have a record r such that r.t = POINTS, r.e = RUSSELL WESTBROOK, and r.m = 50.", "labels": [], "entities": [{"text": "POINTS", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9956377148628235}, {"text": "RUSSELL WESTBROOK", "start_pos": 118, "end_pos": 135, "type": "METRIC", "confidence": 0.880284458398819}]}, {"text": "In this case, r.e gives the player in question, and r.m gives the number of points the player scored.", "labels": [], "entities": []}, {"text": "From these records, we are interested in generating descriptive text, \u02c6 y 1:T = \u02c6 y 1 , . .", "labels": [], "entities": []}, {"text": ", \u02c6 y T of T words such that\u02c6ythat\u02c6 that\u02c6y 1:T is an adequate and fluent summary of s.", "labels": [], "entities": []}, {"text": "A dataset for training data-to-document systems typically consists of (s, y 1:T ) pairs, where y 1:T is a document consisting of a gold (i.e., human generated) summary for database s.", "labels": [], "entities": []}, {"text": "Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being WEATHERGOV () and ROBOCUP.", "labels": [], "entities": [{"text": "text generation task", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.8750027020772299}, {"text": "ROBOCUP", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.6782835125923157}]}, {"text": "Recently, neural generation systems have show strong results on these datasets, with the system of achieving BLEU scores in the 60s and 70s on WEATHERGOV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.9787569046020508}, {"text": "WEATHERGOV", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.8927875757217407}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9993587136268616}, {"text": "ROBOCUP dataset", "start_pos": 204, "end_pos": 219, "type": "DATASET", "confidence": 0.8758615851402283}]}, {"text": "These results are quite promising, and suggest that neural models area good fit for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.854902982711792}]}, {"text": "However, the statistics of these datasets, shown in, indicate that these datasets use relatively simple language and record structure.", "labels": [], "entities": []}, {"text": "Furthermore, there is reason to believe that WEATHERGOV is at least partially machinegenerated.", "labels": [], "entities": [{"text": "WEATHERGOV", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.7092344164848328}]}, {"text": "More recently, introduced the WIKIBIO dataset, which is at least an order of magnitude larger in terms of number of tokens and record types.", "labels": [], "entities": [{"text": "WIKIBIO dataset", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9572343528270721}]}, {"text": "However, as shown in, this dataset too only contains short (single-sentence) generations, and relatively few records per generation.", "labels": [], "entities": []}, {"text": "As such, we believe that early success on these datasets is not yet sufficient for testing the desired linguistic capabilities of text generation at a document-scale.", "labels": [], "entities": [{"text": "text generation", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7195166349411011}]}, {"text": "With this challenge in mind, we introduce anew dataset for data-to-document text generation, available at https://github.com/ harvardnlp/boxscore-data.", "labels": [], "entities": [{"text": "data-to-document text generation", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.679198145866394}]}, {"text": "The dataset is intended to be comparable to WEATHERGOV in terms of token count, but to have significantly longer target texts, a larger vocabulary space, and to require more difficult content selection.", "labels": [], "entities": []}, {"text": "The dataset consists of two sources of articles summarizing NBA basketball games, paired with their corresponding box-and line-score tables.", "labels": [], "entities": []}, {"text": "The data statistics of these two sources, RO-TOWIRE and SBNATION, are also shown in Table 1.", "labels": [], "entities": [{"text": "RO-TOWIRE", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.8462287187576294}, {"text": "SBNATION", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8213956356048584}]}, {"text": "The first dataset, ROTOWIRE, uses professionally written, medium length game summaries targeted at fantasy basketball fans.", "labels": [], "entities": [{"text": "ROTOWIRE", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.706101655960083}]}, {"text": "The writing is colloquial, but relatively well structured, and targets an audience primarily interested in game   , and a selection from the gold document.", "labels": [], "entities": []}, {"text": "The document mentions only a select subset of the records, but may express them in a complicated manner.", "labels": [], "entities": []}, {"text": "In addition to capturing the writing style, a generation system should select similar record content, express it clearly, and order it appropriately.", "labels": [], "entities": []}, {"text": "To address this evaluation challenge, we begin with the intuition that assessing document quality is easier than document generation.", "labels": [], "entities": [{"text": "document generation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6750997006893158}]}, {"text": "In particular, it is much easier to automatically extract information from documents than to generate documents that accurately convey desired information.", "labels": [], "entities": []}, {"text": "As such, simple, high-precision information extraction models can serve as the basis for assessing and better understanding the quality of automatic generations.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7597472369670868}]}, {"text": "We emphasize that such an evaluation scheme is most appropriate when evaluating generations (such as basketball game summaries) that are primarily intended to summarize information.", "labels": [], "entities": [{"text": "basketball game summaries)", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.7553560733795166}, {"text": "summarize information", "start_pos": 159, "end_pos": 180, "type": "TASK", "confidence": 0.9116808474063873}]}, {"text": "While many generation problems do not fall into this category, we believe this to bean interesting category, and one worth focusing on because it is amenable to this sort of evaluation.", "labels": [], "entities": []}, {"text": "To see how a simple information extraction system might work, consider the document in Figure 1.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7853803932666779}]}, {"text": "We may first extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair.", "labels": [], "entities": []}, {"text": "For example, we might extract the entity-value pair (\"Miami Heat\", \"95\") from the first sentence in, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (MIAMI HEAT, 95, POINTS).", "labels": [], "entities": [{"text": "POINTS", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9869419932365417}, {"text": "MIAMI HEAT", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.7711369395256042}, {"text": "POINTS", "start_pos": 233, "end_pos": 239, "type": "METRIC", "confidence": 0.969944179058075}]}, {"text": "Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8652183711528778}, {"text": "relation extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8568585813045502}, {"text": "multi-class classification", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7063072323799133}]}, {"text": "More concretely, given a document\u02c6ydocument\u02c6 document\u02c6y 1:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m.", "labels": [], "entities": []}, {"text": "Importantly, we note that the (s, y 1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction model presented above, since we can obtain (partial) supervision by simply checking whether a candidate record lexically matches a record in s.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.75559002161026}]}, {"text": "1 However, since there maybe multiple records r \u2208 s with the same e and m but with different types r.t, we will not always be able to determine the type of a given entity-value pair found in the text.", "labels": [], "entities": []}, {"text": "We therefore train our classifier to minimize a latent-variable loss: for all document spans e and m, with observed types t(e, m) = {r.t : r \u2208 s, r.e = e, r.m = m} (possibly {}), we minimize We find that this simple system trained in this way is quite accurate at predicting relations.", "labels": [], "entities": []}, {"text": "On the Alternative approaches explicitly align the document with the table for this task ().", "labels": [], "entities": []}, {"text": "ROTOWIRE data it achieves over 90% accuracy on held-out data, and recalls approximately 60% of the relations licensed by the records.", "labels": [], "entities": [{"text": "ROTOWIRE", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7043558359146118}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.999382495880127}]}, {"text": "In this section we highlight a few important details of our models and methods; full details are in the Appendix.", "labels": [], "entities": []}, {"text": "For our ROTOWIRE models, the record encoder produces\u02dcrproduces\u02dc produces\u02dcr j in R 600 , and we use a 2-layer LSTM decoder with hidden states of the same size as the\u02dcrthe\u02dc the\u02dcr j , and dot-product attention and input-feeding in the style of.", "labels": [], "entities": []}, {"text": "Unlike past work, we use two identically structured attention layers, one to compute the standard generation probabilities (gen or p gen ), and one to produce the scores used in copy or p copy . We train the generation models using SGD and truncated BPTT, as in language modeling.", "labels": [], "entities": []}, {"text": "That is, we split each y 1:T into contiguous blocks of length 100, and backprop both the gradients with respect to the current block as well as with respect to the encoder parameters for each block.", "labels": [], "entities": []}, {"text": "Our extractive evaluator consists of an ensemble of 3 single-layer convolutional and 3 singlelayer bidirectional LSTM models.", "labels": [], "entities": []}, {"text": "The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the style of).", "labels": [], "entities": []}, {"text": "Both models are trained with SGD.", "labels": [], "entities": [{"text": "SGD", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.584812581539154}]}, {"text": "Templatized Generator In addition to neural baselines, we also use a problem-specific, template-based generator.", "labels": [], "entities": []}, {"text": "The template-based generator first emits a sentence about the teams playing in the game, using a templatized sentence taken from the training set: The <team1> (<wins1>-<losses1>) defeated the <team2> (<wins2>-<losses2>) <pts1>-<pts2>.", "labels": [], "entities": []}, {"text": "Then, 6 player-specific sentences of the following form are emitted (again adapting a simple sentence from the training set): <player> scored <pts> points (<fgm>-<fga> FG, <tpm>-<tpa> 3PT, <ftm>-<fta> FT) to go with <reb> rebounds.", "labels": [], "entities": [{"text": "FG", "start_pos": 168, "end_pos": 170, "type": "METRIC", "confidence": 0.9066099524497986}, {"text": "FT", "start_pos": 201, "end_pos": 203, "type": "METRIC", "confidence": 0.9747348427772522}]}, {"text": "encouraging, rather than penalizing the TVD between the pk , which might make sense if we were worried about ensuring the pk captured different records.", "labels": [], "entities": [{"text": "TVD", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.853735625743866}]}, {"text": "The 6 highest-scoring players in the game are used to fill in the above template.", "labels": [], "entities": []}, {"text": "Finally, atypical end sentence is emitted: The <team1>' next game will beat home against the Dallas Mavericks, while the <team2> will travel to play the Bulls.", "labels": [], "entities": []}, {"text": "Code implementing all models can be found at https://github.com/harvardnlp/ data2text.", "labels": [], "entities": []}, {"text": "Our encoder-decoder models are based on OpenNMT ().", "labels": [], "entities": []}, {"text": "We also undertook two human evaluation studies, using Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.9685353835423788}]}, {"text": "The first study attempted to determine whether generations considered to be more precise by our metrics were also considered more precise by human raters.", "labels": [], "entities": []}, {"text": "To accomplish this, raters were presented with a particular NBA game's box score and line score, as well as with (randomly selected) sentences from summaries generated by our different models for those games.", "labels": [], "entities": [{"text": "line score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9051145017147064}]}, {"text": "Raters were then asked to count how many facts in each sentence were supported by records in the box or line scores, and how many were contradicted.", "labels": [], "entities": []}, {"text": "We randomly selected 20 distinct games to present to raters, and a total of 20 generated sentences per game were evaluated by raters.", "labels": [], "entities": []}, {"text": "The left two columns of  average numbers of supporting and contradicting facts per sentence as determined by the raters, for each model.", "labels": [], "entities": []}, {"text": "We see that these results are generally inline with the RG and CS metrics, with the Conditional Copy model having the highest number of supporting facts, and the reconstruction terms significantly improving the Joint Copy models.", "labels": [], "entities": []}, {"text": "Using a Tukey HSD post-hoc analysis of an ANOVA with the number of contradicting facts as the dependent variable and the generating model and rater id as independent variables, we found significant (p < 0.01) pairwise differences in contradictory facts between the gold generations and all models except \"Copy+Rec+TVD,\" as well as a significant difference between \"Copy+Rec+TVD\" and \"Copy\".", "labels": [], "entities": []}, {"text": "We similarly found a significant pairwise difference between \"Copy+Rec+TVD\" and \"Copy\" for number of supporting facts.", "labels": [], "entities": []}, {"text": "Our second study attempted to determine whether generated summaries differed in terms of how natural their ordering of records (as captured, for instance, by the DLD metric) is.", "labels": [], "entities": []}, {"text": "To test this, we presented raters with random summaries generated by our models and asked them to rate the naturalness of the ordering of facts in the summaries on a 1-7 Likert scale.", "labels": [], "entities": []}, {"text": "30 random summaries were used in this experiment, each rated 3 times by distinct raters.", "labels": [], "entities": []}, {"text": "The average Likert ratings are shown in the rightmost column of  While it is encouraging that the gold summaries received a higher average score than the generated summaries (and that the reconstruction term again improved the Joint Copy model), a Tukey HSD analysis similar to the one presented above revealed no significant pairwise differences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Vocabulary size, number of total tokens, number of  distinct examples, average generation length, total number of  record types, and average number of records per example for  the ROBOCUP (RC), WEATHERGOV (WG), WIKIBIO (WB),  ROTOWIRE (RW), and SBNATION (SBN) datasets.", "labels": [], "entities": []}, {"text": " Table 2: Performance of induced metrics on gold and system outputs of RotoWire development and test data. Columns indicate  Record Generation (RG) precision and count, Content Selection (CS) precision and recall, Count Ordering (CO) in normalized  Damerau-Levenshtein distance, perplexity, and BLEU. These first three metrics are described in Section 3.2. Models com- pare Joint and Conditional Copy also with addition Reconstruction loss and Total Variation Distance extensions (described in  Section 4).", "labels": [], "entities": [{"text": "Record Generation (RG) precision", "start_pos": 125, "end_pos": 157, "type": "METRIC", "confidence": 0.7317469914754232}, {"text": "Content Selection (CS) precision", "start_pos": 169, "end_pos": 201, "type": "METRIC", "confidence": 0.6611536045869192}, {"text": "recall", "start_pos": 206, "end_pos": 212, "type": "METRIC", "confidence": 0.9739750027656555}, {"text": "Count Ordering (CO)", "start_pos": 214, "end_pos": 233, "type": "METRIC", "confidence": 0.9662363529205322}, {"text": "BLEU", "start_pos": 295, "end_pos": 299, "type": "METRIC", "confidence": 0.9985787868499756}, {"text": "Reconstruction loss", "start_pos": 420, "end_pos": 439, "type": "METRIC", "confidence": 0.9575999975204468}, {"text": "Total Variation Distance extensions", "start_pos": 444, "end_pos": 479, "type": "METRIC", "confidence": 0.8870600610971451}]}, {"text": " Table 3: Average rater judgment of number of box score  fields supporting (left column) or contradicting (middle col- umn) a generated sentence, and average rater Likert rating for  the naturalness of a summary's ordering (right column). All  generations use B=1.", "labels": [], "entities": [{"text": "rater judgment of number of box score  fields", "start_pos": 18, "end_pos": 63, "type": "METRIC", "confidence": 0.8018093183636665}, {"text": "B", "start_pos": 260, "end_pos": 261, "type": "METRIC", "confidence": 0.9922850131988525}]}]}