{"title": [], "abstractContent": [{"text": "Search systems are often focused on providing relevant results for the \"now\", assuming both corpora and user needs that focus on the present.", "labels": [], "entities": []}, {"text": "However, many corpora today reflect significant longitudinal collections ranging from 20 years of the Web to hundreds of years of digitized newspapers and books.", "labels": [], "entities": []}, {"text": "Understanding the temporal intent of the user and retrieving the most relevant historical content has become a significant challenge.", "labels": [], "entities": []}, {"text": "Common search features, such as query expansion, leverage the relationship between terms but cannot function well across all times when relationships vary temporally.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.7718353867530823}]}, {"text": "In this work, we introduce a temporal relationship model that is extracted from longitudinal data collections.", "labels": [], "entities": []}, {"text": "The model supports the task of identifying, given two words, when they relate to each other.", "labels": [], "entities": []}, {"text": "We present an algorithmic framework for this task and show its application for the task of query expansion, achieving high gain.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.785129964351654}]}], "introductionContent": [{"text": "The focus of large-scale Web search engines is largely on providing the best access to present snapshots of text -what we call the \"Now Web\".", "labels": [], "entities": []}, {"text": "The system constraints and motivating use cases of traditional information retrieval (IR) systems, coupled with the relatively short history of the Web, has meant that little attention has been paid to how search engines will function when search must scale not only to the number of documents but also temporally.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.7720314621925354}]}, {"text": "Most IR systems assume fixed language models and lexicons.", "labels": [], "entities": [{"text": "IR", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9794358015060425}]}, {"text": "They focus only on the leading edge of query behavior (i.e., what does the user likely mean today when they type \"Jaguar\").", "labels": [], "entities": []}, {"text": "In this context, features as basic as disambiguation and spelling corrections are fixed to what is most likely today or within the past few years (, query expansions and synonyms are weighted towards current information, and results tend to include the most recent and popular content.", "labels": [], "entities": [{"text": "spelling corrections", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.659433513879776}]}, {"text": "While this problem would seem to be speculative in that it will be years until we need to address it, the reality is the rate of change () of the Web, language, and culture have simply compressed the time in which critical changes happen.", "labels": [], "entities": []}, {"text": "The \"Now Web\" assumptions are entirely reasonable for temporally coherent text collections and allow users (and search engines) to ignore the complexity of changing language and concentrate on a narrower (though by no means simpler) set of issues.", "labels": [], "entities": []}, {"text": "The reality is that this serves a significant user population effectively.", "labels": [], "entities": []}, {"text": "There are nonetheless a growing number of both corpora and users who require access not just to what is relevant at a particular instant (e.g.,, historical news corpora, the Internet Archives, and even fast changing Twitter feeds).", "labels": [], "entities": []}, {"text": "Within such contexts, a search engine will need to vary the way it functions (e.g., disambiguation) and interacts (e.g., suggested query expansions) depending on the period and temporal scale of documents being queried.", "labels": [], "entities": []}, {"text": "This, of course, is further complicated by the fact that Web pages are constantly evolving and replaced.", "labels": [], "entities": []}, {"text": "Take for example the query \"Prime Minister Ariel Sharon\".", "labels": [], "entities": []}, {"text": "When fed into a news archive search engine, the likely intent was finding results about Sharon's role as Israel's Prime Minister, a role held from 2001 to 2006.", "labels": [], "entities": []}, {"text": "refer to this as a Historical Query Intent.", "labels": [], "entities": [{"text": "Historical Query Intent", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.6200204888979594}]}, {"text": "However, most popular search engines return results about Sharon's death in 2011, when he was no longer prime minister.", "labels": [], "entities": []}, {"text": "The searcher may take the additional step of filtering results to a time period, but this requires knowing what that period should be.", "labels": [], "entities": []}, {"text": "Other query features are also unresponsive to temporal context.", "labels": [], "entities": []}, {"text": "For example, the top query suggestions for this query focus on more recent events of his death: \"Former prime minister Sharon dies at 85\", \"Former prime minister Sharon's condition worsens\", etc.", "labels": [], "entities": []}, {"text": "While these might satisfy the searcher if they are looking for the latest results, or the results most covered by the press, there are clearly other possible needs.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the task of measuring word relatedness overtime.", "labels": [], "entities": [{"text": "word relatedness overtime", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6414852341016134}]}, {"text": "Specifically, we infer whether two words (tokens) relate to each other during a certain time range.", "labels": [], "entities": []}, {"text": "This task is an essential building block of many temporal applications and we specifically target time-sensitive query expansion (QE).", "labels": [], "entities": [{"text": "time-sensitive query expansion (QE)", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6794302562872568}]}, {"text": "Our focus is on semantic relatedness rather than semantic similarity.", "labels": [], "entities": []}, {"text": "Relatedness assumes many different kinds of specific relations (e.g. meronymy, antonymy, functional association) and is often more useful for computational linguistics applications than the more narrow notion of similarity ().", "labels": [], "entities": []}, {"text": "We present several temporal word-relatedness algorithms.", "labels": [], "entities": []}, {"text": "Our method utilizes a large scale temporal corpus spanning over 150 years (The New York Times archive) to generate temporal deep word embeddings.", "labels": [], "entities": [{"text": "The New York Times archive)", "start_pos": 75, "end_pos": 102, "type": "DATASET", "confidence": 0.8457278609275818}]}, {"text": "We describe several algorithms to measure word relatedness overtime using these temporal embeddings.", "labels": [], "entities": [{"text": "word relatedness", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6959976255893707}]}, {"text": "presents the performance of one of those algorithms on the words \"Obama\" and \"President\".", "labels": [], "entities": [{"text": "President", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.8685494661331177}]}, {"text": "Note that the highest relatedness score for the words appears during the presidential term of Barack Obama.", "labels": [], "entities": []}, {"text": "Similarly, shows a high score for \"Ariel Sharon\" and \"prime minister\" only during his term.", "labels": [], "entities": []}, {"text": "Using the approach above, we present a specific application -producing temporally appropriate query-expansions.", "labels": [], "entities": []}, {"text": "For example, consider the query: \"Trump Businessman\".", "labels": [], "entities": []}, {"text": "shows the non-temporal query expansion suggestions which focus heavily on the first entity (i.e., \"Trump\") and his current \"state\" (i.e., a focus on Donald Trump as President, rather than presenting suggestions about Trump's business activity as implied by the query).", "labels": [], "entities": []}, {"text": "We present an empirical analysis presenting the strengths and weaknesses of the different temporal query-expansion algorithms and comparing them to current word-embeddings-based QE Figure 1: Similarity identified by our algorithms between words overtime.", "labels": [], "entities": []}, {"text": "Dark gray indicates high similarity whereas light gray indicates nonsignificant similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9779265522956848}]}, {"text": "In this paper we describe a novel problem of evaluating word relatedness overtime and contribute our datasets to evaluate this task to the community . Second, we present novel representations and algorithms for evaluating this task and show high performance.", "labels": [], "entities": [{"text": "word relatedness overtime", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.7258541981379191}]}, {"text": "We share our code with the community as well.", "labels": [], "entities": []}, {"text": "Finally, we present the application of this task to query-expansion and present several methods built on top of the temporal relatedness algorithms that show high performance for QE.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the methods described in Section 3.3, where for Cl we chose to use a Support Vector Machine (SVM) , with an RBF kernel and C=1.0 (chosen empirically).", "labels": [], "entities": []}, {"text": "Two baselines were used for comparison.", "labels": [], "entities": []}, {"text": "The first is the common nontemporal model, i.e. a classifier that uses the global (all-time) word embeddings and the following features: the two entities' global embeddings, and a year.", "labels": [], "entities": []}, {"text": "More formally, Given anew temporal relation, the classifier predicts whether it is true during the referenced year, and we output the classifier's prediction.", "labels": [], "entities": []}, {"text": "The second baseline we compare against is a standard text classifier that uses the global word embeddings as its only features, i.e. F = (v 1 ||v 2 ).", "labels": [], "entities": [{"text": "F", "start_pos": 133, "end_pos": 134, "type": "METRIC", "confidence": 0.9501611590385437}]}, {"text": "The dataset on which we perform the evaluation is described in Section 4.2.", "labels": [], "entities": []}, {"text": "The dataset is not balanced: it contains more negative examples than positive ones.", "labels": [], "entities": []}, {"text": "Therefore, for evaluating the methods that involve a classifier we use stratified 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We remove relations from consideration if there is insufficient data in the corpora for that year (i.e., one of the entities was filtered out due to low incidence).", "labels": [], "entities": [{"text": "incidence", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9765139222145081}]}, {"text": "Recall that our relational corpora consists of 80K temporal relations in the following format: (entity 1 , entity 2 , year, type, class), where type is a relation type, and class is true if the relation holds on year.", "labels": [], "entities": []}, {"text": "For training and evaluating our classifiers we need negative examples as well as positive examples.", "labels": [], "entities": []}, {"text": "We generate negative examples in the following way: for every relation in the corpora, we randomly sample 10 negative examples.", "labels": [], "entities": []}, {"text": "We exclude the years of the true examples from the dataset's year range, and then randomly choose years for the negative examples.", "labels": [], "entities": []}, {"text": "To illustrate, let us observe the case of Obama, President: Obama was president from 2009-2016, so we sample negative examples from 1981 to 2008, such as (Obama, President, 1990, HoldsPoliticalPosition, false).", "labels": [], "entities": []}, {"text": "The resulting dataset contains 420K relations.", "labels": [], "entities": []}, {"text": "We refer to it as the Temporal Relations Dataset 4 . presents the results of our experiments.", "labels": [], "entities": [{"text": "Temporal Relations Dataset 4", "start_pos": 22, "end_pos": 50, "type": "DATASET", "confidence": 0.8402408063411713}]}, {"text": "To evaluate temporal query expansion we use our temporal relations dataset, which will be made publicly available (described in Section 3.4.2).", "labels": [], "entities": [{"text": "temporal query expansion", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7206292351086935}]}, {"text": "First, we evaluate on queries consisting of two entities (n = 2): for each relation, we create a distinct query that consists of its two entities concatenated.", "labels": [], "entities": []}, {"text": "We search The New York Times corpus with this query . We compare search performance when applying the various QE methods described in Section 5.1.", "labels": [], "entities": [{"text": "The New York Times corpus", "start_pos": 10, "end_pos": 35, "type": "DATASET", "confidence": 0.8107655048370361}]}, {"text": "To evaluate the methods that involve a classifier, we use stratified 10-fold cross validation, as the previous task was evaluated (Section 4.1).", "labels": [], "entities": []}, {"text": "We use K = 2 for all methods, i.e. we generate two expansion terms per query.", "labels": [], "entities": []}, {"text": "In addition, we evaluate on queries consisting of three entities (n = 3): we created anew dataset, which contains triplets of entities instead of pairs, by merging every two related (true) relations from our relations dataset.", "labels": [], "entities": []}, {"text": "Two relations are considered related if they share an entity, and their time periods overlap.", "labels": [], "entities": []}, {"text": "We then generate negative relations as described in Section 4.2.", "labels": [], "entities": []}, {"text": "In this new dataset, each temporal relation consists of three entities, a year and a binary classification.", "labels": [], "entities": []}, {"text": "Though a complete evaluation of QE is beyond the scope of this paper we describe here an evaluation suited for the temporal case.", "labels": [], "entities": [{"text": "QE", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.4488897919654846}]}, {"text": "It should be noted that the technique we propose here would likely be used alongside established QE techniques (e.g., log mining).", "labels": [], "entities": [{"text": "log mining", "start_pos": 118, "end_pos": 128, "type": "TASK", "confidence": 0.7191339284181595}]}, {"text": "First, when providing query expansions and suggestions we would like for them to not only retrieve relevant content, but temporally-relevant content.", "labels": [], "entities": []}, {"text": "To test the latter we say that given a temporal relation, a retrieved article is considered \"true\" if it were published within the referenced time, and \"false\" otherwise.", "labels": [], "entities": []}, {"text": "Additional manual validation was done to evaluate its relevance to the query.", "labels": [], "entities": []}, {"text": "This metric, while not the most accurate one, allows us to distinguish between results from the most relevant time period and others.", "labels": [], "entities": []}, {"text": "Precision of the top 10 retrieved documents (P@10) is used to evaluate the retrieval effectiveness.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.971206545829773}, {"text": "P@10)", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9504576027393341}]}, {"text": "The results of the QE evaluation are reported in.", "labels": [], "entities": [{"text": "QE", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.6352590918540955}]}, {"text": "We observe a consistent be-Method P@10 n = 2 n = 3 Baseline (  havior for different query sizes (n = 2, 3).", "labels": [], "entities": []}, {"text": "For our temporal classifiers, for n = 3 there is a 30% increase in precision, compared ton = 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9996066689491272}]}, {"text": "All of our methods performed significantly better compared to the baseline (statistical significance testing has been performed using paired ttest with p < 0.05).", "labels": [], "entities": []}, {"text": "This establishes our claim that utilizing temporal knowledge yields more temporal-promising results.", "labels": [], "entities": []}, {"text": "The Temporal Model Classifier showed the best performance of all.", "labels": [], "entities": []}, {"text": "This, too, suits our claim and fits to the results from the previous task (Section 4.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relations Dataset Composition", "labels": [], "entities": [{"text": "Relations Dataset", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9083971679210663}]}, {"text": " Table 2: Relatedness Learning Evaluation Results  (Accuracy, Recall, Precision, F1, AUC)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9986741542816162}, {"text": "Recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9392081499099731}, {"text": "Precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.907722532749176}, {"text": "F1", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9585863947868347}, {"text": "AUC", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.7979616522789001}]}, {"text": " Table 3: Results of QE Algorithms Evaluation", "labels": [], "entities": [{"text": "QE Algorithms", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7395323216915131}]}]}