{"title": [{"text": "No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering \"Simple\" Questions)", "labels": [], "entities": [{"text": "Answering \"Simple\" Questions)", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.8686662018299103}]}], "abstractContent": [{"text": "First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB).", "labels": [], "entities": [{"text": "factoid question answering", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6388622224330902}]}, {"text": "While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9989538192749023}]}, {"text": "Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB.", "labels": [], "entities": []}, {"text": "We train a recurrent neural network to solve each problem.", "labels": [], "entities": []}, {"text": "On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results-even neural networks based on much more complex architectures.", "labels": [], "entities": [{"text": "SimpleQuestions dataset", "start_pos": 7, "end_pos": 30, "type": "DATASET", "confidence": 0.8422643840312958}]}, {"text": "The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting.", "labels": [], "entities": []}, {"text": "In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users everyday.", "labels": [], "entities": []}], "introductionContent": [{"text": "First-order factoid question answering (QA) assumes that the question can be answered by a single fact in a knowledge base (KB).", "labels": [], "entities": [{"text": "First-order factoid question answering (QA)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7305864351136344}]}, {"text": "For example, \"How old is Tom Hanks\" is about the of.", "labels": [], "entities": []}, {"text": "Also referred to as simple questions by, recent attempts that apply either complex linguistic reasoning or attention-based complex neural network architectures achieve up to 76% accuracy on benchmark sets (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9986079335212708}]}, {"text": "While it is tempting to study QA systems that can handle more complicated questions, it is hard to reach reasonably high precision for unrestricted questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9968691468238831}]}, {"text": "For more than a decade, successful industry applications of QA have focused on first-order questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9626026153564453}]}, {"text": "This bears the question: are users even interested in asking questions beyond first-order (or are these use cases more suitable for interactive dialogue)?", "labels": [], "entities": []}, {"text": "Based on voice logs from a major entertainment platform with millions of users everyday, Comcast X1, we find that most existing use cases of QA fall into the first-order category.", "labels": [], "entities": []}, {"text": "Our strategy is to tailor our approach to firstorder QA by making strong assumptions about the problem structure.", "labels": [], "entities": [{"text": "firstorder QA", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.5363780856132507}]}, {"text": "In particular, we assume that the answer to a first-order question is a single property of a single entity in the KB, and decompose the task into two subproblems: (a) detecting entities in the question and (b) classifying the question as one of the relation types in the KB.", "labels": [], "entities": []}, {"text": "We simply train a vanilla recurrent neural network (RNN) to solve each subproblem).", "labels": [], "entities": []}, {"text": "Despite its simplicity, our approach (RNN-QA) achieves the highest reported accuracy on the SimpleQuestions dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9816723465919495}, {"text": "SimpleQuestions dataset", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.931732565164566}]}, {"text": "While recent literature has focused on building more complex neural network architectures with attention mechanisms, attempting to generalize to broader QA, we enforce stricter assumptions on the problem structure, thereby reducing complexity.", "labels": [], "entities": []}, {"text": "This also means that our solution is efficient, another critical requirement for real-time QA applications.", "labels": [], "entities": []}, {"text": "In fact, we present a performance analysis of RNN-QA on Comcast's X1 entertainment system, used by millions of customers everyday.", "labels": [], "entities": [{"text": "Comcast's X1 entertainment system", "start_pos": 56, "end_pos": 89, "type": "DATASET", "confidence": 0.720359718799591}]}], "datasetContent": [{"text": "Evaluation of RNN-QA was carried out on SimpleQuestions, which uses a subset of Freebase containing 17.8M million facts, 4M unique entities, and 7523 relation types.", "labels": [], "entities": []}, {"text": "Indexes I entity and I reach are built based on this knowledge base.", "labels": [], "entities": []}, {"text": "SimpleQuestions was built by) to serve as a larger and more diverse factoid QA dataset.", "labels": [], "entities": [{"text": "SimpleQuestions", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8836461901664734}, {"text": "QA dataset", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.6406264901161194}]}, {"text": "Freebase facts are sampled in away to ensure a diverse set of questions, then given to human annotators to create questions from, and get labeled with corresponding entity and relation type.", "labels": [], "entities": []}, {"text": "There area total of 1837 unique relation types that appear in SimpleQuestions.", "labels": [], "entities": []}, {"text": "We fixed the embedding layer based on the pre-trained 300-dimensional Google News embedding, 2 since the data size is too small for training embeddings.", "labels": [], "entities": [{"text": "Google News embedding", "start_pos": 70, "end_pos": 91, "type": "DATASET", "confidence": 0.847833514213562}]}, {"text": "Out-of-vocabulary words were assigned to a random vector (sampled from uniform distribution).", "labels": [], "entities": []}, {"text": "Parameters were learned via stochastic gradient descent, using categorical cross-entropy as objective.", "labels": [], "entities": []}, {"text": "In order to handle variable-length input, we limit the input to N tokens and prepend a special pad word if input has fewer.", "labels": [], "entities": []}, {"text": "We tried a variety of configurations for the RNN: four choices for the type of RNN layer (GRU or LSTM, bidirectional or not); depth from 1 to 3; and drop-out ratio from 0 to 0.5, yielding a total of 48 possible configurations.", "labels": [], "entities": []}, {"text": "For each possible setting, we trained the model on the training portion and used the validation portion to avoid over-fitting.", "labels": [], "entities": []}, {"text": "After running all 48 experiments, the most optimal setting was selected by micro-averaged F-score of predicted entities (entity detection) or accuracy (relation prediction) on the validation set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9492422938346863}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9990045428276062}]}, {"text": "We concluded that the optimal model is a 2-layer bidirectional LSTM (BiL-STM2) for entity detection and BiGRU2 for relation prediction.", "labels": [], "entities": [{"text": "entity detection", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.768824577331543}, {"text": "relation prediction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8450813889503479}]}, {"text": "Drop-out was 10% in both cases.", "labels": [], "entities": [{"text": "Drop-out", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9738785624504089}]}], "tableCaptions": [{"text": " Table 1: Top-1 accuracy on test portion of Simple- Questions. Ablation study on last three rows.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9410279393196106}, {"text": "Ablation", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9933115243911743}]}, {"text": " Table 2: Evaluation of RNN-QA on real questions  from X platform.", "labels": [], "entities": []}]}