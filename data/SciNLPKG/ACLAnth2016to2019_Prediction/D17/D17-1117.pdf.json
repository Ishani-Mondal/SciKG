{"title": [{"text": "Deeper Attention to Abusive User Content Moderation", "labels": [], "entities": [{"text": "Deeper Attention to Abusive User Content Moderation", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6335057871682304}]}], "abstractContent": [{"text": "Experimenting with anew dataset of 1.6M user comments from a news portal and an existing dataset of 115K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used logistic regression or an MLP classifier with character or word n-grams.", "labels": [], "entities": []}, {"text": "We also compare against a CNN operating on word embeddings, and a word-list baseline.", "labels": [], "entities": []}, {"text": "A novel, deep, classification-specific attention mechanism improves the performance of the RNN further, and can also highlight suspicious words for free, without including highlighted words in the training data.", "labels": [], "entities": []}, {"text": "We consider both fully automatic and semi-automatic moderation.", "labels": [], "entities": []}], "introductionContent": [{"text": "User comments play a central role in social media and online discussion fora.", "labels": [], "entities": []}, {"text": "News portals and blogs often also allow their readers to comment to get feedback, engage their readers, and build customer loyalty.", "labels": [], "entities": []}, {"text": "User comments, however, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) (.", "labels": [], "entities": []}, {"text": "Social media are under pressure to combat abusive content, but so far rely mostly on user reports and tools that detect frequent words and phrases of reported posts.", "labels": [], "entities": []}, {"text": "estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions.", "labels": [], "entities": []}, {"text": "News portals also suffer from abusive user comments, which damage their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions.", "labels": [], "entities": []}, {"text": "They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments.", "labels": [], "entities": []}, {"text": "Readers are disappointed when non-abusive comments do not appear quickly online because of moderation delays.", "labels": [], "entities": []}, {"text": "Smaller news portals maybe unable to employ moderators, and some are forced to shutdown their comments sections entirely.", "labels": [], "entities": []}, {"text": "We examine how deep learning ( can be employed to moderate user comments.", "labels": [], "entities": []}, {"text": "We experiment with anew dataset of approx. 1.6M manually moderated (accepted or rejected) user comments from a Greek sports news portal (called Gazzetta), which we make publicly available.", "labels": [], "entities": [{"text": "Greek sports news portal (called Gazzetta)", "start_pos": 111, "end_pos": 153, "type": "DATASET", "confidence": 0.7475321106612682}]}, {"text": "This is one of the largest publicly available datasets of moderated user comments.", "labels": [], "entities": []}, {"text": "We also provide word embeddings pre-trained on 5.2M comments from the same portal.", "labels": [], "entities": []}, {"text": "Furthermore, we experiment on the 'attacks' dataset of, approx. 115K English Wikipedia talk page comments labeled as containing personal attacks or not.", "labels": [], "entities": []}, {"text": "Ina fully automatic scenario, there is no moderator and a system accepts or rejects comments.", "labels": [], "entities": []}, {"text": "Although this scenario maybe the only available one, e.g., when news portals cannot afford moderators, it is unrealistic to expect that fully automatic moderation will be perfect, because abusive comments may involve irony, sarcasm, harassment without profane phrases etc., which are particularly difficult fora machine to detect.", "labels": [], "entities": []}, {"text": "When moderators are available, it is more realistic to develop semi- automatic systems aiming to assist, rather than replace the moderators, a scenario that has not been considered in previous work.", "labels": [], "entities": []}, {"text": "In this case, comments for which the system is uncertain) are shown to a moderator to decide; all other comments are accepted or rejected by the system.", "labels": [], "entities": []}, {"text": "We discuss how moderation systems can be tuned, depending on the availability and workload of the moderators.", "labels": [], "entities": []}, {"text": "We also introduce additional evaluation measures for the semi-automatic scenario.", "labels": [], "entities": []}, {"text": "On both datasets (Gazzetta and Wikipedia comments) and for both scenarios (automatic, semiautomatic), we show that a recurrent neural network (RNN) outperforms the system of, the previous state of the art for comment moderation, which employed logistic regression or a multi-layer Perceptron (MLP), and represented each comment as a bag of (character or word) n-grams.", "labels": [], "entities": []}, {"text": "We also propose an attention mechanism that improves the overall performance of the RNN.", "labels": [], "entities": []}, {"text": "Our attention mechanism differs from most previous ones () in that it is used in a classification setting, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models).", "labels": [], "entities": []}, {"text": "In that sense, our attention is similar to that of of, but our attention mechanism is a deeper MLP and it is only applied to words, whereas Yang et al. also have a second attention mechanism that assigns attention scores to entire sentences.", "labels": [], "entities": [{"text": "MLP", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9267054200172424}]}, {"text": "In effect, our attention detects the words of a comment that affect most the classification decision (accept, reject), by examining them in the context of the particular comment.", "labels": [], "entities": []}, {"text": "Although our attention mechanism does not always improve the performance of the RNN, it has the additional advantage of allowing the RNN to highlight suspicious words that a moderator could consider to decide more quickly if a comment should be accepted or rejected.", "labels": [], "entities": []}, {"text": "The highlighting Dataset/Split Accepted Rejected Total: Statistics of the datasets used.", "labels": [], "entities": [{"text": "highlighting Dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.882085919380188}]}, {"text": "comes for free, i.e., the training data do not contain highlighted words.", "labels": [], "entities": []}, {"text": "We also show that words highlighted by the attention mechanism correlate well with words that moderators would highlight.", "labels": [], "entities": []}, {"text": "Our main contributions are: (i) We release a dataset of 1.6M moderated user comments.", "labels": [], "entities": []}, {"text": "(ii) We introduce a novel, deep, classification-specific attention mechanism and we show that an RNN with our attention mechanism outperforms the previous state of the art in user comment moderation.", "labels": [], "entities": []}, {"text": "(iii) Unlike previous work, we also consider a semiautomatic scenario, along with threshold tuning and evaluation measures for it.", "labels": [], "entities": []}, {"text": "(iv) We show that the attention mechanism can automatically highlight suspicious words for free, without manually highlighting words in the training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first discuss the datasets we used, to help acquaint the reader with the problem.", "labels": [], "entities": []}, {"text": "Following, we report in Table 2 AUC scores (area under ROC curve), along with Spearman correlations between systemgenerated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when probabilistic gold labels are available.", "labels": [], "entities": [{"text": "AUC", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9435487389564514}, {"text": "ROC", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9346416592597961}]}, {"text": "always better than CNN and DETOX; there is no clear winner between CNN and DETOX.", "labels": [], "entities": [{"text": "CNN", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9536667466163635}, {"text": "DETOX", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8716521859169006}, {"text": "CNN", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9755531549453735}, {"text": "DETOX", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9182512164115906}]}, {"text": "Furthermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia comments, where RNN is overall slightly better according to.", "labels": [], "entities": []}, {"text": "Also, da-CENT is always worse than a-RNN and RNN, confirming that the hidden states (intuitively, context-aware word embeddings) of the RNN chain are important, even with the attention mechanism.", "labels": [], "entities": []}, {"text": "Increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAIN-L) significantly improves the performance of all methods.", "labels": [], "entities": [{"text": "Gazzetta training set", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.8826891978581747}]}, {"text": "The implementation of DETOX could not handle the size of G-TRAIN-L, which is why we do not report DETOX results for G-TRAIN-L.", "labels": [], "entities": []}, {"text": "Notice, also, that the Wikipedia dataset is easier than the Gazzetta one (all methods perform better on Wikipedia comments, compared to Gazzetta).", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.9627992510795593}]}, {"text": "shows F 2 (P reject , P accept ) on G-TEST-L and W-ATT-TEST, when ta , tr are tuned on G-DEV, W-ATT-DEV for varying coverage.", "labels": [], "entities": [{"text": "F 2", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9891051352024078}, {"text": "P accept )", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.7553369601567587}, {"text": "G-TEST-L", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9515697956085205}, {"text": "G-DEV", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.9230116605758667}]}, {"text": "For G-TEST-L, we show results training on G-TRAIN-S (solid lines) and G-TRAIN-L (dotted).", "labels": [], "entities": []}, {"text": "The differences between RNN and a-RNN are again small, but it is now easier to see that a-RNN is overall better.", "labels": [], "entities": []}, {"text": "Again, a-RNN and RNN are better than CNN and DETOX.", "labels": [], "entities": [{"text": "RNN", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.49632757902145386}, {"text": "CNN", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9534506797790527}, {"text": "DETOX", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.9371265769004822}]}, {"text": "All three deep learning methods benefit from the larger training set (dotted).", "labels": [], "entities": []}, {"text": "In Wikipedia, a-RNN obtains P accept , P reject \u2265 0.94 for all coverages.", "labels": [], "entities": []}, {"text": "On the more difficult Gazzetta dataset, a-RNN still obtains P accept , P reject \u2265 0.85 when tuned for 50% coverage.", "labels": [], "entities": [{"text": "Gazzetta dataset", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.9844860732555389}, {"text": "P accept", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9392141401767731}, {"text": "P reject", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8716182708740234}]}, {"text": "When tuned for 100% coverage, comments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoiding wrongly accepted comments, leading to high P accept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (0.59).", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9551961421966553}, {"text": "F 2", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9690688848495483}, {"text": "P accept (0.82)", "start_pos": 268, "end_pos": 283, "type": "METRIC", "confidence": 0.9554122924804688}, {"text": "P reject (0.59)", "start_pos": 348, "end_pos": 363, "type": "METRIC", "confidence": 0.9420177817344666}]}, {"text": "On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for coverage 50%, and 0.92, 0.48 for coverage 100%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9715782999992371}, {"text": "coverage", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9630429744720459}]}, {"text": "We also repeated the annotator ensemble experiment of on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users).", "labels": [], "entities": [{"text": "W-ATT-TEST", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.8957074284553528}]}, {"text": "The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment.", "labels": [], "entities": []}, {"text": "The gold labels were then compared to the decisions of the systems and the decisions of an ensemble of k other annotators, k ranging from 1 to 10.", "labels": [], "entities": []}, {"text": "shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets).", "labels": [], "entities": [{"text": "AUC", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8900009393692017}, {"text": "errrors", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.9944526553153992}]}, {"text": "We conclude that RNN and a-RNN are as good as an ensemble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is consistent with the results of.: Comparing to an ensemble of k humans.", "labels": [], "entities": [{"text": "CNN", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8361998200416565}, {"text": "DETOX", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.967190682888031}]}, {"text": "To investigate if the attention scores of a-RNN can highlight suspicious words, we focused on G-TEST-S-R, the only dataset with suspicious snippets annotated by humans.", "labels": [], "entities": [{"text": "G-TEST-S-R", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.8345168828964233}]}, {"text": "We removed comments with no human-annotated snippets, leaving 841 comments (515 accepted, 326 rejected), a total of 40,572 tokens, of which 13,146 were inside a suspicious snippet of at least one annotator.", "labels": [], "entities": []}, {"text": "In each remaining comment, each token was assigned a gold suspiciousness score, defined as the percentage of annotators that included it in their snippets.", "labels": [], "entities": [{"text": "gold suspiciousness score", "start_pos": 53, "end_pos": 78, "type": "METRIC", "confidence": 0.8721845547358195}]}, {"text": "We evaluated three methods that score each token wt of a comment c for suspiciousness.", "labels": [], "entities": []}, {"text": "The first one assigns to each wt the attention score at We used the protocol, code, and data of Wulczyn et al.", "labels": [], "entities": []}, {"text": "(Eq. 3) of a-RNN (trained on G-TRAIN-L).", "labels": [], "entities": []}, {"text": "The second method assigns to each wt its precision, as computed by LIST.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9990605711936951}, {"text": "LIST", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.7883510589599609}]}, {"text": "The third method (RAND) assigns to each wt a random (uniform distribution) score between 0 and 1.", "labels": [], "entities": []}, {"text": "In the latter two methods, a softmax is applied to the scores of all the tokens per comment, as in a-RNN.", "labels": [], "entities": []}, {"text": "shows three comments (from W-ATT-TEST) highlighted by a-RNN; heat corresponds to attention.", "labels": [], "entities": []}, {"text": "We computed Pearson and Spearman correlations between the gold suspiciousness scores and the scores of the three methods on the 40,572 tokens.", "labels": [], "entities": [{"text": "Pearson and Spearman correlations", "start_pos": 12, "end_pos": 45, "type": "METRIC", "confidence": 0.7452357560396194}]}, {"text": "shows the correlations on comments that were accepted (left) and rejected (right) by the majority of moderators.", "labels": [], "entities": []}, {"text": "In both cases, a-RNN performs better than LIST and RAND by both Pearson and Spearman correlations.", "labels": [], "entities": []}, {"text": "The high Pearson correlations of a-RNN also show that its attention scores are to a large extent linearly related to the gold ones.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 9, "end_pos": 29, "type": "METRIC", "confidence": 0.9770571887493134}]}, {"text": "By contrast, LIST performs reasonably well in terms of Spearman correlation, but much worse in terms of Pearson, indicating that its precision scores rank reasonably well the tokens from most to least suspicious ones, but are not linearly related to the gold scores.", "labels": [], "entities": [{"text": "LIST", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.7466320991516113}, {"text": "Spearman correlation", "start_pos": 55, "end_pos": 75, "type": "METRIC", "confidence": 0.56026990711689}, {"text": "Pearson", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9977966547012329}, {"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9986604452133179}]}, {"text": "experimented with 952K manually moderated comments from Yahoo Finance, but their dataset is not publicly available.", "labels": [], "entities": [{"text": "Yahoo Finance", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8580720722675323}]}, {"text": "They convert each comment to a comment embedding using DOC2VEC, which is then fed to an LR classifier.", "labels": [], "entities": []}, {"text": "experimented with approx. 3.3M manually moderated comments from Yahoo Finance and News; their data are also not available.", "labels": [], "entities": [{"text": "Yahoo Finance and News", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8978570252656937}]}, {"text": "They used Vowpal Wabbit with character n-grams (n = 3, . .", "labels": [], "entities": [{"text": "Vowpal Wabbit", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9054615497589111}]}, {"text": ", 5) and word n-grams (n = 1, 2), handcrafted features (e.g., number of capitalized or black-listed words), features based on dependency In innocent comments, a-RNN spreads its attention to all tokens, leading to quasi-uniform low color intensity.", "labels": [], "entities": []}, {"text": "According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not.", "labels": [], "entities": []}, {"text": "See http://hunch.net/ \u02dc vw/.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comment classification results. Scores reported by Wulczyn et al. (2017) are shown in brackets.", "labels": [], "entities": [{"text": "Comment classification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6920367777347565}]}, {"text": " Table 3: Comparing to an ensemble of k humans.", "labels": [], "entities": []}]}