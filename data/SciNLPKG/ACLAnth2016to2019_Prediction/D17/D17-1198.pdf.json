{"title": [{"text": "A Simple Language Model based on PMI Matrix Approximations", "labels": [], "entities": []}], "abstractContent": [{"text": "In this study, we introduce anew approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI attest time.", "labels": [], "entities": [{"text": "word-context pointwise mutual information (PMI)", "start_pos": 100, "end_pos": 147, "type": "METRIC", "confidence": 0.6213373669556209}]}, {"text": "Specifically, we show that with minor modifications to word2vec's algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models.", "labels": [], "entities": []}, {"text": "A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LMs) learn to estimate the probability of a word given a context of preceding words.", "labels": [], "entities": []}, {"text": "Recurrent Neural Network (RNN) language models recently outperformed traditional n-gram LMs across a range of tasks (.", "labels": [], "entities": [{"text": "Recurrent Neural Network (RNN) language", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6193093742643084}]}, {"text": "However, an important practical issue associated with such neural-network LMs is the high computational cost incurred.", "labels": [], "entities": []}, {"text": "The key factor that limits the scalability of traditional neural LMs is the computation of the normalization term in the softmax output layer, whose cost is linearly proportional to the size of the word vocabulary.", "labels": [], "entities": []}, {"text": "Several methods have been proposed to cope with this scaling issue by replacing the softmax with a more computationally efficient component at train time.", "labels": [], "entities": []}, {"text": "These include importance sam-pling), hierarchical softmax (), BlackOut ( and Noise Contrastive Estimation (NCE) (.", "labels": [], "entities": []}, {"text": "NCE has been applied to train neural LMs with large vocabularies) and more recently was also successfully used to train LSTM-RNN LMs (.", "labels": [], "entities": [{"text": "NCE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8349649310112}]}, {"text": "NCE-based language models achieved near stateof-the-art performance on language modeling tasks (, and as we later show, are closely related to the method presented in this paper.", "labels": [], "entities": []}, {"text": "Continuous word embeddings were initially introduced as a 'by-product' of learning neural language models).", "labels": [], "entities": []}, {"text": "However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learning models are no longer proper language models.", "labels": [], "entities": []}, {"text": "In particular, the skip-gram with negative sampling (NEG) embedding algorithm (  as implemented in the word2vec toolkit, has become one of the most popular such models today.", "labels": [], "entities": []}, {"text": "This is largely attributed to its scalability to huge volumes of data, which is critical for learning highquality embeddings.", "labels": [], "entities": []}, {"text": "Recently, offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a word-context pointwise mutual information (PMI) matrix.", "labels": [], "entities": [{"text": "NEG objective", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.705175131559372}]}, {"text": "rederived this result by offering an informationtheory interpretation of NEG.", "labels": [], "entities": []}, {"text": "The NEG objective function is considered a simplification of the NCE's objective, unsuitable for learning language models.", "labels": [], "entities": [{"text": "NCE", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9530643820762634}]}, {"text": "However, in this study, we show that despite its simplicity, it can be used in a principled way to effectively train a language model, based on PMI matrix factorization.", "labels": [], "entities": []}, {"text": "More specifically, we use NEG to train a model for estimating the PMI between words and their preceding contexts, and then derive conditional probabilities from PMI attest time.", "labels": [], "entities": []}, {"text": "The obtained PMI-LM can be viewed as a simple variant of word2vec's algorithm, where the context of a predicted word is the preceding sequence of words, rather than a single word within a context window (skip-gram), or a bag-of-context-words (CBOW).", "labels": [], "entities": []}, {"text": "Our analysis shows that the proposed PMI-LM is very closely related to NCE language models (NCE-LMs).", "labels": [], "entities": []}, {"text": "Similar to NCE-LMs, PMI-LM avoids the dependency of train run-time on the size of the word vocabulary by sampling from a negative (noise) distribution.", "labels": [], "entities": []}, {"text": "Furthermore, conveniently, it also has a notably more simplified objective function formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implementations of NCE language models (.", "labels": [], "entities": []}, {"text": "Finally, we report on a perplexity evaluation of PMI and NCE language models on two standard language modeling datasets.", "labels": [], "entities": []}, {"text": "The evaluation yielded comparable results, supporting our theoretical analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the evaluation described in this section is to empirically establish PMI-LM as a sound language model.", "labels": [], "entities": []}, {"text": "We do so by comparing its performance with the well-established NCE-LM, using the popular perplexity measure on two standard datasets, under the same terms.", "labels": [], "entities": [{"text": "NCE-LM", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9632953405380249}]}, {"text": "We describe our hyperparameter choices below and stress that fora fair comparison, we followed prior best practices and avoided hyperparameter optimization in favor of PMI-LM.", "labels": [], "entities": []}, {"text": "All of the models described hereafter were implemented using the Chainer toolkit ().", "labels": [], "entities": []}, {"text": "For our NCE baseline, we used the heuristics that worked well in (, initializing NCE's bias term from Eq. to b w = \u2212 log |V |, where V is the word vocabulary, and using Z c = 1.", "labels": [], "entities": [{"text": "NCE baseline", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9264586865901947}, {"text": "NCE", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8413057327270508}]}, {"text": "The first dataset we used is aversion of the Penn Tree Bank (PTB), commonly used to evaluate language models.", "labels": [], "entities": [{"text": "Penn Tree Bank (PTB)", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.974080761273702}]}, {"text": "It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary.", "labels": [], "entities": []}, {"text": "To build and train the compared models in this setting, we followed the work of, who achieved excellent results on this dataset.", "labels": [], "entities": []}, {"text": "Specifically, we used a 2-layer 300-hidden-units LSTM with a 50% dropout ratio to represent the preceding (left-side) context of a predicted word.", "labels": [], "entities": []}, {"text": "We represented endof-sentence as a special <eos> token and predicted this token like any other word.", "labels": [], "entities": []}, {"text": "During training, we performed truncated back-propagation-throughtime, unrolling the LSTM for 20 steps at a time without ever resetting the LSTM state.", "labels": [], "entities": []}, {"text": "We trained our model for 39 epochs using Stochastic Gradient Descent (SGD) with a learning rate of 1, which is decreased by a factor of 1.2 after every epoch starting after epoch 6.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9754720032215118}]}, {"text": "We clipped the norms of the gradient to 5 and used a mini-batch size of 20.", "labels": [], "entities": []}, {"text": "We set the negative sampling parameter to k = 100 following, who showed highly competitive performance with NCE LMs trained with this number of samples.", "labels": [], "entities": [{"text": "NCE LMs", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.8908862173557281}]}, {"text": "As the second dataset, we used the much larger WMT 1B-word benchmark introduced by.", "labels": [], "entities": [{"text": "WMT 1B-word benchmark", "start_pos": 47, "end_pos": 68, "type": "DATASET", "confidence": 0.7752692500750223}]}, {"text": "This dataset comprises about 0.8B training words and has a held-out set partitioned into 50 subsets.", "labels": [], "entities": []}, {"text": "The test set is the first subset in the heldout, comprising 159K words, including the <eos> tokens.", "labels": [], "entities": []}, {"text": "We used the second subset as the validation set with 165K words.", "labels": [], "entities": []}, {"text": "The original vocabulary size of this dataset is 0.8M words after converting all  words that occur less than 3 times in the corpus to an <unk> token.", "labels": [], "entities": []}, {"text": "However, we followed previous works and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "labels": [], "entities": []}, {"text": "To build and train our models, we used a similar method to the one used with PTB, with the following differences.", "labels": [], "entities": [{"text": "PTB", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.976437509059906}]}, {"text": "We used a single-layer 512-hidden-unit LSTM to represent the preceding context.", "labels": [], "entities": []}, {"text": "We followed, who found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus.", "labels": [], "entities": []}, {"text": "We trained our model for only one epoch using the Adam optimizer) with default parameters, which we found to converge more quickly and effectively than SGD.", "labels": [], "entities": []}, {"text": "We used a mini-batch size of 1000.", "labels": [], "entities": []}, {"text": "The perplexity results achieved by the compared models appear in.", "labels": [], "entities": []}, {"text": "As can be seen, the performance of our PMI-LM is competitive, slightly outperforming the NCE-LM on both test sets.", "labels": [], "entities": [{"text": "NCE-LM", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.9336165189743042}]}, {"text": "To put these numbers in a broader context, we note that state-of-the-art results on these datasets are notably better.", "labels": [], "entities": []}, {"text": "For example, on the small PTB test set, achieved 78.4 perplexity with a larger LSTM model and using the more costly softmax component.", "labels": [], "entities": [{"text": "PTB test set", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.9829575022061666}, {"text": "78.4 perplexity", "start_pos": 49, "end_pos": 64, "type": "METRIC", "confidence": 0.9222300052642822}]}, {"text": "On the larger WMT dataset, achieved 46.1 and 43.7 perplexity numbers using NCE and importance sampling respectively, and with much larger LSTM models trained over the full vocabulary, rather than our trimmed one.", "labels": [], "entities": [{"text": "WMT dataset", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.9323086738586426}, {"text": "NCE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7367064356803894}]}, {"text": "They also achieved 23.7 with an ensemble method, which is the best result on this dataset to date.", "labels": [], "entities": []}, {"text": "Yet, as intended, we argue that our experimental results affirm the claim that PMI-LM is a sound language model on par with NCE-LM.", "labels": [], "entities": [{"text": "NCE-LM", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9382763504981995}]}], "tableCaptions": [{"text": " Table 2: Perplexity results on test sets.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8966583013534546}]}]}