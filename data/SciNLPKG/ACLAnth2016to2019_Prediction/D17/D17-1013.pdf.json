{"title": [{"text": "Neural Machine Translation with Word Predictions", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7699944774309794}]}], "abstractContent": [{"text": "In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.These vectors are generated by parameters which are updated by back-propagation of translation errors through time.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.8700208564599355}]}, {"text": "We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors.", "labels": [], "entities": []}, {"text": "In this paper, we propose to use word predictions as a mechanism for direct supervision.", "labels": [], "entities": []}, {"text": "More specifically, we require these vectors to be able to predict the vocabulary in target sentence.", "labels": [], "entities": []}, {"text": "Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation.", "labels": [], "entities": []}, {"text": "It is also helpful in reducing the target side vocabulary and improving the decoding efficiency.", "labels": [], "entities": []}, {"text": "Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.", "labels": [], "entities": [{"text": "machine translation tasks", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7699826260407766}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9996685981750488}]}], "introductionContent": [{"text": "The encoder-decoder based neural machine translation (NMT) models) have been developing rapidly.", "labels": [], "entities": [{"text": "encoder-decoder based neural machine translation (NMT)", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.778981514275074}]}, {"text": "propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN)) or their variants (.", "labels": [], "entities": []}, {"text": "In this framework, the fixedlength vector plays the crucial role of transitioning the information of the sentence from the source side to the target side.", "labels": [], "entities": []}, {"text": "Later, attention mechanisms are proposed to enhance the source side representations ().", "labels": [], "entities": []}, {"text": "The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder.", "labels": [], "entities": []}, {"text": "However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (, or the average of the bi-directional representations ( ).", "labels": [], "entities": []}, {"text": "Here we refer to the representation as initial state.", "labels": [], "entities": []}, {"text": "Interestingly, find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector.", "labels": [], "entities": []}, {"text": "On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected.", "labels": [], "entities": [{"text": "translation", "start_pos": 78, "end_pos": 89, "type": "TASK", "confidence": 0.9900217652320862}]}, {"text": "We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures.", "labels": [], "entities": []}, {"text": "Due to the large number of parameters, it maybe difficult for the NMT system to learn the proper sentence representation as the initial state.", "labels": [], "entities": []}, {"text": "Thus, the model is very likely to get stuck in local minimums, making the translation process arbitrary and unstable.", "labels": [], "entities": []}, {"text": "In this paper, we propose to augment the current NMT architecture with a word prediction mechanism.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 73, "end_pos": 88, "type": "TASK", "confidence": 0.79754438996315}]}, {"text": "More specifically, we require the initial state of the decoder to be able to predict all the words in the target sentence.", "labels": [], "entities": []}, {"text": "In this way, there is a specific objective for learning the initial state.", "labels": [], "entities": []}, {"text": "Thus the learnt source side representation will be better constrained.", "labels": [], "entities": []}, {"text": "We further extend this idea by applying the word predictions mechanism to all the hidden states of the decoder.", "labels": [], "entities": [{"text": "word predictions", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7005461752414703}]}, {"text": "So the transition between different decoder states could be controlled as well.", "labels": [], "entities": []}, {"text": "Our mechanism is simple and requires no additional data or annotation.", "labels": [], "entities": []}, {"text": "The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding.", "labels": [], "entities": [{"text": "word predictions", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.725704699754715}]}, {"text": "Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems.", "labels": [], "entities": []}, {"text": "Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7174012064933777}]}], "datasetContent": [{"text": "To seethe effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU () on both CH-EN and DE-EN tasks.", "labels": [], "entities": [{"text": "IBM-BLEU", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.7463260293006897}]}, {"text": "The detailed results are show in the    BLEU, but the improvement is smaller than on the baseNMT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9963392019271851}]}, {"text": "On the DE-EN experiments, the phenomenon of experiments is similar to CH-EN experiments.", "labels": [], "entities": []}, {"text": "The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9993218183517456}]}, {"text": "The dropout technique also does notwork on WP ED and the ensemble technique improves 1.79 BLEU.", "labels": [], "entities": [{"text": "WP ED", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.7652118504047394}, {"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.984563410282135}]}, {"text": "These comparisons suggests that our system already learns better and stable values for the parameters, enjoying some of the benefits of general training techniques like dropout and ensemble.", "labels": [], "entities": []}, {"text": "Compared to dropout and ensemble, our method WP ED achieves the highest improvement against the baseline system on both CH-EN and DE-EN experiments.", "labels": [], "entities": [{"text": "WP ED", "start_pos": 45, "end_pos": 50, "type": "TASK", "confidence": 0.6557678580284119}]}, {"text": "Along with ensemble method, the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9994086027145386}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9972846508026123}]}, {"text": "Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7073177248239517}]}, {"text": "For each sentence in the test set, we use the initial state of the given model to make prediction about the possible words.", "labels": [], "entities": []}, {"text": "We denote the set of top n words as T n , the set of words in all the references top-n baseNMT WP E Prec.", "labels": [], "entities": [{"text": "baseNMT WP E Prec", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.5996355041861534}]}, {"text": "Recall 2% 67% 4% 89% top-5k 0.7% 84% 0.9% 95% top-10k 0.4% 90% 0.5% 97%: Comparison between baseNMT and WP E in precision and recall for the different prediction size on the CH-EN experiments.", "labels": [], "entities": [{"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9989885687828064}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9995349645614624}]}, {"text": "We define the precision, recall of the word prediction as follows: We compare the prediction performance of baseNMT and WP E . WP ED has similar prediction results with WP E , so we omit its results.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9993707537651062}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9990401864051819}, {"text": "word prediction", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.6679992526769638}]}, {"text": "As shown in, baseNMT system has a relatively lower prediction precision, for example, 45% in top 10 prediction.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.5106366276741028}]}, {"text": "With an explicit training, the WP E could achieve a much higher precision in all conditions.", "labels": [], "entities": [{"text": "WP E", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.6121832430362701}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9991051554679871}]}, {"text": "Specifically, the precision reaches 73% in top 10.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997043013572693}]}, {"text": "This indicates that the initial state in WP E contains more specific information about the prediction of the target words, which maybe a step towards better semantic representation, and leads to better translation quality.", "labels": [], "entities": []}, {"text": "Because the total words in the references are limited (around 50), the precision goes down, as expected, when a larger prediction set is considered.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9995225667953491}]}, {"text": "On the other hand, the recall of WP E is also much higher than baseNMT.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9998277425765991}, {"text": "WP E", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.6674360632896423}, {"text": "baseNMT", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9543220400810242}]}, {"text": "When given 1k predictions, WP E could successfully predict 89% of the words in the reference.", "labels": [], "entities": []}, {"text": "The recall goes up to 95% with 5k predictions, which is only 1/6 of the current vocabulary.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9991831183433533}]}, {"text": "To analyze the process of word prediction, we draw the attention heatmap (Equation 16) between the initial state s 0 and the bi-directional representation of each source side word hi for an example sentence.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7799474000930786}]}, {"text": "As shown in, both examples show that the initial states have a very strong attention with all the content words in the source sentence.", "labels": [], "entities": []}, {"text": "The blank cells are mostly functions words or high frequent tokens such as \"\u7684 ('s)\", \"\u662f (is)\", \"\u800c (and)\", \"\u5b83 (it)\", comma and period.", "labels": [], "entities": [{"text": "period", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9736323356628418}]}, {"text": "This indicates that the initial state successfully encodes information about most of the content words in the source sentence, which contributes fora high prediction performance and leads to better translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Case-insensitive 4-gram BLEU scores of baseNMT, WP E , WP D , WP ED systems on the CH-EN  experiments. (The \"IMP\" column presents the improvement of test average compared to the baseNMT. )", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.917433500289917}, {"text": "IMP", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9446771144866943}]}, {"text": " Table 2: Case-insensitive 4-gram BLEU scores of  baseNMT, WP E , WP D , WP ED systems on the DE- EN experiments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9051188230514526}, {"text": "DE- EN experiments", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.6626132428646088}]}, {"text": " Table 4: Case-insensitive 4-gram BLEU scores on  the DE-EN experiments for baseNMT and WP ED  systems, with the dropout and ensemble tech- niques.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9287806749343872}]}]}