{"title": [{"text": "Dynamic Data Selection for Neural Machine Translation", "labels": [], "entities": [{"text": "Dynamic Data Selection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6339092155297598}, {"text": "Neural Machine Translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7660099069277445}]}], "abstractContent": [{"text": "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT).", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 152, "end_pos": 191, "type": "TASK", "confidence": 0.7565698772668839}]}, {"text": "With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8315703670183817}]}, {"text": "While state-of-the-art data selection (Ax-elrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT.", "labels": [], "entities": []}, {"text": "Next, we introduce dynamic data selection for NMT, a method in which we vary the selected subset of training data between different training epochs.", "labels": [], "entities": []}, {"text": "Our experiments show that the best results are achieved when applying a technique we call gradual fine-tuning, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9978857636451721}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.9973090887069702}]}], "introductionContent": [{"text": "Recent years have shown a rapid shift from phrase-based (PBMT) to neural machine translation (NMT)) as the most common machine translation paradigm.", "labels": [], "entities": [{"text": "neural machine translation (NMT))", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.8337244987487793}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7604413628578186}]}, {"text": "With large quantities of parallel data, NMT outperforms PBMT for an increasing number of language pairs.", "labels": [], "entities": []}, {"text": "Unfortunately, training an NMT model is often a time-consuming task, with training times of several weeks not being unusual.", "labels": [], "entities": []}, {"text": "Despite its training inefficiency, most work in NMT greedily uses all available training data fora given language pair.", "labels": [], "entities": []}, {"text": "However, it is unlikely * Work done while at University of Amsterdam that all data is equally helpful to create the bestperforming system.", "labels": [], "entities": []}, {"text": "In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality.", "labels": [], "entities": []}, {"text": "Instead, fora given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it beneficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training.", "labels": [], "entities": [{"text": "translation task", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.8904062211513519}]}, {"text": "Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well.", "labels": [], "entities": [{"text": "data selection", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7255991697311401}]}, {"text": "While data selection has been applied to NMT to reduce the size of the data (), the effects on translation quality have not been investigated.", "labels": [], "entities": []}, {"text": "Intuitively, and confirmed by our exploratory experiments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (, and do not have a separate large-scale target-side language model to compensate for smaller parallel training data.", "labels": [], "entities": []}, {"text": "To alleviate the negative effect of small training data on NMT, we introduce dynamic data selection.", "labels": [], "entities": [{"text": "NMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.8046361804008484}]}, {"text": "Following conventional data selection, we still dramatically reduce the training data size, favoring parts of the data which are most relevant to the translation task at hand.", "labels": [], "entities": [{"text": "translation task", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.900791198015213}]}, {"text": "However, we exploit the fact that the NMT training process iterates over the training corpus in multiple epochs, and we alter the quantity or the composition of the training data between epochs.", "labels": [], "entities": []}, {"text": "The proposed method requires no modifications to the NMT architecture or parameters, and substantially speeds up training times while simultaneously improving translation quality with respect to a complete-bitext baseline.", "labels": [], "entities": []}, {"text": "In summary, our contributions are as follows: (i) We compare the effects of a commonly used data selection approach) on PBMT and NMT using four different test sets.", "labels": [], "entities": [{"text": "NMT", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.7625666260719299}]}, {"text": "We find that this method is much less effective for NMT than for PBMT, while using the exact same training data subsets.", "labels": [], "entities": [{"text": "NMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8352686166763306}]}, {"text": "(ii) We introduce dynamic data selection as away to make data selection profitable for NMT.", "labels": [], "entities": [{"text": "data selection", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7509471774101257}, {"text": "NMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.7083593606948853}]}, {"text": "We explore two techniques to alter the selected data subsets, and find that our method called gradual fine-tuning improves over conventional static data selection (up to +2.6 BLEU) and over a high-resource general baseline (up to +3.1 BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9954989552497864}, {"text": "BLEU", "start_pos": 235, "end_pos": 239, "type": "METRIC", "confidence": 0.9939364790916443}]}, {"text": "Moreover, gradual fine-tuning approximates indomain fine-tuning in \u223c20% of the training time, even when no parallel in-domain data is available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate static and dynamic data selection on a German\u2192English translation task comprising four test sets.", "labels": [], "entities": [{"text": "German\u2192English translation task", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6395747840404511}]}, {"text": "Below we describe the MT systems and data specifications.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.978010356426239}]}], "tableCaptions": [{"text": " Table 2: NMT BLEU comparison between using  n-gram LMs and LSTMs for bitext ranking. Selec- tion sizes concern the selected bitext subsets; LMs  are created from the exact same in-domain data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9515411853790283}]}, {"text": " Table 3: German\u2192English BLEU results of various gradual fine-tuning experiments sorted by relative  training time. Indicated improvements are with respect to static selection using 20% of the bitext, and  highest scores per test set are bold-faced. Results from the first experiment are also shown in", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9969603419303894}, {"text": "Indicated", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.977663516998291}]}, {"text": " Table 4: BLEU scores of data selection using rel- evance versus random ranking of the bitext. Grad- ual fine-tuning uses (\u03b1 = 0.5, \u03b2 = 0.7, \u03b7 = 2),  with relative training times of 18-20%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984532594680786}, {"text": "rel- evance", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.9322509169578552}, {"text": "Grad- ual fine-tuning", "start_pos": 95, "end_pos": 116, "type": "METRIC", "confidence": 0.909112811088562}]}]}