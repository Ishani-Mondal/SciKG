{"title": [{"text": "Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video", "labels": [], "entities": []}], "abstractContent": [{"text": "The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video.", "labels": [], "entities": []}, {"text": "In this work, we propose an extrac-tive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images , audios and videos related to a specific topic.", "labels": [], "entities": []}, {"text": "The key idea is to bridge the semantic gaps between multi-modal content.", "labels": [], "entities": []}, {"text": "For audio information, we design an approach to selectively use its transcription.", "labels": [], "entities": []}, {"text": "For visual information, we learn the joint representations of text and images using a neural network.", "labels": [], "entities": []}, {"text": "Finally, all of the multi-modal aspects are considered to generate the textual summary by maximizing the salience, non-redundancy, readability and coverage through the budgeted optimization of submodular functions.", "labels": [], "entities": [{"text": "coverage", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9334388971328735}]}, {"text": "We further introduce an MMS corpus in English and Chinese, which is released to the public 1.", "labels": [], "entities": [{"text": "MMS corpus", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8354670703411102}]}, {"text": "The experimental results obtained on this dataset demonstrate that our method out-performs other competitive baseline methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimedia data (including text, image, audio and video) have increased dramatically recently, which makes it difficult for users to obtain important information efficiently.", "labels": [], "entities": []}, {"text": "Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watching videos from beginning to end.", "labels": [], "entities": [{"text": "Multi-modal summarization (MMS)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8295062363147736}]}, {"text": "1 http://www.nlpr.ia.ac.cn/cip/jjzhang.htm The existing applications related to MMS include meeting record summarization (;), sport video summarization (, movie summarization (, pictorial storyline summarization (, timeline summarization () and social multimedia summarization.", "labels": [], "entities": [{"text": "MMS", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9607641100883484}, {"text": "meeting record summarization", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.7695152163505554}, {"text": "sport video summarization", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.5901154577732086}, {"text": "movie summarization", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.6932058781385422}, {"text": "pictorial storyline summarization", "start_pos": 178, "end_pos": 211, "type": "TASK", "confidence": 0.6171819865703583}, {"text": "timeline summarization", "start_pos": 215, "end_pos": 237, "type": "TASK", "confidence": 0.7412502467632294}, {"text": "social multimedia summarization", "start_pos": 245, "end_pos": 276, "type": "TASK", "confidence": 0.6771041949590048}]}, {"text": "When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions.", "labels": [], "entities": [{"text": "summarizing meeting recordings", "start_pos": 5, "end_pos": 35, "type": "TASK", "confidence": 0.876874307791392}]}, {"text": "For the summarization of pictorial storylines, the input is a set of images with text descriptions.", "labels": [], "entities": [{"text": "summarization of pictorial storylines", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.8314191401004791}]}, {"text": "None of these applications focus on summarizing multimedia data that contain asynchronous information about general topics.", "labels": [], "entities": [{"text": "summarizing multimedia", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.913507342338562}]}, {"text": "In this paper, as shown in, we propose an approach to a generate textual summary from a set of asynchronous documents, images, audios and videos on the same topic.", "labels": [], "entities": []}, {"text": "Since multimedia data are heterogeneous and contain more complex information than pure text does, MMS faces a great challenge in addressing the semantic gap between different modalities.", "labels": [], "entities": [{"text": "MMS", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9342807531356812}]}, {"text": "The framework of our method is shown in.", "labels": [], "entities": []}, {"text": "For the audio information contained in videos, we obtain speech transcriptions through Automatic Speech Recognition (ASR) and design a method to use these transcriptions selectively.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.7290351589520773}]}, {"text": "For visual information, including the key-frames extracted from videos and the images that appear in documents, we learn the joint representations of texts and images by using a neural network; we then can identify the text that is relevant to the image.", "labels": [], "entities": []}, {"text": "In this way, audio and visual information can be integrated into a textual summary.", "labels": [], "entities": []}, {"text": "Traditional document summarization involves two essential aspects: (1) Salience: the summa-  ry should retain significant content of the input documents.", "labels": [], "entities": [{"text": "Salience", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.9669199585914612}]}, {"text": "(2) Non-redundancy: the summary should contain as little redundant content as possible.", "labels": [], "entities": []}, {"text": "For MMS, we consider two additional aspects: (3) Readability: because speech transcriptions are occasionally ill-formed, we should try to get rid of the errors introduced by ASR.", "labels": [], "entities": [{"text": "MMS", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.970607578754425}]}, {"text": "For example, when a transcription provides similar information to a sentence in documents, we should prefer the sentence to the transcription presented in the summary.", "labels": [], "entities": []}, {"text": "(4) Coverage for the visual information: images that appear in documents and videos often capture event highlights that are usually very important.", "labels": [], "entities": []}, {"text": "Thus, the summary should cover as much of the important visual information as possible.", "labels": [], "entities": []}, {"text": "All of the aspects can be jointly optimized by the budgeted maximization of submodular functions (.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: \u2022 We design an MMS method that can automatically generate a textual summary from a set of asynchronous documents, images, audios and videos related to a specific topic.", "labels": [], "entities": []}, {"text": "\u2022 To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions.", "labels": [], "entities": []}, {"text": "\u2022 We introduce an MMS corpus in English and Chinese.", "labels": [], "entities": [{"text": "MMS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9289460778236389}]}, {"text": "The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "There is no benchmark dataset for MMS.", "labels": [], "entities": [{"text": "MMS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9588744640350342}]}, {"text": "We construct a dataset as follows.", "labels": [], "entities": []}, {"text": "We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese.", "labels": [], "entities": []}, {"text": "We set 5 topics for each language as a development set.", "labels": [], "entities": []}, {"text": "For each topic, we collect 20 documents within the same period using Google News search 6 and 5-10 videos in CCTV.com and Youtube . More details of the corpus are illustrated in.", "labels": [], "entities": [{"text": "Google News search 6", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.8908117115497589}, {"text": "CCTV.com", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.9420884251594543}, {"text": "Youtube", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9654656052589417}]}, {"text": "Some examples of news topics are provided.", "labels": [], "entities": []}, {"text": "We employ 10 graduate students to write reference summaries after reading documents and watching videos on the same topic.", "labels": [], "entities": []}, {"text": "We keep 3 reference summaries for each topic.", "labels": [], "entities": []}, {"text": "The criteria for summarizing documents lie in: (1) retaining important content of the input documents and videos; (2) avoiding redundant information; (3) having a good readability; (4) following the length limit.", "labels": [], "entities": [{"text": "summarizing documents", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.9187241196632385}]}, {"text": "We set the length constraint for each English and Chinese summary to 300 words and 500 characters, respectively.", "labels": [], "entities": [{"text": "length constraint", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.959878534078598}]}, {"text": "We use the ROUGE-1.5.5 toolkit () to evaluate the output summaries.", "labels": [], "entities": [{"text": "ROUGE-1.5.5", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.8508197665214539}]}, {"text": "This evaluation metric measures the summary quality by matching n-grams between generated summary and reference summary.", "labels": [], "entities": []}, {"text": "show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regarding to the three reference summaries for each topic in English and Chinese.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9902523756027222}, {"text": "ROUGE-2", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9833962321281433}, {"text": "ROUGE-SU4 (R-SU4) F-scores", "start_pos": 51, "end_pos": 77, "type": "METRIC", "confidence": 0.8020482182502746}]}, {"text": "For the results of the English MMS, from the first three lines in we can see that when summarizing without visual information, the method with guidance strategies performs slightly better than do the first two methods.", "labels": [], "entities": [{"text": "MMS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.5252688527107239}, {"text": "summarizing", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9852067828178406}]}, {"text": "Because Rouge mainly measures word overlaps, manual evaluation is needed to confirm the impact of guidance strategies on improving readability.", "labels": [], "entities": []}, {"text": "It is in-   troduced in Section 4.5.", "labels": [], "entities": []}, {"text": "The rating ranges from 1 (the poorest) to 5 (the best).", "labels": [], "entities": [{"text": "rating", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9368798732757568}]}, {"text": "When summarizing with textual and visual modalities, performances are not always improved, which indicates that the models of image caption, image caption match and image alignment are not suitable to MMS.", "labels": [], "entities": [{"text": "image caption", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.624147891998291}, {"text": "image alignment", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.70055091381073}, {"text": "MMS", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9626844525337219}]}, {"text": "The image match model has a significant advantage over other comparative methods, which illustrates that it can make use of multi-modal information.", "labels": [], "entities": []}, {"text": "shows the Chinese MMS results, which are similar to the English results that the image match model achieves the best performance.", "labels": [], "entities": [{"text": "MMS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.5032851099967957}]}, {"text": "We find that the performance enhancement for the image match model is smaller in Chinese than it is in English, which maybe due to the errors introduced by machine translation.", "labels": [], "entities": []}, {"text": "We provides a generated summary in English using the image match model, which is shown in.", "labels": [], "entities": []}, {"text": "The readability and informativeness for summaries are difficult to evaluate formally.", "labels": [], "entities": [{"text": "summaries", "start_pos": 40, "end_pos": 49, "type": "TASK", "confidence": 0.9805290102958679}]}, {"text": "We ask five graduate students to measure the quality of summaries generated by different methods.", "labels": [], "entities": [{"text": "summaries generated", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.906300812959671}]}, {"text": "We calculate the average score for all of the topics, and the results are displayed in.", "labels": [], "entities": []}, {"text": "Overall, our method with guidance strategies achieves higher scores than do the other methods, but it is still obviously poorer than the reference sum-Ramchandra Tewari , a passenger who suffered ahead injury , said he was asleep when he was suddenly flung to the floor of his coach . The impact of the derailment was so strong that one of the coaches landed on top of another , crushing the one below , said Brig.", "labels": [], "entities": [{"text": "ahead", "start_pos": 196, "end_pos": 201, "type": "METRIC", "confidence": 0.9883408546447754}]}, {"text": "The sentences covering the images are labeled by the corresponding colors.", "labels": [], "entities": []}, {"text": "The text can be partly related to the image because we use simplified sentence based on SRL to match the images.", "labels": [], "entities": []}, {"text": "We can find some mismatched sentences, such as the sentence \"Fourteen cars in the 23-car train derailed , Modak said .\" where our text-image matching model may misunderstand the \"car \" as a \"motor vehicle\" but not a \"coach\". maries.", "labels": [], "entities": []}, {"text": "Specifically, when speech transcriptions are not considered, the informativeness of the summary is the worst.", "labels": [], "entities": []}, {"text": "However, adding speech transcriptions without guidance strategies decreases readability to a large extent, which indicates that guidance strategies are necessary for MMS.", "labels": [], "entities": [{"text": "MMS", "start_pos": 166, "end_pos": 169, "type": "TASK", "confidence": 0.9891480803489685}]}, {"text": "The image match model achieves higher informativeness scores than do the other methods without using images.", "labels": [], "entities": []}, {"text": "We give two instances of readability guidance that arise between document text (DT) and speech transcriptions (ST) in.", "labels": [], "entities": []}, {"text": "The errors introduced by ASR include segmentation (instance A) and recognition (instance B) mistakes.", "labels": [], "entities": [{"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9797699451446533}, {"text": "segmentation", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9613518118858337}]}], "tableCaptions": [{"text": " Table 3: Experimental results (F-score) for En- glish MMS.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9961417317390442}, {"text": "En- glish MMS", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.44532061368227005}]}, {"text": " Table 4: Experimental results (F-score) for Chi- nese MMS.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9984845519065857}, {"text": "Chi- nese MMS", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.5233109891414642}]}, {"text": " Table 6. The errors intro- duced by ASR include segmentation (instance A)  and recognition (instance B) mistakes.", "labels": [], "entities": [{"text": "ASR", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9658110737800598}]}, {"text": " Table 5: Manual summary quality evaluation.  \"Read\" denotes \"Readability\" and \"Inform\" de- notes \"informativeness\".", "labels": [], "entities": []}, {"text": " Table 7: Experimental results (F-score) for En- glish MMS on five topics with manually labeled  text-image pairs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9955357313156128}]}, {"text": " Table 8: Experimental results (F-score) for Chi- nese MMS on five topics with manually labeled  text-image pairs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9976034760475159}, {"text": "Chi- nese MMS", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.5148591697216034}]}]}