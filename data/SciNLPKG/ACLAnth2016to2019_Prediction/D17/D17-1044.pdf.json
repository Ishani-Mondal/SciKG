{"title": [{"text": "Learning the Structure of Variable-Order CRFs: a Finite-State Perspective", "labels": [], "entities": []}], "abstractContent": [{"text": "The computational complexity of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies.", "labels": [], "entities": [{"text": "linear-chain Conditional Random Fields (CRFs)", "start_pos": 32, "end_pos": 77, "type": "TASK", "confidence": 0.6368574925831386}]}, {"text": "Such situations are not rare and arise when dealing with morphologically rich languages or joint labelling tasks.", "labels": [], "entities": []}, {"text": "We extend here recent proposals to consider variable order CRFs.", "labels": [], "entities": []}, {"text": "Using an effective finite-state representation of variable-length dependencies , we propose new ways to perform feature selection at large scale and report experimental results where we outper-form strong baselines on a tagging task.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7112363874912262}]}], "introductionContent": [{"text": "Conditional Random Fields (CRFs) () area method of choice for many sequence labelling tasks such as Part of Speech (PoS) tagging, Text Chunking, or Named Entity Recognition.", "labels": [], "entities": [{"text": "Part of Speech (PoS) tagging", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.6272201027188983}, {"text": "Text Chunking", "start_pos": 130, "end_pos": 143, "type": "TASK", "confidence": 0.7333767712116241}, {"text": "Named Entity Recognition", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.6645853519439697}]}, {"text": "Linearchain CRFs are easy to train by solving a convex optimization problem, can accomodate rich feature patterns, and enjoy polynomial exact inference procedures.", "labels": [], "entities": []}, {"text": "They also deliver state-of-the-art performance for many tasks, sometimes surpassing seq2seq neural models (.", "labels": [], "entities": []}, {"text": "A major issue with CRFs is the complexity of training and inference procedures, which are quadratic in the number of possible output labels for first order models and grow exponentially when higher order dependencies are considered.", "labels": [], "entities": []}, {"text": "This is problematic for tasks such as precise PoS tagging for Morphologically Rich Languages (MRLs), where the number of morphosyntactic labels is in the thousands.", "labels": [], "entities": [{"text": "PoS tagging for Morphologically Rich Languages (MRLs)", "start_pos": 46, "end_pos": 99, "type": "TASK", "confidence": 0.8295144041379293}]}, {"text": "Large label sets also naturally arise when joint labelling tasks (eg. simultaneous PoS tagging and text chunking) are considered, For such tasks, processing first-order models is demanding, and full size higher-order models are out of the question.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7653083503246307}, {"text": "text chunking)", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.7833798130353292}]}, {"text": "Attempts to overcome this difficulty are based on a greedy approach which starts with firstorder dependencies between labels and iteratively increases the scope of dependency patterns under the constraint that a high-order dependency is selected only if it extends an existing lower order feature.", "labels": [], "entities": []}, {"text": "As a result, feature selection may only choose only few higherorder features, motivating the need for an effective variable-order CRF (voCRF) training procedure (.", "labels": [], "entities": []}, {"text": "The latest implementation of this idea () relies on (structured) sparsity promoting regularization) and on finite-state techniques, handling high-order features at a small extra cost (see \u00a7 2).", "labels": [], "entities": []}, {"text": "In this approach, the sparse set of label dependency patterns is represented in a finite-state automaton, which arises as the result of the feature selection process.", "labels": [], "entities": []}, {"text": "In this paper, we somehow reverse the perspective and consider VoCRF training mostly as an automaton inference problem.", "labels": [], "entities": []}, {"text": "This leads us to consider alternative techniques for learning the finitestate machine representing the dependency structure of sparse VoCRFs (see \u00a7 3).", "labels": [], "entities": []}, {"text": "Two lines of enquiries are explored: (a) to take into account the internal structure of large tag sets in order to learn better and/or leaner feature sets; (b) to detect unconditional structural dependencies in label sequences in order to speed-up the discovery of useful features.", "labels": [], "entities": []}, {"text": "These ideas are implemented in 6 feature selection strategies, allowing us to explore a large set of dependency structures.", "labels": [], "entities": []}, {"text": "Relying on lazy finite-state operations, we train VoCRFs up to order 5, and achieve PoS tagging performance that surpass strong baselines for two MRLs (see \u00a7 4).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.7219029068946838}]}], "datasetContent": [{"text": "Experiments are run on two MRLs: for Czech, we use the CoNLL 2009 data set and for German, the Tiger Treebank with the split of).", "labels": [], "entities": [{"text": "CoNLL 2009 data set", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.9751612097024918}, {"text": "Tiger Treebank", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.9777688980102539}]}, {"text": "Both datasets include rich morphological attributes (cf..", "labels": [], "entities": []}, {"text": "All the patterns in Ware combined with lexical features testing the current word x t , its prefixes and suffixes of length 1 to 4, its capitalization and the presence of digit or punctuation symbols.", "labels": [], "entities": [{"text": "Ware", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9676479697227478}]}, {"text": "Additional contextual features also test words in a local window around position t.", "labels": [], "entities": []}, {"text": "These tests greatly increase the feature count and are not provided for all label patterns: for unigram patterns, we test the presence of all unigrams and bigrams of words in a window of 5 words; for bigrams patterns we only test for all unigrams in a window of 3 words.", "labels": [], "entities": []}, {"text": "Contextual features are not used for larger patterns.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental results. Each cell reports accuracy, number of states in A[W] and total training  time. Group lasso is our reimplementation of Vieira et al. (2016) (+Ctx = +context features) ; Greedy  1 is described in section 3.1, Component-wise is the decomposition approach of  \u00a7 3.2, PrunedLM and  MELM (+Gaps) were described in  \u00a7 3.3 and  \u00a7 3.4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.999497652053833}, {"text": "Group lasso", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.672312319278717}, {"text": "MELM", "start_pos": 309, "end_pos": 313, "type": "METRIC", "confidence": 0.9068639874458313}]}]}