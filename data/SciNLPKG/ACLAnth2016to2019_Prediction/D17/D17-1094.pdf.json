{"title": [{"text": "Recovering Question Answering Errors via Query Revision", "labels": [], "entities": [{"text": "Recovering Question Answering Errors via Query Revision", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7752635606697628}]}], "abstractContent": [{"text": "The existing factoid QA systems often lack a post-inspection component that can help models recover from their own mistakes.", "labels": [], "entities": []}, {"text": "In this work, we propose to cross-check the corresponding KB relations behind the predicted answers and identify potential inconsistencies.", "labels": [], "entities": []}, {"text": "Instead of developing anew model that accepts evidences collected from these relations, we choose to plug them back to the original questions directly and check if the revised question makes sense or not.", "labels": [], "entities": []}, {"text": "A bidirectional LSTM is applied to encode revised questions.", "labels": [], "entities": []}, {"text": "We develop a scoring mechanism over the revised question encodings to refine the predictions of abase QA system.", "labels": [], "entities": []}, {"text": "This approach can improve the F 1 score of STAGG (Yih et al., 2015), one of the leading QA systems, from 52.5% to 53.9% on WE-BQUESTIONS data.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9917417963345846}, {"text": "STAGG", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.42085742950439453}, {"text": "WE-BQUESTIONS data", "start_pos": 123, "end_pos": 141, "type": "DATASET", "confidence": 0.9016051888465881}]}], "introductionContent": [{"text": "With the recent advances in building large scale knowledge bases (KB) like Freebase (,, and YAGO () that contain the world's factual information, KB-based question answering receives attention of research efforts in this area.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9620345234870911}, {"text": "KB-based question answering", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.7364595135052999}]}, {"text": "Traditional semantic parsing is one of the most promising approaches that tackles this problem by mapping questions ontological forms using logical languages CCG (), DCS (, or directly query graphs) with predicates closely related to KB schema.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7375115156173706}]}, {"text": "Recently, neural network based models have been applied to question answering (Bordes: Sketch of our approach.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8614925146102905}]}, {"text": "Elements in solid round rectangles are KB relation labels.", "labels": [], "entities": []}, {"text": "Relation on the left is correct, but the base QA system predicts the one on the right.", "labels": [], "entities": [{"text": "Relation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9828379154205322}]}, {"text": "Dotted rectangles represent revised questions with relation labels plugged in.", "labels": [], "entities": []}, {"text": "The left revised question looks semantically closer to the original question and itself is more consistent.", "labels": [], "entities": []}, {"text": "Hence, it shall be ranked higher than the right one.", "labels": [], "entities": []}, {"text": "While these approaches yielded successful results, they often lack a post-inspection component that can help models recover from their own mistakes.", "labels": [], "entities": []}, {"text": "shows the potential improvement we can achieve if such a component exists.", "labels": [], "entities": []}, {"text": "Can we leverage textual evidences related to the predicted answers to recover from a prediction error?", "labels": [], "entities": []}, {"text": "In this work, we show it is possible.", "labels": [], "entities": []}, {"text": "Our strategy is to cross-check the corresponding KB relations behind the predicted answers and identify potential inconsistencies.", "labels": [], "entities": []}, {"text": "As an intermediate step, we define question revision as a tailored transformation of the original question using textual evidences collected from these relations in a knowledge base, and check if the revised questions make sense or not.", "labels": [], "entities": [{"text": "question revision", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.7349202930927277}]}, {"text": "our work from many existing QA studies.", "labels": [], "entities": []}, {"text": "Given a question, we first create its revisions with respect to candidate KB relations.", "labels": [], "entities": []}, {"text": "We encode question revisions using a bidirectional LSTM.", "labels": [], "entities": [{"text": "question revisions", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7328101992607117}]}, {"text": "A scoring mechanism over these encodings is jointly trained with LSTM parameters with the objective that the question revised by a correct KB relation has higher score than that of other candidate KB relations by a certain confidence margin.", "labels": [], "entities": []}, {"text": "We evaluate our method using STAGG ( as the base question answering system.", "labels": [], "entities": [{"text": "STAGG", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8285951018333435}, {"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.757509708404541}]}, {"text": "Our approach is able to improve the F 1 performance of STAGG () from 52.5% to 53.9% on a benchmark dataset WEBQUESTIONS).", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9548184871673584}, {"text": "STAGG", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.5446573495864868}, {"text": "benchmark dataset WEBQUESTIONS", "start_pos": 89, "end_pos": 119, "type": "DATASET", "confidence": 0.7135778466860453}]}, {"text": "Certainly, one can develop specialized LSTMs that directly accommodate text evidences without revising questions.", "labels": [], "entities": []}, {"text": "We have modified QA-LSTM and ATTENTIVE-LSTM () accordingly (See Section 4).", "labels": [], "entities": [{"text": "ATTENTIVE-LSTM", "start_pos": 29, "end_pos": 43, "type": "METRIC", "confidence": 0.9980068802833557}]}, {"text": "However, so far the performance is not as good as the question revision approach.", "labels": [], "entities": [{"text": "question revision", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8191173076629639}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: What if we know the questions on which the sys- tem makes mistakes? Best alternative is computed by re- placing the predictions of incorrectly answered questions  by STAGG with its second top-ranked candidate.", "labels": [], "entities": [{"text": "STAGG", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.8227646350860596}]}, {"text": " Table 2: Comparison of our question revision approach  (QUESREV) on STAGG with variety of recent KB-QA works.", "labels": [], "entities": [{"text": "question revision", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7385939806699753}]}, {"text": " Table 3: F1 performance of variants of our model QUESREV  and alternative solutions on base QA system STAGG.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.998244047164917}, {"text": "QUESREV", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8478135466575623}, {"text": "STAGG", "start_pos": 103, "end_pos": 108, "type": "METRIC", "confidence": 0.47689077258110046}]}]}