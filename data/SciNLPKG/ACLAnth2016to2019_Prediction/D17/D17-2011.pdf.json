{"title": [{"text": "QUINT: Interpretable Question Answering over Knowledge Bases", "labels": [], "entities": [{"text": "Interpretable Question Answering", "start_pos": 7, "end_pos": 39, "type": "TASK", "confidence": 0.6396101216475168}]}], "abstractContent": [{"text": "We present QUINT, a live system for question answering over knowledge bases.", "labels": [], "entities": [{"text": "QUINT", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.7866693139076233}, {"text": "question answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7980603575706482}]}, {"text": "QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers.", "labels": [], "entities": []}, {"text": "When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer.", "labels": [], "entities": []}, {"text": "The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query.", "labels": [], "entities": []}, {"text": "When an answer seems unsatisfactory, the derivation provides valuable insights towards refor-mulating the question.", "labels": [], "entities": []}], "introductionContent": [{"text": "A KB-QA system takes a natural language utterance as input and produces one or more crisp answers as output (.", "labels": [], "entities": []}, {"text": "This is usually done through semantic parsing: translating the utterance to a formal query in a language such as SPARQL, and executing this query over a KB like) or YAGO ( to return one or more answer entities.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.7673661708831787}, {"text": "YAGO", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.884458601474762}]}, {"text": "In addition to answering questions, a KB-QA system should ideally be able to explain how an answer was derived i.e., how the system understood the users' questions.", "labels": [], "entities": []}, {"text": "While rapid progress is being made on the KB-QA task, the quality of answers obtained from KB-QA systems are far from perfect.", "labels": [], "entities": []}, {"text": "This is due to a combination of factors related to the ambiguity of natural language, the underlying data (e.g., KB incompleteness, gaps in lexicon coverage) and the KB-QA systems themselves (e.g., errors in named entity recognition and disambiguation, query ranking).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 208, "end_pos": 232, "type": "TASK", "confidence": 0.6229576567808787}]}, {"text": "Explanations help address this gap in two ways: (i) helping users gain confidence when correct answers are returned, and (ii) making sense of the limitations of the system by looking at explanations for wrong answers, possibly providing cues to workaround them.", "labels": [], "entities": []}, {"text": "For an expert user, explanations also contribute to traceability: identifying the exact point of failure in the KB-QA system pipeline, which can be used for subsequent debugging.", "labels": [], "entities": []}, {"text": "In this work, we demonstrate QUINT (, a state-of-the-art KB-QA system that gives step-by-step explanations of how it derives answers for questions.", "labels": [], "entities": [{"text": "QUINT", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8778220415115356}]}, {"text": "Furthermore, when QUINT is unable to link a specific phrase in the question to a KB item, it asks the user to reformulate the phrase.", "labels": [], "entities": []}, {"text": "Such reformulations can be used to improve various components in the KB-QA pipeline such as underlying lexicons.", "labels": [], "entities": []}, {"text": "QUINT takes the first step towards enabling interactive QA in the future, where the system can ask the user about parts of the question that it is unsure about.", "labels": [], "entities": []}, {"text": "Take the question \"Where was Martin Luther raised?\": QUINT returns Eisleben in Germany as the top answer.", "labels": [], "entities": [{"text": "QUINT", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9963235855102539}]}, {"text": "A quick look by the user at the derivation reveals that (i) 'Martin Luther' was mapped to the KB entity MartinLuther, the theologist, and (ii) 'raised' was interpreted as the KB predicate placeOfBirth.", "labels": [], "entities": [{"text": "KB entity MartinLuther", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.6211291352907816}]}, {"text": "For (i), if the user had intended the US activist MartinLutherKing instead, a simple reformulation with \"martin luther king\" in the input returns Atlanta, the US city where Luther King was born.", "labels": [], "entities": [{"text": "MartinLutherKing", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.5106261372566223}]}, {"text": "On the other hand, for (ii), if the birthplace was not the specific intent, a quick rephrasing of the question to \"Where did Martin Luther live?\" results in Saxony-Anhalt, which is derived from the predicate placesLived.", "labels": [], "entities": []}, {"text": "Motivated by the need for interpretable question answering, QUINT's approach to KB-QA relies on role-aligned templates, where each template consists of an utterance template based on a dependency parse pattern and a corresponding query template based on the SPARQL query language.", "labels": [], "entities": [{"text": "interpretable question answering", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.6246915459632874}]}, {"text": "The template (i) specifies how to chunk an utterance into phrases, (ii) guides how these phrases map to KB primitives by specifying their semantic roles as predicates, entities, or types, and (iii) aligns syntactic structure in the utterance to the semantic predicate-argument structure of the query.", "labels": [], "entities": []}, {"text": "Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions).", "labels": [], "entities": []}, {"text": "The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations.", "labels": [], "entities": []}, {"text": "In contrast, QUINT automatically learns templates from question-answer pairs.", "labels": [], "entities": []}, {"text": "Embedding-based methods () map questions, KB entities, and subgraphs to a shared space for KB-QA without explicitly generating a semantic representation.", "labels": [], "entities": []}, {"text": "This makes it difficult for such systems to generate finegrained explanations to users.", "labels": [], "entities": []}, {"text": "Other approaches to KB-QA () over-generate query candidates fora given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging.", "labels": [], "entities": []}, {"text": "The key contribution of this demo paper is a live online KB-QA system that visualizes the derivation steps for generating an answer, and thus takes the first steps towards explainable question-answering.", "labels": [], "entities": []}, {"text": "The demo is available at the following URL: https://gate.", "labels": [], "entities": []}, {"text": "d5.mpi-inf.mpg.de/quint/quint.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}