{"title": [{"text": "Towards Compact and Fast Neural Machine Translation Using a Combined Method", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.6727571288744608}]}], "abstractContent": [{"text": "Neural Machine Translation (NMT) lays intensive burden on computation and memory cost.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7828578948974609}]}, {"text": "It is a challenge to deploy NMT models on the devices with limited computation and memory budgets.", "labels": [], "entities": []}, {"text": "This paper presents a four stage pipeline to compress model and speedup the decoding for NMT.", "labels": [], "entities": []}, {"text": "Our method first introduces a compact architecture based on convo-lutional encoder and weight shared em-beddings.", "labels": [], "entities": []}, {"text": "Then weight pruning is applied to obtain a sparse model.", "labels": [], "entities": []}, {"text": "Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search.", "labels": [], "entities": []}, {"text": "Hence, the time-consuming beam search can be replaced by simple greedy decoding.", "labels": [], "entities": []}, {"text": "Finally, vocabulary selection is used to reduce the computation of softmax layer.", "labels": [], "entities": [{"text": "vocabulary selection", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7826145589351654}]}, {"text": "Our final model achieves 10\u00d7 speedup, 17\u00d7 parameters reduction, <35MB storage size and comparable performance compared to the baseline model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has recently gained popularity in solving the machine translation problem.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8062491516272227}, {"text": "machine translation problem", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.852263887723287}]}, {"text": "Although NMT has achieved state-of-the-art performance for several language pairs, like many other deep learning domains, it is both computationally intensive and memory intensive.", "labels": [], "entities": []}, {"text": "This leads to a challenge of deploying NMT models on the devices with limited computation and memory budgets.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on English-German (newstest2014) and Chinese-English (nist05) test sets. EWS: embeddings weights shar- ing. VS: vocabulary selection. FSI: fast sequence interpolation. k: beam size. Tdec: decoding time on the test set in seconds.  Pruned models are saved as compressed sparse row (CSR) format with low bit index. Decoding runs on CPU in a preliminary  implementation with TensorFlow, sparse matrix multiplication is unused for pruned models. After applying FSI, beam search  with a beam size 10 is replaced by greedy decoding when recording Tdec.", "labels": [], "entities": [{"text": "EWS", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.5102499127388}, {"text": "VS", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.9101759195327759}, {"text": "vocabulary selection", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.6567659676074982}, {"text": "FSI", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.7661373615264893}]}, {"text": " Table 2: BLEU on test sets with varying pruning rate.  Model config: Conv Encoder+EWS.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998439610004425}, {"text": "EWS", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8109657764434814}]}, {"text": " Table 3: BLEU on development sets with varying class  number. Model config: Conv Encoder.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9961878657341003}]}]}