{"title": [{"text": "Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification", "labels": [], "entities": [{"text": "Implicit Discourse Relationship Representation and Identification", "start_pos": 47, "end_pos": 112, "type": "TASK", "confidence": 0.7544306715329488}]}], "abstractContent": [{"text": "We present a novel multi-task attention-based neural network model to address implicit discourse relationship representation and identification through two types of representation learning, an attention-based neural network for learning discourse relationship representation with two arguments and a multi-task framework for learning knowledge from annotated and unannotated corpora.", "labels": [], "entities": [{"text": "discourse relationship representation and identification", "start_pos": 87, "end_pos": 143, "type": "TASK", "confidence": 0.7812573075294494}, {"text": "learning discourse relationship representation", "start_pos": 228, "end_pos": 274, "type": "TASK", "confidence": 0.6068782061338425}]}, {"text": "The extensive experiments have been performed on two benchmark corpora (i.e., PDTB and CoNLL-2016 datasets).", "labels": [], "entities": [{"text": "PDTB", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9458494186401367}, {"text": "CoNLL-2016 datasets", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.9521935284137726}]}, {"text": "Experimental results show that our proposed model out-performs the state-of-the-art systems on benchmark corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of implicit discourse relation (or rhetorical relation) identification is to recognize how two adjacent text spans without explicit discourse marker (i.e., connective, e.g., because or but ) between them are logically connected to one another (e.g., cause or contrast).", "labels": [], "entities": [{"text": "implicit discourse relation (or rhetorical relation) identification", "start_pos": 12, "end_pos": 79, "type": "TASK", "confidence": 0.7011512551042769}]}, {"text": "It is considered to be a crucial step for discourse analysis and language generation and helpful to many downstream NLP applications, e.g., QA, MT, sentiment analysis, machine comprehension, etc.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7956321239471436}, {"text": "language generation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7150783389806747}, {"text": "MT", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.7300035357475281}, {"text": "sentiment analysis", "start_pos": 148, "end_pos": 166, "type": "TASK", "confidence": 0.9266858100891113}]}, {"text": "With the release of PDTB 2.0 (, lots of work has been done for discourse relation identification on natural (i.e., genuine) discourse data;) with the use of traditional NLP linguistically informed features and machine learning algorithms.", "labels": [], "entities": [{"text": "discourse relation identification", "start_pos": 63, "end_pos": 96, "type": "TASK", "confidence": 0.7422765096028646}]}, {"text": "Recently, more and more researchers resorted to neural networks for implicit discourse recognition;.", "labels": [], "entities": [{"text": "discourse recognition", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7120915651321411}]}, {"text": "Meanwhile, to alleviate the shortage of labeled data, researchers explored multi-task learning with the aid of unannotated data for implicit discourse recognition either in traditional machine learning framework or recently in neural network framework (.", "labels": [], "entities": [{"text": "discourse recognition", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.707659587264061}]}, {"text": "In this work, we present a novel multi-task attention-based neural network to address implicit discourse relationship representation and recognition.", "labels": [], "entities": [{"text": "discourse relationship representation and recognition", "start_pos": 95, "end_pos": 148, "type": "TASK", "confidence": 0.6911579191684722}]}, {"text": "It performs two types of representation learning at the same time.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.9212528765201569}]}, {"text": "An attention-based neural network conducts discourse relationship representation learning through interaction between two discourse arguments.", "labels": [], "entities": [{"text": "discourse relationship representation learning", "start_pos": 43, "end_pos": 89, "type": "TASK", "confidence": 0.7184677869081497}]}, {"text": "Meanwhile, a multi-task learning framework leverages knowledge from auxiliary task to enhance the performance of main task.", "labels": [], "entities": []}, {"text": "Furthermore, these two types of learning are integrated into one neural network framework and work together to maximize the overall performance.", "labels": [], "entities": []}, {"text": "The contributions of this work are listed as follows.", "labels": [], "entities": []}, {"text": "\u2022 We propose a multi-task attention-based neural network model to address implicit discourse relationship representation and recognition, which benefits from both the interaction between discourse arguments and the interaction between different learning tasks; \u2022 Our method achieves the best results on two benchmark corpora in comparison with the state-of-the-art systems so far.", "labels": [], "entities": [{"text": "discourse relationship representation and recognition", "start_pos": 83, "end_pos": 136, "type": "TASK", "confidence": 0.7168751716613769}]}, {"text": "The organization of this work is as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the proposed novel multi-task neural network.", "labels": [], "entities": []}, {"text": "Section 3 introduces the exper-imental settings in detail.", "labels": [], "entities": []}, {"text": "Section 4 reports the comprehensive experimental results on two benchmark corpora.", "labels": [], "entities": []}, {"text": "Section 5 summarized related work.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We adopted three corpora: PDTB 2.0 and CoNLL-2016 datasets are annotated for discourse relation recognition evaluation, and the BLLIP corpus is unlabeled and used as auxiliary task.", "labels": [], "entities": [{"text": "CoNLL-2016 datasets", "start_pos": 39, "end_pos": 58, "type": "DATASET", "confidence": 0.9678109884262085}, {"text": "discourse relation recognition evaluation", "start_pos": 77, "end_pos": 118, "type": "TASK", "confidence": 0.8065764904022217}, {"text": "BLLIP corpus", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.8565275073051453}]}, {"text": "PDTB 2.0 is the largest annotated corpus of discourse relations, which contains 2, 312 Wall Street Journal (WSJ) articles.", "labels": [], "entities": [{"text": "PDTB 2.0", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8808243274688721}, {"text": "Wall Street Journal (WSJ) articles", "start_pos": 87, "end_pos": 121, "type": "DATASET", "confidence": 0.894848244530814}]}, {"text": "The sense label of discourse relations is hierarchically with three levels, i.e., class, type and sub-type.", "labels": [], "entities": []}, {"text": "The top level contains four major semantic classes: Comparison (denoted as Comp.), Contingency (Cont.), Expansion (Exp.) and Temporal (Temp.).", "labels": [], "entities": []}, {"text": "For each class, a set of types is used to refine relation sense.", "labels": [], "entities": []}, {"text": "The set of subtypes is to further specify the semantic contribution of each argument.", "labels": [], "entities": []}, {"text": "We focus on the top level (class) relations.", "labels": [], "entities": []}, {"text": "Following, we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set.: The statistics of four top level implicit discourse relations in PDTB 2.0.", "labels": [], "entities": []}, {"text": "The CoNLL-2016 Shared Task focuses on shallow discourse parsing, which provides two test datasets, i.e., one from PDTB section 23 denoted as CoNLL-Test set, and the other from a similar source and domain (English Wikinews 2 ) denoted as CoNLL-Blind test set.", "labels": [], "entities": [{"text": "shallow discourse parsing", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6630694766839346}, {"text": "PDTB section 23", "start_pos": 114, "end_pos": 129, "type": "DATASET", "confidence": 0.9457295934359232}, {"text": "CoNLL-Blind test set", "start_pos": 237, "end_pos": 257, "type": "DATASET", "confidence": 0.8872768878936768}]}, {"text": "Different from the sense labels in PDTB, the CoNLL-Test set has three sense levels and the EntRel label.", "labels": [], "entities": [{"text": "CoNLL-Test set", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.8813500702381134}]}, {"text": "Moreover, it merges several labels in the original annotation to reduce some sparsity without losing too much of the utility and the semantics of the sense.", "labels": [], "entities": []}, {"text": "BLLIP The North American News Text (Complete) is used as unlabeled data source to generate synthetic labeled data for auxiliary task.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9070351123809814}, {"text": "North American News Text (Complete)", "start_pos": 10, "end_pos": 45, "type": "DATASET", "confidence": 0.8114469732557025}]}, {"text": "We remove the explicit discourse connectives from raw texts and grant the explicit relations as the synthetic implicit relations.", "labels": [], "entities": []}, {"text": "We obtain a resulting corpus with 100, 000 implicit relations by random sampling.", "labels": [], "entities": []}, {"text": "We adopt precision (P), recall (R) and their harmonic mean, i.e., F 1 for performance evaluation.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.9526601135730743}, {"text": "recall (R)", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9566182941198349}, {"text": "F 1", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9893901646137238}]}, {"text": "We also report accuracy for direct comparison with previous works.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996106028556824}]}], "tableCaptions": [{"text": " Table 1: The statistics of four top level implicit  discourse relations in PDTB 2.0.", "labels": [], "entities": []}, {"text": " Table 2: Performance of multiple binary classification on the top level classes in PDTB corpus in terms  of F 1 (%).", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.978731781244278}, {"text": "F 1", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9943922162055969}]}, {"text": " Table 3: Performance of multi-class classification on PDTB and CoNLL-2016 in terms of accuracy (Acc)  (%) and macro-averaged F 1 (%).", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.691916435956955}, {"text": "PDTB and CoNLL-2016", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8529064456621805}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9996476173400879}, {"text": "Acc", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9632795453071594}, {"text": "F 1", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.8863519728183746}]}, {"text": " Table 4: Comparison with the state-of-the-art systems reported on PDTB and CoNLL-2016, where - means N.A.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9758173227310181}, {"text": "CoNLL-2016", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.6016424298286438}]}]}