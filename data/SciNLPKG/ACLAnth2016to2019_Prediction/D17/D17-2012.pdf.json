{"title": [{"text": "Function Assistant: A Tool for NL Querying of APIs", "labels": [], "entities": [{"text": "NL Querying of APIs", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8717295974493027}]}], "abstractContent": [{"text": "In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language.", "labels": [], "entities": []}, {"text": "The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions.", "labels": [], "entities": []}, {"text": "For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richard-son and Kuhn (2017).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 166, "end_pos": 182, "type": "TASK", "confidence": 0.7594588100910187}]}, {"text": "Translations are automatically learned from example text-code pairs in example APIs.", "labels": [], "entities": [{"text": "Translations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9552870988845825}]}, {"text": "The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects.", "labels": [], "entities": []}, {"text": "To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.", "labels": [], "entities": []}], "introductionContent": [{"text": "Software developers frequently shift between using different third-party software libraries, or APIs, when developing new applications.", "labels": [], "entities": []}, {"text": "Much of the development time is dedicated to understanding the structure of these APIs, figuring out where the target functionality lies, and learning about the peculiarities of how such software is structured or how naming conventions work.", "labels": [], "entities": []}, {"text": "When the target API is large, finding the desired functionality can be a formidable and time consuming task.", "labels": [], "entities": []}, {"text": "Often developers resort to resources like Google or StackOverflow to find (usually indirect) answers to questions.", "labels": [], "entities": []}, {"text": "We illustrate these issues in using two example functions from the well-known NLTK toolkit.", "labels": [], "entities": []}, {"text": "Each function is paired with a short docstring, i.e., the quoted description under each function, which provides a user of the software a description of what the function does.", "labels": [], "entities": []}, {"text": "While understanding the documentation and code requires technical knowledge of dependency parsing and graphs, even with such knowledge, the function naming conventions are rather arbitrary.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7673456966876984}]}, {"text": "The function add arc could just as well be called create arc.", "labels": [], "entities": []}, {"text": "An end-user expecting another naming convention might be left astray when searching for this functionality.", "labels": [], "entities": []}, {"text": "Similarly, the available description might deviate from how an end-user would describe such functionality.", "labels": [], "entities": []}, {"text": "Understanding the remove by address function, in contrast, requires knowing the details of the particular DependencyGraph implementation being used.", "labels": [], "entities": []}, {"text": "Nonetheless, the function corresponds to the standard operation of removing anode from a dependency graph.", "labels": [], "entities": []}, {"text": "Here, the technical details about how this removal is specific to a given address might obfuscate the overall purpose of the function, making it hard to find or understand.", "labels": [], "entities": []}, {"text": "At a first approximation, navigating a given API requires knowing correspondences between textual descriptions and source code representations.", "labels": [], "entities": []}, {"text": "For example, knowing that the English expression Adds an arc in translates (somewhat arbitrarily) to add arc, or that given address translates to address.", "labels": [], "entities": []}, {"text": "One must also know how to detect paraphrases of certain target entities or actions, for example that adding an arc means the same as creating an arc in this context.", "labels": [], "entities": []}, {"text": "Other technical correspondences, such as the relation between an address and the target dependency graph implementation, must be learned.", "labels": [], "entities": []}, {"text": "In our previous work (, henceforth RK), we look at learning these types of correspondences from example API collections in a variety of programming languages and source natural languages.", "labels": [], "entities": []}, {"text": "We treat each given API, consisting of text and function representation pairs, as a parallel corpus for training a simple semantic parsing model.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7564655840396881}]}, {"text": "In addition to learning translational correspondences, of the type described above, we achieve improvements by adding document-level features that help to learn other technical correspondences.", "labels": [], "entities": [{"text": "translational correspondences", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.8404434025287628}]}, {"text": "In this paper, we focus on using our models as a tool for querying API collections.", "labels": [], "entities": []}, {"text": "Given a target API, our model learns an MT-based semantic parser that translates text to code representations in the API.", "labels": [], "entities": [{"text": "MT-based semantic parser", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8930107553799947}]}, {"text": "End-users can formulate natural language queries to the background API, which our model will translate into candidate function representations with the goal of finding the desired functionality.", "labels": [], "entities": []}, {"text": "Our tool, called Function Assistant can be used in two ways: as a blackbox pipeline for building models directly from arbitrary API collections.", "labels": [], "entities": []}, {"text": "As well, it can be customized and integrated with other outside components or models using the tool's flexible internal Python API.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the first usage of our tool.", "labels": [], "entities": []}, {"text": "To explore building models for new API collections, we run our pipeline on 27 open source Python projects from the well-known Awesome Python project list.", "labels": [], "entities": [{"text": "Awesome Python project list", "start_pos": 126, "end_pos": 153, "type": "DATASET", "confidence": 0.8868248015642166}]}, {"text": "As in previous work, we perform synthetic experiments on these datasets, which measure how well our models can generate function representations for unseen API descriptions, which mimic user queries.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our current DocExtractor implementation supports building parallel datasets from raw Python source code collections.", "labels": [], "entities": []}, {"text": "Internally, the tool reads source code using the abstract syntax tree utility, ast, in the Python standard library, and extracts sets of function and description pairs.", "labels": [], "entities": []}, {"text": "In addition, the tool also extracts class descriptions, parameter and return value descriptions, and information about the API's internal class hierarchy.", "labels": [], "entities": []}, {"text": "This last type of information is then used to define document-level features.", "labels": [], "entities": []}, {"text": "To experiment with this feature, we built pipelines and ran experiments for 27 popular Python projects.", "labels": [], "entities": []}, {"text": "The goal of these experiments is to test the robustness of our extractor, and see how well our models answer unseen queries for these resources using our previous experimental setup.", "labels": [], "entities": []}, {"text": "The example projects are shown in.", "labels": [], "entities": []}, {"text": "Each dataset is quantified in terms of # Pairs, or the number of parallel function-component representations, the # Symbols in the component output language, the # (NL) Words and Vocab size.", "labels": [], "entities": []}, {"text": "Each dataset is randomly split into train, test, and dev.", "labels": [], "entities": []}, {"text": "sets using a 70%-30% (or 15%/15%) split.", "labels": [], "entities": []}, {"text": "We can think of the held-out sets as mimicking queries that users might ask the model.", "labels": [], "entities": []}, {"text": "Standardly, all models are trained on the training sets, and hyper-parameters are tuned to the dev. sets.", "labels": [], "entities": []}, {"text": "For a unseen text input during testing, the model generates a candidate list of component outputs.", "labels": [], "entities": []}, {"text": "An output is considered correct if it matches exactly the gold function representation.", "labels": [], "entities": []}, {"text": "As before, we measure the Accuracy @1, accuracy within top ten (Accuracy @10), and the MRR.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994315505027771}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9997199177742004}, {"text": "Accuracy @10)", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9564409554004669}, {"text": "MRR", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9367861747741699}]}, {"text": "As in our previous work, three additional baselines are used.", "labels": [], "entities": []}, {"text": "The first is a simple bag-of-words", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: New English Github datasets.", "labels": [], "entities": [{"text": "English Github datasets", "start_pos": 14, "end_pos": 37, "type": "DATASET", "confidence": 0.670757661263148}]}, {"text": " Table 2: Test results on our new Github datasets.", "labels": [], "entities": [{"text": "Github datasets", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8757030963897705}]}]}