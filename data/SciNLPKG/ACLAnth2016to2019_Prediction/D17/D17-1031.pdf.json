{"title": [{"text": "Word Embeddings based on Fixed-Size Ordinally Forgetting Encoding", "labels": [], "entities": [{"text": "Fixed-Size Ordinally Forgetting Encoding", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.614405520260334}]}], "abstractContent": [{"text": "In this paper, we propose to learn word embeddings based on the recent fixed-size ordinally forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence into a fixed-size representation.", "labels": [], "entities": [{"text": "fixed-size ordinally forgetting encoding (FOFE)", "start_pos": 71, "end_pos": 118, "type": "TASK", "confidence": 0.6391209236213139}]}, {"text": "We use FOFE to fully encode the left and right context of each word in a corpus to construct a novel word-context matrix, which is further weighted and factorized using truncated SVD to generate low-dimension word embedding vectors.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.981073260307312}]}, {"text": "We have evaluated this alternative method in encoding word-context statistics and show the new FOFE method has a notable effect on the resulting word embeddings.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9769207835197449}]}, {"text": "Experimental results on several popular word similarity tasks have demonstrated that the proposed method outperforms many recently popular neural prediction methods as well as the conventional SVD models that use canonical count based techniques to generate word context matrices.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.8119259079297384}]}], "introductionContent": [{"text": "Low dimensional vectors as word representations are very popular in NLP tasks such as inferring semantic similarity and relatedness.", "labels": [], "entities": []}, {"text": "Most of these representations are based on either matrix factorization or context sampling described by () as count or predict models.", "labels": [], "entities": []}, {"text": "The basis for both models is the distributional hypothesis, which states that words that appear in similar contexts have similar meaning.", "labels": [], "entities": []}, {"text": "Traditional context representations have been obtained by capturing co-occurrences of words from a fixed-size window relative to the focus word.", "labels": [], "entities": []}, {"text": "This representation however does not encompass the entirety of the context surrounding the focus word.", "labels": [], "entities": []}, {"text": "Therefore, the distributional hypothesis is not being taken advantage of to the fullest extent.", "labels": [], "entities": []}, {"text": "In this work, we seek to capture these contexts through the fixed-size ordinally forgetting encoding (FOFE) method, recently proposed in ().", "labels": [], "entities": [{"text": "fixed-size ordinally forgetting encoding (FOFE)", "start_pos": 60, "end_pos": 107, "type": "TASK", "confidence": 0.5615108524050031}]}, {"text": "In addition to just capturing word co-occurrences, we attempt to use the FOFE to encode the full contexts of each focus word, including the order information of the context sequences.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9829635620117188}]}, {"text": "We believe the full encoding of contexts can enhance the resulting word embedding vectors, derived by factoring the corresponding word-context matrix.", "labels": [], "entities": []}, {"text": "As argued in (), the FOFE method can almost uniquely encode discrete sequences of varying lengths into a fixed-size code, and this encoding method was used to address the challenges of a limited size window when using deep neural networks for language modeling.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8448575735092163}, {"text": "language modeling", "start_pos": 243, "end_pos": 260, "type": "TASK", "confidence": 0.7098584920167923}]}, {"text": "The resulting algorithm fulfills the needs of keeping long term dependency while being fast.", "labels": [], "entities": []}, {"text": "The word order in a sequence is modeled by FOFE using an ordinally-forgetting mechanism which encodes each position of every word in the sequence.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.5175902247428894}]}, {"text": "In this paper, we elaborate how to use the FOFE to fully encode context information of each focus word in text corpora, and present anew method to construct the word-context matrix for word embedding, which maybe weighted and factorized as in traditional vector space models.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9941802024841309}]}, {"text": "Next, we report our experimental results on several popular word similarity tasks, which demonstrate that the proposed FOFE-based approach leads to significantly better performance in these tasks, comparing with the conventional vector space models as well as the popular neural prediction methods, such as word2vec, GloVe and more recent Swivel.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.7516674697399139}, {"text": "FOFE-based", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9935310482978821}]}, {"text": "Finally, this paper will conclude with the analysis and prospects of com-310 bining this approach with other methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on several popular word similarity data sets and compare our FOFE method with other existing word embedding models in these tasks.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9840160608291626}]}, {"text": "In this work, we opt to use five data sets: WordSim353 ( For our training data, we use the standard enwiki9 corpus which contains 130 million words.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.9529908895492554}, {"text": "enwiki9 corpus", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.8021030128002167}]}, {"text": "The pre-processing stage includes discarding extremely long sentences, tokenizing, lowercasing and splitting each sentence as a context.", "labels": [], "entities": []}, {"text": "Our vocabulary size is chosen to be 80,000 for the most frequent words in the corpus.", "labels": [], "entities": []}, {"text": "All words not in the vocabulary are replaced with the token <unk>.", "labels": [], "entities": []}, {"text": "In this work, we use a python-based library, called scipy 2 , to perform truncated SVD to factorize all word-context matrices.", "labels": [], "entities": []}, {"text": "Our first baseline is the conventional vector space model (VSM), relying on the PMI-weighted co-occurrence matrix with dimensionality reduction performed using truncated SVD.", "labels": [], "entities": []}, {"text": "The dimension of word vectors is chosen to be 300 and this number is kept the same for all models examined in this paper.", "labels": [], "entities": []}, {"text": "Our main goal is to outperform VSM as the model proposed in this paper also uses SVD based matrix factorization.", "labels": [], "entities": [{"text": "VSM", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.5470991730690002}]}, {"text": "This allows for appropriate comparisons between the different word encoding methods.", "labels": [], "entities": [{"text": "word encoding", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7089539021253586}]}, {"text": "left FOFE code L right FOFE code R left FOFE code L right FOFE code R left  For the purpose of completeness, the other non-SVD based embedding models, mainly the more recent neural prediction methods, are also compared in our experiments.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9505746960639954}, {"text": "FOFE", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.7183155417442322}, {"text": "FOFE", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.5593118667602539}, {"text": "FOFE", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.8129030466079712}]}, {"text": "As a result, we build the second baseline using the skip-gram model provided by the word2vec software package (), denoted as SGNS.", "labels": [], "entities": []}, {"text": "The word embeddings are generated using the recommended hyper-parameters from (.", "labels": [], "entities": []}, {"text": "Their findings show a larger number of negative samples is preferable and increments on the window size have minimal improvements on word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 133, "end_pos": 154, "type": "TASK", "confidence": 0.7892159223556519}]}, {"text": "In our experiments the number of negative samples is set to 5 and the window size is set to 5.", "labels": [], "entities": []}, {"text": "In addition, we set the subsampling rate to 10 \u22124 and run 3 iterations for training.", "labels": [], "entities": []}, {"text": "In adition to SGNS, we also obtained results for CBOW,) and) models using similar recommended settings.", "labels": [], "entities": [{"text": "CBOW", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.7597883939743042}]}, {"text": "While the window size has a fixed limit in the baseline models, our model does not have a window size parameter as the entire sentence is fully captured as well as distinctions between left and right contexts when generating the FOFE codes.", "labels": [], "entities": [{"text": "FOFE codes", "start_pos": 229, "end_pos": 239, "type": "DATASET", "confidence": 0.7902104258537292}]}, {"text": "The impact of closer context words is further highlighted by the use of the forgetting factor which is unique to the FOFE based word embedding.", "labels": [], "entities": [{"text": "forgetting", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9793533682823181}, {"text": "FOFE", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.8028176426887512}]}, {"text": "Finally, we use the FOFE codes to construct the word-context matrix and generate word embedding as described in sections 3 and 4.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9751998782157898}]}, {"text": "Throughout our experiments, we have chosen to use a constant forgetting factor \u03b1 = 0.7.", "labels": [], "entities": [{"text": "forgetting factor \u03b1", "start_pos": 61, "end_pos": 80, "type": "METRIC", "confidence": 0.9419524669647217}]}, {"text": "There is no significant difference in word similarity scores after experimenting with different \u03b1 values between [0.6, 0.9] when generating FOFE codes.", "labels": [], "entities": [{"text": "word similarity scores", "start_pos": 38, "end_pos": 60, "type": "METRIC", "confidence": 0.6185179352760315}, {"text": "FOFE", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.7476162314414978}]}, {"text": "We have applied the same hyperparameters to both VSM and FOFE methods and fine-tune them based on the recommended settings provided in (.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7432483434677124}]}, {"text": "Although it has been previously reported that context distribution smoothing () can provide a net positive effect, it did not yield significant gains in our experiments.", "labels": [], "entities": [{"text": "context distribution smoothing", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6566119293371836}]}, {"text": "On the other hand, the eigenvalue) proved to be incredibly effective for some datasets but ineffectual in others.", "labels": [], "entities": []}, {"text": "The net benefit however is palpable and we include it for both VSM and FOFE methods.", "labels": [], "entities": [{"text": "FOFE", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.6498953700065613}]}], "tableCaptions": [{"text": " Table 1: The best achieved performance of various word embedding models on all five examined word  similarity tasks.  Method  WordSim353  MEN  Mech Turk Rare Words SimLex-999  VSM+SVD  0.7109  0.7130  0.6258  0.4813  0.3866  CBOW  0.6763  0.6768  0.6621  0.4280  0.3549  GloVe  0.5873  0.6350  0.5831  0.3934  0.2883  SGNS  0.7028  0.6689  0.6187  0.4360  0.3709  Swivel  0.7303  0.7246  0.7024  0.4430  0.3323  FOFE+SVD  0.7580  0.7637  0.6525  0.5002  0.3866", "labels": [], "entities": [{"text": "WordSim353  MEN  Mech Turk Rare Words SimLex-999  VSM", "start_pos": 127, "end_pos": 180, "type": "DATASET", "confidence": 0.7775418348610401}, {"text": "FOFE", "start_pos": 413, "end_pos": 417, "type": "METRIC", "confidence": 0.8682203888893127}]}]}