{"title": [{"text": "ConStance: Modeling Annotation Contexts to Improve Stance Classification", "labels": [], "entities": [{"text": "Improve Stance Classification", "start_pos": 43, "end_pos": 72, "type": "TASK", "confidence": 0.7626449664433798}]}], "abstractContent": [{"text": "Manual annotations area prerequisite for many applications of machine learning.", "labels": [], "entities": []}, {"text": "However, weaknesses in the annotation process itself are easy to overlook.", "labels": [], "entities": []}, {"text": "In particular , scholars often choose what information to give to annotators without examining these decisions empirically.", "labels": [], "entities": []}, {"text": "For subjective tasks such as sentiment analysis , sarcasm, and stance detection, such choices can impact results.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9841273725032806}, {"text": "stance detection", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.9741056561470032}]}, {"text": "Here, for the task of political stance detection on Twit-ter, we show that providing too little context can result in noisy and uncertain annotations , whereas providing too strong a context may cause it to outweigh other signals.", "labels": [], "entities": [{"text": "political stance detection", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.673279732465744}, {"text": "Twit-ter", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.9417797327041626}]}, {"text": "To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions.", "labels": [], "entities": []}, {"text": "Given conflicting labels produced by multiple an-notators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances.", "labels": [], "entities": []}, {"text": "We show that the classifier learned by ConStance outperforms a variety of base-lines at predicting political stance, while the model's interpretable parameters shed light on the effects of each context.", "labels": [], "entities": [{"text": "predicting political stance", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.8616559704144796}]}], "introductionContent": [{"text": "When annotators are asked for objective judgments about a text (e.g., POS tags), the broader context in which the text is situated is often irrelevant.", "labels": [], "entities": []}, {"text": "However, many NLP tasks focus on inference of factors beyond words and syntax.", "labels": [], "entities": []}, {"text": "For example, the present work addresses the task of detecting political stance on Twitter.", "labels": [], "entities": [{"text": "detecting political stance on Twitter", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.8848091006278992}]}, {"text": "We ask annotators to determine whether a given Twitter user supports Donald Trump or Hillary Clinton.", "labels": [], "entities": []}, {"text": "However, inferring something about a user from a single tweet that she writes may prove difficult.", "labels": [], "entities": []}, {"text": "Prior work on stance has relied on annotations collected this way (), but individual tweets do not always contain clear indicators.", "labels": [], "entities": [{"text": "stance", "start_pos": 14, "end_pos": 20, "type": "TASK", "confidence": 0.943773090839386}]}, {"text": "One solution to this issue is to supply the annotator with more information about the user.", "labels": [], "entities": []}, {"text": "For example, for the similar task of classifying a Twitter user's political affiliation, display the user's last 10 tweets., studying gender and age, ask annotators to label users by leveraging all information available in their profile.", "labels": [], "entities": [{"text": "classifying a Twitter user's political affiliation", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.8179140942437308}]}, {"text": "Thus, researchers have provided a range of contexts (or more broadly, information conditions) to annotators in an attempt to balance annotators' exposure to the data needed for accuracy with reasonable costs in terms of time, money and cognitive load.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9728112816810608}]}, {"text": "However, while scholars routinely make such decisions about what information to show annotators, they rarely examine how such decisions actually impact annotations.", "labels": [], "entities": []}, {"text": "The first contribution of this paper (Section 3) is to show that, at least for political stance detection on Twitter, displaying different kinds of context to annotators yields significantly different annotations for the same user.", "labels": [], "entities": [{"text": "political stance detection", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.6573971211910248}]}, {"text": "As a result of these discrepancies, the accuracy of models trained on these annotations varies widely.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994879961013794}]}, {"text": "While it is possible one could select a \"best\" context fora given task, our results suggest that doing so a priori is difficult and that, moreover, different contexts provide complementary information.", "labels": [], "entities": []}, {"text": "What we would prefer, instead, is a model that learns how contexts affect annotators and combines annotations from multiple contexts to create gold standard labels.", "labels": [], "entities": []}, {"text": "Fortunately, prior work suggests mechanisms for such a model.", "labels": [], "entities": []}, {"text": "Typically in annotation tasks, each item is judged by several annotators, and the resulting labels are aggregated, usually by majority vote, to create a gold standard.", "labels": [], "entities": []}, {"text": "As an alternative to majority vote, develop an elegant probabilistic approach for learning to aggregate labels produced by annotators of varying quality.", "labels": [], "entities": []}, {"text": "Their model jointly estimates gold standard labels (in the form of probability scores), infers annotator error rates, and learns a classifier for use on out-of-sample data.", "labels": [], "entities": []}, {"text": "Our second contribution (Section 4) is an extension ofs model to handle labels not only created by annotators of varying quality, but also produced under information conditions of varying quality.", "labels": [], "entities": []}, {"text": "We call this model ConStance . Like, who find that even lowquality annotators are useful, we find that lowquality contexts can be useful.", "labels": [], "entities": []}, {"text": "Specifically, we find that the classifier produced from our model performs better than any classifier trained by majority vote from the same labels.", "labels": [], "entities": []}, {"text": "Furthermore, the model provides an unsupervised method for comparing the information conditions by examining their respective error patterns.", "labels": [], "entities": []}, {"text": "Intuitively, ConStance performs a role analogous to boosting for annotations: for an arbitrary task, it permits collection of labels that capture different aspects of the instances at hand, then combines them automatically to determine which are more reliable and to produce a classifier that takes all this into account.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each of the six contexts separately, we construct labels with which to train a classifier.", "labels": [], "entities": []}, {"text": "Training labels are constructed using majority vote; we also tried weighting the training instances to match the distribution of labels, but it did not perform as well.", "labels": [], "entities": []}, {"text": "We also construct a seventh set of labels using all annotations from all conditions.", "labels": [], "entities": []}, {"text": "We then train a classifier on each set of labels.", "labels": [], "entities": []}, {"text": "We use Random Forest models, as they outperformed regularized logistic regression and SVMs with linear kernels on the development set.", "labels": [], "entities": []}, {"text": "Note that the only difference among the models in this section is the labels they are trained on.", "labels": [], "entities": []}, {"text": "The feature set used, shown in, is meant as a straightforward representation of the information seen by annotators; parts of it follow.", "labels": [], "entities": []}, {"text": "We construct three types of features for each tweet: text, sentiment and user features.", "labels": [], "entities": []}, {"text": "For text features, we collapse the anchor tweet plus all additional textual context seen by any annotator into a single string, then compute various n-grams from it.", "labels": [], "entities": []}, {"text": "For sentiment, we compute various scores from the anchor tweet alone.", "labels": [], "entities": [{"text": "sentiment", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.9817736744880676}]}, {"text": "For user features, we include the user's race and gender, which annotators might have learned from the user's profile picture.", "labels": [], "entities": []}, {"text": "Note that because we want models to generalize beyond registered Democrats or Republicans, we do not include a feature for political party.", "labels": [], "entities": []}, {"text": "Classifier performance on the GS is measured, following prior work (), on the average of the F1 scores on the two classes of interest (\"Clinton\" and \"Trump\").", "labels": [], "entities": [{"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9991961121559143}]}, {"text": "Additionally, we report the average log-loss (the negative log-likelihood, according to the classifier, of the true label).", "labels": [], "entities": []}, {"text": "Log-loss and F1 can be seen as complementary measures: whereas F1 evaluates the quality of the ranking of test instances, log-loss evaluates the quality of their individual probability estimates.", "labels": [], "entities": [{"text": "F1", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9994687438011169}, {"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9966182112693787}]}, {"text": "To compute the probability estimate from a Random Forest, we compute mean class probabilities across all trees.", "labels": [], "entities": []}, {"text": "To assess the statistical significance of differences between two models, we first obtain probability estimates for all GS items.", "labels": [], "entities": []}, {"text": "For log-loss, we use a Mann-Whitney test on the scores from the two models being compared.", "labels": [], "entities": []}, {"text": "For F1, we create 1000 bootstrap iterations of the sample, compute the average F1 of each, and run a non-parametric difference-in-means test, using 95% confidence Anchor tweet, previous (political) tweets, profile description Character n-grams (n \u2208), word n-grams (n \u2208: Inter-annotator agreement, then performance of classifier trained on majority vote labels.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9748902320861816}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.964429497718811}]}, {"text": "(Best possible is 1 for agreement and F1, 0 for logloss.) intervals.", "labels": [], "entities": [{"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9989234805107117}]}], "tableCaptions": [{"text": " Table 3: Inter-annotator agreement, then perfor- mance of classifier trained on majority vote labels.", "labels": [], "entities": []}, {"text": " Table 5: Classification performance of ConStance  and model ablations. Boldface highlights best  scores. Significance tests use the the p < .05  level for log-loss. Compared to the best baselines,  all scores that appear better are statistically signif- icant. Italics indicate the scores that are signifi- cantly worse than ConStance.", "labels": [], "entities": []}]}