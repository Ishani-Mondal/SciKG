{"title": [{"text": "Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics", "labels": [], "entities": [{"text": "Learning Improved Word Representations", "start_pos": 11, "end_pos": 49, "type": "TASK", "confidence": 0.5531408563256264}, {"text": "Ngram Co-occurrence Statistics", "start_pos": 55, "end_pos": 85, "type": "DATASET", "confidence": 0.8528627951939901}]}], "abstractContent": [{"text": "The existing word representation methods mostly limit their information source to word co-occurrence statistics.", "labels": [], "entities": [{"text": "word representation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7412461936473846}]}, {"text": "In this paper , we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix, and its SVD factorization.", "labels": [], "entities": [{"text": "GloVe", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8618142008781433}]}, {"text": "Comprehensive experiments are conducted on word analogy and similarity tasks.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.8040314018726349}]}, {"text": "The results show that improved word representations are learned from ngram co-occurrence statistics.", "labels": [], "entities": []}, {"text": "We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations.", "labels": [], "entities": []}, {"text": "Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burdens brought by ngrams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, deep learning approaches have achieved state-of-the-art results on a range of NLP tasks.", "labels": [], "entities": []}, {"text": "One of the most fundamental work in this field is word embedding, where low-dimensional word representations are learned from unlabeled corpora through neural models.", "labels": [], "entities": [{"text": "word embedding", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7982005178928375}]}, {"text": "The trained word embeddings reflect semantic and syntactic information of words.", "labels": [], "entities": []}, {"text": "They are not only useful in revealing lexical semantics, but also used as inputs of various downstream tasks for better performance).", "labels": [], "entities": []}, {"text": "Most of the word embedding models are trained upon <word, context> pairs in the local window.", "labels": [], "entities": []}, {"text": "Among them, word2vec gains its popularity by its amazing effectiveness and efficiency (.", "labels": [], "entities": []}, {"text": "It achieves stateof-the-art results on a range of linguistic tasks with only a fraction of time compared with previous techniques.", "labels": [], "entities": []}, {"text": "A challenger of word2vec is GloVe ().", "labels": [], "entities": []}, {"text": "Instead of training on <word, context> pairs, GloVe directly utilizes word co-occurrence matrix.", "labels": [], "entities": []}, {"text": "They claim that the change brings the improvement over word2vec on both accuracy and speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9990666508674622}, {"text": "speed", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9792553782463074}]}, {"text": "further reveal that the attractive properties observed in word embeddings are not restricted to neural models such as word2vec and GloVe.", "labels": [], "entities": []}, {"text": "They use traditional count-based method (PPMI matrix with hyper-parameter tuning) to represent words, and achieve comparable results with the above neural embedding models.", "labels": [], "entities": []}, {"text": "The above models limit their information source to word co-occurrence statistics (.", "labels": [], "entities": []}, {"text": "To learn improved word representations, we extend the information source from cooccurrence of 'word-word' type to co-occurrence of 'ngram-ngram' type.", "labels": [], "entities": []}, {"text": "The idea of using ngrams is well supported by language modeling, one of the oldest problems studied in statistical NLP.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7359117120504379}]}, {"text": "In language models, co-occurrence of words and ngrams is used to predict the next word (.", "labels": [], "entities": []}, {"text": "Actually, the idea of word embedding models roots in language models.", "labels": [], "entities": []}, {"text": "They are closely related but are used for different purposes.", "labels": [], "entities": []}, {"text": "Word embedding models aim at learning useful word representations instead of word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7555175423622131}]}, {"text": "Since ngram is a vital part in language modeling, we are inspired to integrate ngram statistical information into the recent word representation methods for better performance.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7159948945045471}]}, {"text": "The idea of using ngrams is intuitive.", "labels": [], "entities": []}, {"text": "However, there is still rare work using ngrams in recent representation methods.", "labels": [], "entities": []}, {"text": "In this paper, we introduce ngrams into SGNS, GloVe, PPMI, and its SVD factorization.", "labels": [], "entities": []}, {"text": "To evaluate the ngram-based models, comprehensive experiments are conducted on word analogy and similarity tasks.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 79, "end_pos": 91, "type": "TASK", "confidence": 0.7651283144950867}]}, {"text": "Experimental results demonstrate that the improved word representations are learned from ngram co-occurrence statistics.", "labels": [], "entities": []}, {"text": "Besides that, we qualitatively evaluate the trained ngram representations.", "labels": [], "entities": []}, {"text": "We show that they are able to reflect ngrams' meanings and syntactic patterns (e.g. 'be + past participle' pattern).", "labels": [], "entities": []}, {"text": "The high-quality ngram representations are useful in many ways.", "labels": [], "entities": []}, {"text": "For example, ngrams in negative form (e.g. 'not interesting') can be used for finding antonyms (e.g. 'boring').", "labels": [], "entities": []}, {"text": "Finally, a novel method is proposed to build ngram co-occurrence matrix.", "labels": [], "entities": []}, {"text": "Our method reduces the disk I/O as much as possible, largely alleviating the costs brought by ngrams.", "labels": [], "entities": []}, {"text": "We unify different representation methods in a pipeline.", "labels": [], "entities": []}, {"text": "The source code is organized as ngram2vec toolkit and released at https://github.com/ zhezhaoa/ngram2vec.", "labels": [], "entities": []}], "datasetContent": [{"text": "The tasks used in this paper is the same with the work of, including six similarity and two analogy datasets.", "labels": [], "entities": []}, {"text": "In similarity task, a scalar (e.g. a score from 0 to 10) is used to measure the relation between the two words.", "labels": [], "entities": []}, {"text": "For example, in a similarity dataset, the 'train, car' pair is given the score of 6.31.", "labels": [], "entities": []}, {"text": "A problem of similarity task is that scalar only reflects the strength of the relation, while the type of relation is totally ignored (.", "labels": [], "entities": []}, {"text": "Due to the deficiency of similarity task, analogy task is widely used as benchmark recently for evaluation of word embedding models.", "labels": [], "entities": []}, {"text": "To answer analogy questions, relations between the two words are reflected by a vector, which is usually obtained by the difference between word embeddings.", "labels": [], "entities": []}, {"text": "Different from a scalar, the vector provides more accurate descriptions of relations.", "labels": [], "entities": []}, {"text": "For example, capital-country relation is encoded in vec(Athens)-vec(Greece), vec(Tokyo)-vec(Japan) and soon.", "labels": [], "entities": []}, {"text": "More concretely, the questions in the analogy task are in the form of 'a is to b as c is to d'.", "labels": [], "entities": []}, {"text": "'d' is an unknown word in the test phase.", "labels": [], "entities": []}, {"text": "To correctly answer the questions, the models should embed the two relations, vec(a)-vec(b) and vec(c)-vec(d), into similar positions in the space.", "labels": [], "entities": []}, {"text": "Following the work of Levy and Goldberg (2014b), both additive (add) and multiplicative (mul) functions are used for finding word 'd'.", "labels": [], "entities": []}, {"text": "The latter one is more suitable for sparse representation in practice.", "labels": [], "entities": []}, {"text": "In this section, we analyze the properties of ngram embeddings trained by SGNS of 'bi bi' type.", "labels": [], "entities": []}, {"text": "Ideally, the trained ngram embeddings should reflect ngrams' semantic meanings.", "labels": [], "entities": []}, {"text": "For example, vec(wasn't able) should be close to vec(unable).", "labels": [], "entities": []}, {"text": "vec(is written) should be close to vec(write) and vec(book).", "labels": [], "entities": []}, {"text": "Also, the trained ngram embeddings should preserve ngrams' syntactic patterns.", "labels": [], "entities": []}, {"text": "For example, 'was written' is in the form of 'be + past participle' and the nearest neighbors should possess similar patterns, such as 'is written' and 'was transcribed'.", "labels": [], "entities": []}, {"text": "lists the target ngrams and their top nearest neighbours.", "labels": [], "entities": []}, {"text": "We divide the target ngrams into six groups according to their patterns.", "labels": [], "entities": []}, {"text": "We can observe that the returned words and ngrams are very intuitive.", "labels": [], "entities": []}, {"text": "As might be expected, synonyms of the target ngrams are returned in top positions (e.g. 'give off' and 'emit'; 'heavy rain' and 'downpours').", "labels": [], "entities": []}, {"text": "From the results of the first group, it can be observed that bigram in negative form 'not X' is useful for finding the antonym of word 'X'.", "labels": [], "entities": []}, {"text": "Besides that, the trained ngram embeddings also preserve some commonsense.", "labels": [], "entities": []}, {"text": "For example, the returned result of 'highest mountain' is a list of mountain names (with a few exceptions such as 'unclimbed').", "labels": [], "entities": []}, {"text": "In terms of syntactic patterns, we can observe that inmost cases, the returned ngrams are in the similar form with target ngrams.", "labels": [], "entities": []}, {"text": "In general, the trained embeddings basically reflect semantic meanings and syntactic patterns of ngrams.", "labels": [], "entities": []}, {"text": "With high-quality ngram embeddings, we have the opportunity to do more interesting things in our future work.", "labels": [], "entities": []}, {"text": "For example, we will construct a antonym dataset to evaluate ngram embeddings systematically.", "labels": [], "entities": []}, {"text": "Besides that, we will find more scenarios for using ngram embeddings.", "labels": [], "entities": []}, {"text": "In our view, ngram embeddings have potential to be used in many NLP tasks.", "labels": [], "entities": []}, {"text": "For example, Johnson and Zhang (2015) use one-hot ngram representation as the input of CNN.", "labels": [], "entities": []}, {"text": "use ngram embeddings to represent texts.", "labels": [], "entities": []}, {"text": "Intuitively, initializing these models with pre-trained ngram embeddings may further improve the accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9814602136611938}]}], "tableCaptions": [{"text": " Table 2: Performance of (ngram) SGNS on analogy datasets.", "labels": [], "entities": []}, {"text": " Table 3: Performance of (ngram) SGNS on similarity datasets.", "labels": [], "entities": []}, {"text": " Table 4: Performance of (ngram) PPMI on analogy and similarity datasets.", "labels": [], "entities": []}, {"text": " Table 5: Performance of (ngram) GloVe on analogy and similarity datasets.", "labels": [], "entities": []}, {"text": " Table 6: Performance of (ngram) SVD on analogy and similarity datasets.", "labels": [], "entities": []}]}