{"title": [{"text": "Segmentation-Free Word Embedding for Unsegmented Languages *", "labels": [], "entities": [{"text": "Segmentation-Free Word Embedding", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8844536741574606}]}], "abstractContent": [{"text": "In this paper, we propose anew pipeline of word embedding for unsegmented languages , called segmentation-free word embedding , which does not require word seg-mentation as a preprocessing step.", "labels": [], "entities": [{"text": "segmentation-free word embedding", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.842316468556722}]}, {"text": "Unlike space-delimited languages, unsegmented languages, such as Chinese and Japanese, require word segmentation as a prepro-cessing step.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7109080851078033}]}, {"text": "However, word segmenta-tion, that often requires manually annotated resources, is difficult and expensive, and unavoidable errors in word segmen-tation affect downstream tasks.", "labels": [], "entities": []}, {"text": "To avoid these problems in learning word vectors of unsegmented languages, we consider word co-occurrence statistics overall possible candidates of segmentations based on frequent character n-grams instead of segmented sentences provided by conventional word segmenters.", "labels": [], "entities": []}, {"text": "Our experiments of noun category prediction tasks on raw Twitter, Weibo, and Wikipedia corpora show that the proposed method outper-forms the conventional approaches that require word segmenters.", "labels": [], "entities": [{"text": "noun category prediction tasks", "start_pos": 19, "end_pos": 49, "type": "TASK", "confidence": 0.82953180372715}, {"text": "word segmenters", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.7288906872272491}]}], "introductionContent": [{"text": "Word embedding, which learns dense vector representation of words from large text corpora, has received much attention in the natural language processing (NLP) community in recent years.", "labels": [], "entities": [{"text": "Word embedding", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6823555678129196}]}, {"text": "It is reported that the representation of words well captures semantic and syntactic properties of words (, * This work was done while the author was at Shimodaira laboratory, Division of Mathematical Science, Graduate School of Engineering Science, Osaka University, and Mathematical Statistics Team, RIKEN Center for Advanced Intelligence Project.: t-SNE projections of vector representation of Japanese nouns that generated by our proposed method without word dictionary.", "labels": [], "entities": []}, {"text": "These proper nouns are color-coded according to its categories which extracted from Wikidata.", "labels": [], "entities": []}, {"text": "2013;, and is useful for many downstream NLP tasks, including part-ofspeech tagging, syntactic parsing, and machine translation.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 62, "end_pos": 83, "type": "TASK", "confidence": 0.7230604887008667}, {"text": "syntactic parsing", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7193101644515991}, {"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8166662752628326}]}, {"text": "In order to train word embedding models on a raw text corpus, we have to do word segmentation as a preprocessing step.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7211008220911026}]}, {"text": "In space-delimited languages such as English and Spanish, simple rulebased and co-occurrence-based approaches offer reasonable segmentations.", "labels": [], "entities": []}, {"text": "On the other hands, these approaches are impractical for unsegmented languages such as Chinese, Japanese, and Thai.", "labels": [], "entities": []}, {"text": "Therefore, machine learning-based approaches are widely used in NLP for unsegmented languages.", "labels": [], "entities": []}, {"text": "Conditional random field (CRF)-based supervised word segmentation () is still the most used one in Japanese and Chinese NLP.", "labels": [], "entities": [{"text": "Conditional random field (CRF)-based supervised word segmentation", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.5782709866762161}]}, {"text": "However, there are some problems for these supervised word segmentation as a preprocessing step of a word embedding pipeline.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7436371743679047}]}, {"text": "First, they require language-specific manually annotated resources such as word dictionaries and segmented corpora.", "labels": [], "entities": []}, {"text": "Since these manually annotated resources are typically unavailable for domainspecific corpora (e.g. Twitter or Weibo corpora that contain many neologisms and informal words), we have to create manually annotated resources if we need.", "labels": [], "entities": []}, {"text": "Second, they cannot take advantage of word occurrence frequencies in a corpus.", "labels": [], "entities": []}, {"text": "Even though a certain proper noun (e.g. \" \" (The Old Man and the Sea)) occurs frequently in a corpus, word segmenters will continue to split the proper noun erroneously (e.g. \" //\" (a old man / and / a sea)) if it is not registered in the word dictionary.", "labels": [], "entities": []}, {"text": "Because of segmentation errors incurred by these problems, the downstream word embedding model cannot learn vector representation of proper nouns, neologisms, and informal words.", "labels": [], "entities": []}, {"text": "In this paper, in order to learn word vectors from a raw text corpus while avoiding the above problems, we propose anew word segmentationfree pipeline for word embedding, referred to as segmentation-free word embedding (sembei).", "labels": [], "entities": []}, {"text": "Our framework first enumerates all possible segmentations (referred to as a frequent n-gram lattice) based on character n-grams that frequently occurred in the raw corpus, and then learns n-gram vectors from co-occurrence frequencies over the frequent n-gram lattice.", "labels": [], "entities": []}, {"text": "Using the general idea of segmentation-free word embedding, we can extend existing word embedding models.", "labels": [], "entities": []}, {"text": "Specifically, in this paper, we propose a segmentationfree version of the widely used skip-gram model with negative sampling (SGNS) (, which we refer to as SGNS-sembei.", "labels": [], "entities": []}, {"text": "Although the frequent character n-grams necessarily include many non-words (i.e. n-grams that are not words), remarkably, our results show that nearest neighbor search works well for frequent words and even proper nouns (e.g. nearest neighbors of n-gram \"\" (Germany) are \"\" (China), \"\" (United Kingdom), etc.).", "labels": [], "entities": []}, {"text": "This observation suggests that we can use the proposed method for automatic acquisition of synonyms from large raw text corpora.", "labels": [], "entities": [{"text": "automatic acquisition of synonyms from large raw text corpora", "start_pos": 66, "end_pos": 127, "type": "TASK", "confidence": 0.8003679348362817}]}, {"text": "We conduct experiments on a noun category prediction task on several corpora and observe that our method outperforms the conventional approaches that use word segmenters.", "labels": [], "entities": [{"text": "noun category prediction task", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.8236376196146011}]}, {"text": "shows a t-SNE projection of vector representation of Japanese nouns which is learned from only a raw Twitter corpus.", "labels": [], "entities": []}, {"text": "We can see that the proposed method can learn vector representation of these nouns, and the learnt representation achieves good separation based on their categories.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our method by the noun category prediction task on Twitter, Weibo, and Wikipedia corpora.", "labels": [], "entities": [{"text": "noun category prediction", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8300502101580302}, {"text": "Weibo", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9355785846710205}, {"text": "Wikipedia corpora", "start_pos": 100, "end_pos": 117, "type": "DATASET", "confidence": 0.9063310921192169}]}, {"text": "The C++ implementation of the proposed method is available on GitHub 1 .", "labels": [], "entities": []}], "tableCaptions": []}