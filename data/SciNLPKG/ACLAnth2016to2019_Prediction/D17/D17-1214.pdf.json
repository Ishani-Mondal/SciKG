{"title": [{"text": "Accurate Supervised and Semi-Supervised Machine Reading for Long Documents", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9639044404029846}, {"text": "Machine Reading", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7106494009494781}]}], "abstractContent": [{"text": "We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.7747354507446289}]}, {"text": "The model divides the document into small, overlapping windows and encodes all windows in parallel with an RNN.", "labels": [], "entities": []}, {"text": "It then attends over these window encodings, reducing them to a single encoding , which is decoded into an answer using a sequence decoder.", "labels": [], "entities": []}, {"text": "This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps.", "labels": [], "entities": []}, {"text": "Ina supervised setting, our model achieves state of the art accuracy of 76.8 on the WikiRead-ing dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996064305305481}, {"text": "WikiRead-ing dataset", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.9709025621414185}]}, {"text": "We also evaluate the model in a semi-supervised setting by downsam-pling the WikiReading training set to create increasingly smaller amounts of supervision , while leaving the full unlabeled document corpus to train a sequence au-toencoder on document windows.", "labels": [], "entities": [{"text": "WikiReading training set", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.8981590867042542}]}, {"text": "We evaluate models that can reuse autoen-coder states and outputs without fine-tuning their weights, allowing for more efficient training and inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, deep neural networks (DNNs) have provided promising results fora variety of reading comprehension and question answering tasks, which require extracting precise information from documents conditioned on a query.", "labels": [], "entities": [{"text": "question answering", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7519091665744781}]}, {"text": "While a basic sequence to sequence (seq2seq) model ) can perform these * Work completed while interning at Google Research.", "labels": [], "entities": []}, {"text": "\u2020 Work completed while at Google Research.", "labels": [], "entities": []}, {"text": "tasks by encoding a question and document sequence and decoding an answer sequence, it has some disadvantages.", "labels": [], "entities": []}, {"text": "The answer maybe encountered early in the text and need to be stored across all the further recurrent steps, leading to forgetting or corruption; Attention can be added to the decoder to solve this problem (.", "labels": [], "entities": []}, {"text": "Even with attention, approaches based on Recurrent Neural Networks (RNNs) require a number of sequential steps proportional to the document length to encode each document position.", "labels": [], "entities": []}, {"text": "Hierarchical reading models address this problem by breaking the document into sentences (.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a simpler hierarchical model that achieves state-of-the-art performance on our benchmark task without this linguistic structure, and use it as framework to explore semisupervised learning for reading comprehension.", "labels": [], "entities": []}, {"text": "We first develop a hierarchical reader called Sliding-Window Encoder Attentive Reader (SWEAR) that circumvents the aforementioned bottlenecks of existing readers.", "labels": [], "entities": [{"text": "Sliding-Window Encoder Attentive Reader (SWEAR)", "start_pos": 46, "end_pos": 93, "type": "TASK", "confidence": 0.8039227894374302}]}, {"text": "SWEAR, illustrated in, first encodes each question into a vector space representation.", "labels": [], "entities": []}, {"text": "It then chunks each document into overlapping, fixed-length windows and, conditioned on the question representation, encodes each window in parallel.", "labels": [], "entities": []}, {"text": "Inspired by recent attention mechanisms such as, SWEAR attends over the window representations and reduces them into a single vector for each document.", "labels": [], "entities": []}, {"text": "Finally, the answer is decoded from this document vector.", "labels": [], "entities": []}, {"text": "Our results show that SWEAR outperforms the previous state-of-the-art on the supervised WikiReading task, improving Mean F1 to 76.8 from the previous 75.6 (.", "labels": [], "entities": [{"text": "SWEAR", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.5626810789108276}, {"text": "Mean", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9919942617416382}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.5165538787841797}]}, {"text": "While WikiReading is a large dataset with millions of labeled examples, many applications of machine reading have a much smaller number of labeled examples among a large set of unlabeled documents.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.736784815788269}]}, {"text": "To model this situation, we constructed a semi-supervised version of WikiReading by downsampling the labeled corpus into a variety of smaller subsets, while preserving the full unlabeled corpus (i.e., Wikipedia).", "labels": [], "entities": []}, {"text": "To take advantage of the unlabeled data, we evaluated multiple methods of reusing unsupervised recurrent autoencoders in semi-supervised versions of SWEAR.", "labels": [], "entities": []}, {"text": "Importantly, in these models we are able to reuse all the autoencoder parameters without fine-tuning, meaning the supervised phase only has to learn to condition the answer on the document and query.", "labels": [], "entities": []}, {"text": "This allows for more efficient training and online operation: Documents can be encoded in a single pass offline and these encodings reused by all models, both during training and when answering queries.", "labels": [], "entities": []}, {"text": "Our semisupervised learning models achieve significantly better performance than supervised SWEAR on several subsets with different characteristics.", "labels": [], "entities": []}, {"text": "The best-performing model reaches 66.5 with 1% of the WikiReading dataset, compared to the 2016 state of the art of 71.8 (with 100% of the dataset).", "labels": [], "entities": [{"text": "WikiReading dataset", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.97027587890625}]}], "datasetContent": [{"text": "As described in Section 2, we evaluate our models on the WikiReading task.", "labels": [], "entities": []}, {"text": "In Section 3.5 we presented results for the supervised SWEAR on the full WikiReading dataset, establishing it as the highest-scoring method so far developed for WikiReading.", "labels": [], "entities": [{"text": "SWEAR", "start_pos": 55, "end_pos": 60, "type": "TASK", "confidence": 0.8776571750640869}, {"text": "WikiReading dataset", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.9123610854148865}]}, {"text": "We now compare our semisupervised models SWEAR-MLR and SWEAR-Model 1% 0.5% 0.1% SWEAR 63.5 57.6 39.5 SWEAR-SS (RAE) 64.7 62.8 55.3 SWEAR-SS (VRAE) 65.7 64.0 60.7 PR over various subsets of the WikiReading dataset, using SWEAR as a baseline.", "labels": [], "entities": [{"text": "WikiReading dataset", "start_pos": 193, "end_pos": 212, "type": "DATASET", "confidence": 0.9275736212730408}]}, {"text": "Following Hewlett et al., we use the Mean F1 metric for WikiReading, which assigns partial credit when there are multiple valid answers.", "labels": [], "entities": [{"text": "Mean F1 metric", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.8434751431147257}, {"text": "WikiReading", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.88910973072052}]}, {"text": "We ran hyperparameter tuning for all models and report the result for the configuration with the highest Mean F1 on the validation set.", "labels": [], "entities": [{"text": "Mean F1", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.8962198197841644}]}, {"text": "The supervised SWEAR model was trained on both the full training (results reported in Section 3.5) and on each subset of training data (results reported below).", "labels": [], "entities": [{"text": "SWEAR", "start_pos": 15, "end_pos": 20, "type": "TASK", "confidence": 0.9715384840965271}]}, {"text": "Unsupervised autoencoders were trained on all documents in the WikiReading training set.", "labels": [], "entities": [{"text": "WikiReading training set", "start_pos": 63, "end_pos": 87, "type": "DATASET", "confidence": 0.9067221879959106}]}, {"text": "We selected the autoencoder with the lowest reconstruction error for use in semi-supervised experiments.", "labels": [], "entities": [{"text": "reconstruction error", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.9248397648334503}]}, {"text": "After initialization with weights from the best autoencoder, learnable parameters in the semi-supervised models were trained exactly as in the supervised model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for SWEAR compared to top published re- sults on the WikiReading test set.", "labels": [], "entities": [{"text": "SWEAR", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.9469627737998962}, {"text": "WikiReading test set", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9387450814247131}]}, {"text": " Table 2: Mean F1 for SWEAR on each type of property com- pared with the best results for each type reported in Hewlett  et al. (2016), which come from different models. Other pub- lications did not report these sub-scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.850824773311615}, {"text": "SWEAR", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.8615003228187561}]}, {"text": " Table 3: Comparison of Mean F1 for SWEAR and a baseline  seq2seq model on the WikiReading test set across different  document length ranges. pct indicates the percentage of the  dataset falling in the given document length range. imp is the  percentage improvement of SWEAR over baseline.", "labels": [], "entities": [{"text": "Mean F1", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8938015997409821}, {"text": "WikiReading test set", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.9290500084559122}]}, {"text": " Table 5: Results for SWEAR and the best SWEAR-SS initial- ization (VRAE) trained on 100-and 200-per-property sub- sets, respectively.", "labels": [], "entities": [{"text": "SWEAR", "start_pos": 22, "end_pos": 27, "type": "TASK", "confidence": 0.7949115633964539}, {"text": "SWEAR-SS initial- ization (VRAE)", "start_pos": 41, "end_pos": 73, "type": "METRIC", "confidence": 0.6500035098620823}]}, {"text": " Table 6: Results for semi-supervised reviewer models trained  on the 1% subset of WikiReading.", "labels": [], "entities": []}]}