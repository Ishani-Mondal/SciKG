{"title": [{"text": "Adversarial Learning for Neural Dialogue Generation", "labels": [], "entities": [{"text": "Neural Dialogue", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7210456430912018}]}], "abstractContent": [{"text": "In this paper, drawing intuition from the Turing test, we propose using adversar-ial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances.", "labels": [], "entities": [{"text": "open-domain dialogue generation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.6644529501597086}]}, {"text": "We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6893673837184906}]}, {"text": "The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues.", "labels": [], "entities": []}, {"text": "In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls.", "labels": [], "entities": []}, {"text": "Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open domain dialogue generation ( aims at generating meaningful and coherent dialogue responses given the dialogue history.", "labels": [], "entities": [{"text": "Open domain dialogue generation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7023688033223152}]}, {"text": "Prior systems, e.g., phrase-based machine translation systems or end-to-end neural systems () approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.6307802796363831}, {"text": "maximum likelihood estimation (MLE) objective", "start_pos": 197, "end_pos": 242, "type": "METRIC", "confidence": 0.8049554143633161}]}, {"text": "Despite its success, this over-simplified training objective leads to problems: responses are dull, generic), repetitive, and short-sighted ().", "labels": [], "entities": []}, {"text": "Solutions to these problems require answering a few fundamental questions: what are the crucial aspects that characterize an ideal conversation, how can we quantitatively measure them, and how can we incorporate them into a machine learning system?", "labels": [], "entities": []}, {"text": "For example, manually define three ideal dialogue properties (ease of answering, informativeness and coherence) and use a reinforcement-learning framework to train the model to generate highly rewarded responses.", "labels": [], "entities": [{"text": "ease", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9849046468734741}]}, {"text": "use keyword retrieval confidence as a reward.", "labels": [], "entities": []}, {"text": "However, it is widely acknowledged that manually defined reward functions can't possibly coverall crucial aspects and can lead to suboptimal generated utterances.", "labels": [], "entities": []}, {"text": "A good dialogue model should generate utterances indistinguishable from human dialogues.", "labels": [], "entities": []}, {"text": "Such a goal suggests a training objective resembling the idea of the Turing test.", "labels": [], "entities": []}, {"text": "We borrow the idea of adversarial training) in computer vision, in which we jointly train two models, a generator (a neural SEQ2SEQ model) that defines the probability of generating a dialogue sequence, and a discriminator that labels dialogues as human-generated or machine-generated.", "labels": [], "entities": []}, {"text": "This discriminator is analogous to the evaluator in the Turing test.", "labels": [], "entities": []}, {"text": "We cast the task as a reinforcement learning problem, in which the quality of machinegenerated utterances is measured by its ability to fool the discriminator into believing that it is a human-generated one.", "labels": [], "entities": []}, {"text": "The output from the discriminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues.", "labels": [], "entities": []}, {"text": "The idea of a Turing test-employing an evaluator to distinguish machine-generated texts from human-generated ones-can be applied not only to training but also testing, where it goes by the name of adversarial evaluation.", "labels": [], "entities": []}, {"text": "Adversarial evaluation was first employed in to evaluate sentence generation quality, and preliminarily studied for dialogue generation by.", "labels": [], "entities": [{"text": "Adversarial evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8469704985618591}, {"text": "sentence generation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7566916048526764}, {"text": "dialogue generation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.778520941734314}]}, {"text": "In this paper, we discuss potential pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our approach produces more interactive, interesting, and non-repetitive responses than standard SEQ2SEQ models trained using the MLE objective function.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss strategies for successful adversarial evaluation.", "labels": [], "entities": []}, {"text": "Note that the proposed adversarial training and adversarial evaluation are separate procedures.", "labels": [], "entities": [{"text": "adversarial evaluation", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.6891961693763733}]}, {"text": "They are independent of each other and share no common parameters.", "labels": [], "entities": []}, {"text": "The idea of adversarial evaluation, first proposed by, is to train a discriminant function to separate generated and true sentences, in an attempt to evaluate the model's sentence generation capability.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 171, "end_pos": 190, "type": "TASK", "confidence": 0.736147403717041}]}, {"text": "The idea has been preliminarily studied by in the context of dialogue generation.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8522213995456696}]}, {"text": "Adversarial evaluation also resembles the idea of the Turing test, which To compensate for the loss of short responses, one can train a separate model using short sequences.", "labels": [], "entities": [{"text": "Adversarial evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8160590827465057}]}, {"text": "We treat each sentence as a document.", "labels": [], "entities": []}, {"text": "Learning rates are normalized within one batch.", "labels": [], "entities": []}, {"text": "For example, suppose t1, t2, ..., ti, ...", "labels": [], "entities": []}, {"text": ",tN denote the tf-idf scores for sentences within current batch and lr denotes the original learning rate.", "labels": [], "entities": []}, {"text": "The learning rate for sentence with index i is N \u00b7 lr \u00b7 ti it i . To avoid exploding learning rates for sequences with extremely rare words, the tf-idf score of a sentence is capped at L times the minimum tf-idf score in the current batch.", "labels": [], "entities": []}, {"text": "L is empirically chosen and is set to 3.", "labels": [], "entities": []}, {"text": "requires a human evaluator to distinguish machinegenerated texts from human-generated ones.", "labels": [], "entities": []}, {"text": "Since it is time-consuming and costly to ask a human to talk to a model and give judgements, we train a machine evaluator in place of the human evaluator to distinguish the human dialogues and machine dialogues, and we use it to measure the general quality of the generated responses.", "labels": [], "entities": []}, {"text": "Adversarial evaluation involves both training and testing.", "labels": [], "entities": [{"text": "Adversarial evaluation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8417452871799469}]}, {"text": "At training time, the evaluator is trained to label dialogues as machine-generated (negative) or human-generated (positive).", "labels": [], "entities": []}, {"text": "At test time, the trained evaluator is evaluated on a held-out dataset.", "labels": [], "entities": []}, {"text": "If the human-generated dialogues and machinegenerated ones are indistinguishable, the model will achieve 50 percent accuracy attest time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9994010925292969}]}, {"text": "In this section, we detail experimental results on adversarial success and human evaluation.).", "labels": [], "entities": [{"text": "human evaluation.", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.6606347262859344}]}, {"text": "multi-utterance dialogue (i.e., input messages and responses) is transformed to a unigram representation; (2) Concat Neural: a neural classification model with a softmax function that takes as input the concatenation of representations of constituent dialogues sentences; (3) Hierarchical Neural: a hierarchical encoder with a structure similar to the discriminator used in the reinforcement; and (4) SVM+Neural+multi-lex-features: a SVM model that uses the following features: unigrams, neural representations of dialogues obtained by the neural model trained using strategy (3), the forward likelihood log p(t|s) and backward likelihood p(s|t).", "labels": [], "entities": [{"text": "forward likelihood log p(", "start_pos": 585, "end_pos": 610, "type": "METRIC", "confidence": 0.863141906261444}]}, {"text": "ERE scores obtained by different models are reported in.", "labels": [], "entities": [{"text": "ERE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7238736748695374}]}, {"text": "As can be seen, the hierarchical neural evaluator (model 3) is more reliable than simply concatenating the sentence-level representations (model 2).", "labels": [], "entities": []}, {"text": "Using the combination of neural features and lexicalized features yields the most reliable evaluator.", "labels": [], "entities": []}, {"text": "For the rest of this section, we report results obtained by the Hierarchical Neural setting due to its end-to-end nature, despite its inferiority to SVM+Neural+multil-features.", "labels": [], "entities": []}, {"text": "presents AdverSuc values for different models, along with machine-vs-random accuracy described in Section 4.3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9571167826652527}]}, {"text": "Higher values of AdverSuc and machine-vs-random are better.", "labels": [], "entities": []}, {"text": "Baselines we consider include standard SEQ2SEQ models using greedy decoding (MLEgreedy), beam-search (MLE+BS) and sampling, as well as the mutual information reranking model of with two algorithmic variations: (1) MMI+p(t|s), in which a large N-best list is first The representation before the softmax layer.: AdverSuc and machine-vs-random scores achieved by different training/decoding strategies.", "labels": [], "entities": []}, {"text": "For human evaluation, we follow protocols defined in, employing crowdsourced judges to evaluate a random sample of 200 items.", "labels": [], "entities": []}, {"text": "We present both an input message and the generated outputs to 3 judges and ask them to decide which of the two outputs is better (single-turn general quality).", "labels": [], "entities": []}, {"text": "Identical strings are assigned the same score.", "labels": [], "entities": []}, {"text": "We also present the judges with multi-turn conversations simulated between the two agents.", "labels": [], "entities": []}, {"text": "Each conversation consists: The gain from the proposed adversarial model over the mutual information system based on pairwise human judgments. of 3 turns.", "labels": [], "entities": []}, {"text": "We observe a significant quality improvement on both single-turn quality and multi-turn quality from the proposed adversarial model.", "labels": [], "entities": []}, {"text": "It is worth noting that the reinforcement learning system described in, which simulates conversations between two bots and is trained based on manually designed reward functions, only improves multiturn dialogue quality, while the model described in this paper improves both single-turn and multiturn dialogue generation quality.", "labels": [], "entities": [{"text": "multiturn dialogue generation", "start_pos": 291, "end_pos": 320, "type": "TASK", "confidence": 0.7148623665173849}]}, {"text": "This confirms that the reward adopted in adversarial training is more general, natural and effective in training dialogue systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: ERE scores obtained by different models.", "labels": [], "entities": [{"text": "ERE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9576101899147034}]}, {"text": " Table 3: AdverSuc and machine-vs-random scores  achieved by different training/decoding strategies.", "labels": [], "entities": []}, {"text": " Table 4: The gain from the proposed adversarial  model over the mutual information system based  on pairwise human judgments.", "labels": [], "entities": []}]}