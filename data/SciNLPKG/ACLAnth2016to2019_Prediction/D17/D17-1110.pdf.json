{"title": [{"text": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "labels": [], "entities": [{"text": "Relevance Matching", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.6788400262594223}]}], "abstractContent": [{"text": "In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.72347092628479}]}, {"text": "While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document.", "labels": [], "entities": []}, {"text": "Extensive experiments on six years' TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.", "labels": [], "entities": [{"text": "TREC Web Track data", "start_pos": 36, "end_pos": 55, "type": "DATASET", "confidence": 0.8738240152597427}]}], "introductionContent": [{"text": "Despite the widespread use of deep neural models across a range of linguistic tasks, to what extent such models can improve information retrieval (IR) and which components a deep neural model for IR should include remain open questions.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 124, "end_pos": 150, "type": "TASK", "confidence": 0.8222602963447571}]}, {"text": "In ad-hoc IR, the goal is to produce a ranking of relevant documents given an open-domain (\"ad hoc\") query and a document collection.", "labels": [], "entities": [{"text": "IR", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.7641313076019287}]}, {"text": "A ranking model thus aims at evaluating the interactions between different documents and a query, assigning higher scores to documents that better match the query.", "labels": [], "entities": []}, {"text": "Learning to rank models, like the recent IRGAN model (, rely on handcrafted features to encode query document interactions, e.g., the relevance scores from unsupervised ranking models.", "labels": [], "entities": []}, {"text": "Neural IR models differ in that they extract interactions directly based on the queries and documents.", "labels": [], "entities": [{"text": "Neural IR", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.7838143706321716}]}, {"text": "Many early neural IR models can be categorized as semantic matching models, as they embed both queries and documents into a low-dimensional space, and then assess their similarity based on such dense representations.", "labels": [], "entities": []}, {"text": "Examples in this regard include DSSM () and DESM (.", "labels": [], "entities": [{"text": "DESM", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.5199164152145386}]}, {"text": "The notion of relevance is inherently asymmetric, however, making it different from well-studied semantic matching tasks such as semantic relatedness and paraphrase detection.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 154, "end_pos": 174, "type": "TASK", "confidence": 0.7817067503929138}]}, {"text": "Instead, relevance matching models such as MatchPyramid (), DRMM ( ) and the recent K-NRM ( resemble traditional IR retrieval measures in that they directly consider the relevance of documents' contents with respect to the query.", "labels": [], "entities": [{"text": "relevance matching", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7240104079246521}, {"text": "DRMM", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8263126015663147}]}, {"text": "The DUET model () is a hybrid approach that combines signals from a local model for relevance matching and a distributed model for semantic matching.", "labels": [], "entities": [{"text": "relevance matching", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7519741356372833}, {"text": "semantic matching", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7164334952831268}]}, {"text": "The two classes of models are fairly distinct.", "labels": [], "entities": []}, {"text": "In this work, we focus on relevance matching models.", "labels": [], "entities": [{"text": "relevance matching", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8227583169937134}]}, {"text": "Given that relevance matching approaches mirror ideas from traditional retrieval models, the decades of research on ad-hoc IR can guide us with regard to the specific kinds of relevance signals a model ought to capture.", "labels": [], "entities": [{"text": "relevance matching", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8249434232711792}, {"text": "IR", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9193483591079712}]}, {"text": "Unigram matches are the most obvious signals to be modeled, as a counterpart to the term frequencies that appear in almost all traditional retrieval models.", "labels": [], "entities": []}, {"text": "Beyond this, positional information, including where query terms occur and how they depend on each other, can also be exploited, as demonstrated in retrieval models that are aware of term proximity ( and term dependencies ().", "labels": [], "entities": []}, {"text": "Query coverage is another factor that can be used to ensure that, for queries with multiple terms, top-ranked documents contain multiple query terms rather than emphasizing only one query term.", "labels": [], "entities": [{"text": "Query coverage", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7813002467155457}]}, {"text": "For example, given the query \"dog adoption requirements\", unigram matching signals correspond to the occurrences of the individual terms \"dog\", \"adoption\", or \"requirements\".", "labels": [], "entities": []}, {"text": "When considering positional information, text passages with \"dog adoption\" or \"requirements for dog adoption\" are highlighted, distinguishing them from text that only includes individual terms.", "labels": [], "entities": [{"text": "dog adoption", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.6846817135810852}, {"text": "dog adoption", "start_pos": 96, "end_pos": 108, "type": "TASK", "confidence": 0.6766991317272186}]}, {"text": "Query coverage, meanwhile, further emphasizes that matching signals for \"dog\", \"adoption\", and \"requirements\" should all be included in a document.", "labels": [], "entities": [{"text": "Query coverage", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.5411234945058823}, {"text": "adoption\"", "start_pos": 80, "end_pos": 89, "type": "TASK", "confidence": 0.8417756855487823}]}, {"text": "Similarity signals from unigram matches are taken as input by DRMM (  after being summarized as histograms, whereas K-NRM () directly digests a query-document similarity matrix and summarizes it with multiple kernel functions.", "labels": [], "entities": [{"text": "DRMM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.8825441002845764}]}, {"text": "As for positional information, both the MatchPyramid () and local DUET () models account for it by incorporating convolutional layers based on similarity matrices between queries and documents.", "labels": [], "entities": [{"text": "MatchPyramid", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.8899462223052979}]}, {"text": "Although this leads to more complex models, both have difficulty in significantly outperforming the DRMM model (.", "labels": [], "entities": []}, {"text": "This indicates that it is non-trivial to go beyond unigrams by utilizing positional information in deep neural IR models.", "labels": [], "entities": []}, {"text": "Intuitively, unlike in standard sequencebased models, the interactions between a query and a document are sequential along the query axis as well as along the document axis, making the problem multi-dimensional in nature.", "labels": [], "entities": []}, {"text": "In addition, this makes it non-trivial to combine matching signals from different parts of the documents and over different query terms.", "labels": [], "entities": []}, {"text": "In fact, we argue that both MatchPyramid and local DUET models fail to fully account for one or more of the aforementioned factors.", "labels": [], "entities": [{"text": "MatchPyramid", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.6273230910301208}]}, {"text": "For example, as a pioneering work, MatchPyramid is mainly motivated by models developed in computer vision, resulting in its disregard of certain IR-specific considerations in the design of components, such as pooling sizes that ignore the query and document dimensions.", "labels": [], "entities": []}, {"text": "Meanwhile, local DUET's CNN filters match entire documents against individual query terms, neglecting proximity and possible dependencies among different query terms.", "labels": [], "entities": []}, {"text": "We conjecture that a suitable combination of convolutional kernels and recurrent layers can lead to a model that better accounts for these factors.", "labels": [], "entities": []}, {"text": "In particular, we present a novel re-ranking model called PACRR (Position-Aware ConvolutionalRecurrent Relevance Matching).", "labels": [], "entities": [{"text": "PACRR", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.8880435824394226}, {"text": "Position-Aware ConvolutionalRecurrent Relevance Matching)", "start_pos": 65, "end_pos": 122, "type": "TASK", "confidence": 0.6765967071056366}]}, {"text": "Our approach first produces similarity matrices that record the semantic similarity between each query term and each individual term occurring in a document.", "labels": [], "entities": []}, {"text": "These matrices are then fed through a series of convolutional, max-k-pooling, and recurrent layers so as to capture interactions corresponding to, for instance, bigram and trigram matches, and finally to aggregate the signals in order to produce global relevance assessments.", "labels": [], "entities": []}, {"text": "In our model, the convolutional layers are designed to capture both unigram matching and positional information over text windows with different lengths; k-max pooling layers are along the query dimension, preserving matching signals over different query terms; the recurrent layer combines signals from different query terms to produce a query-document relevance score.", "labels": [], "entities": []}, {"text": "The rest of this paper unfolds as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes our approach for computing similarity matrices and the architecture of our deep learning model.", "labels": [], "entities": []}, {"text": "The setup and results of our extensive experimental evaluation can be found in Section 3, before concluding in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we empirically evaluate PACRR models using manual relevance judgments from the standard TREC Web Track.", "labels": [], "entities": [{"text": "TREC Web Track", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9151418606440226}]}, {"text": "We compare them against several state-of-the-art neural IR models  We rely on the widely-used 2009-2014 TREC Web Track ad-hoc task benchmarks 3 . The benchmarks are based on the CLUEWEB09 and CLUEWEB12 datasets as document collections.", "labels": [], "entities": [{"text": "TREC Web Track ad-hoc task benchmarks", "start_pos": 104, "end_pos": 141, "type": "DATASET", "confidence": 0.7070799618959427}, {"text": "CLUEWEB12 datasets", "start_pos": 192, "end_pos": 210, "type": "DATASET", "confidence": 0.8907626867294312}]}, {"text": "In total, there are 300 queries and more than 100k judgments (qrels).", "labels": [], "entities": []}, {"text": "Three years  of query-likelihood baselines provided by TREC 5 serve as baseline runs in the RERANKSIMPLE benchmark.", "labels": [], "entities": [{"text": "TREC 5", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.756662517786026}, {"text": "RERANKSIMPLE", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.9563780426979065}]}, {"text": "In the RERANKALL setting, the search results from runs submitted by participants from each year are also considered: there are 71 (2009), 55 Training.", "labels": [], "entities": [{"text": "RERANKALL", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.7519865036010742}]}, {"text": "At each step, we perform Stochastic Gradient Descent (SGD) with a mini-batch of 32 triples.", "labels": [], "entities": [{"text": "Stochastic Gradient Descent (SGD)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7741115142901739}]}, {"text": "For the purpose of choosing the triples, we consider all documents that are judged with a label more relevant than Rel 7 as highly relevant, and put the remaining relevant documents into a relevant group.", "labels": [], "entities": [{"text": "Rel 7", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.8598820865154266}]}, {"text": "To pick each triple, we sample a relevance group with probability proportional to the number of documents in the group within the training set, and then we randomly sample a document with the chosen label to serve as the positive document d + . If the chosen group is the highly relevant group, we randomly sample a document from the relevant group to serve as the negative document d \u2212 . If the chosen group is the relevant group, we randomly sample a non-relevant document as d \u2212 . This sampling procedure ensures that we differentiate between highly relevant documents (i.e., those with a relevance label of HRel, Key or Nav) and relevant documents (i.e., those are labeled as Rel).", "labels": [], "entities": []}, {"text": "The training continues until a given number of iterations is reached.", "labels": [], "entities": []}, {"text": "The model is saved at every iteration.", "labels": [], "entities": []}, {"text": "We use the model with the best ERR@20 on the validation set to make predictions.", "labels": [], "entities": [{"text": "ERR", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9965214729309082}]}, {"text": "Proceeding in a round-robin manner, we report test results on one year by exploiting the respective remaining five years (250 queries) for training.", "labels": [], "entities": []}, {"text": "From these 250 queries, we reserve 50 random queries as a held-out set for validation and hyper-parameter tuning, while the remaining 200 queries serve as the actual training set.", "labels": [], "entities": [{"text": "validation", "start_pos": 75, "end_pos": 85, "type": "TASK", "confidence": 0.9705281853675842}]}, {"text": "As mentioned, model parameters and training iterations are chosen by maximizing the ERR@20 on the validation set.", "labels": [], "entities": [{"text": "ERR", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9923818707466125}]}, {"text": "The selected model is then used to make predictions on the test data.", "labels": [], "entities": []}, {"text": "An example of this training procedure is shown in.", "labels": [], "entities": []}, {"text": "There are four hyper-parameters that govern the behavior of the proposed PACRR-kwindow and PACRR-firstk: the unified length of the document dimension l d , the k-max pooling size n s , the maximum n-gram size lg , and the number of filters used in convolutional layers n f . Due to limited computational resources, we determine the range of hyper-parameters to consider based on pilot experiments and domain insights.", "labels": [], "entities": [{"text": "PACRR-firstk", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.780846893787384}]}, {"text": "In particular, we evaluate l d \u2208, n s \u2208 [1, 2, 3, 4], and lg \u2208 [2, 3, 4].", "labels": [], "entities": []}, {"text": "Due to the limited possible matching patterns given a small kernel size (e.g., lg = 3), n f is fixed to 32.", "labels": [], "entities": []}, {"text": "For PACRR-firstk, we intuitively desire to retain as much information as possible from the input, and thus l dis always set to 768.", "labels": [], "entities": []}, {"text": "DRMM (DRMM LCH\u00d7IDF ), DUET, MatchPyramid and K-NRM are trained under the same settings using the hyperparameters described in their respective papers.", "labels": [], "entities": [{"text": "DRMM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7285667657852173}, {"text": "MatchPyramid", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.7824690937995911}]}, {"text": "In particular, as our focus is on the deep relevance matching model as mentioned in Section 1, we only compare against DUET's local model, denoted as DUETL.", "labels": [], "entities": [{"text": "deep relevance matching", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6158222556114197}, {"text": "DUET", "start_pos": 119, "end_pos": 123, "type": "DATASET", "confidence": 0.8471525311470032}, {"text": "DUETL", "start_pos": 150, "end_pos": 155, "type": "DATASET", "confidence": 0.9329181909561157}]}, {"text": "In addition, K-NRM is trained slightly different from the one described in (, namely, with a frozen word embedding layer.", "labels": [], "entities": []}, {"text": "This is to guarantee its fair comparison with other models, given that most of the compared models can be enhanced by co-training the embedding layers, whereas the focus here is the strength coming from the model architecture.", "labels": [], "entities": []}, {"text": "A fully connected middle layer with 30 neurons is added to compensate for the reduction of trainable parameters in K-NRM, mirroring the size of DRMM's first fully connected layer.", "labels": [], "entities": []}, {"text": "All models are implemented with Keras) using Tensorflow as backend, and are trained on servers with multiple CPU cores.", "labels": [], "entities": []}, {"text": "In particular, the training of PACRR takes 35 seconds per iteration on average, and in total at most 150 iterations are trained for each model variant.", "labels": [], "entities": [{"text": "PACRR", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.5026482343673706}]}], "tableCaptions": [{"text": " Table 1: ERR@20 and nDCG@20 on TREC Web Track 2012-14 when re-ranking search results from  QL. The comparisons are conducted between two variants of PACRR and DRMM (D/d), DUETL (L/l),  MatchPyramid (M/m) and K-NRM (K/k). All methods are compared against the QL (Q/q) baseline.  The upper/lower-case characters in the brackets indicate a significant difference under two-tailed paired  Student's t-tests at 95% or 90% confidence levels relative to the corresponding approach. In addition, the  relative ranks among all runs within the respective years according to ERR@20 and nDCG@20 are also  reported directly after the absolute scores.", "labels": [], "entities": [{"text": "TREC Web Track 2012-14", "start_pos": 32, "end_pos": 54, "type": "DATASET", "confidence": 0.9594198614358902}, {"text": "MatchPyramid", "start_pos": 186, "end_pos": 198, "type": "METRIC", "confidence": 0.9453588724136353}]}]}