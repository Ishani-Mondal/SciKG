{"title": [{"text": "Exploring Vector Spaces for Semantic Relations", "labels": [], "entities": [{"text": "Semantic Relations", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7422143816947937}]}], "abstractContent": [{"text": "Word embeddings are used with success fora variety of tasks involving lexical semantic similarities between individual words.", "labels": [], "entities": []}, {"text": "Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities.", "labels": [], "entities": []}, {"text": "In this paper , we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment.", "labels": [], "entities": []}, {"text": "We propose anew relational similarity measure based on the combination of word2vec's CBOW input and output vectors which outperforms alternative vector representations, when used for unsuper-vised clustering on SemEval 2010 Relation Classification data.", "labels": [], "entities": [{"text": "SemEval 2010 Relation Classification data", "start_pos": 211, "end_pos": 252, "type": "DATASET", "confidence": 0.7813644647598267}]}], "introductionContent": [{"text": "Vector space word representations or word embeddings, both 'count' models and learned vectors (), were proven useful fora variety of semantic tasks).", "labels": [], "entities": [{"text": "Vector space word representations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.659661740064621}]}, {"text": "Word vectors are used with success because they capture a notion of semantics directly extracted from corpora.", "labels": [], "entities": []}, {"text": "Distributional representations allow to compute a functional or topical semantic similarity between two words or, more recently, bigger text units ().", "labels": [], "entities": []}, {"text": "The more similar two entities are semantically, the closer they are in the vector space (quantified usually, but not necessarily in terms of cosine similarity).", "labels": [], "entities": []}, {"text": "Semantic similarity can be exploited for lexical substitution, synonym detection, subcategorization learning etc.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.6976151764392853}, {"text": "synonym detection", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9762770533561707}]}, {"text": "Recent studies suggest that neural word embeddings show higher performance than count models () for most semantic tasks, although argue that this is only due to some specific hyperparameters that can be adapted to count vectors.", "labels": [], "entities": []}, {"text": "In what follows, we will concentrate on exploring whether and how pretrained, general-purpose word embeddings encode relational similarities.", "labels": [], "entities": []}], "datasetContent": [{"text": "For supervised classification tasks, it is desirable to adapt word2vec's hyperparameters to the task and the data at hand ().", "labels": [], "entities": [{"text": "supervised classification tasks", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7575295170148214}]}, {"text": "The interaction between hyperparameters is also to be considered (.", "labels": [], "entities": []}, {"text": "However, our experiment is a clustering scenario aimed at exploratory analysis on a vector space created by pre-trained word embeddings; therefore, we set the parameters once and in advance.", "labels": [], "entities": []}, {"text": "We trained a word2vec CBOW model (Mikolov et al., 2013a) with negative sampling and a window size of 10 words on the ukWaC corpus ( , and extracted both input and output vectors of size = 400 to build the vector combinations above.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 117, "end_pos": 129, "type": "DATASET", "confidence": 0.9859153032302856}]}, {"text": "This size corresponds to the best performing model in the comparative paper by.", "labels": [], "entities": []}, {"text": "An adjacency matrix was constructed for each vector/similarity combination using cosine similarity.", "labels": [], "entities": []}, {"text": "Clustering was implemented with Cluto's () clustering function which takes the adjacency matrix as input.", "labels": [], "entities": []}, {"text": "We used a hierarchical agglomerative clustering with the unweighted average distance (UPGMA) criterion function 2 .  At first, we ran the clustering with 9 clusters (the number of classes in the standard) and tried to make one-to-one correspondences between the standard and the output.", "labels": [], "entities": [{"text": "unweighted average distance (UPGMA) criterion function", "start_pos": 57, "end_pos": 111, "type": "METRIC", "confidence": 0.860467366874218}]}, {"text": "Every cluster is mapped to the standard class that shares the more elements with.", "labels": [], "entities": []}, {"text": "We then calculate precision and recall for each standard class (zero if the class doesn't show up as a majority class in any cluster).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9997197985649109}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9997256398200989}]}, {"text": "Average class-based precision and recall is reported, as well as the number of classes in the standard that could be assigned.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9834476709365845}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996366500854492}]}, {"text": "These scores were published for the SemEval task participants, but ours are not comparable because we only consider one cluster for each class, and because we did the clustering on the training data.", "labels": [], "entities": [{"text": "SemEval task", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.9170005023479462}]}, {"text": "While the scores above can be indicative of the potential of different representations, they do not provide information on other aspects as cluster stability, purity, the amount of post-processing needed.", "labels": [], "entities": [{"text": "purity", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.99481600522995}]}, {"text": "Above all, in a completely unsupervised setting, the number of classes in the standard is not known and cluster quality (precision) plays an important role with respect to interpretability: it is easier to unify two homogeneous clusters than to separate a noisy one.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.966068685054779}]}, {"text": "We ran complementary experiments with different numbers of clusters.", "labels": [], "entities": []}, {"text": "indicates results for 20 and 30 clusters.", "labels": [], "entities": []}, {"text": "The input-output combination method still has an advantage, and concatenation and multiplication also perform well.", "labels": [], "entities": []}, {"text": "However, the advantages over the baseline are less significant than when the number of clusters was identical to the standard.", "labels": [], "entities": []}, {"text": "In the next runs, we measure how stable the different clustering solutions are with settings that are structurally very different from the standard, i.e. have significantly more clusters.", "labels": [], "entities": []}, {"text": "Class-based precision and recall are less relevant measures in this setting, since they take the average over the nine standard classes and not over the produced clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9955002665519714}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9995023012161255}]}, {"text": "We therefore decided to use modified purity, adapted for structurally different clustering solution.", "labels": [], "entities": []}, {"text": "Modified purity gives the proportion of word pairs belonging to the majority Modified purity is indicative of the quality and interpretability of the clusters.", "labels": [], "entities": []}, {"text": "It favorizes small clusters, but singleton clusters were discarded.", "labels": [], "entities": []}, {"text": "This measure corresponds to prediction accuracy in classification if we assign the majority label to clusters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9311885237693787}]}], "tableCaptions": [{"text": " Table 2: Class-based results for 9 clusters", "labels": [], "entities": []}, {"text": " Table 3: Class-based results for 20 and 30 clusters", "labels": [], "entities": []}, {"text": " Table 4: Cluster-based results, 10-50 clusters", "labels": [], "entities": []}, {"text": " Table 5: Cluster-based results, 60-100 clusters", "labels": [], "entities": []}]}