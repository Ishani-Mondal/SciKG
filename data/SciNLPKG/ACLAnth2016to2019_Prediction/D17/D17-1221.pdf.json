{"title": [{"text": "Cascaded Attention based Unsupervised Information Distillation for Compressive Summarization *", "labels": [], "entities": [{"text": "Cascaded Attention", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7158262431621552}, {"text": "Unsupervised Information Distillation", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.6208609839280447}, {"text": "Compressive Summarization", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6132336109876633}]}], "abstractContent": [{"text": "When people recall and digest what they have read for writing summaries, the important content is more likely to attract their attention.", "labels": [], "entities": []}, {"text": "Inspired by this observation , we propose a cascaded attention based unsupervised model to estimate the salience information from the text for compressive multi-document summariza-tion.", "labels": [], "entities": []}, {"text": "The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience.", "labels": [], "entities": []}, {"text": "By adding sparsity constraints on the number of output vectors , we can generate condensed information which can be treated as word salience.", "labels": [], "entities": []}, {"text": "Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7415421903133392}]}, {"text": "Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic 1.", "labels": [], "entities": [{"text": "Multi-Document Summarization (MDS)", "start_pos": 12, "end_pos": 46, "type": "TASK", "confidence": 0.8509333610534668}]}, {"text": "Considering the procedure of summary writing by humans, when people read, they will remember and forget part * The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co.", "labels": [], "entities": [{"text": "remember", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9453370571136475}, {"text": "Huawei Technologies Co", "start_pos": 210, "end_pos": 232, "type": "DATASET", "confidence": 0.9162540833155314}]}, {"text": "Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).", "labels": [], "entities": [{"text": "YB2015100076/TH1510257)", "start_pos": 5, "end_pos": 28, "type": "DATASET", "confidence": 0.7924942672252655}, {"text": "Grant Council of the Hong Kong Special Administrative Region", "start_pos": 37, "end_pos": 97, "type": "DATASET", "confidence": 0.7437300152248807}]}, {"text": "1 A topic represents areal event, e.g., \"AlphaGo versus Lee Sedol\". of the content.", "labels": [], "entities": []}, {"text": "Information which is more important may make a deep impression easily.", "labels": [], "entities": []}, {"text": "When people recall and digest what they have read to write summaries, the important information usually attracts more attention (the behavioral and cognitive process of selectively concentrating on a discrete aspect of information, whether deemed subjective or objective, while ignoring other perceivable information 2 ) since it may repeatedly appears in some documents, or be positioned in the beginning paragraphs.", "labels": [], "entities": []}, {"text": "In the context of multi-document summarization, to generate a summary sentence fora key aspect of the topic, we need to find its relevant parts in the original documents, which may attract more attention.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6426148116588593}]}, {"text": "The semantic parts with high attention weights plausibly represent and reconstruct the topic's main idea.", "labels": [], "entities": []}, {"text": "To this end, we propose a cascaded neural attention model to distill salient information from the input documents in an unsupervised data reconstruction manner, which includes two components: reader and recaller.", "labels": [], "entities": [{"text": "recaller", "start_pos": 203, "end_pos": 211, "type": "METRIC", "confidence": 0.9867475032806396}]}, {"text": "The reader is a gated recurrent neural network (LSTM or GRU) based sentence sequence encoder which can map all the sentences of the topic into a global representation, with the mechanism of remembering and forgetting.", "labels": [], "entities": []}, {"text": "The recaller decodes the global representation into significantly fewer diversified vectors for distillation and concentration.", "labels": [], "entities": [{"text": "recaller", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.7146077156066895}]}, {"text": "A cascaded attention mechanism is designed by incorporating attentions on both the hidden layer (dense distributed representation of a sentence) and the output layer (sparse bag-of-words representation of summary information).", "labels": [], "entities": []}, {"text": "It is worth noting that the output vectors of the recaller can be viewed as word salience, and the attention matrix can be used as sentence salience.", "labels": [], "entities": []}, {"text": "Both of them are automatically learned by data reconstruction in an un-supervised manner.", "labels": [], "entities": [{"text": "data reconstruction", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7414341270923615}]}, {"text": "Thereafter, the word salience is fed into a coarse-grained sentence compression component.", "labels": [], "entities": []}, {"text": "Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation.", "labels": [], "entities": [{"text": "compressive summary generation", "start_pos": 93, "end_pos": 123, "type": "TASK", "confidence": 0.6607774794101715}]}, {"text": "In fact, the notion of \"attention\" has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.8176940083503723}]}, {"text": "However, very few previous works employ attention mechanism to tackle MDS. and employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization.,, and also utilized seq2seq based framework with attention modeling for short text or single document summarization.", "labels": [], "entities": [{"text": "sentence summarization.", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.6561670005321503}, {"text": "single document summarization", "start_pos": 263, "end_pos": 292, "type": "TASK", "confidence": 0.7027379274368286}]}, {"text": "Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: We propose a cascaded attention model that captures salient information in different semantic representations.", "labels": [], "entities": []}, {"text": "(2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience.", "labels": [], "entities": []}, {"text": "By adding sparsity constraints on the number of output vectors of the recaller, we can generate condensed vectors which can be treated as word salience; (3) We thoroughly investigate the performance of combining different attention architectures and cascaded structures.", "labels": [], "entities": []}, {"text": "Experimental results on some benchmark data sets show that our framework achieves better performance than the state-of-the-art models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The linguistic quality of summaries generated by ABS-Phrase, PKUTM, and our model from 20 topics of TAC 2011 is evaluated using the five linguistic quality questions on grammaticality (Q1), non-redundancy (Q2), referential clarity (Q3), focus (Q4), and coherence (Q5) in Document Understanding Conferences (DUC).", "labels": [], "entities": [{"text": "ABS-Phrase", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9486376047134399}, {"text": "PKUTM", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9278996586799622}, {"text": "TAC 2011", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.8142732679843903}]}, {"text": "A Likert scale with five levels is employed with 5 being very good with 1 being very poor.", "labels": [], "entities": [{"text": "Likert scale", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.8509779274463654}]}, {"text": "A summary was blindly evaluated by three assessors on each question.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "PKUTM is an extractive method that picks the original sentences, hence it achieves higher score in Q1 grammaticality.", "labels": [], "entities": [{"text": "PKUTM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8576407432556152}]}, {"text": "ABS-Phrase is an abstractive method and can generate new sentences by merging differ-  Grammaticality of our compression-based framework is better than ABS-Phrase, but not as good as PKUTM.", "labels": [], "entities": [{"text": "ABS-Phrase", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8934182524681091}, {"text": "PKUTM", "start_pos": 183, "end_pos": 188, "type": "DATASET", "confidence": 0.9021158218383789}]}, {"text": "However, our framework performs the best on some other metrics such as Q2 (nonredundancy) and Q4 (focus).", "labels": [], "entities": []}, {"text": "The reason is that our framework can compress and remove some uncritical and redundancy content from the original sentences, which leads to better performance on Q2 and Q4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparisons on TAC 2010", "labels": [], "entities": [{"text": "TAC 2010", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.7225573062896729}]}, {"text": " Table 2: Results on DUC 2006.", "labels": [], "entities": [{"text": "DUC 2006", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9032610952854156}]}, {"text": " Table 3: Results on DUC 2007.", "labels": [], "entities": [{"text": "DUC 2007", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9098769128322601}]}, {"text": " Table 4: Results on TAC 2011.", "labels": [], "entities": [{"text": "TAC 2011", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.6417762637138367}]}, {"text": " Table 5: Evaluation of linguistic quality.", "labels": [], "entities": []}, {"text": " Table 6: Top-10 terms extracted from each topic  according to the word salience", "labels": [], "entities": []}]}