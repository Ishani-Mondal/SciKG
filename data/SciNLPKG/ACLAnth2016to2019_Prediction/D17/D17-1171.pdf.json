{"title": [{"text": "CRF Autoencoder for Unsupervised Dependency Parsing *", "labels": [], "entities": [{"text": "CRF Autoencoder", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7708095610141754}]}], "abstractContent": [{"text": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task.", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6059742569923401}]}, {"text": "Almost all previous work on this task focuses on learning gen-erative models.", "labels": [], "entities": []}, {"text": "In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6844294369220734}, {"text": "CRF autoencoder", "start_pos": 80, "end_pos": 95, "type": "DATASET", "confidence": 0.8308703601360321}]}, {"text": "The encoder part of our model is discrim-inative and globally normalized which allows us to use rich features as well as universal linguistic priors.", "labels": [], "entities": []}, {"text": "We propose an exact algorithm for parsing as well as a tractable learning algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9759075045585632}]}, {"text": "We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised dependency parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing.", "labels": [], "entities": [{"text": "Unsupervised dependency parsing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6580604712168375}, {"text": "natural language processing", "start_pos": 141, "end_pos": 168, "type": "TASK", "confidence": 0.6413843731085459}]}, {"text": "Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7214244306087494}]}, {"text": "Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors, representing dependencies with features, and representing discrete tokens with continuous vectors (.", "labels": [], "entities": []}, {"text": "Besides generative approaches, proposed an unsupervised discrim- * This work was supported by the National Natural Science Foundation of China (61503248).", "labels": [], "entities": [{"text": "generative", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.9739746451377869}, {"text": "National Natural Science Foundation of China (61503248)", "start_pos": 98, "end_pos": 153, "type": "DATASET", "confidence": 0.7310448123349084}]}, {"text": "They designed a convex quadratic objective function under the discriminative clustering framework.", "labels": [], "entities": []}, {"text": "By utilizing global features and linguistic priors, their approach achieves stateof-the-art performance.", "labels": [], "entities": []}, {"text": "However, their approach uses an approximate parsing algorithm, which has no theoretical guarantee.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.8208596706390381}]}, {"text": "In addition, the performance of the approach depends on a set of manually specified linguistic priors.", "labels": [], "entities": []}, {"text": "Conditional random field autoencoder () is anew framework for unsupervised structured prediction.", "labels": [], "entities": [{"text": "unsupervised structured prediction", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7029732664426168}]}, {"text": "There are two components of this model: an encoder and a decoder.", "labels": [], "entities": []}, {"text": "The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input.", "labels": [], "entities": []}, {"text": "The decoder of the model is a generative model generating a transformation of the structured input from the latent structure.", "labels": [], "entities": []}, {"text": "applied the model to two sequential structured prediction tasks, part-of-speech induction and word alignment and showed that by utilizing context information the model can achieve better performance than previous generative models and locally normalized models.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.7576532959938049}, {"text": "word alignment", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7950651347637177}]}, {"text": "However, to the best of our knowledge, there is no previous work applying the CRF autoencoder to tasks with more complicated outputs such as tree structures.", "labels": [], "entities": []}, {"text": "In this paper, we propose an unsupervised discriminative dependency parser based on the CRF autoencoder framework and provide tractable algorithms for learning and parsing.", "labels": [], "entities": []}, {"text": "We performed experiments in eight languages and show that our approach achieves comparable results with previous state-of-the-art models.", "labels": [], "entities": []}, {"text": "Encoder Decoder: The CRF Autoencoder for the input sentence \"These stocks eventually reopened\" and its corresponding parse tree (shown at the top).", "labels": [], "entities": [{"text": "CRF Autoencoder", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8231159448623657}]}, {"text": "x and\u02c6xand\u02c6 and\u02c6x are the original and reconstructed sentence.", "labels": [], "entities": []}, {"text": "y is the dependency parse tree represented by a sequence where y i contains the token and index of the parent of token xi in the parse tree, e.g., y 1 = stocks, 2 and y 2 = reopened, 4.", "labels": [], "entities": []}, {"text": "The encoder is represented by a factor graph (with a global factor specifying valid parse trees) and the decoder is represented by a Bayesian net.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Parsing accuracy on seven languages. Our model is compared with DMV (Klein and Manning,  2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.948477566242218}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9326168894767761}]}, {"text": " Table 3: Comparison of recent unsupervised de- pendency parsing systems on English. Basic setup  is the same as our setup except that linguistic prior  is not used. Extra info includes lexicalization,  longer training sentences, etc.", "labels": [], "entities": []}]}