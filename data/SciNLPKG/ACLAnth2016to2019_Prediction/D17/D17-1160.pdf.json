{"title": [{"text": "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7258696953455607}]}], "abstractContent": [{"text": "We present anew semantic parsing model for answering compositional questions on semi-structured Wikipedia tables.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7536595463752747}, {"text": "answering compositional questions", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8277030189832052}]}, {"text": "Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables.", "labels": [], "entities": []}, {"text": "We also introduce a novel method for training our neural model with question-answer supervision.", "labels": [], "entities": []}, {"text": "On the WIKITABLEQUESTIONS data set, our parser achieves a state-of-the-art accuracy of 43.3% fora single model and 45.9% fora 5-model ensemble, improving on the best prior score of 38.7% set by a 15-model ensemble.", "labels": [], "entities": [{"text": "WIKITABLEQUESTIONS data set", "start_pos": 7, "end_pos": 34, "type": "DATASET", "confidence": 0.9755011002222697}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9987622499465942}]}, {"text": "These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.", "labels": [], "entities": [{"text": "neural semantic parsers", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.6863010823726654}]}], "introductionContent": [{"text": "Semantic parsing is the problem of translating human language into computer language, and therefore is at the heart of natural language understanding.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8328086137771606}, {"text": "natural language understanding", "start_pos": 119, "end_pos": 149, "type": "TASK", "confidence": 0.6560762921969095}]}, {"text": "A typical semantic parsing task is question answering against a database, which is accomplished by translating questions into executable logical forms (i.e., programs) that output their answers.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.7424690425395966}, {"text": "question answering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7798244655132294}]}, {"text": "Recent work has shown that recurrent neural networks can be used for semantic parsing by encoding the question then predicting each token of the logical form in sequence (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.8150339722633362}]}, {"text": "These approaches, while effective, have two major limitations.", "labels": [], "entities": []}, {"text": "First, they treat the logical form as an unstructured sequence, thereby ignoring type constraints on wellformed programs.", "labels": [], "entities": []}, {"text": "Second, they do not address entity linking, which is a critical subproblem of semantic parsing.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7175625562667847}, {"text": "semantic parsing", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.7237276434898376}]}, {"text": "This paper introduces a novel neural semantic parsing model that addresses these limitations of prior work.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6793744663397471}]}, {"text": "Our parser uses an encoder-decoder architecture with two key innovations.", "labels": [], "entities": []}, {"text": "First, the decoder generates from a grammar that guarantees that generated logical forms are well-typed.", "labels": [], "entities": []}, {"text": "This grammar is automatically induced from typed logical forms, and does not require any manual engineering to produce.", "labels": [], "entities": []}, {"text": "Second, the encoder incorporates an entity linking and embedding module that enables it to learn to identify which question spans should be linked to entities.", "labels": [], "entities": []}, {"text": "Finally, we also introduce anew approach for training neural semantic parsers from question-answer supervision.", "labels": [], "entities": [{"text": "training neural semantic parsers", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6708081513643265}]}, {"text": "We evaluate our parser on WIKITABLEQUES-TIONS, a challenging data set for question answering against semi-structured Wikipedia tables.", "labels": [], "entities": [{"text": "WIKITABLEQUES-TIONS", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.8053486943244934}, {"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7984153926372528}]}, {"text": "This data set has abroad variety of entities and relations across different tables, along with complex questions that necessitate long logical forms.", "labels": [], "entities": []}, {"text": "On this data set, our parser achieves a question answering accuracy of 43.3% and an ensemble of 5 parsers achieves 45.9%, both of which outperform the previous state-of-the-art of 38.7% set by an ensemble of 15 models.", "labels": [], "entities": [{"text": "question answering", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.6732765287160873}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9200345277786255}]}, {"text": "We further perform several ablation studies that demonstrate the importance of both type constraints and entity linking to achieving high accuracy on this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9960697889328003}]}], "datasetContent": [{"text": "We evaluate our parser on the WIKITABLEQUES-TIONS data set by comparing it to prior work and ablating several components to understand their contributions.", "labels": [], "entities": [{"text": "WIKITABLEQUES-TIONS data set", "start_pos": 30, "end_pos": 58, "type": "DATASET", "confidence": 0.9488093256950378}]}, {"text": "We used the standard train/test splits of WIK-ITABLEQUESTIONS.", "labels": [], "entities": [{"text": "WIK-ITABLEQUESTIONS", "start_pos": 42, "end_pos": 61, "type": "DATASET", "confidence": 0.9445454478263855}]}, {"text": "The training set consists of 14,152 examples and the test set consists of 4,344 examples.", "labels": [], "entities": []}, {"text": "The training set comes divided into 5 cross-validation folds for development using an 80/20 split.", "labels": [], "entities": []}, {"text": "All data sets are constructed so that the development and test tables are not present in the training set.", "labels": [], "entities": []}, {"text": "We report question answering accuracy measured using the official evaluation script, which performs some simple normalization of numbers, dates, and strings before comparing predictions and answers.", "labels": [], "entities": [{"text": "question answering", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7659008800983429}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9304594397544861}]}, {"text": "When generating answers from a model's predictions, we skip logical forms that do not execute (which may occur for some baseline models) or answer with the empty string (which is never correct).", "labels": [], "entities": []}, {"text": "All reported accuracy numbers are an average of 5 parsers, each trained on one training fold, using the respective development set to perform early stopping.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9991955161094666}, {"text": "early stopping", "start_pos": 142, "end_pos": 156, "type": "TASK", "confidence": 0.6646925061941147}]}, {"text": "We trained our parser with 20 epochs of stochastic gradient descent.", "labels": [], "entities": []}, {"text": "We used 200-dimensional word embeddings for the question and entity tokens, mapping all tokens that occurred < 3 times in the training questions to UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.937063992023468}]}, {"text": "(We tried using a larger vocabulary that included frequent tokens in tables, but this caused the parser to seriously overfit.)", "labels": [], "entities": []}, {"text": "The hidden and output dimensions of the forward/backward encoder LSTMs were set to 100, such that the concatenated representations were also 200-dimensional.", "labels": [], "entities": []}, {"text": "The decoder LSTM uses 100-dimensional action embeddings and has a 200-dimensional hidden state and output.", "labels": [], "entities": []}, {"text": "The action selection MLP has a hidden layer dimension of 100.", "labels": [], "entities": [{"text": "action selection MLP", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.441490372021993}]}, {"text": "We used a dropout probability of 0.5 on the output of both the encoder and decoder LSTMs, as well as on the hidden layer of the action selection MLP.", "labels": [], "entities": []}, {"text": "All parameters are initialized using Glorot initialization).", "labels": [], "entities": []}, {"text": "The learning rate for SGD is initialized to 0.1 with a decay of 0.01.", "labels": [], "entities": [{"text": "SGD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9543496966362}]}, {"text": "At test time, we decode with abeam size of 10.", "labels": [], "entities": []}, {"text": "Our model is implemented as a probabilistic neural program).", "labels": [], "entities": []}, {"text": "This Scala library combines ideas from dynamic neural network frameworks (  and probabilistic programming) to simplify the implementation of complex neural structured prediction models.", "labels": [], "entities": []}, {"text": "This library enables a user to specify the structure of the model in terms of discrete nondeterministic choices -as in probabilistic programming -where a neural network is used to score each choice.", "labels": [], "entities": []}, {"text": "We implement our parser by defining P (|q, T ; \u03b8), from which the library automatically implements both inference and training.", "labels": [], "entities": []}, {"text": "In particular, the beam search and the corresponding backpropagation bookkeeping to implement the objective in Section 3.4 are both automatically handled by the library.", "labels": [], "entities": [{"text": "beam search", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.7806179821491241}]}, {"text": "Code and supplementary material for this paper are available at: http://allenai.org/paper-appendix/emnlp2017-wt/ compares the accuracy of our semantic parser to prior work on WIKITABLEQUESTIONS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9990549683570862}, {"text": "WIKITABLEQUESTIONS", "start_pos": 175, "end_pos": 193, "type": "DATASET", "confidence": 0.864744246006012}]}, {"text": "We distinguish between single models and ensembles, as we expect ensembling to improve accuracy, but not all prior work has used it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9988666772842407}]}, {"text": "Prior work on this data set includes a loglinear semantic parser, that same parser with a neural, paraphrase-based reranker, and a neural programmer that answers questions by predicting a sequence of table operations.", "labels": [], "entities": []}, {"text": "We find that our parser outperforms the best prior result on this data set by 4.6%, despite that prior result using a 15-model ensemble.", "labels": [], "entities": []}, {"text": "An ensemble of 5 parsers improves accuracy by an additional 2.6% fora total improvement of 7.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9996159076690674}]}, {"text": "This ensemble was constructed by averaging the logical form probabilities of parsers trained on each of the 5 crossvalidation folds.", "labels": [], "entities": []}, {"text": "Note that this ensemble is trained on the entire training set -the development data from one fold is training data for the others -so we therefore cannot report its development accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9683892726898193}]}, {"text": "We investigate the sources of this accuracy improvement in the remainder of this section via ablation experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994890689849854}, {"text": "ablation", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9492935538291931}]}], "tableCaptions": [{"text": " Table 1: Development and test set accuracy of our  semantic parser compared to prior work on WIK-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9808830618858337}, {"text": "WIK", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9128222465515137}]}]}