{"title": [{"text": "Assessing Objective Recommendation Quality through Political Forecasting", "labels": [], "entities": [{"text": "Assessing Objective Recommendation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8518939216931661}]}], "abstractContent": [{"text": "Recommendations are often rated for their subjective quality, but few researchers have studied quality in terms of objective utility.", "labels": [], "entities": []}, {"text": "We explore quality assessment with respect to both subjective (i.e. users' ratings) and objective (i.e., did it in-fluence? did it improve decisions?) met-rics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric.", "labels": [], "entities": []}, {"text": "Using a variety of features, we predict all types of quality with better accuracy than the simple yet strong baseline of recommendation length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9985466599464417}]}, {"text": "For example, more complex sentence constructions, as evi-denced by subordinate conjunctions, are characteristic of recommendations leading to objective improvements in forecasting.", "labels": [], "entities": []}, {"text": "Our analyses also reveal rater biases; for example, forecasters are subjectively biased in favor of recommendations mentioning business deals and material things, even though such recommendations do not indeed prove anymore useful objectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finding good recommendations is an integral part of a modern information-seeking life -from purchasing products based on reviews to finding answers to baffling questions.", "labels": [], "entities": []}, {"text": "Following the tradition of sentiment analysis, many have proposed methods to automatically assess the quality of recommendations or comments based on subjective ratings of their usefulness () or of persuasiveness (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9517046511173248}]}, {"text": "However, information thought to be useful does not always prove so, and subjective ratings maybe driven by biases.", "labels": [], "entities": []}, {"text": "Reviews which convince you to watch a movie or buy a product do not guarantee that you will enjoy the product, and the most convincing or highest rated answers to questions on sites like Stack Overflow or Yahoo Answers are not always the most accurate.", "labels": [], "entities": []}, {"text": "We explore recommendations in a unique dataset, an online forecasting competition, which offers a rare glimpse into both subjective and objective quality.", "labels": [], "entities": []}, {"text": "In this competition, the users (forecasters) had a measurable goal -to forecast the outcomes of geopolitical events -and a need to effectively gather information in order to reach this goal.", "labels": [], "entities": []}, {"text": "They viewed recommendations (or \"tips\") from other forecasters, rated them, and potentially updated their own forecast.", "labels": [], "entities": []}, {"text": "This data not only allows us to access what information the forecasters thought useful based on their ratings, but also what was objectively useful based on (a) the rate at which forecasters change their prediction after viewing tips and, (b) the average improvement (or decrease) in their prediction accuracy after this change.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9259880185127258}]}, {"text": "We seek to tease out subjective biases by distinguishing the linguistic characteristics of recommendations with high subjective ratings from those of objective utility.", "labels": [], "entities": []}, {"text": "Since objectively good recommendations tend to get higher subjective assessments, detecting such differences is nontrivial.", "labels": [], "entities": []}, {"text": "Past literature has suggested that subjective quality is well predicted by comment length (; We seek differences beyond this.", "labels": [], "entities": []}, {"text": "We build quality predictive models from linguistic features of recommendations -1 to 3 word sequences, parts-of-speech, and mentions of concepts from a taxonomy -comparing to surface-level features (length and readability).", "labels": [], "entities": []}, {"text": "We then explore the language which distinguishes high quality comments from low quality, controlling for comment length, and ultimately what distinguishes high subjective quality from objective quality in order to reveal subjective biases.", "labels": [], "entities": []}, {"text": "We seethe key contribution of this paper, perhaps non-conventional for NLP, as presenting an evidence-based suggestion for the field to consider metrics of objective quality beyond that of subjective ratings.", "labels": [], "entities": []}, {"text": "To the best of our knowledge this represents the first study of objective comment quality using randomized experimental data.", "labels": [], "entities": []}, {"text": "Specific novel contributions include (a) the development of automated assessments fit to objective outcomes, (b) the identification of linguistic features distinguishing highfrom low-quality comments, (c) the use of anew, important, real-world domain for NLP -geopolitical information, and (d), most consequentially, the identification of subjective biases manifested in the comments' text itself.", "labels": [], "entities": [{"text": "identification of linguistic features distinguishing highfrom low-quality comments", "start_pos": 117, "end_pos": 199, "type": "TASK", "confidence": 0.7798401638865471}, {"text": "identification of subjective biases manifested in the comments' text", "start_pos": 321, "end_pos": 389, "type": "TASK", "confidence": 0.7035667035314772}]}], "datasetContent": [{"text": "To evaluate our models out-of-sample, we use 10-fold cross-validation over the subjective ratings (rating) and update rates (influence).", "labels": [], "entities": [{"text": "update rates (influence)", "start_pos": 111, "end_pos": 135, "type": "METRIC", "confidence": 0.8999806761741638}]}, {"text": "In this process, a random selection of 1/10th of the comments are held-out as a test set, while the other 9/10ths are used to train (estimate) the model.", "labels": [], "entities": []}, {"text": "This model is then used to predict the quality of the rating influence benefit baseline .59 .24 .02 our model .76* .37 .21*: Predictive accuracy (out-of-sample Pearson correlation coefficient) of our content-based models across the subjective and objective measures.", "labels": [], "entities": [{"text": "Predictive", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9241733551025391}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8530257940292358}, {"text": "Pearson correlation coefficient", "start_pos": 160, "end_pos": 191, "type": "METRIC", "confidence": 0.9366406599680582}]}, {"text": "baseline: square-root number of words; our model: based on ngrams, parts-of-speech, and concepts.", "labels": [], "entities": []}, {"text": "*significant reduction in error over the baseline at p < .001.", "labels": [], "entities": [{"text": "error", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9796112179756165}]}, {"text": "comments in the 1/10th sample and compared to the true quality for those comments (using Pearson correlation in this case).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 89, "end_pos": 108, "type": "METRIC", "confidence": 0.9680052697658539}]}, {"text": "However, many of our scores for change in forecaster accuracy (benefit) are based simply on one change and thus quite unreliable.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9297393560409546}]}, {"text": "While it is best to include such noisy data when training, it does not provide a very accurate assessment.", "labels": [], "entities": []}, {"text": "Therefore, we use dedicated training and test sets, where the test set is a random sample of 500 comments with more than 3 updates and thus a more reliable mean change in forecaster accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9004850387573242}]}, {"text": "As a baseline, we use the square-root of the number of words in the comment.", "labels": [], "entities": []}, {"text": "This may seem like weak measure of quality, but the history of automatic quality assessment is saturated with findings that length is the best predictor of quality.", "labels": [], "entities": [{"text": "length", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.983680248260498}]}, {"text": "This holds true for both answers to questions (; as well as e-commerce reviews.", "labels": [], "entities": []}, {"text": "Of course, length is not as shallow as it may seem at first; given no strong incentive for authors to leave long comments, length is likely a proxy for thoroughness of the comment.", "labels": [], "entities": [{"text": "length", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9836575388908386}, {"text": "length", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9735507965087891}]}, {"text": "Still, because we would like to understand the content distinguishing various metrics of quality, we view length as a baseline to move beyond.", "labels": [], "entities": []}, {"text": "compares the accuracy of models built on content (ngrams, parts-of-speech, and concepts) to the baseline of length.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9992579817771912}]}, {"text": "In all cases, our models, based on content, perform significantly better than those based only on length.", "labels": [], "entities": []}, {"text": "Further in the case of benefit (change in forecaster accuracy), length has virtually no predictive power.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9518246650695801}, {"text": "length", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9961129426956177}]}, {"text": "influence .08 .24: Predictive accuracy (out-of-sample Pearson correlation coefficient) of the baseline of length and the baseline of readability and their combination across the subjective and objective measures.", "labels": [], "entities": [{"text": "Predictive", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.993018388748169}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9152224063873291}, {"text": "Pearson correlation coefficient)", "start_pos": 54, "end_pos": 86, "type": "METRIC", "confidence": 0.9336020797491074}]}, {"text": "length: square-root number of words; readability: Flesch-Kincaid Scale.", "labels": [], "entities": [{"text": "length", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.946631133556366}]}], "tableCaptions": []}