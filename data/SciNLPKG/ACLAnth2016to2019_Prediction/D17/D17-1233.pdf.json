{"title": [{"text": "Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "The study on human-computer conversation systems is a hot research topic nowadays.", "labels": [], "entities": []}, {"text": "One of the prevailing methods to build the system is using the gen-erative Sequence-to-Sequence (Seq2Seq) model through neural networks.", "labels": [], "entities": []}, {"text": "However , the standard Seq2Seq model is prone to generate trivial responses.", "labels": [], "entities": []}, {"text": "In this paper , we aim to generate a more meaningful and informative reply when answering a given question.", "labels": [], "entities": []}, {"text": "We propose an implicit content-introducing method which incorporates additional information into the Se-q2Seq model in a flexible way.", "labels": [], "entities": []}, {"text": "Specifically , we fuse the general decoding and the auxiliary cue word information through our proposed hierarchical gated fusion unit.", "labels": [], "entities": []}, {"text": "Experiments on real-life data demonstrate that our model consistently outper-forms a set of competitive baselines in terms of BLEU scores and human evaluation .", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 126, "end_pos": 137, "type": "METRIC", "confidence": 0.9779012501239777}]}], "introductionContent": [{"text": "To establish a conversation system with adequate artificial intelligence is a long-cherished goal for researchers and practitioners.", "labels": [], "entities": []}, {"text": "In particular, automatic conversation systems in open domains are attracting increasing attention due to its wide applications, such as virtual assistants and chatbots.", "labels": [], "entities": []}, {"text": "In open domains, researchers mainly focus on data-driven approaches, since the diversity and uncertainty make it impossible to prepare the interaction logic and domain knowledge.", "labels": [], "entities": []}, {"text": "Basically, there are two mainstream ways to build an opendomain conversation system: 1) to search preestablished database for candidate responses by * Corresponding author: ruiyan@pku.edu.cn query retrieval (;, and 2) to generate anew, tailored utterance given the userissued query.", "labels": [], "entities": []}, {"text": "In these studies, generation-based conversation systems have shown impressive potential.", "labels": [], "entities": []}, {"text": "Especially, the Sequence-to-Sequence (Seq2Seq) model) based on neural networks has been extensively used in practice; the idea is to encode a query as a vector and to decode the vector into a reply.", "labels": [], "entities": []}, {"text": "Inspired by ( , we mainly focus on the generative short-text conversation without context information.", "labels": [], "entities": [{"text": "generative short-text conversation", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.8945243159929911}]}, {"text": "Despite this, the performance of Seq2Seq generation-based conversation systems is far from satisfactory because its generation process is not controllable; it responses to a query according to the pattern learned from the training corpus.", "labels": [], "entities": [{"text": "Seq2Seq generation-based conversation", "start_pos": 33, "end_pos": 70, "type": "TASK", "confidence": 0.8216805855433146}]}, {"text": "As a result, the system is likely to generate an unexpected reply even with little semantics, e.g, \"I don't know\" and \"Okay\" due to the high frequency of these patterns in training data (.", "labels": [], "entities": []}, {"text": "To address this issue, proposed to increase diversity in the Seq2Seq model so that more informative utterances have a chance to standout.", "labels": [], "entities": []}, {"text": "provided a content-introducing approach that generates a reply based on a predicted word.", "labels": [], "entities": []}, {"text": "The word is usually enlightening and drives the generated response to be more meaningful.", "labels": [], "entities": []}, {"text": "However, this method is to some extent rigid; it requires the predicted word to explicitly occur in the generated utterance.", "labels": [], "entities": []}, {"text": "As shown in, sometimes, it is better to generate a semantic related sentence based on the cue word rather than including it in the reply directly.", "labels": [], "entities": []}, {"text": "As for such content-introducing method, there are two aspects that need to betaken into consid-  In this paper, we present an implicit contentintroducing method for generative conversation systems, which incorporates cue words using our proposed hierarchical gated fusion unit (HGFU) in a flexible way.", "labels": [], "entities": [{"text": "generative conversation", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.912960559129715}]}, {"text": "Our main contributions are as follows: \u2022 We propose the cue word GRU, another neural cell, to deal with the auxiliary information.", "labels": [], "entities": []}, {"text": "Compared with other gating methods, our cue word GRU is more flexible.", "labels": [], "entities": [{"text": "GRU", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.807157576084137}]}, {"text": "\u2022 We focus on the implicit content-introducing method during generation: the information of the cue word will be fused into the generation process but not necessarily occur explicitly.", "labels": [], "entities": []}, {"text": "In this way, we change the \"hard\" content-introducing method into anew \"soft\" schema.", "labels": [], "entities": []}, {"text": "The rest of paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by introducing the technical background.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our proposed method.", "labels": [], "entities": []}, {"text": "In Section 4, we illustrate the experimental setup and evaluations against a variety of baselines.", "labels": [], "entities": []}, {"text": "Section 5 briefly reviews related work.", "labels": [], "entities": []}, {"text": "Finally, we conclude our paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare our method with thestate-of-art response generation models based on a huge conversation resource.", "labels": [], "entities": []}, {"text": "The objectives of our experiments are to 1) evaluate the effectiveness of our proposed HGFU model, and 2) explore how cue words affect the process of reply generation.", "labels": [], "entities": [{"text": "reply generation", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.8669406473636627}]}, {"text": "We evaluated our model on a massive Chinese dataset of human conversation crawled from the Baidu Tieba 1 forum.", "labels": [], "entities": [{"text": "Chinese dataset of human conversation crawled from the Baidu Tieba 1 forum", "start_pos": 36, "end_pos": 110, "type": "DATASET", "confidence": 0.9010211502512296}]}, {"text": "There are 500,000 query \u2212 reply pairs for training, 2,000 for validation, and another unseen 27,871 samples for testing.", "labels": [], "entities": []}, {"text": "In total, we kept about 63,000 distinct words.", "labels": [], "entities": []}, {"text": "In our experiments, the encoder, the standard decoder and the cue word decoder have 1,000 hidden units; the word embedding dimensionality is 610 which were initialized randomly and learned during training.", "labels": [], "entities": []}, {"text": "We applied AdaDelta with a minibatch size of 80 for optimization.", "labels": [], "entities": []}, {"text": "These values were mostly chosen empirically.", "labels": [], "entities": []}, {"text": "In order to prevent overfitting, early stopping was implemented using a held-out validation set.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.6385928988456726}]}, {"text": "To evaluate the performance of different methods for the conversation generation task, we leverage BLEU () as the automatic evaluation metric, which is originally designed for machine translation and evaluates the output by using n-gram matching between the output and the reference.", "labels": [], "entities": [{"text": "conversation generation task", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8290658394495646}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9985471367835999}, {"text": "machine translation", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.7755870521068573}]}, {"text": "Here, we use BLEU-1, BLEU-2 and BLEU-3 in our experiments.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9969809651374817}, {"text": "BLEU-2", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9970576763153076}, {"text": "BLEU-3", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9967302083969116}]}, {"text": "Since automatic metrics may not consistently agree with human perception (), human testing is essential to assess subjective quality.", "labels": [], "entities": []}, {"text": "Hence, we randomly sampled 150 queries in the test set, then we invited five annotators to offer a judgment.", "labels": [], "entities": []}, {"text": "For fairness, all of our human evaluation was conducted in a random, blind fashion, i.e., replies obtained from the five evaluated models are pooled and randomly permuted for each annotator.", "labels": [], "entities": []}, {"text": "Three levels are assigned to a reply with scores from 0 to 2: 0 =  Unsuitable reply, 2 = Suitable reply, and 1 = Neutral reply.", "labels": [], "entities": []}, {"text": "To make the annotation task operable, the suitability of the generated reply is judged not only based on Grammar and Fluency, Logic Consistency and Semantic Relevance following), but also Implicit Relevance, i.e., the generated reply should be semantically relevant to the predicted cue word, no matter the cue word explicitly appears in the reply or not.", "labels": [], "entities": [{"text": "Implicit Relevance", "start_pos": 188, "end_pos": 206, "type": "METRIC", "confidence": 0.9607077538967133}]}, {"text": "If any of the first three criteria is contradicted, the reply should be labeled as \"Unsuitable\".", "labels": [], "entities": [{"text": "Unsuitable", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9671760201454163}]}, {"text": "Only the replies conforming to all requirements are labeled as \"Suitable\".", "labels": [], "entities": []}, {"text": "shows an example of the annotation results of a query and its replies.", "labels": [], "entities": []}, {"text": "The first reply is labeled as \"Unsuitable\" because of the logic consistency.", "labels": [], "entities": []}, {"text": "Reply2 and Reply3 are not semantically related to the cue word, and is therefore annotated as \"Neutral\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: An example query, corresponding cue word in bold and its candidate replies with human anno- tation. The query states that people laughed at the author's photo, it is unsuitable to ask the ownership  of this photo in Reply1. Generally, Reply2 and Reply3 apply to this scenario, but they do not reflec- t semantic relevance with the cue word. Reply4 talks about the respondent's situation and related to  \"Photogenic\", thus it is a suitable response.", "labels": [], "entities": []}, {"text": " Table 4: Performance of evaluated methods.", "labels": [], "entities": []}]}