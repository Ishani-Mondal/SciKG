{"title": [{"text": "A Challenge Set Approach to Evaluating Machine Translation", "labels": [], "entities": [{"text": "Evaluating Machine Translation", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8590044379234314}]}], "abstractContent": [{"text": "Neural machine translation represents an exciting leap forward in translation quality.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.725956122080485}, {"text": "translation quality", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.9086850583553314}]}, {"text": "But what longstanding weaknesses does it resolve, and which remain?", "labels": [], "entities": []}, {"text": "We address these questions with a challenge set approach to translation evaluation and error analysis.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9887886345386505}, {"text": "error analysis", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.6727807372808456}]}, {"text": "A challenge set consists of a small set of sentences, each hand-designed to probe a system's capacity to bridge a particular structural divergence between languages.", "labels": [], "entities": []}, {"text": "To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems.", "labels": [], "entities": []}, {"text": "The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.", "labels": [], "entities": []}], "introductionContent": [{"text": "The advent of neural techniques in machine translation (MT)) has led to profound improvements in MT quality.", "labels": [], "entities": [{"text": "machine translation (MT))", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8618046760559082}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.987059473991394}]}, {"text": "For \"easy\" language pairs such as English/French or English/Spanish in particular, neural (NMT) systems are much closer to human performance than previous statistical techniques (.", "labels": [], "entities": []}, {"text": "This puts pressure on automatic evaluation metrics such as BLEU (), which exploit surface-matching heuristics that are relatively insensitive to subtle differences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9973703622817993}]}, {"text": "As NMT continues to improve, these metrics will inevitably lose their effectiveness.", "labels": [], "entities": [{"text": "NMT", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.7607593536376953}]}, {"text": "Another challenge posed by NMT systems is their opacity: while it was usually clear which phenomena were ill-handled * Work performed while at NRC.", "labels": [], "entities": [{"text": "NRC", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.978715181350708}]}], "datasetContent": [{"text": "Our challenge set is meant to measure the ability of MT systems to deal with some of the more difficult problems that arise in translating English into French.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9863747358322144}]}, {"text": "This particular language pair happened to be most convenient for us, but similar sets can be built for any language pair.", "labels": [], "entities": []}, {"text": "One aspect of MT performance excluded from our evaluation is robustness to sparse data.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9948596954345703}]}, {"text": "To control for this, when crafting source and reference sentences, we chose words that occurred at least 100 times in our training corpus (section 4.1).", "labels": [], "entities": []}, {"text": "The challenging aspect of the test set we are presenting stems from the fact that the source English sentences have been chosen so that their closest French equivalent will be structurally divergent from the source in some crucial way.", "labels": [], "entities": []}, {"text": "Translational divergences have been extensively studied in the past-see for example.", "labels": [], "entities": [{"text": "Translational divergences", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9680132269859314}]}, {"text": "We expect the level of difficulty of an MT test set to correlate well with its density in divergence phenomena, which we classify into three main types: morpho-syntactic, lexico-syntactic and purely syntactic divergences.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9716840982437134}]}, {"text": "Given the very small size of our challenge set, it is easy to perform a human evaluation of the respective outputs of a handful of different systems.", "labels": [], "entities": []}, {"text": "The obvious advantage is that the assessment is then absolute instead of relative to one or a few reference translations.", "labels": [], "entities": []}, {"text": "The intent of each challenge sentence is to test one and only one system capability, namely that of coping correctly with the particular associated divergence subtype.", "labels": [], "entities": []}, {"text": "As illustrated in, we provide annotators with a question that specifies the divergence phenomenon currently being tested, along with a reference translation with the areas of divergence highlighted.", "labels": [], "entities": []}, {"text": "As a result, judgments become straightforward: was the targeted divergence correctly bridged, yes or no?", "labels": [], "entities": []}, {"text": "3 There is no need to mentally average over a number of different aspects of the test sentence as one does when rating the global translation quality of a sentence, e.g. on a 5-point scale.", "labels": [], "entities": []}, {"text": "However, we acknowledge that measuring translation performance on complex sentences exhibiting many different phenomena remains crucial.", "labels": [], "entities": [{"text": "translation", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.9578380584716797}]}, {"text": "We see our approach as being complementary to evaluations of overall translation quality.", "labels": [], "entities": []}, {"text": "One consequence of our divergence-focused approach is that faulty translations will be judged as successes when the faults lie outside of the targeted divergence zone.", "labels": [], "entities": []}, {"text": "However, this problem is mitigated by our use of short test sentences.", "labels": [], "entities": []}, {"text": "The 108-sentence English-French challenge set presented in Appendix 7 was submitted to the four MT systems described in section 4: PBMT-1, PBMT-2, NMT, and GNMT.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9694166779518127}, {"text": "GNMT", "start_pos": 156, "end_pos": 160, "type": "DATASET", "confidence": 0.8833847045898438}]}, {"text": "Three bilingual native speakers of French rated each translated sentence as either a successor a failure according to the protocol described in section 3.4.", "labels": [], "entities": []}, {"text": "For example, the 26 sentences of the subcategories S1-S5 of Appendix 7 are all about different cases of subjectverb agreement.", "labels": [], "entities": []}, {"text": "The corresponding translations were judged successful if and only if the translated verb correctly agrees with the translated subject.", "labels": [], "entities": []}, {"text": "The different system outputs for each source sentence were grouped together to reduce the burden on the annotators.", "labels": [], "entities": []}, {"text": "That is, in, anno-tators were asked to answer the question for each of four outputs, rather than just one as shown.", "labels": [], "entities": []}, {"text": "The outputs were listed in random order, without identification.", "labels": [], "entities": []}, {"text": "Questions were also presented in random order to each annotator.", "labels": [], "entities": []}, {"text": "Appendix A in the supplemental materials contains the instructions shown to the annotators.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9231035113334656}]}, {"text": "As we can see, the two PBMT systems fare very poorly on our challenge set, especially in the morpho-syntactic and purely syntactic types.", "labels": [], "entities": []}, {"text": "Their somewhat better handling of lexicosyntactic issues probably reflects the fact that PBMT systems are naturally more attuned to lexical cues than to morphology or syntax.", "labels": [], "entities": []}, {"text": "The two NMT systems are clear winners in all three categories.", "labels": [], "entities": [{"text": "NMT", "start_pos": 8, "end_pos": 11, "type": "DATASET", "confidence": 0.7901175022125244}]}, {"text": "The GNMT system is best overall with a success rate of 68%, likely due to the data and architectural factors mentioned in section 4.3.", "labels": [], "entities": []}, {"text": "WMT BLEU scores correlate poorly with challenge-set performance.", "labels": [], "entities": [{"text": "WMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5369102358818054}, {"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9665688276290894}]}, {"text": "The large gap of 2.3 BLEU points between PBMT-1 and PBMT-2 corresponds to only a 1% gain on the challenge set, while the small gap of 0.4 BLEU between PBMT-2 and NMT corresponds to a 21% gain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9981873631477356}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9969303011894226}, {"text": "NMT", "start_pos": 162, "end_pos": 165, "type": "DATASET", "confidence": 0.818688690662384}]}], "tableCaptions": [{"text": " Table 1: Corpus statistics. The WMT12/13 eval  sets are used for dev, and the WMT14 eval set is  used for test.", "labels": [], "entities": [{"text": "WMT12/13 eval  sets", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.7992782592773438}, {"text": "WMT14 eval set", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.8439362843831381}]}, {"text": " Table 3: Summary of scores by fine-grained categories. \"#\" reports number of questions in each cat- egory, while the reported score is the percentage of questions for which the divergence was correctly  bridged. For each question, the three human judgments were transformed into a single judgment by  taking system outputs with two positive judgments as positive, and all others as negative.", "labels": [], "entities": []}]}