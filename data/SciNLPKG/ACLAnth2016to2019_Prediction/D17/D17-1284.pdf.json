{"title": [{"text": "Entity Linking via Joint Encoding of Types, Descriptions, and Context", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8455351293087006}]}], "abstractContent": [{"text": "For accurate entity linking, we need to capture various information aspects of an entity , such as its description in a KB, contexts in which it is mentioned, and struc-tured knowledge.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7205588519573212}]}, {"text": "Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features.", "labels": [], "entities": []}, {"text": "In this work we present a neural, modular entity linking system that learns a unified dense representation for each entity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types.", "labels": [], "entities": []}, {"text": "We show that the resulting entity linking system is effective at combining these sources, and performs competitively, sometimes out-performing current state-of-the-art systems across datasets, without requiring any domain-specific training data or hand-engineered features.", "labels": [], "entities": []}, {"text": "We also show that our model can effectively \"embed\" entities that are new to the KB, and is able to link its mentions accurately.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity linking, the task of identifying the real-world entity a mention in text refers to, provides the ability to ground text to existing knowledge bases, and thus supports multiple natural language understanding, and knowledge acquisition tasks.", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.748752236366272}, {"text": "natural language understanding", "start_pos": 183, "end_pos": 213, "type": "TASK", "confidence": 0.6909312804539999}, {"text": "knowledge acquisition", "start_pos": 219, "end_pos": 240, "type": "TASK", "confidence": 0.7655736804008484}]}, {"text": "A key challenge for successful entity linking is the need to capture semantic and background information at various levels of granularity.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7209939062595367}]}, {"text": "For example, to resolve the mention \"India\" in \"India plays a match in England today\" to the correct entity, India cricket team, one needs to use * Work performed while these authors were at UIUC.", "labels": [], "entities": [{"text": "UIUC", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.966187596321106}]}, {"text": "mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string \"India\" may refer to it.", "labels": [], "entities": []}, {"text": "The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (, simple classifiers), graphical models), classifiers augmented with ILP inference (, and more recently, neural approaches (.", "labels": [], "entities": []}, {"text": "We present a neural approach to linking 1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources -encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in -thus capturing different aspects of the \"meaning\" of an entity.", "labels": [], "entities": []}, {"text": "Hence, we overcome the shortcomings of several existing models that do not capture all these aspects.", "labels": [], "entities": []}, {"text": "For example, methods, such as Vinculum (, do not make use of the local context of the mention (\"plays\" and \"match\") while others, such as Berkeley-CNN (Francis-, do not take entity-types into account.", "labels": [], "entities": [{"text": "Vinculum", "start_pos": 30, "end_pos": 38, "type": "DATASET", "confidence": 0.9462701082229614}]}, {"text": "Our proposed model uses compositional training to ensure that the learned entity representation captures the various information sources available to it, making it quite modular.", "labels": [], "entities": []}, {"text": "Specifically, we introduce encoders for the different sources of information about the entity, and encourage the entity embedding to be similar to all of the encoded representations.", "labels": [], "entities": []}, {"text": "A key requirement for information extraction systems is their ability to work across texts from various domains.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7982256710529327}]}, {"text": "Some methods) train parameters on domain-specific linked data, thus hampering their ability to generalize to new domains.", "labels": [], "entities": []}, {"text": "By only making use of indirect supervision that is available in Wikipedia/Freebase, we refrain from using domain specific training data, and produce a domain-independent linking system.", "labels": [], "entities": [{"text": "Wikipedia/Freebase", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.8825095891952515}]}, {"text": "Our comprehensive evaluation on recent entity linking benchmarks reveals that the resulting entity linker compares favorably to state-of-the-art systems across datasets, even those that have handengineered features or use dataset-specific training.", "labels": [], "entities": []}, {"text": "We hence show that our model not only leverages all the available information for each entity effectively, but is also robust to missing information, such as entities without links/description in Wikipedia or with incomplete entity types.", "labels": [], "entities": []}, {"text": "In the real-world, new entities are regularly added to the knowledge bases, thus, it is important for any entity linking system to be extendable to such entities, especially the ones that do not have any existing linked mentions.", "labels": [], "entities": []}, {"text": "By the virtue of our model's modular nature, it can easily incorporate new entities not present during training.", "labels": [], "entities": []}, {"text": "Specifically, we show that our model can perform accurate linking for new entities, without having to re-train the existing entity representations, only using their description and types.", "labels": [], "entities": []}], "datasetContent": [{"text": "Here we provide a detailed description of how we train our models, benchmark datasets, linking systems we compare to, and the evaluation metrics.", "labels": [], "entities": []}, {"text": "Training Data Our primary source of information about the entities is Wikipedia (dump dated 2016/09/20).", "labels": [], "entities": [{"text": "Training Data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8177697956562042}, {"text": "Wikipedia (dump dated 2016/09/20)", "start_pos": 70, "end_pos": 103, "type": "DATASET", "confidence": 0.904069048166275}]}, {"text": "We use existing links in Wikipedia, with the anchors as mentions, and links as the true entity, as input to the context encoder (see \u00a7 3.1).", "labels": [], "entities": []}, {"text": "As the description of each entity ( \u00a7 3.2), we use the first 100 tokens of the entity's Wikipedia page (same as Francis-).", "labels": [], "entities": []}, {"text": "To obtain entity types (see \u00a7 3.3), we extract the types for each entity from Freebase and map them to the 112 fine-grained types introduced by.", "labels": [], "entities": []}, {"text": "For context and description encoders, we use pre-trained 300-dimensional case-sensitive word embeddings by as the first layer that is not updated during training.", "labels": [], "entities": []}, {"text": "Hyper-parameters We perform coarse-grained tuning of the hyper-parameters using a fraction of the training data.", "labels": [], "entities": []}, {"text": "The vectors for the entities, types, contexts, and descriptions are of size d = 200.", "labels": [], "entities": []}, {"text": "The size of the local context encoder LSTM hidden layer l, local context output, and the document-context encoder output D m is set to 100(= l = D m ).", "labels": [], "entities": []}, {"text": "The document context vocabulary contains |V G | = 1.5 million strings.", "labels": [], "entities": []}, {"text": "We use dropout () with a probability of 0.4.", "labels": [], "entities": []}, {"text": "Additionally, we use word-dropout where we replace a random subset of tokens (mentionstrings) in the local (document) context with \"unk\" (rate of 0.4 and 0.6 for local and document context respectively).", "labels": [], "entities": []}, {"text": "We use Adam () for optimization, with learning rate 0.005 and mini-batches of size 1000.", "labels": [], "entities": []}, {"text": "Existing Approaches We compare our approach to the following five entity-linking models: (1) Plato (, an unsupervised generative model that uses indirect-supervision from Wikipedia and an additional corpus of 50 million unlabeled webpages, (2) Wikifier (), an unsupervised linker that uses hand-crafted features to rank candidates, (3) Vinculum (, a modular, unsupervised pipeline system, (4) AIDA), a supervised linker trained on CoNLL data and uses hand-crafted features, and: Entity Linking Performance: Accuracy of existing systems, and variations of our model on gold mentions.", "labels": [], "entities": [{"text": "Vinculum", "start_pos": 336, "end_pos": 344, "type": "DATASET", "confidence": 0.9231890439987183}, {"text": "CoNLL data", "start_pos": 431, "end_pos": 441, "type": "DATASET", "confidence": 0.9391851127147675}]}, {"text": "The model using context information is labeled C, entity-description as D, contexttyping as T, and entity-type encoding as E.", "labels": [], "entities": []}, {"text": "Existing models marked in Italics* train domain-specific linkers for each dataset.", "labels": [], "entities": []}, {"text": "Our system performs competitively to these systems, and outperforms Plato (Sup) that uses the same indirect supervision.", "labels": [], "entities": []}, {"text": "a prediction is only considered correct if the system mention boundaries match the gold annotation, and the predicted link is correct (we compare against these by extracting mentions with Stanford-NER).", "labels": [], "entities": []}, {"text": "On the other hand, systems like Plato, AIDA, and Berkeley-CNN assume mentions are provided, and evaluate using the linking accuracy for goldmentions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.7383033633232117}, {"text": "goldmentions", "start_pos": 136, "end_pos": 148, "type": "METRIC", "confidence": 0.7683259844779968}]}, {"text": "Further, the approaches we compare here (including ours) do not predict NIL entities for the datasets evaluated on.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for ACE-2004: F1 is calcu- lated for predicted mentions, and accuracy on gold- mentions. Results for Wikifier and AIDA are from  (", "labels": [], "entities": [{"text": "ACE-2004", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8216024041175842}, {"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9992671608924866}, {"text": "calcu- lated", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.807715117931366}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995700716972351}, {"text": "AIDA", "start_pos": 132, "end_pos": 136, "type": "DATASET", "confidence": 0.7706367373466492}]}, {"text": " Table 3: Cold-Start Entities: Linking new enti- ties by using different information to learn their  embeddings. Our model is able to jointly utilize  description and type information better.", "labels": [], "entities": []}, {"text": " Table 4: Typing Prediction: Performance on the  FIGER (GOLD) dataset. Our performance is com- petitive with FIGER (Ling", "labels": [], "entities": [{"text": "Typing Prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5330148190259933}, {"text": "FIGER", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9751369953155518}, {"text": "GOLD) dataset", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8113228678703308}, {"text": "FIGER", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.8964409828186035}]}]}