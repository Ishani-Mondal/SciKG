{"title": [], "abstractContent": [{"text": "We present a transition-based AMR parser that directly generates AMR parses from plain text.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.8017243444919586}]}, {"text": "We use Stack-LSTMs to represent our parser state and make decisions greedily.", "labels": [], "entities": []}, {"text": "In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data.", "labels": [], "entities": [{"text": "AMR training data", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.6556184391180674}]}, {"text": "Adding additional information, such as POS tags and dependency trees, improves the results further.", "labels": [], "entities": []}], "introductionContent": [{"text": "Transition-based algorithms for natural language parsing () are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.656732976436615}]}, {"text": "Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (, named entity recognition (, CCG parsing () joint syntactic and semantic parsing ( and even abstract-meaning representation parsing (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.7576189339160919}, {"text": "phrase-structure parsing", "start_pos": 129, "end_pos": 153, "type": "TASK", "confidence": 0.8086118102073669}, {"text": "entity recognition", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.7061278074979782}, {"text": "CCG parsing", "start_pos": 185, "end_pos": 196, "type": "TASK", "confidence": 0.7198078483343124}, {"text": "joint syntactic and semantic parsing", "start_pos": 200, "end_pos": 236, "type": "TASK", "confidence": 0.635930472612381}, {"text": "abstract-meaning representation parsing", "start_pos": 248, "end_pos": 287, "type": "TASK", "confidence": 0.7170891563097636}]}, {"text": "AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9582586288452148}, {"text": "named entity recognition", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.6687086721261343}, {"text": "word sense disambiguation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.7103108763694763}, {"text": "joint syntactic and semantic role labeling", "start_pos": 135, "end_pos": 177, "type": "TASK", "confidence": 0.6242811034123102}]}, {"text": "Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (, inter-alia).", "labels": [], "entities": []}, {"text": "Inspired by;; and , we present a shift-reduce algorithm that produces AMR graphs directly from plain text.;; presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph.'s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as dependency parsing, semantic role labeling or named entity recognition.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 388, "end_pos": 406, "type": "TASK", "confidence": 0.811341255903244}, {"text": "semantic role labeling", "start_pos": 408, "end_pos": 430, "type": "TASK", "confidence": 0.6407544811566671}, {"text": "named entity recognition", "start_pos": 434, "end_pos": 458, "type": "TASK", "confidence": 0.6105666160583496}]}, {"text": "The input of our parser is plain text sentences and, through rich word representations, it predicts all actions (in a single algorithm) needed to generate an AMR graph representation for an input sentence; it handles the detection and annotation of named entities, word sense disambiguation and it makes connections between the nodes detected towards building a predicate argument structure.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 265, "end_pos": 290, "type": "TASK", "confidence": 0.6761514544487}]}, {"text": "Even though the system that runs with just words is very competitive, we further improve the results incorporating POS tags and dependency trees into our model.", "labels": [], "entities": []}, {"text": "Stack-LSTMs 2 have proven to be useful in tasks related to syntactic and semantic parsing and named entity recognition (.", "labels": [], "entities": [{"text": "syntactic and semantic parsing", "start_pos": 59, "end_pos": 89, "type": "TASK", "confidence": 0.614986926317215}, {"text": "named entity recognition", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.6301469107468923}]}, {"text": "In this paper, we demonstrate that they can be effectively used for AMR parsing as well.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.951368123292923}]}], "datasetContent": [{"text": "We use the LDC2014T12 dataset 7 for our experiments.", "labels": [], "entities": [{"text": "LDC2014T12 dataset 7", "start_pos": 11, "end_pos": 31, "type": "DATASET", "confidence": 0.9717273910840353}]}, {"text": "shows results, including comparison with prior work that are also evaluated on the same dataset.", "labels": [], "entities": []}, {"text": "8 This dataset is a standard for comparison and has been used for evaluation in recent papers like (.", "labels": [], "entities": []}, {"text": "We use the standard training/development/test split: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences heldout for testing.", "labels": [], "entities": []}, {"text": "The first entry for Damonte et al. is calculated using a pretrained LDC2015 model, available at https:// github.com/mdtux89/amr-eager, but evaluated on the LDC2014 dataset.", "labels": [], "entities": [{"text": "LDC2014 dataset", "start_pos": 156, "end_pos": 171, "type": "DATASET", "confidence": 0.9853757917881012}]}, {"text": "This means that the score is not directly comparable with the rest.", "labels": [], "entities": []}, {"text": "The second entry (0.64) for Damonte et al. is calculated by training their parser with the LDC2014 training set which makes it directly comparable with the rest of the parsers.", "labels": [], "entities": [{"text": "LDC2014 training set", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.9261730114618937}]}, {"text": "shows results without pretrained word embeddings.", "labels": [], "entities": []}, {"text": "(NO PRETRAINED-NO CHARS) shows results without character-based representations and without pretrained word embeddings.", "labels": [], "entities": [{"text": "NO PRETRAINED-NO CHARS", "start_pos": 1, "end_pos": 23, "type": "METRIC", "confidence": 0.6057746112346649}]}, {"text": "The rest of our results include both pretrained embeddings and character-based representations.", "labels": [], "entities": []}, {"text": "Our model achieves 0.68 F1 in the newswire section of the test set just by using character-based representations of words and pretrained word embeddings.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9991030693054199}]}, {"text": "All prior work uses lemmatizers, POS taggers, dependency parsers, named entity recognizers and semantic role labelers that use additional training data while we achieve competitive scores without that.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.7248969972133636}]}, {"text": "reports 0.66 F1 in the full test by using WordNet for concept identification, but their performance drops to 0.61 without WordNet.", "labels": [], "entities": [{"text": "F1", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.9982971549034119}, {"text": "WordNet", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9788383841514587}, {"text": "concept identification", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7241944521665573}, {"text": "WordNet", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9746062755584717}]}, {"text": "It is worth noting that we achieved 0.64 in the same test set without WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.9851016998291016}]}, {"text": "without SRL (via Propbank) achieves only 0.63 in the newswire test set while we achieved 0.69 without SRL (and 0.68 without dependency trees).", "labels": [], "entities": [{"text": "Propbank", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8502668738365173}, {"text": "newswire test set", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.956506609916687}]}, {"text": "In order to see whether pretrained word embeddings and character-based embeddings are useful we carried out an ablation study by showing the results of our parser with and without character-based representations (replaced by standard lookup table learned embeddings) and with and without pretrained word embeddings.", "labels": [], "entities": []}, {"text": "By looking at the results of the parser without character-based embeddings but with pretrained word embeddings we observe that the characterbased representation of words are useful since they help to achieve 2 points better in the Newswire dataset and 1 point more in the full test set.", "labels": [], "entities": [{"text": "Newswire dataset", "start_pos": 231, "end_pos": 247, "type": "DATASET", "confidence": 0.98831906914711}]}, {"text": "The parser with character-based embeddings but without pretrained word embeddings, the parser has more difficulty to learn and only achieves 0.61 in the full test set.", "labels": [], "entities": []}, {"text": "Finally, the model that does not use neither character-based embeddings nor pretrained word embeddings is the worst achieving only 0.59 in the full test set, note that this model has no explicity way of getting any syntactic information through the word embeddings nor a smart way to handle out of vocabulary words.", "labels": [], "entities": []}, {"text": "All the systems marked with * require that the input is a dependency tree, which means that they solve a transduction task between a dependency tree and an AMR graph.", "labels": [], "entities": []}, {"text": "Even though our parser starts from plain text sentences when we incorporate more information into our model, we achieve further improvements.", "labels": [], "entities": []}, {"text": "POS tags provide small improvements (0.6801 without POS tags vs 0.6822 for the model that runs with POS tags).", "labels": [], "entities": []}, {"text": "Dependency trees help a bit more achieving 0.6920.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: AMR results on the LDC2014T12  dataset; Newsire section (left) and full (right).  Rows labeled with OUR-PARSER show our re- sults. POS indicates that the system uses prepro- cessed POS tags, DEP indicates that it uses pre- processed dependency trees, SRL indicates that it  uses preprocessed semantic roles, NER indicates  that it uses preprocessed named entitites. LM in- dicates that it uses a LM trained on AMR data and  WordNet indicates that it uses WordNet to predict  the concepts. Systems marked with * are pipeline  systems that require a dependency parse as input.  (WITH PRETRAINED-NO CHARS) shows the re- sults of our parser without character-based rep- resentations. (NO PRETRAINED-WITH CHARS)", "labels": [], "entities": [{"text": "LDC2014T12  dataset", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.9140027463436127}, {"text": "OUR-PARSER", "start_pos": 110, "end_pos": 120, "type": "METRIC", "confidence": 0.9927026033401489}, {"text": "WordNet", "start_pos": 434, "end_pos": 441, "type": "DATASET", "confidence": 0.9441615343093872}, {"text": "WordNet", "start_pos": 465, "end_pos": 472, "type": "DATASET", "confidence": 0.9507474303245544}]}]}