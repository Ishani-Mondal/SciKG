{"title": [], "abstractContent": [{"text": "We propose a general class of language models that treat reference as discrete stochastic latent variables.", "labels": [], "entities": []}, {"text": "This decision allows for the creation of entity mentions by accessing external databases of refer-ents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality).", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7304525077342987}]}, {"text": "Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to \"Jane\" can be realized as \"she\"), a characteristic feature of reference in natural languages.", "labels": [], "entities": []}, {"text": "Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment.", "labels": [], "entities": [{"text": "Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment", "start_pos": 0, "end_pos": 169, "type": "Description", "confidence": 0.7860867017880082}]}, {"text": "REs occur frequently and they play a key role in communicating information efficiently.", "labels": [], "entities": [{"text": "REs", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8620521426200867}]}, {"text": "While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step.", "labels": [], "entities": []}, {"text": "Here we propose a language modeling framework that explicitly incorporates reference decisions.", "labels": [], "entities": []}, {"text": "In part, this is based on the principle of pointer networks in which copies are made from another source  2016;.", "labels": [], "entities": []}, {"text": "However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference.", "labels": [], "entities": []}, {"text": "depicts examples of REs in the context of the three tasks that we consider in this work.", "labels": [], "entities": [{"text": "REs", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9291611313819885}]}, {"text": "First, many models need to refer to a list of items (.", "labels": [], "entities": []}, {"text": "In the task of recipe generation from a list of ingredients (, the generation of the recipe will frequently refer to these items.", "labels": [], "entities": [{"text": "recipe generation from a list of ingredients", "start_pos": 15, "end_pos": 59, "type": "TASK", "confidence": 0.8116917184420994}]}, {"text": "As shown in, in the recipe \"Blend soy milk and . .", "labels": [], "entities": [{"text": "Blend", "start_pos": 28, "end_pos": 33, "type": "METRIC", "confidence": 0.984914243221283}]}, {"text": "\", soy milk refers to the ingredient summaries.", "labels": [], "entities": []}, {"text": "Second, reference to a database is crucial in many applications.", "labels": [], "entities": []}, {"text": "One example is in task oriented dialogue where access to a database is necessary to answer a user's query (.", "labels": [], "entities": []}, {"text": "Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses.", "labels": [], "entities": [{"text": "restaurant recommendation", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7332570850849152}]}, {"text": "When the system says \"the nirala is a nice restaurant\", it refers to the restaurant name the nirala from the database.", "labels": [], "entities": []}, {"text": "Finally, we address references within a document (, as the generation of words will often refer to previously generated words.", "labels": [], "entities": []}, {"text": "For instance the same entity will often be referred to throughout a document.", "labels": [], "entities": []}, {"text": "In, the entity you refers to I in a previous utterance.", "labels": [], "entities": []}, {"text": "In this case, copying is insufficient-although the referent is the same, the form of the mention is different.", "labels": [], "entities": [{"text": "copying", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.8501824140548706}]}, {"text": "In this work we develop a language model that has a specific module for generating REs.", "labels": [], "entities": []}, {"text": "A series of decisions (should I generate a RE?", "labels": [], "entities": [{"text": "RE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.8300848007202148}]}, {"text": "If yes, which entity in the context should I refer to?", "labels": [], "entities": []}, {"text": "How should the RE be rendered?) augment a traditional recurrent neural network language model and the two components are combined as a mixture model.", "labels": [], "entities": []}, {"text": "Selecting an entity in context is similar to familiar models of attention (), but rather than being a soft decision that reweights representations of elements in the context, it is treated as a hard decision over contextual elements which are stochastically selected and then copied or, if the task warrants it, transformed (e.g., a pronoun rather than a proper name is produced as output).", "labels": [], "entities": []}, {"text": "In cases when the stochastic decision is not available in training, we treat it as a latent variable and marginalize it out.", "labels": [], "entities": []}, {"text": "For each of the three tasks, we pick one representative application and demonstrate our reference aware model's efficacy in evaluations against models that do not explicitly include a reference operation.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We propose a general framework to model reference in language.", "labels": [], "entities": []}, {"text": "We consider reference to entries in lists, tables, and document context.", "labels": [], "entities": []}, {"text": "We instantiate these tasks into three specific applications: recipe generation, dialogue modeling, and coreference based language models.", "labels": [], "entities": [{"text": "recipe generation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8626909852027893}, {"text": "dialogue modeling", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.8847410082817078}]}, {"text": "\u2022 We develop the first neural model of reference that goes being copying and can model (conditional on context) how to form the mention.", "labels": [], "entities": []}, {"text": "\u2022 We perform comprehensive evaluation of our models on the three data sets and verify our proposed models perform better than strong baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our model with baselines that do not model reference explicitly.", "labels": [], "entities": []}, {"text": "For recipe generation and dialogue modeling, we compare our model with basic seq2seq and attention model.", "labels": [], "entities": [{"text": "recipe generation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.9013648927211761}, {"text": "dialogue modeling", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8185094594955444}]}, {"text": "We also apply attention mechanism over the table for dialogue modeling as a baseline.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8838673830032349}]}, {"text": "For coreference based language model, we compare our model with simple RNN language model.", "labels": [], "entities": []}, {"text": "We train all models with simple stochastic gradient descent with gradient clipping.", "labels": [], "entities": []}, {"text": "We use a one-layer LSTM for all RNN components.", "labels": [], "entities": []}, {"text": "Hyperparameters are selected using grid search based on the validation set.", "labels": [], "entities": []}, {"text": "Evaluation of our model is challenging since it involves three rather different applications.", "labels": [], "entities": []}, {"text": "We focus on evaluating the accuracy of predicting the reference tokens, which is the goal of our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.99893718957901}, {"text": "predicting the reference tokens", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8137595802545547}]}, {"text": "Specifically, we report the perplexity of all words, words that can be generated from reference and non-reference words.", "labels": [], "entities": []}, {"text": "The perplexity is calculated by multiplying the probability of decision at each step all together.", "labels": [], "entities": []}, {"text": "Note that for non-reference words, they also appear in the vocabulary.", "labels": [], "entities": []}, {"text": "So it is a fair comparison to models that do not model reference explicitly.", "labels": [], "entities": []}, {"text": "For the recipe task, we also generate the recipes using beam size of 10 and evaluate the generated recipes with BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9982296824455261}]}, {"text": "We didn't use BLEU for dialogue generation since the database entries take only a very small part of all tokens in utterances.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9971358776092529}, {"text": "dialogue generation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8848846554756165}]}], "tableCaptions": [{"text": " Table 2: Fragment of database for dialogue system.", "labels": [], "entities": []}, {"text": " Table 4: Recipe results, evaluated in perplexity and BLEU score. All means all tokens, Ing denotes  tokens from recipes that appear in ingredients. Word means non-table tokens. Pointer and Latent differs  in that for Pointer, we provide supervised signal on when to generate a reference token, while in Latent  it is a latent decision.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9814638793468475}]}, {"text": " Table 5: Dialogue perplexity results. Table means tokens from table, Table OOV denotes table tokens  that do not appear in the training set. Sentence Attn denotes we use attention mechanism over tokens in  utterances from the previous turn.", "labels": [], "entities": [{"text": "OOV", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9823640584945679}]}, {"text": " Table 6: Coreference based LM. Pointer + init  means we initialize the model with the LM  weights.", "labels": [], "entities": []}]}