{"title": [{"text": "Counterfactual Learning from Bandit Feedback under Deterministic Logging: A Case Study in Statistical Machine Translation", "labels": [], "entities": [{"text": "Counterfactual Learning from Bandit Feedback under Deterministic Logging", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.7660325802862644}, {"text": "Statistical Machine Translation", "start_pos": 90, "end_pos": 121, "type": "TASK", "confidence": 0.8272494276364645}]}], "abstractContent": [{"text": "The goal of counterfactual learning for statistical machine translation (SMT) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another, historic SMT system.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.7918456345796585}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9728254675865173}]}, {"text": "A challenge arises by the fact that risk-averse commercial SMT systems deter-ministically log the most probable translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9927288889884949}]}, {"text": "The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning.", "labels": [], "entities": [{"text": "SMT output", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.9098003208637238}]}, {"text": "We show that counterfactual learning from determinis-tic bandit logs is possible nevertheless by smoothing out deterministic components in learning.", "labels": [], "entities": []}, {"text": "This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization.", "labels": [], "entities": [{"text": "empirical risk minimization", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.6632077395915985}]}, {"text": "Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9991496801376343}]}], "introductionContent": [{"text": "Commercial SMT systems allow to record large amounts of interaction log data at no cost.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9895868301391602}]}, {"text": "Such logs typically contain a record of the source, the translation predicted by the system, and the user feedback.", "labels": [], "entities": []}, {"text": "The latter can be gathered directly if explicit user quality ratings of translations are supported, or inferred indirectly from the interaction of the user with the translated content.", "labels": [], "entities": []}, {"text": "Indirect feedback inform user clicks on displayed ads has been shown to be a valuable feedback signal in response prediction for display advertising ().", "labels": [], "entities": [{"text": "response prediction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.6877085119485855}]}, {"text": "Similar to the computational advertising scenario, one could imagine a scenario where SMT systems are optimized from partial information inform of user feedback to predicted translations, instead of from manually created reference translations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9926406741142273}]}, {"text": "This learning scenario has been investigated in the areas of bandit learning) or reinforcement learning (RL).", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6549255013465881}]}, {"text": "illustrates the learning protocol using the terminology of bandit structured prediction (), whereat each round, a system (corresponding to a policy in RL terms) makes a prediction (also called action in RL, or pulling an arm of a bandit), and receives a reward, which is used to update the system.", "labels": [], "entities": [{"text": "bandit structured prediction", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6583473483721415}]}, {"text": "Counterfactual learning attempts to reuse existing interaction data where the predictions have been made by a historic system different from the target system.", "labels": [], "entities": [{"text": "Counterfactual learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8612875938415527}]}, {"text": "This enables offline or batch learning from logged data, and is important if online experiments that deploy the target system are risky and/or expensive.", "labels": [], "entities": []}, {"text": "Counterfactual learning tasks include policy evaluation, i.e. estimating how a target policy would have performed if it had been in control of choosing the predictions for which the rewards were logged, and policy optimization (also called policy learning), i.e. optimizing parameters of a target policy given the logged data from the historic system.", "labels": [], "entities": [{"text": "policy evaluation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7327889204025269}, {"text": "policy optimization", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.7169971466064453}, {"text": "policy learning)", "start_pos": 240, "end_pos": 256, "type": "TASK", "confidence": 0.8194114367167155}]}, {"text": "Both tasks are called counterfactual, or off-policy in RL terms, since the target policy was actually not in control during logging.", "labels": [], "entities": []}, {"text": "shows the learning protocol for off-policy learning from partial feedback.", "labels": [], "entities": []}, {"text": "The crucial trick to obtain unbiased estimators to evaluate and to optimize the off-policy system is to correct the sampling bias of the logging policy.", "labels": [], "entities": []}, {"text": "This can be done by importance sampling where the estimate is corrected by the inverse propensity score) of the historical algorithm, mitigating the problem that predictions there were favored by the historical system are over-represented in the logs.", "labels": [], "entities": []}, {"text": "As shown by or, a sufficient exploration of the output space by the logging system is a prerequisite for counterfactual learning.", "labels": [], "entities": []}, {"text": "If the logging policy acts stochastically in predicting outputs, this condition is satisfied, and inverse propensity scoring can be applied to correct the sampling bias.", "labels": [], "entities": [{"text": "inverse propensity scoring", "start_pos": 98, "end_pos": 124, "type": "METRIC", "confidence": 0.8926549752553304}]}, {"text": "However, commercial SMT systems usually try to avoid any risk and only log the most probable translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9931193590164185}]}, {"text": "This effectively results in deterministic logging policies, making theory and practice of off-policy methods inapplicable to counterfactual learning in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9873206615447998}]}, {"text": "This paper presents a case study in counterfactual learning for SMT that shows that policy optimization from deterministic bandit logs is possible despite these seemingly contradictory theoretical requirements.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9942814111709595}, {"text": "policy optimization", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7929638028144836}]}, {"text": "We formalize our learning problem as an empirical risk minimization over logged data.", "labels": [], "entities": []}, {"text": "While a simple empirical risk minimizer can show degenerate behavior where the objective is minimized by avoiding or over-representing training samples, thus suffering from decreased generalization ability, we show that the use of control variates can remedy this problem.", "labels": [], "entities": []}, {"text": "Techniques such as doubly-robust policy evaluation and learning) or weighted importance sampling can be interpreted as additive or multiplicative control variates) that serve for variance reduction in estimation.", "labels": [], "entities": []}, {"text": "We observe that a further effect of these techniques is that of smoothing out deterministic components by taking the whole output space into account.", "labels": [], "entities": []}, {"text": "Furthermore, we conjecture that while outputs are logged deterministically, the stochastic selection of inputs serves as sufficient exploration in parameter optimization over a joint feature representation over inputs and outputs.", "labels": [], "entities": []}, {"text": "We present experiments using simulated bandit feedback for two different SMT tasks, showing improvements of up to 2 BLEU in SMT domain adaptation from deterministically logged bandit feedback.", "labels": [], "entities": [{"text": "SMT tasks", "start_pos": 73, "end_pos": 82, "type": "TASK", "confidence": 0.931407243013382}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9989925026893616}, {"text": "SMT domain adaptation", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.9199867248535156}]}, {"text": "This result, together with a comparison to the standard case of policy learning from stochastically logged simulated bandit feedback, confirms the effectiveness our proposed techniques.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we aim to simulate the following scenario: We assume that it is possible to divert a small fraction of the user interaction traffic for the purpose of policy evaluation and to perform stochastic logging on this small data set.", "labels": [], "entities": []}, {"text": "The main traffic is assumed to be logged deterministically, following a conservative regime where one-best translations are used  for an SMT system that does not change frequently overtime.", "labels": [], "entities": [{"text": "SMT", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9803879261016846}]}, {"text": "Since our experiments are simulation studies, we will additionally perform stochastic logging, and compare policy learning for the (realistic) case of deterministic logging with the (theoretically motivated) case of stochastic logging.", "labels": [], "entities": []}, {"text": "In our deterministic-based policy learning experiments, we evaluate the empirical risk minimization algorithms derived from objectives (3) (DPM+R) and (4).", "labels": [], "entities": []}, {"text": "For the doubly controlled objective we employ two variants: First, \u02c6 c is set to 1 as in (Dudik et al., 2011) (DC).", "labels": [], "entities": []}, {"text": "Second, we calculat\u00ea c as described in Equation (5) (\u02c6 c DC).", "labels": [], "entities": []}, {"text": "The algorithms used in policy evaluation and for stochastic-based policy learning are variants of these objectives that replace \u00af \u03c0 by \u00af \u03c1 to yield estimators IPS+R, DR, and\u02c6cand\u02c6 and\u02c6c DR of the expected loss.", "labels": [], "entities": [{"text": "policy evaluation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8772889077663422}, {"text": "stochastic-based policy learning", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.6203232407569885}]}, {"text": "All objectives will be employed in a domain adaptation scenario for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.8032146394252777}]}, {"text": "A system trained on out-of-domain data will be used to collect feedback on in-domain data.", "labels": [], "entities": []}, {"text": "This data will serve as the logged data D in the learning experiments.", "labels": [], "entities": []}, {"text": "We conduct two SMT tasks with hypergraph re-decoding: The first is German-to-English and is trained using a concatenation of the Europarl corpus (, the Common Crawl corpus and the News Commentary corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9943361878395081}, {"text": "Europarl corpus", "start_pos": 129, "end_pos": 144, "type": "DATASET", "confidence": 0.9906950294971466}, {"text": "Common Crawl corpus", "start_pos": 152, "end_pos": 171, "type": "DATASET", "confidence": 0.897021492322286}, {"text": "News Commentary corpus", "start_pos": 180, "end_pos": 202, "type": "DATASET", "confidence": 0.8581289450327555}]}, {"text": "The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus.", "labels": [], "entities": [{"text": "TED parallel corpus", "start_pos": 89, "end_pos": 108, "type": "DATASET", "confidence": 0.7571855187416077}]}, {"text": "A second task uses the French-to-English Europarl data with the goal of domain adaptation to news articles with the News Commentary corpus (.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9043013155460358}, {"text": "domain adaptation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7268597632646561}, {"text": "News Commentary corpus", "start_pos": 116, "end_pos": 138, "type": "DATASET", "confidence": 0.8280292550722758}]}, {"text": "We split off two parts from the TED corpus to be used as validation and test data for the learning experiments.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.7646062076091766}]}, {"text": "As validation data for the News Commentary corpus we use the splits provided at the WMT shared task, namely nc-devtest2007 as validation data and nc-test2007 as test data.", "labels": [], "entities": [{"text": "News Commentary corpus", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.8883618911107382}, {"text": "WMT shared task", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.737816055615743}]}, {"text": "An overview of the data statistics can be seen in.", "labels": [], "entities": []}, {"text": "As baseline, an out-of-domain system is built using the SCFG framework CDEC () with dense features (10 standard features and 2 for the language model).", "labels": [], "entities": [{"text": "SCFG framework CDEC", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.8536234895388285}]}, {"text": "After tokenizing and lowercasing the training data, the data were word aligned using CDEC's fast align.", "labels": [], "entities": [{"text": "CDEC", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9459940791130066}]}, {"text": "A 4-gram language model is build on the target languages for the out-of-domain data using KENLM ().", "labels": [], "entities": [{"text": "KENLM", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.7215535640716553}]}, {"text": "For News, we additionally assume access to in-domain target language text and train another in-domain language model on that data, increasing the number of features to 14 for News.", "labels": [], "entities": []}, {"text": "The framework uses a standard linear Gibbs model whose distribution can be peaked using a parameter \u03b1 (see Equation (6)): Higher value of \u03b1 will shift the probability of the one-best translation closer to 1 and all others closer to 0.", "labels": [], "entities": []}, {"text": "Using \u03b1 > 1 during training will promote to learn models that are optimal when outputting the one-best translation.", "labels": [], "entities": []}, {"text": "In our experiments, we found \u03b1 = 5 to work well on validation data.", "labels": [], "entities": [{"text": "validation", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.953624963760376}]}, {"text": "Additionally, we tune a system using CDEC's MERT implementation on the indomain data with their references.", "labels": [], "entities": []}, {"text": "This fullinformation in-domain system conveys the best possible improvement using the given training data.", "labels": [], "entities": []}, {"text": "It can thus be seen as the oracle system for the systems which are learnt using the same input-side training data, but have only bandit feedback available to them as a learning signal.", "labels": [], "entities": []}, {"text": "All systems are evaluated using the corpus-level BLEU metric (   the original out-of-domain systems, and logging the one-best translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9597075581550598}]}, {"text": "For the stochastic experiments, the translations are sampled from the model distribution.", "labels": [], "entities": []}, {"text": "The feedback to the logged translation is simulated using the reference and sentence-level BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9825337529182434}]}, {"text": "When creating the logged data D, we also record the feature vectors of the translations to train the direct reward estimate that is needed for (\u02c6 c)DC.", "labels": [], "entities": []}, {"text": "Using the feature vector as input and the per-sentence BLEU as the output value, we train a regressionbased random forest with 10 trees using scikitlearn).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9905751943588257}]}, {"text": "To measure performance, we perform 5-fold cross-validation and measure the macro average between estimated rewards and the true rewards from the log: We also report the micro average which quantifies how far off one can expect the model to be fora random sample: 1 n |\u03b4(x t , y t ) \u2212 \u02c6 \u03b4(x t , y t )|.", "labels": [], "entities": []}, {"text": "The final model used in the experiments is trained on the full training data.", "labels": [], "entities": []}, {"text": "Cross-validation results for the regression-based direct reward model can be found in.", "labels": [], "entities": []}, {"text": "Policy evaluation aims to use the logged data D to estimate the performance of the target system \u03c0 w . The small logged data D eval that is diverted for policy evaluation is created by translating only 10k sentences of the in-domain training data with the out-ofdomain system and sample translations according to the model probability.", "labels": [], "entities": [{"text": "Policy evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7977150976657867}]}, {"text": "Again we record the sentence-level BLEU as the feedback.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9808518290519714}]}, {"text": "The reference translations that also exist for those 10k sentences are used to measure the ground truth BLEU value for translations using the fullinformation in-domain system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9337109923362732}]}, {"text": "The goal of evaluation is to achieve a value of IPS+R, DR, and\u02c6c and\u02c6 and\u02c6c DR on D eval that are as close as possible to the ground truth BLEU value.", "labels": [], "entities": [{"text": "IPS+R", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.6924147407213846}, {"text": "DR", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9836167097091675}, {"text": "BLEU", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.9533838629722595}]}, {"text": "To be able to measure variance, we create five folds of D eval , differing in random seeds.", "labels": [], "entities": [{"text": "variance", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.936051070690155}]}, {"text": "We report the average difference between the ground truth BLEU score and the value of the log-based policy evaluation, as well as the standard deviation in.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9440879225730896}]}, {"text": "We see that IPS+R underestimates the BLEU value by 7.78 on News.", "labels": [], "entities": [{"text": "IPS+R", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.6088779966036478}, {"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.999237060546875}, {"text": "News.", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9858691692352295}]}, {"text": "\u02c6 c DR achieves the closest estimate, overestimating the true value by less than 1 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9979148507118225}]}, {"text": "On TED, all policy evaluation results are overestimates.", "labels": [], "entities": [{"text": "TED", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.6908000111579895}]}, {"text": "For the DR variants the overestimation result can be explained by the random forests' tendency to overestimate.", "labels": [], "entities": []}, {"text": "Optima\u00ee c DR can correct for this, but not always in a sufficient way.", "labels": [], "entities": [{"text": "Optima\u00ee c DR", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7425439457098643}]}, {"text": "In our learning experiments, learning starts with the weights w 0 from the outof-domain model.", "labels": [], "entities": []}, {"text": "As this was the system that produced the logged data D, the first iteration will have the same translations in the one-best position.", "labels": [], "entities": []}, {"text": "After some iterations, however, the translation that was logged may not be in the first position anymore.", "labels": [], "entities": []}, {"text": "In this case, the n-best list is searched for the correct translation.", "labels": [], "entities": []}, {"text": "Due to speed reasons, the scores of the translation system are normalized to probabilities using the first 1,000 unique entries in the n-best list, rather than using the full hypergraph.", "labels": [], "entities": []}, {"text": "Our experiments showed that this did not impact the quality of learning.", "labels": [], "entities": []}, {"text": "In order for the multiplicative control variate to be effective, the learning procedure has to utilize mini-batches.", "labels": [], "entities": []}, {"text": "If the mini-batch size is chosen too small, the estimates of the control variates may not be reliable.", "labels": [], "entities": []}, {"text": "We test mini-batch sizes of 30k and 10k examples, whereas 30k on News means that we perform batch training since the mini-batch spans the entire training set.", "labels": [], "entities": [{"text": "News", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9783862233161926}]}, {"text": "Minibatch size \u03b2 and early stopping point where selected by choosing the setup and iteration that achieved the highest BLEU score on the one-best translations for the validation data.", "labels": [], "entities": [{"text": "Minibatch size \u03b2", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.7588009635607401}, {"text": "early stopping point", "start_pos": 21, "end_pos": 41, "type": "METRIC", "confidence": 0.9543633262316386}, {"text": "BLEU score", "start_pos": 119, "end_pos": 129, "type": "METRIC", "confidence": 0.9814040660858154}]}, {"text": "The learning rate \u03b7 was selected in the same way, whereas the possible values were 1e\u22124, 1e\u22125, 1e\u22126 or, alternatively, Adadelta, which sets the learning rate on a per-feature basis.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.6917324066162109}]}, {"text": "The results on both validation and test set are reported in.", "labels": [], "entities": []}, {"text": "Statistical significance of the outof-domain system compared to all other systems is measured using Approximate Randomization testing.", "labels": [], "entities": [{"text": "Approximate", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.958939790725708}]}, {"text": "For the deterministic case, we see that in general DPM+R shows the lowest increase but can still significantly outperform the baseline.", "labels": [], "entities": [{"text": "DPM+R", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.8072931369145712}]}, {"text": "An explanation of why DPM+R cannot improve any further, will be addressed separately below.", "labels": [], "entities": [{"text": "DPM+R", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.4268633822600047}]}, {"text": "DC yields improvements of up to 1.5 BLEU points, whil\u00ea c DC obtains improvements of up to 2 BLEU points over the out-of-domain baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9984834790229797}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9971935153007507}]}, {"text": "In more detail on the TED data, DC can close the gap of nearly 3 BLEU by half between the out-of-domain and the full-information indomain system.", "labels": [], "entities": [{"text": "TED data", "start_pos": 22, "end_pos": 30, "type": "DATASET", "confidence": 0.8177850246429443}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9985986351966858}]}, {"text": "\u02c6 c DC can improve by further 0.6 BLEU which is a significant improvement at p = 0.0017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9994696974754333}]}, {"text": "Also note that, whil\u00ea c DC takes more iterations to reach its best result on the validation data, \u02c6 c DC already outperforms DC at the stopping iteration of DC.", "labels": [], "entities": []}, {"text": "At this point\u02c6cpoint\u02c6 point\u02c6c DC is better by 0.18 BLEU on the validation set and continues to increase until its own stopping iteration.", "labels": [], "entities": [{"text": "DC", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9959650039672852}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.999376118183136}]}, {"text": "The final results of\u02c6cof\u02c6 of\u02c6c DC falls only 0.8 BLEU behind the oracle system that had references available during its learning process.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9995744824409485}]}, {"text": "Considering the substantial difference in information that both systems had available, this is remark-: BLEU increases for learning, over the out-of-domain baseline on validation and test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9996914863586426}]}, {"text": "Outof-domain is the baseline and starting system and in-domain is the oracle system tuned on in-domain data with references.", "labels": [], "entities": []}, {"text": "For the deterministic case, all results are statistically significant at p \u2264 0.001 with regards to the baseline.", "labels": [], "entities": []}, {"text": "For the stochastic case, all results are statistically significant at p \u2264 0.002 with regards to the baseline, except for IPS+R on the News corpus. able.", "labels": [], "entities": [{"text": "IPS+R", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.5351101060708364}, {"text": "News corpus. able", "start_pos": 134, "end_pos": 151, "type": "DATASET", "confidence": 0.9774781614542007}]}, {"text": "The improvements on the News corpus show similar tendencies.", "labels": [], "entities": [{"text": "News corpus", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9641402661800385}]}, {"text": "Again there is a gap of nearly 3 BLEU to close and with an improvement of 1.05 BLEU points, DC can achieve a notable result.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9990047812461853}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9984244108200073}]}, {"text": "\u02c6 c DC was able to further improve on this but not as successfully as was the case for the TED corpus.", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.7674829959869385}]}, {"text": "Analyzing the actua\u00ee c values that were calculated in both experiments allows us to gain an insight as to why this was the case: For TED, \u02c6 c is on average 1.35.", "labels": [], "entities": []}, {"text": "In the case of News, however, \u02c6 c has a maximum value of 1.14 and thus stays quite close to 1, which would equate to using DC.", "labels": [], "entities": [{"text": "News", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.8121742010116577}]}, {"text": "It is thus not surprising that there is no significant difference between DC and\u02c6cand\u02c6 and\u02c6c DC.", "labels": [], "entities": []}, {"text": "Comparison to the Stochastic Case.", "labels": [], "entities": []}, {"text": "Even if not realistic for commercial applications of SMT, our simulation study allows us to stochastically log large amounts of data in order to compare learning from deterministic logs to the standard case.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.993502676486969}]}, {"text": "As shown in, the relations between algorithms and even the absolute improvements are similar for stochastic and deterministic logging.", "labels": [], "entities": []}, {"text": "Significance tests between each deterministic/stochastic experiment pair show a significant difference only in case of DC/DR on TED data.", "labels": [], "entities": [{"text": "TED data", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.7768079042434692}]}, {"text": "However, the DR result still does not significantly outperform the best deterministic objective on TED (\u02c6 c DC).", "labels": [], "entities": [{"text": "DR", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.997389018535614}]}, {"text": "The p values for all other experiment pairs lie above 0.1.", "labels": [], "entities": []}, {"text": "From this we can conclude that it is indeed an acceptable practice to log deterministically.", "labels": [], "entities": []}, {"text": "show that counterfactual learning is impossible unless the logging system sufficiently explores the output space.", "labels": [], "entities": []}, {"text": "This condition is seemingly not satisfied if the logging systems acts according to a deterministic policy.", "labels": [], "entities": []}, {"text": "Furthermore, since techniques such as \"exploration over time\" are not applicable to commercial SMT systems that are not frequently changed overtime, the case of counterfactual learning for SMT seems hopeless.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.991664707660675}, {"text": "SMT", "start_pos": 189, "end_pos": 192, "type": "TASK", "confidence": 0.9885538816452026}]}, {"text": "However, our experiments present evidence to the contrary.", "labels": [], "entities": []}, {"text": "In the following, we present an analysis that aims to explain this apparent contradiction.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Evaluation of regression-based reward  estimation by average BLEU differences be- tween estimated and true rewards.", "labels": [], "entities": [{"text": "regression-based reward  estimation", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.5625417331854502}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.998847246170044}]}, {"text": " Table 4: Policy evaluation by macro averaged  difference between estimated and ground truth  BLEU on 10k stochastically logged data, aver- aged over 5 runs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9841637015342712}]}, {"text": " Table 5: BLEU increases for learning, over the out-of-domain baseline on validation and test set. Out- of-domain is the baseline and starting system and in-domain is the oracle system tuned on in-domain  data with references. For the deterministic case, all results are statistically significant at p \u2264 0.001  with regards to the baseline. For the stochastic case, all results are statistically significant at p \u2264 0.002  with regards to the baseline, except for IPS+R on the News corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988899827003479}, {"text": "News corpus", "start_pos": 476, "end_pos": 487, "type": "DATASET", "confidence": 0.94985431432724}]}]}