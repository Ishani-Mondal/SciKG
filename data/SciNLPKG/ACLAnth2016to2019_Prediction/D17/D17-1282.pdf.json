{"title": [{"text": "Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7113773425420126}]}], "abstractContent": [{"text": "In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.6237667600313822}, {"text": "BRNN-CNN", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.48152071237564087}]}, {"text": "Motivated by the observation that named entities are highly related to linguistic constituents , we propose a constituent-based BRNN-CNN for named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.6641105016072592}]}, {"text": "In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents.", "labels": [], "entities": []}, {"text": "Then it classifies these chunks with a constituency tree structure by recur-sively propagating syntactic and semantic information to each constituent node.", "labels": [], "entities": []}, {"text": "This method surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses.", "labels": [], "entities": [{"text": "OntoNotes 5.0", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8760136365890503}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) can be seen as a combined task of locating named entity chunks of texts and classifying which named entity category a chunk falls into.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7897329777479172}]}, {"text": "Traditional approaches label each token in texts as apart of a named entity chunk, e.g. \"person begin\", and achieve high performances in several benchmark datasets.", "labels": [], "entities": []}, {"text": "Being formulated as a sequential labeling problem, NER systems could be naturally implemented by recurrent neural networks.", "labels": [], "entities": [{"text": "NER", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9612187743186951}]}, {"text": "These networks process a token at a time, taking, for each token, the hidden features of its previous token as well as its raw features to compute its own hidden features.", "labels": [], "entities": []}, {"text": "Then they classify each token by these hidden features.", "labels": [], "entities": []}, {"text": "With both forward and backward directions, networks learn how to propagate the information of a token sequence to each token.", "labels": [], "entities": []}, {"text": "utilize a variation of recurrent networks, bidirectional LSTM, attached with a CNN, which learns character-level features instead of handcrafting.", "labels": [], "entities": []}, {"text": "They accomplish state-of-the-art results on both and OntoNotes 5.0 () datasets.", "labels": [], "entities": [{"text": "OntoNotes 5.0 () datasets", "start_pos": 53, "end_pos": 78, "type": "DATASET", "confidence": 0.8674041330814362}]}, {"text": "Classical sequential labeling approaches take little information about phrase structures of sentences.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6908299326896667}]}, {"text": "However, according to our analysis, most named entity chunks are actually linguistic constituents, e.g. noun phrases.", "labels": [], "entities": []}, {"text": "This motivates us to focus on a constituent-based approach for NER where the NER problem is transformed into a named entity classification task on every node of a node.children \u2190 [node.children, newChild]", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on OntoNotes 5.0 NER () and analyze it with several ablation studies.", "labels": [], "entities": [{"text": "OntoNotes 5.0 NER", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.8246924082438151}]}, {"text": "The project sources are publicly available on https:// github.com/jacobvsdanniel/tf_rnn.: Dataset statistics for OntoNotes 5.0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment results on whole dataset. BRNN is BRNN-CNN deprived of character-level em- beddings. Human-labeled parses and automatically generated parses are indicated by gold and auto  respectively. Finkel and Manning used gold parses in training a joint model for parsing and NER.", "labels": [], "entities": [{"text": "BRNN", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.6562510132789612}, {"text": "parsing", "start_pos": 274, "end_pos": 281, "type": "TASK", "confidence": 0.971025288105011}, {"text": "NER", "start_pos": 286, "end_pos": 289, "type": "TASK", "confidence": 0.8887338638305664}]}, {"text": " Table 2: F1 scores on different data sources. From left to right: broadcast conversation, broadcast news,  magazine, newswire, telephone conversation, and blogs & newsgroups.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9992846846580505}]}, {"text": " Table 3: Dataset statistics for OntoNotes 5.0.", "labels": [], "entities": [{"text": "OntoNotes 5.0", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8475667834281921}]}]}