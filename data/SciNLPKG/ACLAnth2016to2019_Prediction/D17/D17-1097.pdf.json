{"title": [{"text": "The Promise of Premise: Harnessing Question Premises in Visual Question Answering", "labels": [], "entities": [{"text": "Harnessing Question Premises in Visual Question Answering", "start_pos": 24, "end_pos": 81, "type": "TASK", "confidence": 0.6039749767099108}]}], "abstractContent": [{"text": "In this paper, we make a simple observation that questions about images often contain premises-objects and relationships implied by the question-and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 188, "end_pos": 219, "type": "TASK", "confidence": 0.7920315563678741}]}, {"text": "When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers.", "labels": [], "entities": []}, {"text": "We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image).", "labels": [], "entities": []}, {"text": "We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises.", "labels": [], "entities": [{"text": "Question Relevance Prediction and Explanation (QRPE)", "start_pos": 56, "end_pos": 108, "type": "TASK", "confidence": 0.7325479239225388}]}, {"text": "We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not.", "labels": [], "entities": [{"text": "question relevance detection", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7177296876907349}]}, {"text": "We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of providing natural language answers to free-form questions about an image -i.e. Visual Question Answering (VQA) -has received substantial attention in the past few years (; Andreas et al., * Denotes equal contribution.", "labels": [], "entities": [{"text": "natural language answers to free-form questions about an image -i.e. Visual Question Answering (VQA)", "start_pos": 22, "end_pos": 122, "type": "TASK", "confidence": 0.657642159391852}]}], "datasetContent": [{"text": "As discussed in Section 1, modern VQA models fail to differentiate between relevant and irrelevant questions, answering either with confidence.", "labels": [], "entities": []}, {"text": "This behavior is detrimental to the real world application of VQA systems.", "labels": [], "entities": []}, {"text": "In this section, we curate anew dataset for question relevance in VQA which we call the Question Relevance Prediction and Explanation (QRPE) dataset.", "labels": [], "entities": [{"text": "VQA", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.8938419222831726}, {"text": "Question Relevance Prediction and Explanation (QRPE)", "start_pos": 88, "end_pos": 140, "type": "TASK", "confidence": 0.6473174765706062}]}, {"text": "We plan to release QRPE publicly to help future efforts.", "labels": [], "entities": []}, {"text": "In order to train and evaluate models for irrelevant question detection, we would like to create a dataset of tuples (I + , Q, P, I \u2212 ) comprised of a natural language question Q, an image I + for which Q is relevant, and an image I \u2212 for which Q is irrelevant because premise P is false.", "labels": [], "entities": [{"text": "irrelevant question detection", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.6838580667972565}]}, {"text": "While it is not required to collect both a relevant and irrelevant image for each question, we argue that doing so is a simple way to balance the dataset and it ensures that biases against rarer questions (which would be irrelevant for most images) cannot be exploited to inflate performance.", "labels": [], "entities": []}, {"text": "We base our dataset on the existing VQA corpus (, taking the human-generated (and therefore relevant) image-question pairs from VQA as I + and Q. As previously discussed, we can define the relevancy of a question in terms of the validity of its premises for an image, so we extract premises from each question Q and must find a suitable irrelevant image I \u2212 . However, there are certainly many images for which one or more of Q's premises are false and an important design decision is then how to select I \u2212 from this set.", "labels": [], "entities": [{"text": "VQA corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9666892886161804}, {"text": "VQA", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9484002590179443}]}, {"text": "To ensure our dataset is as realistic and challenging as possible, we consider irrelevant images which only have a single false question premise under Q which we denote P . For example, the question \"Is the big red dog old?\" could be matched with an image containing a big, white dog or a small red dog, but not a small white dog.", "labels": [], "entities": []}, {"text": "In this way, we ensure that image content is semantically appropriate for the question topic but not quite relevant.", "labels": [], "entities": []}, {"text": "Additionally, this provides each irrelevant image with an explanation for why the question does not apply.", "labels": [], "entities": []}, {"text": "Furthermore, we sort this subset of irrelevant image by their visual distance to the source image I + based on image encodings from a VGGNet) pretrained on ImageNet (.", "labels": [], "entities": [{"text": "VGGNet", "start_pos": 134, "end_pos": 140, "type": "DATASET", "confidence": 0.9510530829429626}, {"text": "ImageNet", "start_pos": 156, "end_pos": 164, "type": "DATASET", "confidence": 0.9549500942230225}]}, {"text": "This ensures that the relevant and irrelevant images are visually similar and act as difficult examples.", "labels": [], "entities": []}, {"text": "A major difficulty with our proposed data collection process is how to verify whether a premise if true or false for any given image in order to identify irrelevant images.", "labels": [], "entities": []}, {"text": "We detail dataset construction and our approach for this problem in the following section.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7547678053379059}]}, {"text": "We curate our QRPE dataset automatically from existing annotations in COCO () and Visual Genome (.", "labels": [], "entities": [{"text": "QRPE dataset", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.8053539991378784}, {"text": "COCO", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.9438591599464417}]}, {"text": "COCO is a set of over 300,000 images annotated with object segmentations and presence information for 80 classes as well as text descriptions of image content.", "labels": [], "entities": [{"text": "COCO", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8764446973800659}]}, {"text": "Visual Genome builds on this dataset, providing more detailed object, attribute, and rela- tionship annotations for over 100,000 COCO images.", "labels": [], "entities": [{"text": "Visual Genome", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8972853720188141}]}, {"text": "We make use of these data sources to extract first and second order premises from VQA questions which are also based on COCO images.", "labels": [], "entities": [{"text": "VQA questions", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9004359543323517}, {"text": "COCO images", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.886681467294693}]}, {"text": "For first order premises (i.e. existential premises), we consider only the 80 classes present in COCO ().", "labels": [], "entities": [{"text": "COCO", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.879423201084137}]}, {"text": "As VQA and COCO share the same images, we can easily determine if a first order premise is true or false fora candidate irrelevant image simply by checking for the absence of the appropriate class annotation.", "labels": [], "entities": [{"text": "VQA", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.8826549649238586}]}, {"text": "For second order premises (i.e. attributed objects), we rely on Visual Genome ( annotations for object and attribute labels.", "labels": [], "entities": []}, {"text": "Unlike in COCO, the lack of a particular object label in an image for Visual Genome does not necessarily indicate that the object is not present, both due to annotation noise and the use of multiple synonyms for objects by human labelers.", "labels": [], "entities": []}, {"text": "As a consequence, we restrict the set of candidate irrelevant images to those which contain a matching object to the question premise but a different attribute.", "labels": [], "entities": []}, {"text": "Without further restriction, the selected irrelevant attributes do not tend to be mutually exclusive with the source attribute (i.e. matching '<dog, old>' and '<dog, red>').", "labels": [], "entities": []}, {"text": "To correct this and ensure a false premise, we further restrict the set to attributes which are antonyms (e.g. '<young>' for source attribute '<old>') or taxonomic sister terms (e.g. '<green>' for source attribute '<red>') of the original premise attribute.", "labels": [], "entities": []}, {"text": "We also experimented with third order premises; however, the lack of a corresponding sense of mutual exclusion for verbs and the sparsity of <ob-ject, relationship, object> premises made finding non-trivial irrelevant images difficult.", "labels": [], "entities": []}, {"text": "To recap, our data collection approach is to take each image-question pair in the VQA dataset and extract its first and second order question premises.", "labels": [], "entities": [{"text": "VQA dataset", "start_pos": 82, "end_pos": 93, "type": "DATASET", "confidence": 0.9781562983989716}]}, {"text": "For each premise, we find all images which lack only this premise and rank them by their visual distance.", "labels": [], "entities": []}, {"text": "The closest of these is kept as the irrelevant image for each image-question pair.", "labels": [], "entities": []}, {"text": "shows sample (I + , Q, P, I \u2212 ) tuples from our dataset.", "labels": [], "entities": []}, {"text": "These examples illustrate the difficulty of our dataset.", "labels": [], "entities": []}, {"text": "For instance, the images in the second column differ only by the presence of the water bottle and images in the fourth column are differentiated by the color of the devices.", "labels": [], "entities": []}, {"text": "Both of these are fine details of the image content.", "labels": [], "entities": []}, {"text": "The QRPE dataset contains 53,911 (I + , Q, P, I \u2212 ) tuples generated from as many premises.", "labels": [], "entities": [{"text": "QRPE dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9321624040603638}]}, {"text": "In total, it contains 1530 unique premises and 28,853 unique questions.", "labels": [], "entities": []}, {"text": "Among the 53,911 premises, 3876 are second-order, attributed object premises while the remaining 50,035 are first-order object/scene premises.", "labels": [], "entities": []}, {"text": "We divide our dataset into two parts -a training set with 35,486 tuples that are generated from the VQA training set and a validation set with 18,425 tuples generated from the VQA validation set.", "labels": [], "entities": [{"text": "VQA training set", "start_pos": 100, "end_pos": 116, "type": "DATASET", "confidence": 0.9582048058509827}, {"text": "VQA validation set", "start_pos": 176, "end_pos": 194, "type": "DATASET", "confidence": 0.9276204109191895}]}, {"text": "We also manually validated 1000 randomly selected (I + , Q, P, I \u2212 ) tuples from our dataset.", "labels": [], "entities": []}, {"text": "We noted that 99.10% of the premises P were valid (i.e. implied by the question) in I + and 97.3% were false for the negative image I \u2212 . This demonstrates the high reliability of our automated annotation pipeline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of Question Relevance models on the  QRPE test set. We find that premise-aware models consis- tently outperform alternative models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9854013323783875}, {"text": "QRPE test set", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9770580132802328}]}, {"text": " Table 1. As  the dataset is balanced in the label space, random  accuracy stands at 50%. We find that the simple  VQA-Bin model achieves 66.5% accuracy while  the attention based model HieCoAtt-Bin at- tains 70.74% accuracy. Surprisingly, the caption- similarity based QC-Sim model significantly out- performs these baseline, obtaining an accuracy  of 74.35% while only reasoning about relevancy  from textual descriptions of images. We note that  the caption similarity based approaches use a large  amount of outside data during pretraining of the  captioning model and the word2vec embeddings,  which may have contributed to the effectiveness of  these methods.  Most interestingly, we find that the addi- tion of extracted premise representations con- sistently improves performance of base mod- els. VQA-Bin-Prem, HieCoAtt-Bin-Prem,  PC-Sim, and QPC-Sim outperform their no-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9383067488670349}, {"text": "VQA-Bin", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.9213495254516602}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9967394471168518}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9958290457725525}, {"text": "accuracy", "start_pos": 340, "end_pos": 348, "type": "METRIC", "confidence": 0.9945573806762695}, {"text": "VQA-Bin-Prem", "start_pos": 806, "end_pos": 818, "type": "DATASET", "confidence": 0.942587673664093}]}, {"text": " Table 2: Answer type distribution of source and premise  questions on the Compositional VQA train set.", "labels": [], "entities": [{"text": "VQA train set", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.8777752121289571}]}, {"text": " Table 3: Accuracy on the standard and compositional VQA  validation sets for different augmentation strategies for Deep- erLSTM(Antol et al., 2015).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9852834939956665}]}, {"text": " Table 4: Overall accuracy of different VQA models on the  Compositional VQA test split using Top-1k-A augmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9995947480201721}, {"text": "VQA test split", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8267764647801717}]}]}