{"title": [{"text": "Knowledge Distillation for Bilingual Dictionary Induction", "labels": [], "entities": [{"text": "Knowledge Distillation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6341376155614853}, {"text": "Bilingual Dictionary Induction", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.7525400916735331}]}], "abstractContent": [{"text": "Leveraging zero-shot learning to learn mapping functions between vector spaces of different languages is a promising approach to bilingual dictionary induction.", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 129, "end_pos": 159, "type": "TASK", "confidence": 0.6711580157279968}]}, {"text": "However, methods using this approach have not yet achieved high accuracy on the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9987753033638}]}, {"text": "In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective.", "labels": [], "entities": [{"text": "knowledge distillation training", "start_pos": 80, "end_pos": 111, "type": "TASK", "confidence": 0.8253094553947449}]}, {"text": "As teachers, rich resource translation paths are exploited in this role.", "labels": [], "entities": []}, {"text": "And as learners, translation paths involving low resource languages learn from the teachers.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9625518918037415}]}, {"text": "Our training objective allows seamless addition of teacher translation paths for any given low resource pair.", "labels": [], "entities": [{"text": "teacher translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.6744473427534103}]}, {"text": "Since our approach relies on the quality of monolin-gual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information.", "labels": [], "entities": []}, {"text": "Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17% accuracy improvements .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9990476965904236}]}], "introductionContent": [{"text": "In traditional supervised learning, a classifier is trained on a labeled dataset of the form.", "labels": [], "entities": []}, {"text": "Each xi \u2208 X is a feature vector representing a single training instance and y i \u2208 Y is the label associated with xi . In zero-shot learning (, attest time we can encounter a test instance x j whose corresponding label was not seen at training time.", "labels": [], "entities": []}, {"text": "This setting occurs in domains where Y can take on many values, and obtaining labeled examples for all possible Y values is expensive.", "labels": [], "entities": []}, {"text": "Computer vision is one such domain, where there are thousands of objects a system needs to recognize yet at training time we may only see examples of some of the objects.", "labels": [], "entities": [{"text": "Computer vision", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7773575782775879}]}, {"text": "In zeroshot learning, instead of learning parameters associated with each possible label in Y , the learning task is cast as a problem of learning a single mapping function from the vector space of input instances to the vector space of the output labels.", "labels": [], "entities": []}, {"text": "The resulting induced function can then be applied to test instances x j whose labels may not have been seen at training time, producing a projected vector, \u02c6 y j , in the label space.", "labels": [], "entities": []}, {"text": "The nearest neighbor of the mapped vector in the label space is then considered to be the label of x j . In this paper, we study zero-shot learning in the context of bilingual dictionary induction, which is the problem of mapping words from a source language to equivalent words in a target language.", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 166, "end_pos": 196, "type": "TASK", "confidence": 0.6029206415017446}]}, {"text": "The label space is the full vocabulary of the target language which can be on the order of millions of tokens.", "labels": [], "entities": []}, {"text": "First, word embeddings are learned separately for each language, and second, using a given seed dictionary, we train a mapping function to connect the two monolingual vector spaces, thereby facilitating bilingual dictionary induction.", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 203, "end_pos": 233, "type": "TASK", "confidence": 0.6199496487776438}]}, {"text": "The advantage of zero-shot learning is that it can help reduce the amount of labeled data for applications with many possible labels, such as the application we study in this paper, bilingual dictionary induction.", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 182, "end_pos": 212, "type": "TASK", "confidence": 0.6994690299034119}]}, {"text": "However, the state-of-the-art accuracy on zero-shot bilingual dictionary induction is still low.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9984642267227173}, {"text": "zero-shot bilingual dictionary induction", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.6081760227680206}]}, {"text": "On the task of English to Italian (en \u2192 it), top-1 and top-10 accuracies are around 40% and 60%, respectively ().", "labels": [], "entities": [{"text": "accuracies", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9671863317489624}]}, {"text": "An important aspect of zero-shot learning for bilingual dictionary induction is that, it relies on availability of a large seed dictionary . Such large es en nl en sv en pt af da Figure 1: Trilingual paths for Portuguese(pt) to English(en) via Spanish (es), Afrikaans(af ) to (en) via Dutch (nl), and Danish(da) to (en) via Swedish(sv).", "labels": [], "entities": [{"text": "bilingual dictionary induction", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6047794918219248}]}, {"text": "training dictionaries might not be available for all languages.", "labels": [], "entities": []}, {"text": "However, fora given language with only a small seed dictionary, there could be a highly related language with a much larger seed dictionary.", "labels": [], "entities": []}, {"text": "For example, we might have a small seed dictionary for translating Portuguese to English (pt \u2192 en), but a large seed dictionary for translating Spanish to English language (es \u2192 en).", "labels": [], "entities": [{"text": "translating Portuguese to English", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.8490135371685028}]}, {"text": "At training time, we can train the (pt \u2192 en) mapping function not only using the small seed dictionary, but also make use of the trilingual path going through Spanish, (pt \u2192 es \u2192 en).", "labels": [], "entities": []}, {"text": "Since pt and es are highly related, a small amount of data maybe sufficient to learn the projection (pt \u2192 es).", "labels": [], "entities": []}, {"text": "This is the idea of using abridge or pivot language in machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7144791781902313}]}, {"text": "Our contribution is a knowledge distillation training objective function that encourages the mapping function ( pt \u2192 en) to predict the true English target words as well as to match the predictions of the trilingual path ( pt \u2192 es \u2192 en) within a margin.", "labels": [], "entities": [{"text": "knowledge distillation training", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7977385322252909}]}, {"text": "This is approach allows seamless Example trilingual paths are shown in.", "labels": [], "entities": []}, {"text": "By setting up our objective function in this way, we are distilling knowledge () from the trilingual paths to train a single mapping function for ( pt \u2192 en).", "labels": [], "entities": []}, {"text": "In our experiments, we show performance gains for several language pairs, 17% for top-10 precision for ( pt \u2192 en).", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9794288277626038}]}, {"text": "We also show that, fora given language pair, our objective seamlessly allows us to distill from several related languages.", "labels": [], "entities": []}, {"text": "Moreover, we learn weights for each of the distillation paths, thereby automatically learning indicative weights of how useful each distillation path is.", "labels": [], "entities": []}, {"text": "Finally, we show that even when we only use unlabeled data to distill knowledge from trilingual paths, we still obtain performance gains over a model trained on a small seed dictionary.", "labels": [], "entities": []}, {"text": "Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information.", "labels": [], "entities": []}, {"text": "In particular, we augment word vectors with additional dimensions capturing corpus statistics of part-of-speech tags of words.", "labels": [], "entities": []}, {"text": "Second, we model sub-word information in the vector representations of words.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we study the following questions: \u2022 What is the effect of modeling linguistic information in the vector representations of the source and target languages on accuracy of bilingual dictionary induction?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 175, "end_pos": 183, "type": "METRIC", "confidence": 0.9986496567726135}, {"text": "bilingual dictionary induction", "start_pos": 187, "end_pos": 217, "type": "TASK", "confidence": 0.602730393409729}]}, {"text": "\u2022 Can our knowledge distillation objective from trilingual paths involving related languages improve accuracy of mapping functions of languages with small seed dictionaries?", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7348390817642212}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9983651041984558}]}, {"text": "In most of our experiments, we use the training data that was used to train the multi-lingual embeddings in ().", "labels": [], "entities": []}, {"text": "We indicate when this is not the training data used.", "labels": [], "entities": []}, {"text": "This data was obtained automatically by using Google Translate.", "labels": [], "entities": []}, {"text": "For test data, we use manual translations either from prior work or from searching the Web, including genealogical word lists 3 . For word vector representations, we use Wikipedia to train 300 dimensional vectors for all languages we evaluate on.", "labels": [], "entities": [{"text": "word vector representations", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6467609306176504}]}, {"text": "Based on a validation set, we set the margin \u03b3 in Equation 3 through Equation 6 to be \u03b3 = 0.4, \u03c6 in Equations 11, 12, and 15 to be \u03c6 = 0.01.", "labels": [], "entities": []}, {"text": "We estimate model parameters using stochastic gradient descent.", "labels": [], "entities": []}, {"text": "To address the first of our evaluation questions, we performed experiments on the dataset introduced by (, where the state-of-the art is the work of (.", "labels": [], "entities": []}, {"text": "This is an Italian to English dataset, which consists of 5K translation pairs as training data, and 1.5K pairs as test data.", "labels": [], "entities": [{"text": "Italian to English dataset", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.614836722612381}]}, {"text": "In both () and (Lazaridou et al., 2015), the embeddings were trained on Wikipedia and additional corpora, we only train on Wikipedia.", "labels": [], "entities": []}, {"text": "The results for this experiment are shown in Table 1.", "labels": [], "entities": []}, {"text": "Our method, THIS, performs well above the previous state of the art ().", "labels": [], "entities": [{"text": "THIS", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.6638123393058777}]}, {"text": "For top-1 precision, as can been seen in: Top-10 precision for eight languages translated to English.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9895114302635193}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.964687168598175}]}, {"text": "The high accuracy on Italian can be explained by the fact that, unlike other language pairs, for Italian we do not use Google Translate training data, but the data of (), as shown in. in, and the corresponding data is shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.999352753162384}, {"text": "Google Translate training data", "start_pos": 119, "end_pos": 149, "type": "DATASET", "confidence": 0.8198949694633484}]}, {"text": "For these language pairs, we do not show results for our method, THIS w/pos, since POS taggers are not available for some of the languages.", "labels": [], "entities": [{"text": "THIS", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9639702439308167}, {"text": "POS taggers", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7327134013175964}]}, {"text": "We also do not show (), as they did not do experiments on these data sets, and we did not have an implementation of their approach.", "labels": [], "entities": []}, {"text": "Additionally, ( did not have trained embeddings for Dutch (nl).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation accuracy on the English to  Italian dataset of (Dinu et al., 2014).", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9544680714607239}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9405930042266846}]}, {"text": " Table 2: Training and test sets for various lan- guage pairs. The training datasets marked with (*)  are from (Ammar et al., 2016) obtained through  Google Translate. Italian to English is from (Dinu  et al., 2014). The Dutch to English training dataset  is introduced in this paper. With the exception of  Italian to English, all test datasets are introduced  in this paper.", "labels": [], "entities": []}, {"text": " Table 3: Top-10 precision for eight languages  translated to English. The high accuracy on Ital- ian can be explained by the fact that, unlike other  language pairs, for Italian we do not use Google  Translate training data, but the data of (", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9947734475135803}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.999432384967804}, {"text": "Google  Translate training data", "start_pos": 193, "end_pos": 224, "type": "DATASET", "confidence": 0.6895751506090164}]}, {"text": " Table 4: Training and test datasets used in the  trilingual path distillation experiments. We evalu- ated sub-parts of trilingual paths such as pt \u2192 es,  and pt \u2192 f r using cross validation hence the test  sets for those languages are zero.", "labels": [], "entities": [{"text": "trilingual path distillation", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6649028658866882}]}, {"text": " Table 6: Trilingual path distillation results for  Afrikaans and Danish.", "labels": [], "entities": [{"text": "Trilingual path distillation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6808802485466003}]}]}