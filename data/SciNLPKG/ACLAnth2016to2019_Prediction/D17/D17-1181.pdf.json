{"title": [{"text": "Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification", "labels": [], "entities": [{"text": "Joint Entity and Relation Classification", "start_pos": 58, "end_pos": 98, "type": "TASK", "confidence": 0.6970965623855591}]}], "abstractContent": [{"text": "We introduce globally normalized convo-lutional neural networks for joint entity classification and relation extraction.", "labels": [], "entities": [{"text": "joint entity classification", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6549854278564453}, {"text": "relation extraction", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8078086376190186}]}, {"text": "In particular, we propose away to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time.", "labels": [], "entities": [{"text": "predicting entity types and relations between entities", "start_pos": 99, "end_pos": 153, "type": "TASK", "confidence": 0.8561744264193943}]}, {"text": "Our experiments show that global normal-ization outperforms a locally normalized softmax layer on a benchmark dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity classification (EC) and relation extraction (RE) are important topics in natural language processing.", "labels": [], "entities": [{"text": "Named entity classification (EC)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7847302307685217}, {"text": "relation extraction (RE)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8590416550636292}, {"text": "natural language processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.637068529923757}]}, {"text": "They are relevant, e.g., for populating knowledge bases or answering questions from text, such as \"Where does X live?\"", "labels": [], "entities": []}, {"text": "Most approaches consider the two tasks independent from each other or treat them as a sequential pipeline by first applying a named entity recognition tool and then classifying relations between entity pairs.", "labels": [], "entities": []}, {"text": "However, named entity types and relations are often mutually dependent.", "labels": [], "entities": []}, {"text": "If the types of entities are known, the search space of possible relations between them can be reduced and vice versa.", "labels": [], "entities": []}, {"text": "This can help, for example, to resolve ambiguities, such as in the case of \"Mercedes\", which can be a person, organization and location.", "labels": [], "entities": []}, {"text": "However, knowing that in the given context, it is the second argument for the relation \"live in\" helps concluding that it is a location.", "labels": [], "entities": []}, {"text": "Therefore, we propose a single neural network (NN) for both tasks.", "labels": [], "entities": []}, {"text": "In contrast to joint training and multitask learning, which calculate taskwise costs, we propose to learn a joint classification layer which is globally normalized on the outputs of both tasks.", "labels": [], "entities": []}, {"text": "In particular, we train the NN parameters based on the loss of a linear-chain conditional random field (CRF) ().", "labels": [], "entities": []}, {"text": "CRF layers for NNs have been introduced for token-labeling tasks like named entity recognition (NER) or part-of-speech tagging.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 70, "end_pos": 100, "type": "TASK", "confidence": 0.7839760631322861}, {"text": "part-of-speech tagging", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7255122661590576}]}, {"text": "Instead of labeling each input token as in previous work, we model the joint entity and relation classification problem as a sequence of length three for the CRF layer.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.719598650932312}]}, {"text": "In particular, we identify the types of two candidate entities (words or short phrases) given a sentence (we call this entity classification to distinguish it from the token-labeling task NER) as well as the relation between them.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this architecture for combining entity and relation classification in a single neural network is novel.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 73, "end_pos": 96, "type": "TASK", "confidence": 0.7139637321233749}]}, {"text": "shows an example of how we model the task: For each sentence, candidate entities are identified.", "labels": [], "entities": []}, {"text": "Every possible combination of candidate entities (query entity pair) then forms the input to our model which predicts the classes for the two query entities as well as for the relation between them.", "labels": [], "entities": []}, {"text": "To sum up, our contributions are as follows: We introduce globally normalized convolutional neural networks fora sentence classification task.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.8229813774426779}]}, {"text": "In particular, we present an architecture which allows us to model joint entity and relation classification with a single neural network and classify entities and relations at the same time, normalizing their scores globally.", "labels": [], "entities": [{"text": "joint entity and relation classification", "start_pos": 67, "end_pos": 107, "type": "TASK", "confidence": 0.7463000178337097}]}, {"text": "Our experiments confirm that a CNN with a CRF output layer outperforms a CNN with locally normalized softmax layers.", "labels": [], "entities": []}, {"text": "Our source code is available at http: //cistern.cis.lmu.de.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the \"entity and relation recognition\" (ERR) dataset from (Roth and Yih, 2004) 4 with the train-test split by . We tune the parameters on a held-out part of train.", "labels": [], "entities": [{"text": "entity and relation recognition\" (ERR)", "start_pos": 12, "end_pos": 50, "type": "TASK", "confidence": 0.7823709398508072}]}, {"text": "The data is labeled with entity types and relations (see).", "labels": [], "entities": []}, {"text": "For entity pairs without a relation, we use the label N.", "labels": [], "entities": []}, {"text": "Dataset statistics and model parameters are provided in the appendix.", "labels": [], "entities": []}, {"text": "Following previous work, we compute F 1 of the individual classes for EC and RE, as well as a taskwise macro F 1 score.", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9929967820644379}, {"text": "RE", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.6655237078666687}, {"text": "taskwise macro F 1 score", "start_pos": 94, "end_pos": 118, "type": "METRIC", "confidence": 0.6472617566585541}]}, {"text": "We also report the average of scores across tasks (Avg EC+RE).", "labels": [], "entities": [{"text": "Avg EC+RE)", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9401734828948974}]}, {"text": "Setup 1: Entity Pair Relations.; train separate models for EC and RE on the ERR dataset.", "labels": [], "entities": [{"text": "EC", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.8063547015190125}, {"text": "RE", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.5747328400611877}, {"text": "ERR dataset", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.889583945274353}]}, {"text": "For RE, they only identify relations between named entity pairs.", "labels": [], "entities": [{"text": "RE", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8422417044639587}]}, {"text": "In this setup, the query entities for our model are only named entity pairs.", "labels": [], "entities": []}, {"text": "Note that this facilitates EC in our experiments.", "labels": [], "entities": [{"text": "EC", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.923301100730896}]}, {"text": "Setup 2: shows the table for the example sentence from.", "labels": [], "entities": []}, {"text": "In this setup, each cell (i, j) with i = j is a separate input query to our model.", "labels": [], "entities": []}, {"text": "Our model outputs a prediction for cell (i, j) (the relation between i and j) and predictions for cells (i, i) and (j, j) (the types of i and j).", "labels": [], "entities": []}, {"text": "To fill the diagonal with entity classes, we aggregate all predictions for the particular entity by using majority vote.", "labels": [], "entities": []}, {"text": "Section 4.4 shows that the individual predictions agree with the majority vote in almost all cases.", "labels": [], "entities": []}, {"text": "Setup 3: Without Entity Boundaries.", "labels": [], "entities": []}, {"text": "The table from setup 2 includes one row/column per multi-token entity, utilizing the given entity boundaries of the ERR dataset.", "labels": [], "entities": [{"text": "ERR dataset", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.7294541150331497}]}, {"text": "In order to investigate the impact of the entity boundaries on the classification results, we also consider another table filling setup where we ignore the boundaries and assign one row/column per token.", "labels": [], "entities": [{"text": "table filling", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.7238494902849197}]}, {"text": "Note that this setup is also used by prior work on table filling).", "labels": [], "entities": [{"text": "table filling", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8069120347499847}]}, {"text": "For evaluation, we follow  and score a multi-token entity as correct if at least one of its comprising cells has been classified correctly.", "labels": [], "entities": []}, {"text": "The most important difference between setup 1 and setup 2 is the number of entity pairs with no relation (test set: \u22483k for setup 1, \u2248121k for setup 2).", "labels": [], "entities": []}, {"text": "This makes setup 2 more challenging.", "labels": [], "entities": []}, {"text": "The same holds for setup 3 which considers the same number of entity pairs with no relation as setup 2.", "labels": [], "entities": []}, {"text": "To cope with this, we randomly subsample negative instances in the train set of setup 2 and 3.", "labels": [], "entities": []}, {"text": "Setup 3 considers the most query Figure 3: Entity-relation table entity pairs in total since multi-token entities are split into their comprising tokens.", "labels": [], "entities": []}, {"text": "However, setup 3 represents a more realistic scenario than setup 1 or setup 2 because inmost cases, entity boundaries are not given.", "labels": [], "entities": []}, {"text": "In order to apply setup 1 or 2 to another dataset without entity boundaries, a preprocessing step, such as entity boundary recognition or chunking would be required.", "labels": [], "entities": [{"text": "entity boundary recognition", "start_pos": 107, "end_pos": 134, "type": "TASK", "confidence": 0.6382287939389547}]}, {"text": "shows the results of our globally normalized model in comparison to the same model with locally normalized softmax output layers (one for EC and one for RE).", "labels": [], "entities": []}, {"text": "For setup 1, the CRF layer performs comparable or better than the softmax layer.", "labels": [], "entities": []}, {"text": "For setup 2 and 3, the improvements are more apparent.", "labels": [], "entities": []}, {"text": "We assume that the model can benefit more from global normalization in the case of table filling because it is the more challenging setup.", "labels": [], "entities": [{"text": "global normalization", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.6680392771959305}, {"text": "table filling", "start_pos": 83, "end_pos": 96, "type": "TASK", "confidence": 0.7941960692405701}]}, {"text": "The comparison between setup 2 and setup 3 shows that the entity classification suffers from not given entity boundaries (in setup 3).", "labels": [], "entities": [{"text": "entity classification", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7188371121883392}]}, {"text": "A reason could be that the model cannot convolve the token embeddings of the multi-token entities anymore when computing the entity representation (context B and D in.", "labels": [], "entities": []}, {"text": "Nevertheless, the relation classification performance is comparable in setup 2 and setup 3.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.9248170852661133}]}, {"text": "This shows that the model can internally account for potentially wrong entity classification results due to missing entity boundaries.", "labels": [], "entities": [{"text": "entity classification", "start_pos": 71, "end_pos": 92, "type": "TASK", "confidence": 0.7099882364273071}]}, {"text": "The overall results (Avg EC+RE) of the CRF are better than the results of the softmax layer for all three setups.", "labels": [], "entities": [{"text": "Avg EC+RE)", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9516494035720825}]}, {"text": "To sum up, the improvements of the linear-chain CRF show that (i) joint EC and RE benefits from global normalization and (ii) our way of creating the input sequence for the CRF for joint EC and RE is effective.", "labels": [], "entities": []}, {"text": "Comparison to State of the Art.", "labels": [], "entities": []}, {"text": "Our results are best comparable with (Gupta et al., 2016) since we use the same setup and traintest splits.", "labels": [], "entities": []}, {"text": "However, their model is more complicated with a lot of hand-crafted features and various iterations of modeling dependencies among entity and relation classes.", "labels": [], "entities": []}, {"text": "In contrast, we only use pre-trained word embeddings and train our model end-to-end with only one iteration per entity pair.", "labels": [], "entities": []}, {"text": "When we compare with their model without additional features), our model performs worse for EC but better for RE and comparable for Avg EC+RE.", "labels": [], "entities": [{"text": "EC", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.8495265245437622}, {"text": "RE", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9889048337936401}]}], "tableCaptions": [{"text": " Table 1: F 1 results for entity classification (EC) and relation extraction (RE) in the three setups", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9636419415473938}, {"text": "entity classification (EC)", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8360344469547272}, {"text": "relation extraction (RE)", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8027016639709472}]}, {"text": " Table 2: Comparison to state of the art (S: setup)", "labels": [], "entities": []}]}