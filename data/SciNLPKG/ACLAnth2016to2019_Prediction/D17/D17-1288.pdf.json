{"title": [], "abstractContent": [{"text": "One of the main obstacles for many Digital Humanities projects is the low data availability.", "labels": [], "entities": []}, {"text": "Texts have to be digitized in an expensive and time consuming process whereas Optical Character Recognition (OCR) post-correction is one of the time-critical factors.", "labels": [], "entities": [{"text": "Optical Character Recognition (OCR)", "start_pos": 78, "end_pos": 113, "type": "TASK", "confidence": 0.7020551959673563}]}, {"text": "At the example of OCR post-correction, we show the adaptation of a generic system to solve a specific problem with little data.", "labels": [], "entities": [{"text": "OCR", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9078711867332458}]}, {"text": "The system accounts fora diversity of errors encountered in OCRed texts coming from different time periods in the domain of literature.", "labels": [], "entities": []}, {"text": "We show that the combination of different approaches, such as e.g. Statistical Machine Translation and spell checking , with the help of a ranking mechanism tremendously improves over single-handed approaches.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7393693923950195}, {"text": "spell checking", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.9036575853824615}]}, {"text": "Since we consider the accessibility of the resulting tool as a crucial part of Digital Humanities collaborations , we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual post-correction.", "labels": [], "entities": [{"text": "text recognition", "start_pos": 165, "end_pos": 181, "type": "TASK", "confidence": 0.7752622067928314}]}], "introductionContent": [{"text": "Humanities are no longer just the realm of scholars turning pages of thick books.", "labels": [], "entities": []}, {"text": "As the worlds of humanists and computer scientists begin to intertwine, new methods to revisit known ground emerge and options to widen the scope of research questions are available.", "labels": [], "entities": []}, {"text": "Moreover, the nature of language encountered in such research attracts the attention of the NLP community (,).", "labels": [], "entities": []}, {"text": "Yet, the basic requirement for the successful implementation of such projects often poses a stumbling block: large digital corpora comprising the textual material of interest are rare.", "labels": [], "entities": []}, {"text": "Archives and individual scholars are in the process of improving this situation by applying Optical Character Recognition (OCR) to the physical resources.", "labels": [], "entities": [{"text": "Optical Character Recognition (OCR)", "start_pos": 92, "end_pos": 127, "type": "TASK", "confidence": 0.6633447955052058}]}, {"text": "In the Google Books 1 project books are being digitized on a large scale.", "labels": [], "entities": [{"text": "Google Books 1 project books", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.9601785659790039}]}, {"text": "But even though collections of literary texts like Project Gutenberg 2 exist, these collections often lack the texts of interest to a specific question.", "labels": [], "entities": []}, {"text": "As an example, we describe the compilation of a corpus of adaptations of Goethe's Sorrows of the young Werther which allows for the analysis of character networks throughout the publishing history of this work.", "labels": [], "entities": []}, {"text": "The success of OCR is highly dependent on the quality of the printed source text.", "labels": [], "entities": [{"text": "OCR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7813688516616821}]}, {"text": "Recognition errors, in turn, impact results of computer-aided research ().", "labels": [], "entities": []}, {"text": "Especially for older books set in hard-to-read fonts and with stained paper the output of OCR systems is not good enough to serve as a basis for Digital Humanities (DH) research.", "labels": [], "entities": []}, {"text": "It needs to be post-corrected in a time-consuming and cost-intensive process.", "labels": [], "entities": []}, {"text": "We describe how we support and facilitate the manual post-correction process with the help of informed automatic post-correction.", "labels": [], "entities": []}, {"text": "To account for the problem of relative data sparsity, we illustrate how a generic architecture agnostic to a specific domain can be adjusted to text specificities such as genre and font characteristics by including just small amounts of domain specific data.", "labels": [], "entities": []}, {"text": "We suggest a system architecture (cf.) with trainable modules which joins general and specific problem solving as required in many applications.", "labels": [], "entities": [{"text": "problem solving", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.8041971921920776}]}, {"text": "We show that the combination of modules via a ranking algorithm yields results far above the performance of single approaches.", "labels": [], "entities": []}, {"text": "We discuss the point of departure for our research in Section 2 and introduce the data we base our system on in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we illustrate the most common errors and motivate our multimodular, partly customized architecture.", "labels": [], "entities": []}, {"text": "Section 6 gives an overview of techniques included in our system and the ranking algorithm.", "labels": [], "entities": []}, {"text": "In Section 7, we discuss results, the limitations of automatic post-correction and the influence the amount of training data takes on the performance of such a system.", "labels": [], "entities": []}, {"text": "Finally, Section 8 describes away to efficiently integrate the results of our research into a digitization work-flow as we seethe easy accessibility of computer aid as a central point in Digital Humanities collaborations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe and evaluate our data by means of word error rate (WER) and character error rate (CER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 46, "end_pos": 67, "type": "METRIC", "confidence": 0.9022258818149567}, {"text": "character error rate (CER)", "start_pos": 72, "end_pos": 98, "type": "METRIC", "confidence": 0.9571473598480225}]}, {"text": "The error rates area commonly used metric in speech recognition and machine translation evaluation and can also be referred to as length normalized edit distance.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8094070255756378}, {"text": "machine translation evaluation", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8643091718355814}, {"text": "length normalized edit distance", "start_pos": 130, "end_pos": 161, "type": "METRIC", "confidence": 0.8832616955041885}]}, {"text": "They quantify the number of operations, namely the number of insertions, deletions and substitutions, that are needed to transform the suggested string into the manually corrected string and are computed as follows:   To guarantee diversity, we split each of texts 1-4 (cf.: DTA parallel corpus of OCR text and corrected text showing the number of tokens before and after post-correction along with WER and CER constitution.", "labels": [], "entities": [{"text": "DTA parallel corpus of OCR text", "start_pos": 275, "end_pos": 306, "type": "DATASET", "confidence": 0.7998307049274445}, {"text": "WER", "start_pos": 399, "end_pos": 402, "type": "METRIC", "confidence": 0.9672439694404602}, {"text": "CER constitution", "start_pos": 407, "end_pos": 423, "type": "METRIC", "confidence": 0.92059525847435}]}, {"text": "It constitutes an evaluation in which no initial manual correction as support for the automatic correction is included in the workflow.", "labels": [], "entities": []}, {"text": "We henceforth call this unknown set test unk (text 6).", "labels": [], "entities": []}, {"text": "In contrast, the second set contains parts of the same texts as the training, thus specific vocabulary might have been introduced already.", "labels": [], "entities": []}, {"text": "The results for this test set give a first indication of the extent to which pre-informing the system with manually correcting parts of a text could assist the automatic correction process.", "labels": [], "entities": []}, {"text": "Since this scenario can be described as a text-specific initiated post-correction, we henceforth refer to this test set as test init . We further on experiment with an extended training set train ext (train with texts 7 and 8) to assess the influence of the size of the specific training set on the overall performance.", "labels": [], "entities": []}, {"text": "The sizes of the datasets before and after correction along with WER and CER are summarized in.", "labels": [], "entities": [{"text": "WER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9704283475875854}, {"text": "CER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9745205044746399}]}, {"text": "The sizes for the general dataset before and after correction along with WER and CER are summarized in.", "labels": [], "entities": [{"text": "WER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9745498895645142}, {"text": "CER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9783552885055542}]}, {"text": "In the following we concentrate on the comparison of WER and CER before and after automatic post-correction.", "labels": [], "entities": [{"text": "WER", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9854869842529297}, {"text": "CER", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9724512100219727}]}, {"text": "As a baseline for our system we chose the strongest single-handed module (SMT on character-level trained on Werther data).: WER and CER for both test sets before and after automatic post-correction for the system trained with the small training set (train) and the larger training set (train ext ).", "labels": [], "entities": [{"text": "SMT", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.8193867802619934}, {"text": "Werther data", "start_pos": 108, "end_pos": 120, "type": "DATASET", "confidence": 0.8769346177577972}, {"text": "WER", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9992589354515076}, {"text": "CER", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.997395396232605}]}, {"text": "Baselines: the original text coming from the OCR system and the character-level SMT system trained on the Werther data.", "labels": [], "entities": [{"text": "OCR system", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9027137756347656}, {"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8508564233779907}, {"text": "Werther data", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.9293765723705292}]}, {"text": "Overall performance As indicated previously, our test sets differ with respect to their similarity to the training set.", "labels": [], "entities": []}, {"text": "The results for both test scenarios for systems trained on our two training sets are summarized in.", "labels": [], "entities": []}, {"text": "The results from test init and test unk show that our system performs considerably better than the baseline and can improve quality of the OCR output considerably.", "labels": [], "entities": []}, {"text": "For test unk , the system improves the quality by almost 20 points of WER from 36.7 to 15.4 and over 10 points in CER from 30.0 to 19.6.", "labels": [], "entities": [{"text": "quality", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9717560410499573}, {"text": "WER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9955641031265259}, {"text": "CER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.8809115290641785}]}, {"text": "For test init , our system improves the quality of the text with a reduction of approximately 20 points of WER from 23.5 to 4.7 and 7 points in CER from 15.1 to 8.0.", "labels": [], "entities": [{"text": "WER", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9972776770591736}, {"text": "CER", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.8883881568908691}]}, {"text": "It is not surprising that the decrease in WER is stronger than the decrease in CER.", "labels": [], "entities": [{"text": "WER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9968246221542358}, {"text": "CER", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9904592037200928}]}, {"text": "This is due to the fact that many words contain more than one error and require more than one character level operation to get from the incorrect to the correct string.", "labels": [], "entities": []}, {"text": "Just slight improvement can be shown by adding training material to the Werther-specific parts of the system (cf. train ext row of).", "labels": [], "entities": [{"text": "Werther-specific", "start_pos": 72, "end_pos": 88, "type": "DATASET", "confidence": 0.9001864790916443}]}, {"text": "Merely the CER can be improved whereas the WER stays about the same.", "labels": [], "entities": [{"text": "CER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9981496334075928}, {"text": "WER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9962080717086792}]}, {"text": "The improvement in test unk is higher than for test init . Module specific analysis Since a WER and CER evaluation is not expedient for all modules as they were designed to correct specific problems and not the entirety of them, we look into the specialized modules in terms of correct suggestions contributed to the suggestion pool and correct suggestions only suggested by one module (unique suggestions).", "labels": [], "entities": [{"text": "WER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.75733482837677}, {"text": "CER", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.8136098384857178}]}, {"text": "As the system including the extended training set train ext delivered slightly better results, in the following we will describe the contribution of the single modules testinit test unk module # overcorrected # corrected # unique correct # overcorrected # corrected # unique correct 2,642 - to the overall performance of this system (cf.).", "labels": [], "entities": []}, {"text": "For test unk the number of corrected tokens along with the number of overcorrections is higher than for test init throughout all modules.", "labels": [], "entities": []}, {"text": "Clearly, for test init the Werther-specific modules are strongest.", "labels": [], "entities": []}, {"text": "The more general modules prove useful for test unk . The number of corrected words increases for the SMT module trained on DTA data on character-level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.989884614944458}, {"text": "DTA data", "start_pos": 123, "end_pos": 131, "type": "DATASET", "confidence": 0.8033961951732635}]}, {"text": "The usefulness of the module extracting specific words (text-internal vocab) as well as the general SMT model and the spellchecker becomes evident in terms of unique suggestions contributed by those modules.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9830272793769836}]}, {"text": "The analysis of the output of the individual modules and their contribution to the overall system uncovers an issue: those modules that produce a high number of incorrect suggestions, thus overcorrecting actually correct input tokens, are at the same time those modules that are the only ones producing correct suggestions for some of the incorrect input words.", "labels": [], "entities": []}, {"text": "Consequently, those uniquely suggested corrections are not chosen in the decision modules due to an overall weak performance of this module.", "labels": [], "entities": []}, {"text": "These suggestions are often crucial to the texts like the suggestions by the special vocabulary module which contain named entities or words specific to the time period.", "labels": [], "entities": []}, {"text": "For our test unk set, the text-internal vocabulary module yields around 60 unique suggestions, out of which 15 are names (Friedrich, Amalia) or words really specific to the text (Auftrit spelled with one t instead of two).", "labels": [], "entities": []}, {"text": "Challenges In the context of literature OCR post-correction is a challenging problem since the texts themselves can be considered non-standard text.", "labels": [], "entities": [{"text": "OCR post-correction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.873347669839859}]}, {"text": "The aim is not to bring the text at hand to an agreed upon standard form but to digitize exactly what was contained in the print version.", "labels": [], "entities": []}, {"text": "This can be far from the standard form of a language.", "labels": [], "entities": []}, {"text": "In one of our texts, we find a character speaking German with a strong dialect.", "labels": [], "entities": []}, {"text": "Her speech contains a lot of words that are incorrect in standard German, however, the goal is it to preserve this \"errors\" in the digital version.", "labels": [], "entities": []}, {"text": "Thus, correction merely on the basis of the OCR text without consulting the printed version or an image-digitized facsimile, can essentially never be perfect.", "labels": [], "entities": [{"text": "OCR text", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.867177277803421}]}, {"text": "It follows, that the integration of automatic post-correction techniques into the character recognition process could lead to further improvements.", "labels": [], "entities": [{"text": "character recognition process", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.9425661762555441}]}], "tableCaptions": [{"text": " Table 1: Werther texts included in our corpus from different authors and times of origin.", "labels": [], "entities": [{"text": "Werther texts", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.8234080076217651}]}, {"text": " Table 2: Werther specific parallel corpus of OCR  text and corrected text showing the number of to- kens before and after post-correction along with  WER and CER", "labels": [], "entities": [{"text": "WER", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9387657642364502}, {"text": "CER", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.8523447513580322}]}, {"text": " Table 3: DTA parallel corpus of OCR text and  corrected text showing the number of tokens be- fore and after post-correction along with WER and  CER", "labels": [], "entities": [{"text": "WER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9187828898429871}, {"text": "CER", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.8141464591026306}]}, {"text": " Table 4: WER and CER for both test sets be- fore and after automatic post-correction for the  system trained with the small training set (train)  and the larger training set (train ext ). Baselines:  the original text coming from the OCR system  and the character-level SMT system trained on the  Werther data.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9976600408554077}, {"text": "CER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9845343828201294}, {"text": "OCR system", "start_pos": 235, "end_pos": 245, "type": "DATASET", "confidence": 0.9576500058174133}, {"text": "SMT", "start_pos": 271, "end_pos": 274, "type": "TASK", "confidence": 0.8974229693412781}, {"text": "Werther data", "start_pos": 298, "end_pos": 310, "type": "DATASET", "confidence": 0.9553131759166718}]}, {"text": " Table 6: Number of tokens in the English and French corpus provided by the competition on OCR- postcorrection.", "labels": [], "entities": [{"text": "English and French corpus", "start_pos": 34, "end_pos": 59, "type": "DATASET", "confidence": 0.6621780544519424}, {"text": "OCR- postcorrection", "start_pos": 91, "end_pos": 110, "type": "DATASET", "confidence": 0.8571633696556091}]}, {"text": " Table 7: The results reported in word error rate  (WER) and character error rate (CER) for the En- glish and French test set.", "labels": [], "entities": [{"text": "word error rate  (WER)", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.903534730275472}, {"text": "character error rate (CER)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.9337865312894186}, {"text": "En- glish and French test set", "start_pos": 96, "end_pos": 125, "type": "DATASET", "confidence": 0.8793482865606036}]}]}