{"title": [{"text": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "labels": [], "entities": [{"text": "Text Generation", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7561215460300446}]}], "abstractContent": [{"text": "In this paper we explore the effect of architectural choices on learning a variational autoencoder (VAE) for text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7740717530250549}]}, {"text": "In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvo-lutional components with a recurrent language model.", "labels": [], "entities": []}, {"text": "Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly , it helps to avoid the issue of the VAE collapsing to a deterministic model.", "labels": [], "entities": [{"text": "VAE", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.6804890036582947}]}], "introductionContent": [{"text": "Generative models of texts are currently at the cornerstone of natural language understanding enabling recent breakthroughs in machine translation (, dialogue modelling, abstractive summarization (, etc.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.6601266364256541}, {"text": "machine translation", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7629098296165466}, {"text": "dialogue modelling", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8540184795856476}, {"text": "abstractive summarization", "start_pos": 170, "end_pos": 195, "type": "TASK", "confidence": 0.5175063014030457}]}, {"text": "Currently, RNN-based generative models hold state-of-the-art results in both unconditional) and conditional () text generation.", "labels": [], "entities": [{"text": "RNN-based generative", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.8113133013248444}, {"text": "text generation", "start_pos": 111, "end_pos": 126, "type": "TASK", "confidence": 0.7591927647590637}]}, {"text": "At a high level, these models represent a class of autoregressive models that work by generating outputs sequentially one step at a time where the next predicted element is conditioned on the history of elements generated thus far.", "labels": [], "entities": []}, {"text": "Variational autoencoders (VAE), recently introduced by), offer a different approach to generative modeling by integrating stochastic latent variables into the conventional autoencoder architecture.", "labels": [], "entities": [{"text": "generative modeling", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.9831618070602417}]}, {"text": "The primary purpose of learning VAE-based generative models is to be able to generate realistic examples as if they were drawn from the input data distribution by simply feeding noise vectors through the decoder.", "labels": [], "entities": [{"text": "VAE-based generative", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.8628756701946259}]}, {"text": "Additionally, the latent representations obtained by applying the encoder to input examples give a fine-grained control over the generation process that is harder to achieve with more conventional autoregressive models.", "labels": [], "entities": []}, {"text": "Similar to compelling examples from image generation, where it is possible to condition generated human faces on various attributes such as hair, skin color and style, in text generation it should be possible to also control various attributes of the generated sentences, such as, for example, sentiment or writing style.", "labels": [], "entities": [{"text": "text generation", "start_pos": 171, "end_pos": 186, "type": "TASK", "confidence": 0.69581039249897}]}, {"text": "While training VAE-based models seems to pose little difficulty when applied to the tasks of generating natural images) and speech (, their application to natural text generation requires additional care.", "labels": [], "entities": []}, {"text": "As discussed by, the core difficulty of training VAE models is the collapse of the latent loss (represented by the KL divergence term) to zero.", "labels": [], "entities": [{"text": "latent loss", "start_pos": 83, "end_pos": 94, "type": "METRIC", "confidence": 0.9263493120670319}, {"text": "KL divergence term)", "start_pos": 115, "end_pos": 134, "type": "METRIC", "confidence": 0.8331542313098907}]}, {"text": "In this case the generator tends to completely ignore latent representations and reduces to a standard language model.", "labels": [], "entities": []}, {"text": "This is largely due to the high modeling power of the RNN-based decoders which with sufficiently small history can achieve low reconstruction errors while not relying on the latent vector provided by the encoder.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel VAE model for texts that is more effective at forcing the decoder to make use of latent vectors.", "labels": [], "entities": [{"text": "VAE", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.5420376658439636}]}, {"text": "Contrary to existing work, where both encoder and decoder layers are LSTMs, the core of our model is a feed-forward architecture composed of one-dimensional convolutional and deconvolutional layers.", "labels": [], "entities": []}, {"text": "This choice of architecture helps to gain more control over the KL term, which is crucial for training a VAE model.", "labels": [], "entities": []}, {"text": "Given the difficulty of generating long sequences in a fully feed-forward manner, we augment our network with an RNN language model layer.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this paper is the first work that successfully applies deconvolutions in the decoder of a latent variable generative model of natural text.", "labels": [], "entities": [{"text": "latent variable generative model of natural text", "start_pos": 120, "end_pos": 168, "type": "TASK", "confidence": 0.7524879319327218}]}, {"text": "We empirically verify that our model is easier to train than its fully recurrent alternative, which, in our experiments, fails to converge on longer texts.", "labels": [], "entities": []}, {"text": "To better understand why training VAEs for texts is difficult we carryout detailed experiments, discuss optimization difficulties, and propose effective ways to address them.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that sampling from our model yields realistic texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use KL term annealing and input dropout when training the LSTM VAE models from and KL term annealing and regularized objective function from Eq (3) when training our models.", "labels": [], "entities": [{"text": "LSTM VAE", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.46672628819942474}]}, {"text": "All models were trained with the Adam optimization algorithm) with decaying learning rate.", "labels": [], "entities": []}, {"text": "We use Layer Normalization () in LSTM layers and Batch Normalization () in convolutional and deconvolutional layers.", "labels": [], "entities": []}, {"text": "To make our results easy to reproduce we have released the source code of all our experiments 1 . Data.", "labels": [], "entities": []}, {"text": "Our first task is character-level language generation performed on the standard Penn Treebank dataset.", "labels": [], "entities": [{"text": "character-level language generation", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.7026414374510447}, {"text": "Penn Treebank dataset", "start_pos": 80, "end_pos": 101, "type": "DATASET", "confidence": 0.9958756764729818}]}, {"text": "One of the goals is to test the ability of the models to successfully learn the representations of long sequences.", "labels": [], "entities": []}, {"text": "For training, fixed-size data samples are selected from random positions in the standard training and validation sets.", "labels": [], "entities": []}], "tableCaptions": []}