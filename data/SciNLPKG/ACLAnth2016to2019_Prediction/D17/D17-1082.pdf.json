{"title": [{"text": "RACE: Large-scale ReAding Comprehension Dataset From Examinations", "labels": [], "entities": [{"text": "RACE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9412162899971008}, {"text": "ReAding Comprehension", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.8196736872196198}]}], "abstractContent": [{"text": "We present RACE, anew dataset for benchmark evaluation of methods in the reading comprehension task.", "labels": [], "entities": [{"text": "RACE", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.5914120674133301}]}, {"text": "Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning.", "labels": [], "entities": [{"text": "RACE", "start_pos": 112, "end_pos": 116, "type": "TASK", "confidence": 0.9373830556869507}]}, {"text": "In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%).", "labels": [], "entities": [{"text": "RACE", "start_pos": 85, "end_pos": 89, "type": "TASK", "confidence": 0.9597066640853882}]}, {"text": "We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension.", "labels": [], "entities": []}, {"text": "The dataset is freely available at", "labels": [], "entities": []}], "introductionContent": [{"text": "Constructing an intelligence agent capable of understanding text as people is the major challenge of NLP research.", "labels": [], "entities": []}, {"text": "With recent advances in deep learning techniques, it seems possible to achieve human-level performance in certain language understanding tasks, and a surge of effort has been devoted to the machine comprehension task where people aim to construct a system with the ability to \u21e4 * indicates equal contribution answer questions related to a document that it has to comprehend.", "labels": [], "entities": [{"text": "language understanding tasks", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.798885186513265}]}, {"text": "Towards this goal, several large-scale datasets ( have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance.", "labels": [], "entities": []}, {"text": "While having a suitable dataset is crucial for evaluating the system's true ability in reading comprehension, the existing datasets suffer several critical limitations.", "labels": [], "entities": []}, {"text": "Firstly, in all datasets, the candidate options are directly extracted from the context (as a single entity or a text span), which leads to the fact that lots of questions can be solved trivially via word-based search and context-matching without deeper reasoning; this constrains the types of questions as well.", "labels": [], "entities": []}, {"text": "Secondly, answers and questions of most datasets are either crowd-sourced or automatically-generated, bringing a significant amount of noises in the datasets and limits the ceiling performance by domain experts, such as 82% for Childrens Book Test and 84% for Who-didWhat.", "labels": [], "entities": [{"text": "Childrens Book Test", "start_pos": 228, "end_pos": 247, "type": "DATASET", "confidence": 0.9404486219088236}]}, {"text": "Yet another issue in existing datasets is that the topic coverages are often biased due to the specific ways that the data were initially collected, making it hard to evaluate the ability of systems in text comprehension over a broader range of topics.", "labels": [], "entities": []}, {"text": "To address the aforementioned limitations, we constructed anew dataset by collecting a large set of questions, answers and associated passages in the English exams for middle-school and high-school Chinese students within the 12-18 age range.", "labels": [], "entities": []}, {"text": "Those exams were designed by domain experts (instructors) for evaluating the reading comprehension ability of students, with ensured quality and broad topic coverage.", "labels": [], "entities": []}, {"text": "Furthermore, the answers by machines or by humans can be objectively graded for evaluation and comparison using the same evaluation metrics.", "labels": [], "entities": []}, {"text": "Although efforts have been made with a similar motivation, including the MCTest dataset created by () (containing 500 passages and 2000 questions) and several others, the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets.", "labels": [], "entities": [{"text": "MCTest dataset created", "start_pos": 73, "end_pos": 95, "type": "DATASET", "confidence": 0.9465602437655131}]}, {"text": "Our new dataset, namely RACE, consists of 27,933 passages and 97,687 questions.", "labels": [], "entities": [{"text": "RACE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.569213330745697}]}, {"text": "After reading each passage, each student is asked to answer several questions where each question is provided with four candidate answers -only one of them is correct . Unlike existing datasets, both the questions and candidate answers in RACE are not restricted to be the text spans in the original passage; instead, they can be described in any words.", "labels": [], "entities": [{"text": "RACE", "start_pos": 239, "end_pos": 243, "type": "TASK", "confidence": 0.8664902448654175}]}, {"text": "A sample from our dataset is presented in.", "labels": [], "entities": []}, {"text": "Our latter analysis shows that correctly answering a large portion of questions in RACE requires the ability of reasoning, the most important feature as a machine comprehension dataset.", "labels": [], "entities": [{"text": "RACE", "start_pos": 83, "end_pos": 87, "type": "TASK", "confidence": 0.9829826951026917}]}, {"text": "RACE also offers two important subdivisions of the reasoning types in its questions, namely passage summarization and attitude analysis, which have not been introduced by the any of the existing large-scale datasets to our knowledge.", "labels": [], "entities": [{"text": "RACE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8594464063644409}, {"text": "passage summarization", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.825859397649765}, {"text": "attitude analysis", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.8070791363716125}]}, {"text": "In addition, compared to other existing datasets where passages are either domain-specific or of a single fixed style (namely news stories for CNN/-Daily Mail, NEWSQA and Who-did-What, fiction stories for Children's Book Test and Book Test, and Wikipedia articles for SQUAD), passages in RACE almost coverall types of human articles, such as news, stories, ads, biography, philosophy, etc., in a variety of styles.", "labels": [], "entities": [{"text": "CNN/-Daily Mail", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.8481526374816895}, {"text": "Book Test", "start_pos": 230, "end_pos": 239, "type": "DATASET", "confidence": 0.8508264124393463}, {"text": "RACE", "start_pos": 288, "end_pos": 292, "type": "TASK", "confidence": 0.9504262208938599}]}, {"text": "This comprehensiveness of topic/style coverage makes RACE a desirable resource for evaluating the reading comprehension ability of machine learning systems in general.", "labels": [], "entities": [{"text": "RACE", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.983511209487915}]}, {"text": "The advantages of our proposed dataset over existing large datasets in machine reading comprehension can be summarized as follows: \u2022 All questions and candidate options are generated by human experts, which are intentionally designed to test human agent's ability in reading comprehension.", "labels": [], "entities": []}, {"text": "This makes RACE a relatively accurate indicator for reflecting the text comprehension ability of machine learning systems under human judge.", "labels": [], "entities": [{"text": "RACE", "start_pos": 11, "end_pos": 15, "type": "TASK", "confidence": 0.9695853590965271}]}, {"text": "\u2022 The questions are substantially more difficult than those in existing datasets, in terms of the large portion of questions involving reasoning.", "labels": [], "entities": []}, {"text": "At the meantime, it is also sufficiently large to support the training of deep learning models.", "labels": [], "entities": []}, {"text": "\u2022 Unlike existing large-scale datasets, candidate options in RACE are human generated sentences which may not appear in the original passage.", "labels": [], "entities": [{"text": "RACE", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.9827221632003784}]}, {"text": "This makes the task more challenging and allows a rich type of questions such as passage summarization and attitude analysis.", "labels": [], "entities": [{"text": "passage summarization", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.9321357905864716}, {"text": "attitude analysis", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.9043954312801361}]}, {"text": "\u2022 Broad coverage in various domains and writing styles: a desirable property for evaluating generic (in contrast to domain/style-specific) comprehension ability of learning models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The past few years have witnessed several largescale cloze-style datasets (, whose questions are formulated by obliterating a word or an entity in a sentence.", "labels": [], "entities": []}, {"text": "In datasets such as SQUAD (), NEWSQA () and MS MARCO (), the answer to each question is in the form of a text span in the article.", "labels": [], "entities": [{"text": "NEWSQA", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.9147270917892456}, {"text": "MS", "start_pos": 44, "end_pos": 46, "type": "DATASET", "confidence": 0.9500017762184143}, {"text": "MARCO", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.5708251595497131}]}, {"text": "Articles of SQUAD, NEWSQA and MS MARCO come from Wikipedia, CNN news and the Bing search engine respectively.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.8267698884010315}, {"text": "NEWSQA", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.8715742826461792}, {"text": "MS", "start_pos": 30, "end_pos": 32, "type": "DATASET", "confidence": 0.8831530213356018}, {"text": "MARCO", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.7488355040550232}, {"text": "CNN news", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.8888136744499207}]}, {"text": "The answer to a certain question may not be unique and could be multiple spans.", "labels": [], "entities": []}, {"text": "Instead of evaluating the accuracy, researchers need to use F1 score, BLEU () or ROUGE ( as metrics, which measure the overlap between the prediction and ground truth answers since the questions come without candidate spans.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994739890098572}, {"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9926575422286987}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9972659349441528}, {"text": "ROUGE", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.997491717338562}]}, {"text": "Datasets with span-based answers are challenging as the space of possible spans is usually large.", "labels": [], "entities": []}, {"text": "However, restricting answers to be text spans in the context passage maybe unrealistic and more importantly, may not be intuitive even for humans, indicated by the suffered human performance of 80.3% on SQUAD (or 65% claimed by) and 46.5% on NEWSQA.", "labels": [], "entities": [{"text": "SQUAD", "start_pos": 203, "end_pos": 208, "type": "DATASET", "confidence": 0.8360515832901001}, {"text": "NEWSQA", "start_pos": 242, "end_pos": 248, "type": "DATASET", "confidence": 0.979248046875}]}, {"text": "In other words, the format of span-based answers may not necessarily be a good examination of reading comprehension of machines whose aim is to approach the comprehension ability of humans.", "labels": [], "entities": []}, {"text": "There have been several datasets extracted from examinations, aiming at evaluating systems under the same conditions as how humans are evaluated in schools.", "labels": [], "entities": []}, {"text": "E.g., the AI2 Elementary School Science Questions dataset ( contains 1080 questions for students in elementary schools; NTCIR QA Lab () evaluates systems by the task of solving real-world university entrance exam questions; The Entrance Exams task at CLEF QA Track () evaluates the system's reading comprehension ability.", "labels": [], "entities": [{"text": "AI2 Elementary School Science Questions dataset", "start_pos": 10, "end_pos": 57, "type": "DATASET", "confidence": 0.8369925121466318}, {"text": "NTCIR QA Lab", "start_pos": 120, "end_pos": 132, "type": "DATASET", "confidence": 0.8432673414548238}, {"text": "CLEF QA Track", "start_pos": 251, "end_pos": 264, "type": "DATASET", "confidence": 0.911789079507192}]}, {"text": "However, data provided in these existing tasks are far from sufficient for the training of advanced data-driven machine reading models, partially due to the expensive data generation process by human experts.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7023723870515823}]}, {"text": "To the best of our knowledge, RACE is the first large-scale dataset of this type, where questions are created based on exams designed to evaluate human performance in reading comprehension.", "labels": [], "entities": [{"text": "RACE", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9843890070915222}]}, {"text": "As mentioned in section 1, RACE is collected from English examinations designed for 12-15 year-old middle school students, and 15-18 yearold high school students in China.", "labels": [], "entities": [{"text": "RACE", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.8975309729576111}]}, {"text": "To distinguish the two subgroups with drastic difficulty gap, RACE-M denotes the middle school examinations and RACE-H denotes high school examinations.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9129653573036194}, {"text": "RACE-H", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.6399730443954468}]}, {"text": "We split 5% data as the development set and 5% as the test set for RACE-M and RACE-H respectively.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.5037450194358826}, {"text": "RACE-H", "start_pos": 78, "end_pos": 84, "type": "DATASET", "confidence": 0.6016703844070435}]}, {"text": "The number of samples in each set is shown in.", "labels": [], "entities": []}, {"text": "The statistics for RACE-M and RACE-H is summarized in.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 19, "end_pos": 25, "type": "TASK", "confidence": 0.623167872428894}, {"text": "RACE-H", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.610846221446991}]}, {"text": "We can find that the length of the passages and the vocabulary size in the RACE-H are much larger than that of the RACE-M, an evidence of the higher difficulty of high school examinations.", "labels": [], "entities": [{"text": "RACE-H", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.8392294049263}, {"text": "RACE-M", "start_pos": 115, "end_pos": 121, "type": "DATASET", "confidence": 0.7709069848060608}]}, {"text": "However, notice that since the articles and questions are selected and designed to test Chinese students learning English as a foreign language, the vocabulary size and the complexity of the language constructs are simpler than news articles and Wikipedia articles in other QA datasets.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 274, "end_pos": 285, "type": "DATASET", "confidence": 0.8237614929676056}]}, {"text": "As described in section 3.2, a randomly sampled subset of test set has been labeled by Amazon Turkers, which contains 500 questions with half from RACE-H and with the other half from RACE-M.", "labels": [], "entities": [{"text": "Amazon Turkers", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9138974249362946}, {"text": "RACE-M", "start_pos": 183, "end_pos": 189, "type": "DATASET", "confidence": 0.8612841367721558}]}, {"text": "The turkers' performance is 85% for RACE-M and 70% for RACE-H.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.5794103145599365}, {"text": "RACE-H", "start_pos": 55, "end_pos": 61, "type": "DATASET", "confidence": 0.8183308243751526}]}, {"text": "However, it is hard to guarantee that every turker performs the survey carefully, given the difficult and long passages of high school problems.", "labels": [], "entities": []}, {"text": "Therefore, to obtain the ceiling human performance on RACE, we manually labeled the proportion of valid questions.", "labels": [], "entities": [{"text": "RACE", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.8788836002349854}]}, {"text": "A question is valid if it is unambiguous and has a correct answer.", "labels": [], "entities": []}, {"text": "We found that 94.5% of the data is valid, which sets the ceiling human performance.", "labels": [], "entities": []}, {"text": "Similarly, the ceiling performance on RACE-M and RACE-H is 95.4% and 94.2% respectively.", "labels": [], "entities": [{"text": "RACE-M", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.49429112672805786}, {"text": "RACE-H", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.670667827129364}]}], "tableCaptions": [{"text": " Table 1: Sample reading comprehension problems from our dataset.", "labels": [], "entities": [{"text": "Sample reading comprehension", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9192197124163309}]}, {"text": " Table 2: The separation of the training, development and test sets of RACE-M,RACE-H and RACE", "labels": [], "entities": [{"text": "RACE-M,RACE-H", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.6245501041412354}, {"text": "RACE", "start_pos": 89, "end_pos": 93, "type": "TASK", "confidence": 0.40384384989738464}]}, {"text": " Table 3: Statistics of RACE where Len denotes  length and Vocab denotes Vocabulary.", "labels": [], "entities": [{"text": "RACE", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.9014627933502197}, {"text": "Len", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.947223424911499}, {"text": "length", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9711849689483643}, {"text": "Vocab", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.8735185861587524}]}, {"text": " Table 4: Statistic information about Reasoning type in different datasets. * denotes the numbers coming  from (Trischler et al., 2016) based on 1000 samples per dataset, and numbers with  \u2020 come from (Chen  et al., 2016).", "labels": [], "entities": []}, {"text": " Table 5: Accuracy of models and human on the each dataset, where  \u2020 denotes the results coming from  previous publications. DM denotes Daily Mail and WDW denotes Who-Did-What .", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976801872253418}, {"text": "Daily Mail", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.9268919825553894}]}]}