{"title": [], "abstractContent": [{"text": "One of the most pressing issues in dis-continuous constituency transition-based parsing is that the relevant information for parsing decisions could be located in any part of the stack or the buffer.", "labels": [], "entities": [{"text": "constituency transition-based parsing", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.511240541934967}, {"text": "parsing decisions", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.9090555310249329}]}, {"text": "In this paper , we propose a solution to this problem by replacing the structured percep-tron model with a recursive neural model that computes a global representation of the configuration, therefore allowing even the most remote parts of the configuration to influence the parsing decisions.", "labels": [], "entities": [{"text": "parsing", "start_pos": 274, "end_pos": 281, "type": "TASK", "confidence": 0.9659169316291809}]}, {"text": "We also provide a detailed analysis of how this representation should be built out of sub-representations of its core elements (words, trees and stack).", "labels": [], "entities": []}, {"text": "Additionally, we investigate how different types of swap oracles influence the results.", "labels": [], "entities": []}, {"text": "Our model is the first neural discontinuous constituency parser, and it outperforms all the previously published models on three out of four datasets while on the fourth it obtains second place by a tiny difference.", "labels": [], "entities": [{"text": "neural discontinuous constituency parser", "start_pos": 23, "end_pos": 63, "type": "TASK", "confidence": 0.6963554993271828}]}], "introductionContent": [{"text": "Research on constituency parsing has been mostly concentrated on projective trees, which can be modeled with Context-Free Grammars (CFGs).", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8974804878234863}]}, {"text": "One of the main reasons for this is that modeling non-projective trees often requires richer grammar formalisms, which in practice implies slower runtime.", "labels": [], "entities": []}, {"text": "For instance, the parsing algorithms for binary LCFRS-the most prominent grammarbased approach to parsing non-projective constituency trees-have computational complexity O(n 3k ), where k is the fan-out of the grammar.", "labels": [], "entities": [{"text": "parsing non-projective constituency trees-have computational complexity O", "start_pos": 98, "end_pos": 171, "type": "TASK", "confidence": 0.7934885323047638}]}, {"text": "For this reason, researchers turned to faster approximate methods.", "labels": [], "entities": []}, {"text": "Approximations can be done in two ways: either on the types of structures that are predicted or on the parsing algorithm.", "labels": [], "entities": []}, {"text": "The first approach approximates discontinuous constituency structures with simpler structures for which more efficient algorithms exist.", "labels": [], "entities": []}, {"text": "This method works as a pipeline: it converts the input to a simpler formalism, parses with it, and then converts it back.", "labels": [], "entities": []}, {"text": "Relevant examples are the parsers by and, who convert discontinuous constituents to dependencies, and, who also applied a conversion but in this case to the projective constituency trees.", "labels": [], "entities": []}, {"text": "The second approach-approximation on the parsing algorithm-consists of an approximate search for the most probable parse.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9702966213226318}]}, {"text": "This is analogous to the search done by transition-based parsers, which greedily search through the space of all possible parses, resulting in very fast models.", "labels": [], "entities": []}, {"text": "The first transition-based discontinuous constituency parser of this sort was presented in, and it consists of a shift-reduce parser that handles discontinuities with swap transitions.", "labels": [], "entities": []}, {"text": "This parser was very similar to dependency parsers with swap transitions), but unlike its dependency equivalents, it did not exhibit higher accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.997209370136261}]}, {"text": "Later work on discontinuous transition-based parsing was largely focused on finding alternative transitioning systems to handle discontinuity. and proposed new types of swap operations (CompoundSwap and SkipShift) to make the transition sequences shorter-and therefore easier to learn.", "labels": [], "entities": [{"text": "discontinuous transition-based parsing", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.5373576482137045}]}, {"text": "went even further by modifying not only the transitions but the whole configuration structure by introducing an additional stack.", "labels": [], "entities": []}, {"text": "Over the years the transitioning system has seen some progress, but the learning model has remained the same : a sparse linear model trained with structured perceptron and early update strategy.", "labels": [], "entities": []}, {"text": "This model requires heavy feature engineering and has a limited capacity in modeling interaction between the features.", "labels": [], "entities": []}, {"text": "argue that one of the biggest problems of transition based systems is precisely their greedy search, because they cannot recover from the bad decisions made in earlier parsing steps.", "labels": [], "entities": []}, {"text": "Some researchers try to account for this problem by increasing the beam size, but there is a limit on how much the beam can be increased while remaining efficient for practical use.", "labels": [], "entities": []}, {"text": "The solution we propose is to use a probabilistic model that exploits the information from the whole configuration structure when making the decision for the next action.", "labels": [], "entities": []}, {"text": "This can be achieved by using recurrent neural models that allow information to flow all the way from the individual characters, up trough the words, POS tags, subtrees, stack and buffer until the final configuration representation.", "labels": [], "entities": []}, {"text": "Thanks to using a neural network model, which removes the need for feature engineering, we can concentrate on the question of which representations are more relevant for the model at each step of the flow.", "labels": [], "entities": []}, {"text": "Thus, we reflect on how alternative representations should impact the task, and we report their relative contribution in an ablation study.", "labels": [], "entities": []}, {"text": "In our work, we also reduce the number of swap transitions by trying to postpone them as much as possible, in a style similar to the lazy-swap used in  -albeit with an even lower number of swaps.", "labels": [], "entities": []}, {"text": "This change influences the model indirectly by introducing a helpful inductive bias.", "labels": [], "entities": []}, {"text": "Our model gets state-of-the-art results on Negra, Negra-30 and TigerSPMRL datasets, and on the TigerHN achieves the second best published result.", "labels": [], "entities": [{"text": "Negra", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9558095335960388}, {"text": "Negra-30", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7576516270637512}, {"text": "TigerSPMRL datasets", "start_pos": 63, "end_pos": 82, "type": "DATASET", "confidence": 0.9363732933998108}, {"text": "TigerHN", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.9804276823997498}]}, {"text": "To the best of our knowledge this is the first work that uses neural networks in the context of discontinuous constituency parsing.", "labels": [], "entities": [{"text": "discontinuous constituency parsing", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.6591779490311941}]}], "datasetContent": [{"text": "We empirically test the performance of our parser on two German constituency treebanks: Negra and Tiger.", "labels": [], "entities": [{"text": "German constituency treebanks", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.7662980159123739}, {"text": "Negra", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.8480640053749084}]}, {"text": "The preprocessing applied to these treebanks follows the same methods used in other discontinuous constituency parsing literature, as described in Maier (2015) and implemented in the tree-tools software 2 . We use two different versions of the Negra treebank.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.6814824640750885}, {"text": "Negra treebank", "start_pos": 244, "end_pos": 258, "type": "DATASET", "confidence": 0.964432954788208}]}, {"text": "The first version is filtered for the sentences up to 30 words, in order to remain comparable to previous grammar-based models; the second version includes sentences of all lengths.", "labels": [], "entities": []}, {"text": "As for the Tiger treebank, we use two different splits:.", "labels": [], "entities": [{"text": "Tiger treebank", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.9923745691776276}]}, {"text": "We evaluated the model with the evaluation module of discodop 3 parser.", "labels": [], "entities": []}, {"text": "Our model is implemented with DyNet (Neubig et al., 2017) and the code is available at https://github.com/stanojevic/ BadParser.", "labels": [], "entities": []}, {"text": "The concrete hyper-parameters of our model are shown in.", "labels": [], "entities": []}, {"text": "We optimize the parameters with Adam optimizer on the training set, for 10 iterations with 100 random restarts, and we do model selection on the validation set for the F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 168, "end_pos": 175, "type": "METRIC", "confidence": 0.9414944052696228}]}, {"text": "During test time we use beam search with beam of size 16.", "labels": [], "entities": []}, {"text": "We conducted the development of our model on the TigerHN train and development sets.", "labels": [], "entities": [{"text": "TigerHN train", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9902373850345612}]}, {"text": "First we will analyze the effect of different model design decisions and then we show the results over the test set.", "labels": [], "entities": []}, {"text": "The development set scores on TigerHN are shown in.", "labels": [], "entities": [{"text": "TigerHN", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9837905764579773}]}], "tableCaptions": [{"text": " Table 1: Hyper-parameters of the model", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9623051881790161}]}, {"text": " Table 2: Average number of swaps and jump sizes  per sentence", "labels": [], "entities": [{"text": "Average number of swaps and jump sizes", "start_pos": 10, "end_pos": 48, "type": "METRIC", "confidence": 0.7528231399399894}]}, {"text": " Table 3: Precision (P), Recall (R), F-score (F) and Exact (E), for our best model and ablated versions.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9445944428443909}, {"text": "Recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9578457027673721}, {"text": "F-score (F)", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9608015418052673}, {"text": "Exact (E)", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9733233749866486}]}, {"text": " Table 4: Final results on test set, computed with discodop evaluation module. Trained on Negra-All.  Evaluated with SPRML scripts.", "labels": [], "entities": [{"text": "Negra-All", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.9021041393280029}]}, {"text": " Table 5: Results on SPMRL data with predicted  tags.", "labels": [], "entities": [{"text": "SPMRL", "start_pos": 21, "end_pos": 26, "type": "TASK", "confidence": 0.9530290961265564}]}]}