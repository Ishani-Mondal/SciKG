{"title": [{"text": "An End-to-End Deep Framework for Answer Triggering with a Novel Group-Level Objective", "labels": [], "entities": [{"text": "Answer Triggering", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9910236895084381}]}], "abstractContent": [{"text": "Given a question and a set of answer candidates, answer triggering determines whether the candidate set contains any correct answers.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7686761617660522}]}, {"text": "If yes, it then outputs a correct one.", "labels": [], "entities": []}, {"text": "In contrast to existing pipeline methods which first consider individual candidate answers separately and then make a prediction based on a threshold , we propose an end-to-end deep neu-ral network framework, which is trained by a novel group-level objective function that directly optimizes the answer triggering performance.", "labels": [], "entities": []}, {"text": "Our objective function penalizes three potential types of error and allows training the framework in an end-to-end manner.", "labels": [], "entities": []}, {"text": "Experimental results on the WIKIQA benchmark show that our framework outperforms the state of the arts by a 6.6% absolute gain under F 1 measure 1 .", "labels": [], "entities": [{"text": "WIKIQA benchmark", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.913877546787262}, {"text": "F 1 measure", "start_pos": 133, "end_pos": 144, "type": "METRIC", "confidence": 0.9461014866828918}]}], "introductionContent": [{"text": "Question Answering (QA) aims at automatically responding to natural language questions with direct answers).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8632235944271087}]}, {"text": "Most existing QA systems always output an answer for any question, no matter whether their answer candidate set contains correct answers or not.", "labels": [], "entities": []}, {"text": "In practice, however, this can greatly hurt user experience, especially when it is hard for users to judge answer correctness.", "labels": [], "entities": []}, {"text": "In this paper, we study the critical yet under-addressed Answer Triggering ( problem: Given a question and a set of answer candidates, determine whether the candidate set contains any correct answer, and if so, select a correct answer as system output.", "labels": [], "entities": [{"text": "Answer Triggering", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8108768165111542}]}, {"text": "The answer triggering problem can be logically divided into two sub-problems: P 1 : Build an individual-level model to rank answer candidates so that a correct one (if it exists) gets the highest score.", "labels": [], "entities": [{"text": "answer triggering problem", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.9002306461334229}]}, {"text": "P 2 : Make a group-level binary prediction on the existence of correct answers within the candidate set.", "labels": [], "entities": []}, {"text": "Previous work () attack the problem via a pipeline approach: First solve P 1 as a ranking task and then solve P 2 by choosing an optimal threshold upon the previous step's highest ranking score.", "labels": [], "entities": []}, {"text": "However, the yielded answer triggering performance is far from satisfactory, with F 1 between 32% and 36%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9951088428497314}]}, {"text": "An alternative pipeline approach is to first solve P 2 and then P 1 , i.e., first determine whether there's a correct answer in the candidate set and then rank all candidates to find a correct one.", "labels": [], "entities": []}, {"text": "However, as we will show using state-of-theart Multiple Instance Learning (MIL) algorithms in Section 4, P 2 by itself is currently a very challenging task, partly because of the difficulty of extracting features from a set of candidate answers that are effective for answer triggering.", "labels": [], "entities": [{"text": "answer triggering", "start_pos": 268, "end_pos": 285, "type": "TASK", "confidence": 0.8743366301059723}]}, {"text": "Because both P 1 and P 2 performances are far from perfect, the above pipeline approaches also suffer from error propagation ().", "labels": [], "entities": []}, {"text": "We propose Group-level Answer Triggering (GAT), an end-to-end framework for jointly optimizing P 1 and P 2 . Our key contribution in GAT is a novel group-level objective function, which aggregates individual-level information and penalizes three potential error types in answer triggering as a group-level task.", "labels": [], "entities": [{"text": "Group-level Answer Triggering (GAT)", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.7874212662378947}, {"text": "answer triggering", "start_pos": 271, "end_pos": 288, "type": "TASK", "confidence": 0.7560930252075195}]}, {"text": "By optimizing this objective function, we can directly back-propagate the final answer triggering errors to the entire framework and learn all the parameters simultaneously.", "labels": [], "entities": []}, {"text": "We conduct evaluation using the same dataset and measure as in previous work, and our framework improves the F 1 score by 6.6% (from 36.65% to 43.27%), compared with the state of the art.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9869600931803385}]}], "datasetContent": [{"text": "We use the WIKIQA dataset () for evaluation.", "labels": [], "entities": [{"text": "WIKIQA dataset", "start_pos": 11, "end_pos": 25, "type": "DATASET", "confidence": 0.966628760099411}]}, {"text": "It contains 3,047 questions from Bing query logs, each associated with a group of candidate answer sentences from Wikipedia and manually labeled via crowdsourcing.", "labels": [], "entities": []}, {"text": "Several intuitive features are also included in WIKIQA: two word matching features (IDF-weighted and unweighted word-overlapping counts between questions and candidate answers, denoted as Cnt), the length of a question (QLen), and the length of a candidate answer (SLen).", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8313534259796143}, {"text": "length of a question (QLen)", "start_pos": 198, "end_pos": 225, "type": "METRIC", "confidence": 0.6422206461429596}, {"text": "length of a candidate answer (SLen)", "start_pos": 235, "end_pos": 270, "type": "METRIC", "confidence": 0.8224457278847694}]}, {"text": "As in previous works, we also test the effect of these features, by combining them with other features as input into the Softmax layer in our framework.", "labels": [], "entities": []}, {"text": "We use the standard 70% (train), 10% (dev), and 20% (test) split of WIK-IQA.", "labels": [], "entities": [{"text": "WIK-IQA", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.980614423751831}]}, {"text": "We also use the same data pre-processing steps for fair comparison: Truncate questions and sentences to a maximum of 40-token long and initialize the 300-dimensional word vectors using pretrained word2vec embedding (Mikolov et al., 2013).", "labels": [], "entities": []}, {"text": "We use precision, recall, and F 1 , defined in the same way as in previous work.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9996607303619385}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9995138645172119}, {"text": "F 1", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9900955259799957}]}, {"text": "A question is treated as a positive case only if it contains one or more correct answers in its candidate set.", "labels": [], "entities": []}, {"text": "For the prediction of a question, only the candidate with the highest matching score is considered.", "labels": [], "entities": [{"text": "prediction of a question", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.9111610949039459}]}, {"text": "A true positive prediction shall meet two criteria: (1) the score is above a threshold (0.5 for our framework; tuned on dev set in other work), and (2) the candidate is labeled as a correct answer to the question.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test set.", "labels": [], "entities": []}]}