{"title": [], "abstractContent": [{"text": "The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neu-ral sequence-to-sequence models to the task.", "labels": [], "entities": [{"text": "generation of complex derived word forms", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7449798285961151}]}, {"text": "We overview the theoretical motivation fora paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion.", "labels": [], "entities": [{"text": "derivational paradigm completion", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.6298893292744955}]}, {"text": "State-of-the-art neural models, adapted from the inflection task, are able to learn a range of derivation patterns, and outperform a non-neural baseline by 16.4%.", "labels": [], "entities": []}, {"text": "However, due to semantic, historical, and lexical considerations involved in deriva-tional morphology, future work will be needed to achieve performance parity with inflection-generating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unlike inflectional morphology, which produces grammatical variants of the same core lexical item (e.g., take \u2192takes), derivational morphology is one of the key processes by which new lemmata are created.", "labels": [], "entities": []}, {"text": "For example, the English verb corrode can evolve into the noun corrosion, the adjective corrodent, and numerous other complex derived forms such as anticorrosive.", "labels": [], "entities": []}, {"text": "Derivational morphology is often highly productive, leading to the ready creation of neologisms such as Rao-Blackwellize and Rao-Blackwellization, both originating from the Rao-Blackwell theorem.", "labels": [], "entities": [{"text": "Derivational morphology", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.856008768081665}]}, {"text": "Despite the prevalence of productive derivational morphology, however, there has been little work on its generation.", "labels": [], "entities": []}, {"text": "Commonly used derivational resources such as NomBank () are still finite.", "labels": [], "entities": [{"text": "NomBank", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9239938855171204}]}, {"text": "Moreover, the complex phonological and historical changes (e.g., the adjectivization corrode \u2192corrosive) and affix selection (e.g., choosing between English deverbal suffixes -ment and -tion) make generation of derived forms an interesting and challenging problem for NLP.", "labels": [], "entities": []}, {"text": "In this work, we show that viewing derivational morphological processes as paradigmatic maybe fruitful for generation.", "labels": [], "entities": []}, {"text": "This means that there area number of well-defined form-function pairs associated with a core word.", "labels": [], "entities": []}, {"text": "For example, atypical English verb may have five forms in its inflectional paradigm, corresponding to its base (take), past tense (took), past participle (taken), progressive (taking) and third-person singular (takes) forms.", "labels": [], "entities": []}, {"text": "These forms are related by a consistent set of relations, such as affixation.", "labels": [], "entities": []}, {"text": "Similarly, a verb may have several slots in its derivational paradigm: The form take has the agentive nominalization taker, and the abilitative adjectivization takable.", "labels": [], "entities": []}, {"text": "Note there are also consistent patterns associated with each derivational slot, e.g., the -er suffix regularly produces the agentive.", "labels": [], "entities": []}, {"text": "Exploiting this paradigmatic characterization of derivational morphology allows us to create a statistical model capable of generating derivationally complex forms.", "labels": [], "entities": []}, {"text": "We apply state-of-the-art models for inflection generation, which learn mappings from fixed paradigm slots to derived forms.", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.7672447860240936}]}, {"text": "Empirically, we compare results for two models on the new task of derivational paradigm completion: a neural sequence-to-sequence model and a standard non-neural baseline.", "labels": [], "entities": [{"text": "derivational paradigm completion", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.6892126301924387}]}, {"text": "Our best neural model for derivation achieves 71.7% accuracy, beating the non-neural baseline by 16.4 points.", "labels": [], "entities": [{"text": "derivation", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.977257490158081}, {"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9996899366378784}]}, {"text": "Nevertheless, we note this is about 25 points lower than the equivalent model on the English inflection task (and even 20 points lower than the model's performance on the harder Finnish inflection generation).", "labels": [], "entities": []}, {"text": "These results point to additional complications in derivation that require more elaborate models or data annotation to overcome.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on 3 metrics: accuracy, average edit distance, and F 1 . Accuracy measures how often system output exactly matches the gold string.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999610960483551}, {"text": "average edit distance", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.8027166724205017}, {"text": "F", "start_pos": 63, "end_pos": 64, "type": "METRIC", "confidence": 0.999626636505127}, {"text": "Accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9991577863693237}]}, {"text": "Edit distance, by comparison, measures the Levenshtein distance between system output and the gold string.", "labels": [], "entities": [{"text": "Edit distance", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.954452633857727}, {"text": "Levenshtein distance", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.7103266716003418}]}, {"text": "Finally, we calculate affix F 1 scores for individual derivational affixes.", "labels": [], "entities": [{"text": "affix F 1 scores", "start_pos": 22, "end_pos": 38, "type": "METRIC", "confidence": 0.9422536492347717}]}, {"text": "E.g., for -ment precision is the number of words where the model correctly predicted -ment (out of total predictions) and recall is the number of words where the model correctly predicted out of the number of true words.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9380742311477661}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9993615746498108}]}, {"text": "Unfortunately, NomBank does not provide the necessary annotations inmost cases.", "labels": [], "entities": []}, {"text": "For instance, there is noway to differentiate actor and actress without gender.", "labels": [], "entities": []}, {"text": "It also does not distinguish the semantics of some adjective nominalizations, e.g., activism and activity.", "labels": [], "entities": []}, {"text": "Future work will reannotate NomBank to make these finer-grained distinctions.", "labels": [], "entities": []}, {"text": "We observe mistakes on less frequent suffixes, e.g., -age-we predict * draination instead of drainage.", "labels": [], "entities": []}, {"text": "Also, there are several cases where NomBank only lists one available form, e.g., complexity, and our model predicts complexness.", "labels": [], "entities": []}, {"text": "We also see mistakes on irregular adverbs, e.g., we generate advancely from advance, rather than inadvance, as well as in PATIENT nominalizations, e.g., the model produces containee in place of content-this last distinction is unpredictable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results under two metrics (accuracy and Levenshtein  distance) comparing the non-neural baseline from the 201 SIG- MORPHON shared task and the neural sequence-to-sequence  model (both for 1-best and 10-best output).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9994797110557556}, {"text": "Levenshtein  distance", "start_pos": 50, "end_pos": 71, "type": "METRIC", "confidence": 0.9154256880283356}, {"text": "SIG- MORPHON shared task", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.41014673709869387}]}]}