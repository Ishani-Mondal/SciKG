{"title": [{"text": "A Mention-Ranking Model for Abstract Anaphora Resolution", "labels": [], "entities": []}], "abstractContent": [{"text": "Resolving abstract anaphora is an important , but difficult task for text understanding.", "labels": [], "entities": [{"text": "Resolving abstract anaphora", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8946464459101359}, {"text": "text understanding", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8449368178844452}]}, {"text": "Yet, with recent advances in representation learning this task becomes a more tangible aim.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9462567567825317}]}, {"text": "A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typical-ly non-nominal) antecedent.", "labels": [], "entities": []}, {"text": "We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net.", "labels": [], "entities": []}, {"text": "We overcome the lack of training data by generating artificial anaphoric sentence-antecedent pairs.", "labels": [], "entities": []}, {"text": "Our model outperforms state-of-the-art results on shell noun resolution.", "labels": [], "entities": [{"text": "shell noun resolution", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.5996031165122986}]}, {"text": "We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus.", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 77, "end_pos": 89, "type": "DATASET", "confidence": 0.9657077193260193}]}, {"text": "This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders.", "labels": [], "entities": []}, {"text": "We found model variants that outperform the base-lines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors.", "labels": [], "entities": []}, {"text": "Our model selects syntactically plausible candidates and-if disregarding syntax-discriminates candidates using deeper features .", "labels": [], "entities": []}], "introductionContent": [{"text": "Current research in anaphora (or coreference) resolution is focused on resolving noun phrases referring to concrete objects or entities in the real \u2020 Leo Born, Juri Opitz and Anette Frank contributed equally to this world, which is arguably the most frequently occurring type.", "labels": [], "entities": [{"text": "coreference) resolution", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8469483653704325}]}, {"text": "Distinct from these are diverse types of abstract anaphora (AA) where reference is made to propositions, facts, events or properties.", "labels": [], "entities": [{"text": "abstract anaphora (AA)", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7473109722137451}]}, {"text": "An example is given in (1) below.", "labels": [], "entities": []}, {"text": "While recent approaches address the resolution of selected abstract shell nouns, we aim to resolve a wide range of abstract anaphors, such as the NP this trend in (1), as well as pronominal anaphors (this, that, or it).", "labels": [], "entities": [{"text": "resolution of selected abstract shell nouns", "start_pos": 36, "end_pos": 79, "type": "TASK", "confidence": 0.7977416515350342}]}, {"text": "Henceforth, we refer to a sentence that contains an abstract anaphor as the anaphoric sentence (AnaphS), and to a constituent that the anaphor refers to as the antecedent (Antec) (cf. (1)).", "labels": [], "entities": []}, {"text": "(1) Ever-more powerful desktop computers, designed with one or more microprocessors as their \"brains\", are expected to increasingly take on functions carried out by more expensive minicomputers and mainframes.", "labels": [], "entities": []}, {"text": "\"[Antec The guys that make traditional hardware are really being obsoleted by microprocessor-based machines]\", said Mr. Benton.", "labels": [], "entities": []}, {"text": "[ AnaphS As a result of this trendAA, longtime powerhouses HP, IBM and Digital Equipment Corp. are scrambling to counterattack with microprocessor-based systems of their own.]", "labels": [], "entities": [{"text": "AnaphS", "start_pos": 2, "end_pos": 8, "type": "DATASET", "confidence": 0.8725399971008301}]}, {"text": "A major obstacle for solving this task is the lack of sufficient amounts of annotated training data.", "labels": [], "entities": []}, {"text": "We propose a method to generate large amounts of training instances covering a wide range of abstract anaphor types.", "labels": [], "entities": []}, {"text": "This enables us to use neural methods which have shown great success in related tasks: coreference resolution), textual entailment (, learning textual similarity, and discourse relation sense classification (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.9188436269760132}, {"text": "textual entailment", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7725151479244232}, {"text": "discourse relation sense classification", "start_pos": 167, "end_pos": 206, "type": "TASK", "confidence": 0.643413208425045}]}, {"text": "Our model is inspired by the mention-ranking model for coreference resolution and combines it with a Siamese Net, () for learning similarity between sentences.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9458488821983337}]}, {"text": "Given an anaphoric sentence (AntecS in (1)) and a candidate antecedent (any constituent in a given context, e.g. being obsoleted by microprocessor-based machines in (1)), the LSTM-Siamese Net learns representations for the candidate and the anaphoric sentence in a shared space.", "labels": [], "entities": []}, {"text": "These representations are combined into a joint representation used to calculate a score that characterizes the relation between them.", "labels": [], "entities": []}, {"text": "The learned score is used to select the highest-scoring antecedent candidate for the given anaphoric sentence and hence its anaphor.", "labels": [], "entities": []}, {"text": "We consider one anaphor at a time and provide the embedding of the context of the anaphor and the embedding of the head of the anaphoric phrase to the input to characterize each individual anaphorsimilar to the encoding proposed by for individuating multiply occurring predicates in SRL.", "labels": [], "entities": []}, {"text": "With deeper inspection we show that the model learns a relation between the anaphor in the anaphoric sentence and its antecedent.", "labels": [], "entities": []}, {"text": "In contrast to other work, our method for generating training data is not confined to specific types of anaphora such as shell nouns) or anaphoric connectives.", "labels": [], "entities": []}, {"text": "It produces large amounts of instances and is easily adaptable to other languages.", "labels": [], "entities": []}, {"text": "This enables us to build a robust, knowledge-lean model for abstract anaphora resolution that easily extends to multiple languages.", "labels": [], "entities": [{"text": "abstract anaphora resolution", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6288005610307058}]}, {"text": "We evaluate our model on the shell noun resolution dataset of and show that it outperforms their state-of-the-art results.", "labels": [], "entities": [{"text": "noun resolution", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.737568199634552}]}, {"text": "Moreover, we report results of the model (trained on our newly constructed dataset) on unrestricted abstract anaphora instances from the ARRAU corpus (.", "labels": [], "entities": [{"text": "ARRAU corpus", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9570691287517548}]}, {"text": "To our knowledge this provides the first state-of-the-art benchmark on this data subset.", "labels": [], "entities": []}, {"text": "Our TensorFlow 2 implementation of the model and scripts for data extraction are available at: https://github.com/amarasovic/ neural-abstract-anaphora.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.745892196893692}]}, {"text": "In contrast to nominal anaphora, abstract anaphora is difficult to resolve, given that agreement and lexical match features are not applicable.", "labels": [], "entities": []}, {"text": "Annotation of abstract anaphora is also difficult for humans, and thus, only few smaller-scale corpora have been constructed.", "labels": [], "entities": [{"text": "Annotation of abstract anaphora", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8449502289295197}]}, {"text": "We evaluate our models on a subset of the AR-RAU corpus () that contains abstract anaphors and the shell noun corpus used in.", "labels": [], "entities": [{"text": "AR-RAU corpus", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8786895871162415}]}, {"text": "We are not aware of other freely available abstract anaphora datasets.", "labels": [], "entities": []}, {"text": "Little work exists for the automatic resolution of abstract anaphora.", "labels": [], "entities": [{"text": "automatic resolution of abstract anaphora", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.7943046748638153}]}, {"text": "Early work has focused on spoken language, which exhibits specific properties.", "labels": [], "entities": []}, {"text": "Recently, event coreference has been addressed using feature-based classifiers.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8054795861244202}]}, {"text": "Event coreference is restricted to a subclass of events, and usually focuses on coreference between verb (phrase) and noun (phrase) mentions of similar abstractness levels (e.g. purchase -acquire) with no special focus on (pro)nominal anaphora.", "labels": [], "entities": [{"text": "Event coreference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6867821961641312}, {"text": "coreference between verb (phrase) and noun (phrase) mentions", "start_pos": 80, "end_pos": 140, "type": "TASK", "confidence": 0.7599534591039022}]}, {"text": "Abstract anaphora typically involves a full-fledged clausal antecedent that is referred to by a highly abstract (pro)nominal anaphor, as in (1).", "labels": [], "entities": []}, {"text": "proposed a model for resolution of events in biomedical text that refer to a single or multiple clauses.", "labels": [], "entities": [{"text": "resolution of events in biomedical text", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.8788185516993204}]}, {"text": "However, instead of selecting the correct antecedent clause(s) (our task) fora given event, their model is restricted to classifying the event into six abstract categories: this these changes, responses, analysis, context, finding, observation, based on its surrounding context.", "labels": [], "entities": []}, {"text": "While related, their task is not comparable to the full-fledged abstract anaphora resolution task, since the events to be classified are known to be coreferent and chosen from a set of restricted abstract types.", "labels": [], "entities": [{"text": "full-fledged abstract anaphora resolution task", "start_pos": 51, "end_pos": 97, "type": "TASK", "confidence": 0.7116632103919983}]}, {"text": "More related to our work is who present an antecedent ranking account for sluicing using classical machine learning based on a small training dataset.", "labels": [], "entities": []}, {"text": "They employ features modeling distance, containment, discourse structure, and -less effectively -content and lexical correlates.", "labels": [], "entities": []}, {"text": "Closest to our work is (KZH13) and on shell noun resolution, using classical machine learning techniques.", "labels": [], "entities": [{"text": "shell noun resolution", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.5977791448434194}]}, {"text": "Shell nouns are abstract nouns, such as fact, possibility, or issue, which can only be interpreted jointly with their shell content (their embedded clause as in or antecedent as in).", "labels": [], "entities": []}, {"text": "KZH13 refer to shell nouns whose antecedent occurs in the prior discourse as anaphoric shell nouns (ASNs) (cf. KZH13 presented an approach for resolving six typical shell nouns following the observation that CSNs are easy to resolve based on their syntactic structure alone, and the assumption that ASNs share linguistic properties with their embedded (CSN) counterparts.", "labels": [], "entities": [{"text": "KZH13", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9104655385017395}]}, {"text": "They manually developed rules to identify the embedded clause (i.e. cataphoric antecedent) of CSNs and trained SVM rank) on such instances.", "labels": [], "entities": []}, {"text": "The trained SVM rank model is then used to resolve ASNs.", "labels": [], "entities": [{"text": "resolve ASNs", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.634185791015625}]}, {"text": "KH14 generalized their method to be able to create training data for any given shell noun, however, their method heavily exploits the specific properties of shell nouns and does not apply to other types of abstract anaphora.", "labels": [], "entities": [{"text": "KH14", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9367385506629944}]}, {"text": "Stede and Grishina (2016) study a related phenomenon for German.", "labels": [], "entities": []}, {"text": "They examine inherently anaphoric connectives (such as demzufolge -according to which) that could be used to access their abstract antecedent in the immediate context.", "labels": [], "entities": []}, {"text": "Yet, such connectives are restricted in type, and the study shows that such connectives are often ambiguous with nominal anaphors and require sense disambiguation.", "labels": [], "entities": []}, {"text": "We conclude that they cannot be easily used to acquire antecedents automatically.", "labels": [], "entities": []}, {"text": "In our work, we explore a different direction: we construct artificial training data using a general pattern that identifies embedded sentence constituents, which allows us to extract relatively secure training data for abstract anaphora that captures a wide range of anaphora-antecedent rela-tions, and apply this data to train a model for the resolution of unconstrained abstract anaphora.", "labels": [], "entities": []}, {"text": "Recent work in entity coreference resolution has proposed powerful neural network-based models that we will adapt to the task of abstract anaphora resolution.", "labels": [], "entities": [{"text": "entity coreference resolution", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.8212361931800842}, {"text": "abstract anaphora resolution", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6447599828243256}]}, {"text": "Most relevant for our task is the mention-ranking neural coreference model proposed in, and their improved model in, which integrates a loss function () which learns distinct feature representations for anaphoricity detection and antecedent ranking.", "labels": [], "entities": [{"text": "anaphoricity detection", "start_pos": 203, "end_pos": 225, "type": "TASK", "confidence": 0.7928009033203125}]}, {"text": "Siamese Nets distinguish between similar and dissimilar pairs of samples by optimizing a loss over the metric induced by the representations.", "labels": [], "entities": []}, {"text": "It is widely used in vision (), and in NLP for semantic similarity, entailment, query normalization and QA).", "labels": [], "entities": [{"text": "query normalization", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7229387760162354}]}], "datasetContent": [{"text": "We evaluate our model on two types of anaphora: (a) shell noun anaphora and (b) (pro)nominal abstract anaphors extracted from ARRAU. a. Shell noun resolution dataset.", "labels": [], "entities": [{"text": "ARRAU. a. Shell noun resolution dataset", "start_pos": 126, "end_pos": 165, "type": "DATASET", "confidence": 0.8037903308868408}]}, {"text": "For comparability we train and evaluate our model for shell noun resolution, using the original training (CSN) and test (ASN) corpus of.", "labels": [], "entities": [{"text": "shell noun resolution", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6247875491778055}]}, {"text": "9 In case of ambiguous conjunctions (e.g. as interpreted as causal or temporal), we generally choose the most frequent interpretation.", "labels": [], "entities": []}, {"text": "This also alleviates problems with languages like German, where (non-)embedded sentences differ in surface position of the finite verb.", "labels": [], "entities": []}, {"text": "We can either adapt the order or ignore it, when producing anaphoric sentence -antecedent pairs.", "labels": [], "entities": []}, {"text": "We thank the authors for providing the available data.", "labels": [], "entities": []}, {"text": "We follow the data preparation and evaluation protocol of.", "labels": [], "entities": []}, {"text": "The CSN corpus was constructed from the NYT corpus using manually developed patterns to identify the antecedent of cataphoric shell nouns (CSNs).", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.923732578754425}]}, {"text": "In KZH13, all syntactic constituents of the sentence that contains both the CSN and its antecedent were considered as candidates for training a ranking model.", "labels": [], "entities": [{"text": "KZH13", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9060201644897461}]}, {"text": "Candidates that differ from the antecedent in only one word or one word and punctuation were as well considered as antecedents . To all other candidates we refer to as negative candidates.", "labels": [], "entities": []}, {"text": "For every shell noun, KZH13 used the corresponding part of the CSN data to train SVM rank . The ASN corpus serves as the test corpus.", "labels": [], "entities": [{"text": "KZH13", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8416829705238342}, {"text": "CSN data", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.8429622650146484}, {"text": "ASN corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.7892845869064331}]}, {"text": "It was also constructed from the NYT corpus, by selecting anaphoric instances with the pattern \"this shell noun\" for all covered shell nouns.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9462578296661377}]}, {"text": "For validation, crowdsourced annotations for the sentence which contains the antecedent, which KZH13 refer to as abroad region.", "labels": [], "entities": []}, {"text": "Candidates for the antecedent were obtained by using all syntactic constituents of the broad region as candidates and ranking them using the SVM rank model trained on the CSN corpus.", "labels": [], "entities": [{"text": "CSN corpus", "start_pos": 171, "end_pos": 181, "type": "DATASET", "confidence": 0.9284795522689819}]}, {"text": "The top 10 ranked candidates were presented to the crowd workers and they chose the best answer that represents the ASN antecedent.", "labels": [], "entities": []}, {"text": "The workers were encouraged to select None when they did not agree with any of the displayed answers and could provide information about how satisfied they were with the displayed candidates.", "labels": [], "entities": []}, {"text": "We consider this dataset as gold, as do KZH13, although it maybe biased towards the offered candidates.", "labels": [], "entities": [{"text": "KZH13", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.8815409541130066}]}, {"text": "Abstract anaphora resolution data set.", "labels": [], "entities": []}, {"text": "We use the automatically constructed data from the WSJ corpus (Section 4) for training.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.9562077224254608}]}, {"text": "Our test data for unrestricted abstract anaphora resolution is obtained from the ARRAU corpus ().", "labels": [], "entities": [{"text": "unrestricted abstract anaphora resolution", "start_pos": 18, "end_pos": 59, "type": "TASK", "confidence": 0.6050261408090591}, {"text": "ARRAU corpus", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9658476114273071}]}, {"text": "We extracted all abstract anaphoric instances from the WSJ part of ARRAU that are marked with the category abstract or plan,   Data statistics.", "labels": [], "entities": [{"text": "WSJ part of ARRAU", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9148123413324356}]}, {"text": "gives statistics of the datasets: the number of anaphors (row 1), the median length (in tokens) of antecedents (row 2), the median length (in tokens) for all anaphoric sentences (row 3), the median of the number of antecedents and candidates that are not antecedents (negatives) (rows 4-5), the number of pronominal and nominal anaphors (rows 6-7).", "labels": [], "entities": []}, {"text": "Both training sets, artificial and CSN, have only one possible antecedent for which we accept two minimal variants differing in only one word or one word and punctuation.", "labels": [], "entities": []}, {"text": "On the contrary, both test sets by design allow annotation of more than one antecedent that differ in more than one word.", "labels": [], "entities": []}, {"text": "Every anaphor in the artificial training dataset is pronominal, whereas anaphors in CSN and ASN are nominal only.", "labels": [], "entities": []}, {"text": "ARRAU-AA has a mixture of nominal and pronominal anaphors.", "labels": [], "entities": [{"text": "ARRAU-AA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8140846490859985}]}, {"text": "Other details can be found in Supplementary Materials.", "labels": [], "entities": [{"text": "Supplementary Materials", "start_pos": 30, "end_pos": 53, "type": "DATASET", "confidence": 0.7513392269611359}]}, {"text": "Following KZH13, we report success@n (s@n), which measures whether the antecedent, or a candidate that differs in one word , is in the first n ranked candidates, for n \u2208 {1, 2, 3, 4}.", "labels": [], "entities": [{"text": "KZH13", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8852062225341797}]}, {"text": "Additionally, we report the preceding sentence baseline (PS BL ) that chooses the previous sentence for the antecedent and TAGbaseline (TAG BL ) that randomly chooses a candidate with the constituent tag label in {S, VP, ROOT, SBAR}.", "labels": [], "entities": [{"text": "TAGbaseline (TAG BL )", "start_pos": 123, "end_pos": 144, "type": "METRIC", "confidence": 0.8207990527153015}, {"text": "ROOT", "start_pos": 221, "end_pos": 225, "type": "METRIC", "confidence": 0.9028236865997314}]}, {"text": "For TAG BL we report the average of 10 runs with 10 fixed seeds.", "labels": [], "entities": [{"text": "TAG BL", "start_pos": 4, "end_pos": 10, "type": "TASK", "confidence": 0.39519810676574707}]}, {"text": "PS BL always performs worse than the KZH13 model on the ASN, so we report it only for ARRAU-AA.", "labels": [], "entities": [{"text": "BL", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.501419186592102}, {"text": "ASN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.9011523127555847}, {"text": "ARRAU-AA", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.8576528429985046}]}], "tableCaptions": [{"text": " Table 2: Data statistics. For the ASN and CSN we  report statistics over all shell nouns, but classifiers  are trained independently.", "labels": [], "entities": []}, {"text": " Table 3: Shell noun resolution results.", "labels": [], "entities": [{"text": "Shell noun resolution", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8640373349189758}]}, {"text": " Table 4: Architecture ablation for reason.", "labels": [], "entities": []}, {"text": " Table 5: Results table for the ARRAU-AA test set. Refer to text for explanation of duplicated rows.", "labels": [], "entities": [{"text": "ARRAU-AA test set", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.909631629784902}]}]}