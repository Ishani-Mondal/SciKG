{"title": [{"text": "Learning to Predict Charges for Criminal Cases with Legal Basis", "labels": [], "entities": [{"text": "Learning to Predict Charges for Criminal Cases with Legal Basis", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.7256951063871384}]}], "abstractContent": [{"text": "The charge prediction task is to determine appropriate charges fora given case, which is helpful for legal assistant systems where the user input is fact description.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7448744177818298}]}, {"text": "We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework.", "labels": [], "entities": [{"text": "charge prediction task", "start_pos": 157, "end_pos": 179, "type": "TASK", "confidence": 0.7683641413847605}, {"text": "article extraction task", "start_pos": 197, "end_pos": 220, "type": "TASK", "confidence": 0.7759131590525309}]}, {"text": "The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.6688689291477203}]}], "introductionContent": [{"text": "The task of automatic charge prediction is to determine appropriate charges, such as fraud, larceny or homicide, fora case by analyzing its textual fact description.", "labels": [], "entities": [{"text": "automatic charge prediction", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6092358430226644}]}, {"text": "Such techniques are crucial for legal assistant systems, where users could find similar cases or possible penalties by describing a case with their own words.", "labels": [], "entities": []}, {"text": "This is helpful for non-legal professionals to get to know the legal basis of their interested cases, e.g., cases they or their friends are involved in, since the massive legal materials and the lack of knowledge of legal jargons make it hard for outsiders to do it on their own.", "labels": [], "entities": []}, {"text": "However, predicting appropriate charges based on fact descriptions is not trivial: (1) The differences between two charges can be subtle, for example, in the context of criminal cases in China, distinguishing intentional homicide from intentional injury would require to determine, from the fact description, whether the defendant intended to kill the victim, or just intended to hurt the victim, who, unfortunately died of severe injury.", "labels": [], "entities": []}, {"text": "(2) Multiple crimes maybe involved in a single case, which means we need to conduct charge prediction in the multi-label classification paradigm.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.7888232469558716}, {"text": "multi-label classification", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.7018977105617523}]}, {"text": "Although we can expect an off-the-shelf classification model to learn to label a case with corresponding charges through massive training data, it is always more convincing to make the prediction with its involved law articles explicitly shown to the users, as legal basis to support the prediction.", "labels": [], "entities": []}, {"text": "This issue is crucial in countries using the civil law system, e.g., China (except Hong Kong), where judgements are made based on statutory laws only.", "labels": [], "entities": []}, {"text": "For example, in, a judgement document in China always includes relevant law articles (in the court view part) to support the decision.", "labels": [], "entities": []}, {"text": "Even in countries using the common law system, e.g., the United States (except Louisiana), where the judgement is based mainly on decisions of previous cases, there are still some statutory laws that need to be followed when making decisions.", "labels": [], "entities": []}, {"text": "Existing attempts formulate the task of automatic charge prediction as a single-label classification problem, by either adopting a k-Nearest Neighbor (KNN) (;) as the classifier with shallow textual features, or manually designing key factors for specific charges to help text understanding (, which make those works hard to scale to more types of charges.", "labels": [], "entities": [{"text": "automatic charge prediction", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6420098145802816}, {"text": "text understanding", "start_pos": 272, "end_pos": 290, "type": "TASK", "confidence": 0.751995176076889}]}, {"text": "There are also works addressing a related task, finding the law articles that are involved in a given case.", "labels": [], "entities": []}, {"text": "A simple solution is to convert this multi-label problem into a multiclass classification task by only considering a fixed set of article combinations (Liu and;), which can only be applied to a small set of articles and does not fit to real applications.", "labels": [], "entities": [{"text": "multiclass classification task", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.7691313823064169}]}, {"text": "Recent improvement takes a twostep approach by performing a preliminary classification first and then re-ranking the results with word-level and article-level features ( . These efforts advance the applications of machine learning and natural language processing methods into legal assistance services, however, they are still in an early stage, e.g., relying on expert knowledge, using relatively simple classification paradigms, and shallow textual analysis.", "labels": [], "entities": []}, {"text": "More importantly, related tasks, e.g., charge prediction and relevant article extraction, are treated independently, ignoring the fact that they could benefit from each other.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8363100588321686}, {"text": "relevant article extraction", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.5864159365495046}]}, {"text": "Recent advances in neural networks enable us to jointly model charge prediction and relevant article extraction in a unified framework, where the latent correspondence from the fact description about a case to its related law articles and further to its charges can be explicitly addressed by a two-stack attention mechanism.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7714496552944183}, {"text": "article extraction", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7083655297756195}]}, {"text": "Specifically, we use a sentence-level and a documentlevel Bi-directional Gated Recurrent Units (Bi-GRU) () with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case.", "labels": [], "entities": [{"text": "Bi-directional Gated Recurrent Units (Bi-GRU)", "start_pos": 58, "end_pos": 103, "type": "METRIC", "confidence": 0.7191281276089805}]}, {"text": "Given the analysis of the fact description, we accordingly learn a stack of article-side attention components to attentively select the most supportive law articles from the statutory laws to support our charge prediction, which is investigated in the multi-label paradigm.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 204, "end_pos": 221, "type": "TASK", "confidence": 0.7110392302274704}]}, {"text": "We evaluate our model in the context of predicting charges for criminal cases in China.", "labels": [], "entities": [{"text": "predicting charges for criminal cases", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.8275964021682739}]}, {"text": "We collect publicly available judgement documents from China's government website, from which we can automatically extract fact descriptions, relevant law articles and the charges using simple rules, as shown in.", "labels": [], "entities": []}, {"text": "Experimental results show that our neural network method can effectively predict appropriate charges fora given case, and also provide relevant law articles as legal basis to support the prediction.", "labels": [], "entities": []}, {"text": "Our experiments also provide quantitive analysis about the effect of factside and article-side information on charge predicion, and confirm that, apart from providing legal basis, relevant articles also contain useful information that can help to improve charge prediction in the civil law system.", "labels": [], "entities": [{"text": "charge predicion", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.6670608967542648}, {"text": "charge prediction", "start_pos": 255, "end_pos": 272, "type": "TASK", "confidence": 0.7475549280643463}]}, {"text": "We also examine our model on the news reports about criminal cases.", "labels": [], "entities": []}, {"text": "Although trained on judgement documents, our model can still achieve promising performance on news data, showing a reasonable generalization ability over different expression styles.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use HanLP 3 for Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "HanLP 3", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8534010350704193}, {"text": "Chinese word segmentation", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.5827542245388031}, {"text": "POS tagging", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.7825705111026764}]}, {"text": "Word embeddings are trained using word2vec) on judgement documents, web pages from several legal forums and Baidu Encyclopedia.", "labels": [], "entities": [{"text": "Baidu Encyclopedia", "start_pos": 108, "end_pos": 126, "type": "DATASET", "confidence": 0.9545878469944}]}, {"text": "The resulting word embeddings contain 573,353 words, with 100 dimension.", "labels": [], "entities": []}, {"text": "We randomly initialize a 50-d vector for each POS tag, which is concatenated with the word embedding as the final input.", "labels": [], "entities": []}, {"text": "Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150.", "labels": [], "entities": []}, {"text": "The relevant article extractor generates top 20 articles, the weight of the article attention loss (\u03b2 in Eq. 6) is 0.1, and prediction threshold \u03c4 is 0.4.", "labels": [], "entities": [{"text": "article attention loss", "start_pos": 76, "end_pos": 98, "type": "METRIC", "confidence": 0.6387355128924052}, {"text": "prediction threshold \u03c4", "start_pos": 124, "end_pos": 146, "type": "METRIC", "confidence": 0.9749328295389811}]}, {"text": "We use Stochastic Gradient Descent (SGD) for training, with learning rate 0.1, and batch size 8.", "labels": [], "entities": [{"text": "learning rate 0.1", "start_pos": 60, "end_pos": 77, "type": "METRIC", "confidence": 0.9431832432746887}]}, {"text": "We compare our full model with two variations: without article attention supervision and only using facts for charge prediction.", "labels": [], "entities": [{"text": "charge prediction", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.705393522977829}]}, {"text": "The latter one is similar to the state-of-art document classification model (, but adapted to the multilabel nature of our problem.", "labels": [], "entities": [{"text": "document classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.6523750275373459}]}, {"text": "We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law (.", "labels": [], "entities": []}, {"text": "Specifically, the SVM model takes bag-of-words TF-IDF features as input, and uses chi-square to select top 2,000 features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Charge prediction results. Left and  right side of the slash refer to micro and macro  statistics, respectively. gold art refers to using  gold standard articles mentioned in judgements  (marked in blue in", "labels": [], "entities": [{"text": "Charge prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8395475149154663}]}, {"text": " Table 2: Refined Article Extraction Performance", "labels": [], "entities": [{"text": "Refined Article Extraction", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.6742424567540487}]}, {"text": " Table 3: Performance (micro statistics) on News", "labels": [], "entities": [{"text": "News", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9589496850967407}]}]}