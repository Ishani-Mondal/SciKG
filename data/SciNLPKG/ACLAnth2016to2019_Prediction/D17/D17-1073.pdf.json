{"title": [{"text": "Don't Throw Those Morphological Analyzers Away Just Yet: Neural Morphological Disambiguation for Arabic", "labels": [], "entities": [{"text": "Neural Morphological Disambiguation", "start_pos": 57, "end_pos": 92, "type": "TASK", "confidence": 0.6499361793200175}]}], "abstractContent": [{"text": "This paper presents a model for Arabic morphological disambiguation based on Recurrent Neural Networks (RNN).", "labels": [], "entities": [{"text": "Arabic morphological disambiguation", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.7823730111122131}]}, {"text": "We train Long Short-Term Memory (LSTM) cells in several configurations and embedding levels to model the various morphological features.", "labels": [], "entities": []}, {"text": "Our experiments show that these models outperform state-of-the-art systems without explicit use of feature engineering.", "labels": [], "entities": []}, {"text": "However, adding learning features from a morphological analyzer to model the space of possible analyses provides additional improvement.", "labels": [], "entities": []}, {"text": "We make use of the resulting morphological models for scoring and ranking the analyses of the morphological analyzer for morphological disambiguation.", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7379685044288635}]}, {"text": "The results show significant gains inaccuracy across several evaluation metrics.", "labels": [], "entities": []}, {"text": "Our system results in 4.4% absolute increase over the state-of-the-art in full morphological analysis accuracy (30.6% relative error reduction), and 10.6% (31.5% relative error reduction) for out-of-vocabulary words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9891858100891113}, {"text": "relative error reduction", "start_pos": 118, "end_pos": 142, "type": "METRIC", "confidence": 0.7154205838839213}, {"text": "relative error reduction)", "start_pos": 162, "end_pos": 187, "type": "METRIC", "confidence": 0.7599808424711227}]}], "introductionContent": [{"text": "Recurrent Neural Networks (RNN) in general, and Long Short-Term Memory (LSTM) cells in particular, have been proven very successful for various Natural Language Processing (NLP) tasks, especially those involving sequential data tagging.", "labels": [], "entities": [{"text": "sequential data tagging", "start_pos": 212, "end_pos": 235, "type": "TASK", "confidence": 0.682357519865036}]}, {"text": "RNN models can produce near or above stateof-the-art performance with minimal languagespecific feature engineering.", "labels": [], "entities": []}, {"text": "These models have the capacity of capturing syntactic and semantic features through the lexical word-level embeddings, and subword features through characterlevel embeddings.", "labels": [], "entities": []}, {"text": "Morphologically rich languages pose many challenges to NLP through their high degree of ambiguity and sparsity.", "labels": [], "entities": []}, {"text": "These challenges are exacerbated for languages with limited resources.", "labels": [], "entities": []}, {"text": "Morphological analyzers help reduce sparsity by providing several out-of-context morpheme-based analyses for words, but they usually introduce ambiguity by returning multiple analyses for the same surface form.", "labels": [], "entities": [{"text": "Morphological analyzers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7278403341770172}]}, {"text": "Therefore, the model would require a further step of morphological disambiguation to choose the correct analysis in context.", "labels": [], "entities": []}, {"text": "Morphological modeling involves heavy use of sequential tagging, so using an LSTM-based model would be highly advantageous.", "labels": [], "entities": []}, {"text": "LSTM models are also optimal for long-sequence tagging in particular, so such systems should be able to outperform other deep learning models with fixed window-based modeling.", "labels": [], "entities": []}, {"text": "Morphological disambiguation is a well studied problem in the literature, but LSTM-based contributions are still relatively scarce.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9114918112754822}]}, {"text": "In this paper we use Bidirectional-LSTM (Bi-LSTM) models for morphological tagging and language modeling, and use the results of these models in ranking the analyses of the morphological analyzer.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.748054027557373}, {"text": "language modeling", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7245221883058548}]}, {"text": "We incorporate various subword and morphological features at different linguistic depths in the tagger, along with both wordbased and character-based embeddings.", "labels": [], "entities": []}, {"text": "Our results show significant accuracy gains for all the morphological features we study, and across several evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9991360306739807}]}, {"text": "We compare our system against a strong baseline and a stateof-the-art-system.", "labels": [], "entities": []}, {"text": "We achieve 4.4% absolute over the state-of-the-art in full morphological analysis accuracy (30.6% relative error reduction).", "labels": [], "entities": [{"text": "absolute", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9881069660186768}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.994941771030426}, {"text": "relative error reduction", "start_pos": 98, "end_pos": 122, "type": "METRIC", "confidence": 0.8289709488550822}]}, {"text": "When evaluated for the out-of-vocabulary (OOV) words alone, the system achieves 10.6% absolute increase (31.5% relative error reduction), and shows significant performance boost across all evaluation metrics.", "labels": [], "entities": [{"text": "absolute increase", "start_pos": 86, "end_pos": 103, "type": "METRIC", "confidence": 0.9689794182777405}, {"text": "relative error reduction", "start_pos": 111, "end_pos": 135, "type": "METRIC", "confidence": 0.7486448486646017}]}], "datasetContent": [{"text": "Size  Evaluation: We use accuracy as the evaluation metric for all experiments reported in the paper.", "labels": [], "entities": [{"text": "Size  Evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8623109459877014}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9990424513816833}]}, {"text": "Baselines: We use the Maximum Likelihood Estimation (MLE) baseline, calculated by counting the frequency scores for each given word/tag out of context, with backoff to the most frequent tag for unknown words.", "labels": [], "entities": [{"text": "Maximum Likelihood Estimation (MLE) baseline", "start_pos": 22, "end_pos": 66, "type": "METRIC", "confidence": 0.7373122785772596}]}, {"text": "We also use the MADAMIRA (release-2.1) scores as another baseline, designated as the state-of-the-art system.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9683933854103088}]}, {"text": "Unless otherwise specified, MADAMIRA was configured in the ADD_PROP backoff mode, which adds a proper noun analysis to all words.", "labels": [], "entities": [{"text": "ADD_PROP backoff mode", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.8677488803863526}]}, {"text": "We use this configuration to match the analyzer format we used in training the deep learning system, and to match the models in previous contributions.", "labels": [], "entities": []}, {"text": "We use the following accuracy metrics to evaluate the disambiguation model, which also use in their evaluation: \u2022 EVALFULL: The percentage of correctly analyzed words across all morphological features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9971083998680115}, {"text": "EVALFULL", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9972248077392578}]}, {"text": "This is the strictest possible metric.", "labels": [], "entities": []}, {"text": "\u2022 EVALDIAC: The percentage of words where the chosen analysis has the correct fully diacritized form.: Accuracy results of the disambiguation system, evaluated using different metrics, for all words and out-of-vacbulary (OOV) words alone.", "labels": [], "entities": [{"text": "EVALDIAC", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9977127313613892}, {"text": "Accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9972184896469116}]}, {"text": "OOV percentage of all words is 7.9%.", "labels": [], "entities": [{"text": "OOV percentage", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.957800030708313}]}, {"text": "\u2022 EVALLEX: The percentage of words where the chosen analysis has the correct lemma.", "labels": [], "entities": [{"text": "EVALLEX", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9978836178779602}]}, {"text": "\u2022 EVALPOS: The percentage of words where the chosen analysis has the correct part-ofspeech.", "labels": [], "entities": [{"text": "EVALPOS", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.9946563243865967}]}, {"text": "\u2022 EVALATBTOK: The percentage of words that have a correct ATB tokenization.", "labels": [], "entities": [{"text": "EVALATBTOK", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9874690771102905}, {"text": "ATB tokenization", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.6096325814723969}]}, {"text": "3 Deep learning models, through word embeddings, provide an advantage in terms of the analysis of unseen words.", "labels": [], "entities": []}, {"text": "So, in addition to calculating the metrics for all the words in the testing set, we also calculate these metrics for the out-ofvocabulary (OOV) words alone.", "labels": [], "entities": []}, {"text": "shows the accuracy scores for MADAMIRA and our system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997679591178894}, {"text": "MADAMIRA", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.46051692962646484}]}, {"text": "All evaluation metrics indicate the performance boost of our system relative to MADAMIRA, with significant relative error reduction.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9345594048500061}, {"text": "error", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.9644580483436584}]}, {"text": "The same trend stands for the OOV words, with even higher absolute and relative error reduction scores, especially for EVALLEX, EVALPOS, and EVALATBTOK.", "labels": [], "entities": [{"text": "relative error reduction scores", "start_pos": 71, "end_pos": 102, "type": "METRIC", "confidence": 0.7876339554786682}, {"text": "EVALLEX", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.7660714387893677}, {"text": "EVALATBTOK", "start_pos": 141, "end_pos": 151, "type": "DATASET", "confidence": 0.9162495136260986}]}, {"text": "This increase in OOV analysis accuracy is the result of modeling the data on a semantic level, with the embeddings and neural networks, instead of pure lexical approach.", "labels": [], "entities": [{"text": "OOV analysis", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.7780119478702545}, {"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9460204243659973}]}], "tableCaptions": [{"text": " Table 2: An example highlighting Arabic's rich morphology and ambiguous orthography. The word", "labels": [], "entities": []}, {"text": " Table 4: Maximum Likelihood Estimation (MLE)  and MADAMIRA baselines for POS tagging.", "labels": [], "entities": [{"text": "Maximum Likelihood Estimation (MLE)", "start_pos": 10, "end_pos": 45, "type": "METRIC", "confidence": 0.7864031791687012}, {"text": "MADAMIRA baselines", "start_pos": 51, "end_pos": 69, "type": "METRIC", "confidence": 0.9596547186374664}, {"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8542487323284149}]}, {"text": " Table 5: Results for word embeddings (Word) and  character-level embeddings (Char) for POS tag- ging. We don't provide character-level embed- dings results for the Fixed Character Affixes ap- proach, because such features would be redundant  with the character embeddings themselves.", "labels": [], "entities": []}, {"text": " Table 6: Morphological tagging results. The absolute increase and error reduction are of the disam- biguated Bi-LSTM against MADAMIRA.", "labels": [], "entities": [{"text": "Morphological tagging", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.848663866519928}, {"text": "absolute increase", "start_pos": 45, "end_pos": 62, "type": "METRIC", "confidence": 0.9455592632293701}, {"text": "error reduction", "start_pos": 67, "end_pos": 82, "type": "METRIC", "confidence": 0.9822525382041931}, {"text": "Bi-LSTM", "start_pos": 110, "end_pos": 117, "type": "METRIC", "confidence": 0.91334468126297}]}, {"text": " Table 8: Accuracy results of the disambiguation system, evaluated using different metrics, for all words  and out-of-vacbulary (OOV) words alone. OOV percentage of all words is 7.9%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984278678894043}, {"text": "OOV percentage", "start_pos": 147, "end_pos": 161, "type": "METRIC", "confidence": 0.9783974587917328}]}]}