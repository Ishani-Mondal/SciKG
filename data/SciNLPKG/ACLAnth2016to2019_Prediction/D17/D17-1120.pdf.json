{"title": [{"text": "Neural Sequence Learning Models for Word Sense Disambiguation", "labels": [], "entities": [{"text": "Neural Sequence Learning", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7798415025075277}, {"text": "Word Sense Disambiguation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.7648826738198599}]}], "abstractContent": [{"text": "Word Sense Disambiguation models exist in many flavors.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7667762438456217}]}, {"text": "Even though supervised ones tend to perform best in terms of accuracy, they often lose ground to more flexible knowledge-based solutions, which do not require training by a word expert for every disambiguation target.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9972534775733948}]}, {"text": "To bridge this gap we adopt a different perspective and rely on sequence learning to frame the disambiguation problem: we propose and study in depth a series of end-to-end neural architectures directly tailored to the task, from bidirectional Long Short-Term Memory to encoder-decoder models.", "labels": [], "entities": []}, {"text": "Our extensive evaluation over standard benchmarks and in multiple languages shows that sequence learning enables more versatile all-words models that consistently lead to state-of-the-art results, even against word experts with engineered features.", "labels": [], "entities": []}], "introductionContent": [{"text": "As one of the long-standing challenges in Natural Language Processing (NLP), Word Sense Disambiguation, WSD) has received considerable attention over recent years.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7577467560768127}, {"text": "Word Sense Disambiguation, WSD)", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.7748331129550934}]}, {"text": "Indeed, by dealing with lexical ambiguity an effective WSD model brings numerous benefits to a variety of downstream tasks and applications, from Information to Machine Translation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9271503686904907}, {"text": "Machine Translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.7196363806724548}]}, {"text": "Recently, WSD has also been leveraged to build continuous vector representations for word senses.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6205012202262878}]}, {"text": "Inasmuch as WSD is described as the task of associating words in context with the most suitable entries in a pre-defined sense inventory, the majority of WSD approaches to date can be grouped into two main categories: supervised (or semisupervised) and knowledge-based.", "labels": [], "entities": []}, {"text": "Supervised models have been shown to consistently outperform knowledge-based ones in all standard benchmarks (, at the expense, however, of harder training and limited flexibility.", "labels": [], "entities": []}, {"text": "First of all, obtaining reliable sense-annotated corpora is highly expensive and especially difficult when non-expert annotators are involved (de, and as a consequence approaches based on unlabeled data and semisupervised learning are emerging.", "labels": [], "entities": []}, {"text": "Apart from the shortage of training data, a crucial limitation of current supervised approaches is that a dedicated classifier (word expert) needs to be trained for every target lemma, making them less flexible and hampering their use within endto-end applications.", "labels": [], "entities": []}, {"text": "In contrast, knowledge-based systems do not require sense-annotated data and often draw upon the structural properties of lexicosemantic resources.", "labels": [], "entities": []}, {"text": "Such systems construct a model based only on the underlying resource, which is then able to handle multiple target words at the same time and disambiguate them jointly, whereas word experts are forced to treat each disambiguation target in isolation.", "labels": [], "entities": []}, {"text": "In this paper our focus is on supervised WSD, but we depart from previous approaches and adopt a different perspective on the task: instead of framing a separate classification problem for each given word, we aim at modeling the joint disambiguation of the target text as a whole in terms of a sequence labeling problem.", "labels": [], "entities": [{"text": "WSD", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.8436188697814941}, {"text": "sequence labeling", "start_pos": 294, "end_pos": 311, "type": "TASK", "confidence": 0.6339385658502579}]}, {"text": "From this standpoint, WSD amounts to translating a sequence of words into a sequence of potentially sense-tagged tokens.", "labels": [], "entities": [{"text": "WSD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9900150299072266}]}, {"text": "With this in mind, we design, analyze and compare experimentally various neural architectures of different complexities, ranging from a single bidirectional Long Short-Term Memory (, LSTM) to a sequence-tosequence approach).", "labels": [], "entities": []}, {"text": "Each architecture reflects a particular way of modeling the disambiguation problem, but they all share some key features that set them apart from previous supervised approaches to WSD: they are trained end-to-end from sense-annotated text to sense labels, and learn a single all-words model from the training data, without fine tuning or explicit engineering of local features.", "labels": [], "entities": [{"text": "WSD", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.9544561505317688}]}, {"text": "The contributions of this paper are twofold.", "labels": [], "entities": []}, {"text": "First, we show that neural sequence learning represents a novel and effective alternative to the traditional way of modeling supervised WSD, enabling a single all-words model to compete with a pool of word experts and achieve state-of-the-art results, while also being easier to train, arguably more versatile to use within downstream applications, and directly adaptable to different languages without requiring additional sense-annotated data (as we show in Section 6.2); second, we carryout an extensive experimental evaluation where we compare various neural architectures designed for the task (and somehow left underinvestigated in previous literature), exploring different configurations and training procedures, and analyzing their strengths and weaknesses on all the standard benchmarks for all-words WSD.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we detail the setup of our experimental evaluation.", "labels": [], "entities": []}, {"text": "We first describe the training corpus and all the standard benchmarks for all-words WSD; we then report technical details on the architecture and on the training process for all the models described throughout Section 3 and their multitask augmentations (Section 4).", "labels": [], "entities": [{"text": "WSD", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.8778761625289917}]}, {"text": "https://wordnet.princeton.edu/man/ lexnames.5WN.html We use a dummy LEX label (other) for punctuation and function words.", "labels": [], "entities": []}, {"text": "We evaluated our models on the English all-words WSD task, considering both the fine-grained and coarsegrained benchmarks (Section 6.1).", "labels": [], "entities": [{"text": "WSD task", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.8180186152458191}]}, {"text": "As regards fine-grained WSD, we relied on the evaluation framework of, which includes five standardized test sets from the Senseval/SemEval series: Senseval-2 (Edmonds and Cotton, 2001, SE2), Senseval-3 (Snyder and Palmer, 2004, SE3),, and.", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9658012986183167}]}, {"text": "Due to the lack of a reasonably large development set for our setup, we considered the smallest among these test sets, i.e., SE07, as development set and excluded it from the evaluation of Section 6.1.", "labels": [], "entities": [{"text": "SE07", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.6719111800193787}, {"text": "Section 6.1", "start_pos": 189, "end_pos": 200, "type": "DATASET", "confidence": 0.8611103296279907}]}, {"text": "As for coarse-grained WSD, we used the SemEval-2007 task 7 test set (: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Raganato et al. (including the development set SE07).", "labels": [], "entities": [{"text": "SemEval-2007 task 7 test set", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.7569611549377442}, {"text": "F-scores", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9928708672523499}]}, {"text": "The first system with a statistically significant difference from our best models is marked with (unpaired t-test, p < 0.05).", "labels": [], "entities": []}, {"text": "At testing time, given a target word w, our models used the probability distribution over O, computed by the softmax layer at the corresponding time step, to rank the candidate senses of w; we then simply selected the top ranking candidate as output of the model.", "labels": [], "entities": []}, {"text": "To set a level playing field with comparison systems on English all-words WSD, we followed and, for all our models, we used a layer of word embeddings pre-trained 8 on the English ukWaC corpus ( as initialization, and kept them fixed during the training process.", "labels": [], "entities": [{"text": "English ukWaC corpus", "start_pos": 172, "end_pos": 192, "type": "DATASET", "confidence": 0.6837796568870544}]}, {"text": "For all architectures we then employed 2 layers of bidirectional LSTM with 2048 hidden units (1024 units per direction).", "labels": [], "entities": []}, {"text": "As regards multilingual all-words WSD (Section 6.2), we experimented, instead, with two different configurations of the embedding layer: the pre-trained bilingual embeddings by Mrk\u0161i\u00b4 Training.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4 Training", "start_pos": 177, "end_pos": 192, "type": "DATASET", "confidence": 0.9158312678337097}]}, {"text": "We used SemCor 3.0 ( as training corpus for all our experiments.", "labels": [], "entities": []}, {"text": "Widely known and utilized in the WSD literature, SemCor is one of the largest corpora annotated manually with word senses from the sense inventory of WordNet () for all openclass parts of speech.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8829660415649414}]}, {"text": "We used the standardized version of SemCor as provided in the evaluation framework 9 which also includes coarse-grained POS tags from the universal tagset.", "labels": [], "entities": []}, {"text": "All models were trained fora fixed number of epochs E = 40 using Adadelta) with learning rate 1.0 and batch size 32.", "labels": [], "entities": [{"text": "Adadelta", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.8074780702590942}, {"text": "learning rate 1.0", "start_pos": 80, "end_pos": 97, "type": "METRIC", "confidence": 0.9621580243110657}]}, {"text": "After each epoch we evaluated our models on the development set, and then compared the best iterations (E * ) on the development set with the reported state of the art in each benchmark.", "labels": [], "entities": []}, {"text": "Throughout this section we identify the models based on the LSTM tagger (Sections 3.1-3.2) by the label BLSTM, and the sequence-to-sequence models (Section 3.3) by the label Seq2Seq.", "labels": [], "entities": [{"text": "BLSTM", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.5108250379562378}]}, {"text": "shows the performance of our models on the standardized benchmarks for all-words finegrained WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8272263407707214}]}, {"text": "We report the F1-score on each in-: F-scores (%) for coarse-grained WSD.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9994373917579651}, {"text": "F-scores", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9970742464065552}, {"text": "WSD", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.6979848146438599}]}], "tableCaptions": [{"text": " Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.996239185333252}, {"text": "WSD", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.8878433704376221}]}, {"text": " Table 3: F-scores (%) for multilingual WSD.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9959119558334351}, {"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.7220892906188965}]}]}