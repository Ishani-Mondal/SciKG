{"title": [{"text": "Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce Zipporah, a fast and scal-able data cleaning system.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7329729795455933}]}, {"text": "We propose a novel type of bag-of-words translation feature , and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space.", "labels": [], "entities": [{"text": "bag-of-words translation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.656887024641037}]}, {"text": "The trained model is used to score parallel sentences in the data pool for selection.", "labels": [], "entities": []}, {"text": "As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool.", "labels": [], "entities": []}, {"text": "In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9799522459506989}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems require the use of parallel corpora for training the internal model parameters.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8201571305592855}]}, {"text": "Data quality is vital for the performance of the SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9963496923446655}]}, {"text": "To acquire a massive parallel corpus, many researchers have been using the Internet as a resource, but the quality of data acquired from the Internet usually has no guarantee, and data cleaning/data selection is needed before the data is used in actual systems.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 180, "end_pos": 193, "type": "TASK", "confidence": 0.7228463888168335}, {"text": "data selection", "start_pos": 194, "end_pos": 208, "type": "TASK", "confidence": 0.7093839943408966}]}, {"text": "Usually data cleaning refers to getting rid of a small amount of very noisy data from a large data pool, and data selection refers to selecting a small subset of clean (or in-domain) data from the data pool; both have the objective of improving translation performances.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.8222475945949554}, {"text": "data selection", "start_pos": 109, "end_pos": 123, "type": "TASK", "confidence": 0.7008986324071884}]}, {"text": "For practical purposes, it is highly desirable to perform data selection in a very fast and scalable manner.", "labels": [], "entities": [{"text": "data selection", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7255365550518036}]}, {"text": "In this paper we introduce Zipporah 1 , a fast and scalable system which can select an arbitrary size of good data from a large noisy data pool to be used in SMT model training.", "labels": [], "entities": [{"text": "SMT model training", "start_pos": 158, "end_pos": 176, "type": "TASK", "confidence": 0.9324228962262472}]}, {"text": "https://github.com/hainan-xv/zipporah", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate Zipporah on 3 language pairs, French-English, German-English and SpanishEnglish.", "labels": [], "entities": []}, {"text": "The noisy web-crawled data comes from an early version of http://statmt.org/ paracrawl.", "labels": [], "entities": []}, {"text": "The number of words are (in millions) 340, 487 and 70 respectively.", "labels": [], "entities": []}, {"text": "To generate the dictionaries for computing the adequacy scores, we use fast align) to align the Europarl () corpus and generate probabilistic dictionaries from the alignments.", "labels": [], "entities": [{"text": "Europarl () corpus", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.9233593940734863}]}, {"text": "We set the n-gram order to be 5 and use SRILM) to train language models on the Europarl corpus and generate the n-gram scores.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.959974467754364}, {"text": "Europarl corpus", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.9950282275676727}]}, {"text": "For each language pair, we use scikit-learn (Pedregosa et al., 2011) to train a logistic regression model to classify between the original and the synthetic noisy corpus of newstest09, and the trained model is used to score all sentence pairs in the data pool.", "labels": [], "entities": []}, {"text": "We keep selecting the best ones until the desired number of words is reached.", "labels": [], "entities": []}, {"text": "To evaluate the quality, we train a Moses ( SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sentence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS (, each of which contains 3000 sentence pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9720155596733093}, {"text": "TED-talks corpus", "start_pos": 205, "end_pos": 221, "type": "DATASET", "confidence": 0.8537517488002777}, {"text": "OPUS", "start_pos": 257, "end_pos": 261, "type": "DATASET", "confidence": 0.8123558163642883}]}, {"text": "show the BLEU performance of the selected subsets of the Zipporah system compared to the baseline, which selects sentence pairs at random; for comparison, we also give the BLEU performance of systems trained on Europarl.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9991795420646667}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9991589784622192}, {"text": "Europarl", "start_pos": 211, "end_pos": 219, "type": "DATASET", "confidence": 0.9889467358589172}]}, {"text": "The Zipporah system gives consistently better performance across multiple datasets and multiple languages than the baseline.: BLEU Performance, Spanish-English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9986347556114197}]}], "tableCaptions": [{"text": " Table 1: Tuning cross-entropy constant c", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9746381044387817}]}, {"text": " Table 2: BLEU Performance, French-English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981871247291565}]}, {"text": " Table 3: BLEU Performance, German-English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9970651268959045}]}, {"text": " Table 4: BLEU Performance, Spanish-English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977522492408752}]}]}