{"title": [{"text": "A causal framework for explaining the predictions of black-box sequence-to-sequence models", "labels": [], "entities": []}], "abstractContent": [{"text": "We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair.", "labels": [], "entities": []}, {"text": "Our method returns an \"explanation\" consisting of groups of input-output tokens that are causally related.", "labels": [], "entities": []}, {"text": "These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components.", "labels": [], "entities": []}, {"text": "We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations.", "labels": [], "entities": []}, {"text": "We test our method across several NLP sequence generation tasks.", "labels": [], "entities": [{"text": "NLP sequence generation tasks", "start_pos": 34, "end_pos": 63, "type": "TASK", "confidence": 0.8206885159015656}]}], "introductionContent": [{"text": "Interpretability is often the first casualty when adopting complex predictors.", "labels": [], "entities": []}, {"text": "This is particularly true for structured prediction methods at the core of many natural language processing tasks such as machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.8379274547100067}]}, {"text": "For example, deep learning models for NLP involve a large number of parameters and complex architectures, making them practically black-box systems.", "labels": [], "entities": []}, {"text": "While such systems achieve state-of-the-art results in MT (), summarization) and speech recognition (, they remain largely uninterpretable, although attention mechanisms () can shed some light on how they operate.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9679287672042847}, {"text": "summarization", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.9818275570869446}, {"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7926735281944275}]}, {"text": "Stronger forms of interpretability could offer several advantages, from trust in model predictions, error analysis, to model refinement.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7288616299629211}]}, {"text": "For example, critical medical decisions are increasingly being assisted by complex predictions that should lend themselves to easy verification by human experts.", "labels": [], "entities": []}, {"text": "Without understanding how inputs get mapped to the outputs, it is also challenging to diagnose the source of potential errors.", "labels": [], "entities": []}, {"text": "A slightly less obvious application concerns model improvement () where interpretability can be used to detect biases in the methods.", "labels": [], "entities": [{"text": "model improvement", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7072237432003021}]}, {"text": "Interpretability has been approached primarily from two main angles: model interpretability, i.e., making the architecture itself interpretable, and prediction interpretability, i.e., explaining particular predictions of the model (cf. ().", "labels": [], "entities": [{"text": "prediction interpretability", "start_pos": 149, "end_pos": 176, "type": "TASK", "confidence": 0.764578640460968}]}, {"text": "Requiring the model itself to be transparent is often too restrictive and challenging to achieve.", "labels": [], "entities": []}, {"text": "Indeed, prediction interpretability can be more easily sought a posteriori for black-box systems including neural networks.", "labels": [], "entities": [{"text": "prediction interpretability", "start_pos": 8, "end_pos": 35, "type": "TASK", "confidence": 0.9663676023483276}]}, {"text": "In this work, we propose a novel approach to prediction interpretability with only oracle access to the model generating the prediction.", "labels": [], "entities": [{"text": "prediction interpretability", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.9534474611282349}]}, {"text": "Following (, we turn the local behavior of the model around the given input into an interpretable representation of its operation.", "labels": [], "entities": []}, {"text": "In contrast to previous approaches, we consider structured prediction where both inputs and outputs are combinatorial objects, and our explanation consists of a summary of operation rather than a simpler prediction method.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7306535542011261}]}, {"text": "Our method returns an \"explanation\" consisting of sets of input and output tokens that are causally related under the black-box model.", "labels": [], "entities": []}, {"text": "Causal dependencies arise from analyzing perturbed versions of inputs that are passed through the black-box model.", "labels": [], "entities": []}, {"text": "Although such perturbations might be available in limited cases, we generate them automatically.", "labels": [], "entities": []}, {"text": "For sentences, we adopt a variational autoencoder to produce semantically related sentence variations.", "labels": [], "entities": []}, {"text": "The resulting inferred causal dependencies (interval estimates) form a dense bipartite graph over tokens from which explanations can be derived as robust min-cut k-partitions.", "labels": [], "entities": []}, {"text": "We demonstrate quantitatively that our method can recover known dependencies.", "labels": [], "entities": []}, {"text": "As a starting point, we show that a grapheme-to-phoneme dictionary can be largely recovered if given to the method as a black-box model.", "labels": [], "entities": []}, {"text": "We then show that the explanations provided by our method closely resemble the attention scores used by a neural machine translation system.", "labels": [], "entities": []}, {"text": "Moreover, we illustrate how our summaries can be used to gain insights and detect biases in translation systems.", "labels": [], "entities": []}, {"text": "Our main contributions are: \u2022 We propose a general framework for explaining structured black-box models \u2022 For sequential data, we propose a variational autoencoder for controlled generation of input perturbations required for causal analysis \u2022 We evaluate the explanations produced by our framework on various sequence-tosequence prediction tasks, showing they can recover known associations and provide insights into the workings of complex systems.", "labels": [], "entities": [{"text": "sequence-tosequence prediction tasks", "start_pos": 310, "end_pos": 346, "type": "TASK", "confidence": 0.7742467621962229}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: \"Good\" dialogue system predictions.", "labels": [], "entities": []}]}