{"title": [{"text": "Adapting Topic Models using Lexical Associations with Tree Priors", "labels": [], "entities": [{"text": "Adapting", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9644272923469543}]}], "abstractContent": [{"text": "Models work best when they are optimized taking into account the evaluation criteria that people care about.", "labels": [], "entities": []}, {"text": "For topic models, people often care about inter-pretability, which can be approximated using measures of lexical association.", "labels": [], "entities": []}, {"text": "We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured byword embeddings.", "labels": [], "entities": [{"text": "topic optimization", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7558571994304657}]}, {"text": "Tree priors improve topic interpretability without hurting ex-trinsic performance.", "labels": [], "entities": [{"text": "topic interpretability", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7073891758918762}]}], "introductionContent": [{"text": "Goodman (1996) introduces a key insight for machine learning models in natural language processing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others.", "labels": [], "entities": []}, {"text": "Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9761481285095215}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8263531029224396}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9900286793708801}]}, {"text": "provide a similar insight for topic models (: if what you care about is the interpretability of topics, the standard objective function for parameter inference (likelihood) is not only poorly correlated with a human-centered measurement of topic coherence, but inversely correlated.", "labels": [], "entities": []}, {"text": "Nonetheless, most topic models are still trained using methods that optimize likelihood).", "labels": [], "entities": []}, {"text": "We take the logical next step suggested when you bring together the insights of and, namely incorporating an approximation of human topic interpretability into the topic model optimization process in away that is effective and more straightforward than previous methods).", "labels": [], "entities": [{"text": "topic model optimization", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.6258125205834707}]}, {"text": "We take advantage of the human-centered evaluation of, which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus ().", "labels": [], "entities": []}, {"text": "We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior-specifically, we use external lexical association to create a tree structure and then use tree LDA, which derives topics using a given tree prior.", "labels": [], "entities": []}, {"text": "We construct tree priors with combinations of two types of word association scores (skip-gram probability () and G2 likelihood ratio) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication).", "labels": [], "entities": [{"text": "G2 likelihood ratio", "start_pos": 113, "end_pos": 132, "type": "METRIC", "confidence": 0.9502053260803223}]}, {"text": "Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets.", "labels": [], "entities": [{"text": "Amazon reviews", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.8818390369415283}, {"text": "20NewsGroups datasets", "start_pos": 77, "end_pos": 98, "type": "DATASET", "confidence": 0.9411333501338959}]}, {"text": "tLDA topics are more coherent compared with \"vanilla\" LDA topics, while retaining and often slightly improving topics' extrinsic performance as features for supervised classification.", "labels": [], "entities": []}, {"text": "Our approach can be viewed as a form of adaptation, and the flexibility of the tree prior approach-amenable to any kind of association score-suggests that there are many directions to pursue beyond the two flavors of association explored here.: An example of a tree prior (the tree structure) and gold posterior edge and word probabilities learned by tLDA.", "labels": [], "entities": []}, {"text": "Numbers beside the edges denote the probability of moving from the parent node to the child node.", "labels": [], "entities": []}, {"text": "A word's probability, i.e., the number below the word, is the product of probabilities moving from the root to the leaf.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compute two versions of word association scores from Gigaword, using word2vec () and G2 likelihood ratio.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9109883904457092}, {"text": "G2 likelihood ratio", "start_pos": 88, "end_pos": 107, "type": "METRIC", "confidence": 0.929301381111145}]}, {"text": "Given the word vectors vi and v j , which represent words w i and w j , their word2vec association score is Then we apply the three tree construction algorithms to construct six tree priors.", "labels": [], "entities": [{"text": "word2vec association score", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.6770896514256796}]}, {"text": "In the two-level trees, the value of N , i.e., the number of child nodes per internal node, is ten.", "labels": [], "entities": []}, {"text": "We use Amazon reviews (Jindal and Liu, 2008) and 20NewsGroups.", "labels": [], "entities": []}, {"text": "We apply the same tokenization and stopword removal methods.", "labels": [], "entities": [{"text": "stopword removal", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7122478783130646}]}, {"text": "We then sort the words by their document frequencies and return the top words, while also removing words that appear in more than 30% of the documents.", "labels": [], "entities": []}, {"text": "Both corpora are split into five folds.", "labels": [], "entities": []}, {"text": "For classification tasks, each fold is further equally split into a development set and a test set.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9223401248455048}]}, {"text": "All the results are averaged across five-fold cross-validation using 20 topics with hyper-parameters \u03b1 = \u03b2 = 0.01.", "labels": [], "entities": []}, {"text": "For 20NewsGroups classification, a post's newsgroup is its label.", "labels": [], "entities": [{"text": "20NewsGroups classification", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6318390369415283}]}, {"text": "For Amazon reviews, 4-5 star reviews have positive labels, 1-2 stars negative, and reviews with 3 stars are discarded.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The average perplexity results on the test  sets by various models. LDA gives the lowest per- plexity, because tLDA models have constraint from  the tree priors and sacrifice the perplexity.", "labels": [], "entities": []}, {"text": " Table 4: Accuracies of topical classification on  20NewsGroups and sentiment analysis on Ama- zon reviews. Although not significantly improving  the performance, tLDA topics at least do not hurt.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9711771011352539}, {"text": "topical classification", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8331733047962189}, {"text": "sentiment analysis", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.847726434469223}, {"text": "Ama- zon reviews", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9030457735061646}]}]}