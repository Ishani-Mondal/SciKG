{"title": [{"text": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues", "labels": [], "entities": []}], "abstractContent": [{"text": "Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions.", "labels": [], "entities": []}, {"text": "Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI.", "labels": [], "entities": []}, {"text": "We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue.", "labels": [], "entities": []}, {"text": "For the first time, we show it is possible to train end-to-end models for negotiation , which must learn both linguistic and reasoning skills with no annotated dialogue states.", "labels": [], "entities": []}, {"text": "We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance.", "labels": [], "entities": []}, {"text": "Our code and dataset are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Intelligent agents often need to cooperate with others who have different goals, and typically use natural language to agree on decisions.", "labels": [], "entities": []}, {"text": "Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised.", "labels": [], "entities": []}, {"text": "Such dialogues contain both cooperative and adversarial elements, and require agents to understand, plan, and generate utterances to achieve their goals ().", "labels": [], "entities": []}, {"text": "We collect the first large dataset of natural language negotiations between two people, and show that end-to-end neural models can be trained to negotiate by maximizing the likelihood of human actions.", "labels": [], "entities": []}, {"text": "This approach is scalable and domainindependent, but does not model the strategic skills required for negotiating well.", "labels": [], "entities": []}, {"text": "We further show that models can be improved by training and decoding to maximize reward instead of likelihood-by training with self-play reinforcement learning, and using rollouts to estimate the expected reward of utterances during decoding.", "labels": [], "entities": []}, {"text": "To study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task.", "labels": [], "entities": []}, {"text": "Users were shown a set of items with a value for each, and asked to agree how to divide the items with another user who has a different, unseen, value function ().", "labels": [], "entities": []}, {"text": "We first train recurrent neural networks to imitate human actions.", "labels": [], "entities": []}, {"text": "We find that models trained to maximise the likelihood of human utterances can generate fluent language, but make comparatively poor negotiators, which are overly willing to compromise.", "labels": [], "entities": []}, {"text": "We therefore explore two methods for improving the model's strategic reasoning skillsboth of which attempt to optimise for the agent's goals, rather than simply imitating humans: Firstly, instead of training to optimise likelihood, we show that our agents can be considerably improved using self play, in which pre-trained models practice negotiating with each other in order to optimise performance.", "labels": [], "entities": []}, {"text": "To avoid the models diverging from human language, we interleave reinforcement learning updates with supervised updates.", "labels": [], "entities": []}, {"text": "For the first time, we show that end-toend dialogue agents trained using reinforcement learning outperform their supervised counterparts in negotiations with humans.", "labels": [], "entities": []}, {"text": "Secondly, we introduce anew form of planning for dialogue called dialogue rollouts, in which an: A dialogue in our Mechanical Turk interface, which we used to collect a negotiation dataset.", "labels": [], "entities": []}, {"text": "agent simulates complete dialogues during decoding to estimate the reward of utterances.", "labels": [], "entities": []}, {"text": "We show that decoding to maximise the reward function (rather than likelihood) significantly improves performance against both humans and machines.", "labels": [], "entities": []}, {"text": "Analysing the performance of our agents, we find evidence of sophisticated negotiation strategies.", "labels": [], "entities": []}, {"text": "For example, we find instances of the model feigning interest in a valueless issue, so that it can later 'compromise' by conceding it.", "labels": [], "entities": []}, {"text": "Deceit is a complex skill that requires hypothesising the other agent's beliefs, and is learnt relatively late in child development ().", "labels": [], "entities": [{"text": "Deceit", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9387274980545044}]}, {"text": "Our agents have learnt to deceive without any explicit human design, simply by trying to achieve their goals.", "labels": [], "entities": []}, {"text": "The rest of the paper proceeds as follows: \u00a72 describes the collection of a large dataset of humanhuman negotiation dialogues.", "labels": [], "entities": []}, {"text": "\u00a73 describes a baseline supervised model, which we then show can be improved by goal-based training ( \u00a74) and decoding ( \u00a75).", "labels": [], "entities": []}, {"text": "\u00a76 measures the performance of our models and humans on this task, and \u00a77 gives a detailed analysis and suggests future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For development, we use measured the perplexity of user generated utterances, conditioned on the input and previous dialogue.", "labels": [], "entities": []}, {"text": "Results are shown in, and show that the simple LIKELIHOOD model produces the most human-like responses, and the alternative training and decoding strategies cause a divergence from human language.", "labels": [], "entities": [{"text": "LIKELIHOOD", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9573417901992798}]}, {"text": "Note however, that this divergence may not necessarily correspond to lower quality language-it may also indicate different strategic decisions about what to say.", "labels": [], "entities": []}, {"text": "Results in \u00a76.4 show all models could converse with humans.", "labels": [], "entities": []}, {"text": "We measure end-to-end performance in dialogues both with the likelihood-based agent and with humans on Mechanical Turk, on held out scenarios.", "labels": [], "entities": []}, {"text": "Humans were told that they were interacting with other humans, as they had been during the collection of our dataset (and few appeared to realize they were in conversation with machines).", "labels": [], "entities": []}, {"text": "We measure the following statistics: Score: The average score for each agent (which could be a human or model), out of 10.", "labels": [], "entities": [{"text": "Score", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9750708937644958}]}, {"text": "Agreement: The percentage of dialogues where both agents agreed on the same decision.", "labels": [], "entities": []}, {"text": "Pareto Optimality: The percentage of Pareto optimal solutions for agreed deals (a solution is Pareto optimal if neither agent's score can be improved without lowering the other's score).", "labels": [], "entities": []}, {"text": "Lower scores indicate inefficient negotiations.", "labels": [], "entities": []}, {"text": "Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the LIKELIHOOD model, particularly the RL+ROLLOUTS model.", "labels": [], "entities": [{"text": "ROLLOUTS", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8884214758872986}]}, {"text": "The percentage of Pareto optimal solutions also increases, showing a better exploration of the solution space.", "labels": [], "entities": []}, {"text": "Compared to human-human negotiations, the best models achieve a higher agreement rate, better scores, and similar Pareto efficiency.", "labels": [], "entities": [{"text": "Pareto", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9875245094299316}]}, {"text": "This result confirms that attempting to maximise reward can outperform simply imitating humans.", "labels": [], "entities": []}, {"text": "Similar trends hold in dialogues with humans, with goal-based reasoning outperforming imitation learning.", "labels": [], "entities": []}, {"text": "The ROLLOUTS model achieves comparable scores to its human partners, and the RL+ROLLOUTS model actually achieves higher scores.", "labels": [], "entities": [{"text": "RL+ROLLOUTS", "start_pos": 77, "end_pos": 88, "type": "METRIC", "confidence": 0.6027949253718058}]}, {"text": "However, we also find significantly more cases of the goal-based models failing to agree a deal with humans-largely a consequence of their more aggressive negotiation tactics (see \u00a77).", "labels": [], "entities": []}, {"text": "shows large gains from goal-based methods.", "labels": [], "entities": []}, {"text": "In this section, we explore the strengths and weaknesses of our models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: End task evaluation on heldout scenarios, against the LIKELIHOOD model and humans from  Mechanical Turk. The maximum score is 10. Score (all) gives 0 points when agents failed to agree.", "labels": [], "entities": [{"text": "LIKELIHOOD", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.918415904045105}, {"text": "Score", "start_pos": 140, "end_pos": 145, "type": "METRIC", "confidence": 0.9774367809295654}]}, {"text": " Table 2: Statistics on our dataset of crowd- sourced dialogues between humans.", "labels": [], "entities": []}, {"text": " Table 3: Intrinsic evaluation showing the average  perplexity of tokens and rank of complete turns  (out of 2083 unique human messages from the test  set). Lower is more human-like for both.", "labels": [], "entities": []}]}