{"title": [{"text": "Learning Chinese Word Representations From Glyphs Of Characters", "labels": [], "entities": [{"text": "Learning Chinese Word Representations From Glyphs Of Characters", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.722554873675108}]}], "abstractContent": [{"text": "In this paper, we propose new methods to learn Chinese word representations.", "labels": [], "entities": [{"text": "learn Chinese word representations", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.5828898921608925}]}, {"text": "Chi-nese characters are composed of graphical components, which carry rich semantics.", "labels": [], "entities": []}, {"text": "It is common fora Chinese learner to comprehend the meaning of a word from these graphical components.", "labels": [], "entities": [{"text": "comprehend the meaning of a word", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.7629920740922292}]}, {"text": "As a result, we propose models that enhance word representations by character glyphs.", "labels": [], "entities": []}, {"text": "The character glyph features are directly learned from the bitmaps of characters by con-volutional auto-encoder(convAE), and the glyph features improve Chinese word representations which are already enhanced by character embeddings.", "labels": [], "entities": []}, {"text": "Another contribution in this paper is that we created several evaluation datasets in traditional Chi-nese and made them public.", "labels": [], "entities": []}], "introductionContent": [{"text": "No matter which target language it is, high quality word representations (also known as word \"embeddings\") are keys to many natural language processing tasks, for example, sentence classification, question answering (, machine translation (), etc.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.7942096292972565}, {"text": "question answering", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.8664531409740448}, {"text": "machine translation", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.763592928647995}]}, {"text": "Besides, word-level representations are building blocks in producing phrase-level () and sentence-level ( representations.", "labels": [], "entities": []}, {"text": "In this paper, we focus on learning Chinese word representations.", "labels": [], "entities": [{"text": "learning Chinese word representations", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.5916275680065155}]}, {"text": "A Chinese word is composed of characters which contain rich semantics.", "labels": [], "entities": []}, {"text": "The meaning of a Chinese word is often related to the meaning of its compositional characters.", "labels": [], "entities": []}, {"text": "Therefore, Chinese word embedding can be enhanced by its compositional character embeddings (.", "labels": [], "entities": []}, {"text": "Furthermore, a Chinese character is composed of several graphical components.", "labels": [], "entities": []}, {"text": "Characters with the same component share similar semantic or pronunciation.", "labels": [], "entities": []}, {"text": "When a Chinese user encounters a previously unseen character, it is instinctive to guess the meaning (and pronunciation) from its graphical components, so understanding the graphical components and associating them with semantics help people learning Chinese.", "labels": [], "entities": []}, {"text": "Radicals 1 are the graphical components used to index Chinese characters in a dictionary.", "labels": [], "entities": []}, {"text": "By identifying the radical of a character, one obtains a rough meaning of that character, so it is used in learning Chinese word embedding () and character embedding (.", "labels": [], "entities": []}, {"text": "However, other components in addition to radicals may contain potentially useful information in word representation learning.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 96, "end_pos": 124, "type": "TASK", "confidence": 0.867607057094574}]}, {"text": "Our research begins with a question: Can machines learn Chinese word representations from glyphs of characters?", "labels": [], "entities": []}, {"text": "By exploiting the glyphs of characters as images in word representation learning, all the graphical components in a character are considered, not limited to radicals.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.7527918815612793}]}, {"text": "In our proposed methods, we render character glyphs to fixed-size grayscale images which are referred to as \"character bitmaps\", as illustrated in.", "labels": [], "entities": []}, {"text": "A similar idea was also used in () to help classifying wikipedia article titles into 12 categories.", "labels": [], "entities": [{"text": "classifying wikipedia article titles", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.792545422911644}]}, {"text": "We use a convAE to extract character features from the bitmap to represent the glyphs.", "labels": [], "entities": []}, {"text": "It is also possible to represent the glyph of a character by the graphical components in it.", "labels": [], "entities": []}, {"text": "We do not choose this way because there is no unique way to decompose a character, and directly learning representation from bitmaps is more straightforward.", "labels": [], "entities": []}, {"text": "Then we use the models parallel to) or GloVe () to learn word representations from the character glyph features.", "labels": [], "entities": []}, {"text": "Although we only consider traditional Chinese characters in this paper, and the examples given below are based on the traditional characters, the same ideas and methods can be applied on the simplified characters.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman's correlation between human  annotated scores and cosine similarity of word  representations on three datasets: WordSim-240,  WordSim-296 and SimLex-999. The higher the  values, the better the results.", "labels": [], "entities": [{"text": "WordSim-240", "start_pos": 131, "end_pos": 142, "type": "DATASET", "confidence": 0.9826985001564026}, {"text": "WordSim-296", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.9456357359886169}]}, {"text": " Table 2. We found only \"tarG\"  method has a p-value less than 0.05 over CWE.", "labels": [], "entities": [{"text": "CWE", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8769178986549377}]}, {"text": " Table 2: p-values of significance tests between  proposed methods and existing ones.", "labels": [], "entities": []}, {"text": " Table 3: Accuracy of analogy problems for capi- tals of countries, (China) states/provinces of cities,  family relations, and our proposed job&place  (J&P) dataset. The higher the values, the better  the results.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9873565435409546}]}, {"text": " Table 4: Case study on word pairs in SimLex-999.", "labels": [], "entities": []}, {"text": " Table 5: Counter examples to which GWE meth- ods give higher similarity scores than CBOW or  CWE.", "labels": [], "entities": [{"text": "similarity", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.969708263874054}]}]}