{"title": [{"text": "Using Context Information for Dialog Act Classification in DNN Framework", "labels": [], "entities": [{"text": "Dialog Act Classification", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.9383402466773987}, {"text": "DNN Framework", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9051178991794586}]}], "abstractContent": [{"text": "Previous work on dialog act (DA) classification has investigated different methods, such as hidden Markov models, maximum entropy, conditional random fields, graph-ical models, and support vector machines.", "labels": [], "entities": [{"text": "dialog act (DA) classification", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.7511636316776276}]}, {"text": "A few recent studies explored using deep learning neural networks for DA classification , however, it is not clear yet what is the best method for using dialog context or DA sequential information, and how much gain it brings.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.9845536053180695}]}, {"text": "This paper proposes several ways of using context information for DA classification, all in the deep learning framework.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.9895313680171967}]}, {"text": "The baseline system classifies each utterance using the con-volutional neural networks (CNN).", "labels": [], "entities": []}, {"text": "Our proposed methods include using hierarchical models (recurrent neural networks (RNN) or CNN) for DA sequence tagging where the bottom layer takes the sentence CNN representation as input, concatenat-ing predictions from the previous utterances with the CNN vector for classification , and performing sequence decoding based on the predictions from the sentence CNN model.", "labels": [], "entities": [{"text": "DA sequence tagging", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8532816569010416}]}, {"text": "We conduct thorough experiments and comparisons on the Switchboard corpus, demonstrate that incorporating context information significantly improves DA classification, and show that we achieve new state-of-the-art performance for this task.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7840149104595184}, {"text": "DA classification", "start_pos": 149, "end_pos": 166, "type": "TASK", "confidence": 0.9731961488723755}]}], "introductionContent": [{"text": "Dialog act (DA) represents a function of a speaker's utterance in either human-to-human or human-to-computer conversations.", "labels": [], "entities": [{"text": "Dialog act (DA)", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5443506836891174}]}, {"text": "Correct identification of DAs is important for understanding human conversations, as well as for developing intelligent human-to-computer dialog systems (either written or spoken dialogs).", "labels": [], "entities": [{"text": "Correct identification of DAs", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6997511684894562}]}, {"text": "For example, recognizing DAs can help identify questions and answers in meetings, customer service, online forum, etc.", "labels": [], "entities": []}, {"text": "Many machine learning techniques have been investigated and shown reasonable performance for DA classification, for example, (, just to name a few.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.9809275567531586}]}, {"text": "Intuitively we would expect that leveraging dialog context can help classify the current utterance.", "labels": [], "entities": []}, {"text": "For example, if the previous sentence is a question, then there is a high probability that the current sentence is a response to that question.", "labels": [], "entities": []}, {"text": "Such context information has been explored in some previous methods, for example, hidden Markov models (HMM), conditional random fields (CRF), dynamic Bayesian networks (DBN).", "labels": [], "entities": []}, {"text": "Given the recent success of the deep learning framework in various language processing tasks, in this work we also employ neural networks for DA classification.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.9568378329277039}]}, {"text": "In fact, such models have been used in some recent studies for DA classification, e.g.,; however, previous work has not thoroughly evaluated the use of context information for this task, and there is still alack of good understanding about how we can use context information and how useful it is.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9915479123592377}]}, {"text": "This is the question we aim to answer in this work.", "labels": [], "entities": []}, {"text": "The contributions of this paper are: 1) We propose several ways to incorporate context information for DA classification over the baseline method of using convolutional neural networks (CNN) for sentence classification, including: (a) a hierarchical RNN/LSTM and CNN to model the utterance sequence in the conversation, where the input to the higher level LSTM and CNN unit is the sentence vector from the sentence level CNN model; (b) a two-step approach where the predicted DA results for the previous utterances, either labels or probability distributions, are concatenated with the sentence CNN vector for the current utterance as the new input for classification; (c) sequence level decoding based on the predicted DA probabilities and the transition probabilities between DA labels.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.9624148309230804}, {"text": "sentence classification", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.7378157079219818}]}, {"text": "Some of these methods have not been exploited previously for this task.", "labels": [], "entities": []}, {"text": "2) We perform a detailed and thorough analysis of different modeling approaches and some impacting factors in the models (such as the context length, representations and quality of the predictions).", "labels": [], "entities": []}, {"text": "This is the first study with such kind of comparisons.", "labels": [], "entities": []}, {"text": "3) We achieve new state-of-the-art results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: DA classification results (%) when using  the hierarchical structure: sentence CNN followed  by dialog sequence level CNN or RNN/BLSTM.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8497419059276581}, {"text": "BLSTM", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.3685266673564911}]}, {"text": " Table 5: DA classification results (%) when incorporating history DA information in the current utterance  in the CNN method. Three factors are examined: context history length, DA representations, and where  DA information is from.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9230079650878906}]}, {"text": " Table 6: DA classification results (%) using differ- ent systems.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7317393124103546}]}]}