{"title": [{"text": "Initializing Convolutional Filters with Semantic Features for Text Classification", "labels": [], "entities": [{"text": "Text Classification", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7615862190723419}]}], "abstractContent": [{"text": "Convolutional Neural Networks (CNNs) are widely used in NLP tasks.", "labels": [], "entities": []}, {"text": "This paper presents a novel weight initialization method to improve the CNNs for text classification.", "labels": [], "entities": [{"text": "weight initialization", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.5882193744182587}, {"text": "text classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.8442123830318451}]}, {"text": "Instead of randomly initializing the convolutional filters, we encode semantic features into them, which helps the model focus on learning useful features at the beginning of the training.", "labels": [], "entities": []}, {"text": "Experiments demonstrate the effectiveness of the ini-tialization technique on seven text classification tasks, including sentiment analysis and topic classification.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.7822254697481791}, {"text": "sentiment analysis", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.9607973992824554}, {"text": "topic classification", "start_pos": 144, "end_pos": 164, "type": "TASK", "confidence": 0.8479121625423431}]}], "introductionContent": [{"text": "Recently, neural networks (NNs) dominate the state-of-the-art results on a wide range of natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "The commonly used neural networks in NLP include Recurrent NNs, Convolutional NNs, Recursive NNs and their combinations.", "labels": [], "entities": []}, {"text": "NNs are known for their strong abilities to learn features automatically.", "labels": [], "entities": [{"text": "NNs", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8108717799186707}]}, {"text": "However, the lack of data or inappropriate parameter settings might greatly limit the generalization abilities of the models ().", "labels": [], "entities": []}, {"text": "To enhance the performance, a lot of improved methods have been proposed, e.g. developing advanced structures (), introducing prior knowledge () and utilizing external resources (.", "labels": [], "entities": []}, {"text": "It is also noteworthy that the neural networks' performance is sensitive to weight initialization \u2020 Corresponding author.", "labels": [], "entities": []}, {"text": "because their objectives are non-convex.", "labels": [], "entities": []}, {"text": "In fact, initialization techniques even play a role of catalyst for the revival of neural networks;.", "labels": [], "entities": [{"text": "initialization", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.9759913086891174}]}, {"text": "Most improvements on initializing weights are based on mathematical methods, e.g. xavier initialization and orthogonal initialization).", "labels": [], "entities": [{"text": "initializing", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.9804329872131348}]}, {"text": "For NLP tasks, an influential technique is to use pretrained word vectors to initialize embedding layers.", "labels": [], "entities": []}, {"text": "Consider the embedding layers could be initialized by pretrained word vectors, how about weights in other layers that are still randomly initialized?", "labels": [], "entities": []}, {"text": "Inspired by this question, we propose a simple yet effective method to improve CNNs by initializing convolutional layers (filters).", "labels": [], "entities": []}, {"text": "Unlike the previous weight initialization based on mathematical methods, we encode semantic features into the filters instead of initializing them randomly.", "labels": [], "entities": []}, {"text": "As CNNs exploit 1-D convolutional filters to extract n-gram features, our method aims at helping the filters focus on learning useful n-grams, e.g. \"not bad\" which is more useful than \"watch a movie\" for determining reviews' polarities.", "labels": [], "entities": []}, {"text": "Specifically, we select n-grams from training data via a novel Naive Bayes (NB) weighting technique, and then cluster the n-gram embeddings with K-means algorithm.", "labels": [], "entities": []}, {"text": "After that, we use the centroid vectors of the clusters to initialize the filters.", "labels": [], "entities": []}, {"text": "With this initialization method, CNN filters tend to extract important n-gram features at the beginning of the training process.", "labels": [], "entities": []}, {"text": "By integrating our method into a classic CNN model for text classification, we observe significant im-provements in sentiment analysis and topic classification tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.768526703119278}, {"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9548328220844269}, {"text": "topic classification tasks", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.8608787854512533}]}, {"text": "The advantages of our approach are as follows: \u2022 Features are directly extracted from training data without involving any external resources; \u2022 The computation brought by our method is relatively small, resulting in small additional training costs; \u2022 The filter initialization is task independent.", "labels": [], "entities": [{"text": "filter initialization", "start_pos": 255, "end_pos": 276, "type": "TASK", "confidence": 0.6602961421012878}]}, {"text": "It could be easily applied to other NLP tasks.", "labels": [], "entities": []}, {"text": "Also, we further analyze the filters, shedding some light on the mechanism how our method influences the training process.", "labels": [], "entities": []}, {"text": "The source code is released at https://github.com/ shenshen-hungry/Semantic-CNN.", "labels": [], "entities": []}], "datasetContent": [{"text": "CNN-non-static 1 (short for CNN) proposed by is used as our baseline, which consists of one embedding layer, one convolutional layer, one max pooling layer, and one fully connected layer.", "labels": [], "entities": []}, {"text": "The model proposed by is a strong baseline in sentence classification.", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.7991189062595367}]}, {"text": "For details of the model, one can see.", "labels": [], "entities": []}, {"text": "Pre-trained word embeddings on Google News via word2vec toolkit 2 are used for initializing the convolutional filters, besides initializing the embedding layer of CNN as in).", "labels": [], "entities": []}, {"text": "For a fair comparison, we use the same seven datasets 3 and hyper-parameter setting with Kim (2014)'s work for training and testing.", "labels": [], "entities": []}, {"text": "Uni, bi, and tri-gram features are used to initialize the filters.", "labels": [], "entities": []}, {"text": "For a K-way classification problem, we select top 10% n-grams in each class according to NB weighting.", "labels": [], "entities": [{"text": "K-way classification", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.6180698275566101}]}, {"text": "Since 300 filters are used in's work, we follow this setting and aggregate n-grams into 300/K clusters for each class.", "labels": [], "entities": []}, {"text": "Centroid vectors are used for filling the filters.", "labels": [], "entities": []}, {"text": "Taking binary classification dataset MR as an example, 150 \"positive\" filters and 150 \"negative\" filters are obtained after initialization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effectiveness of filter initialization.", "labels": [], "entities": [{"text": "filter initialization", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6689006090164185}]}, {"text": " Table 2: Comparisons of state-of-the-arts.", "labels": [], "entities": []}, {"text": " Table 3: \"+\" and \"-\" are used to denote the num- ber of positive and negative weights respectively.  The data in the table are obtained from MR by the  average of 10 times training. Every time we select  100 filters. 50 of them are initialized with positive  n-grams and the rest are with negative n-grams.", "labels": [], "entities": []}]}