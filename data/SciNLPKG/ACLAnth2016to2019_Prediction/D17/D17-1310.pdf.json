{"title": [{"text": "Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction *", "labels": [], "entities": [{"text": "Aspect Term Extraction", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8951076666514078}]}], "abstractContent": [{"text": "We propose a novel LSTM-based deep multi-task learning framework for aspect term extraction from user review sentences.", "labels": [], "entities": [{"text": "aspect term extraction from user review sentences", "start_pos": 69, "end_pos": 118, "type": "TASK", "confidence": 0.8125374828066144}]}, {"text": "Two LSTMs equipped with extended memories and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions.", "labels": [], "entities": []}, {"text": "Sentimental sentence constraint is also added for more accurate prediction via another LSTM.", "labels": [], "entities": [{"text": "Sentimental sentence constraint", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6234477758407593}]}, {"text": "Experiment results over two benchmark datasets demonstrate the effectiveness of our framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "The aspect-based sentiment analysis (ABSA) task is to identify opinions expressed towards specific entities such as laptop or attributes of entities such as price (.", "labels": [], "entities": [{"text": "aspect-based sentiment analysis (ABSA)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8196384161710739}]}, {"text": "This task involves three subtasks: Aspect Term Extraction (ATE), Aspect Polarity Detection and Aspect Category Detection.", "labels": [], "entities": [{"text": "Aspect Term Extraction (ATE)", "start_pos": 35, "end_pos": 63, "type": "METRIC", "confidence": 0.6245354612668356}, {"text": "Aspect Polarity Detection", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.6286035676797231}, {"text": "Aspect Category Detection", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.6556521554787954}]}, {"text": "As a fundamental subtask in ABSA, the goal of the ATE task is to identify opinionated aspect expressions.", "labels": [], "entities": [{"text": "ABSA", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.9454814195632935}, {"text": "ATE task", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.8324312567710876}]}, {"text": "One of most important characteristics is that opinion words can provide indicative clues for aspect detection since opinion words should co-occur with aspect words.", "labels": [], "entities": [{"text": "aspect detection", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.9638001322746277}]}, {"text": "Most publicly available datasets contain the gold standard annotations for opinionated aspects, but the ground truth of the corresponding opinion words is not commonly provided.", "labels": [], "entities": []}, {"text": "Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning (Jin * The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).", "labels": [], "entities": [{"text": "ATE task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9345579743385315}, {"text": "aspect term modeling", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.6992227832476298}]}, {"text": "We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback.", "labels": [], "entities": []}, {"text": "They fail to leverage opinion information which is supposed to be useful clues.", "labels": [], "entities": []}, {"text": "Some works tackling the ATE task consider opinion information () in an unsupervised or partially supervised manner.", "labels": [], "entities": [{"text": "ATE task", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9146887362003326}]}, {"text": "proposed Double Propagation (DP) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph.", "labels": [], "entities": []}, {"text": "One drawback is that it heavily relies on the dependency parser, which is prone to generate mistakes when applying on informal online reviews.", "labels": [], "entities": []}, {"text": "modeled relation between aspects and opinions by constructing a bipartite heterogenous graph.", "labels": [], "entities": []}, {"text": "It cannot perform well without a high-quality phrase chunker and POS tagger reducing its flexibility.", "labels": [], "entities": [{"text": "phrase chunker", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7099461853504181}, {"text": "POS tagger", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.6622620075941086}]}, {"text": "As unsupervised or partially supervised frameworks cannot take the full advantages of aspect annotations commonly found in the training data, the above methods lead to deficiency in leveraging the data.", "labels": [], "entities": []}, {"text": "Recently, considered relation between opinion words and aspect words in a supervised model named RNCRF.", "labels": [], "entities": [{"text": "RNCRF", "start_pos": 97, "end_pos": 102, "type": "DATASET", "confidence": 0.7386202812194824}]}, {"text": "However, RNCRF tends to suffer from parsing errors since the structure of the recursive network hinges on the dependency parse tree.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9794864654541016}]}, {"text": "CMLA () used a multilayer neural model where each layer consists of aspect attention and opinion attention.", "labels": [], "entities": [{"text": "CMLA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9109204411506653}]}, {"text": "However CMLA merely employs standard GRU without extended memories.", "labels": [], "entities": [{"text": "CMLA", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.878384530544281}]}, {"text": "We propose MIN (Memory Interaction Network), a novel LSTM-based deep multi-task learning framework for the ATE task.", "labels": [], "entities": [{"text": "ATE task", "start_pos": 107, "end_pos": 115, "type": "TASK", "confidence": 0.845545768737793}]}, {"text": "Two LSTMs with extended memory are designed for handling the extraction tasks of aspects and opinions.", "labels": [], "entities": [{"text": "extraction tasks of aspects and opinions", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.7936794757843018}]}, {"text": "The aspect-opinion relationship is established based on neural memory interactions between aspect extraction and opinion extraction where the global indicator score of opinion terms and local positional relevance between aspects and opinions are considered.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 91, "end_pos": 108, "type": "TASK", "confidence": 0.7260055094957352}, {"text": "opinion extraction", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7097073942422867}]}, {"text": "To ensure that aspects are from sentimental sentences, MIN employs a third LSTM for sentimental sentence classification facilitating more accurate aspect term extraction.", "labels": [], "entities": [{"text": "MIN", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8316454291343689}, {"text": "sentimental sentence classification", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.7616150180498759}, {"text": "aspect term extraction", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.7223639488220215}]}, {"text": "Experiment results over two benchmark datasets show that our framework achieves superior performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments on two benchmark datasets from SemEval ABSA challenge ( as shown in.", "labels": [], "entities": [{"text": "SemEval ABSA challenge", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6674508651097616}]}, {"text": "D 1 (SemEval 2014) contains reviews from the laptop domain and D 2 (SemEval 2016) contains reviews from the restaurant domain.", "labels": [], "entities": []}, {"text": "In these datasets, aspect terms have been labeled and sentences containing at least one golden truth aspect are regarded as sentimental sentences.", "labels": [], "entities": []}, {"text": "As gold standard annotations for opinion words are not provided, we select words with strong subjectivity from MPQA 1 as potential opinion words.", "labels": [], "entities": [{"text": "MPQA 1", "start_pos": 111, "end_pos": 117, "type": "DATASET", "confidence": 0.9285919368267059}]}, {"text": "Apart from the common opinion words in the sentiment lexicon, we also treat words, which directly depend on gold standard aspect terms through highprecision dependency rules, as opinion words.", "labels": [], "entities": []}, {"text": "To evaluate the proposed MIN framework, we perform comparison with the following two groups of methods: (1) CRF based methods: \u2022 CRF: Conditional Random Fields with basic feature templates 2 and word embeddings.", "labels": [], "entities": []}, {"text": "\u2022 Semi-CRF: First-order semi-Markov conditional random fields () and the feature template in is adopted.", "labels": [], "entities": []}, {"text": "\u2022 IHS RD), NLANGP (Toh and Su, 2016): Best systems in ATE subtask in SemEval ABSA challenges ().", "labels": [], "entities": [{"text": "IHS RD", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.5770590603351593}, {"text": "NLANGP", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7538977265357971}, {"text": "SemEval ABSA challenges", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7992332776387533}]}, {"text": "\u2022 DLIREC (Toh and), AUEB (Xenos et al., 2016): Top-ranked CRF-based systems in ATE subtask in SemEval ABSA challenges ().", "labels": [], "entities": [{"text": "DLIREC", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.7637283802032471}, {"text": "AUEB", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.8213307857513428}, {"text": "SemEval ABSA", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.8302914202213287}]}, {"text": "\u2022 WDEmb (Yin et al., 2016): Enhanced CRF with word embeddings, linear context embeddings and dependency path embeddings.", "labels": [], "entities": [{"text": "WDEmb", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.7780811190605164}]}, {"text": "(2) Neural Network based methods \u2022 LSTM: Vanilla bi-directional LSTM with pre-trained word embeddings 3 . \u2022 RNCRF (: Dependency Tree based Recursive Neural Network with CRF extractor 4 . For datasets in the restaurant domain, we train word embeddings of dimension 200 with word2vec () on Yelp reviews . For those in laptop domain, we use pre-trained glove.840B.300d 6 . 2 http://sklearn-crfsuite.readthedocs.io/en/latest/ 3 As we use our own implementation of LSTM, the reported results are different from those in ( 4 Specifically, we list the result of RNCRF over D1 without opinion annotations for fair comparison.", "labels": [], "entities": []}, {"text": "As no result is provided for RNCRF-no-opinion over D2, we report the corresponding performance of the full model.", "labels": [], "entities": [{"text": "RNCRF-no-opinion", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.7245906591415405}]}, {"text": "See their following works (.", "labels": [], "entities": []}, {"text": "Also, CMLA () reports better results than RNCRF but we do not compare with it.", "labels": [], "entities": [{"text": "CMLA", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.872641921043396}, {"text": "RNCRF", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8245721459388733}]}, {"text": "The reason is that CMLA introduces the gold standard opinion labels in the training data while such labels are not available for our experiments   The hyper-parameters are selected via ten-fold cross validation.", "labels": [], "entities": [{"text": "CMLA", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9222483038902283}]}, {"text": "The dimension of hidden representations are 100, 20, 40 for A-LSTM, O-LSTM and S-LSTM respectively.", "labels": [], "entities": []}, {"text": "The dropout rate for O-LSTM and S-LSTM is 0.4.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.967644453048706}, {"text": "O-LSTM", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8216939568519592}]}, {"text": "The size of the aspect (opinion) memory n m is 4.", "labels": [], "entities": []}, {"text": "The batch size is set to 32.", "labels": [], "entities": []}, {"text": "As for initialization of network parameters, we adopt the strategy that the initial weights are sampled from the uniform distribution.", "labels": [], "entities": []}, {"text": "We employ ADAM () as optimizer and the default settings of ADAM are used.", "labels": [], "entities": []}, {"text": "To better reveal the capability of the proposed MIN, we train 5 models with the same group of hyper-parameters and report the average F 1 score over the testing set.", "labels": [], "entities": [{"text": "MIN", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9780651926994324}, {"text": "F 1 score", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9835922916730245}]}, {"text": "With memory interactions and consideration of sentimental sentence, our MIN boosts the performance of vanilla bi-directional LSTM (+2.0% and +1.7% respectively).", "labels": [], "entities": []}, {"text": "It validates the effectiveness of the manually designed memory operations and the proposed memory interaction mechanism.", "labels": [], "entities": []}, {"text": "MIN also outperforms the state-of-the-art RNCRF on each dataset suggesting that memory interactions can bean alternative strategy instead of syntactic parsing.", "labels": [], "entities": []}, {"text": "To further study the impact of each element in MIN, we conduct ablation experiments.", "labels": [], "entities": [{"text": "MIN", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.7893815636634827}]}, {"text": "As shown in Table 3, removing bi-directionality decreases the extraction performances (-2.0% and -1.0%).", "labels": [], "entities": []}, {"text": "The soft sentimental constraint proves to be useful since MIN is 1.5% and 1.0% superior than the framework without S-LSTM on D 1 and D 2 respectively.", "labels": [], "entities": []}, {"text": "O-LSTM brings in the largest performance gains on D 2 compared with ablated framework (i.e., MIN without O-LSTM), verifying our postulation that aspect-opinion \"interaction\" is more effective than only considering aspect terms.", "labels": [], "entities": []}, {"text": "We also observe that the contribution of O-LSTM is less significant than that of bi-directionality on D 1 (+1.6% vs +2.0%).", "labels": [], "entities": []}, {"text": "This is reasonable since using opinion words as adjective modifiers placed after the aspects is common in English.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of datasets.", "labels": [], "entities": []}, {"text": " Table 3: Ablation experiment results.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9672203063964844}]}]}