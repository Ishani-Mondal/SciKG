{"title": [{"text": "Word Re-Embedding via Manifold Dimensionality Retention", "labels": [], "entities": [{"text": "Manifold Dimensionality Retention", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.5569819808006287}]}], "abstractContent": [{"text": "Word embeddings seek to recover a Eu-clidean metric space by mapping words into vectors, starting from words co-occurrences in a corpus.", "labels": [], "entities": []}, {"text": "Word embed-dings may underestimate the similarity between nearby words, and overestimate it between distant words in the Euclidean metric space.", "labels": [], "entities": []}, {"text": "In this paper, we re-embed pre-trained word embeddings with a stage of manifold learning which retains dimen-sionality.", "labels": [], "entities": []}, {"text": "We show that this approach is theoretically founded in the metric recovery paradigm, and empirically show that it can improve on state-of-the-art embed-dings in word similarity tasks 0.5 \u2212 5.0% points depending on the original space.", "labels": [], "entities": [{"text": "metric recovery paradigm", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.771194597085317}]}], "introductionContent": [{"text": "Concepts have been hypothesized in the cognitive psychometric literature as points in a Euclidean metric space, with empirical support from human judgement experiments.", "labels": [], "entities": []}, {"text": "Word embeddings, such as GloVe () and Word2Vec (, harvest observed features of the latent Euclidean space such as words co-occurrence counts in a corpus and turn words into dense vectors of a few hundred dimensions.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.9514437317848206}]}, {"text": "Word embeddings have proved useful in downstream NLP tasks such as Part of Speech Tagging, Named Entity Recognition (, and Machine Translation).", "labels": [], "entities": [{"text": "Part of Speech Tagging", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.8209745734930038}, {"text": "Named Entity Recognition", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.6100006699562073}, {"text": "Machine Translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.8301089107990265}]}, {"text": "However, the potential of word embeddings and further improvements remain a research question.", "labels": [], "entities": []}, {"text": "When comparing word pairs similarities obtained from word embeddings, to word pairs similarities obtained from human judgement, it is observed that word embeddings slightly underestimate the similarity between similar words, and overestimate the similarity between distant words.", "labels": [], "entities": []}, {"text": "For example, in the WS353 () word similarity ground truth: sim(\"shore\", \"woodland\") = 3.08 < sim(\"physics\", \"proton\") = 8.12 However, the use of GloVe 42B 300d embedding with cosine similarity (see Section 4) yields the opposite order: sim(\"shore\", \"woodland\") = 0.36 > sim(\"physics\", \"proton\") = 0.33 Re-embedding the space using a manifold learning stage can rectify this.", "labels": [], "entities": [{"text": "WS353", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.9520249366760254}]}, {"text": "Manifold learning works by estimating the distance between nearby words using direct similarity assignment in a local neighbourhood, while distance between faraway words is approximated by multiple neighbourhoods based on the manifold shape.", "labels": [], "entities": [{"text": "Manifold learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7341894805431366}]}, {"text": "This observation forms the basis for the rest of this paper.", "labels": [], "entities": []}, {"text": "For instance, using Locally Linear Embedding (LLE) ) on top of GloVe, as described in this paper, can recover the right pairs order yielding: sim(\"shore\", \"woodland\") = 0.08 < sim(\"physics\", \"proton\") = 0.25) put word embeddings under a paradigm which seeks to recover the underlying Euclidean metric semantic space.", "labels": [], "entities": []}, {"text": "In this paradigm, word embeddings land into a space where a Euclidean metric can be used.", "labels": [], "entities": []}, {"text": "They show that co-occurrence counts are the results of random walk sequences in the metric space, corresponding to sentences in a corpus.", "labels": [], "entities": []}, {"text": "Hashimoto et al. link this to manifold learning which also seeks to recover a Euclidean space (Human Judgement) Euclidean Metric Space e.g. Word Embedding Start: words co-occurrence e.g.),).", "labels": [], "entities": []}, {"text": "but starting from local neighbourhoods of objects, such as images or words.", "labels": [], "entities": []}, {"text": "Global distances are built by adding up small local neighbourhoods.", "labels": [], "entities": []}, {"text": "The authors show that word embedding algorithms can be used to solve manifold learning by generating random walks, aka sentences, on the manifold neighbourhood graph, and then embedding them.", "labels": [], "entities": [{"text": "manifold learning", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7387553155422211}]}, {"text": "In this work we follow a methodology which adheres to this paradigm and adopt a different angle, as per.", "labels": [], "entities": []}, {"text": "We start from an off-the-shelf word embedding, then we take a sample of it and feed it into manifold learning which leverages local word neighbourhoods formed in the original embedding space, learns the manifold, and embeds it into anew Euclidean space.", "labels": [], "entities": []}, {"text": "The resulting re-embedding space is a recovery of a Euclidean metric space that is empirically better than the original word embedding when tested on word similarity tasks.", "labels": [], "entities": [{"text": "word similarity tasks", "start_pos": 150, "end_pos": 171, "type": "TASK", "confidence": 0.7401058971881866}]}], "datasetContent": [{"text": "The original word embeddings used are pre-trained GloVe models: Wikipedia 2014 + Gigaword 5 (6B tokens,, and Common Crawl (42B tokens, 1.9M vocab, 300d vectors) ().", "labels": [], "entities": [{"text": "Wikipedia 2014", "start_pos": 64, "end_pos": 78, "type": "DATASET", "confidence": 0.9149761199951172}]}, {"text": "The vectors are ordered by the frequency of their corresponding words, so the vector representing the word 'the' comes first in the space.", "labels": [], "entities": []}, {"text": "We use similarity tasks WS353) and RG65.", "labels": [], "entities": [{"text": "RG65", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.5092737674713135}]}, {"text": "We use the performance by the original word embeddings on the tasks.", "labels": [], "entities": []}, {"text": "For each original space, we normalize features using their minimum and maximum values to, and then normalize vectors to unit norms.", "labels": [], "entities": []}, {"text": "For each pair of words in the similarity task, we get the normalized vectors and measure the cosine similarity.", "labels": [], "entities": []}, {"text": "We finally compute the Spearman Rank Correlation with human judgements.", "labels": [], "entities": []}, {"text": "For a given original embedding, we normalize vectors to unit norms, then we conduct Manifold (Mfd) Re-Embedding using LLE as explained in Section 3.", "labels": [], "entities": []}, {"text": "For each similarity task, we transform the vectors of test words into the re-embedding space before computing the cosine similarity, and the final Spearman score.", "labels": [], "entities": []}, {"text": "We vary relevant parameters and see what effect they have on the performance, so we can understand the effectiveness of the approach and its limits.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average performance on similarity tasks.  (Window start \u2208 [5000, 15000], Number of LLE  local neighbours =1000, Window length = 1001,  Manifold dimensionality = Space dimensionality.)", "labels": [], "entities": []}]}