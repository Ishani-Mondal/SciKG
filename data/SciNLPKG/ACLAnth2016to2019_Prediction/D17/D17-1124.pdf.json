{"title": [], "abstractContent": [{"text": "Idioms are peculiar linguistic constructions that impose great challenges for representing the semantics of language, especially in current prevailing end-to-end neural models, which assume that the semantics of a phrase or sentence can be literally composed from its constitutive words.", "labels": [], "entities": []}, {"text": "In this paper, we propose an idiom-aware distributed semantic model to build representation of sentences on the basis of understanding their contained idioms.", "labels": [], "entities": []}, {"text": "Our models are grounded in the literal-first psycholinguistic hypothesis, which can adaptively learn semantic composi-tionality of a phrase literally or idiomatically.", "labels": [], "entities": []}, {"text": "To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms.", "labels": [], "entities": [{"text": "idiom-enriched sentiment classification", "start_pos": 52, "end_pos": 91, "type": "TASK", "confidence": 0.652862936258316}]}, {"text": "The qualitative and quantitative experimental analyses demonstrate the efficacy of our models.", "labels": [], "entities": []}, {"text": "The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (, semantic matching (, and machine translation ( ).", "labels": [], "entities": [{"text": "text classification", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8174008429050446}, {"text": "semantic matching", "start_pos": 143, "end_pos": 160, "type": "TASK", "confidence": 0.7460895478725433}, {"text": "machine translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7900511026382446}]}, {"text": "The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words.", "labels": [], "entities": []}, {"text": "Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence.", "labels": [], "entities": []}, {"text": "The form of compositional function involves many kinds of neural networks, such as recurrent neural networks), convolutional neural networks, and recursive neural networks.", "labels": [], "entities": []}, {"text": "However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words.", "labels": [], "entities": []}, {"text": "For example, \"pulling my leg\" is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words.", "labels": [], "entities": []}, {"text": "Due to its importance, some previous work focuses on automatic identification of idioms (;.", "labels": [], "entities": [{"text": "automatic identification of idioms", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.7422718480229378}]}, {"text": "However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences.", "labels": [], "entities": []}, {"text": "Motivated by the literal-first psycholinguistic hypothesis proposed by, in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network) to learn the compositional semantics over a constituent tree.", "labels": [], "entities": []}, {"text": "More concretely, we introduce a neural idiom detector for each phrase in a sentence to adaptively determine their compositionality: literal or idiomatic manner.", "labels": [], "entities": []}, {"text": "For the literal phrase, we compute its semantics from its constituents while for the idiomatic phrase, we design two different ways to learn representations of idioms grounded in two different linguistic views of idioms.", "labels": [], "entities": []}, {"text": "To evaluate our models towards the ability to understand sentences with idioms, we conduct our experiments on sentiment classification task due to the following reasons: 1) Idioms typically imply an affective stance toward something and they are common in reviews and comments).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 110, "end_pos": 134, "type": "TASK", "confidence": 0.9034207761287689}]}, {"text": "2) The error analysis of sentiment classification results reveals that a large number of errors are caused by idioms (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9395833909511566}]}, {"text": "The contributions of this work are summarized as follows: \u2022 We grow the capacity of recursive neural network, enabling it to model idiomatic phrases and handle ubiquitous phenomenon of idiomatic variations when learning a sentential representation.", "labels": [], "entities": []}, {"text": "\u2022 We integrate idioms understanding into a real-world NLP task instead of evaluating idiom detection as a standalone task.", "labels": [], "entities": [{"text": "idiom detection", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7338088601827621}]}, {"text": "\u2022 We construct anew real-world dataset covering abundant idioms with original and variational forms.", "labels": [], "entities": []}, {"text": "The elaborate qualitative and quantitative experimental analyses show the effectiveness of our models.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our models, we need a task that heavily depends on the understanding of idioms.", "labels": [], "entities": []}, {"text": "In this paper, we choose sentiment classification task due to following reasons: 1) Idioms typically imply an affective stance toward something and they are common in reviews and comments).", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.9341527223587036}]}, {"text": "2) The error analysis of sentiment classification results reveals that a large number of errors are caused by idioms (.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.9395833909511566}]}, {"text": "In this section, we will first give a brief description of the most commonly used datasets for sentiment classification so as to motivate the need fora new benchmark dataset.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.9559329748153687}]}, {"text": "We list four kinds of datasets which are most commonly used for sentiment classification in NLP community.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.9556625187397003}]}, {"text": "Additionally, we also evaluate our models on these datasets to make a comparison with many recent proposed models.", "labels": [], "entities": []}, {"text": "Each dataset is briefly described as follows.", "labels": [], "entities": []}, {"text": "\u2022 SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective.", "labels": [], "entities": [{"text": "SUBJ Subjectivity data set", "start_pos": 2, "end_pos": 28, "type": "DATASET", "confidence": 0.7398976385593414}]}, {"text": "( The detailed statistics about these four datasets are listed in.", "labels": [], "entities": []}, {"text": "Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task.", "labels": [], "entities": [{"text": "idiom detection", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7214725762605667}, {"text": "idiom understanding", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.7222369313240051}, {"text": "sentiment classification", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.9020616710186005}]}, {"text": "However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon.", "labels": [], "entities": []}, {"text": "To better evaluate our models on idiom understanding task, we proposed an idiom-enriched sentiment classification dataset, in which each sentence contains at least one idiom.", "labels": [], "entities": [{"text": "idiom understanding task", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.7876084546248118}, {"text": "idiom-enriched sentiment classification", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.6243647436300913}]}, {"text": "Additionally, considering most idioms have certain flexibility in morphology, lexicon and syntax, we enrich our dataset by introducing different types of idiom variations so that we can further evaluate the ability that the model handle different idiomatic variations.", "labels": [], "entities": []}, {"text": "As shown in, we sum up two types of phenomena towards idiom variations and, for each variation, we obtain several corresponding sentences from a large corpora.", "labels": [], "entities": []}, {"text": "We first evaluate our proposed models on four popular sentiment datasets, so that we can make a comparison with varieties of competitors.", "labels": [], "entities": []}, {"text": "And then, we use the newly-introduced dataset to make more detailed experiment analyses.", "labels": [], "entities": []}, {"text": "Loss Function Given a sentence and its label, the output of neural network is the probabilities of the different classes.", "labels": [], "entities": []}, {"text": "The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions.", "labels": [], "entities": []}, {"text": "To minimize the objective, we use stochastic gradient descent with the diagonal variant of).", "labels": [], "entities": []}, {"text": "The experimental results are shown in.", "labels": [], "entities": []}, {"text": "We can see Cont-TLSTM outperforms TLSTM on all four tasks, showing the importance of contextsensitive composition.", "labels": [], "entities": []}, {"text": "Besides, both iTLSTM-Lo and iTLSTM-Mo achieve better results than TL-STM and Cont-LSTM, which indicates the effectiveness of our introduced modules (detector and idiomatic interpreter).", "labels": [], "entities": []}, {"text": "Additionally, compared with iTLSTM-Lo, iTLSTM-Mo behaves better, suggesting its char-based idiomatic interpreter is more powerful.", "labels": [], "entities": []}, {"text": "Although four mainstream datasets are not rich in idioms, we could also observe substantial improvement gained from our models.", "labels": [], "entities": []}, {"text": "We attribute this success to the power of introduced detector in identifying other non-compositional collocations besides idioms.", "labels": [], "entities": []}, {"text": "We will discuss about this later.", "labels": [], "entities": []}, {"text": "Since iSent is a newly-introduced dataset, there is no existing baselines.", "labels": [], "entities": []}, {"text": "Nevertheless, we provide several strong baselines implemented by ourselves as shown in, and we can observe that: \u2022 Differing from the improvement achieved on mainstream datasets, proposed models have shown their advantages on idiom-enriched sentences.", "labels": [], "entities": []}, {"text": "They obtain more significant improvements.", "labels": [], "entities": []}, {"text": "\u2022 Additionally, iTLSTM-Lo performs worse than iTLSTM-Mo while still surpasses baseline models, which also indicates the variation-sensitive model (iTLSTM-Mo) of idioms could further improve the performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of the four mainstream datasets  for sentiment classification. L avg denotes the av- erage length of documents; |V| denotes the size of  vocabulary.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.9456013143062592}, {"text": "av- erage length", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.6943305134773254}]}, {"text": " Table 3: Idiom variations at morphological and  lexical level. Add. and Sub. refer to lexical addi- tion and substitution respectively.", "labels": [], "entities": []}, {"text": " Table 4: Key statistics for the idioms and sen- tences in iSent dataset. O(Original) denotes the  idioms in dev/test sets are in original forms and  have appeared in training set. M(Morphology) and  L(Lexical) represent the morphology and lexical  idiom variations respectively and they are unseen  in training set.", "labels": [], "entities": [{"text": "iSent dataset", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9545171558856964}, {"text": "O", "start_pos": 74, "end_pos": 75, "type": "METRIC", "confidence": 0.968609631061554}]}, {"text": " Table 5: Accuracies of our models on four datasets  against state-of-the-art neural models.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9707214832305908}]}, {"text": " Table 6: Accuracies of our models on iSent dataset  against typical baselines. BiLSTM represents bidi- rectional LSTM.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9806097149848938}, {"text": "iSent dataset", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9471810758113861}, {"text": "BiLSTM", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.7228503227233887}]}]}