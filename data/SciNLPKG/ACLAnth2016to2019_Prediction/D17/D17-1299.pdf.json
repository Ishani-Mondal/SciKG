{"title": [{"text": "A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.699635922908783}]}], "abstractContent": [{"text": "Stylistic variations of language, such as formality, carry speakers' intention beyond literal meaning and should be conveyed adequately in translation.", "labels": [], "entities": []}, {"text": "We propose to use lexical formality models to control the formality level of machine translation output.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.7573390007019043}]}, {"text": "We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically analyzing and generating natural language requires capturing not only what is said, but also how to say it.", "labels": [], "entities": []}, {"text": "Consider the sentences \"anybody hurt?\" and \"is someone wounded?\".", "labels": [], "entities": []}, {"text": "The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used.", "labels": [], "entities": []}, {"text": "Such differences in formality have been identified as an important dimension of style or tone In this paper, we build on prior computational work that has focused on analyzing formality of texts () with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 332, "end_pos": 351, "type": "TASK", "confidence": 0.7771053612232208}]}, {"text": "Human translators translate a document fora specific audience, and often ask what is the expected tone of the content when taking anew translation job.", "labels": [], "entities": [{"text": "translators translate a document", "start_pos": 6, "end_pos": 38, "type": "TASK", "confidence": 0.8026578575372696}]}, {"text": "We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7062230110168457}]}, {"text": "While ultimately we would like systems to preserve the formality of the source, this is a challenging task that requires not only automatically inferring the formality of the source, but also understanding how formality differs across languages and cultures.", "labels": [], "entities": []}, {"text": "As a first step, we therefore limit our study to the scenario where the expected output formality is given to the MT system as an additional input.", "labels": [], "entities": [{"text": "MT", "start_pos": 114, "end_pos": 116, "type": "TASK", "confidence": 0.9272404313087463}]}, {"text": "We first select a formality model providing the most accurate scores on intrinsic formality datasets.", "labels": [], "entities": []}, {"text": "We compare existing lexical formality models and novel variants based on inducing formality dimensions or subspaces in vector space models.", "labels": [], "entities": []}, {"text": "We then turn to machine translation and show that a lexical formality model can have a positive impact when used to control the formality of machine translation output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7887134552001953}]}, {"text": "When the expected formality matches the reference, we obtain improvement of translation quality evaluated by automatic metrics (BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.701602041721344}]}, {"text": "A human assessment also verified the effectiveness of our proposed system in generating translations at diverse levels of formality.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before evaluating our FSMT framework, we evaluate the formality models at the sentence level. and collected 5-way human scores for 11,263 sentences in the genres of blog, email, answers and news.", "labels": [], "entities": []}, {"text": "Following, we averaged human scores for each sentence as the gold standard.", "labels": [], "entities": []}, {"text": "As in prior work, the score quality was evaluated by the Spearman correlation.", "labels": [], "entities": []}, {"text": "A large mixed-topic corpus is required to train vector space models.", "labels": [], "entities": []}, {"text": "As suggested by, we used the ICWSM 2009 Spinn3r dataset (English tier-1) which consists of about 1.6 billion words ().", "labels": [], "entities": [{"text": "ICWSM 2009 Spinn3r dataset", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.9411525130271912}]}, {"text": "We also compared the term-document association model Latent Semantic Analysis (LSA)) and the term-term association model word2vec (W2V) (.", "labels": [], "entities": []}, {"text": "We used the same 105 formal seeds and 138 informal seeds as.", "labels": [], "entities": []}, {"text": "Followed, to achieve best performance, we used a small dimensionality (10) for training LSA and word2vec.", "labels": [], "entities": []}, {"text": "In practice, we normalized the LSA word vectors to make them have unit length for SVM and PCA, but did not applied it to word2vec.", "labels": [], "entities": []}, {"text": "This suggests that the magnitude of LSA word vectors is harmful for formality modeling.", "labels": [], "entities": []}, {"text": "We also compared formality models based on word representations to a baseline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (.", "labels": [], "entities": []}, {"text": "This method requires language examples of diverse formality.", "labels": [], "entities": []}, {"text": "Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (), Switchboard (Godfrey et al., 1992), SBCSAE (, CallHome 1 , CallFriend 2 , BOLT SMS/Chat () and NPS Chatroom.", "labels": [], "entities": [{"text": "NPS Chatroom", "start_pos": 209, "end_pos": 221, "type": "DATASET", "confidence": 0.9587641060352325}]}, {"text": "As the formal counterpart, we extracted comparable size of text from Europarl (.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9870731830596924}]}, {"text": "This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments).", "labels": [], "entities": []}, {"text": "shows that all models based on the vector space achieve similar performance in terms of Spearman's \u03c1 (except SVM-W2V which yields lower performance).", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.699679970741272}]}, {"text": "The baseline method based on unigram models was outperformed by 0.1+ point.", "labels": [], "entities": []}, {"text": "So we select DENSIFIER-LSA as a representative for our FSMT system.", "labels": [], "entities": [{"text": "DENSIFIER-LSA", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.6053121089935303}, {"text": "FSMT system", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.8361306190490723}]}, {"text": "Set-up We evaluate this approach on a French to English translation task.", "labels": [], "entities": [{"text": "French to English translation task", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.6363381028175354}]}, {"text": "Two parallel FrenchEnglish corpora are used: (1), which is extracted from the United Nations website, and can be considered to be formal text; (2) OpenSubtitles2016 (Lison and Tiedemann, 2016), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations.", "labels": [], "entities": [{"text": "United Nations website", "start_pos": 78, "end_pos": 100, "type": "DATASET", "confidence": 0.875715414683024}]}, {"text": "Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments).", "labels": [], "entities": []}, {"text": "Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles.", "labels": [], "entities": []}, {"text": "Moses () is used to build our phrase-based MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.8432254791259766}]}, {"text": "We followed the standard training pipeline with default parameters.", "labels": [], "entities": []}, {"text": "Word alignments were generated using fast align, and symmetrized using the grow-diag-final-and heuristic.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6409999877214432}]}, {"text": "We used 4-gram language models, trained using.", "labels": [], "entities": []}, {"text": "Model weights were tuned using batch MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9880530834197998}]}, {"text": "We used constant size n=1000 for n-best lists in all experiments.", "labels": [], "entities": []}, {"text": "The re-ranking is a log-linear model trained using batch MIRA.", "labels": [], "entities": []}, {"text": "We report results averaged over 5 random tuning re-starts to compensate for tuning noise).", "labels": [], "entities": []}, {"text": "FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different  We first report standard automatic evaluation results using the BLEU score to compare FSMT output given different desired formality level on each bins (See).", "labels": [], "entities": [{"text": "FSMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7509661912918091}, {"text": "BLEU score", "start_pos": 260, "end_pos": 270, "type": "METRIC", "confidence": 0.9676243960857391}]}, {"text": "The best BLEU scores for each formality level are obtained when the level of formality given as input to the MT system matches the nature of the text being translated, as can be seen in the scores along the diagonal in.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9979141354560852}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.9189279079437256}]}, {"text": "Comparing with the baseline system, which produces the top translation from each n-best list, translation quality improves by +0.5 BLEU on informal text, +0.3 BLEU on neutral text, and remains constant on formal text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9984446167945862}, {"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9983775615692139}]}, {"text": "The impact increases with the distance to formal language increases.", "labels": [], "entities": []}, {"text": "This can be explained by the fact that more formal sentences tend to be longer, and the impact of alternate lexical choice fora small number of words per sentence is smaller in longer sentences.", "labels": [], "entities": []}, {"text": "In addition, the formal sentences are mostly drawn from UN data which is sufficiently different from the other genres in the heterogeneous training corpus that the informal examples do not affect baseline performance on formal data.", "labels": [], "entities": [{"text": "UN data", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8104223012924194}]}], "tableCaptions": [{"text": " Table 1: Sentence-level formality quantifying  evaluation (Spearman's \u03c1) among different mod- els with different vector spaces.", "labels": [], "entities": [{"text": "Sentence-level formality quantifying  evaluation", "start_pos": 10, "end_pos": 58, "type": "TASK", "confidence": 0.8242819607257843}]}, {"text": " Table 2. Comparing with  the baseline system, which produces the top trans- lation from each n-best list, translation quality  improves by +0.5 BLEU on informal text, +0.3  BLEU on neutral text, and remains constant on  formal text. The impact increases with the dis- tance to formal language increases. This can be  explained by the fact that more formal sentences  tend to be longer, and the impact of alternate lex- ical choice for a small number of words per sen- tence is smaller in longer sentences. In addition,  the formal sentences are mostly drawn from UN  data which is sufficiently different from the other  genres in the heterogeneous training corpus that  the informal examples do not affect baseline per-", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9983847141265869}, {"text": "BLEU", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.9973794221878052}, {"text": "UN  data", "start_pos": 564, "end_pos": 572, "type": "DATASET", "confidence": 0.8480045795440674}]}]}