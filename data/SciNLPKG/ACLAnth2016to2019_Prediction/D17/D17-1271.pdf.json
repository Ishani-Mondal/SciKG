{"title": [{"text": "Classification of telicity using cross-linguistic annotation projection", "labels": [], "entities": [{"text": "Classification of telicity", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8211733897527059}]}], "abstractContent": [{"text": "This paper addresses the automatic recognition of telicity, an aspectual notion.", "labels": [], "entities": [{"text": "automatic recognition of telicity", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6790341138839722}]}, {"text": "A telic event includes a natural endpoint (she walked home), while an atelic event does not (she walked around).", "labels": [], "entities": []}, {"text": "Recognizing this difference is a prerequisite for temporal natural language understanding.", "labels": [], "entities": [{"text": "temporal natural language understanding", "start_pos": 50, "end_pos": 89, "type": "TASK", "confidence": 0.6496468558907509}]}, {"text": "In En-glish, this classification task is difficult, as telicity is a covert linguistic category.", "labels": [], "entities": []}, {"text": "In contrast, in Slavic languages, aspect is part of a verb's meaning and even available in machine-readable dictionaries.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "We successfully leverage additional silver standard training data in the form of projected annotations from parallel English-Czech data as well as context information, improving automatic telicity classification for English significantly compared to previous work.", "labels": [], "entities": [{"text": "telicity classification", "start_pos": 188, "end_pos": 211, "type": "TASK", "confidence": 0.7070263624191284}]}, {"text": "We also create anew data set of English texts manually annotated with telicity.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the computational modeling of telicity, a linguistic feature which represents whether the event type evoked by a sentence's verb constellation (i.e., the verb and its arguments and modifiers) has a natural endpoint or not, see (1a) and (1b).", "labels": [], "entities": []}, {"text": "Automatic recognition of telicity is a necessary step for natural language understanding tasks that require reasoning about time, e.g., natural language generation, summarization, question answering, information extraction or machine translation ().", "labels": [], "entities": [{"text": "Automatic recognition of telicity", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7478247657418251}, {"text": "natural language understanding tasks", "start_pos": 58, "end_pos": 94, "type": "TASK", "confidence": 0.6945849359035492}, {"text": "natural language generation", "start_pos": 136, "end_pos": 163, "type": "TASK", "confidence": 0.6340659956137339}, {"text": "summarization", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.9757134914398193}, {"text": "question answering", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.838901162147522}, {"text": "information extraction", "start_pos": 200, "end_pos": 222, "type": "TASK", "confidence": 0.8222127854824066}, {"text": "machine translation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.740416631102562}]}, {"text": "For example, there is an entailment relation between English Progressive and Perfect constructions (as shown in), but only for atelic verb constellations.", "labels": [], "entities": []}, {"text": "|= He has swum across the lake.", "labels": [], "entities": []}, {"text": "We model telicity at the word-sense level, corresponding to the fundamental aspectual class of, i.e., we take into account the verb and its arguments and modifiers, but no additional aspectual markers (such as the Progressive).", "labels": [], "entities": []}, {"text": "In (2) we classify whether the event types \"swim in the lake\" and \"swim across the lake\" have natural endpoints.", "labels": [], "entities": []}, {"text": "This is defined on a linguistic level rather than by world knowledge requiring inference.", "labels": [], "entities": []}, {"text": "\"Swimming in the lake\" has no natural endpoint, as also shown by the linguistic test presented in.", "labels": [], "entities": []}, {"text": "In contrast, \"swimming across the lake\" will necessarily be finished once the other side is reached.", "labels": [], "entities": []}, {"text": "In English, the aspectual notion of telicity is a covert category, i.e., a semantic distinction that is not expressed overtly by lexical or morphological means.", "labels": [], "entities": []}, {"text": "As illustrated by (2) and (3), the same verb type (lemma) can introduce telic and atelic events to the discourse depending on the context in which it occurs.", "labels": [], "entities": []}, {"text": "In Slavic languages, aspect is a component of verb meaning.", "labels": [], "entities": []}, {"text": "Most verb types are either perfective or imperfective (and are marked as such in dictionaries).", "labels": [], "entities": []}, {"text": "For example, the two occurrences of \"drink\" in (3) are translated into Czech using the imperfective verb \"pil\" and the perfective verb \"vypil,\" respectively He was drinking (some) coffee.", "labels": [], "entities": []}, {"text": "He drank up (all) the coffee.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: (1) using the English-Czech part of InterCorp) and a valency lexicon for Czech verbs, we create a large silver standard with automatically derived annotations and validate our approach by comparing the labels given by humans versus the projected labels; (2) we provide a freely available data set of English texts taken from MASC () manually annotated for telicity; (3) we show that using contextual features and the silver standard as additional training data improves computational modeling of telicity for English in terms of F 1 compared to previous work.", "labels": [], "entities": [{"text": "InterCorp", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.8719883561134338}, {"text": "MASC", "start_pos": 359, "end_pos": 363, "type": "DATASET", "confidence": 0.8955508470535278}, {"text": "F 1", "start_pos": 563, "end_pos": 566, "type": "METRIC", "confidence": 0.9857090413570404}]}], "datasetContent": [{"text": "We evaluate our models via 10-fold cross validation (CV) on the MASC data set.", "labels": [], "entities": [{"text": "MASC data set", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.970436155796051}]}, {"text": "We split the data into folds by documents in order to make sure that no training data from the same document is available for each instance in order to avoid an unfair bias.", "labels": [], "entities": []}, {"text": "We report results in terms of accuracy, F 1 per class and macroaverage F 1 (the harmonic mean of macro-average precision and recall).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9995550513267517}, {"text": "F 1", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9797040522098541}, {"text": "macroaverage F 1", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.8919499516487122}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.8975558280944824}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9973326921463013}]}, {"text": "We test significance between differences in F 1 (for each class) using approximate randomization) with p < 0.1 and significance between differences inaccuracy using McNemar's test with p < 0.01.", "labels": [], "entities": [{"text": "F 1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8924420773983002}]}, {"text": "shows our results: significantly different scores are marked with the same symbol where relevant (per column).", "labels": [], "entities": []}, {"text": "A simple baseline of labeling each instance with the overall majority class (telic) has a very high accuracy, but the output of this baseline is uninformative and results in a low F 1 . Rows titled \"verb type\" use the verb's lemma as their single feature and thus correspond to the informed baseline of using the training set majority class for each verb type.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9991933703422546}, {"text": "F 1", "start_pos": 180, "end_pos": 183, "type": "METRIC", "confidence": 0.970634937286377}]}, {"text": "Rows labeled \"+IC\" indicate that the full set of instances with projected labels extracted from InterCorp has been added as additional training data in each fold; in rows titled \"+ICs,\" the telic instances in InterCorp have been upsampled to match the 80:20 distribution in MASC.", "labels": [], "entities": [{"text": "InterCorp", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.8985514044761658}, {"text": "MASC", "start_pos": 274, "end_pos": 278, "type": "DATASET", "confidence": 0.8790943026542664}]}, {"text": "Our model using the full set of features significantly outperforms the verb type baseline as well as SMK2000 (see \u2020 \u2021 * ).", "labels": [], "entities": [{"text": "SMK2000", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8387762308120728}]}, {"text": "Using the additional training data from InterCorp results in a large improvement in the case of the difficult (because infrequent) atelic class (see ), leading to the best overall results in terms of F 1 . The best results regarding accuracy and F 1 are reached using the sampled version of the silver standard; the differences compared to the respective best scores in each column (in bold) are not significant.", "labels": [], "entities": [{"text": "InterCorp", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.949325442314148}, {"text": "F 1", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9936136603355408}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.9994022846221924}, {"text": "F 1", "start_pos": 246, "end_pos": 249, "type": "METRIC", "confidence": 0.9927850067615509}]}, {"text": "Ablation experiments on the MASC data show that features describing the clause's main verb are most important: when ablating part-of-speech tag and tense and aspect (Progressive or Perfect), performance deteriorates by 1.8% inaccuracy and 5% F 1 , hinting at a correlation between telicity and choice of tense-aspect form.", "labels": [], "entities": [{"text": "MASC data", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9086275100708008}, {"text": "F 1", "start_pos": 242, "end_pos": 245, "type": "METRIC", "confidence": 0.9929105937480927}]}, {"text": "Whether this is due to an actual correlation of how telic and atelic verbs are used in context or merely due to annotation errors remains to be investigated in future work.", "labels": [], "entities": []}, {"text": "In sum, our experiments show that using annotations projected onto English text from parallel Czech text as cheap additional training data is a step forward to creating better models for the task of classifying telicity of verb occurrences.", "labels": [], "entities": [{"text": "classifying telicity of verb occurrences", "start_pos": 199, "end_pos": 239, "type": "TASK", "confidence": 0.8279925227165222}]}], "tableCaptions": [{"text": " Table 2: Results for telicity classification on  MASC data (1863 instances), 10-fold CV.", "labels": [], "entities": [{"text": "telicity classification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.9264927208423615}, {"text": "MASC data", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9043534696102142}]}]}