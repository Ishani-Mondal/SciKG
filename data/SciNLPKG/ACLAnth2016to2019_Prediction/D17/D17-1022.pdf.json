{"title": [{"text": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "labels": [], "entities": [{"text": "Hypernymy Detection", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7459065914154053}, {"text": "Directionality", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.6146453619003296}]}], "abstractContent": [{"text": "We present a novel neural model HyperVec to learn hierarchical em-beddings for hypernymy detection and directionality.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7466954290866852}]}, {"text": "While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym-hyponym distributional hierarchy.", "labels": [], "entities": [{"text": "HyperVec", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.7912241220474243}]}, {"text": "Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages.", "labels": [], "entities": []}, {"text": "Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7391999065876007}, {"text": "predicting graded lexical entailment", "start_pos": 176, "end_pos": 212, "type": "TASK", "confidence": 0.7665292471647263}]}], "introductionContent": [{"text": "Hypernymy represents a major semantic relation and a key organization principle of semantic memory.", "labels": [], "entities": []}, {"text": "It is an asymmetric relation between two terms, a hypernym (superordinate) and a hyponym (subordiate), as in animal-bird and flower-rose, where the hyponym necessarily implies the hypernym, but not vice versa.", "labels": [], "entities": []}, {"text": "From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation), recognizing textual entailment (, and text generation), among many others.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6965115368366241}, {"text": "taxonomy creation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8590710461139679}, {"text": "recognizing textual entailment", "start_pos": 118, "end_pos": 148, "type": "TASK", "confidence": 0.7587387760480245}, {"text": "text generation", "start_pos": 156, "end_pos": 171, "type": "TASK", "confidence": 0.7505830526351929}]}, {"text": "Two families of approaches to identify and discriminate hypernyms are predominent in NLP, both of them relying on word vector representations.", "labels": [], "entities": []}, {"text": "Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods.", "labels": [], "entities": []}, {"text": "Unsupervised measures exploit the distributional inclusion hypothesis, or the distributional informativeness hypothesis ().", "labels": [], "entities": []}, {"text": "These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs.", "labels": [], "entities": []}, {"text": "Typically, Average Precision (AP)) is applied to rank and distinguish between the predicted relations.", "labels": [], "entities": [{"text": "Average Precision (AP))", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.9548463225364685}]}, {"text": "Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors).", "labels": [], "entities": []}, {"text": "The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy.", "labels": [], "entities": [{"text": "Logistic Regression (LR)", "start_pos": 72, "end_pos": 96, "type": "METRIC", "confidence": 0.7617088437080384}]}, {"text": "Across approaches, demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations.", "labels": [], "entities": []}, {"text": "Furthermore, showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words.", "labels": [], "entities": []}, {"text": "Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy.", "labels": [], "entities": []}, {"text": "proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs.", "labels": [], "entities": [{"text": "hypernymy identification", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8832237124443054}]}, {"text": "Recently, proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs. hyponyms, but also their contextual information.", "labels": [], "entities": []}, {"text": "The performance of this family of models is typically evaluated by using an SVM to discriminate hypernymy from other relations.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel neural model HyperVec to learn hierarchical embeddings that (i) discriminate hypernymy from other relations (detection task), and (ii) distinguish between the hypernym and the hyponym in a given hypernymy relation pair (directionality task).", "labels": [], "entities": []}, {"text": "Our model learns to strengthen the distributional similarity of hypernym pairs in comparison to other relation pairs, by moving hyponym and hypernym vectors close to each other.", "labels": [], "entities": []}, {"text": "In addition, we generate a distributional hierarchy between hyponyms and hypernyms.", "labels": [], "entities": []}, {"text": "Relying on these two new aspects of hypernymy distributions, the similarity of hypernym pairs receives higher scores than the similarity of other relation pairs; and the distributional hierarchy of hyponyms and hypernyms indicates the directionality of hypernymy.", "labels": [], "entities": []}, {"text": "Our model is inspired by the distributional inclusion hypothesis, that prominent context words of hyponyms are expected to appear in a subset of the hypernym contexts.", "labels": [], "entities": []}, {"text": "We assume that each context word which appears with both a hyponym and its hypernym can be used as an indicator to determine which of the two words is semantically more general: Common context word vectors which represent distinctive characteristics of a hyponym are expected to be closer to the hyponym vector than to its hypernym vector.", "labels": [], "entities": []}, {"text": "For example, the context word flap is more characteristic fora bird than for its hypernym animal; hence, the vector of flap should be closer to the vector of bird than to the vector of animal.", "labels": [], "entities": [{"text": "context word flap", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6713362733523051}]}, {"text": "We evaluate our HyperVec model on both unsupervised and supervised hypernymy detection and directionality tasks.", "labels": [], "entities": [{"text": "hypernymy detection and directionality tasks", "start_pos": 67, "end_pos": 111, "type": "TASK", "confidence": 0.8118828654289245}]}, {"text": "In addition, we apply the model to the task of graded lexical entailment, and we assess the capability of HyperVec on generalizing hypernymy by mapping to German and Italian.", "labels": [], "entities": [{"text": "graded lexical entailment", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6056730051835378}]}, {"text": "Results on benchmark datasets of hypernymy show that the hierarchical embeddings outperform state-of-the-art measures and previous embedding models.", "labels": [], "entities": []}, {"text": "Furthermore, the implementation of our models is made publicly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first describe the experimental settings in our experiments (Section 4.1).", "labels": [], "entities": []}, {"text": "We then evaluate the performance of HyperVec on three different tasks: i) unsupervised hypernymy detection and directionality (Section 4.2), where we assess HyperVec on ranking and classifying hypernymy; ii) supervised hypernymy detection (Section 4.3), where we apply supervised classification to detect hypernymy; iii) graded lexical entailment (Section 4.4), where we predict the strength of hypernymy pairs.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7152417451143265}, {"text": "supervised hypernymy detection", "start_pos": 208, "end_pos": 238, "type": "TASK", "confidence": 0.687318742275238}]}, {"text": "We use the ENCOW14A corpus) with approx. 14.5 billion tokens for training the hierarchical embeddings and the default SGNS model.", "labels": [], "entities": [{"text": "ENCOW14A corpus", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9341592490673065}]}, {"text": "We train our model with 100 dimensions, a window size of 5, 15 negative samples, and 0.025 as the learning rate.", "labels": [], "entities": []}, {"text": "The threshold \u03b8 is set to 0.05.", "labels": [], "entities": []}, {"text": "The hypernymy resource for nouns comprises 105, 020 hyponyms, 24, 925 hypernyms, and 1, 878, 484 hyponym-hypernym pairs.", "labels": [], "entities": []}, {"text": "The hypernymy resource for verbs consists of 11, 328 hyponyms, 4, 848 hypernyms, and 130, 350 hyponym-hypernym pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the semantic relations and the  number of instances in each dataset.", "labels": [], "entities": []}, {"text": " Table 2: AP results of HyperScore in comparison  to state-of-the-art measures.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9879831075668335}, {"text": "HyperScore", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.6812880635261536}]}, {"text": " Table 4: Classification results for BLESS and EN- TAILMENT in terms of accuracy.", "labels": [], "entities": [{"text": "BLESS", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9961327314376831}, {"text": "EN- TAILMENT", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.6928479472796122}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9994792342185974}]}, {"text": " Table 6: AP results across languages, comparing  SGNS and the projected representations.", "labels": [], "entities": [{"text": "AP", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.5102538466453552}]}]}