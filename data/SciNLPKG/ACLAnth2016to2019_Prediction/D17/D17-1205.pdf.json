{"title": [{"text": "CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles", "labels": [], "entities": []}], "abstractContent": [{"text": "Crowdsourcing has proven to bean effective method for generating labeled data fora range of NLP tasks.", "labels": [], "entities": []}, {"text": "However, multiple recent attempts of using crowdsourcing to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.770040770371755}, {"text": "SRL", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9623029828071594}]}, {"text": "In this paper, we postulate that while producing SRL annotation does require expert involvement in general , a large subset of SRL labeling tasks is in fact appropriate for the crowd.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.9231954514980316}, {"text": "SRL labeling tasks", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.9406237999598185}]}, {"text": "We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties.", "labels": [], "entities": []}, {"text": "Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 167, "end_pos": 181, "type": "TASK", "confidence": 0.9301756620407104}]}], "introductionContent": [{"text": "Semantic role labeling (SRL) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles ().", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8381100197633108}]}, {"text": "It has been found useful fora wide variety of NLP tasks such as question-answering, information extraction) and machine translation (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7980133295059204}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8440039753913879}]}, {"text": "A major bottleneck impeding the wide adoption of SRL is the need for large amounts of labeled training data to * The work was done while the author was at IBM Research -Almaden.", "labels": [], "entities": [{"text": "SRL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9860305786132812}, {"text": "IBM Research -Almaden", "start_pos": 155, "end_pos": 176, "type": "DATASET", "confidence": 0.7946921736001968}]}, {"text": "Such data requires trained experts and is highly costly to produce ().", "labels": [], "entities": []}, {"text": "Crowdsourcing SRL Crowdsourcing has shown its effectiveness to generate labeled data fora range of NLP tasks (.", "labels": [], "entities": [{"text": "Crowdsourcing SRL Crowdsourcing", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5798191726207733}]}, {"text": "A core advantage of crowdsourcing is that it allows the annotation workload to be scaled out among large numbers of inexpensive crowd workers.", "labels": [], "entities": []}, {"text": "Not surprisingly, a number of recent SRL works have also attempted to leverage crowdsourcing to generate labeled training data for SRL and investigated a variety of ways of formulating crowdsourcing tasks ().", "labels": [], "entities": [{"text": "SRL", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9649153351783752}, {"text": "SRL", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.982122540473938}]}, {"text": "All have found that crowd feedback generally suffers from low interannotator agreement scores and often produces incorrect labels.", "labels": [], "entities": []}, {"text": "These results seem to indicate that, regardless of the design of the task, SRL is simply too difficult to be effectively crowdsourced.", "labels": [], "entities": [{"text": "SRL", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9549254775047302}]}, {"text": "Proposed Approach We observe that there are significant differences in difficulties among SRL annotation tasks, depending on factors such as the complexity of a specific sentence or the difficulty of a specific semantic role.", "labels": [], "entities": [{"text": "Approach", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9662787914276123}, {"text": "SRL annotation tasks", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.9301205277442932}]}, {"text": "We therefore postulate that a subset of annotation tasks is in fact suitable for crowd workers, while others require expert involvement.", "labels": [], "entities": []}, {"text": "We also postulate that it is possible to use a classifier to predict whether a specific task is easy enough for crowd workers.", "labels": [], "entities": []}, {"text": "Based on these intuitions, we propose CROWD-IN-THE-LOOP, a hybrid annotation approach that involves both crowd workers and experts: All annotation tasks are passed through a decision function (referred to as TASKROUTER) that classifies them as either crowd-appropriate or expertrequired, and sent to crowd or expert annotators accordingly.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 208, "end_pos": 218, "type": "METRIC", "confidence": 0.9748543500900269}]}, {"text": "Refer to for an illustration of this workflow.", "labels": [], "entities": []}, {"text": "We conduct an experimental evaluation that shows (1) that we are able to design a classifier that can distinguish between crowd-appropriate and expert-required tasks at very high accuracy (96%), and (2) that our proposed workflow allows us to pass over two-thirds of the annotation workload to crowd workers, thereby significantly reducing the need for costly expert involvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9956275224685669}]}, {"text": "Contributions In detail, our contributions are: \u2022 We propose CROWD-IN-THE-LOOP, a novel approach for creating annotated SRL data with both crowd workers and experts.", "labels": [], "entities": [{"text": "SRL", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.931156575679779}]}, {"text": "It reduces overall labeling costs by leveraging the crowd whenever possible, and maintains annotation quality by involving experts whenever necessary.", "labels": [], "entities": [{"text": "labeling", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9605893492698669}]}, {"text": "\u2022 We propose TASKROUTER, an annotation task decision function (or classifier), that classifies each annotation task into one of two categories: expert-required or crowdappropriate.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9839447140693665}]}, {"text": "We carefully define the classification task, discuss features and evaluate different classification models.", "labels": [], "entities": []}, {"text": "\u2022 We conduct a detailed experimental evaluation of the proposed workflow against several baselines including standard crowdsourcing and other hybrid annotation approaches.", "labels": [], "entities": []}, {"text": "We analyze the strengths and weaknesses of each approach and illustrate how expert involvement is required to address errors made by crowd workers.", "labels": [], "entities": []}, {"text": "Outline This paper is organized as follows: We first conduct a baseline study of crowdsourcing SRL annotation, and analyze the difficulties of relying solely on crowd workers (Section 2).", "labels": [], "entities": [{"text": "Outline", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.926802396774292}, {"text": "crowdsourcing SRL annotation", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.7896849115689596}]}, {"text": "Based on this analysis, we define the classification problem for CROWD-IN-THE-LOOP, discuss the design of our classifier, and evaluate its accuracy (Section 3).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9995429515838623}]}, {"text": "We then employ this classifier in the proposed CROWD-IN-THE-LOOP approach and comparatively evaluate it against a number of crowdsourcing and hybrid workflows (Section 4).", "labels": [], "entities": []}, {"text": "We discuss related work (Section 5) and conclude the study in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the accuracy of TASKROUTER we use the standard measure of accuracy for binary classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9992796778678894}, {"text": "TASKROUTER", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.8728055953979492}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9986677169799805}]}, {"text": "As  in which we train an SVM with (1) task-level features, (2) sentence-level features, (3) all features, and (4) our proposed fuzzy two-layer classifier.", "labels": [], "entities": []}, {"text": "We use the dataset created in our crowdsourcing study (see Section 2.2), which consists of 2,549 annotation tasks labeled as either expertrequired or crowd-appropriate according to our definitions and the results of the study.", "labels": [], "entities": []}, {"text": "We leverage five-fold cross validation to train the classifiers over a training split (80%).", "labels": [], "entities": []}, {"text": "The cross validation results are listed in.", "labels": [], "entities": []}, {"text": "Our proposed classifier outperforms all baselines and reaches a classification accuracy of 0.96.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.914313793182373}]}, {"text": "Interestingly, we also note that tasklevel features are significantly more important than sentence-level features, as the setup SVM task outperforms SVM sentence by 6 accuracy points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9946863651275635}]}, {"text": "Furthermore, our proposed approach outperforms SVM task+sentence , indicating a positive impact of modeling the global interplay of annotation tasks.", "labels": [], "entities": []}, {"text": "These experiments confirm our initial postulation that it is possible to train a high quality classifier to detect expert-required tasks.", "labels": [], "entities": []}, {"text": "We refer to the best performing setup as TASKROUTER.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9548743963241577}]}, {"text": "Data We use the dataset created in the crowdsourcing study in Section 2, consisting of 2,549 annotation tasks labeled either as expert-required  or crowd-appropriate 4 . As shown in Section 3.3, we use 80% of the dataset to train TASKROUTER under cross validation, and conduct the comparative evaluation using the remaining 20%.", "labels": [], "entities": []}, {"text": "Human annotators & curation We simulate an expert annotator using the CoNLL-2009 gold SRL labels and reuse the crowd answers from the study for crowd annotators.", "labels": [], "entities": [{"text": "CoNLL-2009 gold SRL labels", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.9693801999092102}]}, {"text": "For each setting, we gather crowd and expert answers to the annotation tasks, and interpret the answers to curate the SRL labels that were produced by the statistical SRL system.", "labels": [], "entities": [{"text": "SRL labels", "start_pos": 118, "end_pos": 128, "type": "TASK", "confidence": 0.7543226182460785}]}, {"text": "After curation, we evaluate the resulting labeled sentences against gold-labeled data to determine the annotation quality in terms of precision, recall and F 1 -score.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9997000694274902}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9996864795684814}, {"text": "F 1 -score", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9900552481412888}]}, {"text": "Evaluation Metrics Next to the quality of resulting annotations, we are interested to evaluate how effectively we integrate the crowd.", "labels": [], "entities": []}, {"text": "We measure this in two metrics.", "labels": [], "entities": []}, {"text": "One is the percentage of tasks that go to the crowd and to experts respectively.", "labels": [], "entities": []}, {"text": "Note that in the HYBRID setup, some tasks go to both crowd workers and experts, so that the percentages can add up to over a hundred percent.", "labels": [], "entities": []}, {"text": "This information is illustrated in column WORK-LOAD in.", "labels": [], "entities": [{"text": "WORK-LOAD", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9825397729873657}]}, {"text": "The second is the overall validity of crowd feedback, referred to as correctness, measured as the ratio of correct answers among all answers retrieved from the crowd.", "labels": [], "entities": [{"text": "validity", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9780893921852112}, {"text": "correctness", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.8752184510231018}]}, {"text": "We provide two values for correctness in, under column CORRECTNESS: The first is the correctness only overcrowd feedback.", "labels": [], "entities": [{"text": "CORRECTNESS", "start_pos": 55, "end_pos": 66, "type": "METRIC", "confidence": 0.9694180488586426}]}, {"text": "Note that this value is the same for all CROWD and HYBRID setups since in these approaches 100% of annotation tasks are passed to the crowd.", "labels": [], "entities": []}, {"text": "The second named hybrid is the overall correctness of the resolved answers with both expert and crowd feedback.", "labels": [], "entities": []}, {"text": "We will release the dataset soon.", "labels": [], "entities": []}, {"text": "The results of our experiments are summarized in.", "labels": [], "entities": []}, {"text": "We make the following observations: CROWD-IN-THE-LOOP significantly increases annotation quality.", "labels": [], "entities": [{"text": "CROWD-IN-THE-LOOP", "start_pos": 36, "end_pos": 53, "type": "METRIC", "confidence": 0.9200366735458374}]}, {"text": "Our evaluation shows that CROWD-IN-THE-LOOP produces SRL annotation with significantly higher quality compared to crowdsourcing or hybrid scenarios.", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8947220742702484}]}, {"text": "With a resulting F 1 -score of 0.94, it outperforms the best performing crowdsourcing setup (0.90) by 4 points.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9914655834436417}]}, {"text": "More importantly, our proposed approach also outperforms other hybrid approaches that partially leverage experts.", "labels": [], "entities": []}, {"text": "It outperforms the best hybrid approach (0.91) by 3 points, indicating that TASKROUTER is better to select expert-required tasks than a method with only crowd agreement.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9042052626609802}]}, {"text": "Significantly less expert involvement required.", "labels": [], "entities": []}, {"text": "In our experiments, more than two-thirds of all tasks were determined to be crowd-appropriate by TASKROUTER.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.7893377542495728}]}, {"text": "This considerably reduces the need for expert involvement compared to expert labeling, while still maintaining relatively high annotation quality.", "labels": [], "entities": []}, {"text": "In particular, our approach compares favorably to other hybrid setups in which a similar partition of tasks is completed by experts.", "labels": [], "entities": []}, {"text": "Since TASKROUTER is more capable to choose expert-required tasks than previous approaches, we achieve higher overall quality at similar levels of expert involvement.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.5194653868675232}]}, {"text": "As the correctness column in shows, the selection of tasks by TASKROUTER is more appropriate for the crowd in general.", "labels": [], "entities": [{"text": "TASKROUTER", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.8087038397789001}]}, {"text": "Their average correctness increases to 0.92, compared to 0.84 if the crowd completes 100% of the tasks.", "labels": [], "entities": [{"text": "correctness", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9541557431221008}]}], "tableCaptions": [{"text": " Table 2: Tasks in our crowdsourcing study by ratio of how", "labels": [], "entities": []}, {"text": " Table 3: Breakdown of annotation tasks by question types and semantic labels, and proportion of expert-required tasks", "labels": [], "entities": []}, {"text": " Table 6: Comparative evaluation of different approaches for generating gold-standard SRL annotation. The improvements of", "labels": [], "entities": [{"text": "SRL annotation", "start_pos": 86, "end_pos": 100, "type": "TASK", "confidence": 0.7766136527061462}]}]}