{"title": [{"text": "Chinese Zero Pronoun Resolution with Deep Memory Network", "labels": [], "entities": [{"text": "Chinese Zero Pronoun Resolution", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.797833725810051}]}], "abstractContent": [{"text": "Existing approaches for Chinese zero pronoun resolution typically utilize only syn-tactical and lexical features while ignoring semantic information.", "labels": [], "entities": [{"text": "Chinese zero pronoun resolution", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6503514051437378}]}, {"text": "The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents.", "labels": [], "entities": []}, {"text": "Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content.", "labels": [], "entities": []}, {"text": "In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents.", "labels": [], "entities": []}, {"text": "Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations.", "labels": [], "entities": [{"text": "resolver", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.97802734375}]}, {"text": "Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.", "labels": [], "entities": [{"text": "OntoNotes 5.0 dataset", "start_pos": 19, "end_pos": 40, "type": "DATASET", "confidence": 0.8830083807309469}]}], "introductionContent": [{"text": "A zero pronoun (ZP) is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (.", "labels": [], "entities": []}, {"text": "A ZP can be either anaphoric if it corefers to one or more preceding noun phrases (antecedents) in the associated text, or non-anaphoric if there are no such noun phrases.", "labels": [], "entities": []}, {"text": "Below is an example of ZPs and their antecedents, where \"\u03c6\" denotes the ZP.", "labels": [], "entities": []}, {"text": "[\u8b66\u65b9] \u8868\u793a \u4ed6\u4eec \u81ea\u6740 \u7684 \u53ef\u80fd\u6027 \u5f88\u9ad8\uff0c \u4e0d \u8fc7 \u03c6 1 \u4e5f \u4e0d \u6392\u9664 \u03c6 2 \u6709 \u4ed6\u6740 \u7684 \u53ef\u80fd\u3002", "labels": [], "entities": []}, {"text": "([The police] said that they are more likely to commit suicide, but \u03c6 1 could not rule out \u03c6 2 the possibility of homicide.)", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.9586068987846375}]}, {"text": "In this example, the ZP \"\u03c6 1 \" is an anaphoric ZP that refers to the antecedent \"\u8b66\u65b9/The police\" while the ZP \"\u03c6 2 \" is non-anaphoric.", "labels": [], "entities": []}, {"text": "Unlike overt pronouns, ZPs lack grammatical attributes such as gender and number that have been proven to be essential in pronoun resolution, which makes ZP resolution a more challenging task than overt pronoun resolution.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7870252728462219}, {"text": "ZP resolution", "start_pos": 154, "end_pos": 167, "type": "TASK", "confidence": 0.8301403820514679}, {"text": "overt pronoun resolution", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.7757539749145508}]}, {"text": "Automatic Chinese ZP resolution is typically composed of two steps, i.e., anaphoric zero pronoun (AZP) identification that identifies whether a ZP is anaphoric; and AZP resolution, which determines antecedents for AZPs.", "labels": [], "entities": [{"text": "Chinese ZP resolution", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.644248386224111}, {"text": "AZP resolution", "start_pos": 165, "end_pos": 179, "type": "METRIC", "confidence": 0.9355542957782745}]}, {"text": "For AZP identification, state-of-the-art resolvers use machine learning algorithms to build AZP classifiers in a supervised manner).", "labels": [], "entities": [{"text": "AZP identification", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8949674367904663}]}, {"text": "For AZP resolution, literature approaches include unsupervised methods, feature-based supervised models (, and neural network models.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.96688112616539}]}, {"text": "Neural network models for AZP resolution are of growing interest for their capacity to learn task-specific representations without extensive feature engineering and to effectively exploit lexical information for ZPs and their candidate antecedents in a more scalable manner than feature-based models.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8917328715324402}]}, {"text": "Despite these advantages, existing supervised approaches ( for AZP resolution typically utilize only syntactical and lexical information through features.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.8986737132072449}]}, {"text": "They overlook semantic information that is regarded as an important factor in the resolution of common noun phrases.", "labels": [], "entities": [{"text": "resolution of common noun phrases", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.8995610117912293}]}, {"text": "The fundamental reason is that ZPs have no descriptive information, which results in difficulty in calculating semantic similarities and relatedness scores between the ZPs and their antecedents.", "labels": [], "entities": []}, {"text": "Therefore, the proper representations of ZPs are required so as to take advantage of semantic information when resolving ZPs.", "labels": [], "entities": []}, {"text": "However, representing ZPs is challenging because they are merely gaps that convey no actual content.", "labels": [], "entities": []}, {"text": "One straightforward method to address this issue is to represent ZPs with supplemental information provided by some available components, such as contexts and candidate antecedents.", "labels": [], "entities": []}, {"text": "Motivated by who encode a ZP's lexical contexts by utilizing its preceding word and governing verb, we notice that a ZP's context can help to describe the ZP itself.", "labels": [], "entities": []}, {"text": "As an example of its usefulness, given the sentence \"\u03c6 taste spicy\", people may resolve the ZP \"\u03c6\" to the candidate antecedent \"red peppers\", but can hardly regard \"my shoes\" as its antecedent, because they naturally look at the ZP's context \"taste spicy\" to resolve it (\"my shoes\" cannot \"taste spicy\").", "labels": [], "entities": []}, {"text": "Meanwhile, considering that the antecedents of a ZP provide the necessary information for interpreting the gap (ZP), it is a natural way to express a ZP by its potential antecedents.", "labels": [], "entities": [{"text": "interpreting the gap", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8254725337028503}]}, {"text": "However, only some subsets of candidate antecedents are needed to represent a ZP 1 . To achieve this goal, a desirable solution should be capable of explicitly capturing the importance of each candidate antecedent and using them to buildup the representation for the ZP.", "labels": [], "entities": []}, {"text": "In this paper, inspired by the recent success of computational models with attention mechanism and explicit memory, we focus on AZP resolution, proposing the zero pronounspecific memory network (ZPMN) that is competent for representing a ZP with information obtained from its contexts and candidate antecedents.", "labels": [], "entities": [{"text": "AZP resolution", "start_pos": 128, "end_pos": 142, "type": "TASK", "confidence": 0.8685348927974701}]}, {"text": "These representations provide our system with an ability to take advantage of semantic information when resolving ZPs.", "labels": [], "entities": []}, {"text": "Our ZPMN consists of multiple computational layers with shared parameters.", "labels": [], "entities": []}, {"text": "With the underlying intuition that not all candidate antecedents are equally relevant for representing the ZP, we develop each computational layer as an attention-based model, which first learns the importance of each candidate antecedent and then utilizes this information to calculate the continu-ous distributed representation of the ZP.", "labels": [], "entities": []}, {"text": "The attention weights over candidate antecedents with respect to the ZP's representation obtained by the last layer are regarded as the ZP coreference classification result.", "labels": [], "entities": [{"text": "coreference classification", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7617480754852295}]}, {"text": "Given that every component is differentiable, the entire model could be efficiently trained end-to-end with gradient descent.", "labels": [], "entities": []}, {"text": "We evaluate our method on the Chinese portions of the OntoNotes 5.0 corpus by comparing with the baseline systems in different experimental settings.", "labels": [], "entities": [{"text": "Chinese portions of the OntoNotes 5.0 corpus", "start_pos": 30, "end_pos": 74, "type": "DATASET", "confidence": 0.7506340231214251}]}, {"text": "Results show that our approach significantly outperforms the baseline algorithms and achieves state-of-the-art performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets: Following Chen and Ng, we run experiments on the Chinese portion of the OntoNotes Release 5.0 dataset 3 used in the).", "labels": [], "entities": [{"text": "OntoNotes Release 5.0 dataset", "start_pos": 82, "end_pos": 111, "type": "DATASET", "confidence": 0.8299086540937424}]}, {"text": "The dataset consists of three parts, i.e., a training set, a development set and a test set.", "labels": [], "entities": []}, {"text": "Since only the training set and the development set contain ZP coreference annotations, we train our model on the training set and utilize the development set for testing purposes.", "labels": [], "entities": []}, {"text": "Meanwhile, we reserve 20% of the training set as a held-out development set for tuning the hyperparameters of our network.", "labels": [], "entities": []}, {"text": "The same experimental data setting is utilized in the baseline system.", "labels": [], "entities": []}, {"text": "shows the statistics of our corpus.", "labels": [], "entities": []}, {"text": "Besides, documents in the datasets come from six sources, i.e., broadcast news (BN), newswires (NW), broadcast conversations (BC), telephone conversations (TC), web blogs (WB) and magazines (MZ  Evaluation metrics: Same as previous studies on Chinese ZP resolution (, we use three metrics to evaluate the quality of our model: recall, precision and F-score (denoted as R, P and F, respectively).", "labels": [], "entities": [{"text": "Chinese ZP resolution", "start_pos": 243, "end_pos": 264, "type": "TASK", "confidence": 0.5888308882713318}, {"text": "recall", "start_pos": 327, "end_pos": 333, "type": "METRIC", "confidence": 0.9996044039726257}, {"text": "precision", "start_pos": 335, "end_pos": 344, "type": "METRIC", "confidence": 0.9991839528083801}, {"text": "F-score", "start_pos": 349, "end_pos": 356, "type": "METRIC", "confidence": 0.9988967180252075}, {"text": "F", "start_pos": 378, "end_pos": 379, "type": "METRIC", "confidence": 0.9625797271728516}]}, {"text": "3 http://catalog.ldc.upenn.edu/LDC2013T19 Experimental settings: We employ three Chinese ZP resolution systems as our baselines, i.e.,;.", "labels": [], "entities": []}, {"text": "Consistent with, three experimental settings are designed to evaluate our approach.", "labels": [], "entities": []}, {"text": "In Setting 1, we directly employ the gold syntactic parse trees and gold AZPs that are obtained from the OntoNotes dataset.", "labels": [], "entities": [{"text": "AZPs", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9419385194778442}, {"text": "OntoNotes dataset", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9505870938301086}]}, {"text": "In Setting 2, we utilize gold syntactic parse trees and system (automatically identified) AZPs . In Setting 3, we employ system AZP and system syntactic parse trees that obtained through the Berkeley parser 5 , which is the state-of-the-art parsing model.", "labels": [], "entities": []}, {"text": "shows the experimental results of the baseline systems and our model on entire test set.", "labels": [], "entities": []}, {"text": "Our approach is abbreviated to ZPMN (k), where k indicates the number of hops.", "labels": [], "entities": []}, {"text": "The best methods in each of the three experimental settings are in bold text.", "labels": [], "entities": []}, {"text": "From, we can observe that our approach outperforms all previous baseline systems by a substantial margin.", "labels": [], "entities": []}, {"text": "Meanwhile, among all our models from single hop to six hops, using more computational layers could generally lead to better performance.", "labels": [], "entities": []}, {"text": "The best performance is achieved by the model with six hops under experimental Setting 1 and 2, and with four hops in experimental Setting 3.", "labels": [], "entities": []}, {"text": "Furthermore, the ZPMN (with six hops) significantly outperforms the state-of-the-art baseline system (Chen and Ng, 2016) under three experimental settings by 2.7%, 2.7%, and 3.9% in terms of overall F-score 6 , respectively.", "labels": [], "entities": [{"text": "F-score 6", "start_pos": 199, "end_pos": 208, "type": "METRIC", "confidence": 0.9785562753677368}]}, {"text": "In all words, our model is an extremely strong performer and substantially outperforms baseline methods, which demonstrate the efficiency of the proposed zero pronoun-specific memory network.", "labels": [], "entities": []}, {"text": "It is well accepted that computational models that are composed of multiple processing layers could learn representations of data with multiple levels of abstraction ().", "labels": [], "entities": []}, {"text": "In our approach, multiple computation layers allow the model to learn representations of AZPs with multiple levels of abstraction generated by candidate antecedents.", "labels": [], "entities": []}, {"text": "Each layer/hop retrieves important candidate antecedents, and transforms the repre-   sentation at previous level into a representation at a higher, slightly more abstract level.", "labels": [], "entities": []}, {"text": "We regard this representation as the \"key extension\" of the AZP, by which our model learns to encode the AZP in an efficient manner.", "labels": [], "entities": []}, {"text": "For per-source results, we conduct experiments by comparing the ZPMN (with six hops) with the state-of-the-art baseline system on six sources of test data, as shown in Table 3.", "labels": [], "entities": []}, {"text": "The rows in are the experimental results from different sources under the three experimental settings.", "labels": [], "entities": []}, {"text": "In experimental Settings 1 and 3, ZPMN improves results further across all the six sources of data.", "labels": [], "entities": []}, {"text": "Under experimental Setting 2, our model outperforms the baseline system in five of the six sources of data, only slightly underperforms in source TC.", "labels": [], "entities": []}, {"text": "All these prove that our approach achieves a considerable improvement in Chinese ZP resolution.", "labels": [], "entities": [{"text": "Chinese ZP resolution", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.6631267269452413}]}, {"text": "Moreover, to evaluate the effectiveness of our methods for modeling the AZP and candidate antecedents proposed in Section 2.2 and 2.3, we compare with three models that are all simplified versions of the ZPMN, namely, ZPContextFree where an AZP is initially represented by its governing verb and preceding word; AntContentAvg where the candidate antecedents are encoded by their averaged content word embeddings; and AntContentHead where each candidate antecedent is represented by the embedding of its headword.", "labels": [], "entities": []}, {"text": "To make comparison as fair as possible, we keep the other parts of these models unchanged from the ZPMN with six computational layers (hop 6).", "labels": [], "entities": [{"text": "ZPMN", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.8342575430870056}]}, {"text": "To minimize the external influence, we run experiments under experimental Setting 1 (gold parse and gold AZPs).", "labels": [], "entities": [{"text": "AZPs", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.6790546774864197}]}, {"text": "With an intuition that contexts of an AZP provide more sufficient information than only a few specific of words in expressing the AZP, the performance of ZPContextFree is unsurprisingly worse than that of the ZPMN, which reflects the effects of the ZP-centered LSTM proposed to generate the initial representation for the AZP.", "labels": [], "entities": []}, {"text": "In addition, the performance of AntContentAvg is relatively low.", "labels": [], "entities": [{"text": "AntContentAvg", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.5692058801651001}]}, {"text": "We attribute this to the model assigning the same importance to all the content words in a phrase, which causes difficulty for the model to capture informative words in a candidate antecedent.", "labels": [], "entities": []}, {"text": "Meanwhile, AntContent-Head only models limited information when encoding candidate antecedents, thereby underperforms the ZPMN whose external memory contains sentence-level information both outside and inside the candidate antecedents.", "labels": [], "entities": []}, {"text": "These demonstrate the utility of the method for modeling candidate antecedents.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the training and test corpus.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results on the test data. ZPMN represents the proposed zero pronoun-specific  memory network model, and the number beside ZPMN in each row denotes the number of hops.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results on each source of test data. The strongest F-score in each row is in bold.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9971154928207397}]}, {"text": " Table 4: Experimental results of different models.", "labels": [], "entities": []}]}