{"title": [{"text": "Exploiting Morphological Regularities in Distributional Word Representations", "labels": [], "entities": [{"text": "Exploiting Morphological Regularities", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7542852163314819}, {"text": "Distributional Word Representations", "start_pos": 41, "end_pos": 76, "type": "TASK", "confidence": 0.6390690406163534}]}], "abstractContent": [{"text": "We present a simple, fast and unsuper-vised approach for exploiting morphological regularities present in high dimensional vector spaces.", "labels": [], "entities": []}, {"text": "We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators.", "labels": [], "entities": []}, {"text": "We evaluate this approach on MSR word analogy test set (Mikolov et al., 2013d) with an accuracy of 85% which is 12% higher than the previous best known system.", "labels": [], "entities": [{"text": "MSR word analogy test set", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.76801997423172}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9990324974060059}]}], "introductionContent": [{"text": "Vector representation of words are presently being used to solve a variety of problems like document classification), question answering () and chunking.", "labels": [], "entities": [{"text": "document classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.7306438833475113}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8754792809486389}]}, {"text": "Word representations capture both syntactic and semantic properties) of natural language.", "labels": [], "entities": []}, {"text": "exploited these regularities to generate prefix/suffix based morphological transformation rules in an unsupervised manner.", "labels": [], "entities": [{"text": "prefix/suffix based morphological transformation", "start_pos": 41, "end_pos": 89, "type": "TASK", "confidence": 0.7077508370081583}]}, {"text": "These morphological transformations were represented as vectors in the same embedding space as the vocabulary.", "labels": [], "entities": []}, {"text": "They used a graph based approach and represented transformations as \"type:from:to\" triples and a direction vector: for example \"suffix:ion:e:\u2191 creation \" implies a suffix change just like in the case \"creation\" to \"create\".", "labels": [], "entities": []}, {"text": "Using Soricut's transformation rules, the major problem is identifying the correct direction vector to use fora given case, i.e. if we have to generate an embedding for \"runs\", which rule to apply on \"run\".", "labels": [], "entities": []}, {"text": "Experimental results showed that \"walk -* These authors contributed equally to this walks\" gives better results than rules like \"invent -invents\" or \"object -objects\" in generating word embedding for \"runs\".", "labels": [], "entities": []}, {"text": "In this paper, we try to explore if we can harness this morphological regularity in a much better way, than applying a single direction using vector arithmetic.", "labels": [], "entities": []}, {"text": "Hence, we tried to come up with a global transformation operator, which aligns itself with the source word, to give best possible word embedding for target word.", "labels": [], "entities": []}, {"text": "We will have a single transformation operator for each rule, irrespective of the form of root word (like verb or a noun).", "labels": [], "entities": []}, {"text": "Our transformation operator is in the form of a matrix, which when applied on a word embedding (cross product of vector representation of word with transformation matrix) gives us a word embedding for target word.", "labels": [], "entities": []}, {"text": "The intuition is not to solve for \"invent is to invents as run is to ?\" or \"object is to objects as run is to ?\", but instead we are solving for \"walk is to walks, object is to objects, invent is to invents, ....", "labels": [], "entities": []}, {"text": "as run is to ?\".", "labels": [], "entities": []}, {"text": "A transformation operator aims to be a unified transition function for different forms of the same transition.", "labels": [], "entities": []}, {"text": "Learning a representation of this operator would allow us to capture the semantic changes associated with the transition.", "labels": [], "entities": []}, {"text": "As word embeddings for rare and out-of-vocabulary words are poorly trained or not trained at all, learning this operator will be beneficial to reducing the sparsity in corpus.", "labels": [], "entities": []}, {"text": "The idea of projection learning has been applied to a multitude of tasks such as in the learning of cross lingual mappings for translation of English to) and for unsupervised mapping between vector spaces (Akhtar et al., 2017a).", "labels": [], "entities": [{"text": "projection learning", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9247017502784729}]}, {"text": "Our approach has its basis on the same lines but with a different formulation and end goal to learn morphological rules rather than semantic associations and translational constraints.", "labels": [], "entities": []}, {"text": "In summary, we present anew method to harness morphological regularities present in high dimensional word embeddings and learn its representation in the form of a matrix.", "labels": [], "entities": []}, {"text": "Using this method, we present state of the art results on MSR word analogy dataset.", "labels": [], "entities": [{"text": "MSR word analogy", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.7262770533561707}]}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first discuss the corpus used for training the transformation operators in section 2.", "labels": [], "entities": []}, {"text": "In section 3, we discuss how these transformation operators are trained.", "labels": [], "entities": []}, {"text": "Later in sections 4, we analyze and discuss the results of our experiments.", "labels": [], "entities": []}, {"text": "We finish this paper with future scope of our work in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are using word embeddings trained on Google News corpus () for our experiments.", "labels": [], "entities": [{"text": "Google News corpus", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.8683241605758667}]}, {"text": "For the model trained in this paper, we have used the Skip-gram ( algorithm.", "labels": [], "entities": []}, {"text": "The dimensionality has been fixed at 300 with a minimum count of 5 along with negative sampling.", "labels": [], "entities": []}, {"text": "As training set and for estimating the frequencies of words, we use the Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9785241186618805}]}, {"text": "The corpus contains about 1 billion tokens.", "labels": [], "entities": []}, {"text": "The MSR dataset (Mikolov et al., 2013d) contains 8000 analogy questions.", "labels": [], "entities": [{"text": "MSR dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8320713937282562}]}, {"text": "This data set has been used by us for testing our model.", "labels": [], "entities": []}, {"text": "The relations portrayed by these questions are morphosyntactic, and can be categorized according to parts of speech -adjectives, nouns and verbs.", "labels": [], "entities": []}, {"text": "Adjective relations include comparative and superlative (good is to best as smart is to smartest).", "labels": [], "entities": []}, {"text": "Noun relations include singular and plural, possessive and non-possessive (dog is to dog's as cat is to cat's).", "labels": [], "entities": []}, {"text": "Verb relations are tense modifications (work is to worked as accept is to accepted).", "labels": [], "entities": []}, {"text": "For all the experiments, we have calculated the fraction of answers correctly answered by the system on MSR word analogy dataset.", "labels": [], "entities": [{"text": "MSR word analogy dataset", "start_pos": 104, "end_pos": 128, "type": "DATASET", "confidence": 0.7871168702840805}]}], "tableCaptions": [{"text": " Table 2: Scores on MSR word analogy test set.", "labels": [], "entities": [{"text": "MSR word analogy test set", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.7750826120376587}]}]}