{"title": [{"text": "Identifying attack and support argumentative relations using deep learning", "labels": [], "entities": [{"text": "Identifying attack", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9110768437385559}]}], "abstractContent": [{"text": "We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another , of the kind that naturally occur in a debate.", "labels": [], "entities": []}, {"text": "The architecture uses two (uni-directional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.", "labels": [], "entities": [{"text": "relation-based) argument mining", "start_pos": 276, "end_pos": 307, "type": "TASK", "confidence": 0.6557097062468529}]}], "introductionContent": [{"text": "Argument Mining (AM) is a relatively new research area which involves, amongst others, the automatic detection in text of arguments, argument components, and relations between arguments (see) for an overview).", "labels": [], "entities": [{"text": "Argument Mining (AM)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8726302862167359}, {"text": "automatic detection in text of arguments", "start_pos": 91, "end_pos": 131, "type": "TASK", "confidence": 0.7541600863138834}]}, {"text": "We focus on a specific type of AM, referred to as Relation-based AM, which has recently received attention by several researchers (e.g. see ().", "labels": [], "entities": [{"text": "Relation-based AM", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.6820386499166489}]}, {"text": "This type of AM aims at identifying argumentative relations of attack and support between natural language arguments in text, by classifying pairs of pieces of text as belonging to attack, support or neither attack nor support relations.", "labels": [], "entities": [{"text": "AM", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9669556617736816}]}, {"text": "For example, consider the three texts taken from Carstens and Toni (2015): t 1 : 'We should grant politicians immunity from prosecution' t 2 : 'Giving politicians immunity allows them to focus on performing their duties' t 3 : 'The ability to prosecute politicians is the ultimate protection against abuse of power' Here t 2 supports t 1 , t 3 attacks t 1 , and t 2 and t 3 neither attack nor support one another.", "labels": [], "entities": []}, {"text": "Relation-based AM is useful, for example, to pave the way towards identifying accepted opinions () or divisive issues () within debates.", "labels": [], "entities": [{"text": "Relation-based AM", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7068793177604675}]}, {"text": "We propose a deep learning architecture for Relation-based AM based on Long-Short Term Memory (LSTM) networks.", "labels": [], "entities": [{"text": "Relation-based AM", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8655568957328796}]}, {"text": "Within the architecture, each input text is fed, as a (trained or non-trained) 100-dimensional GloVe embedding (), into a (unidirectional or bidirectional) LSTM which produces a vector representation of the text independently of the other text being analysed.", "labels": [], "entities": []}, {"text": "The two vectors are then merged (using element-wise sum or concatenation) and the resulting vector is fed to a softmax classifier which predicts whether the pair of input texts belongs to the attack, support or neither relations.", "labels": [], "entities": []}, {"text": "The input texts maybe at most 50 words long, but are not restricted to single sentences.", "labels": [], "entities": []}, {"text": "We experimented with several instances of the architecture and achieved 89.53% accuracy and 89.07% F 1 using unidirectional LSTMs and concatenation as the merge layer, considerably outperforming feature-based supervised classifiers used in the studies which presented the corpus we also use ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9974708557128906}, {"text": "F 1", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.992699146270752}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we discuss related work and the corpus we use.", "labels": [], "entities": []}, {"text": "In Section 3 we describe our deep learning architecture and report experiments and results in Section 4.", "labels": [], "entities": []}, {"text": "We conclude the paper and propose directions for future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Determining relations between any texts can be seen as a three-class problem, with labels L = {attack, support, neither}.", "labels": [], "entities": []}, {"text": "We used a dataset covering various topics such as movies, technology and politics 1 , where attack relations represent 31% of the dataset, support relations represent 32% of the dataset and neither relations represent 37% of the dataset.", "labels": [], "entities": []}, {"text": "We have also explored the use of other corpora (e.g. the SNLI corpus ( and Araucaria in AIFdb 2 ) that we ultimately decided not to include due to their structure not being directly amenable to our analysis.", "labels": [], "entities": [{"text": "SNLI corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.885450929403305}]}, {"text": "summarises the deep learning architecture that we use for predicting which relation from L = {attack, support, neither} holds between the first and the second texts in any input pair.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: 5x10 fold cross-validation results, using c(oncat) or s(um) for merging the output of the two  (Bi)LSTMs, with (non-)trained embeddings; T (True)/F (False) represent inclusion/omission, respec- tively, of the Dense 32 ReLU layer. std represents standard deviation of 5x10 fold cross-validation.", "labels": [], "entities": []}]}