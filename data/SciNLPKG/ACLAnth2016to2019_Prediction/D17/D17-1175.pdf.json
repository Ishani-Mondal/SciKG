{"title": [{"text": "Stack-based Multi-layer Attention for Transition-based Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.6042583882808685}]}], "abstractContent": [{"text": "Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as machine translation and text summarization, simply applying this approach to transition-based dependency parsing cannot yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.8171212673187256}, {"text": "text summarization", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.7236412167549133}, {"text": "transition-based dependency parsing", "start_pos": 184, "end_pos": 219, "type": "TASK", "confidence": 0.6109424432118734}, {"text": "head selection", "start_pos": 324, "end_pos": 338, "type": "TASK", "confidence": 0.8666734099388123}]}, {"text": "In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information.", "labels": [], "entities": []}, {"text": "In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees.", "labels": [], "entities": []}, {"text": "We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.", "labels": [], "entities": [{"text": "PTB and CTB datasets", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.7237860336899757}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9993168115615845}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.7920939922332764}]}], "introductionContent": [{"text": "Deep learning models have been proven very effective in solving various NLP problems such as language modeling, machine translation and syntactic parsing.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7413346171379089}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8283388614654541}, {"text": "syntactic parsing", "start_pos": 136, "end_pos": 153, "type": "TASK", "confidence": 0.7353593111038208}]}, {"text": "For dependency parsing, one line of research aims to incrementally integrate distributed word representations into classic dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8698575496673584}, {"text": "dependency parsing", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.745489627122879}]}, {"text": "Another line of research attempts to leverage end-to-end neural network to perform dependency parsing, such as stack-LSTM and sequence-to-sequence (seq2seq) model.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8001519441604614}]}, {"text": "Recently seq2seq model has made significant success in many NLP tasks, such as machine translation and text summarization (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8468725085258484}, {"text": "text summarization", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7758595049381256}]}, {"text": "Unfortunately, to our best knowledge, simply applying seq2seq model to transition-based dependency parsing cannot achieve comparable results as in other state-of-the-art methods like stack-LSTM and head selection.", "labels": [], "entities": [{"text": "transition-based dependency parsing", "start_pos": 71, "end_pos": 106, "type": "TASK", "confidence": 0.5914764503637949}, {"text": "head selection", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.828416109085083}]}, {"text": "One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information, which plays a key role in classic transition-based or graph-based dependency parsing models, cannot be explicitly employed.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8380953371524811}, {"text": "transition-based or graph-based dependency parsing", "start_pos": 149, "end_pos": 199, "type": "TASK", "confidence": 0.7200416207313538}]}, {"text": "For example, classic transition-based parsing algorithm utilizes a stack to manage the heads of partial sub-trees and leverages these evidents for action selection, while such information is missing from current seq2seq models.", "labels": [], "entities": []}, {"text": "Another problem is related to the limit of the conventional attention network being used in seq2seq network, which is unable to capture dependencies between words in the input.", "labels": [], "entities": []}, {"text": "As a matter of fact, various types of features (word unigram, bigram, trigram, . .", "labels": [], "entities": []}, {"text": ") traditionally adopted by transition-based parsing algorithm are usually ignored by the current attention mechanism, but they are very important to capture word dependencies in generated partial trees.", "labels": [], "entities": [{"text": "transition-based parsing algorithm", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.6173912783463796}]}, {"text": "In this paper, we propose a stack-based multilayer attention mechanism to solve the above problems.", "labels": [], "entities": []}, {"text": "To simulate the stack used in the transition-based dependency parsing, we introduce two binary vectors, one indicates whether a word is pushed into the stack, and another indicates whether a word is popped out from it.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7417111992835999}]}, {"text": "To model the complex structural information, we propose a multi-layer attention based on the stack information, previous action and input sentence.", "labels": [], "entities": []}, {"text": "The multi-layer attention aims to capture multiple word dependencies in partial trees for action prediction.", "labels": [], "entities": [{"text": "action prediction", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7836147546768188}]}, {"text": "We evaluate our model on English and Chinese datasets.", "labels": [], "entities": [{"text": "English and Chinese datasets", "start_pos": 25, "end_pos": 53, "type": "DATASET", "confidence": 0.5980291813611984}]}, {"text": "Experimental results show that our proposed model can significantly outperform the basic seq2seq model with 1.87 UAS (English) and 1.61 UAS (Chinese), matching the state-of-the-art parsing performance.", "labels": [], "entities": []}, {"text": "With 4 models ensembled, we obtain further improvements with accuracies of 94.16 UAS (English) and 87.97 UAS (Chinese).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9719093441963196}, {"text": "UAS", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9909136295318604}, {"text": "UAS", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9863937497138977}]}], "datasetContent": [{"text": "In this section, we evaluate our parsing model on the English and Chinese datasets.", "labels": [], "entities": [{"text": "parsing", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9645400643348694}, {"text": "English and Chinese datasets", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.6612028926610947}]}, {"text": "Following, Stanford Dependencies (de Marneffe and Manning, 2008) conversion of the Penn Treebank (PTB) ( and Chinese Treebank 5.1 (CTB) are adopted.", "labels": [], "entities": [{"text": "Stanford Dependencies (de Marneffe and Manning, 2008)", "start_pos": 11, "end_pos": 64, "type": "DATASET", "confidence": 0.9428814709186554}, {"text": "Penn Treebank (PTB)", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.9708034753799438}, {"text": "Chinese Treebank 5.1 (CTB)", "start_pos": 109, "end_pos": 135, "type": "DATASET", "confidence": 0.9436255594094595}]}, {"text": "We leverage the arc-standard algorithm for our dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.8334172368049622}]}, {"text": "In addition, we limit the vocabulary to contain up to 20k most frequent words and convert remaining words into the <unk> token.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Impact of l on English PTB dataset.", "labels": [], "entities": [{"text": "English PTB dataset", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.8758732875188192}]}, {"text": " Table 3: Impact of the different components on  English PTB dataset.", "labels": [], "entities": [{"text": "English PTB dataset", "start_pos": 49, "end_pos": 68, "type": "DATASET", "confidence": 0.9015660285949707}]}]}