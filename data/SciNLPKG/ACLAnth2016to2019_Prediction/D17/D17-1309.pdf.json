{"title": [{"text": "Natural Language Processing with Small Feed-Forward Networks", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6499897440274557}]}], "abstractContent": [{"text": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of un-structured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models.", "labels": [], "entities": []}, {"text": "Motivated by resource-constrained environments like mobile phones, we show-case simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep and recurrent neural networks with large network capacity have become increasingly accurate for challenging language processing tasks.", "labels": [], "entities": []}, {"text": "For example, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions () or billions ( of parameters.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7997179627418518}]}, {"text": "These models, however, may not be feasible in all computational settings.", "labels": [], "entities": []}, {"text": "In particular, models running on mobile devices are often constrained in terms of memory and computation.", "labels": [], "entities": []}, {"text": "Long Short-Term Memory (LSTM) models) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of have only roughly 900,000 parameters.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.7230664193630219}]}, {"text": "Latency, however, can still bean issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone.", "labels": [], "entities": [{"text": "Latency", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9926056861877441}, {"text": "LSTM translation", "start_pos": 201, "end_pos": 217, "type": "TASK", "confidence": 0.7457934021949768}]}, {"text": "Feed-forward neural networks have the potential to be much faster.", "labels": [], "entities": []}, {"text": "In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach.", "labels": [], "entities": []}, {"text": "We begin by introducing the network model structure and the character-based representations we use throughout all tasks ( \u00a72).", "labels": [], "entities": []}, {"text": "The four tasks that we address are: language identification (Lang-ID), part-of-speech (POS) tagging, word segmentation, and preordering for translation.", "labels": [], "entities": [{"text": "language identification (Lang-ID)", "start_pos": 36, "end_pos": 69, "type": "TASK", "confidence": 0.7872850060462951}, {"text": "part-of-speech (POS) tagging", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.6074927508831024}, {"text": "word segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7817955315113068}]}, {"text": "In order to use feed-forward networks for structured prediction tasks, we use transition systems) with feature embeddings as proposed by, and introduce two novel transition systems for the last two tasks.", "labels": [], "entities": []}, {"text": "We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy fora given memory budget: 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9978461265563965}]}, {"text": "Quantization: Using more dimensions and less precision (Lang-ID: \u00a73.1).", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9980362057685852}]}, {"text": "2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: \u00a73.2).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.7375360131263733}]}, {"text": "3. Selected features: Adding explicit feature conjunctions (segmentation: \u00a73.3).", "labels": [], "entities": []}, {"text": "4. Pipelines: Introducing another task in a pipeline and allocating parameters to the auxiliary task instead (preordering: \u00a73.4).", "labels": [], "entities": []}, {"text": "We achieve results at or near state-of-the-art with small (< 3 MB) models on all four tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation.", "labels": [], "entities": [{"text": "language identification", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.7804314494132996}, {"text": "part-of-speech tagging", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7045149505138397}, {"text": "word segmentation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7756006419658661}, {"text": "statistical machine translation", "start_pos": 163, "end_pos": 194, "type": "TASK", "confidence": 0.71614208817482}]}, {"text": "Evaluation Metrics In addition to standard task-specific quality metrics, our evaluations also consider model size and computational cost.", "labels": [], "entities": []}, {"text": "We skirt implementation details by calculating size as the number of kilobytes (1KB=1024 bytes) needed to represent all model parameters and resources.", "labels": [], "entities": []}, {"text": "We approximate the computational cost as the number of floating-point operations (FLOPs) performed for one forward pass through the network given an embedding vector h 0 . This cost is dominated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal-ization, but still accounts for the final layer logits.", "labels": [], "entities": []}, {"text": "To ground this metric, we also provide indicative absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language Identification. Quantization allows trad- ing numerical precision for larger embeddings. The two mod- els from Baldwin and Lui (2010) are the nearest neighbor  (NN) and nearest prototype (NP) approaches.", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6807415336370468}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9292416572570801}]}, {"text": " Table 2: POS tagging. Embedded word clusters improves ac- curacy and allows the use of smaller embedding dimensions.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7558196187019348}]}, {"text": " Table 4: Segmentation results. Explicit bigrams are useful.", "labels": [], "entities": []}]}