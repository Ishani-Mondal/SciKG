{"title": [{"text": "Towards a Universal Sentiment Classifier in Multiple languages", "labels": [], "entities": [{"text": "Sentiment Classifier", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.8201970756053925}]}], "abstractContent": [{"text": "Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 9, "end_pos": 30, "type": "TASK", "confidence": 0.7700749933719635}]}, {"text": "In this paper we aim to build a universal sentiment classifier with a single classification model in multiple different languages.", "labels": [], "entities": [{"text": "universal sentiment classifier", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6688612500826517}]}, {"text": "In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in English and unlabeled parallel data available in a few language pairs.", "labels": [], "entities": []}, {"text": "It is not required that the parallel data exist between English and any other language , because the sentiment information can be transferred into any language via pivot languages.", "labels": [], "entities": []}, {"text": "We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between English and the target languages are not used.", "labels": [], "entities": [{"text": "universal sentiment classifier", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.6432790060838064}]}, {"text": "Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages .", "labels": [], "entities": [{"text": "cross-language sentiment classifiers", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.6714424192905426}]}], "introductionContent": [{"text": "Nowadays, a large amount of user-generated content (UGC) appears online everyday, such as tweets, comments and product reviews.", "labels": [], "entities": []}, {"text": "Sentiment classification on these data has become a popular research topic over the past few years ().", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9740849137306213}]}, {"text": "Distributed representations of words or word embeddings have been widely explored, and have proved its great usability for the sentiment classification task (.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.8991303046544393}]}, {"text": "Most existing sentiment classifiers rely on labeled training data and the data are usually language-dependent.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.9072245359420776}]}, {"text": "In other words, a sentiment classifier is learned from a labeled dataset in a specific language and this sentiment classifier can be used for sentiment classification in this language.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7807998657226562}, {"text": "sentiment classification", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.903974324464798}]}, {"text": "However, labeled training data for sentiment classification are not available or not easy to obtain in many languages in the world (e.g., Malaysian, Mongolian, Uighur).", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9769796431064606}]}, {"text": "Without reliable labeled data, it is hard to build a sentiment classifier in these resource-poor languages.", "labels": [], "entities": [{"text": "sentiment classifier", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8157064616680145}]}, {"text": "Fortunately, there area few studies investigating the task of cross-language sentiment classification (, which aims to make use of the labeled data in a source language (English inmost cases) to build a sentiment classifier in a target language.", "labels": [], "entities": [{"text": "cross-language sentiment classification", "start_pos": 62, "end_pos": 101, "type": "TASK", "confidence": 0.826757808526357}]}, {"text": "However, cross-language sentiment classification methods rely on parallel data between the source and target languages Ina resource-poor language, the parallel data between this language and the source language may not be available or is not easy to obtain.", "labels": [], "entities": [{"text": "cross-language sentiment classification", "start_pos": 9, "end_pos": 48, "type": "TASK", "confidence": 0.7939963340759277}]}, {"text": "In this circumstance, previous cross-language sentiment classification meth-ods will fail to work.", "labels": [], "entities": [{"text": "cross-language sentiment classification meth-ods", "start_pos": 31, "end_pos": 79, "type": "TASK", "confidence": 0.7543573752045631}]}, {"text": "Another shortcoming of previous crosslanguage sentiment classification researches is that we have to build an individual cross-language sentiment classifier for each target language, even when we want to perform sentiment classification in a couple of languages at the same time.", "labels": [], "entities": [{"text": "crosslanguage sentiment classification", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.7891170978546143}, {"text": "sentiment classification", "start_pos": 212, "end_pos": 236, "type": "TASK", "confidence": 0.8094907402992249}]}, {"text": "In this study, instead of building a sentiment classifier for each target language, we aim to build a universal sentiment classifier in multiple languages and this universal sentiment classifier only learns one single sentiment classification model and it can be applied for sentiment classification in many languages.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 275, "end_pos": 299, "type": "TASK", "confidence": 0.9018980264663696}]}, {"text": "In order to achieve this goal, we propose an approach to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in English and unlabeled parallel data available in a few language pairs.", "labels": [], "entities": []}, {"text": "As mentioned earlier, in some resource-poor languages, there do not exist direct parallel data between these languages and the source English language.", "labels": [], "entities": []}, {"text": "In order to address this problem, we propose a pivot-based model to transfer the sentiment information from the source language to any resource-poor language via pivot languages.", "labels": [], "entities": []}, {"text": "Finally, a universal sentiment classifier can be built because the multilingual word embeddings are in the same semantic space.", "labels": [], "entities": [{"text": "universal sentiment classifier", "start_pos": 11, "end_pos": 41, "type": "TASK", "confidence": 0.6608604888121287}]}, {"text": "We build three different models (Bilingual Model, Pivot-Driven Bilingual Model and Universal Multilingual Model) and compare them empirically in order to answer two questions in this paper: 1) Can pivot-based models learn bilingual sentiment-aware word embeddings effectively?", "labels": [], "entities": []}, {"text": "2) Can an effective universal sentiment classifier be built for multiple languages?", "labels": [], "entities": [{"text": "universal sentiment classifier", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7450029253959656}]}, {"text": "Without loss of generality, we present and compare the evaluation results of the models in five languages.", "labels": [], "entities": []}, {"text": "Evaluation results show that pivotdriven bilingual models perform as well as the bilingual model using direct parallel data, which lays the solid foundation of our universal model.", "labels": [], "entities": []}, {"text": "Moreover, it is very promising that our universal sentiment classifier can work well in five languages, and it can achieve very promising classification results as compared to several typical crosslanguage sentiment classification models.", "labels": [], "entities": [{"text": "universal sentiment classifier", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.6444595356782278}, {"text": "crosslanguage sentiment classification", "start_pos": 192, "end_pos": 230, "type": "TASK", "confidence": 0.6862978140513102}]}, {"text": "The main contributions of our study in this paper are summarized as follows: \u2022 We are the first to build a universal sentiment classifier in multiple languages by learning multilingual sentiment-aware word embeddings, which cannot be addressed by previous researches on cross-language sentiment classification.", "labels": [], "entities": [{"text": "cross-language sentiment classification", "start_pos": 270, "end_pos": 309, "type": "TASK", "confidence": 0.8216736912727356}]}, {"text": "\u2022 We propose pivot-based models to bridge two languages in which there do not exist parallel data, and thus the sentiment information can be transferred to any target language.", "labels": [], "entities": []}, {"text": "\u2022 Evaluation results on five languages demonstrate the efficacy of our proposed pivotbased models and the universal sentiment classifier.", "labels": [], "entities": []}], "datasetContent": [{"text": "Without loss of generality, we evaluate our models in five languages (including three western languages and two Asian languages): English (en), German (de), French (fr), Japanese (jp) and Chinese (en/zh).", "labels": [], "entities": []}, {"text": "Among these languages, the English language is the source language with labeled training data, and we do no use any labeled data in the other languages.", "labels": [], "entities": []}, {"text": "Particularly, we use the multilingual multidomain Amazon review dataset 4 provided by and the NLPC-C2013 dataset . The review dataset provided by We further obtain unlabeled parallel data from Europarl v7 6, which is created manually by translating Japanese Wikipedia articles (related to Kyoto) into English.", "labels": [], "entities": [{"text": "Amazon review dataset", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.6857080161571503}, {"text": "NLPC-C2013 dataset", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.982269674539566}, {"text": "Europarl v7 6", "start_pos": 193, "end_pos": 206, "type": "DATASET", "confidence": 0.942311962445577}]}, {"text": "In addition, CJWikiCorpus (CN-JP) is a Chinese-Japanese Parallel Corpus Constructed from Wikipedia For the the BM model, we use en-de (\u2208 Eu v7) and en-fr (\u2208 Eu v7), en-zh (\u2208 CN-EN), and en-jp (\u2208 JP-EN).", "labels": [], "entities": []}, {"text": "For the PDBM model, we use en-fr (\u2208 UN v1.0) with fr-de (\u2208 Eu v7) to get the case en-fr-de (fr acts as a pivot), en-zh (\u2208 CN-EN) with zh-jp (\u2208 CN-JP) to build the case en-zh-jp (zh acts as a pivot), en-zh (\u2208 CN-EN) with zh-fr (\u2208 UN v1.0) to build en-zh-fr (zh acts as a pivot), and en-fr (\u2208 Eu v7) with zh-fr (\u2208 UN v1.0) to build en-fr-zh (fr acts as a pivot).", "labels": [], "entities": []}, {"text": "Note that any pivot language can be selected if the parallel corpora between the pivot language and other languages can be obtained, but in our experiments, we only use one pivot language in each test case to validate the feasibility of our proposed model.", "labels": [], "entities": []}, {"text": "In practice, a popular language (such as English, Chinese) can be used as the pivot because it can act as a link between two unpopular languages.", "labels": [], "entities": []}, {"text": "While for the UMM model, we use all the corpora used in PDBM to build a universal model.", "labels": [], "entities": []}, {"text": "All the details can be found in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison results (accuracy) on DE (German), FR (French) and JP (Japanese).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994010925292969}, {"text": "DE", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9930638670921326}, {"text": "FR", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9885740280151367}, {"text": "JP", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9468609094619751}]}, {"text": " Table 3: Comparison results (accuracy) on CN (Chinese) and EN (English).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992414712905884}]}]}