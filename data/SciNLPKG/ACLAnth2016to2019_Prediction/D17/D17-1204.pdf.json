{"title": [{"text": "Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data", "labels": [], "entities": []}], "abstractContent": [{"text": "Crowdsourcing offers a convenient means of obtaining labeled data quickly and inexpensively.", "labels": [], "entities": []}, {"text": "However, crowdsourced labels are often noisier than expert-annotated data, making it difficult to aggregate them meaningfully.", "labels": [], "entities": []}, {"text": "We present an aggregation approach that learns a regression model from crowdsourced annotations to predict aggregated labels for instances that have no expert adjudications.", "labels": [], "entities": []}, {"text": "The predicted labels achieve a correlation of 0.594 with expert labels on our data, outperforming the best alternative aggregation method by 11.9%.", "labels": [], "entities": []}, {"text": "Our approach also outperforms the alternatives on third-party datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Publicly-available labeled datasets are scarce for many NLP tasks, and crowdsourcing services such as Amazon Mechanical Turk 1 (AMT) offer researchers a quick, inexpensive means of labeling their data.", "labels": [], "entities": []}, {"text": "However, workers employed by these services are typically unfamiliar with the annotation tasks, and they may have little motivation to perform high-quality work due to factors such as low pay and anonymity.", "labels": [], "entities": []}, {"text": "To further complicate matters, some workers may produce spam or malicious responses.", "labels": [], "entities": []}, {"text": "Thus, it is not uncommon for workers to correlate poorly with one another.", "labels": [], "entities": []}, {"text": "Researchers using crowdsourcing services commonly aggregate the labels they receive via simple strategies such as using the majority or average label.", "labels": [], "entities": []}, {"text": "These methods are best suited for simple, straightforward tasks; with noisier data such as that which maybe obtained for more difficult or subjective tasks, these strategies may produce skewed labels that misrepresent the instance.", "labels": [], "entities": []}], "datasetContent": [{"text": "In addition to evaluating our approach on our data, we evaluate it on three existing crowdsourcing datasets that differ in terms of their size, noise level, and number of annotators.", "labels": [], "entities": []}, {"text": "Details about each dataset are shown in, with additional information below.", "labels": [], "entities": []}, {"text": "Each third-party dataset was randomly divided into 66% training and 34% test.", "labels": [], "entities": []}, {"text": "Affect (Emotion and Valence).", "labels": [], "entities": [{"text": "Affect", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.887030839920044}]}, {"text": "Affect (Emotion) and Affect (Valence) were created for work, and contain emotion (anger, fear, disgust, joy, sadness, and surprise) and valence ratings for 100 headlines from the SemEval affective text annotation task) test set.", "labels": [], "entities": [{"text": "Affect", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.976129412651062}, {"text": "Affect", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9859333634376526}, {"text": "valence", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9782577157020569}, {"text": "SemEval affective text annotation task) test set", "start_pos": 179, "end_pos": 227, "type": "DATASET", "confidence": 0.7963365390896797}]}, {"text": "Annotations indicate the degree of emotion in an emotion-headline pair (Affect (Emotion)) and the overall positive or negative valence of a headline (Affect (Valence)).", "labels": [], "entities": [{"text": "Annotations", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9495419859886169}]}, {"text": "Snow et al. report an average correlation among annotators of 0.669 (emotion) and 0.844 (valence).", "labels": [], "entities": [{"text": "valence", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.9470407962799072}]}, {"text": "WebRel was originally created for the, and its annotations indicate the relevance of web documents retrieved for queries.", "labels": [], "entities": [{"text": "WebRel", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.926211416721344}]}, {"text": "The full dataset contains crowdsourced annotations for 20,232 topic-document pairs; 3277 of those pairs additionally have gold-standard labels.", "labels": [], "entities": []}, {"text": "The number of annotations collected per instance varied.", "labels": [], "entities": []}, {"text": "We used the subset of instances with gold standard labels and at least five annotations, and reconstructed their HIT groupings based on the workers that annotated each instance (we assumed all instances annotated by the exact same set of workers were originally from the same HIT).", "labels": [], "entities": []}, {"text": "Average correlation per HIT was 0.102 (quite noisy).", "labels": [], "entities": [{"text": "correlation", "start_pos": 8, "end_pos": 19, "type": "METRIC", "confidence": 0.9826259613037109}]}, {"text": "We compare our approach to a number of alternative methods, detailed with justifications in Table 4.", "labels": [], "entities": []}, {"text": "The alternatives are popular aggregation techniques that address different potential flaws in non-expert annotation.", "labels": [], "entities": []}, {"text": "We train our approach on the training (and validation, for our dataset) data, and test on the test set.", "labels": [], "entities": []}, {"text": "Since MACE (used for Item-Response) learns from and outputs predictions for the same data, we provide it with the entire dataset (training, validation if available, and test), but report its results for the test instances only.", "labels": [], "entities": []}, {"text": "We provide input to MACE in an ndimensional sparse matrix (1 row per instance and 1 column per each of n distinct annotators in the dataset, with filled values only for the annotators who provided annotations for that instance), since the approach requires knowledge of which annotator provided each annotation to function properly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Comparison with alternative methods.", "labels": [], "entities": []}, {"text": " Table 6: Comparison on third-party datasets.", "labels": [], "entities": []}, {"text": " Table 7: Feature subset performance comparison.", "labels": [], "entities": [{"text": "Feature subset performance comparison", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.5718354657292366}]}]}