{"title": [{"text": "Getting the Most out of AMR Parsing", "labels": [], "entities": [{"text": "AMR Parsing", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.7299972176551819}]}], "abstractContent": [{"text": "This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser: concept identification and alignment.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8916570544242859}, {"text": "AMR parser", "start_pos": 91, "end_pos": 101, "type": "TASK", "confidence": 0.7257871031761169}, {"text": "concept identification", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.7152653634548187}]}, {"text": "We first build a Bidirectional LSTM based concept iden-tifier that is able to incorporate richer contextual information to learn sparse AMR concept labels.", "labels": [], "entities": [{"text": "AMR concept labels", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7441133856773376}]}, {"text": "We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph.", "labels": [], "entities": [{"text": "HMM-based word-to-concept alignment", "start_pos": 18, "end_pos": 53, "type": "TASK", "confidence": 0.6134469509124756}, {"text": "AMR graph", "start_pos": 179, "end_pos": 188, "type": "DATASET", "confidence": 0.7720982432365417}]}, {"text": "We show integrating the two components into an existing AMR parser results in consistently better performance over the state of the art on various datasets.", "labels": [], "entities": [{"text": "AMR parser", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.6737217903137207}]}], "introductionContent": [{"text": "Abstract Meaning Representation (AMR) () is a semantic representation where the meaning of a sentence is encoded as a rooted, directed graph.", "labels": [], "entities": [{"text": "Abstract Meaning Representation (AMR)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8774117628733317}]}, {"text": "A number of AMR parsers have been developed in recent years (, and the initial benefit of AMR parsing has been demonstrated in various downstream applications such as Information Extraction (, Machine Comprehension (, and Language Generation (.", "labels": [], "entities": [{"text": "AMR parsers", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.8861388862133026}, {"text": "AMR parsing", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.906737208366394}, {"text": "Information Extraction", "start_pos": 167, "end_pos": 189, "type": "TASK", "confidence": 0.8568364679813385}, {"text": "Language Generation", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.7533949315547943}]}, {"text": "However, AMR parsing parsing accuracy is still in the high 60%, as measured by the SMatch score , and a significant improvement is needed in order for it to positively impact a larger number of applications.", "labels": [], "entities": [{"text": "AMR parsing parsing", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.9230010509490967}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9746496677398682}, {"text": "SMatch", "start_pos": 83, "end_pos": 89, "type": "TASK", "confidence": 0.9496856927871704}]}, {"text": "Previous research has shown that concept identification is the bottleneck to further improvement of AMR parsing.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7687851190567017}, {"text": "AMR parsing", "start_pos": 100, "end_pos": 111, "type": "TASK", "confidence": 0.9581699669361115}]}, {"text": "For example, JAMR), the first AMR parser, is able to achieve an F-score of 80% (close to the interannotator agreement of 83) if gold concepts are provided.", "labels": [], "entities": [{"text": "JAMR", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.7164003252983093}, {"text": "AMR parser", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.7875391244888306}, {"text": "F-score", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9991344809532166}]}, {"text": "Its parsing accuracy drops sharply to 62.3% when the concepts are identified automatically.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9618489742279053}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9767957925796509}]}, {"text": "One of the challenges in AMR concept identification is data sparsity.", "labels": [], "entities": [{"text": "AMR concept identification", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.9494292338689169}]}, {"text": "A large portion of AMR's concepts are either word lemmas or sense-disambiguated lemmas drawn from Propbank ().", "labels": [], "entities": [{"text": "Propbank", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.959792971611023}]}, {"text": "Since the AMR Bank is relatively small, many of the concept labels in the development or test set only occur a few times or never appear in the training set.", "labels": [], "entities": [{"text": "AMR Bank", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.810163140296936}]}, {"text": "addresses this problem by defining a set of generative actions that maps words in the sentence to their AMR concepts and use a local classifier to learn these actions.", "labels": [], "entities": []}, {"text": "Given such sparse data, making full use of contextual information is crucial to accurate concept labeling.", "labels": [], "entities": [{"text": "concept labeling", "start_pos": 89, "end_pos": 105, "type": "TASK", "confidence": 0.6440668702125549}]}, {"text": "Bidirectional LSTM has shown its success on many sequence labeling tasks since it is able to combine contextual information from both directions and avoid manual feature engineering.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.7504699726899465}]}, {"text": "However, it is non-trivial to formalize concept identification as a sequence labeling problem because of the large concept label set.", "labels": [], "entities": [{"text": "concept identification", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6997714191675186}]}, {"text": "Inspired by, who first apply the Bidirectional LSTM to AMR concept identification by categorizing the large labels into a finite set of predefined types, we propose to address concept identification using Bidirectional LSTM with Factored Concept Labels (FCL), where we re-group the concept label set based on their shared graph structure.", "labels": [], "entities": [{"text": "AMR concept identification", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.9062302708625793}, {"text": "concept identification", "start_pos": 176, "end_pos": 198, "type": "TASK", "confidence": 0.7299317568540573}]}, {"text": "This makes it possible for different concepts to be represented by one common label that captures the shared semantics of these concepts.", "labels": [], "entities": []}, {"text": "Accurate concept identification also crucially depends on the word-to-AMR-concept alignment.", "labels": [], "entities": [{"text": "Accurate concept identification", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6980976363023123}]}, {"text": "Since there is no manual alignment in the AMR annotation, typically either a rule-based or unsupervised aligner is applied to the training data to extract the mapping between words and concepts.", "labels": [], "entities": []}, {"text": "This mapping will then be used as reference data to train concept identification models.", "labels": [], "entities": []}, {"text": "The JAMR aligner greedily aligns a span of words to graph fragments using a set of heuristics.", "labels": [], "entities": []}, {"text": "While it can easily incorporate information from additional linguistic sources such as WordNet, it is not adaptable to other domains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.9724948406219482}]}, {"text": "Unsupervised aligners borrow techniques from Machine Translation and treat sentence-to-AMR alignment as a word alignment problem between a source sentence and its linearized AMR graph () and solve it with IBM word alignment models.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7454928457736969}, {"text": "sentence-to-AMR alignment", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7417071461677551}, {"text": "word alignment", "start_pos": 209, "end_pos": 223, "type": "TASK", "confidence": 0.6666158437728882}]}, {"text": "However, the distortion model in the IBM models is based on the linear distance between source side words while the linear order of the AMR concepts has no linguistic significance, unlike word order in natural language.", "labels": [], "entities": []}, {"text": "A more appropriate sentence-to-AMR alignment model should be one that takes the hierarchical structure of the AMR into account.", "labels": [], "entities": [{"text": "sentence-to-AMR alignment", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.6168735325336456}]}, {"text": "We develop a Hidden Markov Model (HMM)-based sentence-to-AMR alignment method with a novel Graph Distance distortion model to take advantage of the structural information in AMR, and apply a structural constraint to re-score the posterior during decoding time.", "labels": [], "entities": [{"text": "sentence-to-AMR alignment", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.682109922170639}, {"text": "Graph Distance distortion", "start_pos": 91, "end_pos": 116, "type": "METRIC", "confidence": 0.7502764264742533}]}, {"text": "We present experimental results that show incorporating these two improvements to CAMR (, a state-of-the-art transition-based AMR parser, results in consistently better Smatch scores over the state of the art on various datasets.", "labels": [], "entities": []}, {"text": "The rest of paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes related work on AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9599062204360962}]}, {"text": "Section 3 describes our improved LSTM based concept identification model, and Section 4 describes our alignment method.", "labels": [], "entities": [{"text": "LSTM based concept identification", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8034573048353195}, {"text": "alignment", "start_pos": 102, "end_pos": 111, "type": "TASK", "confidence": 0.923194944858551}]}, {"text": "We present experimental results in Section 5, and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first test the performance of our Bidirectional LSTM concept identifier and HMM-based aligner as standalone tasks, where we investigate the effectiveness of each component in AMR parsing.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 178, "end_pos": 189, "type": "TASK", "confidence": 0.9596123099327087}]}, {"text": "Then we report the final results by incorporating both components to CAMR (.", "labels": [], "entities": [{"text": "CAMR", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8714945316314697}]}, {"text": "At the model development stage, we mainly use the dataset LDC2015E86 used in the SemEval Shared Task.", "labels": [], "entities": [{"text": "SemEval Shared Task", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.710352341334025}]}, {"text": "Note that this dataset includes :wiki relations where every named entity concept is linked to its wikipedia entry.", "labels": [], "entities": []}, {"text": "We remove this information in the training data throughout the development of our models.", "labels": [], "entities": []}, {"text": "At the final testing stage, we add wikification using an off-theshelf AMR wikifier () as a postprocessing step.", "labels": [], "entities": []}, {"text": "All AMR parsing results are evaluated using the) scorer.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.7934509515762329}]}, {"text": "In order to isolate the effects of our concept identifier, we first use the official alignments provided by SemEval.", "labels": [], "entities": []}, {"text": "The alignment is generated by the unsupervised aligner described in ().", "labels": [], "entities": []}, {"text": "After getting the alignment table, we generate our FCL label set by filtering out noisy FCL labels that occur fewer than 30 times in the training data.", "labels": [], "entities": []}, {"text": "The remaining FCL labels account for 96% of the MULTICONCEPT cases in the development set.", "labels": [], "entities": []}, {"text": "Adding other labels that include PREDICATE, NON-PREDICATE and CONST gives us 116 canonical labels.", "labels": [], "entities": [{"text": "NON-PREDICATE", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.5655925869941711}, {"text": "CONST", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.5847346782684326}]}, {"text": "UNK label is added to handle the unseen concepts.", "labels": [], "entities": [{"text": "UNK label", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9380569458007812}]}, {"text": "In the Bidirectional LSTM, the hyperparameter settings are as follows: word embedding dimension d wd = 128, NER tag embedding dimension d t = 8, character embedding dimension dc = 50, character level embedding dimension d wch = 50, convolutional layer window size k = 2.", "labels": [], "entities": []}, {"text": "shows the performance on the development set of LDC2015E86, where the precision, recall and F-score are computed by treating other as the negative label and accuracy is calculated using all labels.", "labels": [], "entities": [{"text": "LDC2015E86", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.7607625722885132}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9996236562728882}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9976982474327087}, {"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9984979629516602}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.999427080154419}]}, {"text": "We include accuracy here since correctly predicting words that don't invoke concepts is also important.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9994538426399231}, {"text": "predicting words that don't invoke concepts", "start_pos": 41, "end_pos": 84, "type": "TASK", "confidence": 0.751586799110685}]}, {"text": "We can see that utilizing CNN-based character level embedding yields an improvement of around 2 percentage points absolute for both F-score and accuracy, which indicates that morphological and word shape information is important for concept identification.", "labels": [], "entities": [{"text": "F-score", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9963451027870178}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9985332489013672}, {"text": "concept identification", "start_pos": 233, "end_pos": 255, "type": "TASK", "confidence": 0.7871219217777252}]}, {"text": "To validate the effectiveness of our proposed alignment methods, we first evaluate our for-  ward (English-to-AMR) and reverse (AMR-toEnglish) aligners against the baseline HMM word alignment model, which is the Berkeley aligner toolkit.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.7531005342801412}]}, {"text": "Then we combine the forward and reverse alignment results using competitive thresholding.", "labels": [], "entities": []}, {"text": "We set the threshold \u03b3 to be 0.5 in the following experiments.", "labels": [], "entities": []}, {"text": "To evaluate the alignment quality, we use 200 hand-aligned sentences from () split equally as the development and test sets.", "labels": [], "entities": []}, {"text": "We process the English sentences by removing stop words, following similar procedure as in).", "labels": [], "entities": []}, {"text": "When linearizing AMR graphs, we instead remove all the relations and only keep the concepts.", "labels": [], "entities": [{"text": "linearizing AMR", "start_pos": 5, "end_pos": 20, "type": "TASK", "confidence": 0.6576357185840607}]}, {"text": "For all models, we run 5 iterations of IBM Model 1 and 2 iterations of HMM on the whole dataset.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.9155234694480896}]}, {"text": "From, we can see that our graphdistance based model improves both the precision and recall by a large margin, which indicates the graph distance distortion better fits the Englishto-AMR alignment task.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.999663233757019}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9992656111717224}, {"text": "Englishto-AMR alignment", "start_pos": 172, "end_pos": 195, "type": "TASK", "confidence": 0.611375480890274}]}, {"text": "For the reverse model, although our HMM rescaling model loses accuracy in recall, it is able to improve the precision by around 4 percentage points, which confirms our intuition that the rescoring factor is able to keep reliable alignments and penalize unreliable ones.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9992916584014893}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9948368072509766}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9996246099472046}]}, {"text": "We then combine our forward and reverse alignment result using competitive thresholding.: AMR parsing result (without wikification) with different aligner on development and test of LDC2015E86, where JAMR is the rulebased aligner, ISI is the modified IBM Model 4 aligner Impact on AMR Parsing To investigate our aligner's contribution to AMR parsing, we replace the alignment table generated by the best performing aligner (the forward and reverse combined) in the previous section and re-train CAMR with the predicted concept label features included.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.6864083707332611}, {"text": "AMR Parsing", "start_pos": 281, "end_pos": 292, "type": "TASK", "confidence": 0.8434329628944397}, {"text": "AMR parsing", "start_pos": 338, "end_pos": 349, "type": "TASK", "confidence": 0.9446269273757935}]}, {"text": "From, we can see that the unsupervised aligner (ISI and HMM) generally outperforms the JAMR rule-based aligner, and our improved HMM aligner is more consistent than the ISI aligner (), which is a modified version of IBM Model 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of Bidirectional LSTM with  different input.", "labels": [], "entities": []}, {"text": " Table 2: Performance of AMR parsing with  c pred features without wikification on dev set of  LDC2015E86. The first row is performance of the  baseline parser. The second row adds unknown  concept generation and the last row additionally  extends the baseline parser with c pred features.", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8054615259170532}]}, {"text": " Table 3: Combined HMM alignment result evalu- ation.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.8426340520381927}]}, {"text": " Table 4: AMR parsing result (without wikifica- tion) with different aligner on development and  test of LDC2015E86, where JAMR is the rule- based aligner, ISI is the modified IBM Model 4  aligner", "labels": [], "entities": [{"text": "AMR parsing", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6872721761465073}]}, {"text": " Table 5: Comparison with the winning systems in  SemEval (with wikification) on test and blind test  sets", "labels": [], "entities": []}, {"text": " Table 6: Comparison with the existing parsers on  full test set of LDC2014T12", "labels": [], "entities": [{"text": "LDC2014T12", "start_pos": 68, "end_pos": 78, "type": "DATASET", "confidence": 0.6944471597671509}]}]}