{"title": [{"text": "Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension", "labels": [], "entities": []}], "abstractContent": [{"text": "Reasoning with commonsense knowledge is critical for natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.6868625581264496}]}, {"text": "Traditional methods for commonsense machine comprehension mostly only focus on one specific kind of knowledge, neglecting the fact that commonsense reasoning requires simultaneously considering different kinds of commonsense knowledge.", "labels": [], "entities": [{"text": "commonsense machine comprehension", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.8195220232009888}]}, {"text": "In this paper, we propose a multi-knowledge reasoning method, which can exploit heterogeneous knowledge for commonsense machine comprehension.", "labels": [], "entities": [{"text": "multi-knowledge reasoning", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6827321350574493}]}, {"text": "Specifically, we first mine different kinds of knowledge (including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge) and encode them as inference rules with costs.", "labels": [], "entities": []}, {"text": "Then we propose a multi-knowledge reasoning model, which selects inference rules fora specific reasoning context using attention mechanism, and reasons by summarizing all valid inference rules.", "labels": [], "entities": []}, {"text": "Experiments on RocStories show that our method outperforms traditional models significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Commonsense knowledge is fundamental in artificial intelligence, and has long been a key component in natural language understanding and human-like reasoning.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.6727834939956665}]}, {"text": "For example, to understand the relation between sentences \"Mary walked to a restaurant\" and \"She ordered some foods\", we need commonsense knowledge such as \"Mary is a girl\", \"restaurant sells food\", etc.", "labels": [], "entities": []}, {"text": "The task of understanding natural language with commonsense knowledge is usually referred as commonsense machine comprehension, which has been a hot topic in recent years (.", "labels": [], "entities": []}, {"text": "Recently, RocStories (Mostafazadeh et al., 2016a), a commonsense machine comprehension task, has attached many researchers' attention due to its significant difference from previous machine comprehension tasks.", "labels": [], "entities": []}, {"text": "RocStories focuses on reasoning with implicit commonsense knowledge, rather than matching with explicit information in given contexts.", "labels": [], "entities": []}, {"text": "In this task, a system requires choosing a sentence, namely hypothesis, to complete a given commonsense story, called as premise document.", "labels": [], "entities": []}, {"text": "RocStories proposes a challenging benchmark task for evaluating commonsensebased language understanding.", "labels": [], "entities": [{"text": "commonsensebased language understanding", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.7421395778656006}]}, {"text": "As investigated by, this dataset does not have any boundary cases and thus results in 100% human performance.", "labels": [], "entities": []}, {"text": "Commonsense machine comprehension, however, is an natural ability for human but could be very challenging for computers.", "labels": [], "entities": [{"text": "Commonsense machine comprehension", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6489330530166626}]}, {"text": "In general, any world knowledge whatsoever in the reader's mind can affect the choice of an interpretation (.", "labels": [], "entities": []}, {"text": "That is, a person can learn any heterogeneous commonsense knowledge and make inference of given information based on all knowledge in his mind.", "labels": [], "entities": []}, {"text": "For example, to choose the right hypothesis for the first premise document in, we needs the event narrative knowledge that \"X does a thorough job\" will lead to \"commends X\", rather than \"fire X\".", "labels": [], "entities": []}, {"text": "Besides, people can further confirm their judgement based on the sentimental coherence between \"finish super early\" and \"job well done\".", "labels": [], "entities": []}, {"text": "Furthermore, in the second example, even both hypothesises are consistent with the premise document in both event and sentimental facets, we can still infer the right answer easily using the commonsense knowledge that \"puppy\" is a dog, meanwhile \"kitten\" is a cat.", "labels": [], "entities": []}, {"text": "In recent years, many methods have been proposed for commonsense machine comprehension.", "labels": [], "entities": [{"text": "commonsense machine comprehension", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.876931885878245}]}, {"text": "However, these methods mostly either focus on matching explicit information in given texts, or paid attention to one specific kind of commonsense knowledge, such as event temporal relation and event causality).", "labels": [], "entities": []}, {"text": "As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge.", "labels": [], "entities": []}, {"text": "To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and represent different kinds of implicit knowledge that commonsense machine comprehension needs.", "labels": [], "entities": []}, {"text": "For example, to complete the first example in, we need a system equipped with the event narrative knowledge that \"commends X\" can be inferred from \"X does a thorough job\", as well as the sentiment coherent knowledge that \"insubordination\" and \"finish super early\" are sentimental incoherent.", "labels": [], "entities": []}, {"text": "2) How to reason with various kinds of commonsense knowledge.", "labels": [], "entities": []}, {"text": "As shown above, knowledge that reasoning process needs varies for different contexts.", "labels": [], "entities": [{"text": "reasoning process", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8950090408325195}]}, {"text": "For human-like commonsense machine comprehension, a system should take various kinds of knowledge into consideration, decide what knowledge will be utilized in a specific reasoning contexts, and make the final decision by taking all utilized knowledge into consideration.", "labels": [], "entities": [{"text": "commonsense machine comprehension", "start_pos": 15, "end_pos": 48, "type": "TASK", "confidence": 0.7424446940422058}]}, {"text": "To address the above problems, this paper proposes anew commonsense reasoning approach, which can mine and exploit heterogeneous knowledge for commonsense machine comprehension.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.904392272233963}]}, {"text": "Specifically, we first mine different kinds of knowledge from raw text and relevant knowledge base, including event narrative knowledge, entity semantic knowledge and sentiment coherent knowledge.", "labels": [], "entities": []}, {"text": "These heterogeneous knowledge are encoded into a uniform representation -inference rules between elements under different kinds of relations, with an inference cost for each rule.", "labels": [], "entities": []}, {"text": "Then we design a rule selection model using attention mechanism, modeling which inference rules will be applied in a specific reasoning context.", "labels": [], "entities": [{"text": "rule selection", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7151089608669281}]}, {"text": "Finally, we propose a multi-knowledge reasoning model, which measures the reasoning distance from a premise document to a hypothesis as the expected cost sum of all inference rules applied in the reasoning process.", "labels": [], "entities": []}, {"text": "By modeling and exploiting heterogeneous knowledge during commonsense reasoning, our method can achieve more accurate and more robust performance than traditional methods.", "labels": [], "entities": [{"text": "commonsense reasoning", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.8626514077186584}]}, {"text": "Furthermore, our method is a general framework, which can be extended to incorporate new knowledge easily.", "labels": [], "entities": []}, {"text": "Experiments show that our method achieves a 13.7% accuracy improvement on the standard RocStories dataset, a significant improvement over previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9993581175804138}, {"text": "RocStories dataset", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.945639044046402}]}], "datasetContent": [{"text": "We evaluated our approach on the Test Set Spring 2016 of RocStories, which consists of 1871 commonsense stories, with each story has two candidate story endings.", "labels": [], "entities": [{"text": "Test Set Spring 2016 of RocStories", "start_pos": 33, "end_pos": 67, "type": "DATASET", "confidence": 0.8719276487827301}]}, {"text": "Because stories in the training set of RocStories do not contain wrong hypothesis, and our model has a compact size of parameters, we estimated the parameters of our model using the Validation Set Spring 2016 of RocStories with 1871 commonsense stories.", "labels": [], "entities": []}, {"text": "We mined event narrative knowledge from the Training Set Spring 2016 of RocStories, which consists of 45502 commonsense stories.", "labels": [], "entities": [{"text": "Training Set Spring 2016 of RocStories", "start_pos": 44, "end_pos": 82, "type": "DATASET", "confidence": 0.8740765750408173}]}, {"text": "We performed lemmatisation, part of speech annotation, named entity tagging, and dependency parsing using Stanford CoreNLP toolkits ( ).", "labels": [], "entities": [{"text": "named entity tagging", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.5978766679763794}, {"text": "dependency parsing", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7987688779830933}, {"text": "Stanford CoreNLP toolkits", "start_pos": 106, "end_pos": 131, "type": "DATASET", "confidence": 0.9381203452746073}]}, {"text": "We used the Jan. 30, 2010 English version of Wikipedia and processed it according to the method described by.", "labels": [], "entities": [{"text": "Jan. 30, 2010 English version of Wikipedia", "start_pos": 12, "end_pos": 54, "type": "DATASET", "confidence": 0.7056151926517487}]}, {"text": "We used normalized initialization () to initialize attention parameters in our model.", "labels": [], "entities": []}, {"text": "For calibration parameters, we initialized all w f to 1 and bf to 0.", "labels": [], "entities": []}, {"text": "The model parameters were trained using minibatch stochastic gradient descent algorithm.", "labels": [], "entities": []}, {"text": "As for hyper-parameters, we set the batch size as 32, the learning rate as 1, the dimension of attention hidden layer K as 32, and the smoothing factor \u03b3 as 0.5.", "labels": [], "entities": []}, {"text": "We compared our approach with following three baselines: 1) Narrative Event Chain (Chambers and Jurafsky, 2008), which scores hypothesis using PMI scores between events.", "labels": [], "entities": []}, {"text": "We used a simplified version of the original model by using only verbs as event, ignoring the dependency relation between verbs and their participants.", "labels": [], "entities": []}, {"text": "We found such a simplified version achieved better performance than its original one whose performance was reported in ().", "labels": [], "entities": []}, {"text": "2) Deep Structured Semantic Model (DSS-M), which achieved the best performance on RocStories as reported by.", "labels": [], "entities": []}, {"text": "This model measures the reasoning score between a premise document D and a hypothesis H by calculating the cosine similarity between the overall vector representations of D and H, and do not consider any other task-relevant knowledge.", "labels": [], "entities": []}, {"text": "3) Recurrent Neural Network(RNN) Model proposed by, which transforms all events and their arguments into a sequence and predict next events and arguments using a Long Short-Term Memory network.", "labels": [], "entities": []}, {"text": "We used the average generating probability of all elements in H as the reasoning score, and choose the hypothesis with largest reasoning score as the system answer.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of Inference Rules.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of the performance using s- ingle type of knowledge.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of the performance by re- moving one single type of knowledge.", "labels": [], "entities": []}]}