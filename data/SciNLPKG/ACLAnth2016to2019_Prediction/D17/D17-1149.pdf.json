{"title": [{"text": "Translating Phrases in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.6533133288224539}]}], "abstractContent": [{"text": "Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005).", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.6353891392548879}, {"text": "machine translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7406600266695023}]}, {"text": "However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word byword.", "labels": [], "entities": []}, {"text": "In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (SMT)", "start_pos": 123, "end_pos": 173, "type": "TASK", "confidence": 0.7594703776495797}]}, {"text": "At each decoding step, the phrase memory is first rewritten by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9460284113883972}]}, {"text": "Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory.", "labels": [], "entities": []}, {"text": "If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.76553013920784}, {"text": "phrase translation", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7676595747470856}]}, {"text": "Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does.", "labels": [], "entities": []}, {"text": "Experiment results on the Chinese\u2192English translation show that the proposed model achieves significant improvements over the baseline on various test sets.", "labels": [], "entities": [{"text": "Chinese\u2192English translation", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.46675342321395874}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has been receiving increasing attention due to its impressive * Corresponding author translation performance.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8093996246655782}]}, {"text": "Significantly different from conventional statistical machine translation (SMT) (), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 42, "end_pos": 79, "type": "TASK", "confidence": 0.7832130094369253}]}, {"text": "Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word byword.", "labels": [], "entities": []}, {"text": "The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases.", "labels": [], "entities": [{"text": "translate multi-word phrases", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.8243563771247864}]}, {"text": "Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation () as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.6454774339993795}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7172682285308838}]}, {"text": "Unfortunately current NMT is essentially a word-based or character-based ( translation system where phrases are not considered as translation units.", "labels": [], "entities": []}, {"text": "In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9819138646125793}]}, {"text": "Therefore, a natural question arises: Can we translate phrases in NMT?", "labels": [], "entities": []}, {"text": "Recently, there have been some attempts on multi-word phrase generation in NMT (.", "labels": [], "entities": [{"text": "multi-word phrase generation", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6143335600694021}]}, {"text": "However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework.", "labels": [], "entities": [{"text": "wordby-word generation", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.6925297677516937}]}, {"text": "To explore the phrase generation in NMT beyond the word-byword generation framework, we propose a novel architecture that integrates a phrase-based SMT model into NMT.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8075965642929077}, {"text": "word-byword generation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.714738056063652}, {"text": "SMT", "start_pos": 148, "end_pos": 151, "type": "TASK", "confidence": 0.8402572870254517}]}, {"text": "Specifically, we add an auxiliary phrase memory to store target phrases in symbolic form.", "labels": [], "entities": []}, {"text": "At each decoding step, guided by the decoding information from the NMT decoder, the SMT model dynamically generates relevant target phrase translations and writes them to the memory.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9811912178993225}]}, {"text": "Then the NMT decoder scores phrases in the phrase memory and selects a proper phrase or word with the highest probability.", "labels": [], "entities": []}, {"text": "If the phrase generation is carried out, the NMT decoder generates a multi-word phrase and updates its decoding state by consuming the words in the selected phrase.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7264931201934814}]}, {"text": "Furthermore, in order to enhance the ability of the NMT decoder to effectively select appropriate target phrases, we modify the encoder of NMT to make it fit for exploring structural information of source sentences.", "labels": [], "entities": []}, {"text": "Particularly, we integrate syntactic chunk information into the NMT encoder, to enrich the source-side representation.", "labels": [], "entities": []}, {"text": "We validate our proposed model on the Chinese\u2192English translation task.", "labels": [], "entities": [{"text": "Chinese\u2192English translation task", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.6215562343597412}]}, {"text": "Experiment results show that the proposed model significantly outperforms the conventional attention-based NMT by 1.07 BLEU points on multiple NIST test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9989952445030212}, {"text": "NIST test sets", "start_pos": 143, "end_pos": 157, "type": "DATASET", "confidence": 0.9229935805002848}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly introduces the attentionbased NMT as background knowledge.", "labels": [], "entities": []}, {"text": "Section 3 presents our proposed model which incorporates the phrase memory into the NMT encoder-decoder architecture, as well as the reading and writing procedures of the phrase memory.", "labels": [], "entities": []}, {"text": "Section 4 presents our experiments on the Chinese\u2192English translation task and reports the experiment results.", "labels": [], "entities": [{"text": "Chinese\u2192English translation task", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.6077897727489472}]}, {"text": "Finally we discuss related work in Section 5 and conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluated the effectiveness of our model on the Chinese\u2192English machine translation task.", "labels": [], "entities": [{"text": "Chinese\u2192English machine translation task", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.5932094703118006}]}, {"text": "The training corpora consisted of about 1.25 million sentence pairs  We compared our proposed model with two state-of-the-art systems: * Moses: a state-of-the-art phrase-based SMT system () with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.8900341987609863}, {"text": "minimum error rate training (MERT)", "start_pos": 265, "end_pos": 299, "type": "METRIC", "confidence": 0.8096887724740165}]}, {"text": "* RNNSearch: an in-house implementation of the attention-based NMT system (Bahdanau et al., 2015) with its default settings.", "labels": [], "entities": []}, {"text": "For Moses, we used the full bilingual training data to train the phrase-based SMT model and the target portion of the bilingual training data to train a 4-gram language model using KenLM . We ran Giza++ on the training data in both Chinese-to-English and English-to-Chinese directions and applied the \"grow-diag-final\" refinement rule () to obtain word alignments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.891505241394043}, {"text": "KenLM", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.8209789991378784}, {"text": "word alignments", "start_pos": 348, "end_pos": 363, "type": "TASK", "confidence": 0.6893070489168167}]}, {"text": "The maximum phrase length is set to 7.", "labels": [], "entities": []}, {"text": "For RNNSearch, we generally followed settings in the previous work (.", "labels": [], "entities": []}, {"text": "We only kept a shortlist of the most frequent 30,000 words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages respectively.", "labels": [], "entities": []}, {"text": "We constrained our source and target sequences to have a maximum length of 50 words in the training data.", "labels": [], "entities": []}, {"text": "The size of embedding layer of both sides was set to 620 and the size of hidden layer was set to 1000.", "labels": [], "entities": []}, {"text": "We used a minibatch stochastic gradient descent (SGD) algorithm of size 80 together with Adadelta to train the NMT models.", "labels": [], "entities": []}, {"text": "The decay rates \u03c1 and were set as 0.95 and 10 \u22126 . We clipped the gradient norm to 1.0 (Pascanu et al., 2013).", "labels": [], "entities": []}, {"text": "We also adopted the dropout technique.", "labels": [], "entities": []}, {"text": "Dropout was applied only on the output layer and the dropout rate was set to 0.5.", "labels": [], "entities": []}, {"text": "We used a simple beam search decoder with beam size 10 to find the most likely translation.", "labels": [], "entities": []}, {"text": "For the proposed model, we used a Chinese chunker 6 () to chunk the sourceside Chinese sentences.", "labels": [], "entities": []}, {"text": "13 chunking tags appeared in our chunked sentences and the size of chunking tag embedding was set to 10.", "labels": [], "entities": []}, {"text": "We used the trained phrase-based SMT to translate the source-side chunks.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8798367977142334}]}, {"text": "The top 5 translations according to their translation scores (Equation 10) were kept and among them multi-word phrases were used as phrasal recommendations for each source chunk phrase.", "labels": [], "entities": [{"text": "Equation", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9223520159721375}]}, {"text": "For a source-side chunk phrase, if there exists phrasal recommendations from SMT, the output chunk tag was used as its chunking tag feature as described in Section 3.1.", "labels": [], "entities": [{"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9036121368408203}]}, {"text": "Otherwise, the words in the chunk were treated as general words by being tagged with the default tag.", "labels": [], "entities": []}, {"text": "In the phrase memory, we only keep the top 7 target translations with highest SMT scores at each decoding step.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9431256055831909}]}, {"text": "We used a forward neural network with two hidden layers for both the balancer (Equation 8) and the scoring function (Equation 11).", "labels": [], "entities": []}, {"text": "The numbers of units in the hidden layers were set to 2000 and 500 respectively.", "labels": [], "entities": []}, {"text": "We used a backward RNN encoder to learn the phrase representations of target phrases in the phrase memory.: Percentages of sentences that contain phrases generated by the proposed model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main experiment results on the NIST Chinese-English translation task. BLEU scores in the  table are case insensitive. Moses and RNNSearch are SMT and NMT baseline system respectively. \" \u2020\":  significantly better than RNNSearch (p < 0.05); \" \u2021\": significantly better than RNNSearch (p < 0.01).", "labels": [], "entities": [{"text": "NIST Chinese-English translation task", "start_pos": 41, "end_pos": 78, "type": "TASK", "confidence": 0.7517330646514893}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9990091323852539}]}, {"text": " Table 2: Percentages of sentences that contain  phrases generated by the proposed model.", "labels": [], "entities": []}, {"text": " Table 3: Percentages of phrase categories to the  total number of generated ones. \"All\" denotes all  generated phrases, and \"New\" means new phrases  that cannot be found in translations generated by  the baseline system. \"Total\" is the total number of  generated phrases and \"Correct\" denotes the fully  correct ones.", "labels": [], "entities": [{"text": "Correct", "start_pos": 277, "end_pos": 284, "type": "METRIC", "confidence": 0.9802647829055786}]}, {"text": " Table 4: Percentages of phrases with different  word counts to the total number of generated ones.", "labels": [], "entities": []}, {"text": " Table 5: Additional experiment results on the  translation task to directly measure the im- provement obtained by the phrase generation.  \"+NULL\" denotes that we replace the generated  target phrases with a special symbol /NULL0  in test sets. BLEU scores in the table are case in- sensitive.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7169305235147476}, {"text": "BLEU", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.9989979863166809}]}]}