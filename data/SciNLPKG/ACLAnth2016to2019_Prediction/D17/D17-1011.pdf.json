{"title": [{"text": "Past, Present, Future: A Computational Investigation of the Typology of Tense in 1000 Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "We present SuperPivot, an analysis method for low-resource languages that occur in a superparallel corpus, i.e., in a corpus that contains an order of magnitude more languages than parallel corpora currently in use.", "labels": [], "entities": []}, {"text": "We show that SuperPivot performs well for the crosslingual analysis of the linguistic phenomenon of tense.", "labels": [], "entities": []}, {"text": "We produce analysis results for more than 1000 languages, conducting-to the best of our knowledge-the largest crosslin-gual computational study performed to date.", "labels": [], "entities": []}, {"text": "We extend existing methodology for leveraging parallel corpora for typological analysis by overcoming a limiting assumption of earlier work: We only require that a linguistic feature is overtly marked in a few of thousands of languages as opposed to requiring that it be marked in all languages under investigation.", "labels": [], "entities": [{"text": "typological analysis", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7365008294582367}]}], "introductionContent": [{"text": "Significant linguistic resources such as machinereadable lexicons and part-of-speech (POS) taggers are available for at most a few hundred languages.", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6558245062828064}]}, {"text": "This means that the majority of the languages of the world are low-resource.", "labels": [], "entities": []}, {"text": "Lowresource languages like Fulani are spoken by tens of millions of people and are politically and economically important; e.g., to manage a sudden refugee crisis, NLP tools would be of great benefit.", "labels": [], "entities": []}, {"text": "Even \"small\" languages are important for the preservation of the common heritage of humankind that includes natural remedies and linguistic and cultural diversity that can potentially enrich everybody.", "labels": [], "entities": []}, {"text": "Thus, developing analysis methods for low-resource languages is one of the most important challenges of NLP today.", "labels": [], "entities": []}, {"text": "We address this challenge by proposing anew method for analyzing what we call superparallel corpora, corpora that are by an order of magnitude more parallel than corpora that have been available in NLP to date.", "labels": [], "entities": []}, {"text": "The corpus we work within this paper is the Parallel Bible Corpus (PBC) that consists of translations of the New Testament in 1169 languages.", "labels": [], "entities": [{"text": "Parallel Bible Corpus (PBC)", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.8137706021467844}]}, {"text": "Given that no NLP analysis tools are available for most of these 1169 languages, how can we extract the rich information that is potentially hidden in such superparallel corpora?", "labels": [], "entities": [{"text": "NLP analysis", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.7973962426185608}]}, {"text": "The method we propose is based on two hypotheses.", "labels": [], "entities": []}, {"text": "H1 Existence of overt encoding.", "labels": [], "entities": []}, {"text": "For any important linguistic distinction f that is frequently encoded across languages in the world, there area few languages that encode f overtly on the surface.", "labels": [], "entities": []}, {"text": "H2 Overt-to-overt and overt-tonon-overt projection.", "labels": [], "entities": []}, {"text": "For a language l that encodes f , a projection off from the \"overt languages\" to l in the superparallel corpus will identify the encoding that l uses for f , both in cases in which the encoding that l uses is overt and in cases in which the encoding that l uses is nonovert.", "labels": [], "entities": []}, {"text": "Based on these two hypotheses, our method proceeds in 5 steps.", "labels": [], "entities": []}, {"text": "1. Selection of a linguistic feature.", "labels": [], "entities": [{"text": "Selection", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.9617907404899597}]}, {"text": "We select a linguistic feature f of interest.", "labels": [], "entities": []}, {"text": "Running example: We select past tense as feature f . 2. Heuristic search for head pivot.", "labels": [], "entities": []}, {"text": "Through a heuristic search, we find a language l h that contains ahead pivot p h that is highly correlated with the linguistic feature of interest.", "labels": [], "entities": []}, {"text": "Running example: \"ti\" in Seychelles Creole (CRS).", "labels": [], "entities": []}, {"text": "CRS \"ti\" meets our requirements fora head pivot well as will be verified empirically in \u00a73.", "labels": [], "entities": [{"text": "CRS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7831509709358215}]}, {"text": "First, \"ti\" is a surface marker: it is easily identifable through whitespace tokenization and it is not ambiguous, e.g., it does not have a second meaning apart from being a grammatical marker.", "labels": [], "entities": []}, {"text": "Second, \"ti\" is a good marker for past tense in terms of both \"precision\" and \"recall\".", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9986715316772461}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9953029155731201}]}, {"text": "CRS has mandatory past tense marking (as opposed to languages in which tense marking is facultative) and \"ti\" is highly correlated with the general notion of past tense.", "labels": [], "entities": [{"text": "CRS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8615179657936096}, {"text": "past tense marking", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6879187027613322}]}, {"text": "This does not mean that every clause that a linguist would regard as past tense is marked with \"ti\" in CRS.", "labels": [], "entities": []}, {"text": "For example, some tense-aspect configurations that are similar to English present perfect are marked with \"in\" in CRS, not with \"ti\" (e.g., ENG \"has commanded\" is translated as \"in ordonn\").", "labels": [], "entities": []}, {"text": "Our goal is not to find ahead language and ahead pivot that is a perfect marker off . Such ahead pivot probably does not exist; or, more precisely, linguistic features are not completely rigorously defined.", "labels": [], "entities": []}, {"text": "Ina sense, one of the contributions of this work is that we provide more rigorous definitions of past tense across languages; e.g., \"ti\" in CRS is one such rigorous definition of past tense and it automatically extends (through projection) to 1000 languages in the superparallel corpus.", "labels": [], "entities": []}, {"text": "3. Projection of head pivot to larger pivot set.", "labels": [], "entities": []}, {"text": "Based on an alignment of the head language to the other languages in the superparallel corpus, we project the head pivot to all other languages and search for highly correlated surface markers, i.e., we search for additional pivots in other languages.", "labels": [], "entities": []}, {"text": "This projection to more pivots achieves three goals.", "labels": [], "entities": []}, {"text": "First, it makes the method more robust.", "labels": [], "entities": []}, {"text": "Relying on a single pivot would result in many errors due to the inherent noisiness of linguistic data and because several components we use (e.g., alignment of the languages in the superparallel corpus) are imperfect.", "labels": [], "entities": []}, {"text": "Second, as we discussed above, the head pivot does not necessarily have high \"recall\"; our example was that CRS \"ti\" is not applied to certain clauses that would be translated using present perfect in English.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9977216124534607}]}, {"text": "Thus, moving to a larger pivot set increases recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9982278943061829}]}, {"text": "Third, as we will see below, the pivot set can be leveraged to create a fine-grained map of the linguistic feature.", "labels": [], "entities": []}, {"text": "Consider clauses referring to eventualities in the past that English speakers would render in past progressive, present perfect and simple past tense.", "labels": [], "entities": []}, {"text": "Our hope is that the pivot set will cover these distinctions, i.e., one of the pivots marks past progressive, but not present prefect and simple past, another pivot marks present perfect, but not the other two and soon.", "labels": [], "entities": []}, {"text": "An example of this type of map, including distinctions like progressive and perfective aspect, is given in \u00a74.", "labels": [], "entities": []}, {"text": "Running example: We compute the correlation of \"ti\" with words in other languages and select the 100 most highly correlated words as pivots.", "labels": [], "entities": []}, {"text": "Examples of pivots we find this way are Torres Strait Creole \"bin\" (from English \"been\") and Tzotzil \"laj\".", "labels": [], "entities": []}, {"text": "\"laj\" is a perfective marker, e.g., \"Laj meltzaj -uk\" 'LAJ be-made subj' means \"It's done being built\".", "labels": [], "entities": []}, {"text": "4. Projection of pivot set to all languages.", "labels": [], "entities": []}, {"text": "Now that we have a large pivot set, we project the pivots to all other languages to search for linguistic devices that express the linguistic feature f . Up to this point, we have made the assumption that it is easy to segment text in all languages into pieces of a size that is not too small (individual characters of the Latin alphabet would be too small) and not too large (entire sentences as tokens would be too large).", "labels": [], "entities": []}, {"text": "Segmentation on standard delimiters is a good approximation for the majority of languages -but not for all: it undersegments some (e.g., the polysynthetic language Inuit) and oversegments others (e.g., languages that use punctuation marks as regular characters).", "labels": [], "entities": []}, {"text": "For this reason, we do not employ tokenization in this step.", "labels": [], "entities": []}, {"text": "Rather we search for character ngrams (2 \u2264 n \u2264 6) to find linguistic devices that express f . This implementation of the search procedure is a limitation -there are many linguistic devices that cannot be found using it, e.g., templates in templatic morphology.", "labels": [], "entities": []}, {"text": "We leave addressing this for future work ( \u00a77).", "labels": [], "entities": []}, {"text": "Running example: We find \"-ed\" for English and \"-te\" for German as surface features that are highly correlated with the 100 past tense pivots.", "labels": [], "entities": []}, {"text": "The result of the previous steps is a superparallel corpus that is richly annotated with information about linguistic feature f . This structure can be exploited for the analysis of a single language l i that maybe the focus of a linguistic investigation.", "labels": [], "entities": []}, {"text": "Starting with the character n-grams that were found in the step \"projection of pivot set to all languages\", we can explore their use and function, e.g, for the mined n-gram \"-ed\" in English (assuming English is the language l i and it is unfamiliar to us).", "labels": [], "entities": []}, {"text": "Many of the other 1000 languages provide annotations of linguistic feature f for l i : both the languages that are part of the pivot set (e.g., Tzotzil \"laj\") and the mined ngrams in other languages that we may have some knowledge about (e.g., \"-te\" in German).", "labels": [], "entities": []}, {"text": "We can also use the structure we have generated for typological analysis across languages following the work of Michael Cysouw (), \u00a75).", "labels": [], "entities": [{"text": "typological analysis", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.6905845701694489}]}, {"text": "Our method is an advancement computationally over Cysouw's work because our method scales to thousands of languages as we demonstrate below.", "labels": [], "entities": []}, {"text": "Running example: We sketch the type of analysis that our new method makes possible in \u00a74.", "labels": [], "entities": []}, {"text": "The above steps \"1.", "labels": [], "entities": []}, {"text": "heuristic search for head pivot\" and \"2.", "labels": [], "entities": []}, {"text": "projection of head pivot to larger pivot set\" are based on H1: we assume the existence of overt coding in a subset of languages.", "labels": [], "entities": []}, {"text": "The above steps \"2.", "labels": [], "entities": []}, {"text": "projection of head pivot to larger pivot set\" and \"3.", "labels": [], "entities": []}, {"text": "projection of pivot set to all languages\" are based on H2: we assume that overt-to-overt and overt-to-non-overt projection is possible.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we will refer to the method that consists of steps 1 to 5 as SuperPivot: \"linguistic analysis of SUPERparallel corpora using surface PIVOTs\".", "labels": [], "entities": []}, {"text": "(i) Our basic hypotheses are H1 and H2.", "labels": [], "entities": []}, {"text": "(H1) For an important linguistic feature, there exist a few languages that mark it overtly and easily recognizably.", "labels": [], "entities": []}, {"text": "(H2) It is possible to project overt markers to overt and non-overt markers in other languages.", "labels": [], "entities": []}, {"text": "Based on these two hypotheses we design SuperPivot, anew method for analyzing highly parallel corpora, and show that it performs well for the crosslingual analysis of the linguistic phenomenon of tense.", "labels": [], "entities": []}, {"text": "(ii) Given a superparallel corpus, SuperPivot can be used for the analysis of any low-resource language represented in that corpus.", "labels": [], "entities": []}, {"text": "In the supplementary material, we present results of our analysis for three tenses (past, present, future) for 1163 languages.", "labels": [], "entities": []}, {"text": "An evaluation of accuracy is presented in.2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995200634002686}]}, {"text": "(iii) We extend Michael Cysouw's method of typological analysis using parallel corpora by overcoming several limiting factors.", "labels": [], "entities": [{"text": "typological analysis", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.7064753472805023}]}, {"text": "The most important is that Cysouw's method is only applicable if markers of the relevant linguistic feature are recognizable on the surface in all languages.", "labels": [], "entities": []}, {"text": "In contrast, we only assume that markers of the relevant linguistic feature are recognizable on the surface in a small number of languages.", "labels": [], "entities": []}, {"text": "1. Selection of a linguistic feature.", "labels": [], "entities": [{"text": "Selection", "start_pos": 3, "end_pos": 12, "type": "TASK", "confidence": 0.9617907404899597}]}, {"text": "The linguistic feature of interest f is selected by the person who performs a SuperPivot analysis, i.e., by a linguist, NLP researcher or data scientist.", "labels": [], "entities": []}, {"text": "Henceforth, we will refer to this person as the linguist.", "labels": [], "entities": []}, {"text": "In this paper, f \u2208 F = {past, present, future}.", "labels": [], "entities": []}, {"text": "2. Heuristic search for head pivot.", "labels": [], "entities": []}, {"text": "There are several ways for finding the head language and the head pivot.", "labels": [], "entities": []}, {"text": "Perhaps the linguist knows a language that has a good head pivot.", "labels": [], "entities": []}, {"text": "Or she is a trained typologist and can find the head pivot by consulting the typological literature.", "labels": [], "entities": [{"text": "head pivot", "start_pos": 48, "end_pos": 58, "type": "TASK", "confidence": 0.7852311730384827}]}, {"text": "In this paper, we use our knowledge of English and an alignment from English to all other languages to find head pivots.", "labels": [], "entities": []}, {"text": "(See below for details on alignment.)", "labels": [], "entities": [{"text": "alignment", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.8918969035148621}]}, {"text": "We define a \"query\" in English and search for words that are highly correlated to the query in other languages.", "labels": [], "entities": []}, {"text": "For future tense, the query is simply the word \"will\", so we search for words in other languages that are highly correlated with \"will\".", "labels": [], "entities": []}, {"text": "For present tense, the query is the union of \"is\", \"are\" and \"am\".", "labels": [], "entities": []}, {"text": "So we search for words in other languages that are highly correlated with the \"merger\" of these three words.", "labels": [], "entities": []}, {"text": "For past tense, we POS tag the English part of PBC and merge all words tagged as past tense into one past tense word.", "labels": [], "entities": [{"text": "POS tag the English part of PBC", "start_pos": 19, "end_pos": 50, "type": "DATASET", "confidence": 0.5505088248423168}]}, {"text": "We then search for words in other languages that are highly correlated with this artificial past tense word.", "labels": [], "entities": []}, {"text": "As an additional constraint, we do not select the most highly correlated word as the head pivot, but the most highly correlated word in a Creole language.", "labels": [], "entities": []}, {"text": "Our rationale is that Creole languages are more regular than other languages because they are young and have not accumulated \"historical baggage\" that may make computational analysis more difficult.", "labels": [], "entities": []}, {"text": "lists the three head pivots for F . 3. Projection of head pivot to larger pivot set.", "labels": [], "entities": [{"text": "F . 3.", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7876998782157898}]}, {"text": "We first use fast align () to align the head language to all other languages in the corpus.", "labels": [], "entities": []}, {"text": "This alignment is on the word level.", "labels": [], "entities": []}, {"text": "We compute a score for each word in each language based on the number of times it is aligned to the head pivot, the number of times it is aligned to another word and the total frequencies of head pivot and word.", "labels": [], "entities": []}, {"text": "We use \u03c7 2 as the score throughout this paper.", "labels": [], "entities": []}, {"text": "Finally, we select the k words as pivots that have the highest association score with the head pivot.", "labels": [], "entities": [{"text": "association score", "start_pos": 63, "end_pos": 80, "type": "METRIC", "confidence": 0.9462504684925079}]}, {"text": "We impose the constraint that we only select one pivot per language.", "labels": [], "entities": []}, {"text": "So as we go down the list, we skip pivots from languages for which we already have found a pivot.", "labels": [], "entities": []}, {"text": "We set k = 100 in this paper.", "labels": [], "entities": []}, {"text": "gives the top 10 pivots.", "labels": [], "entities": []}, {"text": "4. Projection of pivot set to all languages.", "labels": [], "entities": []}, {"text": "As discussed above, the process so far has been based on tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9633277654647827}]}, {"text": "To be able to find markers that cannot be easily detected on the surface (like \"-ed\" in English), we identify non-tokenizationbased character n-gram features in step 4.", "labels": [], "entities": []}, {"text": "The immediate challenge is that without tokens, we have no alignment between the languages anymore.", "labels": [], "entities": []}, {"text": "We could simply assume that the occurrence of a pivot has scope over the entire verse.", "labels": [], "entities": []}, {"text": "But this is clearly inadequate, e.g., for the sentence \"I arrived yesterday, I'm staying today, and I will leave tomorrow\", it is incorrect to say that it is marked as past tense (or future tense) in its entirety.", "labels": [], "entities": []}, {"text": "Fortunately, the verses in the New Testament mostly have a simple structure that limits the variation in where a particular piece of content occurs in the verse.", "labels": [], "entities": []}, {"text": "We therefore make the assumption that a particular relative position in language l 1 (e.g., the character at relative position 0.62) is aligned with the same relative position in l 2 (i.e., the character at relative position 0.62).", "labels": [], "entities": []}, {"text": "This is likely to work fora simple example like \"I arrived yesterday, I'm staying today, and I will leave tomorrow\" across languages.", "labels": [], "entities": []}, {"text": "In our analysis of errors, we found many cases where this assumption breaks down.", "labels": [], "entities": []}, {"text": "A wellknown problematic phenomenon for our method is the difference between, say, VSO and SOV languages: the first class puts the verb at the beginning, the second at the end.", "labels": [], "entities": []}, {"text": "However, keep in mind that we accumulate evidence over k = 100 pivots and then compute aggregate statistics over the entire corpus.", "labels": [], "entities": []}, {"text": "As our evaluation below shows, the \"linear alignment\" assumption does not seem to do much harm given the general robustness of our method.", "labels": [], "entities": []}, {"text": "One design element that increases robustness is that we find the two positions in each verse that are most highly (resp.", "labels": [], "entities": []}, {"text": "least highly) correlated with the linguistic feature f . Specifically, we compute the relative position x of each pivot that occurs in the verse and apply a Gaussian filter (\u03c3 = 6 where the unit of length is the character), i.e., we set p(x) \u2248 0.066 (0.066 is the density of a Gaussian with \u03c3 = 6 at x = 0) and center a bell curve around x.", "labels": [], "entities": []}, {"text": "The total score fora position x is then the sum of the filter values at x summed overall occurring pivots.", "labels": [], "entities": []}, {"text": "Finally, we select the positions x min and x max with lowest and highest values for each verse.", "labels": [], "entities": []}, {"text": "\u03c7 2 is then computed based on the number of times a character n-gram occurs in a window of size w around x max (positive count) and in a window of size w around x min (negative count).", "labels": [], "entities": []}, {"text": "Verses in which no pivot occurs are used for the negative count in their entirety.", "labels": [], "entities": []}, {"text": "The top-ranked character ngrams are then output for analysis by the linguist.", "labels": [], "entities": []}, {"text": "We set w = 20. 5. Linguistic analysis.", "labels": [], "entities": [{"text": "Linguistic analysis", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7957730293273926}]}, {"text": "We now have created a structure that contains rich information about the linguistic feature: for each verse we have relative positions of pivots that can be projected across languages.", "labels": [], "entities": []}, {"text": "We also have maximum positions within averse that allow us to pinpoint the most likely place in the vicinity of which linguistic feature f is marked in all languages.", "labels": [], "entities": []}, {"text": "This structure can be used for the analysis of individual low-resource languages as well as for typological analysis.", "labels": [], "entities": [{"text": "typological analysis", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6991619020700455}]}, {"text": "We will give an example of such an analysis in \u00a74.", "labels": [], "entities": []}], "datasetContent": [{"text": "Selection of a linguistic feature.", "labels": [], "entities": []}, {"text": "We conduct three experiments for the linguistic features past tense, present tense and future tense.", "labels": [], "entities": []}, {"text": "2. Heuristic search for head pivot.", "labels": [], "entities": []}, {"text": "We use the queries described in \u00a72 for finding the following three head pivots.", "labels": [], "entities": []}, {"text": "(i) Past tense head pivot: \"ti\" in Seychellois Creole (CRS).", "labels": [], "entities": []}, {"text": "(ii) Present tense head pivot: \"ta\" in Papiamentu (PAP).", "labels": [], "entities": [{"text": "Papiamentu (PAP)", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9177929908037186}]}, {"text": "(iii) Future tense head pivot: \"bai\" in Tok Pisin (TPI).", "labels": [], "entities": [{"text": "Tok Pisin (TPI)", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.7757532596588135}]}, {"text": "3. Projection of head pivot to larger pivot set.", "labels": [], "entities": []}, {"text": "Using the method described in \u00a72, we project each head pivot to a set of k = 100 pivots.", "labels": [], "entities": []}, {"text": "gives the top 10 pivots for each tense.", "labels": [], "entities": []}, {"text": "4. Projection of pivot set to all languages.", "labels": [], "entities": []}, {"text": "Using the method described in \u00a72, we compute highly correlated character n-gram features, 2 \u2264 n \u2264 6, for all 1163 languages.", "labels": [], "entities": []}, {"text": "See \u00a74 for the last step of SuperPivot: 5.", "labels": [], "entities": []}, {"text": "We rank n-gram features and retain the top 10, for each linguistic feature, for each language and for each n-gram size.", "labels": [], "entities": []}, {"text": "Thus, in total, we extract 1556 \u00d7 5 \u00d7 10 n-grams..2 shows Mean Reciprocal Rank (MRR) for 10 languages.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 58, "end_pos": 84, "type": "METRIC", "confidence": 0.965636690457662}]}, {"text": "The rank fora particular ranking of n-grams is the first n-gram that is highly correlated with the relevant tense; e.g., character subsequences of the name \"Paulus\" are evaluated as incorrect, the subsequence \"-ed\" in English as correct for past.", "labels": [], "entities": []}, {"text": "MRR is averaged overall n-gram sizes, 2 \u2264 n \u2264 6.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.4898419976234436}]}, {"text": "Chinese has consistent tense marking only for future, so results are poor.", "labels": [], "entities": []}, {"text": "Russian and Polish perform poorly because their central grammatical category is aspect, not tense.", "labels": [], "entities": []}, {"text": "The poor performance on Arabic is due to the limits of character n-gram features fora \"templatic\" language.", "labels": [], "entities": []}, {"text": "During this evaluation, we noticed a surprising amount of variation within translations of one language; e.g., top-ranked n-grams for some German translations include names like \"Paulus\".", "labels": [], "entities": []}, {"text": "We suspect that for literal translations, linear alignment ( \u00a72) yields good n-grams.", "labels": [], "entities": [{"text": "literal translations", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.746735006570816}, {"text": "linear alignment", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6054214239120483}]}, {"text": "But many translations are free, e.g., they change the sequence of clauses.", "labels": [], "entities": []}, {"text": "A reviewer points out that simple baselines maybe available if all we want to do is compute features highly associated with past tense as evaluated in.2.", "labels": [], "entities": []}, {"text": "As one such baseline, they suggested to first perform a word alignment with the head pivot and then search for highly associated features in the words that were aligned with the head pivot.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.6732247620820999}]}, {"text": "We implemented this baseline and measured its performance.", "labels": [], "entities": []}, {"text": "Indeed, the results were roughly comparable to the more complex method that we evaluate in.2.", "labels": [], "entities": []}, {"text": "However, our evaluation was not designed to be a direct evaluation of our method, but only meant as a relatively easy way of getting a quantitative sense of the accuracy of our results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9973342418670654}]}, {"text": "The core result of our method is a corpus in which each language annotates each other language.", "labels": [], "entities": []}, {"text": "This is only meaningful on the token or context level, not on the word level.", "labels": [], "entities": []}, {"text": "For example, recognizing \"-ed\" as a possible past tense marker in English and applying it uniformly throughout the corpus would result in the incorrect annotation of the adjective \"red\" as a past tense form.", "labels": [], "entities": []}, {"text": "In our proposed method, this will not happen since the annotation proceeds from reliable pivots to less reliable features, not the other way round.", "labels": [], "entities": []}, {"text": "Nevertheless, we agree with the reviewer that we do not make enough use of \"type-level\" features in our method (type-level features of non-pivot languages) and this is something we plan to address in the future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top ten past, present, and future tense pivots extracted from 1163 languages. HPs = head pivots.  C. = Creole", "labels": [], "entities": []}, {"text": " Table 2: MRR results for step 4. See text for de- tails.", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6736304759979248}]}]}