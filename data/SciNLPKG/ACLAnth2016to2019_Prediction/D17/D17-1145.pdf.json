{"title": [{"text": "Neural Lattice-to-Sequence Models for Uncertain Inputs", "labels": [], "entities": []}], "abstractContent": [{"text": "The input to a neural sequence-to-sequence model is often determined by an upstream system, e.g. a word seg-menter, part of speech tagger, or speech recognizer.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.6675031632184982}]}, {"text": "These upstream models are potentially error-prone.", "labels": [], "entities": []}, {"text": "Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form.", "labels": [], "entities": []}, {"text": "In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model.", "labels": [], "entities": []}, {"text": "We integrate lattice posterior scores into this architecture by extending the TreeLSTM's child-sum and forget gates and introducing a bias term into the attention mechanism.", "labels": [], "entities": [{"text": "TreeLSTM", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9429724812507629}]}, {"text": "We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.", "labels": [], "entities": [{"text": "speech translation lattices", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7782712380091349}]}], "introductionContent": [{"text": "In many natural language processing tasks, we will require a down-stream system to consume the input of an up-stream system, such as word segmenters, part of speech taggers, or automatic speech recognizers.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 133, "end_pos": 148, "type": "TASK", "confidence": 0.7270834445953369}, {"text": "speech taggers", "start_pos": 158, "end_pos": 172, "type": "TASK", "confidence": 0.7256180942058563}]}, {"text": "Among these, one of the most prototypical and widely used examples is speech translation, where a down-stream translation system must consume the output of an up-stream automatic speech recognition (ASR) system.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.802670419216156}, {"text": "up-stream automatic speech recognition (ASR)", "start_pos": 159, "end_pos": 203, "type": "TASK", "confidence": 0.7871113419532776}]}, {"text": "Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g.) as an effective tool to pass on uncertainties from a previous step (.", "labels": [], "entities": [{"text": "phrase-based or tree-based statistical machine translation", "start_pos": 33, "end_pos": 91, "type": "TASK", "confidence": 0.6371234754721323}]}, {"text": "Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output.", "labels": [], "entities": []}, {"text": "Examples include translating lattice representations of ASR output, multiple word segmentations, and morphological alternatives (.", "labels": [], "entities": [{"text": "ASR output", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.7538001835346222}]}, {"text": "Recently, neural sequence-to-sequence (seq2seq) models) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling.", "labels": [], "entities": []}, {"text": "These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works.", "labels": [], "entities": []}, {"text": "As a remedy, proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model.", "labels": [], "entities": []}, {"text": "This is achieved by extending the encoder's Gated Recurrent Units (GRUs) () to be conditioned on multiple predecessor paths.", "labels": [], "entities": []}, {"text": "The authors demonstrate improvements in Chineseto-English translation by translating lattices that combine the output of multiple word segmenters, rather than a single segmented sequence.", "labels": [], "entities": [{"text": "Chineseto-English translation", "start_pos": 40, "end_pos": 69, "type": "TASK", "confidence": 0.6018787324428558}]}, {"text": "However, this model does not address one aspect of lattices that we argue is critical to obtaining good translation results: their ability to encode the certainty or uncertainty of the paths through the use of posterior scores.", "labels": [], "entities": [{"text": "certainty", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9587898850440979}]}, {"text": "Specifically, we postulate that these scores are essential for tasks that require handling lattices with a considerable amount of erroneous content, such as those produced by ASR systems.", "labels": [], "entities": [{"text": "ASR", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9593206644058228}]}, {"text": "In this paper, we propose a lattice-tosequence model that accounts for this uncertainty.", "labels": [], "entities": []}, {"text": "Specifically, our contributions are as follows: \u2022 We employ the popular child-sum to derive a lattice encoder that replaces the sequential encoder in an attentional encoder-decoder model.", "labels": [], "entities": []}, {"text": "We show empirically that this approach yields only minor improvements compared to a baseline fine-tuned on sequential ASR outputs.", "labels": [], "entities": [{"text": "ASR", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.8816392421722412}]}, {"text": "This finding stands in contrast to the positive results by, and by on a lattice classification task, and suggests higher learning complexity of our speech translation task.", "labels": [], "entities": [{"text": "speech translation task", "start_pos": 148, "end_pos": 171, "type": "TASK", "confidence": 0.7895613809426626}]}, {"text": "\u2022 We hypothesize that lattice scores are crucial in aiding training and inference, and propose several techniques for integrating lattice scores into the model: (1) We compute weighted childsums, 1 where hidden units in the lattice encoder are conditioned on their predecessor hidden units such that predecessors with low probability are less influential on the current hidden state.", "labels": [], "entities": []}, {"text": "(2) We bias the TreeLSTM's forget gates for each incoming connection toward being more forgetful for predecessors with low probability, such that their cell states become relatively less influential.", "labels": [], "entities": []}, {"text": "We bias the attention mechanism to put more focus on source embeddings belonging to nodes with high lattice scores.", "labels": [], "entities": []}, {"text": "We demonstrate empirically that the third proposed technique is particularly effective and produces strong gains over the baseline.", "labels": [], "entities": []}, {"text": "According to our knowledge, this is the first attempt of integrating lattice scores already at the training stage of a machine translation model.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7460857927799225}]}, {"text": "\u2022 We exploit the fact that our lattice encoder is a strict generalization of a sequential encoder by pre-training on sequential data, obtaining faster and better training convergence on large corpora of parallel sequential data.", "labels": [], "entities": []}, {"text": "We conduct experiments on the Fisher and Callhome Spanish-English Speech Translation Corpus () and report improvements of 1.4 BLEU points on Fisher and 0.8 BLEU points on Callhome, compared to a strong baseline optimized for translating 1-best ASR outputs.", "labels": [], "entities": [{"text": "Callhome Spanish-English Speech Translation Corpus", "start_pos": 41, "end_pos": 91, "type": "DATASET", "confidence": 0.8056402564048767}, {"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9990038275718689}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9981083869934082}, {"text": "ASR outputs", "start_pos": 244, "end_pos": 255, "type": "TASK", "confidence": 0.7494392693042755}]}, {"text": "We find that the proposed integration of lattice scores is crucial for achieving these improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "Next, we conduct an ablation study to assess the impact of the three proposed extensions for integrating lattice scores ( \u00a74.2).", "labels": [], "entities": []}, {"text": "We train models with different peakiness coefficients S, either ignoring lattices scores by fixing S=0, using lattice scores as-is by fixing S=1, or optimizing S during training.", "labels": [], "entities": []}, {"text": "Overall, joint training of S gives similar results as fixing S=1, but both clearly outperform fixing S=0.", "labels": [], "entities": []}, {"text": "Removing confidences (setting S=0) in one place at a time reveals that the attention mechanism is clearly the most important point of integration, while gains from the integration into child-sum and forget gate are smaller and not always consistent.", "labels": [], "entities": []}, {"text": "We also analyzed what peakiness values were actually learned.", "labels": [], "entities": []}, {"text": "We found that all 3 models that we trained for the averaging purposes converged to S a =0.62.", "labels": [], "entities": []}, {"text": "S hand S f had per-vector means between 0.92 and 1.0, at standard deviations between 0.02 and 0.04.", "labels": [], "entities": []}, {"text": "We conclude that while the peakiness coefficients were not particularly helpful in our experiments, stable convergence behavior makes them safe to use, and they might be helpful on other data sets that may contain lattice scores of higher or lower reliability.", "labels": [], "entities": []}, {"text": "In this experiment, we test a situation in which we have a reasonable amount of sequential data available for pre-training, but only a limited amount of lattice training data for the fine-tuning step.", "labels": [], "entities": []}, {"text": "This maybe a more realistic situation, because speech translation corpora are scarce.", "labels": [], "entities": [{"text": "speech translation corpora", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7592655420303345}]}, {"text": "To investigate in this scenario, we again pre-train our models on Fisher/Train, but then fine-tune them on the 9 times smaller Callhome/Train portion of the corpus.", "labels": [], "entities": [{"text": "Fisher/Train", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9309024016062418}, {"text": "Callhome/Train portion of the corpus", "start_pos": 127, "end_pos": 163, "type": "DATASET", "confidence": 0.8594033207212176}]}, {"text": "We fine-tune for 10 epochs, all other settings are as before.", "labels": [], "entities": []}, {"text": "We use Callhome/Evltest for testing.: BLEU scores (4 references) for differently configured peakiness coefficients S a , S h , S f . 0/1 means fixing to that value, * indicates optimization during training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9994463324546814}]}, {"text": "Statistically significant improvement over 1-best/R+1 is in bold.", "labels": [], "entities": [{"text": "1-best/R+1", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.8092524886131287}]}, {"text": "0.8 BLEU points, which in turn beats the pretrained system (1-best/R) by 1.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9994827508926392}, {"text": "1-best/R)", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.6919691413640976}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9952303767204285}]}, {"text": "Including the lattice scores is clearly beneficial, although lattices without scores also improve over 1-best inputs in this experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Development data statistics. Average sen- tence length is between 11.8 and 13.1.", "labels": [], "entities": [{"text": "Average sen- tence length", "start_pos": 39, "end_pos": 64, "type": "METRIC", "confidence": 0.7850383400917054}]}, {"text": " Table 3: BLEU scores (4 references) and perplexities (in brackets). Models are pre-trained only (R),  fine-tuned on either 1-best outputs (R+1), lattices without scores (R+L), or lattices with scores (R+L+S).  Statistically significant improvement (paired bootstrap resampling, p < 0.05) over 1-best/R+1 is in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979081153869629}]}, {"text": " Table 4: BLEU scores (4 references) for differ- ently configured peakiness coefficients S a , S h , S f .  0/1 means fixing to that value, * indicates opti- mization during training. Statistically significant  improvement over 1-best/R+1 is in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993122816085815}, {"text": "1-best/R+1", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.7913790345191956}]}, {"text": " Table 5: BLEU scores on Callhome/Evltest  (1 reference). All models are pre-trained on  Fisher/Train references (R), and potentially fine- tuned on Callhome/Train. The best result using  1-best or lattice inputs is in bold. Statistically sig- nificant improvement over 1-best/R+1 is in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983435869216919}, {"text": "Callhome/Evltest", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.8568090200424194}]}]}