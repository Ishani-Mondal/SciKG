{"title": [{"text": "Distinguishing Japanese Non-standard Usages from Standard Ones", "labels": [], "entities": [{"text": "Distinguishing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9569117426872253}]}], "abstractContent": [{"text": "We focus on non-standard usages of common words on social media.", "labels": [], "entities": []}, {"text": "In the context of social media, words sometimes have other usages that are totally different from their original.", "labels": [], "entities": []}, {"text": "In this study, we attempt to distinguish non-standard usages on social media from standard ones in an unsuper-vised manner.", "labels": [], "entities": []}, {"text": "Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context.", "labels": [], "entities": []}, {"text": "For this purpose, we use context em-beddings derived from word embeddings.", "labels": [], "entities": []}, {"text": "Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use.", "labels": [], "entities": []}], "introductionContent": [{"text": "On social media such as Twitter, we often find posts that are difficult to interpret without prior knowledge on non-standard usage of words.", "labels": [], "entities": []}, {"text": "For example, consider the following Japanese sentence 1 : mackerel-POSS load-NOM increase-PRS \"The load on a mackerel increases\", which does not make sense given the standard usages for the words in the sentence.", "labels": [], "entities": []}, {"text": "But here, mackerel is a non-standard usage that means computer server.", "labels": [], "entities": []}, {"text": "The entire sentence should be interpreted as \"The load on the computer server increases\".", "labels": [], "entities": []}, {"text": "The Japanese word \" (saba)\" (i.e., mackerel) is used to mean computer server by Japanese computer geeks because saba happens to have a pronunciation that is similar to s\u00af ab\u00af a (i.e., computer server).", "labels": [], "entities": []}, {"text": "When a word is used in a meaning that is different from its dictionary meaning, we call such a usage non-standard.", "labels": [], "entities": []}, {"text": "Non-standard usages can be found in many languages.", "labels": [], "entities": []}, {"text": "For example, the word \"catfish\" means a ray-finned fish as in a standard dictionary, but on social media, it can mean a person who pretends to be someone else in order to create a fake identity.", "labels": [], "entities": []}, {"text": "Such non-standard usages would bean obstacle to a variety of language processings including machine translation; Google Translate cannot correctly interpret examples such as this.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7863450646400452}]}, {"text": "Humans, however, would be able to notice non-standard usages from the inconsistency between the expected word meaning and the context.", "labels": [], "entities": []}, {"text": "The purpose of this work is to develop a method for distinguishing non-standard usages of Japanese words from standard ones.", "labels": [], "entities": [{"text": "distinguishing non-standard usages of Japanese words", "start_pos": 52, "end_pos": 104, "type": "TASK", "confidence": 0.8178238769372305}]}, {"text": "Since it is impractical to construct a large labeled data set for each word, we focus on unsupervised approaches.", "labels": [], "entities": []}, {"text": "The main idea in our method is that the difference between the target word's embedding learned from a general corpus and the embedding predicted from the given context would be a good indicator of the degree of non-standardness.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our model has three characteristics: (input and output) word embeddings, decaying weights, and a general balanced corpus.", "labels": [], "entities": []}, {"text": "We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones.", "labels": [], "entities": []}, {"text": "First, we verified the effectiveness of the input and output embeddings.", "labels": [], "entities": []}, {"text": "We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between , which is a similar framework to that of previous work.", "labels": [], "entities": []}, {"text": "We then tested a method based on the positive pointwise mutual information (PPMI) (  Next, we replaced the decaying weights \u03b1 with uniform weights to examine the impact of decaying weights.", "labels": [], "entities": [{"text": "positive pointwise mutual information (PPMI)", "start_pos": 37, "end_pos": 81, "type": "METRIC", "confidence": 0.6494555728776115}]}, {"text": "Finally, we conducted experiments with different training corpora to examine the impact of the balanced corpus.", "labels": [], "entities": []}, {"text": "We used four corpora as training data for obtaining word embeddings.", "labels": [], "entities": []}, {"text": "These corpora are described in.", "labels": [], "entities": []}, {"text": "In the training of the word embeddings, we set the window size to 5, and the dimensions of the word embeddings to 300.", "labels": [], "entities": []}, {"text": "We regarded the words with frequency counts of 5 or less in the training data as unknown words and replaced those words with \"<unk>\".", "labels": [], "entities": []}, {"text": "We used gensim as an implementation of SGNS, where we set the number of negative samples to 10.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.7565428018569946}]}, {"text": "We used the code provided by as the SVD implementation.", "labels": [], "entities": []}, {"text": "For the evaluations, we ranked test instances in ascending order of standardness score and evaluated the ranking in terms of the area under the ROC curve (AUC) ().", "labels": [], "entities": [{"text": "standardness score", "start_pos": 68, "end_pos": 86, "type": "METRIC", "confidence": 0.956622838973999}, {"text": "ROC curve (AUC)", "start_pos": 144, "end_pos": 159, "type": "METRIC", "confidence": 0.9459839224815368}]}, {"text": "shows the AUC for each model.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8991901874542236}]}, {"text": "First, we examined the impact of the choice of training corpus for obtaining word embeddings.", "labels": [], "entities": []}, {"text": "The models with BCCWJ are constantly better than those with other corpora, although BCCWJ is smaller than the others).", "labels": [], "entities": [{"text": "BCCWJ", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.7898505926132202}, {"text": "BCCWJ", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.8052937388420105}]}, {"text": "This result suggests that use of a balanced corpus is crucial in our method for this task.", "labels": [], "entities": []}, {"text": "The Balanced Corpus of Contemporary Written Japanese (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the dataset.", "labels": [], "entities": []}, {"text": " Table 2: Description of corpora.", "labels": [], "entities": []}, {"text": " Table 3: Area under the ROC curve (AUC) in us- age classification task for each model.", "labels": [], "entities": [{"text": "Area", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9641854166984558}, {"text": "ROC curve (AUC)", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9513451099395752}, {"text": "us- age classification", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.5481825023889542}]}]}