{"title": [{"text": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.6627149085203806}]}], "abstractContent": [{"text": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8501936892668406}]}, {"text": "It is typically regarded as an important step in the standard NLP pipeline.", "labels": [], "entities": []}, {"text": "As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model.", "labels": [], "entities": []}, {"text": "We propose aversion of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs.", "labels": [], "entities": []}, {"text": "GCNs over syntactic dependency trees are used as sentence en-coders, producing latent feature representations of words in a sentence.", "labels": [], "entities": []}, {"text": "We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and En-glish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling (SRL) () can be informally described as the task of discovering who did what to whom.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8516225616137186}]}, {"text": "For example, consider an SRL dependency graph shown above the sentence in.", "labels": [], "entities": [{"text": "SRL dependency", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8274199366569519}]}, {"text": "Formally, the task includes (1) detection of predicates (e.g., makes); (2) labeling the predicates with a sense from a sense inventory (e.g., make.01); (3) identifying and assigning arguments to semantic roles (e.g., Sequa is A0, i.e., an agent / 'doer' for the corresponding predicate, and engines is A1, i.e., a patient / 'an affected entity').", "labels": [], "entities": []}, {"text": "SRL is often regarded as an important step in the standard NLP pipeline, providing information to downstream tasks such as information extraction and question answering.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7356408834457397}, {"text": "information extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.8012558817863464}, {"text": "question answering", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.8718273639678955}]}, {"text": "The semantic representations are closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial.", "labels": [], "entities": []}, {"text": "For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in) are mirrored in the semantic dependency graph.", "labels": [], "entities": []}, {"text": "Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics.", "labels": [], "entities": [{"text": "predicting semantics", "start_pos": 163, "end_pos": 183, "type": "TASK", "confidence": 0.874717503786087}]}, {"text": "Though historically most SRL approaches did rely on syntax, the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (, and outperformed syntactically-driven methods on standard benchmarks.", "labels": [], "entities": [{"text": "SRL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9860825538635254}]}, {"text": "We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words).", "labels": [], "entities": []}, {"text": "In this paper we propose one way how to address this limitation.", "labels": [], "entities": []}, {"text": "Specifically, we rely on graph convolutional networks (GCNs) (, a recent class of multilayer neural networks operating on graphs.", "labels": [], "entities": []}, {"text": "For every node in the graph (in our case a word in a sentence), GCN encodes relevant information about its neighborhood as a real-valued feature vector.", "labels": [], "entities": []}, {"text": "GCNs have been studied largely in the context of undirected unlabeled graphs.", "labels": [], "entities": []}, {"text": "We introduce aversion of GCNs for modeling syntactic dependency structures and generally applicable to labeled directed graphs.", "labels": [], "entities": []}, {"text": "One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., information about nodes at most K hops aways).", "labels": [], "entities": []}, {"text": "This contrasts with recurrent and recursive neural networks) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence.", "labels": [], "entities": []}, {"text": "However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs).", "labels": [], "entities": []}, {"text": "When we stack GCNs on top of LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark, both for English and Chinese.", "labels": [], "entities": []}, {"text": "Interestingly, again unlike recursive neural networks, GCNs do not constrain the graph to be a tree.", "labels": [], "entities": []}, {"text": "We believe that there are many applications in NLP, where GCN-based encoders of sentences or even documents can be used to incorporate knowledge about linguistic structures (e.g., representations of syntax, semantics or discourse).", "labels": [], "entities": []}, {"text": "For example, GCNs can take as input combined syntactic-semantic graphs (e.g., the entire graph from) and be used within downstream tasks such as machine translation or question answering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8235564827919006}, {"text": "question answering", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.8449365794658661}]}, {"text": "However, we leave this for future work and here solely focus on SRL.", "labels": [], "entities": [{"text": "SRL", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9633533954620361}]}, {"text": "The contributions of this paper can be summarized as follows: \u2022 we are the first to show that GCNs are effective for NLP; \u2022 we propose a generalization of GCNs suited The code is available at https://github.com/ diegma/neural-dep-srl.", "labels": [], "entities": []}, {"text": "to encoding syntactic information at word level; \u2022 we propose a GCN-based SRL model and obtain state-of-the-art results on English and Chinese portions of the CoNLL-2009 dataset; \u2022 we show that bidirectional LSTMs and syntax-based GCNs have complementary modeling power.", "labels": [], "entities": [{"text": "CoNLL-2009 dataset", "start_pos": 159, "end_pos": 177, "type": "DATASET", "confidence": 0.9778769910335541}]}], "datasetContent": [{"text": "We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets.", "labels": [], "entities": [{"text": "SRL", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9293012619018555}, {"text": "English and Chinese CoNLL-2009 dataset", "start_pos": 40, "end_pos": 78, "type": "DATASET", "confidence": 0.6982152581214904}]}, {"text": "The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers.", "labels": [], "entities": [{"text": "CoNLL-2009 shared-task organizers", "start_pos": 63, "end_pos": 96, "type": "DATASET", "confidence": 0.8466359575589498}]}, {"text": "For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Bj\u00f6rkelund et al. for Chinese.", "labels": [], "entities": []}, {"text": "We parsed English sentences with the BIST Parser, whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers.", "labels": [], "entities": [{"text": "BIST Parser", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.8534014225006104}, {"text": "CoNLL-2009 shared-task organizers", "start_pos": 125, "end_pos": 158, "type": "DATASET", "confidence": 0.8772183855374655}]}, {"text": "For English, we used external embeddings of , learned using the structured skip n-gram approach of . For Chinese we used external embeddings produced with the neural language model of.", "labels": [], "entities": []}, {"text": "We used edge dropout in GCN: when  v , we ignore each node v 2 N (v) with probability \ud97b\udf59.", "labels": [], "entities": [{"text": "GCN", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.8411513566970825}]}, {"text": "Adam ( was used as an optimizer.", "labels": [], "entities": []}, {"text": "The hyperparameter tuning and all model selection were performed on the English development set; the chosen values are shown in Appendix.", "labels": [], "entities": [{"text": "English development set", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9225032528241476}, {"text": "Appendix", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.8522722125053406}]}], "tableCaptions": [{"text": " Table 1: SRL results without predicate disam- biguation on the English development set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9355196356773376}, {"text": "English development set", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.7934942046801249}]}, {"text": " Table 2: SRL results without predicate disam- biguation on the Chinese development set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8837198615074158}, {"text": "Chinese development set", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.9059353470802307}]}, {"text": " Table 3: Results on the test set for English.", "labels": [], "entities": []}, {"text": " Table 4: Results on the Chinese test set.", "labels": [], "entities": [{"text": "Chinese test set", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.9822543660799662}]}, {"text": " Table 5: Results on the out-of-domain test set.", "labels": [], "entities": []}]}