{"title": [{"text": "Rotated Word Vector Representations and their Interpretability", "labels": [], "entities": [{"text": "Rotated Word Vector Representations", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9010644257068634}]}], "abstractContent": [{"text": "Vector representation of words improves performance in various NLP tasks, but the high-dimensional word vectors are very difficult to interpret.", "labels": [], "entities": []}, {"text": "We apply several rotation algorithms to the vector representation of words to improve the interpretabil-ity.", "labels": [], "entities": []}, {"text": "Unlike previous approaches that induce sparsity, the rotated vectors are in-terpretable while preserving the expressive performance of the original vectors.", "labels": [], "entities": []}, {"text": "Furthermore , any pre-built word vector representation can be rotated for improved in-terpretability.", "labels": [], "entities": []}, {"text": "We apply rotation to skip-grams and glove and compare the expressive power and interpretability with the original vectors and the sparse overcom-plete vectors.", "labels": [], "entities": []}, {"text": "The results show that the rotated vectors outperform the original and the sparse overcomplete vectors for inter-pretability and expressiveness tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector representations of words contain rich semantic and syntactic information and thus improve the performance of numerous natural language processing tasks.", "labels": [], "entities": []}, {"text": "The vectors also play a basic role as an embedding layer in deep learning models for NLP, affecting the expressive performance of the model (.", "labels": [], "entities": []}, {"text": "However, the many dimensions comprising the vector representation are not amenable to interpretation.", "labels": [], "entities": []}, {"text": "Previous research on vector representation of words has proposed improving interpretability while keeping the expressive performance by inducing sparsity in word vector dimensions).", "labels": [], "entities": []}, {"text": "Recent research has proposed to build sparse vector representations from a large corpus and added the nonnegativity constraint using improved projected gradient (), while () learns l1-regularised vectors.", "labels": [], "entities": []}, {"text": "But, these models cannot be learned over pre-trained word vectors based on skip-gram () or glove () which are widely used.", "labels": [], "entities": [{"text": "glove", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9485518932342529}]}, {"text": "proposes an alternative approach to stand-alone models by forming sparse representations based on the pre-trained models.", "labels": [], "entities": []}, {"text": "To do this, they use overcomplete vectors, which are much higher in dimensionality than the original vectors.", "labels": [], "entities": []}, {"text": "Unlike these sparsity-inducing approaches, we construct an interpretable word vector representation by using the pre-trained word vectors as input and using a basis rotation algorithm from the Exploratory Factor Analysis (EFA) literature used in developing psychological scales.", "labels": [], "entities": []}, {"text": "Like the word vector representation, every single item in the scale is represented as a numeric vector in the latent factor space.", "labels": [], "entities": []}, {"text": "The set of item vectors are represented in a factor loading matrix, and the matrix is rotated such that the factors (i.e., dimensions) become interpretable.", "labels": [], "entities": []}, {"text": "The rotation achieves a Simple Structure through minimizing the row and the column complexity of the matrix.", "labels": [], "entities": []}, {"text": "We elaborate on this process in the next section.", "labels": [], "entities": []}, {"text": "As in EFA, we rotate the word vector representation matrix to obtain dimension-wise interpretability while retaining the number of dimensions the same.", "labels": [], "entities": []}, {"text": "For example, shows the rotated skip-gram vectors for two groups of words.", "labels": [], "entities": []}, {"text": "These words are top five words of two dimensions from rotated Word2Vec.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9443608522415161}]}, {"text": "Our main contribution is applying the matrix rotation algorithm from psychometric analysis to word vector representation models to improve the interpretability of the vector.", "labels": [], "entities": []}, {"text": "This approach gives an answer to the question why and how word vec- tor representations work well by revealing a hidden structure of the original word vectors.", "labels": [], "entities": []}, {"text": "That is, it is meaningful to transform the hard-to-interpret dimensions of the pre-built word vectors, which are widely used, to more interpretable vectors.", "labels": [], "entities": []}, {"text": "We also show that the rotated vectors retain their effectiveness with respect to downstream tasks without re-building the vector representations.", "labels": [], "entities": []}, {"text": "Our method can be applied to any type of word vectors as a post-processing method such that it does not require a large corpus to be trained.", "labels": [], "entities": []}, {"text": "In addition, it does not require additional number of dimensions so it does not increase the complexity of the model.", "labels": [], "entities": []}, {"text": "Furthermore, we explore the characteristics of the rotated word vectors.", "labels": [], "entities": []}], "datasetContent": [{"text": "We choose the Wikipedia English articles 1 to train the word vector models.", "labels": [], "entities": []}, {"text": "The corpus contains 5.3M articles, 83M sentences and 1,676M tokens.", "labels": [], "entities": []}, {"text": "For preprocessing, we leave only the alphanumeric tokens and apply lowercase to all words.", "labels": [], "entities": []}, {"text": "Then we remove the words with frequency less than 50, and the size of the remaining vocabulary is 306,491.", "labels": [], "entities": []}, {"text": "We train skip-gram 2 (Mikolov et al., 2013) and glove) based on the corpus by using existing implementations.", "labels": [], "entities": []}, {"text": "We set the window size to 5 for both skip-gram and glove.", "labels": [], "entities": []}, {"text": "We set the number of negative samples to 5 and the number of dimensions to 300.", "labels": [], "entities": []}, {"text": "We use the default values for the other hyperparameters.", "labels": [], "entities": []}, {"text": "The size of the resulting word vector matrix is.", "labels": [], "entities": []}, {"text": "We compare our model with two baseline models: sparse overcomplete vector representations (SOV) and the non-negative version of the SOV.", "labels": [], "entities": []}, {"text": "We set the hyperparameters of these models as \u03bb = .5, \u03c4 = 10 \u22125 , K = 3000 for SG, and \u03bb = 1.0, \u03c4 = 10 \u22125 , K = 3000 for Glove).", "labels": [], "entities": []}, {"text": "We excluded methods as baselines that construct interpretable word vectors using huge training corpora because our method works with pre-trained vectors.", "labels": [], "entities": []}, {"text": "We apply four rotation algorithms for each orthogonal and oblique rotation, listed in.", "labels": [], "entities": []}, {"text": "Since we have two original word vector representations, we have 16 (4 x 2 x 2) rotated vectors in total.", "labels": [], "entities": []}, {"text": "We implement the algorithm through TensorFlow (, and it is publicly available on GitHub 4 .  We briefly describe the seven benchmark tasks: word similarity and semantic/syntactic analogy, and four classification tasks.", "labels": [], "entities": [{"text": "word similarity and semantic/syntactic analogy", "start_pos": 140, "end_pos": 186, "type": "TASK", "confidence": 0.6933789338384356}]}, {"text": "For the classification tasks, we average the word vectors in each training sentence or phrase to use them as features.", "labels": [], "entities": []}, {"text": "SVM and random forest classifier are trained to predict the target values, and hyperparameters are tuned on the validation set.", "labels": [], "entities": []}, {"text": "Word Similarity (Simil.)", "labels": [], "entities": [{"text": "Word Similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6129478514194489}]}, {"text": "SimLex-999 ( presented to evaluate the similarity of word pairs, rather than relatedness.", "labels": [], "entities": [{"text": "SimLex-999", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9045035243034363}]}, {"text": "We compute the cosine similarity between the given word pairs, and report the Spearman's correlation coefficient as a measure of consistency between the similarity and human ratings.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient", "start_pos": 78, "end_pos": 112, "type": "METRIC", "confidence": 0.8020066767930984}]}, {"text": "Semantic and Syntactic Analogies (Analg. sem, syn).", "labels": [], "entities": [{"text": "Semantic and Syntactic Analogies", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5811427608132362}]}, {"text": "The second and third tasks are word analogy tasks proposed by.", "labels": [], "entities": [{"text": "word analogy tasks", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8219159642855326}]}, {"text": "The semantic task includes 8,869 questions (sem) and the syntactic task includes 10,675 questions (syn).", "labels": [], "entities": []}, {"text": "Sentiment Analysis (Sent.)", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9471074342727661}]}, {"text": "The first classification task is sentiment classification on the movie reviews (: Evaluation results of the original skip-gram, sparse overcomplete vectors (SOV), and the rotated (orthogonal and oblique) word vectors on various tasks.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.9668562412261963}]}, {"text": "The left three columns show tasks based on cosine similarity, and the right four columns show classification tasks using average word vectors as features.", "labels": [], "entities": []}, {"text": "Overall, the rotated word vectors show higher or comparable performance to that of the SOV and the original.", "labels": [], "entities": []}, {"text": "We observe a similar pattern in Glove as well.", "labels": [], "entities": [{"text": "Glove", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9250208735466003}]}, {"text": "6,920, 872, 1,821 sentences for training, development, and test, respectively.", "labels": [], "entities": []}, {"text": "The goal of this task is to predict positive or negative sentiment of the reviews.", "labels": [], "entities": []}, {"text": "Question Classification (Ques.)", "labels": [], "entities": [{"text": "Question Classification (Ques.)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6582504312197367}]}, {"text": "Next, we use TREC dataset to classify categories of the questions (.", "labels": [], "entities": [{"text": "TREC dataset", "start_pos": 13, "end_pos": 25, "type": "DATASET", "confidence": 0.7303761690855026}]}, {"text": "We divide the dataset into 4,952, 500, 500 for training, development, and test.", "labels": [], "entities": []}, {"text": "The dataset has six types of questions including about person, location, etc.", "labels": [], "entities": []}, {"text": "Topic Classification (Topics: Sp.)", "labels": [], "entities": [{"text": "Topic Classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8304097056388855}]}, {"text": "Next, we obtain the 20 newsgroup dataset to classify Sports (baseball vs. hockey) topics).", "labels": [], "entities": []}, {"text": "The dataset consists of 958, 239, 796 for training, development, and test.", "labels": [], "entities": []}, {"text": "NP bracketing (NP brckt.)", "labels": [], "entities": [{"text": "NP bracketing", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7599568367004395}]}, {"text": "The final task is classifying noun phrases in terms of bracketing (.", "labels": [], "entities": []}, {"text": "Each phrase consists of three words, and the task is to predict the correct bracketing to match the similar words.", "labels": [], "entities": []}, {"text": "We compute the average of NPs and perform ten-fold cross-validation over 2,227 phrases.", "labels": [], "entities": []}, {"text": "The classifiers are trained and the hyperparameters are tuned for every fold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of the factor rotation process to verify the construct validity of the psychological  scale and its intended latent factor (left) in development. Items and loadings are from", "labels": [], "entities": []}, {"text": " Table 3: Overall distance ratio (DR overall ) of  the original, sparse overcomplete vectors, and the  rotated (orthogonal and oblique) vector repre- sentations. Rotated vectors show improved inter- pretability over SOV and the original.", "labels": [], "entities": [{"text": "distance ratio (DR overall )", "start_pos": 18, "end_pos": 46, "type": "METRIC", "confidence": 0.9021513859430949}]}, {"text": " Table 5: Evaluation results of the original skip-gram, sparse overcomplete vectors (SOV), and the rotated  (orthogonal and oblique) word vectors on various tasks. The left three columns show tasks based on  cosine similarity, and the right four columns show classification tasks using average word vectors as  features. Overall, the rotated word vectors show higher or comparable performance to that of the SOV  and the original. We observe a similar pattern in Glove as well.", "labels": [], "entities": [{"text": "Glove", "start_pos": 463, "end_pos": 468, "type": "DATASET", "confidence": 0.9237600564956665}]}, {"text": " Table 6: Overall distance ratio based on the top words extracted from the values in word vectors sorted  by descending order (Hi) and ascending order (Lo). Cor(Hi, Lo) is correlation between two distance  ratios based on both directions. Next three columns present correlation between the absolute word vector  values of the top words and distance ratios. The last columns shows selective distance ratio measure. The  results implies generally both direction is interpretable, one direction is more interpretable than the other  within a dimension, and larger absolute value in a dimension means higher interpretability. (* p < .05,  ** p < .01, *** p < .001)", "labels": [], "entities": [{"text": "Cor", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9846221804618835}]}]}