{"title": [{"text": "Quantifying the Effects of Text Duplication on Semantic Models", "labels": [], "entities": [{"text": "Quantifying the Effects of Text Duplication", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.584909200668335}]}], "abstractContent": [{"text": "Duplicate documents area pervasive problem in text datasets and can have a strong effect on unsupervised models.", "labels": [], "entities": []}, {"text": "Methods to remove duplicate texts are typically heuristic or very expensive, so it is vital to know when and why they are needed.", "labels": [], "entities": []}, {"text": "We measure the sensitivity of two latent semantic methods to the presence of different levels of document repetition.", "labels": [], "entities": []}, {"text": "By artificially creating different forms of duplicate text we confirm several hypotheses about how repeated text impacts models.", "labels": [], "entities": []}, {"text": "While a small amount of duplication is tolerable , substantial over-representation of subsets of the text may overwhelm meaningful topical patterns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Different discussions of the same subject tend to use similar words.", "labels": [], "entities": []}, {"text": "Unsupervised models such as latent semantic analysis (LSA)) and latent Dirichlet allocation (LDA) () look for these statistical signatures of topicality in the form of repeated word cooccurrences.", "labels": [], "entities": []}, {"text": "These methods have become increasingly popular because they are powerful and easy to apply to large unlabeled datasets.", "labels": [], "entities": []}, {"text": "The apparent ease-of-use of LSA and LDA, however, makes it easy to overlook potential problems in text corpora.", "labels": [], "entities": []}, {"text": "In this work, we focus on measuring the impact of one such issue: duplicate text.", "labels": [], "entities": []}, {"text": "Latent semantic methods look for patterns of repetition.", "labels": [], "entities": []}, {"text": "But when text is repeated exactly, statistical methods that look for patterns maybe diverted from more meaningful semantic groups: verbatim repetition looks, to the algorithm, more topical than actual topics.", "labels": [], "entities": []}, {"text": "If not accounted for, repeated text can change measures of fitness to overvalue fit on repeated texts, or even \"leak\" held out data that is duplicated in the training data.", "labels": [], "entities": []}, {"text": "At best, duplication may cause us to overestimate the expressiveness and reliability of models.", "labels": [], "entities": []}, {"text": "At worst, models skewed by text duplication may invalidate any conclusions drawn from them, and, by extension, the method itself.", "labels": [], "entities": []}, {"text": "Text replication is a persistent and difficult problem in natural language corpora.", "labels": [], "entities": [{"text": "Text replication", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7918579578399658}]}, {"text": "In social media settings, partial duplication due to quotation and threading is ubiquitous.", "labels": [], "entities": []}, {"text": "Of the 20k posts in the 20 Newsgroups corpus, 1151 are exact duplicates, and 25% of the remaining tokens are quoted text from other newsgroup messages.", "labels": [], "entities": [{"text": "20 Newsgroups corpus", "start_pos": 24, "end_pos": 44, "type": "DATASET", "confidence": 0.6613925993442535}]}, {"text": "In literary corpora, different versions of the same document may also conflict: text files for Hamlet may differ slightly due to publisher information, line numbers, editorial changes between Shakespeare's folios, and footnotes.", "labels": [], "entities": []}, {"text": "Removing exactly identical duplicates of texts is possible through direct lexicographic matching, but for lexical near-duplicates and partial textual overlap, we may need more careful heuristics to detect duplicates, forcing researchers to make judgments about what text to remove and what to keep.", "labels": [], "entities": []}, {"text": "Evaluating what level of duplication is \"safe\" can therefore not only reduce the risk of false conclusions but also save great amounts of work spent identifying and removing duplication.", "labels": [], "entities": []}, {"text": "In this work, we investigate the effect of text duplication on LSA and LDA by experimentally amplifying the magnitude of text duplication in a variety of corpora.", "labels": [], "entities": [{"text": "text duplication", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6952971369028091}]}, {"text": "We look both at how models shift to over-represent repeated text and how that shift affects the model representation of documents without repetition.", "labels": [], "entities": []}, {"text": "To account for the variety of types of duplication, we look at exact du-plication of whole documents as well as repetition of a text segment across many documents.", "labels": [], "entities": []}, {"text": "Finally, we recommend what aspects of text deduplication one should focus onto successfully reduce negative effects, with different suggestions depending on the chosen model.", "labels": [], "entities": [{"text": "text deduplication", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8061854839324951}]}], "datasetContent": [{"text": "We quantitatively examine several aspects of models with varying forms and degrees of duplication to determine the magnitude of the change produced by repeated text.", "labels": [], "entities": []}, {"text": "It is important to note that our goal is simply to measure the difference between models, and not to make normative statements about the quality of topics.", "labels": [], "entities": []}, {"text": "Indeed, many measures of topic quality such as word intrusion () and word co-occurrence () may improve as a result of degenerate, single-document topics: most documents are internally coherent, so a single document's word distribution may appear to be a sensible topic.", "labels": [], "entities": []}, {"text": "Loss The first aspect is model loss.", "labels": [], "entities": []}, {"text": "As stated in Section 3, as a segment of text is repeated more, we anticipate that the fit over documents containing repeated text will improve, while the fit over documents not containing repeated text will worsen.", "labels": [], "entities": []}, {"text": "To evaluate this for LSA, we examine the Frobenius norm of the difference between the reconstruction WA T and C for the rows corresponding to documents with and without repeated text.", "labels": [], "entities": [{"text": "Frobenius norm", "start_pos": 41, "end_pos": 55, "type": "METRIC", "confidence": 0.9174708127975464}]}, {"text": "For LDA, we estimate the perplexity of both the training data and held-out data without repetitions from the same corpus.", "labels": [], "entities": [{"text": "LDA", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9013358354568481}]}, {"text": "Concentration Secondly, we examine component (e.g. topic/dimension) concentration.", "labels": [], "entities": []}, {"text": "Repetition of a document amplifies the co-occurrence between the terms contained in the document.", "labels": [], "entities": [{"text": "Repetition of a document", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9008185863494873}]}, {"text": "As this signal grows stronger, we expect models to begin \"memorizing\" these words.", "labels": [], "entities": []}, {"text": "We anticipate that affected models will develop a simpler latent representation for the repeated document, one concentrated over a small number of components.", "labels": [], "entities": []}, {"text": "For example, if a model is devoting topic k to a repeated document, then instances of that document should have a high proportion of topic k.", "labels": [], "entities": []}, {"text": "Concentration measurements relate to loss, but focuses specifically on the document-component or document-topic patterns, while loss also includes information about the topic-word dynamics.", "labels": [], "entities": []}, {"text": "The effect of components converging to a single piece of repeated text should be easily observed by examining how close topics are to the unigram language model induced by the repeated text.", "labels": [], "entities": []}, {"text": "If we repeat multiple documents independently, however, we may also expect to see distinct components correlated with disjoint subsets of the repeated texts.", "labels": [], "entities": []}, {"text": "To account for this, we evaluate component concentration separately for documents with repeated text and without repeated text.", "labels": [], "entities": []}, {"text": "For LDA, we examine the entropy of document vectors.", "labels": [], "entities": []}, {"text": "Information entropy represents the expectation of the representation length of a given outcome as a function of the probability distribution over outcomes: where \u03b8 dk is the probability of a token generated in document d having topic k.", "labels": [], "entities": [{"text": "Information entropy", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7493895292282104}]}, {"text": "Entropy is inverse to concentration: the entropy of text should lower as the text is repeated more, as all of their topical mass would be concentrated in topics converging to modeling duplicate texts.", "labels": [], "entities": [{"text": "Entropy", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9512222409248352}]}, {"text": "Conversely, documents not containing repeated text may also have their entropy increase as text repetition increases, as topics will less adequately fit to the behavior of the singular documents.", "labels": [], "entities": []}, {"text": "In LSA, entropy is not as directly applicable: vectors in W = U \u03a3 can be arbitrarily realnumbered.", "labels": [], "entities": []}, {"text": "However, we still want to access a similar basic concept, the amount a vector representation of a document is concentrated in a few dimensions.", "labels": [], "entities": []}, {"text": "So, we examine the absolute dispersion of each row vector din W : where d k is the kth component of d.", "labels": [], "entities": []}, {"text": "Absolute dispersion measures the entropy of the L1-normalized masses of the vectors in W . Expressivity The final aspect is expressivity of topics.", "labels": [], "entities": []}, {"text": "If one topic converges to the unigram language model of repeated documents, the resulting model has effectively lost one topic worth of expressive power by focusing on overly-specific themes.", "labels": [], "entities": []}, {"text": "Someone looking to learn generalized semantic corpus patterns from a topic model will therefore have one fewer topic of interest available.", "labels": [], "entities": []}, {"text": "The frequency of terms in the repeated text may also overwhelm the most probable terms in many of the topics, again reducing the ability to interpret these topics or to understand their content through a summarized representation.", "labels": [], "entities": []}, {"text": "While expressivity in the form of topic summaries makes little sense for LSA, using LDA models, we may examine topic summaries, obtained as the top s most probable terms of a topic where sis a fixed parameter.", "labels": [], "entities": []}, {"text": "We may select the same number of terms s from the most probable in a unigram language model of the repeated text, and determine what proportion of the tokens obtained from concatenating topic similarities are the top terms of the repeated text language model.", "labels": [], "entities": []}, {"text": "Data To ensure our experiments are the only cause of exact duplication of text in our corpora, we use strict methods of text deduplication.", "labels": [], "entities": []}, {"text": "When two or more documents have more than 70% unigram overlap, we remove all but the longest document.", "labels": [], "entities": []}, {"text": "In addition, we delete 7-grams that appear in more than 10 documents based upon existing thresholds for plagiarism detection.", "labels": [], "entities": []}, {"text": "To account for stopwords, we remove all terms appearing in more than 80% of documents.", "labels": [], "entities": []}, {"text": "Finally, we remove documents with fewer than 7 tokens after processing.", "labels": [], "entities": []}, {"text": "We perform this process on a random sample of 30,000 documents from each corpus to ensure we may obtain a sample of 25,000 curated documents for each of our two corpora.", "labels": [], "entities": []}, {"text": "We also produce 10% samples of these corpora, containing 2,500 documents each, to measure the effect of corpus size.", "labels": [], "entities": []}, {"text": "Text Duplication Treatments We use our deduplicated news corpora to construct datasets with artificial text duplication.", "labels": [], "entities": [{"text": "Text Duplication", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6736510843038559}]}, {"text": "We examine two different duplication scenarios: exact document duplication and template string duplication.", "labels": [], "entities": [{"text": "exact document duplication", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.5945712625980377}, {"text": "template string duplication", "start_pos": 79, "end_pos": 106, "type": "TASK", "confidence": 0.6302794913450877}]}, {"text": "In exact document duplication, we randomly sample p% of the documents in the dataset and include c copies of each sampled document in our final corpus along with one copy each of the remaining documents, which we refer to as singular documents.", "labels": [], "entities": [{"text": "exact document duplication", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6760459740956625}]}, {"text": "To test the extremes of this effect, we also perform single document tests for large c with only one repeated document.", "labels": [], "entities": []}, {"text": "From these synthetically duplicative corpora, we can determine whether effects are triggered by the sheer volume of duplicated text or if they are influenced by the diversity of the copied documents.", "labels": [], "entities": []}, {"text": "In template string duplication, rather than duplicating the sampled p% of documents, we prepend a fixed string to each document in the p% sample, producing what we refer to as templated documents or texts.", "labels": [], "entities": [{"text": "template string duplication", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6582612494627634}]}, {"text": "As repeated text maybe lexically similar or different from the non-repeated text of the corpus, we consider two different types of prepended string.", "labels": [], "entities": []}, {"text": "The first is a randomlysampled document from the deduplicated corpus but not included in the training set (Sampled Template), simulating repeated text that is lexically similar to the document content.", "labels": [], "entities": []}, {"text": "The second is: Training perplexity with LDA models trained on the REUSL 25k corpus with 80 topics.", "labels": [], "entities": [{"text": "REUSL 25k corpus", "start_pos": 66, "end_pos": 82, "type": "DATASET", "confidence": 0.9183954993883768}]}, {"text": "Perplexity decreases significantly for the duplicated documents with repetition, but the effect on singular documents is negligible with repeated proportion of the corpus smaller than 0.1.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9779876470565796}, {"text": "repetition", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9526864290237427}]}, {"text": "Training We analyze two types of semantic models: LSA and LDA.", "labels": [], "entities": []}, {"text": "LSA models are trained using tf-idf weighting on word-document matrices using custom Python code.", "labels": [], "entities": []}, {"text": "3 LDA models are trained using Mallet) with fixed hyperparameters \u03b1 = 50/K and \u03b2 = 0.01 for ease of comparison.", "labels": [], "entities": [{"text": "Mallet", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9266988635063171}]}, {"text": "To compute perplexity, we use log likelihood estimates from Mallet's built-in left-to-right estimation ().", "labels": [], "entities": [{"text": "log likelihood", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.7386742532253265}]}], "tableCaptions": [{"text": " Table 1: As the number of total topics increases,  the average number of topics fitting the Lorem  Template duplicate text remains stable, only ris- ing above 1 with repeated proportion of the corpus  p = 0.1 and at least 80 topics.", "labels": [], "entities": [{"text": "Lorem  Template duplicate text", "start_pos": 93, "end_pos": 123, "type": "DATASET", "confidence": 0.8129152208566666}]}]}