{"title": [{"text": "Incorporating Global Visual Features into Attention-Based Neural Machine Translation", "labels": [], "entities": [{"text": "Attention-Based Neural Machine Translation", "start_pos": 42, "end_pos": 84, "type": "TASK", "confidence": 0.6734692305326462}]}], "abstractContent": [{"text": "We introduce multi-modal, attention-based Neural Machine Translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder.", "labels": [], "entities": [{"text": "attention-based Neural Machine Translation (NMT)", "start_pos": 26, "end_pos": 74, "type": "TASK", "confidence": 0.7871375850268773}]}, {"text": "Global image features are extracted using a pre-trained convo-lutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state.", "labels": [], "entities": []}, {"text": "In our experiments , we evaluate translations into En-glish and German, how different strategies to incorporate global image features compare and which ones perform best.", "labels": [], "entities": []}, {"text": "We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models.", "labels": [], "entities": []}, {"text": "We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated.", "labels": [], "entities": [{"text": "Multi30k data set", "start_pos": 159, "end_pos": 176, "type": "DATASET", "confidence": 0.9154223799705505}]}, {"text": "To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all met-rics evaluated on this data set.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has recently been proposed as an instantiation of the sequence to sequence (seq2seq) learning problem.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7809697687625885}]}, {"text": "In this problem, each training example consists of one source and one target variable-length sequence, with no prior information regarding the alignments between the two.", "labels": [], "entities": []}, {"text": "A model is trained to translate sequences in the source language into corresponding sequences in the target.", "labels": [], "entities": []}, {"text": "This framework has been successfully used in many different tasks, such as handwritten text generation, image description generation (, machine translation () and video description generation ( . Recently, there has been an increase in the number of natural language generation models that explicitly use attention-based decoders, i.e. decoders that model an intra-sequential mapping between source and target representations.", "labels": [], "entities": [{"text": "handwritten text generation", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.6216080685456594}, {"text": "image description generation", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.8160417477289835}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7633956968784332}, {"text": "video description generation", "start_pos": 163, "end_pos": 191, "type": "TASK", "confidence": 0.7737072010835012}]}, {"text": "For instance, proposed an attentionbased model for the task of Image Description Generation (IDG) where the model learns to attend to specific parts of an image (the source) as it generates its description (the target).", "labels": [], "entities": [{"text": "Image Description Generation (IDG)", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.8550768146912257}]}, {"text": "In MT, one can intuitively interpret this attention mechanism as inducing an alignment between source and target sentences, as first proposed by.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9876432418823242}]}, {"text": "The common idea is to explicitly frame a learning task in which the decoder learns to attend to the relevant parts of the source sequence when generating each part of the target sequence.", "labels": [], "entities": []}, {"text": "We are inspired by recent successes in using attention-based models in both IDG and NMT.", "labels": [], "entities": []}, {"text": "Our main goal in this work is to propose end-toend multi-modal NMT models which effectively incorporate visual features in different parts of the attention-based NMT framework.", "labels": [], "entities": []}, {"text": "The main contributions of our work are: \u2022 We propose novel attention-based multimodal NMT models which incorporate visual features into the encoder and the decoder.", "labels": [], "entities": []}, {"text": "\u2022 We discuss the impact that adding synthetic multi-modal and multilingual data brings to multi-modal NMT.", "labels": [], "entities": []}, {"text": "\u2022 We show that images bring useful information to an NMT model and report state-ofthe-art results.", "labels": [], "entities": []}, {"text": "One additional contribution of our work is that we corroborate previous findings by that suggested that using image features directly as additional context to update the hidden state of the decoder (at each time step) prevents learning.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In \u00a71.1 we briefly discuss relevant previous related work.", "labels": [], "entities": []}, {"text": "We then revise the attention-based NMT framework and further expand it into different multi-modal NMT models ( \u00a72).", "labels": [], "entities": []}, {"text": "In \u00a73 we introduce the data sets we use in our experiments.", "labels": [], "entities": []}, {"text": "In \u00a74 we detail the hyperparameters, parameter initialisation and other relevant details of our models.", "labels": [], "entities": [{"text": "parameter initialisation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6589820235967636}]}, {"text": "Finally, in \u00a76 we draw conclusions and provide some avenues for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our encoder is a bidirectional RNN with GRU (one 1024D single-layer forward RNN and one 1024D single-layer backward RNN).", "labels": [], "entities": [{"text": "GRU", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9833905100822449}]}, {"text": "Source and target word embeddings are 620D each and both are trained jointly with our model.", "labels": [], "entities": []}, {"text": "All nonrecurrent matrices are initialised by sampling from a Gaussian (\u00b5 = 0, \u03c3 = 0.01), recurrent matrices are orthogonal and bias vectors are all initialised to zero.", "labels": [], "entities": []}, {"text": "Our decoder RNN also uses GRU and is a neural LM () conditioned on its previous emissions and the source sentence by means of the source attention mechanism.", "labels": [], "entities": [{"text": "GRU", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9413812756538391}]}, {"text": "Image features are obtained by feeding images to the pre-trained VGG19 network of Simonyan and Zisserman (2014) and using the activations of the penultimate fully-connected layer FC7.", "labels": [], "entities": [{"text": "VGG19 network", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9355470836162567}]}, {"text": "We apply dropout with a probability of 0.2 in both source and target word embeddings and with a probability of 0.5 in the image features (in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and before the readout operation in the decoder RNN.", "labels": [], "entities": []}, {"text": "We follow and apply dropout to the encoder bidirectional RNN and decoder RNN using the same mask in all time steps.", "labels": [], "entities": []}, {"text": "Our models are trained using stochastic gradient descent with Adadelta (Zeiler, 2012) and minibatches of size 40 for improved generalisation (, where each training instance consists of one English sentence, one German sentence and one image.", "labels": [], "entities": []}, {"text": "We apply early stopping for model selection based on BLEU scores, so that if a model does not improve on the validation set for more than 20 epochs, training is halted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9989830851554871}]}, {"text": "We evaluate our models' translation quality quantitatively in terms of BLEU4 (), METEOR), TER (), and chrF3 scores 5 (Popovi\u00b4cPopovi\u00b4c, 2015) and we report statistical significance for the three first metrics using approximate randomisation computed with).", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9987599849700928}, {"text": "METEOR", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9964355230331421}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9908570647239685}]}, {"text": "As our main baseline we train an attentionbased NMT model ( \u00a72) in which only the textual part of M30k T is used for training.", "labels": [], "entities": []}, {"text": "We also train a PBSMT model built with Moses on the same English\u2192German (German\u2192English) data, where the LM is a 5-gram LM with modified Kneser-Ney smoothing trained on the German (English) of the M30k T dataset.", "labels": [], "entities": [{"text": "English\u2192German (German\u2192English) data", "start_pos": 57, "end_pos": 93, "type": "DATASET", "confidence": 0.6374128129747179}, {"text": "M30k T dataset", "start_pos": 197, "end_pos": 211, "type": "DATASET", "confidence": 0.9430398742357889}]}, {"text": "We use minimum error rate training) for tuning the model parameters for BLEU scores.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 7, "end_pos": 34, "type": "METRIC", "confidence": 0.780737116932869}, {"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.99467933177948}]}, {"text": "Our third baseline (English\u2192German), is the best comparable multi-modal model by and also their best model with additional object detections: respectively models m1 (image at head) and m3 in the authors' paper.", "labels": [], "entities": []}, {"text": "Finally, our fourth baseline (German\u2192English) is We specifically compute character 6-gram F3 scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU4, METEOR, chrF3 (higher is bet- ter) and TER scores (lower is better) on the M30k T  test set for the two text-only baselines PBSMT  and NMT, the two multi-modal NMT models by  Huang et al. (2016) (English\u2192German only) and  our MNMT models that: (i) use images as words  in the source sentence (IMG 1W , IMG 2W ), (ii) use  images to initialise the encoder (IMG E ), and (iii)  use images as additional data to initialise the de- coder (IMG D ). Best text-only baselines are un- derscored and best overall results appear in bold.  We highlight in parentheses the improvements  brought by our models compared to the best cor- responding text-only baseline score. Results differ  significantly from PBSMT baseline ( \u2020) or NMT  baseline ( \u2021) with p = 0.05.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9987244009971619}, {"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9803084135055542}, {"text": "TER", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9993719458580017}, {"text": "M30k T  test set", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.8551998287439346}]}, {"text": " Table 3: Results for different combinations of multi-modal models, all trained on the original M30k T  training data only, evaluated on the M30k T test set.", "labels": [], "entities": [{"text": "M30k T  training data", "start_pos": 96, "end_pos": 117, "type": "DATASET", "confidence": 0.87748983502388}, {"text": "M30k T test set", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.9548092633485794}]}]}