{"title": [], "abstractContent": [{"text": "Variations of word associations across different groups of people can provide insights into people's psychologies and their world views.", "labels": [], "entities": []}, {"text": "To capture these variations, we introduce the task of demographic-aware word associations.", "labels": [], "entities": []}, {"text": "We build anew gold standard dataset consisting of word association responses for approximately 300 stimulus words, collected from more than 800 respondents of different gender (male/female) and from different locations (India/United States), and show that there are significant variations in the word associations made by these groups.", "labels": [], "entities": []}, {"text": "We also introduce anew demographic-aware word association model based on a neu-ral net skip-gram architecture, and show how computational methods for measuring word associations that specifically account for writer demographics can outper-form generic methods that are agnostic to such information.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding the associations that are formed in the mind is paramount to understanding the way humans acquire language throughout a lifetime of learning ().", "labels": [], "entities": []}, {"text": "Furthermore, word associations are believed to mirror the mental model of the conceptual connections in a human mind, and constitute a direct path to assessing one's semantic knowledge (.", "labels": [], "entities": []}, {"text": "Word associations start forming early in life, as language is acquired and one learns based on the environment where concepts lie in relation to each other.", "labels": [], "entities": []}, {"text": "For example, we may learn to associate \"mother\" with \"warmth,\" or \"fire\" with \"burn.\"", "labels": [], "entities": []}, {"text": "Yet, this mental model is not static but highly dynamic, and is shaped by new experiences over a lifetime.", "labels": [], "entities": []}, {"text": "For instance, showed that word associations change with time, and that for respondents in younger age groups their variability is lower, while for those in older age groups the variability is higher, as their life experiences modify the commonality between respondents from the same group.", "labels": [], "entities": []}, {"text": "Computational linguistics has traditionally taken the \"one-size-fits-all\" approach, with most models being agnostic to the language of the speakers behind the language.", "labels": [], "entities": [{"text": "Computational linguistics", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8147584199905396}]}, {"text": "With the introduction and adoption of Web 2.0, there has been an exponential increase in the availability of digital user-centric data in the form of blogs, microblogs and other forms of online participation.", "labels": [], "entities": []}, {"text": "Such data oftentimes can be augmented with demographic or other user-focused attributes, whether these are user-provided (e.g., from a user's online profile) or labeled using an automatic system.", "labels": [], "entities": []}, {"text": "This enables computational linguists to go beyond generic corpus-based metrics of word associations, and attempt to extract associations that pertain to given demographic groups that would not have been possible without administering time consuming and resource intensive word association surveys.", "labels": [], "entities": []}, {"text": "While current NLP methods generally deal with more advanced tasks (relation extraction, text similarity, etc.), at their very core many of these tasks assume someway of drawing connections (or associations) between words.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7856583595275879}, {"text": "text similarity", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.6835363656282425}]}, {"text": "Therefore, as a step toward demographic-aware NLP, we choose to work on the core task of \"word association.\"", "labels": [], "entities": [{"text": "word association", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.737371414899826}]}, {"text": "The algorithms we introduce can be immediately applied to demographic-aware word similarity, and with some minor changes to demographicaware text similarity.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.6622046381235123}]}, {"text": "Future stages could also include demographic-aware labeled associations, and more advanced applications such as information retrieval (which relies heavily on word associations/similarity), demographic-aware keyword extraction, dialogue personalization, and so forth.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7640352547168732}, {"text": "demographic-aware keyword extraction", "start_pos": 190, "end_pos": 226, "type": "TASK", "confidence": 0.6610285441080729}, {"text": "dialogue personalization", "start_pos": 228, "end_pos": 252, "type": "TASK", "confidence": 0.7606821358203888}]}, {"text": "Note that a few other researchers have explored demographic-aware NLP models with promising results, primarily focusing on the use of demographics for various forms of text classification or sentiment and subjectivity classification ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7325351685285568}, {"text": "sentiment and subjectivity classification", "start_pos": 191, "end_pos": 232, "type": "TASK", "confidence": 0.573247067630291}]}, {"text": "The paper makes several main contributions.", "labels": [], "entities": []}, {"text": "First, we create a novel dataset of demographic-aware word associations, consisting of approximately 300 stimulus words along with 800 responses per word collected from a demographically-diverse group of respondents, fora total of 228,800 responses.", "labels": [], "entities": []}, {"text": "Removing spam responses resulted in 176,097 responses.", "labels": [], "entities": []}, {"text": "Analyses that we perform on this dataset demonstrate that indeed word associations vary across user dimensions.", "labels": [], "entities": []}, {"text": "Second, we show that the associations we obtained follow the same pattern as those elicited during traditional classroom surveys.", "labels": [], "entities": []}, {"text": "Third, we propose an evaluation metric suited for the free association norms task.", "labels": [], "entities": []}, {"text": "Fourth, we introduce a demographic-aware model based on a skip-gram architecture and through several comparative experiments, we show that we are able to surpass the performance attainable on demographic agnostic models.", "labels": [], "entities": []}, {"text": "We specifically focus on two demographic dimensions: location and gender.", "labels": [], "entities": []}, {"text": "For location, we consider India and United States (US), choice made primarily because these two countries have a large English-speaking population, represented both on social media and on crowdsourcing platforms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Word association data collection typically consists of providing participants with a list of words, also known in the psycholinguistics literature as stimulus words, and asking them to provide the first word that comes to mind in response to each stimulus.", "labels": [], "entities": [{"text": "Word association data collection", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6789044663310051}]}, {"text": "For instance, given a stimulus word such as cat, one would expect answers such as dog or mouse.", "labels": [], "entities": []}, {"text": "Earlier work on word associations administered the tests in classroom settings, with 100 words per survey, and the results were compiled into tables of norms of word associations).", "labels": [], "entities": []}, {"text": "Since our goal is to explore the effect of demographics on word associations, we created a task on Amazon Mechanical Turk (AMT) able to reach a wide and demographically diverse audience.", "labels": [], "entities": [{"text": "word associations", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7148167192935944}, {"text": "Amazon Mechanical Turk (AMT)", "start_pos": 99, "end_pos": 127, "type": "DATASET", "confidence": 0.8166235089302063}]}, {"text": "The survey was structured into two sections: the word association part, followed by a demographic survey.", "labels": [], "entities": []}, {"text": "Given the online nature of the survey, and since we aimed fora high quality dataset, each participant was presented with a set of 50 stimulus words at a time (instead of 100).", "labels": [], "entities": []}, {"text": "The demographic section consisted of seven questions covering gender, age, location, occupation, ethnicity, education, and income.", "labels": [], "entities": []}, {"text": "The stimulus list consists of a set of approximately 300 words.", "labels": [], "entities": []}, {"text": "Among these, 99 words are sourced from the word list proposed by (standard list).", "labels": [], "entities": []}, {"text": "The remaining words are identified using the method for finding word-usage differences between two groups introduced in (, which relies on large collections of texts authored by the two groups to identify words that can be accurately classified by an automatic classifier as belonging to one group versus another.", "labels": [], "entities": []}, {"text": "Using their method, we obtain 100 words as the topmost dif-ferent words between US and India (culture list), and another set of 100 words as the topmost different words between male and female (gender list).", "labels": [], "entities": []}, {"text": "The reunion of these three lists results in 286 stimulus words for which we collect word associations.", "labels": [], "entities": []}, {"text": "The task was published separately for respondents from US and India, as AMT has an option of only presenting the survey to people from a preselected geographical location.", "labels": [], "entities": [{"text": "AMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.698359489440918}]}, {"text": "Six different surveys, each including approximately 50 stimulus words, were administered for each region.", "labels": [], "entities": []}, {"text": "The survey was conducted in English for both countries, noting that one of the official languages of India is English (alongside Hindi).", "labels": [], "entities": []}, {"text": "Each survey also included four spam-checking questions with previously known answers (e.g., What is the color of the sky?, with five options blue, red, pink, green, yellow), which were used to filter out respondents who were filling out the survey without reading the questions.", "labels": [], "entities": []}, {"text": "For each set, we gathered 400 responses per region, resulting in 800 responses for both US and India.", "labels": [], "entities": []}, {"text": "After removing the respondents who did not pass the spam-checking questions, we were left with an average of 752 responses per word, which we then balanced by gender, to retain an equal number of Indian women, Indian men, US women, and US men.", "labels": [], "entities": []}, {"text": "This resulted in 492 and 480 responses for the two sets of 50 standard stimulus words, 436 and 468 for the culture words, and 440 and 432 for the gender words.", "labels": [], "entities": []}, {"text": "Similar to), all the responses were normalized (i.e. plural was mapped to singular, gerund to infinitive, etc.); in our case we used the Stanford CoreNLP Lemmatizer (), ultimately aggregating the responses into a gold standard.", "labels": [], "entities": [{"text": "Stanford CoreNLP Lemmatizer", "start_pos": 137, "end_pos": 164, "type": "DATASET", "confidence": 0.8510559995969137}]}, {"text": "shows the top associations fora few sample stimuli, as collected from India and US, and males and females.", "labels": [], "entities": []}, {"text": "Finer-grained qualitative analyses also reveal interesting distinctions.", "labels": [], "entities": []}, {"text": "For instance bath is overwhelmingly associated by men with water, while US women associate it with bubble, and Indian women with soap.", "labels": [], "entities": []}, {"text": "Interestingly, US men seem to provide responses based on collocations, e.g., they answer Kane for citizen (citizen Kane), weight for heavy (heavyweight), or lion for mountain (mountain lion); on the contrary, women more often provide responses that consist of synonym or antonym words, e.g., person for citizen, health for sick, or light for heavy.", "labels": [], "entities": []}, {"text": "For further insight, number of different responses obtained fora given stimulus word, with the lowest variability word, and the highest variability word.", "labels": [], "entities": []}, {"text": "The second column lists the correlations between the frequency of the primary response and the number of different responses, as also reported by.", "labels": [], "entities": []}, {"text": "This correlation is negative, as the more people agree on the primary response, the fewer overall unique answers fora stimulus word are provided.", "labels": [], "entities": []}, {"text": "Additionally, shows the Zipfian distribution of average norm frequency; the most frequent response is given on average by 24% of the respondents, while the third most frequent response is given by 7% of them.: Average number of responses obtained fora given stimulus word, correlation between frequency of primary response and number of different responses, words exhibiting the lowest variability, and words with the highest variability.", "labels": [], "entities": []}, {"text": "To model norm strength within a given demographic group or across groups, we tabulate how often respondents from a group match the most frequent answer (Primary) or one of the most frequent ten answers for that group (Top10).", "labels": [], "entities": []}, {"text": "That is, given the response for one stimulus word as provided by one held-out survey respondent at a time, we determine whether that response matches the most frequent association of the remaining members of the same group, Primary columns), or one of the top 10 associations pertaining to that same group, Top10 columns).", "labels": [], "entities": []}, {"text": "Similarly, we measure the match with the most frequent or the top 10 responses from the other group, as shown in.", "labels": [], "entities": []}, {"text": "As expected, the intra-group similarities are significantly higher than the inter-group similarities, which supports our hypothesis that different groups make different word associations, which tend to be more coherent within a group than across groups.", "labels": [], "entities": []}, {"text": "While males and females have similar ranges for their agreement figures, we notice that on average US respondents have stronger intra-group agreements.", "labels": [], "entities": []}, {"text": "Note also that inter-group similarities are asymmetrical, as multiple words may have the same association frequency for one group, yet for the complementary group that may not be the case.", "labels": [], "entities": []}, {"text": "As an additional analysis of demographic variations in the responses received, for each respondent, we predict his / her demographic group using a majority vote conducted across all the user's responses using a simple rule-based system that assigns each response to the group having the highest frequency for that particular association.", "labels": [], "entities": []}, {"text": "For instance, given the response sun obtained from a respondent for the stimulus yellow, we assign the respondent to either India or US depending on the highest normalized frequency of the response sun for the same stimulus in each of those groups.", "labels": [], "entities": []}, {"text": "A similar rule-based assignment is also used for gender.", "labels": [], "entities": []}, {"text": "Thus, we compute the response words and their normalized frequencies based on the responses from 80% of the users chosen randomly, and accordingly predict the demographic group for the remaining 20% of the users based on a decision across the entire set of a user's responses.", "labels": [], "entities": []}, {"text": "shows the results of these predictions, which indicate high location variability (i.e., we can predict with high accuracy the location of a respondent), and medium gender variability.", "labels": [], "entities": []}, {"text": "All our models require textual data with demographic information.", "labels": [], "entities": []}, {"text": "We introduce below the data we used and the metrics we adopted for evaluation.", "labels": [], "entities": []}, {"text": "Given the requirement of having gender and location information associated with the data, we resort to blogs, and collect from Google Blogger 4 a large set of blog posts authored between 1999 and 2016.", "labels": [], "entities": []}, {"text": "shows the breakdown of the raw blog counts per demographic category.", "labels": [], "entities": []}, {"text": "From these, we retain only those posts with nonempty content, and preprocess the data by removing HTML tags, converting all the tokens to their lemmatized forms, and discarding those lemmas with a frequency less than 10, in order to avoid misspellings and other noise characteristic to social media content.", "labels": [], "entities": []}, {"text": "From the above pool of blog posts, we create two datasets with complementary demographic classes (1) location: India-US and (2) gender: male-female.", "labels": [], "entities": []}, {"text": "We process each of these datasets so that they are profile-balanced with no peaks for any specific years, by applying several heuristics: (1) Compute the minimum number of users n overall the classes (e.g., Indian and US authors in the case of the location dataset).", "labels": [], "entities": []}, {"text": "(2) From each class, select the top n users based on the number of years they were blogging and the number of posts they wrote.", "labels": [], "entities": []}, {"text": "This ensures that the maximum amount of data will be available for the selected users.", "labels": [], "entities": []}, {"text": "(3) For each of these n users, pick at most 50 posts in a round-robin fashion from the years in which they blogged.", "labels": [], "entities": []}, {"text": "(4) Let M be the total number of posts collected in this manner from all the classes.", "labels": [], "entities": []}, {"text": "In order to avoid having most of the posts coming from a small number of years, set a cutoff X as a fraction of M . For each year, a maximum of X posts will be chosen from the set of M posts (X = 0.1M ).", "labels": [], "entities": []}, {"text": "(5) To ensure that all the users get to contribute posts, and that the contribution of prolific writers is kept in check, maintain user participation scores: p(user) = posts collected from user total number of posts collected (5) These scores are updated after every year is processed, as explained further.", "labels": [], "entities": []}, {"text": "(6) Sort the years in increasing order of number of posts and iterate through them; identify the lowest number of posts contributed by the least prolific writer, then collect the minimum number of posts from all users who published in that year in a round-robin manner.", "labels": [], "entities": []}, {"text": "Then, select additional posts from users in increasing order of participation scores, until the number of posts for the year reaches the cutoff X.", "labels": [], "entities": []}, {"text": "After each year, update the user participation scores.", "labels": [], "entities": []}, {"text": "shows the number of users and posts retained after balancing.", "labels": [], "entities": []}, {"text": "This particular composition is used in our location data set (consisting of India and US posts) and gender data set (consisting of females and males posts).", "labels": [], "entities": []}, {"text": "Given that the word association task is relatively similar to the lexical substitution task, in terms of open vocabulary and lack of a \"right\" answer, we decided to borrow the best and outof-ten (oo10) evaluation metrics traditionally used for the latter), yet corrected for weight (.", "labels": [], "entities": [{"text": "word association task", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7786900301774343}]}, {"text": "Briefly, these measures take the best (or top ten) responses from a system, and compare them against the gold standard, while accounting for the frequencies of the responses in the gold standard.", "labels": [], "entities": []}, {"text": "In addition, since shows that the top three ranking norms are provided as answers by approximately 42% of the respondents, with the remaining norms following along Zipfian distribution in terms of frequency of appearance, we also compute outof-three (oo3), which represents a more focused approximation of our ability to predict human associations (note that out-of-ten covers 62% of the responses).", "labels": [], "entities": []}, {"text": "Several recent papers on word associations evaluated their models indirectly via Pearson or Spearman correlation performance on a word similarity task; we choose instead to evaluate word associations directly, by using metrics that more closely align with the evaluations performed in the field of psychology where the best output of a system is compared against the most frequent human response.", "labels": [], "entities": []}, {"text": "For a given stimulus word w with human responses H w , suppose a system returns a set of answers S w . We estimate how well this system can find a best substitute for w using Equation 6, where the function freq w (s) returns the count of a system response sin H w , and maxf req w returns the maximum count of any response in H w . Equation 7 measures the coverage of a system by allowing it to offer a set Sn w of n responses for w, where each response sis weighted by its frequency freq w (s) in H w .  We conduct evaluations using all the word association models described in Section 4.", "labels": [], "entities": []}, {"text": "The results using the best, out-of-three, and out-of-ten evaluation metrics are listed in.", "labels": [], "entities": []}, {"text": "For all the embeddings experiments, we use 300 latent dimensions.", "labels": [], "entities": []}, {"text": "The Gen variation uses the demographicblind dataset, whereas the DA variation uses the demographic-aware dataset.", "labels": [], "entities": [{"text": "demographicblind dataset", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.7801589369773865}]}, {"text": "The MI and VSM models do not perform well in the word association prediction task, whether considering the generic or the demographic-aware data.", "labels": [], "entities": [{"text": "word association prediction task", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.8686977326869965}]}, {"text": "We should emphasize, however, that the generic version of these models is able to consider co-occurrences across the entire generic datasets, while the demographic-aware co-occurrences can only be computed from the section of the dataset that matches a particular demographic; as such, these latter models are placed at a disadvantage.", "labels": [], "entities": []}, {"text": "Perhaps not surprisingly, the neural network skip-gram-based architectures, whether SGLM or our C-SGM, always achieve better results when compared to MI or VSM.", "labels": [], "entities": []}, {"text": "The demographic-aware variation proposed by) uses an extended skip-gram architecture that encodes a generic embedding, and several demographicbased filters per class, which in our case translates into three matrices of 300 dimensions each, the first for the generic words, and the subsequent ones for skews to be applied to the generic words in order to render the embedding through the lens of a given demographic.", "labels": [], "entities": []}, {"text": "SGLM \u2212 Gen in our case are the predictions based on the generic matrix, while SGLM \u2212 DA are the predictions modified along the lines of a particular demographic.", "labels": [], "entities": [{"text": "SGLM \u2212 DA", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.548569917678833}]}, {"text": "Our composite skip-gram models encode a single matrix that contains a mix of demographicaware and generic words expressed as 300 latent dimensions.", "labels": [], "entities": [{"text": "demographicaware", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.9155045747756958}]}, {"text": "For both gender and location, our gender-aware models (EM B1 and EM B2) surpass the SGLM gender-aware model.", "labels": [], "entities": []}, {"text": "Surprisingly, while SGLM was never meant to be generic, the predictions based on its generic embedding matrix prove to be a difficult baseline to surpass, similar to C-SGM generic.", "labels": [], "entities": []}, {"text": "Nonetheless, the composite skip-gram models (EM B1 and EM B2) do achieve best and second best rankings in the vast majority of cases (when compared to the best among all the other methods), with EM B1 being the more robust variation performing well both for gender and for location.", "labels": [], "entities": []}, {"text": "Focusing on the performance of EM B1, the highest gains are observed for India-based predictions, for best (from 0.05 to 0.08) and out-of-three (from 0.07 to 0.12); for male-based predictions increasing from 0.11 to 0.13 for best, and from 0.17 to 0.20 for out-ofthree; and for female-based predictions, increasing from 0.13 to 0.14 for best, and from 0.17 to 0.20 for out-of-three.", "labels": [], "entities": [{"text": "EM B1", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.47737541794776917}]}, {"text": "US-based associations are the hardest to predict, probably because of the diverse makeup of society; additional evaluations are needed to pinpoint the exact cause.", "labels": [], "entities": []}, {"text": "To determine how susceptible the embedding model is to skewed, but larger training data, we also run a separate experiment on the entire raw set of blogs we collected (described on the left of), where we re-generate the EM B1 and EM B2 models.", "labels": [], "entities": []}, {"text": "While the entire dataset is significantly larger than the balanced set, it is also significantly skewed: the data in the India:US dataset was skewed in a proportion of 1:0.48 tokens, while for Female:Male the proportion was 1:0.41 tokens.", "labels": [], "entities": [{"text": "India:US dataset", "start_pos": 121, "end_pos": 137, "type": "DATASET", "confidence": 0.5473840534687042}]}, {"text": "As was the case for the balanced dataset, the EM B1 model is still the most robust (see the bottom section in, and it achieves significant gains when compared to its balanced counterpart, in particular for best (for the US demographic from 0.03 to 0.13, and for India from 0.08 to 0.11), and for out-of-three (for US from 0.07 to 0.15, and for India from 0.12 to 0.17), which suggests that as an avenue for future research, we can explore the use of significantly larger even if unbalanced datasets to train our models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top three most frequent responses for sample stimulus words.", "labels": [], "entities": []}, {"text": " Table 3: Intra-group similarities (the higher the  similarity, the more cohesive the group is).", "labels": [], "entities": []}, {"text": " Table 4: Inter-group similarities (the higher the  similarity, the less distinct the groups are).", "labels": [], "entities": []}, {"text": " Table 5: Predictions based on similarity to group.", "labels": [], "entities": [{"text": "Predictions", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9265916347503662}]}, {"text": " Table 6: Raw and balanced blog dataset statistics.", "labels": [], "entities": []}, {"text": " Table 7: Best, out-of-three (oo3), and out-of-ten (oo10) scores across the various methods. IN: India,  US: United States, M: Male, F: Female. The numbers in bold mark the highest scores, those in italics,  the second highest.", "labels": [], "entities": []}]}