{"title": [{"text": "Neural Sequence-Labelling Models for Grammatical Error Correction", "labels": [], "entities": [{"text": "Neural Sequence-Labelling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7960540652275085}]}], "abstractContent": [{"text": "We propose an approach to N-best list re-ranking using neural sequence-labelling models.", "labels": [], "entities": []}, {"text": "We train a compositional model for error detection that calculates the probability of each token in a sentence being corrector incorrect, utilising the full sentence as context.", "labels": [], "entities": [{"text": "error detection", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7225514799356461}]}, {"text": "Using the error detection model, we then re-rank the N best hypotheses generated by statistical machine translation systems.", "labels": [], "entities": [{"text": "error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6754150539636612}, {"text": "statistical machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.6306343277295431}]}, {"text": "Our approach achieves state-of-the-art results on error correction for three different datasets, and it has the additional advantage of only using a small set of easily computed features that require no linguistic input.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical Error Correction (GEC) in nonnative text attempts to automatically detect and correct errors that are typical of those found in learner writing.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7677409996589025}]}, {"text": "High precision and good coverage of learner errors is important in the development of GEC systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.9990002512931824}, {"text": "GEC", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.8789121508598328}]}, {"text": "Phrase-based Statistical Machine Translation (SMT) approaches to GEC have attracted considerable attention in recent years as they have been shown to achieve state-of-the-art results.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.77304075161616}, {"text": "GEC", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9405302405357361}]}, {"text": "Given an ungrammatical input sentence, the task is formulated as \"translating\" it to its grammatical counterpart.", "labels": [], "entities": []}, {"text": "Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input.", "labels": [], "entities": [{"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9934707880020142}]}, {"text": "To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process) via reranking the SMT decoder's output ) to neural-network adaptation components to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9917621612548828}, {"text": "SMT", "start_pos": 319, "end_pos": 322, "type": "TASK", "confidence": 0.9859296679496765}]}, {"text": "In this paper, we propose an approach to N -best list re-ranking using neural sequence-labelling models.", "labels": [], "entities": []}, {"text": "N -best list re-ranking allows for fast experimentation since the decoding process remains unchanged and only needs to be performed once.", "labels": [], "entities": []}, {"text": "Crucially, it can be applied to any GEC system that can produce multiple alternative hypotheses.", "labels": [], "entities": []}, {"text": "More specifically, we train a neural compositional model for error detection that calculates the probability of each token in a sentence being corrector incorrect, utilising the full sentence as context.", "labels": [], "entities": [{"text": "error detection", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7207151353359222}]}, {"text": "Using the error detection model, we then re-rank the N best hypotheses generated by the SMT system.", "labels": [], "entities": [{"text": "error detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6876949816942215}, {"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9883432984352112}]}, {"text": "Detection models can be more fine-tuned to finer nuances of grammaticality and acceptability, and therefore better able to distinguish between correct and incorrect versions of a sentence.", "labels": [], "entities": []}, {"text": "Our approach achieves state-of-the-art results on GEC for three different datasets, and it has the additional advantage of using only a small set of easily computed features that require no linguistic information, in contrast to previous work that has utilised a large set of features in a supervised setting).", "labels": [], "entities": [{"text": "GEC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7888079881668091}]}], "datasetContent": [{"text": "We use the First Certificate in English (FCE) dataset, and the NUS Corpus of Learner English (NUCLE)) that was used in the CoNLL GEC shared tasks.", "labels": [], "entities": [{"text": "First Certificate in English (FCE) dataset", "start_pos": 11, "end_pos": 53, "type": "DATASET", "confidence": 0.5316504053771496}, {"text": "NUS Corpus of Learner English (NUCLE))", "start_pos": 63, "end_pos": 101, "type": "DATASET", "confidence": 0.9556121826171875}, {"text": "CoNLL GEC shared tasks", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.7191920131444931}]}, {"text": "Both datasets are annotated with the language errors committed and suggested corrections from expert annotators.", "labels": [], "entities": []}, {"text": "The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university students from an Asian language background.", "labels": [], "entities": []}, {"text": "We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators).", "labels": [], "entities": [{"text": "FCE train/test split", "start_pos": 18, "end_pos": 38, "type": "DATASET", "confidence": 0.924579119682312}, {"text": "NUCLE train/test set", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9533985733985901}, {"text": "CoNLL 2014", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.8938129246234894}]}, {"text": "We also use the publicly available Lang-8 corpus ( and the JHU FLuency-Extended GUG corpus (J-FLEG) (.", "labels": [], "entities": [{"text": "Lang-8 corpus", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9559965431690216}, {"text": "JHU FLuency-Extended GUG corpus (J-FLEG)", "start_pos": 59, "end_pos": 99, "type": "DATASET", "confidence": 0.8582452535629272}]}, {"text": "Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers.", "labels": [], "entities": []}, {"text": "JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9366481900215149}, {"text": "GEC evaluation", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.9301135241985321}]}, {"text": "We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and the FCE and CoNLL test sets.", "labels": [], "entities": [{"text": "FCE and CoNLL training sets", "start_pos": 22, "end_pos": 49, "type": "DATASET", "confidence": 0.7894591867923737}, {"text": "JFLEG", "start_pos": 131, "end_pos": 136, "type": "DATASET", "confidence": 0.8927860856056213}, {"text": "FCE and CoNLL test sets", "start_pos": 146, "end_pos": 169, "type": "DATASET", "confidence": 0.7749613583087921}]}, {"text": "For JFLEG, we use the 754 sentences on which have already benchmarked four leading GEC systems.", "labels": [], "entities": [{"text": "JFLEG", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7520800232887268}]}, {"text": "As our development set, we use a subset of the FCE training data.", "labels": [], "entities": [{"text": "FCE training data", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9052279392878214}]}, {"text": "All digits in the text are replaced with the character '0'.", "labels": [], "entities": []}, {"text": "Tokens that occur less than 2 times in the training data share an out-of-vocabulary (OOV) token embedding, whereas the characterlevel component still operates over the original tokens.", "labels": [], "entities": []}, {"text": "The model hyperparameters are tuned based on F 0.5 on the FCE development set (Section 3) and \u03b3 is set to 0.1.", "labels": [], "entities": [{"text": "F", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.9844480752944946}, {"text": "FCE development set", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9891169468561808}]}, {"text": "The model is optimised using Adam (, and training is stopped when F 0.5 does not improve on the development set over 5 epochs.", "labels": [], "entities": [{"text": "F", "start_pos": 66, "end_pos": 67, "type": "METRIC", "confidence": 0.9244848489761353}]}, {"text": "Token representations have size 300 and are initialised with pretrained word2vec embeddings trained on Google News ().", "labels": [], "entities": []}, {"text": "The character representations have size 50 and are initialised randomly.", "labels": [], "entities": []}, {"text": "The LSTM hidden layers have size 200 for each direction.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of our re-ranking approach on three different datasets: FCE, CoNLL 2014 and JFLEG.", "labels": [], "entities": [{"text": "FCE", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9708237051963806}, {"text": "CoNLL 2014", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.8415303528308868}, {"text": "JFLEG", "start_pos": 106, "end_pos": 111, "type": "DATASET", "confidence": 0.881240725517273}]}, {"text": "We report F 0.5 using the shared task's M 2 scorer (Dahlmeier and Ng, 2012b), and GLEU scores ().", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9788568615913391}, {"text": "M 2 scorer", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.93569944302241}, {"text": "GLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9989550113677979}]}, {"text": "The latter is based on a variant of BLEU () that is designed to reward correct edits and penalise ungrammatical ones.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9970676302909851}]}, {"text": "As mentioned in Section 5, we re-rank the 10-best lists of two SMT systems: Yuan et al.", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9915186762809753}]}, {"text": "(2016) (CAMB16 SMT ) and Junczys-Dowmunt and Grundkiewicz (2016) (AMU16 SMT ).", "labels": [], "entities": [{"text": "CAMB16 SMT", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.6772710680961609}, {"text": "AMU16 SMT", "start_pos": 66, "end_pos": 75, "type": "TASK", "confidence": 0.7388581037521362}]}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "We replicate the AMU16 SMT system to obtain the 10-best output, and report results using this We experimented with a small set of values (from 0 to 2 with increments of .1), though not exhaustively.", "labels": [], "entities": [{"text": "AMU16 SMT", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.7140362858772278}]}, {"text": "Compared to the original results on CoNLL reported in their paper (AMU16 SMT (reported) ), we obtain slightly lower performance.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.746789813041687}, {"text": "AMU16 SMT", "start_pos": 67, "end_pos": 76, "type": "TASK", "confidence": 0.7143151164054871}]}, {"text": "We can see that AMU16 SMT is the current state of the art on CoNLL, with an F 0.5 of 49.49.", "labels": [], "entities": [{"text": "AMU16 SMT", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.675017774105072}, {"text": "CoNLL", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.9621856808662415}, {"text": "F 0.5", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9866675138473511}]}, {"text": "On the other hand, CAMB16 SMT generalises better on FCE and JFLEG: 52.90 and 52.44 F 0.5 respectively.", "labels": [], "entities": [{"text": "CAMB16 SMT", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.4685621112585068}, {"text": "FCE", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7839173674583435}, {"text": "JFLEG", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.7657262682914734}]}, {"text": "The lower performance of AMU16 SMT can be attributed to the fact that it is tuned for the CoNLL shared task.", "labels": [], "entities": [{"text": "AMU16 SMT", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.676833301782608}, {"text": "CoNLL shared task", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.663955807685852}]}, {"text": "The current state of the art on FCE is a neural machine translation system, CAMB16, which is also the best model on JFLEG in terms of GLEU.", "labels": [], "entities": [{"text": "FCE", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.7788682579994202}, {"text": "neural machine translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7625056107838949}, {"text": "GLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.7173005938529968}]}, {"text": "The rest of the baselines we report are:: Ablation tests on the FCE test set when removing one feature of the re-ranking system at a time.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9982560276985168}, {"text": "FCE test set", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.9733591079711914}]}, {"text": "When using our LSTM detection model to rerank the 10-best list (+ LSTM), we can see that performance improves across all three datasets for both SMT systems.", "labels": [], "entities": [{"text": "LSTM detection", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7207421362400055}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9829022884368896}]}, {"text": "F 0.5 performance of CAMB16 SMT on FCE improves from 52.90 to 54.15, on CoNLL from 37.33 to 39.53, and on JF-LEG from 52.44 to 53.50 (the latter demonstrating that the detection model also helps with fluency edits).", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9786703586578369}, {"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.5810334086418152}, {"text": "FCE", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9466320276260376}, {"text": "CoNLL", "start_pos": 72, "end_pos": 77, "type": "DATASET", "confidence": 0.7385140061378479}, {"text": "JF-LEG", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.8436588644981384}]}, {"text": "This improved result is also better than the state of the art CAMB16 NMT on FCE.", "labels": [], "entities": [{"text": "CAMB16 NMT", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.7230192124843597}, {"text": "FCE", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.939588725566864}]}, {"text": "When looking at AMU16 SMT , we can see that reranking (+ LSTM) further improves the best result on CoNLL from 49.34 (replicated) to 49.66 F 0.5 , and there is a similar level of improvement for both FCE and JFLEG.", "labels": [], "entities": [{"text": "AMU16 SMT", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.8858785629272461}, {"text": "LSTM", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9543583989143372}, {"text": "CoNLL", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.7577426433563232}, {"text": "FCE", "start_pos": 199, "end_pos": 202, "type": "DATASET", "confidence": 0.9054765105247498}, {"text": "JFLEG", "start_pos": 207, "end_pos": 212, "type": "DATASET", "confidence": 0.9089332818984985}]}, {"text": "As a further experiment, we re-train our error detection model on the same training data as CAMB16 SMT (+ LSTM camb ).", "labels": [], "entities": [{"text": "error detection", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6576226502656937}, {"text": "CAMB16 SMT", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.8812694549560547}]}, {"text": "More specifically, we use the Cambridge Learner Corpus (CLC), a collection of learner texts of various proficiency levels, written in response to exam prompts and manually annotated with the errors committed (around 2M sentence pairs).", "labels": [], "entities": [{"text": "Cambridge Learner Corpus (CLC)", "start_pos": 30, "end_pos": 60, "type": "DATASET", "confidence": 0.9381573498249054}]}, {"text": "In, we can see that the detection model further improves performance across all datasets and SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9865541458129883}]}, {"text": "Compared to just doing SMT with CAMB16 SMT , re-ranking improves F 0.5 from 52.90 to 55.60 on FCE (performance increases further even though CAMB16 SMT 's training set includes a large set of FCE data), from 37.33 to 42.44 on CoNLL, and from 52.44 to 54.66 on JFLEG.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9944764971733093}, {"text": "CAMB16 SMT", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.7268370091915131}, {"text": "F 0.5", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.991959422826767}, {"text": "FCE", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.7831080555915833}, {"text": "FCE data", "start_pos": 192, "end_pos": 200, "type": "DATASET", "confidence": 0.8274309039115906}, {"text": "CoNLL", "start_pos": 226, "end_pos": 231, "type": "DATASET", "confidence": 0.9386155605316162}, {"text": "JFLEG", "start_pos": 260, "end_pos": 265, "type": "DATASET", "confidence": 0.9658756852149963}]}, {"text": "The largest improvement is on CoNLL (5%), which is likely because CoNLL is not included in the training set.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.6215866804122925}]}, {"text": "AMU16 SMT (replicated) is specifically tuned for CoNLL; nevertheless, the detection model also improves F 0.5 on CoNLL from 49.34 to 51.08.", "labels": [], "entities": [{"text": "AMU16 SMT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8966604471206665}, {"text": "CoNLL", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8649958968162537}, {"text": "F 0.5", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9941401183605194}, {"text": "CoNLL", "start_pos": 113, "end_pos": 118, "type": "DATASET", "confidence": 0.856913149356842}]}, {"text": "Re-ranking using a small set of detection-based features produces state-of-the-art results on all three datasets (we note that CAMB16 SMT generalises better across all).", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.4933359920978546}]}, {"text": "We next run ablation tests to investigate the extent to which each feature contributes to performance.", "labels": [], "entities": []}, {"text": "Results obtained on the FCE test set after excluding each of the features of the 'CAMB16 SMT + LSTM camb ' re-ranking system are presented in.", "labels": [], "entities": [{"text": "FCE test set", "start_pos": 24, "end_pos": 36, "type": "DATASET", "confidence": 0.9804415106773376}, {"text": "CAMB16 SMT + LSTM camb", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.7521799445152283}]}, {"text": "Overall, all features have a positive effect on performance, though the sentence probability feature does have the biggest impact: its removal is responsible fora 1.47 and 1.11 decrease of F 0.5 and GLEU respectively.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 189, "end_pos": 194, "type": "METRIC", "confidence": 0.9872539043426514}, {"text": "GLEU", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.9796444177627563}]}, {"text": "A similar pattern is observed on the other datasets too.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Token-level error detection performance of our detection models (LSTM FCE and LSTM) on  FCE and the two CoNLL 2014 test set annotations. Baseline LSTM FCE and LSTM FCE are trained only  on the public FCE training set.", "labels": [], "entities": [{"text": "Token-level error detection", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7323614855607351}, {"text": "FCE", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8948598504066467}, {"text": "CoNLL 2014 test set annotations", "start_pos": 114, "end_pos": 145, "type": "DATASET", "confidence": 0.9569941997528076}, {"text": "FCE training set", "start_pos": 210, "end_pos": 226, "type": "DATASET", "confidence": 0.824052651723226}]}, {"text": " Table 2: Using the neural sequence-labelling model for error detection ('+ LSTM' or '+ LSTM camb ') to  re-rank the 10-best lists of two SMT systems -Yuan et al. (2016) (CAMB16 SMT ) and Junczys-Dowmunt  and Grundkiewicz (2016) (AMU16 SMT ).", "labels": [], "entities": [{"text": "error detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.7007566392421722}, {"text": "SMT", "start_pos": 138, "end_pos": 141, "type": "TASK", "confidence": 0.9860408902168274}, {"text": "AMU16 SMT", "start_pos": 230, "end_pos": 239, "type": "TASK", "confidence": 0.7395869195461273}]}, {"text": " Table 3: Ablation tests on the FCE test set when  removing one feature of the re-ranking system at a  time.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9928479194641113}, {"text": "FCE test set", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9838105241457621}]}, {"text": " Table 4:  Re-ranking performance using  LSTM camb as the N -best list varies in size  from 1 to 10 for CAMB16 SMT and its oracle.", "labels": [], "entities": [{"text": "Re-ranking", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.7502198219299316}, {"text": "CAMB16 SMT", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.6904363632202148}]}, {"text": " Table 6: Error-type performance before and af- ter re-ranking on the FCE test set (largest impact  highlighted in bold; bottom part of the table dis- plays negative effects on performance).", "labels": [], "entities": [{"text": "Error-type", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9828621745109558}, {"text": "FCE test set", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9715285698572794}]}]}