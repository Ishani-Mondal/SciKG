{"title": [{"text": "Unsupervised Pretraining for Sequence to Sequence Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "This work presents a general unsuper-vised learning method to improve the accuracy of sequence to sequence (seq2seq) models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9987282156944275}]}, {"text": "In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data.", "labels": [], "entities": []}, {"text": "We apply this method to challenging benchmarks in machine translation and abstrac-tive summarization and find that it significantly improves the subsequent supervised models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8216872811317444}, {"text": "abstrac-tive summarization", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.4559476524591446}]}, {"text": "Our main result is that pretraining improves the generalization of seq2seq models.", "labels": [], "entities": []}, {"text": "We achieve state-of-the-art results on the WMT English\u2192German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation.", "labels": [], "entities": [{"text": "WMT English\u2192German task", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.6110963821411133}, {"text": "phrase-based machine translation", "start_pos": 109, "end_pos": 141, "type": "TASK", "confidence": 0.6205871403217316}, {"text": "neural machine translation", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.7484784523646036}]}, {"text": "Our method achieves a significant improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English\u2192German.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9973379969596863}, {"text": "WMT'14", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.9652253985404968}, {"text": "WMT'15 English\u2192German", "start_pos": 107, "end_pos": 128, "type": "DATASET", "confidence": 0.890764445066452}]}, {"text": "We also conduct human evaluations on abstractive summa-rization and find that our method outper-forms a purely supervised learning base-line in a statistically significant manner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence to sequence (seq2seq) models; \u02dc Neco and Forcada, 1997) are extremely effective on a variety of tasks that require a mapping between a variable-length input sequence to a variable-length output sequence.", "labels": [], "entities": []}, {"text": "The main weakness of sequence to sequence models, and deep networks in general, lies in the fact that they can easily overfit when the amount of supervised training data is small.", "labels": [], "entities": []}, {"text": "In this work, we propose a simple and effective technique for using unsupervised pretraining to improve seq2seq models.", "labels": [], "entities": []}, {"text": "Our proposal is to initialize both encoder and decoder networks with pretrained weights of two language models.", "labels": [], "entities": []}, {"text": "These pretrained weights are then fine-tuned with the labeled corpus.", "labels": [], "entities": []}, {"text": "During the fine-tuning phase, we jointly train the seq2seq objective with the language modeling objectives to prevent overfitting.", "labels": [], "entities": []}, {"text": "We benchmark this method on machine translation for English\u2192German and abstractive summarization on CNN and Daily Mail articles.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7415145635604858}, {"text": "abstractive summarization", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.6788778305053711}, {"text": "CNN and Daily Mail articles", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.8172093033790588}]}, {"text": "Our main result is that a seq2seq model, with pretraining, exceeds the strongest possible baseline in both neural machine translation and phrasebased machine translation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 107, "end_pos": 133, "type": "TASK", "confidence": 0.6896539926528931}, {"text": "phrasebased machine translation", "start_pos": 138, "end_pos": 169, "type": "TASK", "confidence": 0.7040752371152242}]}, {"text": "Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English\u2192German.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9992603659629822}, {"text": "WMT'14", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9747162461280823}, {"text": "WMT'15 English\u2192German", "start_pos": 94, "end_pos": 115, "type": "DATASET", "confidence": 0.8834184557199478}]}, {"text": "On human evaluations for abstractive summarization, we find that our model outperforms a purely supervised baseline, both in terms of correctness and in avoiding unwanted repetition.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.620529979467392}]}, {"text": "We also perform ablation studies to understand the behaviors of the pretraining method.", "labels": [], "entities": []}, {"text": "Our study confirms that among many other possible choices of using a language model in seq2seq with attention, the above proposal works best.", "labels": [], "entities": []}, {"text": "Our study also shows that, for translation, the main gains come from the improved generalization due to the pretrained features.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9897582530975342}]}, {"text": "For summarization, pretraining the encoder gives large improvements, suggesting that the gains come from the improved optimization of the encoder that has been unrolled for hundreds of timesteps.", "labels": [], "entities": [{"text": "summarization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9853341579437256}]}, {"text": "On both tasks, our proposed method always improves generalization on the test sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following section, we apply our approach to two important tasks in seq2seq learning: machine translation and abstractive summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8210124373435974}, {"text": "abstractive summarization", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.6359950602054596}]}, {"text": "On each task, we compare against the previous best systems.", "labels": [], "entities": []}, {"text": "We also perform ablation experiments to understand the behavior of each component of our method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: English\u2192German performance on WMT test sets. Our pretrained model outperforms all other  models. Note that the model without pretraining uses the LM objective.", "labels": [], "entities": [{"text": "WMT test sets", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8154816230138143}]}, {"text": " Table 2: Results on the anonymized CNN/Daily Mail dataset.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9214163780212402}]}]}