{"title": [{"text": "Exploiting Word Internal Structures for Generic Chinese Sentence Representation", "labels": [], "entities": [{"text": "Generic Chinese Sentence Representation", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.5757869780063629}]}], "abstractContent": [{"text": "We introduce a novel mixed character-word architecture to improve Chinese sentence representations, by utilizing rich semantic information of word internal structures.", "labels": [], "entities": [{"text": "Chinese sentence representations", "start_pos": 66, "end_pos": 98, "type": "TASK", "confidence": 0.5989180306593577}]}, {"text": "Our architecture uses two key strategies.", "labels": [], "entities": []}, {"text": "The first is a mask gate on characters , learning the relation among characters in a word.", "labels": [], "entities": []}, {"text": "The second is a max-pooling operation on words, adaptively finding the optimal mixture of the atomic and compositional word representations.", "labels": [], "entities": []}, {"text": "Finally, the proposed architecture is applied to various sentence composition models, which achieves substantial performance gains over baseline models on sentence similarity task.", "labels": [], "entities": [{"text": "sentence composition", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7093715220689774}]}], "introductionContent": [{"text": "To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems.", "labels": [], "entities": [{"text": "understand the meaning of a sentence", "start_pos": 3, "end_pos": 39, "type": "TASK", "confidence": 0.7445944249629974}]}, {"text": "Obviously, this requires a good representation of the meaning of a sentence.", "labels": [], "entities": []}, {"text": "Recently, neural network based methods have shown advantage in learning task-specific sentence representations ( and generic sentence representations (.", "labels": [], "entities": []}, {"text": "To learn generic sentence representations that perform robustly across tasks as effective as word representations, proposes an architecture based on the supervision from the Paraphrase Database (.", "labels": [], "entities": []}, {"text": "Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representation-: An example sentence that consists of five words as \"\u642d\u4e58(take) \u51fa\u79df\u8f66(taxi) \u5230(to) \u8679 \u6865(Hongqiao) \u673a\u573a(airport)\".", "labels": [], "entities": []}, {"text": "Most of these words are compositional, namely word \"\u642d\u4e58\" consists of characters \"\u642d(take)\" and \"\u4e58(ride)\", word \"\u51fa \u79df\u8f66\" constitutes characters \"\u51fa(out)\", \"\u79df(rent)\" and \"\u8f66(car)\", and word \"\u673a\u573a\" is composed of characters \"\u673a(machine)\" and \"\u573a(field)\".", "labels": [], "entities": []}, {"text": "The color depth represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning.", "labels": [], "entities": []}, {"text": "The deeper color means more contributions. s. In contrast to English, Chinese characters contain rich information and are capable of indicating semantic meanings of words.", "labels": [], "entities": []}, {"text": "As illustrated in, the internal structures of Chinese words express two characteristics: (1) Each character in a word contribute differently to the compositional word meaning () such as the word \"\u51fa\u79df\u8f66(taxi)\".", "labels": [], "entities": []}, {"text": "The first two characters \"\u51fa\u79df(rent)\" are descriptive modifiers of the last character \"\u8f66(car)\", and make the last character play the most important role in expressing word meaning.", "labels": [], "entities": [{"text": "expressing word meaning", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.8155507445335388}]}, {"text": "(2) The atomic and compositional representations contribute differently to different types of words.", "labels": [], "entities": []}, {"text": "For instance, the meaning of \"\u673a \u573a(airport)\", a low-frequency word, can be better expressed by the compositional word representation, while the non-transparent word \"\u8679\u6865(Hongqiao)\" is better expressed by the atomic word representation.", "labels": [], "entities": []}, {"text": "The word internal structures have been proven to be useful for Chinese word representations.", "labels": [], "entities": [{"text": "Chinese word representations", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.7021885712941488}]}, {"text": "proposes a character-enhanced word representation model by adding the averaged character embeddings to the word embedding.", "labels": [], "entities": []}, {"text": "extends this work by using weighted character embeddings.", "labels": [], "entities": []}, {"text": "The weights are cosine similarities between embeddings of a word's English translation and its constituent characters' English translations.", "labels": [], "entities": []}, {"text": "However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship.", "labels": [], "entities": []}, {"text": "Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning.", "labels": [], "entities": []}, {"text": "Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (.", "labels": [], "entities": []}, {"text": "It should be noted that these studies are not focus on learning sentence embeddings.", "labels": [], "entities": []}, {"text": "In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models.", "labels": [], "entities": []}, {"text": "In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations.", "labels": [], "entities": []}, {"text": "Experiments on sentence similarity (as well as word similarity) demonstrate the effectiveness of our method.", "labels": [], "entities": [{"text": "sentence similarity", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7063832730054855}, {"text": "word similarity", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7164891213178635}]}, {"text": "In addition, as there are no publicly available Chinese sentence similarity datasets, we build a dataset to directly test the quality of sentence representations.", "labels": [], "entities": []}, {"text": "The code and data will be publicly released.", "labels": [], "entities": []}], "datasetContent": [{"text": "We construct four groups of models (G1\u02dcG4) which serve as baselines to test the proposed mixed character-word models (G5).", "labels": [], "entities": []}, {"text": "Group G1 includes six baseline models, which have shown impressive performance in English.", "labels": [], "entities": []}, {"text": "The first two are averaged word vectors and averaged character vectors.", "labels": [], "entities": []}, {"text": "Followed by PV-DM model which uses auxiliary vectors to represent sentences and trains them together with word vectors, and FastSent model which utilizes a encoder-decoder model and encodes sentences as averaged word embeddings.", "labels": [], "entities": []}, {"text": "The last two are Char-CNN model which is CNN model with character n-gram filters, and Charagram model which represents sentences with a character n-gram count vector.", "labels": [], "entities": []}, {"text": "Group G2 are the sentence representation models proposed by, which utilize only word level information.", "labels": [], "entities": [{"text": "sentence representation", "start_pos": 17, "end_pos": 40, "type": "TASK", "confidence": 0.7007784396409988}]}, {"text": "We also compared our method with word representation models of and in Group G3 and G4 respectively, by incorporate them into five sentence composition models in Section 2.2.", "labels": [], "entities": [{"text": "word representation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7148535996675491}]}, {"text": "In all models, the word and character embeddings are initialized with 300-dimension vectors trained by Skip-gram model () on a corpus with 3 billion Chinese words.", "labels": [], "entities": []}, {"text": "All models are implemented with) and Lasagne (, and optimized using Adam ().", "labels": [], "entities": []}, {"text": "The hyper-parameters 1 are selected by testing different values and evaluating their effects on the development set.", "labels": [], "entities": []}, {"text": "In this paper, we run all experiments 5 times and report the mean values.", "labels": [], "entities": []}, {"text": "The training dataset is a set of paraphrase pairs in which two sentences in each pair represent the same meanings.", "labels": [], "entities": []}, {"text": "Specifically, we extract Chinese paraphrases in machine translation evaluation corpora NIST2003 2 and CWMT2015 3 . Moreover, we select aligned sub-sentence pairs between paraphrases to enlarge the training corpus.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.7594859600067139}, {"text": "NIST2003", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.6337910294532776}]}, {"text": "Specifically, we first segment the sentences into sub-sentences according to punctuations of comma, semicolon, colon, question mark, ellipses, and periods.", "labels": [], "entities": []}, {"text": "Then we pair all sub-sentences between a paraphrase and select sub-sentence pairs (s 1 , s 2 ) which satisfy the following two constraints: (1) the number of overlapping words of sub-sentence s 1 and s 2 should meet the condition: 0.9 > len(overlap(s 1 , s 2 ))/min(len(s 1 ), len(s 2 )) > 0.2, where len(s) denotes the number of words in sentence s; (2) the relative length of sub-sentence should meet the condition: max(len(s 1 ), len(s 2 ))/min(len(s 1 ), len(s 2 )) <= 2.", "labels": [], "entities": [{"text": "max", "start_pos": 418, "end_pos": 421, "type": "METRIC", "confidence": 0.9542573094367981}]}, {"text": "Finally, we get 30,846 paraphrases (18,187 paraphrases from NIST including 11,413 sub-sentence pairs, and 12,659 paraphrases from CWMT which include 7,912 sub-sentence pairs).", "labels": [], "entities": [{"text": "NIST", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.9320235252380371}, {"text": "CWMT", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9243901968002319}]}, {"text": "We also build the testing dataset, which are sentence pairs collocated with human similarity ratings.", "labels": [], "entities": []}, {"text": "We choose candidate sentences from the People's Daily and Baidu encyclopedia corpora.", "labels": [], "entities": [{"text": "People's Daily and Baidu encyclopedia corpora", "start_pos": 39, "end_pos": 84, "type": "DATASET", "confidence": 0.8087385467120579}]}, {"text": "To assure sentence pairs to be representative of the full variation in semantic similarity, we choose high similarity sentence pairs and then randomly pair the single sentences to construct low similarity sentence pairs.", "labels": [], "entities": []}, {"text": "To collect human similarity ratings for sentence pairs, we use online questionnaire and follow the gold standard to guide the rating process of participants.", "labels": [], "entities": []}, {"text": "The subjects are paid 7 cents for rating each sentence pair within a range of 0 5 score.", "labels": [], "entities": []}, {"text": "In total, we obtain 104 valid questionnaires and every sentence pair is evaluated by average 8 persons.", "labels": [], "entities": []}, {"text": "We use the average subjects' ratings for one paraphrase as its final similarity score, and the higher score means that the two sentences have more similar meaning.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.9377667903900146}]}, {"text": "We then randomly partition the datasets into test and development splits in 9:1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation coefficients of model predic- tions with subject similarity ratings on Chinese  sentence similarity task. The bold data refers to  best among models with same composition func- tion.", "labels": [], "entities": [{"text": "Chinese  sentence similarity task", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.5678521916270256}]}, {"text": " Table 2: Correlation coefficients of model predic- tions with subject similarity ratings on Chinese  word similarity task, where G2 \u223c G5 are the same  as in", "labels": [], "entities": []}]}