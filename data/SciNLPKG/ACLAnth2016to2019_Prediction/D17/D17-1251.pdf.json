{"title": [{"text": "Never Abandon Minorities: Exhaustive Extraction of Bursty Phrases on Microblogs Using Set Cover Problem", "labels": [], "entities": [{"text": "Exhaustive Extraction of Bursty Phrases", "start_pos": 26, "end_pos": 65, "type": "TASK", "confidence": 0.7449263095855713}]}], "abstractContent": [{"text": "We propose a language-independent data-driven method to exhaustively extract bursty phrases of arbitrary forms (e.g., phrases other than simple noun phrases) from microblogs.", "labels": [], "entities": []}, {"text": "The burst (i.e., the rapid increase of the occurrence) of a phrase causes the burst of overlapping N-grams including incomplete ones.", "labels": [], "entities": []}, {"text": "In other words, bursty incomplete N-grams inevitably overlap bursty phrases.", "labels": [], "entities": []}, {"text": "Thus, the proposed method performs the extraction of bursty phrases as the set cover problem in which all bursty N-grams are covered by a minimum set of bursty phrases.", "labels": [], "entities": []}, {"text": "Experimental results using Japanese Twitter data showed that the proposed method outper-formed word-based, noun phrase-based, and segmentation-based methods both in terms of accuracy and coverage.", "labels": [], "entities": [{"text": "Japanese Twitter data", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.6768070161342621}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.9988279938697815}]}], "introductionContent": [{"text": "Trends on microblogs reflect manifold real-world events including natural disaster, new product launch, television broadcasting, public speech, airplane accident, scandal and national holiday.", "labels": [], "entities": []}, {"text": "To catch realworld events, not a few researchers and practitioners have sought ways to detect trends on microblogs.", "labels": [], "entities": []}, {"text": "Trend detection often involves bursty phrase extraction, i.e., extracting phrases of which occurrence rate in microblog texts posted within a certain period of time (and from a certain location) is much higher than that of the normal state.", "labels": [], "entities": [{"text": "Trend detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9651094973087311}, {"text": "phrase extraction", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7419604361057281}]}, {"text": "Extracted bursty phrases are directly used as trends as Twitter 1 officially provides, or sent to higher-order processes such as clustering and event labeling.", "labels": [], "entities": [{"text": "event labeling", "start_pos": 144, "end_pos": 158, "type": "TASK", "confidence": 0.6208863258361816}]}, {"text": "Bursty phrases on microblogs are likely to be noun phrases, but sometimes deviate from such standards.", "labels": [], "entities": []}, {"text": "The title of a movie, song, game or any creation can bean arbitrary form like along and/or general phrase (e.g., Spielberg's movie \"catch me if you can\", the Beatles' song \"let it be\") . A memorable phrase (e.g., Steve Jobs's phrase \"stay hungry, stay foolish\") can also be a bursty phrase on microblogs.", "labels": [], "entities": []}, {"text": "Even numbers and symbols can be potentially bursty phrases (e.g., \"1984\" can be a novel, \"!!!\" can bean artist).", "labels": [], "entities": []}, {"text": "Any filtering rule such as stop word removal, part-of-speech (POS) tag restrictions, or length limit can mistakenly filter out bursty phrases.", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7037834127744039}, {"text": "length limit", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.9243507981300354}]}, {"text": "Extracting irregularly-formed bursty phrases as described in the previous paragraph is difficult since no restriction can be used anymore to filter out incomplete N-grams.", "labels": [], "entities": [{"text": "Extracting irregularly-formed bursty phrases", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8767998814582825}]}, {"text": "However, they are rare and little influence the overall accuracy even if they are correctly extracted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993433356285095}]}, {"text": "Not only that, tackling such difficult and rare cases easily leads to extracting many incomplete N-grams and deteriorating the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9993091821670532}]}, {"text": "Almost all existing work has therefore ignored difficult and rare cases, and concentrated on extracting simple phrases (e.g., unigrams, bi-grams, tri-grams, or noun phrases identified by POS taggers).", "labels": [], "entities": []}, {"text": "By sacrificing minorities, most bursty phrases can be extracted with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9922353625297546}]}, {"text": "Thus, irregularly-formed phrases have been abandoned in bursty phrase extraction (and alike in many text analysis tasks).", "labels": [], "entities": [{"text": "bursty phrase extraction", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6444613138834635}, {"text": "text analysis tasks", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8037439982096354}]}, {"text": "In this work, we aim to accurately and exhaustively extract bursty phrases of arbitrary forms from microblogs.", "labels": [], "entities": []}, {"text": "The challenge here is: How do we avoid extracting bursty incomplete N-grams without introducing any filtering rule?", "labels": [], "entities": []}, {"text": "To solve this challenging problem, we propose a set cover-based method.", "labels": [], "entities": []}, {"text": "We found that the: Representative trend detection methods based on bursty phrases.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed method using two weeks of Japanese Twitter data.", "labels": [], "entities": [{"text": "Japanese Twitter data", "start_pos": 52, "end_pos": 73, "type": "DATASET", "confidence": 0.8133223454157511}]}, {"text": "We created a dataset using Twitter Streaming API statuses/sample.", "labels": [], "entities": []}, {"text": "We chose Japanese as a language because it was one of popular languages used in Twitter and because it has no word boundary and finding phrases is difficult compared to space-delimited languages such as English.", "labels": [], "entities": []}, {"text": "We specifically collected 15 days of tweets from September 30 to . For each day from October 1 to 14, we extracted bursty phrases in reference to the previous day.", "labels": [], "entities": []}, {"text": "To maximally alleviate the influence of autogenerated contents such as tweets posted by bots and spammers, we filtered out them.", "labels": [], "entities": []}, {"text": "Detecting bots and spammers () is a nontrivial research task and out of the scope of this paper.", "labels": [], "entities": [{"text": "Detecting bots and spammers", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8684648871421814}]}, {"text": "In this experiment, we used simple but effective heuristics.", "labels": [], "entities": []}, {"text": "First, we only used tweets posted by Twitter official clients 5 because they were mainly used by normal users.", "labels": [], "entities": []}, {"text": "Second, we discarded tweets including URLs because most spammers tried to lure users to visit their websites.", "labels": [], "entities": []}, {"text": "Third, retweets (actions to propagate someone's tweets) were discarded.", "labels": [], "entities": []}, {"text": "The maximum and minimum numbers of remaining tweets per day were 326,002 (Oct. 2) and 253,044 (Oct. 13), respectively.", "labels": [], "entities": []}, {"text": "Additionally, we deleted hashtags (starting by #) and mentions (starting by @) from tweets.", "labels": [], "entities": []}, {"text": "Note that the degree of the burst for URLs, hashtags, and mentions can be independently measured.", "labels": [], "entities": []}, {"text": "We concentrated on extracting bursty phrases from plain texts.", "labels": [], "entities": [{"text": "extracting bursty phrases from plain texts", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.8155598839124044}]}, {"text": "To create the ground truth, we mixed N-grams extracted with all methods and then manually annotated each N-gram by checking its real usage in tweets.", "labels": [], "entities": []}, {"text": "While most N-grams were easily judged as complete phrase (i.e., correct) or incomplete N-grams (i.e., incorrect), a few N-grams were difficult to judge (e.g., a last name that was not frequently used to indicate the person in tweets).", "labels": [], "entities": []}, {"text": "We annotated such N-grams as maybe correct and regarded as correct in the strict case and incorrect in the loose case when measuring evaluation metrics.", "labels": [], "entities": []}, {"text": "In the proposed method (Proposed), we processed Japanese texts as sequences of characters.", "labels": [], "entities": []}, {"text": "Default threshold parameters \u03b8 and \u03f5 were set to 10 and 0.5, respectively.", "labels": [], "entities": []}, {"text": "Comparative methods included the word-based method (Word), noun phrase-based method (NP), and segmentation-based method (Segment) (.", "labels": [], "entities": []}, {"text": "Because these methods require word breaking, we used MeCab () (version 0.996, ipadic as a dictionary), a Japanese morphological analyzer.", "labels": [], "entities": [{"text": "word breaking", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.7970279157161713}]}, {"text": "The word-based method uses all self-sufficient words as candidate phrases.", "labels": [], "entities": []}, {"text": "The noun phrase-based method regards concatenated successive nouns as a candidate phrase.", "labels": [], "entities": []}, {"text": "In word-based and noun phrase-based methods, the dictionary or vocabulary significantly affects the performance.", "labels": [], "entities": []}, {"text": "Thus, we also used neologd () (version v0.0.5 updated at May 2, 2016), a neologism dictionary extracted from many language resources on the web, as an additional dictionary (+Dic).", "labels": [], "entities": []}, {"text": "The segmentationbased method detects segments (chunks) from a sequence of words as candidate phrases.", "labels": [], "entities": []}, {"text": "The segmentation model was constructed using three    months of tweets (from July 1 to Sept. and Japanese Wikipedia dump data (as of Oct. 1, 2016).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9807222485542297}, {"text": "Japanese Wikipedia dump data", "start_pos": 97, "end_pos": 125, "type": "DATASET", "confidence": 0.7272718697786331}]}, {"text": "We employed precision and min-z-score of top K bursty phrases (K was set to 10, 20, 30, 40, or 50) as evaluation metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9994237422943115}]}, {"text": "We measured the precision both in strict and loose cases based on the ground truth labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9995961785316467}]}, {"text": "The minz-score (the minimum of the z-score, computed from raw document frequency) was introduced to evaluate how much the top K output ranked by zscores included highly bursty phrases.", "labels": [], "entities": []}, {"text": "To increase the min-z-score, all the top K phrases should have high z-scores and hence highly bursty phrases should not be ignored.", "labels": [], "entities": []}, {"text": "Thus the min-z-score can evaluate the coverage of extracted bursty phrases using a fixed size of the output.", "labels": [], "entities": []}, {"text": "Higher precision and min-z-score indicate that the method can more accurately and exhaustively extract bursty phrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9994217157363892}]}, {"text": "show the precision of bursty phrase extraction.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9989733695983887}, {"text": "phrase extraction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.714880496263504}]}, {"text": "It was surprisingly that the proposed method achieved higher precision than nounphrase based methods, which were supposed to be safety by sacrificing irregularly-formed phrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9978415966033936}]}, {"text": "The precision of the proposed method for top 50 bursty phrases was 97.3% (correct phrases were 681 out of 700) in the strict case and 99.1% (694 out of 700) in the loose case.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996912479400635}]}, {"text": "The precision for top 10 bursty phrases was 99.3% (139 out of 140) even in the strict case.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996355772018433}]}, {"text": "The results demonstrate that the burst information alone can accurately find the boundary of bursty phrases using the set cover problem.", "labels": [], "entities": []}, {"text": "Error cases of the proposed method were largely classified into two: base sequences of diversified expressions and phrases with stronglycorrelated attached characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average precision of top K bursty  phrases (strict case).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9053203463554382}]}, {"text": " Table 3: Average precision of top K bursty  phrases (loose case).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9120515584945679}]}, {"text": " Table 4: Average min-z-score of top K bursty  phrases.", "labels": [], "entities": [{"text": "Average min-z-score", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9096352756023407}]}, {"text": " Table 5: Number of bursty phrases extracted with  the proposed method (K = 10) but completely  missed with comparative methods.", "labels": [], "entities": []}, {"text": " Table 7: Average min-z-score of top K bursty  phrases with different parameter settings in the  proposed method.", "labels": [], "entities": []}, {"text": " Table 8: Example of top 10 bursty phrases in  Japanese (Oct. 1, 2016).", "labels": [], "entities": []}]}