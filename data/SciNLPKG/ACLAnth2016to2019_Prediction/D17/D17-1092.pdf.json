{"title": [{"text": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "labels": [], "entities": [{"text": "Temporal Information Extraction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8021697402000427}, {"text": "Question Answering", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.8165394067764282}]}], "abstractContent": [{"text": "In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text.", "labels": [], "entities": []}, {"text": "Using the shortest dependency path between entities as input, the same architecture is implemented to extract intra-sentence, cross-sentence, and document creation time relations.", "labels": [], "entities": []}, {"text": "A \"double-checking\" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes.", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9988947510719299}]}, {"text": "An efficient pruning algorithm resolves conflicts globally.", "labels": [], "entities": []}, {"text": "Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin.", "labels": [], "entities": [{"text": "QA-TempEval (SemEval2015 Task 5)", "start_pos": 13, "end_pos": 45, "type": "DATASET", "confidence": 0.7524547477563223}]}, {"text": "We also conduct intrinsic evaluation and post state-of-the-art results on Timebank-Dense.", "labels": [], "entities": [{"text": "Timebank-Dense", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.9923850297927856}]}], "introductionContent": [{"text": "Recovering temporal information from text is essential to many text processing tasks that require deep language understanding, such as answering questions about the timeline of events or automatically producing text summaries.", "labels": [], "entities": [{"text": "Recovering temporal information from text", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.885878324508667}, {"text": "answering questions about the timeline of events", "start_pos": 135, "end_pos": 183, "type": "TASK", "confidence": 0.8173248171806335}]}, {"text": "This work presents intermediate results of an effort to build a temporal reasoning framework with contemporary deep learning techniques.", "labels": [], "entities": []}, {"text": "Until recently, there has been remarkably few attempts to evaluate temporal information extraction (TemporalIE) methods in context of downstream applications that require reasoning over the temporal representation.", "labels": [], "entities": [{"text": "temporal information extraction", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.6676693161328634}]}, {"text": "One recent effort to conduct such evaluation was SemEval2015 Task 5, a.k.a.", "labels": [], "entities": [{"text": "SemEval2015 Task 5", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.795390784740448}]}, {"text": "QA-TempEval (, which used question answering (QA) as the target application.", "labels": [], "entities": [{"text": "QA-TempEval", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.7679088115692139}, {"text": "question answering (QA)", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7919049203395844}]}, {"text": "QA-TempEval evaluated systems producing TimeML () annotation based on how well their output could be used in QA.", "labels": [], "entities": []}, {"text": "We believe that application-based evaluation of TemporalIE should eventually completely replace the intrinsic evaluation if we are to make progress, and therefore we evaluated our techniques mainly using QA-TempEval setup.", "labels": [], "entities": []}, {"text": "Despite the recent advances produced by multilayer neural network architectures in a variety of areas, the research community is still struggling to make neural architectures work for linguistic tasks that require long-distance dependencies (such as discourse parsing or coreference resolution).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 250, "end_pos": 267, "type": "TASK", "confidence": 0.718028262257576}, {"text": "coreference resolution", "start_pos": 271, "end_pos": 293, "type": "TASK", "confidence": 0.8954153954982758}]}, {"text": "Our goal was to see if a relatively simple architecture with minimal capacity for retaining information was able to incorporate the information required to identify temporal relations in text.", "labels": [], "entities": []}, {"text": "Specifically, we use several simple LSTMbased components to recover ordering relations between temporally relevant entities (events and temporal expressions).", "labels": [], "entities": []}, {"text": "These components are fairly uniform in their architecture, relying on dependency relations recovered with a very small number of mature, widely available processing tools, and require minimal engineering otherwise.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first attempt to apply such simplified techniques to the TemporalIE task, and we demonstrate this streamlined architecture is able to outperform state-of-the-art results on a temporal QA task with a large margin.", "labels": [], "entities": []}, {"text": "In order to demonstrate generalizability of our proposed architecture, we also evaluate it intrinsically using TimeBank-Dense 1 ().", "labels": [], "entities": []}, {"text": "TimeBank-Dense annotation aims to approximate a complete temporal relation graph by including all intra-sentential relations, all relations between adjacent sentences, and all relations with document creation time.", "labels": [], "entities": []}, {"text": "Although our system was not optimized for such a paradigm, and this data is quite different in terms of both the annotation scheme and the evaluation method, we obtain state-of-the-art results on this corpus as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used QA-TempEval (SemEval 2015 Task 5) 2 data and evaluation methods in our experiments.", "labels": [], "entities": [{"text": "QA-TempEval (SemEval 2015 Task 5)", "start_pos": 8, "end_pos": 41, "type": "DATASET", "confidence": 0.7240788170269558}]}, {"text": "The training set contains 276 annotated TimeML files, mostly news articles from major agencies or Wikinews from late 1990s to early 2000s.", "labels": [], "entities": [{"text": "TimeML files", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9199807345867157}]}, {"text": "This data contains annotations for events, temporal expressions (referred to as TIMEXes), and temporal relations (referred to as TLINKs).", "labels": [], "entities": [{"text": "TIMEXes", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.8192067742347717}]}, {"text": "The test set contains unannotated files in three genres: 10 news articles composed in 2014, 10 Wikipedia articles about world history, and 8 blogs entries from early 2000s.", "labels": [], "entities": []}, {"text": "In QA-TempEval, evaluation is done via a QA toolkit which contains yes/no questions about temporal relations between two events or an event and a temporal expression.", "labels": [], "entities": []}, {"text": "QA evaluation is not available for most of the training data except for 25 files, for which 79 questions are available.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.5997385680675507}]}, {"text": "We used used this subset of the training data for validation.", "labels": [], "entities": [{"text": "validation", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.9651281237602234}]}, {"text": "The test set contains unannotated files, so QA is the only way to measure the performance.", "labels": [], "entities": [{"text": "QA", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.8303274512290955}]}, {"text": "The total of 294 questions is available for the test data, see.", "labels": [], "entities": []}, {"text": "We also use TimeBank-Dense dataset, which contains a subset of the documents in QATempEval.", "labels": [], "entities": [{"text": "TimeBank-Dense dataset", "start_pos": 12, "end_pos": 34, "type": "DATASET", "confidence": 0.9525827467441559}]}, {"text": "In TimeBank-Dense, all entity pairs in the same sentence or in consecutive sentences are labeled.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 3, "end_pos": 17, "type": "DATASET", "confidence": 0.9202182292938232}]}, {"text": "If there is no information about the relation between two entities, it is labeled as \"vague\".", "labels": [], "entities": []}, {"text": "We follow the experimental setup in (), which splits the corpus into training/validation/test sets of 22, 5, and 9 documents, respectively.", "labels": [], "entities": []}, {"text": "In this section, we first describe the model selection experiments on QA-TempEval validation data, selectively highlighting results of interest.", "labels": [], "entities": [{"text": "QA-TempEval validation data", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.7818440198898315}]}, {"text": "We then present the results obtained with the optimized model on the QA-TempEval task and on TimeBank-Dense.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.9297080636024475}]}, {"text": "As mentioned before, \"gold\" TLINKs are sparse, so we cannot merely rely on the F1 scores on validation data to do model selection.", "labels": [], "entities": [{"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9981839060783386}]}, {"text": "Instead, we used the QA toolkit.", "labels": [], "entities": [{"text": "QA toolkit", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8337894082069397}]}, {"text": "The toolkit contains 79 yesno questions about temporal relations between entities in the validation data.", "labels": [], "entities": []}, {"text": "Originally, only 6 questions have \"no\" as the correct answer, and 1 question is listed as \"unknown\".", "labels": [], "entities": []}, {"text": "After investigating the questions and answers, however, we found some errors and typos 6 . After fixing the errors, there are 7 no-questions and 72 yes-questions in total.", "labels": [], "entities": []}, {"text": "All evaluations are performed on the fixed data.", "labels": [], "entities": []}, {"text": "The evaluation tool draws answers from the annotations only.", "labels": [], "entities": []}, {"text": "If an entity (event or TIMEX) involved in a question is not annotated, or the TLINK cannot be found, the question will then be counted as not answered.", "labels": [], "entities": [{"text": "TLINK", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.9162720441818237}]}, {"text": "There is noway for participants to give an answer directly, other than de-6 Question 24 from XIE19980821.0077.tml should be answered with \"yes\", but the answer key contains a typo \"is\".", "labels": [], "entities": [{"text": "XIE19980821.0077.tml", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.7070352435112}]}, {"text": "Question 34 from APW19980219.0476.tml has BE-FORE that should be replaced with AFTER.", "labels": [], "entities": [{"text": "APW19980219.0476.tml", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.9202192425727844}, {"text": "BE-FORE", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9993278980255127}, {"text": "AFTER", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9950770735740662}]}, {"text": "Question 29 from XIE19980821.0077.tml has \"unknown\" in the answer key, but after reading the article, we believe the correct answer is \"no\".", "labels": [], "entities": [{"text": "XIE19980821.0077.tml", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.8861342668533325}]}, {"text": "The program generates Timegraphs to infer relations from the annotated TLINKs.", "labels": [], "entities": []}, {"text": "As a result, relations without explicit TLINK labels can still be used if they can be inferred from the annotations.", "labels": [], "entities": []}, {"text": "The QA toolkit uses the following evaluation measures: shows the results produced by different models on the validation data.", "labels": [], "entities": []}, {"text": "The results of the four systems above the first horizontal line are provided by the task organizer.", "labels": [], "entities": []}, {"text": "Among them, the top two use annotations provided by human experts.", "labels": [], "entities": []}, {"text": "As we can see, the precision is very high, both above 0.90.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9997678399085999}]}, {"text": "Our models cannot reach that precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9988141059875488}]}, {"text": "In spite of the lower precision, automated systems can have much higher coverages i.e. answer a lot more questions.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9989810585975647}, {"text": "coverages", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9813660383224487}]}, {"text": "As a starting point, we evaluated the validation files in their original form, and the results are shown as \"orig.", "labels": [], "entities": []}, {"text": "The precision was good, but with very low coverage.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992495179176331}, {"text": "coverage", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9599416851997375}]}, {"text": "This supports our claim that the TLINKs provided by the training/validation files are not complete.", "labels": [], "entities": []}, {"text": "We also tried using the event and TIMEX tags from the validation data, but performing TLINK classification with our system.", "labels": [], "entities": [{"text": "TIMEX", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9279835224151611}, {"text": "TLINK classification", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.7064440697431564}]}, {"text": "As shown with \"orig.", "labels": [], "entities": []}, {"text": "tags TEA tlinks\" in the table, now the coverage rises to 64 (or 0.81), and the overall F1 score reaches 0.52.", "labels": [], "entities": [{"text": "TEA", "start_pos": 5, "end_pos": 8, "type": "METRIC", "confidence": 0.949647843837738}, {"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9967623949050903}, {"text": "F1 score", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9825214743614197}]}, {"text": "The TEA-initial system uses our own annotators.", "labels": [], "entities": [{"text": "TEA-initial", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.5629265904426575}]}, {"text": "The performance is similar, with a slight improvement in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9989808201789856}]}, {"text": "This result shows our event and TIMEX tags work well, and are not inferior to the ones provided by the training data.", "labels": [], "entities": [{"text": "TIMEX", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.8826889395713806}]}, {"text": "The double-checking technique boosts the coverage a lot, probably because we allow positive results to veto NO-LINKs.", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9799298048019409}]}, {"text": "Combining doublechecking with the pruning technique yields the best results, with F1 score 0.58, answering 42 out of 79 questions correctly.", "labels": [], "entities": [{"text": "F1 score 0.58", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9808260202407837}]}, {"text": "In order to validate the choice of the dependency path-based context, we also experimented with a conventional flat context window, using the same hyperparameters.", "labels": [], "entities": []}, {"text": "Every entity is represented by a 11-word window, with the entity mention in the middle.", "labels": [], "entities": []}, {"text": "If two entities are near each other, their windows are cut short before reaching the other entity.", "labels": [], "entities": []}, {"text": "Using the flat context instead of dependency paths yields a much weaker performance.", "labels": [], "entities": []}, {"text": "This confirms our hypothesis that syntactic dependencies represent temporal relations better than word windows.", "labels": [], "entities": []}, {"text": "However, it should be noted that we did not separately optimize the models for the flat context setting.", "labels": [], "entities": []}, {"text": "The large performance drop we saw from switching to flat context did not warrant performing a separate parameter search.", "labels": [], "entities": []}, {"text": "We also wanted to check whether a comprehensive annotation of TLINKs in the training data can improve model performance on the QA task.", "labels": [], "entities": []}, {"text": "We therefore trained our model on TimeBank-Dense data and evaluated it with QA (see the TEA-Dense line in).", "labels": [], "entities": [{"text": "TimeBank-Dense data", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9216285347938538}, {"text": "QA", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9860854148864746}, {"text": "TEA-Dense", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.6428037881851196}]}, {"text": "Interestingly, the performance is nearly as good as our top model, although TimeBank-Dense only uses five major classes of relations.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.9640340805053711}]}, {"text": "For one thing, it shows that our system may perform equally after being trained on sparsely labeled data and on densely labeled data, judged from the QA evaluation tool.", "labels": [], "entities": []}, {"text": "If this is true, excessively annotated data may not be necessary in some tasks.", "labels": [], "entities": []}, {"text": "We use the QA toolkit provided by the QATempEval organizers to evaluate our system on the test data.", "labels": [], "entities": [{"text": "QATempEval organizers", "start_pos": 38, "end_pos": 59, "type": "DATASET", "confidence": 0.8553287088871002}]}, {"text": "The documents in test data are not annotated at all, so the event tags, TIMEX tags, and TLINKs are all created by our system.", "labels": [], "entities": [{"text": "TIMEX tags", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9211695194244385}, {"text": "TLINKs", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.8419557213783264}]}, {"text": "shows the the statistics of test data.", "labels": [], "entities": []}, {"text": "As we can see, the vast majority of the questions in the test set should be answered with yes.", "labels": [], "entities": []}, {"text": "Generally speaking, it is much more difficult to validate a specific relation (answer yes) than to reject it (answer no) when we have as many as 12 types of relations in addition to the vague NO-LINK class.", "labels": [], "entities": []}, {"text": "dist-means questions involving entities that are in the same sentence or in consecutive sentences.", "labels": [], "entities": []}, {"text": "dist+ means the entities are farther away.", "labels": [], "entities": []}, {"text": "The QA-TempEval task organizers used two evaluation methods.", "labels": [], "entities": []}, {"text": "The first method is exactly the same as the one we used on validation data.", "labels": [], "entities": []}, {"text": "The second method used a so-called Time Expression Reasoner (TREFL) to add relations between TIMEXes, and evaluated the augmented results.", "labels": [], "entities": [{"text": "Time Expression Reasoner (TREFL", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.653309041261673}, {"text": "TIMEXes", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9076727628707886}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We give the results for the hlt-fbk systems that were submitted by the top team.", "labels": [], "entities": []}, {"text": "Among them, hlt-fbk-ev2-trel2 was the overall winner of TempEval task in 2015.", "labels": [], "entities": []}, {"text": "ClearTK, CAEVO, TIPSEMB and TIPSem were some off-the-shelf systems provided by the task organizers for reference.", "labels": [], "entities": [{"text": "ClearTK", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8885287642478943}, {"text": "TIPSEMB", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.7107846140861511}]}, {"text": "These systems were not optimized for the task ().", "labels": [], "entities": []}, {"text": "For news and Wikipedia genres, our system outperforms all other systems by a large margin.", "labels": [], "entities": []}, {"text": "For blogs genre, however, the advantage of our system is unclear.", "labels": [], "entities": []}, {"text": "Recall that our training set contains news articles only.", "labels": [], "entities": []}, {"text": "While the trained model works well on Wikipedia dataset too, blog dataset is fundamentally different in the following ways: (1) each blog article is very short, (2) the style of writing in blogs is much more informal, with nonstandard spelling and punctuation, and   are written in first person, and the content is usually personal stories and feelings.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.90965935587883}]}, {"text": "Interestingly, the comparison between different hlt-fbk submissions suggests that resolving event coreference (implemented by hlt-fbk-ev2-trel2) substantially improves system performance for the news and Wikipedia genres.", "labels": [], "entities": [{"text": "event coreference", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.703848347067833}]}, {"text": "However, although our system does not attempt to handle event coreference explicitly, it easily outperforms the hlt-fbk-ev2-trel2 system in the genres where coreference seems to matter the most.", "labels": [], "entities": []}, {"text": "Evaluation with TREFL The extra evaluation with TREFL has a post-processing step that adds TLINKs between TIMEX entities.", "labels": [], "entities": [{"text": "TREFL", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.7504417300224304}, {"text": "TLINKs", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9577873349189758}]}, {"text": "Our model already employs such a strategy, so this postprocessing does not help.", "labels": [], "entities": []}, {"text": "In fact, it drags down the scores a little.", "labels": [], "entities": []}, {"text": "summarizes the results overall genres before and after applying TREFL.", "labels": [], "entities": [{"text": "TREFL", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9194576144218445}]}, {"text": "For comparison, we include the top 2015 system, hlt-fbk-ev2-trel2.", "labels": [], "entities": []}, {"text": "As we can see, TEA generally shows substantially higher scores.", "labels": [], "entities": [{"text": "TEA", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.6109156608581543}]}, {"text": "We trained and evaluated the same system on TimeBank-Dense to see how it performs on a similar task with a different set of labels and another method of evaluation.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9251245856285095}]}, {"text": "In this experiment, we used the event and TIMEX tags from test data, as.", "labels": [], "entities": [{"text": "TIMEX", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9796608090400696}]}, {"text": "Since all the NO-LINK (vague) relations are labeled, downsampling was not necessary.", "labels": [], "entities": []}, {"text": "We did use double-checking in the final conflict resolution, but without giving positive cases the veto power over NO-LINK.", "labels": [], "entities": [{"text": "conflict resolution", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8243249356746674}, {"text": "NO-LINK", "start_pos": 115, "end_pos": 122, "type": "DATASET", "confidence": 0.6876006126403809}]}, {"text": "Because NO-LINK relations dominate, especially for cross-sentence pairs, we set class weights to be inversely proportional to the class frequencies during training.", "labels": [], "entities": []}, {"text": "We also reduced input batch size to counteract class imbalance.", "labels": [], "entities": []}, {"text": "We ran two sets of experiments.", "labels": [], "entities": []}, {"text": "One used the uniform configurations for all the neural network models, similar to our experiments with QATempEval.", "labels": [], "entities": []}, {"text": "The other tuned the hyperparameters for each component model (number of neurons, dropout rates, and early stop) separately.: TEA results on TimeBank-Dense.", "labels": [], "entities": [{"text": "early stop)", "start_pos": 100, "end_pos": 111, "type": "METRIC", "confidence": 0.96514364083608}, {"text": "TEA", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9833419322967529}, {"text": "TimeBank-Dense", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9610667824745178}]}, {"text": "ClearTK, NavyT, and CAEVO are systems from.", "labels": [], "entities": [{"text": "ClearTK", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9609485268592834}, {"text": "NavyT", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9245471954345703}, {"text": "CAEVO", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.7079212665557861}]}, {"text": "CATENA is from The results from TimeBank-Dense are shown in Talble 9.", "labels": [], "entities": [{"text": "CATENA", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.876473069190979}, {"text": "TimeBank-Dense", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9724612236022949}, {"text": "Talble 9", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9234585762023926}]}, {"text": "Even though TimeBank-Dense has a very different methodology for both annotation and evaluation, our \"out-of-the-box\" model which uses uniform configurations across different components obtains F1 0.505, compared to the best F1 of 0.511 in previous work.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.9538790583610535}, {"text": "F1", "start_pos": 193, "end_pos": 195, "type": "METRIC", "confidence": 0.9995021820068359}, {"text": "F1", "start_pos": 224, "end_pos": 226, "type": "METRIC", "confidence": 0.9930590391159058}]}, {"text": "Our best result of 0.519 is obtained by tuning hyperparameters on intrasentence, cross-sentence, and DCT models independently.", "labels": [], "entities": []}, {"text": "For the QA-TempEval task, we intentionally tagged a lot of events, and let the pruning algorithm resolve potential conflicts.", "labels": [], "entities": []}, {"text": "In the TimeBankDense experiment, however, we only used the provided event tags, which are sparser than what we have in QA-TempEval.", "labels": [], "entities": []}, {"text": "The system may have lost some leverage that way.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Effects of downsampling and double-checking on  intra-sentence results. 0.5 NO-LINK ratio means that NO- LINKs are downsampled to a half of the number of all positive  instances combined. BEFORE as AFTER shows the fraction of  BEFORE misclassified as AFTER, and vice versa.", "labels": [], "entities": [{"text": "NO-LINK ratio", "start_pos": 86, "end_pos": 99, "type": "METRIC", "confidence": 0.979030430316925}, {"text": "NO- LINKs", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9024872779846191}, {"text": "BEFORE", "start_pos": 198, "end_pos": 204, "type": "METRIC", "confidence": 0.9949281215667725}, {"text": "AFTER", "start_pos": 208, "end_pos": 213, "type": "METRIC", "confidence": 0.5792952179908752}]}, {"text": " Table 4: TIMEX and event evaluation on validation set.", "labels": [], "entities": [{"text": "TIMEX", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8253877758979797}]}, {"text": " Table 5: QA results on validation data. There are 79 ques- tions in total. The 4 systems on the top of the table are pro- vided with the toolkit. The systems starting with \"human- \" are annotated by human experts. TEA-final utilizes both  double-check and pruning. TEA-flat uses the flat context.  TEA-Dense is trained on TimeBank-Dense.", "labels": [], "entities": [{"text": "TimeBank-Dense", "start_pos": 323, "end_pos": 337, "type": "DATASET", "confidence": 0.9588088393211365}]}, {"text": " Table 6: Test data statistics. Adapted from Table 1 in Llorens  et al. (2015a).", "labels": [], "entities": []}, {"text": " Table 7: QA evaluation on test data without TREFL", "labels": [], "entities": [{"text": "TREFL", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9809144735336304}]}, {"text": " Table 7. We give the  results for the hlt-fbk systems that were submitted  by the top team. Among them, hlt-fbk-ev2-trel2  was the overall winner of TempEval task in 2015.  ClearTK, CAEVO, TIPSEMB and TIPSem were  some off-the-shelf systems provided by the task  organizers for reference. These systems were not  optimized for the task (", "labels": [], "entities": []}, {"text": " Table 8: Test results over all genres.", "labels": [], "entities": []}, {"text": " Table 9: TEA results on TimeBank-Dense. ClearTK, NavyT,  and CAEVO are systems from", "labels": [], "entities": [{"text": "TEA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9182969927787781}, {"text": "TimeBank-Dense", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9552428126335144}, {"text": "ClearTK", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.8966169357299805}, {"text": "NavyT", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8824228644371033}, {"text": "CAEVO", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.7984471321105957}]}]}