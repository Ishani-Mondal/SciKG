{"title": [], "abstractContent": [{"text": "We model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a set of transition actions without syntax information.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.8144775331020355}]}, {"text": "Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.682351216673851}, {"text": "disfluency detection", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.727089524269104}]}, {"text": "Experiments show that our model achieves state-of-the-art F-score on both the commonly used English Switchboard test set and a set of in-house annotated Chinese data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9991825222969055}, {"text": "English Switchboard test set", "start_pos": 92, "end_pos": 120, "type": "DATASET", "confidence": 0.9152264893054962}]}], "introductionContent": [{"text": "Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8708567321300507}]}, {"text": "As shown in, standard annotation of disfluency structure indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.).", "labels": [], "entities": []}, {"text": "Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively.", "labels": [], "entities": [{"text": "repetitions", "start_pos": 86, "end_pos": 97, "type": "METRIC", "confidence": 0.9095882773399353}, {"text": "corrections", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9627580642700195}]}, {"text": "Interregnums are easy to detect as they often consist of fixed phrases (e.g. \"uh\", \"you know\").", "labels": [], "entities": []}, {"text": "However, reparandums are more difficult to detect, because they can be in arbitrary form.", "labels": [], "entities": []}, {"text": "Most previ- ous disfluency detection work focuses on detecting reparandums.", "labels": [], "entities": [{"text": "previ- ous disfluency detection", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.7793474674224854}, {"text": "detecting reparandums", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8405269086360931}]}, {"text": "The main challenges of detecting reparandums include that they vary in length, may occur in different locations, and are sometimes nested.", "labels": [], "entities": [{"text": "detecting reparandums", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.8609552085399628}]}, {"text": "For example, the longest reparandum in our training set has fifteen words.", "labels": [], "entities": []}, {"text": "Hence, it is very important to capture long-range dependencies for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.8189292848110199}]}, {"text": "Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks.", "labels": [], "entities": []}, {"text": "Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.8115809261798859}]}, {"text": "These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances.", "labels": [], "entities": []}, {"text": "Another drawback of these approaches is that they are unable to exploit chunk-level features. is used to alleviate this issue to some extent.", "labels": [], "entities": []}, {"text": "Semi-CRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding.", "labels": [], "entities": []}, {"text": "A different line of work () adopts transition-based parsing models for disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.802102118730545}]}, {"text": "This line of work can be seen as a joint of disfluency detection and parsing.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8160356879234314}]}, {"text": "The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information.", "labels": [], "entities": []}, {"text": "However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space.", "labels": [], "entities": []}, {"text": "Inspired by the above observations, we investigate a transition-based model without syntactic information.", "labels": [], "entities": []}, {"text": "Our model incrementally constructs and labels the disfluency chunks of input sentences using an algorithm similar to transition-based depency parsing.", "labels": [], "entities": [{"text": "transition-based depency parsing", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.6976068417231241}]}, {"text": "As shown in, the model state consists of four components: (i) O, a conventional sequential LSTM) to store the words that have been labeled as fluency.", "labels": [], "entities": []}, {"text": "(ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunklevel information.", "labels": [], "entities": []}, {"text": "(iii) A, a conventional sequential LSTM to represent history of actions.", "labels": [], "entities": []}, {"text": "(iiii) B, a Bi-LSTM to represent words that have not yet been processed.", "labels": [], "entities": []}, {"text": "A sequence of transition actions are used to consume input tokens and construct the output from left to right.", "labels": [], "entities": []}, {"text": "To reduce error propagation, we use beam-search () and scheduled sampling, respectively.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6605678200721741}]}, {"text": "We evaluate our model on the commonly used English Switchboard test set and a in-house annotated Chinese data set.", "labels": [], "entities": [{"text": "English Switchboard test set", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9367957562208176}, {"text": "Chinese data set", "start_pos": 97, "end_pos": 113, "type": "DATASET", "confidence": 0.8415290911992391}]}, {"text": "Results show that our model outperforms previous state-of-the-art systems.", "labels": [], "entities": []}, {"text": "The code is released 1 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Experiment results on the development  and test data of English Switchboard data.", "labels": [], "entities": [{"text": "English Switchboard data", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.8943503499031067}]}, {"text": " Table 5: Comparison with previous state-of-the- art methods on the test set of English Switchboard.", "labels": [], "entities": [{"text": "English Switchboard", "start_pos": 80, "end_pos": 99, "type": "DATASET", "confidence": 0.8749663233757019}]}, {"text": " Table 6: Test result of our transition-based model  using DPS files for training.", "labels": [], "entities": [{"text": "DPS files", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.8943083882331848}]}, {"text": " Table 7: performance on Chinese annotated data", "labels": [], "entities": []}, {"text": " Table 8: Results of feature ablation experiments  on English Switchboard test data. \"-Bi-LSTM\"  means using unidirectional LSTM for buffer", "labels": [], "entities": [{"text": "English Switchboard test data", "start_pos": 54, "end_pos": 83, "type": "DATASET", "confidence": 0.9735917299985886}, {"text": "Bi-LSTM", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.9626042246818542}]}, {"text": " Table 9: F-score of different types of reparandums  on English Switchboard test data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9982208609580994}, {"text": "English Switchboard test data", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.9032981544733047}]}]}