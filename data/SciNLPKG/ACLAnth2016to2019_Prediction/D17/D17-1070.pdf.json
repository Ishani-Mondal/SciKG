{"title": [{"text": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "labels": [], "entities": [{"text": "Supervised Learning of Universal Sentence Representations from Natural Language Inference", "start_pos": 0, "end_pos": 89, "type": "TASK", "confidence": 0.7096624165773392}]}], "abstractContent": [{"text": "Many modern NLP systems rely on word embeddings, previously trained in an un-supervised manner on large corpora, as base features.", "labels": [], "entities": []}, {"text": "Efforts to obtain embed-dings for larger chunks of text, such as sentences, have however not been so successful.", "labels": [], "entities": []}, {"text": "Several attempts at learning unsu-pervised representations of sentences have not reached satisfactory enough performance to be widely adopted.", "labels": [], "entities": []}, {"text": "In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsu-pervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference datasets", "start_pos": 103, "end_pos": 147, "type": "DATASET", "confidence": 0.6122635185718537}]}, {"text": "Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.", "labels": [], "entities": []}, {"text": "Our encoder is publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributed representations of words (or word embeddings) () have shown to provide useful features for various tasks in natural language processing and computer vision.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 152, "end_pos": 167, "type": "TASK", "confidence": 0.7204884886741638}]}, {"text": "While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence.", "labels": [], "entities": []}, {"text": "That is, how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved.", "labels": [], "entities": []}, {"text": "In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.", "labels": [], "entities": []}, {"text": "Two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained.", "labels": [], "entities": []}, {"text": "Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought ( ) or FastSent (.", "labels": [], "entities": []}, {"text": "Here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the ImageNet () before being transferred.", "labels": [], "entities": []}, {"text": "We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9396966099739075}]}, {"text": "We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.", "labels": [], "entities": []}, {"text": "Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.", "labels": [], "entities": []}, {"text": "Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes.", "labels": [], "entities": [{"text": "representational transferability", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.9266403913497925}]}, {"text": "Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset, yields state-of-the-art sentence embeddings com-pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.", "labels": [], "entities": [{"text": "Stanford Natural Language Inference (SNLI) dataset", "start_pos": 114, "end_pos": 164, "type": "DATASET", "confidence": 0.5289472788572311}]}, {"text": "We establish this finding on abroad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our aim is to obtain general-purpose sentence embeddings that capture generic information that is name N task C examples MR 11k sentiment (movies) 2 \"Too slow fora younger crowd , too shallow for an older one.\"", "labels": [], "entities": []}, {"text": "(neg) CR 4k product reviews 2 \"We tried it out christmas night and it worked great .\" (pos) SUBJ 10k subjectivity/objectivity 2 \"A movie that doesn't aim too high , but doesn't need to.\"", "labels": [], "entities": []}, {"text": "(subj) MPQA 11k opinion polarity 2 \"don't want\"; \"would like to tell\"; (neg, pos) TREC 6k question-type 6 \"What are the twin cities ?\" (LOC:city) SST 70k sentiment (movies) 2 \"Audrey Tautou has a knack for picking roles that magnify her [..]\" (pos)) to fit a logistic regression classifier, with batch size 64.", "labels": [], "entities": [{"text": "MPQA", "start_pos": 7, "end_pos": 11, "type": "DATASET", "confidence": 0.9187352657318115}, {"text": "TREC", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.759355902671814}]}, {"text": "Binary and multi-class classification We use a set of binary classification tasks (see) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).", "labels": [], "entities": [{"text": "sentence classification", "start_pos": 117, "end_pos": 140, "type": "TASK", "confidence": 0.7806572020053864}, {"text": "sentiment analysis (MR, SST)", "start_pos": 152, "end_pos": 180, "type": "TASK", "confidence": 0.779940128326416}]}, {"text": "We generate sentence vectors and train a logistic regression on top.", "labels": [], "entities": []}, {"text": "A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited.", "labels": [], "entities": []}, {"text": "We tune the L2 penalty of the logistic regression with grid-search on the validation set.", "labels": [], "entities": []}, {"text": "Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R).", "labels": [], "entities": [{"text": "SICK dataset", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8467079699039459}]}, {"text": "We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint representation.", "labels": [], "entities": [{"text": "Regression", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.6842906475067139}]}, {"text": "For semantic relatedness evaluation, we follow the approach of () and learn to predict the probability distribution of relatedness scores.", "labels": [], "entities": [{"text": "semantic relatedness evaluation", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7210230827331543}]}], "tableCaptions": [{"text": " Table 2: Natural Language Inference and Semantic Textual Similarity tasks. NLI labels are contra- diction, neutral and entailment. STS labels are scores between 0 and 5.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.6559242904186249}]}, {"text": " Table 3: Performance of sentence encoder ar- chitectures on SNLI and (aggregated) transfer  tasks. Dimensions of embeddings were selected  according to best aggregated scores (see", "labels": [], "entities": []}, {"text": " Table 4: Transfer test results for various architectures trained in different ways. Underlined are  best results for transfer learning approaches, in bold are best results among the models trained in the  same way.  \u2020 indicates methods that we trained, other transfer models have been extracted from (", "labels": [], "entities": []}, {"text": " Table 5: COCO retrieval results. SkipThought is trained either using 82k training samples with VGG19  features, or with 113k samples and ResNet-101 features (our setting). We report the average results on 5  splits of 1k test images.", "labels": [], "entities": [{"text": "SkipThought", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9036779403686523}, {"text": "VGG19", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.810865044593811}]}]}