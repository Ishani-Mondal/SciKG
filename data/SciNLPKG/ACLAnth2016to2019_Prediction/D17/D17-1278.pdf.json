{"title": [{"text": "MinIE: Minimizing Facts in Open Information Extraction", "labels": [], "entities": [{"text": "MinIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.775213360786438}, {"text": "Minimizing Facts in Open Information Extraction", "start_pos": 7, "end_pos": 54, "type": "TASK", "confidence": 0.8199926912784576}]}], "abstractContent": [{"text": "The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.785163144270579}]}, {"text": "In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.997791051864624}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9976612329483032}]}, {"text": "MinIE approaches these goals by (1) representing information about polarity, modality, attri-bution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific.", "labels": [], "entities": []}, {"text": "We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9978606104850769}, {"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.9973366856575012}]}], "introductionContent": [{"text": "Open Information Extraction (OIE) () is the task of generating a structured, machine-readable representation of information expressed in natural language text in an unsupervised, domain-independent manner.", "labels": [], "entities": [{"text": "Open Information Extraction (OIE)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7559228390455246}]}, {"text": "In contrast to traditional IE systems, OIE systems do not require an upfront specification of the target schema (e.g., target relations) or access to background knowledge (e.g., a knowledge base).", "labels": [], "entities": []}, {"text": "Instead, extractions are (usually) represented in the form of surface subject-relation-object triples.", "labels": [], "entities": []}, {"text": "OIE serves as input for deeper understanding tasks such as relation extraction (, knowledge base construction (), question answering), word analogy (, or information retrieval (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.8144043982028961}, {"text": "knowledge base construction", "start_pos": 82, "end_pos": 109, "type": "TASK", "confidence": 0.6569989621639252}, {"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.826596587896347}, {"text": "word analogy", "start_pos": 135, "end_pos": 147, "type": "TASK", "confidence": 0.8244489431381226}, {"text": "information retrieval", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7606942653656006}]}, {"text": "Consider, for example, the sentence \"Superman was born on Krypton.\"", "labels": [], "entities": []}, {"text": "An OIE system aims to extract the triple (Superman, was born on, Krypton), which most of the available systems will correctly produce.", "labels": [], "entities": []}, {"text": "As another example, consider the more involved sentence \"Pinocchio believes that the hero Superman was not actually born on beautiful Krypton\", and the corresponding extractions of various systems in, extractions 1-6.", "labels": [], "entities": []}, {"text": "Although most of the extractions are correct, they are often overly specific in that their constituents contain specific modifiers or even complete clauses.", "labels": [], "entities": []}, {"text": "Such extractions severely limit the usefulness of OIE results (e.g., they are often pruned in relation extraction tasks).", "labels": [], "entities": [{"text": "relation extraction tasks", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.8586529095967611}]}, {"text": "The main goals of OIE should be (i) to provide useful, compact extractions and (ii) to produce extractions with high precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9980512857437134}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9973042011260986}]}, {"text": "The key challenge in OIE is how to achieve both goals simultaneously.", "labels": [], "entities": [{"text": "OIE", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.683401346206665}]}, {"text": "In fact, most of the available systems (often implicitly) focus on either compactness (e.g.,) or precision/recall (e.g.,).", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9986199140548706}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9132369160652161}]}, {"text": "We propose MinIE, an OIE system that aims to address and trade-off both goals.", "labels": [], "entities": []}, {"text": "MinIE is built on top of ClausIE, a state-of-the-art OIE system that achieves high precision and recall, but often produces overly-specific extractions.", "labels": [], "entities": [{"text": "MinIE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8380986452102661}, {"text": "ClausIE", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.8606799840927124}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9982957243919373}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9982812404632568}]}, {"text": "To generate more useful and semantically richer extractions, MinIE (i) provides semantic annotations for each extraction, (ii) minimizes overly-specific constituents, and (iii) produces additional extractions that capture implicit relations.", "labels": [], "entities": []}, {"text": "shows the output of (variants of) MinIE for the example sentence.", "labels": [], "entities": []}, {"text": "Note that MinIE's extractions are significantly more compact but retain correctness.", "labels": [], "entities": []}, {"text": "MinIE's semantic annotations represent information about polarity, modality, attribution, and quantities.", "labels": [], "entities": []}, {"text": "The idea of using annotations has al-Pinocchio believes that the hero Superman was not actually born on beautiful Krypton.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experimental study was to investigate the differences in the various modes of MinIE w.r.t. precision, recall, and extraction length as well as to compare it with popular prior methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9281996488571167}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.997905969619751}]}, {"text": "Source code, dictionaries, datasets, extractions, labels, and labeling guidelines are made available.", "labels": [], "entities": []}, {"text": "We used (1) 10,000 random sentences from the New York Times Corpus (NYT-10k), (2) a random sample of 200 sentences from the same corpus (NYT), and (3) a random sample of 200 sentences from Wikipedia (Wiki).", "labels": [], "entities": [{"text": "New York Times Corpus (NYT-10k)", "start_pos": 45, "end_pos": 76, "type": "DATASET", "confidence": 0.7397173259939466}, {"text": "NYT", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.9140756130218506}]}, {"text": "NYT and Wiki were used in the evaluation of ClausIE and NestIE.", "labels": [], "entities": [{"text": "NYT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9781051278114319}, {"text": "ClausIE", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8711658716201782}, {"text": "NestIE", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.937387228012085}]}, {"text": "We used ClausIE, OLLIE, and Stanford OIE as baseline systems.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.8668903112411499}, {"text": "OLLIE", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9582686424255371}, {"text": "Stanford", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.6379721760749817}, {"text": "OIE", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.6336494088172913}]}, {"text": "We adapted the publicly available version of ClausIE to Stanford CoreNLP 3.8.0 and implemented MinIE on top.", "labels": [], "entities": [{"text": "ClausIE", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9439572095870972}, {"text": "Stanford CoreNLP 3.8.0", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8889915545781454}]}, {"text": "For MinIE-D, we built dictionary D from the entire NYT and Wikipedia corpus, respectively.", "labels": [], "entities": [{"text": "NYT and Wikipedia corpus", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.7946417182683945}]}, {"text": "Labelers provided two labels per extraction of NYT and Wiki: one for the triple (without attribution) and one for the attribution.", "labels": [], "entities": [{"text": "NYT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9282394051551819}]}, {"text": "A triple is labeled as correct if it is entailed by its corresponding clause; here factuality annotations are taken into account but attribution errors are ignored.", "labels": [], "entities": []}, {"text": "For example, all triples except #3 of Tab.", "labels": [], "entities": []}, {"text": "An attribution is incorrect if there is an attribution in the sentence which is neither present in the triple nor in the attribution annotation.", "labels": [], "entities": []}, {"text": "1, the attribution is incorrect for extractions #2, #3, #5, and #6.", "labels": [], "entities": []}, {"text": "Attribution is labeled only when the fact triple is labeled correct.", "labels": [], "entities": [{"text": "Attribution", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.656914472579956}]}, {"text": "See the labeling guidelines for further details.", "labels": [], "entities": []}, {"text": "Overall, there were more than 9,400 distinct extractions on NYT and Wiki.", "labels": [], "entities": [{"text": "NYT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.9565259218215942}, {"text": "Wiki", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.7525765299797058}]}, {"text": "Each extraction was labeled by two independent labelers.", "labels": [], "entities": []}, {"text": "We treat an extraction as correct if both labelers labeled it as correct.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement was moderate (NYT: Cohen's \u03ba = 0.53, 78% of labels agree; Wiki: \u03ba = 0.5, 79% of labels agree).", "labels": [], "entities": [{"text": "NYT", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.887420654296875}]}, {"text": "For each system, we measured the total number of extractions, the total number of correct triples (recall), the fraction of correct triples out of all extractions (factual precision), and the fraction of correct triples that have correct attributions (attribution precision).", "labels": [], "entities": [{"text": "recall)", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.968783050775528}, {"text": "precision", "start_pos": 172, "end_pos": 181, "type": "METRIC", "confidence": 0.7903978228569031}, {"text": "precision", "start_pos": 264, "end_pos": 273, "type": "METRIC", "confidence": 0.7463489770889282}]}, {"text": "We also determined the mean word count per triple (\u00b5) and its standard deviation (\u03c3) as a proxy for minimality.", "labels": [], "entities": [{"text": "mean word count per triple (\u00b5)", "start_pos": 23, "end_pos": 53, "type": "METRIC", "confidence": 0.8484299071133137}, {"text": "standard deviation (\u03c3)", "start_pos": 62, "end_pos": 84, "type": "METRIC", "confidence": 0.8622645378112793}]}, {"text": "Finally, as some systems produced a large number of redundant extractions, we also report the number of non-redundant extractions.", "labels": [], "entities": []}, {"text": "For simplicity, we consider a triplet 1 redundant if it appears as subsequence in some other triplet 2 produced by the same extractor from the same sentence (e.g., extraction #5 in Tab. 1 is redundant given extrac-tion #6).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results on the unlabeled NYT-10k dataset (\u00b5=avg. extraction length, \u03c3=standard deviation)", "labels": [], "entities": [{"text": "NYT-10k dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.9692434966564178}, {"text": "avg. extraction length", "start_pos": 54, "end_pos": 76, "type": "METRIC", "confidence": 0.7323077917098999}]}, {"text": " Table 4: Results on the labeled NYT and Wiki datasets", "labels": [], "entities": [{"text": "NYT and Wiki datasets", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.7108459174633026}]}]}