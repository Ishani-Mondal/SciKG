{"title": [{"text": "Learning to Paraphrase for Question Answering", "labels": [], "entities": [{"text": "Paraphrase for Question Answering", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.5816548317670822}]}], "abstractContent": [{"text": "Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.904710328578949}]}, {"text": "In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks.", "labels": [], "entities": []}, {"text": "Our method is trained end-to-end using question-answer pairs as a supervision signal.", "labels": [], "entities": []}, {"text": "A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers.", "labels": [], "entities": []}, {"text": "We evaluate our approach on QA over Freebase and answer sentence selection.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.768724262714386}]}, {"text": "Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Enabling computers to automatically answer questions posed in natural language on any domain or topic has been the focus of much research in recent years.", "labels": [], "entities": []}, {"text": "Question answering (QA) is challenging due to the many different ways natural language expresses the same information need.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9442846655845643}]}, {"text": "As a result, small variations in semantically equivalent questions, may yield different answers.", "labels": [], "entities": []}, {"text": "For example, a hypothetical QA system must recognize that the questions \"who created microsoft\" and \"who started microsoft\" have the same meaning and that they both convey the founder relation in order to retrieve the correct answer from a knowledge base.", "labels": [], "entities": []}, {"text": "Given the great variety of surface forms for semantically equivalent expressions, it should come as no surprise that previous work has investigated the use of paraphrases in relation to question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 186, "end_pos": 204, "type": "TASK", "confidence": 0.8042581379413605}]}, {"text": "There have been three main strands of research.", "labels": [], "entities": []}, {"text": "The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 97, "end_pos": 113, "type": "TASK", "confidence": 0.7431419789791107}]}, {"text": "use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms.", "labels": [], "entities": []}, {"text": "Another strand of work uses paraphrases in the context of neural question answering models ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7239054590463638}]}, {"text": "These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases.", "labels": [], "entities": []}, {"text": "The third strand of research uses paraphrases more directly.", "labels": [], "entities": []}, {"text": "The idea is to paraphrase the question and then submit the rewritten version to a QA module.", "labels": [], "entities": []}, {"text": "Various resources have been used to produce question paraphrases, such as rule-based machine translation), lexical and phrasal rules from the Paraphrase Database (, as well as rules mined from Wiktionary () and large-scale paraphrase corpora.", "labels": [], "entities": [{"text": "rule-based machine translation", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.6755709449450175}, {"text": "Paraphrase Database", "start_pos": 142, "end_pos": 161, "type": "DATASET", "confidence": 0.6855408251285553}]}, {"text": "A common problem with the generated paraphrases is that they often contain inappropriate candidates.", "labels": [], "entities": []}, {"text": "Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance.", "labels": [], "entities": []}, {"text": "To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer.", "labels": [], "entities": []}, {"text": "Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned: We use three different methods to generate candidate paraphrases for input q.", "labels": [], "entities": []}, {"text": "The question and its paraphrases are fed into a neural model which scores how suitable they are.", "labels": [], "entities": []}, {"text": "The scores are normalized and used to weight the results of the question answering model.", "labels": [], "entities": [{"text": "question answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7213492691516876}]}, {"text": "The entire system is trained end-to-end using question-answer pairs as a supervision signal.", "labels": [], "entities": []}, {"text": "to the question answering tasks at hand.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.8841248353322347}]}, {"text": "Based on the large variety of possible transformations that can generate paraphrases, it seems likely that the kinds of paraphrases that are useful would depend on the QA application of interest.", "labels": [], "entities": []}, {"text": "use features that are defined over the original question and its rewrites to score paraphrases.", "labels": [], "entities": []}, {"text": "Examples include the pointwise mutual information of the rewrite rule, the paraphrase's score according to a language model, and POS tag features.", "labels": [], "entities": []}, {"text": "In the context of semantic parsing, also use the ID of the rewrite rule as a feature.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.715411901473999}]}, {"text": "However, most of these features are not informative enough to model the quality of question paraphrases, or cannot easily generalize to unseen rewrite rules.", "labels": [], "entities": []}, {"text": "In this paper, we present a general framework for learning paraphrases for question answering tasks.", "labels": [], "entities": [{"text": "question answering tasks", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.8291405439376831}]}, {"text": "Given a natural language question, our model estimates a probability distribution over candidate answers.", "labels": [], "entities": []}, {"text": "We first generate paraphrases for the question, which can be obtained by one or several paraphrasing systems.", "labels": [], "entities": []}, {"text": "A neural scoring model predicts the quality of the generated paraphrases, while learning to assign higher weights to those which are more likely to yield correct answers.", "labels": [], "entities": []}, {"text": "The paraphrases and the original question are fed into a QA model that predicts a distribution over answers given the question.", "labels": [], "entities": []}, {"text": "The entire system is trained end-to-end using question-answer pairs as a supervision signal.", "labels": [], "entities": []}, {"text": "The framework is flexible, it does not rely on specific paraphrase or QA models.", "labels": [], "entities": []}, {"text": "In fact, this plug-and-play functionality allows to learn specific paraphrases for different QA tasks and to explore the merits of different paraphrasing models for different applications.", "labels": [], "entities": []}, {"text": "We evaluate our approach on QA over Freebase and text-based answer sentence selection.", "labels": [], "entities": [{"text": "text-based answer sentence selection", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.5541875287890434}]}, {"text": "We employ a range of paraphrase models based on the Paraphrase Database (PPDB;), neural machine translation, and rules mined from the WikiAnswers corpus.", "labels": [], "entities": [{"text": "Paraphrase Database (PPDB", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.7323084026575089}, {"text": "neural machine translation", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7135445276896158}, {"text": "WikiAnswers corpus", "start_pos": 134, "end_pos": 152, "type": "DATASET", "confidence": 0.7771781086921692}]}, {"text": "Results on three datasets show that our framework consistently improves performance; it achieves state-of-the-art results on GraphQuestions and competitive performance on two additional benchmark datasets using simple QA models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared our model which we call PARA4QA (as shorthand for learning to paraphrase for question answering) against multiple previous systems on three datasets.", "labels": [], "entities": [{"text": "PARA4QA", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.7763130068778992}, {"text": "question answering)", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.75263183315595}]}, {"text": "In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results.", "labels": [], "entities": []}, {"text": "Our model was trained on three datasets, representative of different types of QA tasks.", "labels": [], "entities": [{"text": "QA tasks", "start_pos": 78, "end_pos": 86, "type": "TASK", "confidence": 0.8377532362937927}]}, {"text": "The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7360361665487289}, {"text": "answer sentence selection", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.7565896312395731}]}], "tableCaptions": [{"text": " Table 3: Statistics of generated paraphrases across  datasets (training set). avg(|q|): average ques- tion length; avg(|q |): average paraphrase length;  avg(#q ): average number of paraphrases; cover- age: the proportion of questions that have at least  one candidate paraphrase.", "labels": [], "entities": [{"text": "ques- tion length", "start_pos": 97, "end_pos": 114, "type": "METRIC", "confidence": 0.7287996858358383}, {"text": "cover- age", "start_pos": 196, "end_pos": 206, "type": "METRIC", "confidence": 0.9218365550041199}]}, {"text": " Table 4: Model performance on GRAPHQUES- TIONS and WEBQUESTIONS. Results with addi- tional task-specific resources are shown in paren- theses. The base QA model is SIMPLEGRAPH.  Best results in each group are shown in bold.", "labels": [], "entities": [{"text": "GRAPHQUES- TIONS", "start_pos": 31, "end_pos": 47, "type": "METRIC", "confidence": 0.9297077854474386}, {"text": "WEBQUESTIONS", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.7536083459854126}]}, {"text": " Table 5: Model performance on WIKIQA. +CNT:  word matching features introduced in Yang et al.  (2015). The base QA model is BILSTM. Best re- sults in each group are shown in bold.", "labels": [], "entities": [{"text": "WIKIQA", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.8561186194419861}, {"text": "word matching", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7687874734401703}, {"text": "BILSTM", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9661654829978943}]}, {"text": " Table 6: Questions and their top-five paraphrases  with probabilities learned by the model. The Free- base relations used to query the correct answers  are shown in brackets. The original question is  underlined. Questions with incorrect predictions  are in red.", "labels": [], "entities": []}, {"text": " Table 7: We group GRAPHQUESTIONS into sim- ple and complex questions and report model per- formance in each split. Best results in each group  are shown in bold. The values in brackets are ab- solute improvements of average F1 scores.", "labels": [], "entities": [{"text": "GRAPHQUESTIONS", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.9879051446914673}, {"text": "F1", "start_pos": 225, "end_pos": 227, "type": "METRIC", "confidence": 0.9988769888877869}]}]}