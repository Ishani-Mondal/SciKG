{"title": [{"text": "A Continuously Growing Dataset of Sentential Paraphrases", "labels": [], "entities": []}], "abstractContent": [{"text": "A major challenge in paraphrase research is the lack of parallel corpora.", "labels": [], "entities": [{"text": "paraphrase", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9759376049041748}]}, {"text": "In this paper , we present anew method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs.", "labels": [], "entities": [{"text": "collect large-scale sentential paraphrases from Twitter", "start_pos": 42, "end_pos": 97, "type": "TASK", "confidence": 0.7144676893949509}]}, {"text": "The main advantage of our method is its simplicity, as it gets rid of the classi-fier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 177, "end_pos": 202, "type": "TASK", "confidence": 0.8539443910121918}]}, {"text": "We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.8223394453525543}]}, {"text": "In addition , we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at \u223c70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9967154264450073}, {"text": "phrasal paraphrase extraction", "start_pos": 205, "end_pos": 234, "type": "TASK", "confidence": 0.682951827843984}]}, {"text": "We make our code and data freely available.", "labels": [], "entities": []}], "introductionContent": [{"text": "A paraphrase is a restatement of meaning using different expressions.", "labels": [], "entities": []}, {"text": "It is a fundamental semantic relation inhuman language, as formalized in the Meaning-Text linguistic theory which defines meaning as 'invariant of paraphrases').", "labels": [], "entities": []}, {"text": "Researchers have shown benefits of using paraphrases in a wide range of applications (, including question answering, semantic parsing, information extraction), machine translation, textual entailment (, vector semantics (, and semantic textual similarity (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.855434238910675}, {"text": "semantic parsing", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7410004436969757}, {"text": "information extraction", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.7896604239940643}, {"text": "machine translation", "start_pos": 161, "end_pos": 180, "type": "TASK", "confidence": 0.8098019957542419}]}, {"text": "Studying paraphrases in Twitter can also help track unfolding events or the spread of information () on social networks.", "labels": [], "entities": []}, {"text": "In this paper, we address a major challenge in paraphrase research -the lack of parallel corpora.", "labels": [], "entities": [{"text": "paraphrase", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9743505716323853}]}, {"text": "There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases: 2 the MSRP corpus derived from clustered news articles) and the PIT-2015 corpus from Twitter trending topics (.", "labels": [], "entities": [{"text": "MSRP corpus", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.8536640107631683}, {"text": "PIT-2015 corpus", "start_pos": 178, "end_pos": 193, "type": "DATASET", "confidence": 0.9162894487380981}]}, {"text": "Our goal is not only to create anew annotated paraphrase corpus, but to identify anew data source and method that can narrow down the search space of paraphrases without using the classifier-biased or human-in-the-loop data selection as in.", "labels": [], "entities": []}, {"text": "This is so that sentential paraphrases can be conveniently and continuously harvested in large quantities to benefit downstream applications.", "labels": [], "entities": []}, {"text": "We present an effective method to collect sentential paraphrases from tweets that refer to the same URL and contribute anew gold-standard annotated corpus of 51,524 sentence pairs, which is the largest to date.", "labels": [], "entities": []}, {"text": "We show the different characteristics of this new dataset contrasting the two existing corpora through the first system-: Summary of publicly available large sentential paraphrase corpora with manual quality assurance.", "labels": [], "entities": []}, {"text": "Our Twitter News URL Corpus has the advantages of including both meaningful non-paraphrases (Non-Para.) and multiple references (Multi-Ref.), which are important for training paraphrase identification and evaluating paraphrase generation, respectively.", "labels": [], "entities": [{"text": "Twitter News URL Corpus", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7062153443694115}, {"text": "paraphrase identification", "start_pos": 175, "end_pos": 200, "type": "TASK", "confidence": 0.8313049674034119}, {"text": "paraphrase generation", "start_pos": 216, "end_pos": 237, "type": "TASK", "confidence": 0.8307410478591919}]}, {"text": "atic study of paraphrase identification across multiple datasets.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.8935767710208893}]}, {"text": "Our new corpus is complementary to previous work, as the corpus contains multiple references of both formal well-edited and informal user-generated texts.", "labels": [], "entities": []}, {"text": "This is also the first work that provides a continuously growing collection, with more than 30,000 new sentential paraphrases per month automatically labeled at \u223c70% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9926788806915283}]}, {"text": "We demonstrate that up-to-date phrasal paraphrases can then be extracted via word alignment (see examples in).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.7218550592660904}]}, {"text": "We plan to continue collecting paraphrases using our method and release a constantly updating paraphrase resource.", "labels": [], "entities": []}], "datasetContent": [{"text": "Difference The results on three benchmark paraphrase corpora are shown in, 9 and 10.", "labels": [], "entities": [{"text": "Difference", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9685590863227844}]}, {"text": "The random baseline reflects that close to 80% sentence pairs are paraphrases in the MSPR corpus.", "labels": [], "entities": [{"text": "MSPR corpus", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.7582653164863586}]}, {"text": "This is atypical in the real-world text data and may cause falsely positive predictions.", "labels": [], "entities": []}, {"text": "Both the edit distance and the LR models exploit surface word features.", "labels": [], "entities": []}, {"text": "In particular, the LR model that uses lemmatization and ngram overlap features achieves very competitive performance on all datasets.", "labels": [], "entities": [{"text": "ngram overlap", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.8963420987129211}]}, {"text": "shows a closer look at ngram differences across datasets measured by the PINC metric (, which is the opposite of BLEU ().", "labels": [], "entities": [{"text": "PINC metric", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.535470724105835}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9987892508506775}]}, {"text": "MSRP consists of paraphrases with more ngram overlap (lower PINC), while PIT-2015 contains shorter and more lexically dissimilar sentences.", "labels": [], "entities": [{"text": "MSRP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.939204752445221}, {"text": "ngram overlap", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.897208571434021}, {"text": "PINC", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9178466200828552}]}, {"text": "Our new URL corpus is in between the two, and is more similar to PIT-2015.", "labels": [], "entities": [{"text": "PIT-2015", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9540073871612549}]}, {"text": "It includes user's intentional rephrasing of an original tweet from a news agency with some words untouched, as well as some dramatic paraphrases that are challenging for any automatic identification methods, such as CO2 levels mark 'new era' in the world's changing climate and CO2 levels haven't been this high for 3 to 5 million years.", "labels": [], "entities": []}, {"text": "MultiP exploits a restrictive constraint that the candidate sentence pairs share a same topical phrase.", "labels": [], "entities": []}, {"text": "It achieves the best performance on PIT-2015, which naturally contains such phrases.", "labels": [], "entities": [{"text": "PIT-2015", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9073805809020996}]}, {"text": "For MSRP and URL datasets, we uses the named entity tagged with the longest span as an approximation of a shared topic phrase and thus suffered a performance drop.", "labels": [], "entities": []}, {"text": "Both Glove and WMT/OrMF utilize the underlying co-occurrence statistics of the text corpus.", "labels": [], "entities": [{"text": "Glove", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.7699423432350159}, {"text": "WMT/OrMF", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.723065714041392}]}, {"text": "WMT/OrMF use global matrix factorization to project sentences into lower dimension and show great advantages on measuring sentence-level semantic similarities over Glove, which focuses on word representations.", "labels": [], "entities": [{"text": "WMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8590769171714783}, {"text": "OrMF", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.516218900680542}]}, {"text": "shows that the finegrained distribution of the OrMF-based cosine similarities and that the URL-linked Twitter data works well with OrMF to yield sentential paraphrases.", "labels": [], "entities": [{"text": "URL-linked Twitter data", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.6020931601524353}]}, {"text": "Once combined with ngram overlap features, LEX-WMF and LEX-OrMF show consistently high performance across different datasets, close to the more complicated DeepPairwiseWord.", "labels": [], "entities": []}, {"text": "The similarity focus mechanism on important pairwise word interactions in DeepPairwiseWord is more helpful for the two Twitter datasets, due to the fact that they contain lexically divergent paraphrases while MSRP has an artificial bias toward sentences with high n-gram overlap.", "labels": [], "entities": []}, {"text": "We compared the quality of paraphrases extracted by our method with the closest previous work (BUCC-2013) (, in which a similar phrase table was created using Moses from monolingual parallel tweets that contain the same named entity and calendar date.", "labels": [], "entities": [{"text": "BUCC-2013", "start_pos": 95, "end_pos": 104, "type": "DATASET", "confidence": 0.9194024801254272}]}, {"text": "We randomly sampled 500 phrase pairs from each phrase table and collected human judgements on a 5-point Likert scale, as described in. shows the evaluation results.", "labels": [], "entities": []}, {"text": "We focused on the highest-quality paraphrases that rated as 5 (\"all of the meaning of the original phrase is retained, and nothing is added\") and their presence among all extracted paraphrases sorted by ranking scores.", "labels": [], "entities": []}, {"text": "We were also interested in how these phrasal paraphrases compared with those in PPDB.", "labels": [], "entities": []}, {"text": "We sampled an equal amount of 420 paraphrase pairs from our phrase tables and PPDB, and then checked what percentage out of the total 840 could be found in our phrase tables and PPDB, respectively.", "labels": [], "entities": []}, {"text": "As shown in, there is little overlap between URL data and PPDB, only 1.3% (51.3-50%) plus 0.8% (50.8-50%).", "labels": [], "entities": []}, {"text": "Our Twitter URL data complements well with the existing paraphrase resources, such as PPDB, which are primarily derived from well-edited texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Statistics of tweets in Twitter's streaming  data and news account data. Many tweets contain  more than one URL because media such as photo  or video is also represented by URLs.", "labels": [], "entities": []}, {"text": " Table 6: Impact of filtering of manual retweets.", "labels": [], "entities": []}, {"text": " Table 7. As sentences tend to be longer in MSRP  and shorter in PIT-2015, we also normalized the  numbers by the length of sentences to be more  comparable to the URL dataset.", "labels": [], "entities": [{"text": "MSRP", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8782727718353271}, {"text": "PIT-2015", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9318810701370239}, {"text": "URL dataset", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.7098725736141205}]}, {"text": " Table 7: Mean number of instances of paraphrase  phenomena per sentence pair across three corpora.", "labels": [], "entities": []}, {"text": " Table 8: Paraphrase models in the MSR Para- phrase Corpus (MSRP). The bold font in the table  represents top three models in the dataset.", "labels": [], "entities": [{"text": "MSR Para- phrase Corpus (MSRP)", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.8595592007040977}]}, {"text": " Table 10: Paraphrase models in Twitter URL Cor- pus (this work).", "labels": [], "entities": [{"text": "Twitter URL Cor- pus", "start_pos": 32, "end_pos": 52, "type": "DATASET", "confidence": 0.7281497240066528}]}, {"text": " Table 11: Percentage of high-quality phrasal para- phrases extracted from Twitter URL data (this  work) by GIZA++, Jacana, Sultan aligners , com- paring to the previous work (BUCC-2013).", "labels": [], "entities": [{"text": "BUCC-2013", "start_pos": 176, "end_pos": 185, "type": "DATASET", "confidence": 0.9176425337791443}]}]}