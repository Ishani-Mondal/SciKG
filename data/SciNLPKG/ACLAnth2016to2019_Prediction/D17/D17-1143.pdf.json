{"title": [{"text": "Here's My Point: Joint Pointer Architecture for Argument Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "In order to determine argument structure in text, one must understand how individual components of the overall argument are linked.", "labels": [], "entities": []}, {"text": "This work presents the first neural network-based approach to link extraction in argument mining.", "labels": [], "entities": [{"text": "link extraction in argument mining", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7187253475189209}]}, {"text": "Specifically , we propose a novel architecture that applies Pointer Network sequence-to-sequence attention modeling to structural prediction in discourse parsing tasks.", "labels": [], "entities": [{"text": "Pointer Network sequence-to-sequence attention modeling", "start_pos": 60, "end_pos": 115, "type": "TASK", "confidence": 0.6538044095039368}, {"text": "structural prediction in discourse parsing tasks", "start_pos": 119, "end_pos": 167, "type": "TASK", "confidence": 0.7185105085372925}]}, {"text": "We then develop a joint model that extends this architecture to simultaneously address the link extraction task and the classification of argument components.", "labels": [], "entities": [{"text": "link extraction", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.7360332757234573}]}, {"text": "The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, showing far superior performance than the previously proposed corpus-specific and heavily feature-engineered models.", "labels": [], "entities": []}, {"text": "Furthermore , our results demonstrate that jointly optimizing for both tasks is crucial for high performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important goal in argument mining is to understand the structure in argumentative text.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.8612232804298401}]}, {"text": "One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs).", "labels": [], "entities": []}, {"text": "The types of ACs are generally characterized as a claim or a premise, with premises acting as support (or possibly attack) units for claims (though some corpora have further AC types, such as major claim).", "labels": [], "entities": []}, {"text": "The task of processing argument structure encapsulates four distinct subtasks (our work focuses on subtasks 2 and 3): (1) Given a sequence of tokens that represents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; (2) Given an AC, determine the type of AC (claim, premise, etc.); (3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; (4) Given two linked ACs, determine whether the link is a supporting or attacking relation.", "labels": [], "entities": []}, {"text": "This can be labeled as a 'micro' approach to argument mining (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.8227298557758331}]}, {"text": "In contrast, there have been a number of efforts to identify argument structure at a higher level, as well as slightly re-ordering the pipeline with respect to AC types ().", "labels": [], "entities": []}, {"text": "There are two key assumptions our work makes going forward.", "labels": [], "entities": []}, {"text": "First, we assume subtask 1 has been completed, i.e. ACs have already been identified.", "labels": [], "entities": []}, {"text": "Second, we follow previous work that assumes a tree structure for the linking of ACs (.", "labels": [], "entities": []}, {"text": "Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links.", "labels": [], "entities": []}, {"text": "Furthermore, there is a 'head' component that has no outgoing link (the top of the tree).", "labels": [], "entities": []}, {"text": "Depending on the corpus (see Section 4), an argument structure can be either a singletree or a forest, consisting of multiple trees.", "labels": [], "entities": []}, {"text": "shows an example that we will use throughout the paper to concretely explain how our approach works.", "labels": [], "entities": []}, {"text": "First, the left side of the figure presents the raw text of a paragraph in a persuasive essay, with the ACs contained in square brackets.", "labels": [], "entities": []}, {"text": "Squiggly vs straight underlining differentiates between claims and premises, respectively.", "labels": [], "entities": []}, {"text": "The ACs have been annotated as to how they are linked, and the right side of the figure reflects this structure.", "labels": [], "entities": []}, {"text": "The argument: An example of argument structure with four ACs.", "labels": [], "entities": []}, {"text": "The left side shows raw text that has been annotated for the presence of ACs.", "labels": [], "entities": []}, {"text": "Squiggly or straight underlining means an AC is a claim or premise, respectively.", "labels": [], "entities": []}, {"text": "The ACs in the text have also been annotated for links to other ACs, which is shown in the right figure.", "labels": [], "entities": []}, {"text": "ACs 3 and 4 are premises that link to another premise, AC2.", "labels": [], "entities": []}, {"text": "Finally, AC2 links to a claim, AC1.", "labels": [], "entities": []}, {"text": "AC1 therefore acts as the central argumentative component.", "labels": [], "entities": []}, {"text": "structure with four ACs forms a tree, where AC2 has two incoming links, and AC1 acts as the head, with no outgoing links.", "labels": [], "entities": []}, {"text": "We also specify the type of AC, with the head AC marked as a claim and the remaining ACs marked as premises.", "labels": [], "entities": []}, {"text": "Lastly, we note that the order of argument components can be a strong indicator of how components should relate.", "labels": [], "entities": []}, {"text": "Linking to the first argument component can provide a competitive baseline heuristic (.", "labels": [], "entities": []}, {"text": "Given the above considerations, we propose that sequence-to-sequence attention modeling, in the spirit of a Pointer Network (PN) (), can be effective for predicting argument structure.", "labels": [], "entities": [{"text": "sequence-to-sequence attention modeling", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.6575754980246226}, {"text": "predicting argument structure", "start_pos": 154, "end_pos": 183, "type": "TASK", "confidence": 0.9013919432957967}]}, {"text": "To the best of our knowledge, a clean, elegant implementation of a PN-based model has yet to be introduced for discourse parsing tasks.", "labels": [], "entities": [{"text": "discourse parsing tasks", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.7885787089665731}]}, {"text": "A PN is a sequence-to-sequence model ) that outputs a distribution over the encoding indices at each decoding timestep.", "labels": [], "entities": []}, {"text": "More generally, it is a recurrent model with attention (), and we claim that as such, it is promising for link extraction because it inherently possesses three important characteristics: (1) it is able to model the sequential nature of ACs, (2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure, and (3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks.", "labels": [], "entities": [{"text": "link extraction", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.7240992784500122}]}, {"text": "Furthermore, we believe the sequence-tosequence aspect of the model provides two distinct benefits: (1) it allows for two separate representations of a single AC (one for the source and one for the target of the link), and (2) the decoder networkcould learn to predict correct sequences of linked indices, which is a second recurrence over ACs.", "labels": [], "entities": []}, {"text": "Note that we also test the sequence-to-sequence architecture against a simplified model that only uses hidden states from an encoding network to make predictions (see Section 5).", "labels": [], "entities": []}, {"text": "The main technical contribution of our work is a joint model that simultaneously predicts links between ACs and determines their type.", "labels": [], "entities": []}, {"text": "Our joint model uses the hidden representation of ACs produced during the encoding step (see Section 3.4).", "labels": [], "entities": []}, {"text": "While PNs were originally proposed to allow a variable length decoding sequence, our model differs in that it decodes for the same number of timesteps as there are inputs.", "labels": [], "entities": []}, {"text": "This is a key insight that allows fora sequence-to-sequence model to be used for structural prediction.", "labels": [], "entities": [{"text": "structural prediction", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.9600801169872284}]}, {"text": "Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of.", "labels": [], "entities": []}, {"text": "Lastly, in respect to the broad task of parsing, our model is flexible because it can easily handle nonprojective, multi-root dependencies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.978094756603241}]}, {"text": "We evaluate our models on the corpora of and, and compare our results with the results of the aformentioned authors.", "labels": [], "entities": []}, {"text": "Our results show that (1) joint modeling is imperative for competitive performance on the link extraction task, (2) the presence of the second recurrence improves performance over a non-sequence-to-sequence model, and (3) the joint model can outperform models with heavy featureengineering and corpus-specific constraints.", "labels": [], "entities": [{"text": "link extraction task", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.8296619057655334}]}], "datasetContent": [{"text": "As we have mentioned, our work assumes that ACs have already been identified.", "labels": [], "entities": [{"text": "ACs", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.8382964730262756}]}, {"text": "The order of ACs corresponds directly to the order in which the ACs appear in the text.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our proposed model on a dataset of persuasive essays (PEC), as well as a dataset of microtexts (MTC)).", "labels": [], "entities": []}, {"text": "The feature space for the PEC has roughly 3,000 dimensions, and the MTC feature space has between 2,500 and 3,000 dimensions, depending on the data split.", "labels": [], "entities": [{"text": "PEC", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.8852035999298096}, {"text": "MTC feature space", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9222699602444967}]}, {"text": "The PEC contains a total of 402 essays, with a frozen set of 80 essays held out for testing.", "labels": [], "entities": [{"text": "PEC", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7572286128997803}]}, {"text": "There are three AC types in this corpus: major claim, claim, and premise.", "labels": [], "entities": []}, {"text": "In this corpus, individual structures can be either trees or forests.", "labels": [], "entities": []}, {"text": "Also, in this corpus, each essay has multiple paragraphs, and argument structure is only uncovered within a given paragraph.", "labels": [], "entities": []}, {"text": "The MTC contains 112 short texts.", "labels": [], "entities": [{"text": "The MTC contains 112 short texts", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.8212091128031412}]}, {"text": "Unlike the PEC, each text in this corpus is itself a complete example, as well as a singletree.", "labels": [], "entities": []}, {"text": "Since the dataset is small, the authors have created 10 sets of 5-fold cross-validation, reporting the the average across all splits for final model evaluation.", "labels": [], "entities": []}, {"text": "This corpus contains only two types of ACs: claim and premise.", "labels": [], "entities": []}, {"text": "Note that link prediction is directed, i.e., predicting a link between the pair We implement our models in TensorFlow (.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7209013998508453}]}, {"text": "We use the following parameters: hidden input dimension size 512, hidden layer size 256 for the bidirectional LSTMs, hidden layer size 512 for the LSTM decoder, \u03b1 equal to 0.5, and dropout () of 0.9.", "labels": [], "entities": []}, {"text": "We believe the need for such high dropout is due to the small amounts of training data, particularly in the MTC.", "labels": [], "entities": [{"text": "MTC", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.6954421401023865}]}, {"text": "All models are trained with Adam optimizer) with a batch size of 16.", "labels": [], "entities": []}, {"text": "For a given training set, we randomly select 10% to become the validation set.", "labels": [], "entities": []}, {"text": "Training occurs for 4,000 epochs.", "labels": [], "entities": []}, {"text": "Once training is completed, we select the model with the highest validation accuracy (on the link prediction task) and evaluate it on the held-out test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.7510139346122742}, {"text": "link prediction task", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.7789713144302368}]}, {"text": "At test time, we take a greedy approach and select the index of the probability distribution (whether link or type prediction) with the highest value.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the Persuasive Essay corpus. All models we tested are joint models, except for  the Single-Task Model model, which only predicts links. All model have a fully-connected input layer,  except for the row titled 'Joint Model No FC Input'. See Section 5 for a full description of the models.", "labels": [], "entities": []}, {"text": " Table 2: Results on the Microtext corpus.", "labels": [], "entities": [{"text": "Microtext corpus", "start_pos": 25, "end_pos": 41, "type": "DATASET", "confidence": 0.7186621874570847}]}, {"text": " Table 3: Feature ablation study. * indicates that both BOW and Structural are present, as well as the  stated embedding type.", "labels": [], "entities": [{"text": "BOW", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9862467050552368}]}, {"text": " Table 4: Results of binning test data by length of AC sequence. * indicates that this bin does not contain  any major claim labels, and this average only applies to claim and premise classes. However, we do not  disable the model from predicting this class: the model was able to avoid predicting this class on its own.", "labels": [], "entities": []}]}