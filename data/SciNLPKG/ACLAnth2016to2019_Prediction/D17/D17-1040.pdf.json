{"title": [{"text": "Efficient Attention using a Fixed-Size Memory Representation", "labels": [], "entities": [{"text": "Efficient Attention", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.713553786277771}]}], "abstractContent": [{"text": "The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step.", "labels": [], "entities": []}, {"text": "In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient.", "labels": [], "entities": []}, {"text": "Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory.", "labels": [], "entities": []}, {"text": "We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences.", "labels": [], "entities": []}, {"text": "By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence-to-sequence models) have achieved state of the art results across a wide variety of tasks, including Neural Machine Translation (NMT) (), text summarization (, speech recognition (, image captioning (, and conversational modeling ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.8052992622057596}, {"text": "text summarization", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.7921293377876282}, {"text": "speech recognition", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7849766314029694}, {"text": "image captioning", "start_pos": 191, "end_pos": 207, "type": "TASK", "confidence": 0.7311971783638}, {"text": "conversational modeling", "start_pos": 215, "end_pos": 238, "type": "TASK", "confidence": 0.8042854964733124}]}, {"text": "The most popular approaches are based on an encoder-decoder architecture consisting of two recurrent neural networks (RNNs) and an attention mechanism that aligns target to source tokens (.", "labels": [], "entities": []}, {"text": "The typical attention mechanism used in these architectures computes anew attention context at each decoding * Equal Contribution.", "labels": [], "entities": []}, {"text": "step based on the current state of the decoder.", "labels": [], "entities": []}, {"text": "Intuitively, this corresponds to looking at the source sequence after the output of every single target token.", "labels": [], "entities": []}, {"text": "Inspired by how humans process sentences, we believe it maybe unnecessary to look back at the entire original source sequence at each step.", "labels": [], "entities": []}, {"text": "We thus propose an alternative attention mechanism (section 3) that leads to smaller computational time complexity.", "labels": [], "entities": []}, {"text": "Our method predicts K attention context vectors while reading the source, and learns to use a weighted average of these vectors at each step of decoding.", "labels": [], "entities": []}, {"text": "Thus, we avoid looking back at the source sequence once it has been encoded.", "labels": [], "entities": []}, {"text": "We show (section 4) that this speeds up inference while performing on-par with the standard mechanism on both toy and real-world WMT translation datasets.", "labels": [], "entities": [{"text": "WMT translation", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.8789292573928833}]}, {"text": "We also show that our mechanism leads to larger speedups as sequences get longer.", "labels": [], "entities": []}, {"text": "Finally, by visualizing the attention scores (section 5), we verify that the proposed technique learns meaningful alignments, and that different attention context vectors specialize on different parts of the source.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the reduction of computational time complexity we expect our method to yield performance gains especially for longer sequences and tasks where the source can be compactly represented in a fixed-size memory matrix.", "labels": [], "entities": []}, {"text": "To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in.", "labels": [], "entities": [{"text": "speed", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9903839826583862}]}, {"text": "We generated 4 training datasets of 100,000 examples and a validation dataset of 1,000 examples.", "labels": [], "entities": []}, {"text": "The vocabulary size was 20.", "labels": [], "entities": []}, {"text": "For each dataset, the sequences had lengths randomly chosen between 0 to L, for L\u2208{10,50,100,200} unique to each dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores and computation times with  varying K and sequence length compared to baseline  models with and without attention.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983934760093689}]}, {"text": " Table 2: BLEU scores on WMT'17 translation datasets from the memory attention models and regular attention  baselines. We picked the best out of the four scoring function combinations on the validation set. Note that  en-tr does not have an official test set. Best test scores on each dataset are highlighted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999299168586731}, {"text": "WMT'17 translation datasets", "start_pos": 25, "end_pos": 52, "type": "DATASET", "confidence": 0.8331584731737772}]}, {"text": " Table 3: Decoding time, averaged across 10 runs, for  the en-de validation set (2169 examples) with average  sequence length of 35. Results are similar for both  PE and non-PE models.", "labels": [], "entities": []}]}