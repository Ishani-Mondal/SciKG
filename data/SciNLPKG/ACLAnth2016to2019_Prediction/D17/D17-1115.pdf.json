{"title": [{"text": "Tensor Fusion Network for Multimodal Sentiment Analysis", "labels": [], "entities": [{"text": "Multimodal Sentiment Analysis", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.6685981849829356}]}], "abstractContent": [{"text": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a mul-timodal setup where other relevant modalities accompany language.", "labels": [], "entities": [{"text": "Multimodal sentiment analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8447943528493246}, {"text": "sentiment analysis", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.9167338311672211}]}, {"text": "In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7407617171605428}]}, {"text": "We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end.", "labels": [], "entities": []}, {"text": "The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice.", "labels": [], "entities": []}, {"text": "In the experiments , our model outperforms state-of-the-art approaches for both multimodal and unimodal sentiment analysis.", "labels": [], "entities": [{"text": "unimodal sentiment analysis", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.7080480655034384}]}], "introductionContent": [{"text": "Multimodal sentiment analysis) is an increasingly popular area of affective computing research () that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).", "labels": [], "entities": [{"text": "Multimodal sentiment analysis", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8023101290067037}, {"text": "generalizing text-based sentiment analysis", "start_pos": 114, "end_pos": 156, "type": "TASK", "confidence": 0.7477915287017822}]}, {"text": "This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis  since there is a growing trend of sharing opinions in videos instead of text, specially in social media).", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.7694165706634521}, {"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8476628065109253}]}, {"text": "The central challenge in multimodal sentiment analysis is to model the inter-modality dynamics: the interactions between \u2020 means equal contribution language, visual and acoustic behaviors that change the perception of the expressed sentiment.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.7603321075439453}]}, {"text": "illustrates these complex inter-modality dynamics.", "labels": [], "entities": []}, {"text": "The utterance \"This movie is sick\" can be ambiguous (either positive or negative) by itself, but if the speaker is also smiling at the same time, then it will be perceived as positive.", "labels": [], "entities": []}, {"text": "On the other hand, the same utterance with a frown would be perceived negatively.", "labels": [], "entities": []}, {"text": "A person speaking loudly \"This movie is sick\" would still be ambiguous.", "labels": [], "entities": []}, {"text": "These examples are illustrating bimodal interactions.", "labels": [], "entities": []}, {"text": "Examples of trimodal interactions are shown in Figure 1 when loud voice increases the sentiment to strongly positive.", "labels": [], "entities": []}, {"text": "The complexity of inter-modality dynamics is shown in the second trimodal example where the utterance \"This movie is fair\" is still weakly positive, given the strong influence of the word \"fair\".", "labels": [], "entities": []}, {"text": "A second challenge in multimodal sentiment analysis is efficiently exploring intra-modality dynamics of a specific modality (unimodal interaction).", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7861681977907816}]}, {"text": "Intra-modality dynamics are particularly challenging for the language analysis since multimodal sentiment analysis is performed on spoken language.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 85, "end_pos": 114, "type": "TASK", "confidence": 0.6623568634192148}]}, {"text": "A spoken opinion such as \"I think it was alright . .", "labels": [], "entities": []}, {"text": "ok yeah\" almost never happens in written text.", "labels": [], "entities": []}, {"text": "This volatile nature of spoken opinions, where proper language structure is often ignored, complicates sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9656948745250702}]}, {"text": "Visual and acoustic modalities also contain their own intra-modality dynamics which are expressed through both space and time.", "labels": [], "entities": []}, {"text": "Previous works in multimodal sentiment analysis does not account for both intra-modality and intermodality dynamics directly, instead they either perform early fusion (a.k.a., feature-level fusion) or late fusion (a.k.a., decision-level fusion).", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 18, "end_pos": 47, "type": "TASK", "confidence": 0.759586234887441}, {"text": "decision-level fusion", "start_pos": 222, "end_pos": 243, "type": "TASK", "confidence": 0.7523389756679535}]}, {"text": "Early fusion consists in simply concatenating multimodal features mostly at input level.", "labels": [], "entities": []}, {"text": "This fusion approach does not allow the intra-modality dynamics to be efficiently modeled.", "labels": [], "entities": []}, {"text": "This is due to the fact that inter-modality dynamics can be more complex at input level and can dominate the learning processor result in overfitting.", "labels": [], "entities": []}, {"text": "Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (.", "labels": [], "entities": [{"text": "Late fusion", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6669290661811829}, {"text": "decision voting", "start_pos": 93, "end_pos": 108, "type": "TASK", "confidence": 0.8475252985954285}]}, {"text": "This prevents the model from learning inter-modality dynamics in an efficient way by assuming that simple weighted averaging is a proper fusion approach.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew model, termed Tensor Fusion Network (TFN), which learns both the intra-modality and inter-modality dynamics end-to-end.", "labels": [], "entities": []}, {"text": "Inter-modality dynamics are modeled with anew multimodal fusion approach, named Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions.", "labels": [], "entities": [{"text": "Tensor Fusion", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7653832733631134}]}, {"text": "Intramodality dynamics are modeled through three Modality Embedding Subnetworks, for language, visual and acoustic modalities, respectively.", "labels": [], "entities": []}, {"text": "In our extensive set of experiments, we show (a) that TFN outperforms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 111, "end_pos": 140, "type": "TASK", "confidence": 0.7980198264122009}, {"text": "multimodal sentiment analysis", "start_pos": 217, "end_pos": 246, "type": "TASK", "confidence": 0.7940618793169657}, {"text": "unimodal state-of-the-art unimodal sentiment analysis", "start_pos": 365, "end_pos": 418, "type": "TASK", "confidence": 0.7559471964836121}]}], "datasetContent": [{"text": "In this paper, we devise three sets of experiments each addressing a different research question: Experiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 176, "end_pos": 205, "type": "TASK", "confidence": 0.7728502551714579}]}, {"text": "Experiment 2: We study the importance of the TFN subtensors and the impact of each individual modality (see).", "labels": [], "entities": []}, {"text": "We also compare with the commonly-used early fusion approach.", "labels": [], "entities": []}, {"text": "Experiment 3: We compare the performance of our three modality-specific networks (language, visual and acoustic) with state-of-the-art unimodal approaches.", "labels": [], "entities": []}, {"text": "Section 5.4 describes our experimental methodology which is kept constant across all experiments.", "labels": [], "entities": []}, {"text": "Section 6 will discuss our results in more details with a qualitative analysis.: Comparison with state-of-the-art approaches for multimodal sentiment analysis.", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 129, "end_pos": 158, "type": "TASK", "confidence": 0.7989415327707926}]}, {"text": "TFN outperforms both neural and non-neural approaches as shown by \u2206 SOT A .  We also perform a comparison with the early fusion approach (TFN early ) by simply concatenating all three modality embeddings < z l , z a , z v > and passing it directly as input to U s . This approach was depicted on the left side of.", "labels": [], "entities": []}, {"text": "When looking at results, we see that our TFN approach outperforms the early fusion approach 2 .  In this experiment, we compare the performance of our Modality Embedding Networks with stateof-the-art approaches for language-based, visualbased and acoustic-based sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 262, "end_pos": 280, "type": "TASK", "confidence": 0.7760184109210968}]}], "tableCaptions": [{"text": " Table 1: Comparison with state-of-the-art ap- proaches for multimodal sentiment analysis. TFN  outperforms both neural and non-neural approaches  as shown by \u2206 SOT A .", "labels": [], "entities": [{"text": "multimodal sentiment analysis", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.6738184988498688}]}, {"text": " Table 2: Comparison of TFN with its subtensor  variants. All the unimodal, bimodal and trimodal  subtensors are important. TFN also outperforms  early fusion.", "labels": [], "entities": []}, {"text": " Table 3: Language Sentiment Analysis. Compari- son of with state-of-the-art approaches for language  sentiment analysis. \u2206 SOT A  language shows improvement.", "labels": [], "entities": [{"text": "Language Sentiment Analysis", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.7834870219230652}, {"text": "language  sentiment analysis", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.806746373573939}]}, {"text": " Table 4: Visual Sentiment Analysis. Comparison  with state-of-the-art approaches for visual senti- ment analysis and emotion recognition. \u2206 SOT A", "labels": [], "entities": [{"text": "Visual Sentiment Analysis", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6877527038256327}, {"text": "senti- ment analysis", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.753885418176651}, {"text": "emotion recognition", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7412481755018234}]}, {"text": " Table 5: Acoustic Sentiment Analysis. Compari- son with state-of-the-art approaches for audio sen- timent analysis and emotion recognition. \u2206 SOT A", "labels": [], "entities": [{"text": "Acoustic Sentiment Analysis", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6842020650704702}, {"text": "audio sen- timent analysis", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.6535983920097351}, {"text": "emotion recognition", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7512636482715607}]}, {"text": " Table 6: Examples from the CMU-MOSI dataset. The ground truth sentiment labels are between strongly  negative (-3) and strongly positive (+3). For each example, we show the prediction output of the three  unimodal models ( TFN acoustic , TFN visual and TFN language ), the early fusion model TFN early and our  proposed TFN approach. TFN early seems to be mostly replicating language modality while our TFN  approach successfully integrate intermodality dynamics to predict the sentiment level.", "labels": [], "entities": [{"text": "CMU-MOSI dataset", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.967458188533783}]}]}