{"title": [{"text": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog", "labels": [], "entities": []}], "abstractContent": [{"text": "A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision!", "labels": [], "entities": []}, {"text": "In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one-showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not inter-pretable or compositional.", "labels": [], "entities": []}, {"text": "In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature.", "labels": [], "entities": []}, {"text": "We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.", "labels": [], "entities": []}], "introductionContent": [{"text": "One fundamental goal of artificial intelligence (AI) is the development of goal-driven dialog agents -specifically, agents that can perceive their environment (through vision, audition, or other sensors), and communicate with humans or other agents in natural language towards a goal.", "labels": [], "entities": []}, {"text": "While historically such agents have been based on slot filling (), the dominant paradigm today is neural dialog models,b) trained on large quantities of data.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 50, "end_pos": 62, "type": "TASK", "confidence": 0.8388976752758026}]}, {"text": "Perhaps somewhat counterintuitively, this current paradigm treats dialog as a static supervised learning problem, rather than as the interactive agent learning problem that it naturally is.", "labels": [], "entities": []}, {"text": "Specifically, atypical pipeline is to collect a large dataset of human-human dialog (, inject a machine in the middle of a dialog from the dataset, and supervise it to mimic the human response.", "labels": [], "entities": []}, {"text": "While this teaches the agent correlations between symbols, it does not convey the functional meaning of language, grounding (mapping physical concepts to words), compositionality (combining knowledge of simpler concepts to describe richer concepts), or aspects of planning (why are we having this conversation?).", "labels": [], "entities": []}, {"text": "An alternative paradigm that has along history) and is witnessing a recent resurgence () -is situated language learning.", "labels": [], "entities": []}, {"text": "A number of recent works have proposed reinforcement learning techniques for learning the communication protocols of agents situated in virtual environments in a completely end-to-end manner -from perceptual input (e.g. pixels) to communication (discrete symbols without any prespecified meanings) to action (e.g. signaling in reference games or navigating in an environment) -and have simultaneously found the emergence of grounded human-interpretable (often compositional) language among agents, without any human supervision or pretraining, simply to succeed at the task.", "labels": [], "entities": [{"text": "learning the communication protocols of agents situated in virtual environments in a completely end-to-end manner -from perceptual input (e.g. pixels) to communication (discrete symbols without any prespecified meanings) to action (e.g. signaling in reference games or navigating in an environment", "start_pos": 77, "end_pos": 374, "type": "Description", "confidence": 0.7386799234410991}]}, {"text": "In this short paper, we study the following question -what are the conditions that lead to the emergence of human-interpretable or compositional grounded language?", "labels": [], "entities": []}, {"text": "Our key finding is that natural language does not emerge 'naturally' in multi-agent dialog, despite independently reported successful demonstrations in recent literature.", "labels": [], "entities": []}, {"text": "Specifically, in a sequence of 'negative' results culminating in a 'positive' one, we find that while agents always successfully invent communication protocols and languages to achieve their goals with near-perfect accuracies, the invented languages are decidedly not compositional, interpretable, or 'natural'; and that it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.", "labels": [], "entities": []}, {"text": "The starting point for our investigation is the recent work of, who proposed a cooperative reference game between two agents, where communication is necessary to accomplish the goal due to an information asymmetry.", "labels": [], "entities": []}, {"text": "Our key contribution over is an exhaustive study of the conditions that must be present before compositional grounded language emerges, and subtle but important differences in execution -tabular QLearning (which does not scale) vs. REINFORCE (which does), and generalization to novel environments (not studied in prior work).", "labels": [], "entities": [{"text": "REINFORCE", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9635311365127563}]}, {"text": "In the spirit of , we hope our findings shed more light into the interpretability of languages invented in cooperative multi-agent settings, place recent work inappropriate context, and inform fruitful directions for future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Overview of settings we explore to analyze the lan- guage learnt by two agents in a cooperative game, Task &  Talk. Last two columns measure generalization in terms of  prediction accuracy of both or at least one of the attribute  pair, on a held-out test set containing unseen instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.8517995476722717}]}]}