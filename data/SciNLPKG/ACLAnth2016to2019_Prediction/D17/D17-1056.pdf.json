{"title": [{"text": "Refining Word Embeddings for Sentiment Analysis", "labels": [], "entities": [{"text": "Refining Word Embeddings", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8630086382230123}, {"text": "Sentiment Analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9625146090984344}]}], "abstractContent": [{"text": "Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks.", "labels": [], "entities": []}, {"text": "However , existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information.", "labels": [], "entities": []}, {"text": "This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.8596057295799255}]}, {"text": "Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe).", "labels": [], "entities": [{"text": "word vector refinement", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6731788317362467}, {"text": "Word2vec", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9579441547393799}]}, {"text": "The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SST)", "start_pos": 199, "end_pos": 232, "type": "DATASET", "confidence": 0.8848552703857422}]}], "introductionContent": [{"text": "Word embeddings area technique to learn continuous low-dimensional vector space representations of words by leveraging the contextual information from large corpora.", "labels": [], "entities": []}, {"text": "Examples include C&W), and).", "labels": [], "entities": []}, {"text": "In addition to the contextual information, characterlevel subwords () and semantic knowledge resources) such as WordNet are also useful information for learning word embeddings.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9656739830970764}]}, {"text": "These embeddings have been successfully used for various natural language processing tasks.", "labels": [], "entities": []}, {"text": "In general, existing word embeddings are semantically oriented.", "labels": [], "entities": []}, {"text": "They can capture semantic and syntactic information from unlabeled data in an unsupervised manner but fail to capture sufficient sentiment information.", "labels": [], "entities": []}, {"text": "This makes it difficult to directly apply existing word embeddings to sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9414328336715698}]}, {"text": "Prior studies have reported that words with similar vector representations (similar contexts) may have opposite sentiment polarities, as in the example of happy-sad mentioned in and good-bad in (.", "labels": [], "entities": []}, {"text": "Composing these word vectors may produce sentence vectors with similar vector representations but opposite sentiment polarities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations).", "labels": [], "entities": []}, {"text": "Building on such ambiguous vectors will affect sentiment classification performance.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.9742108583450317}]}, {"text": "To enhance the performance of distinguishing words with similar vector representations but opposite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner.", "labels": [], "entities": []}, {"text": "The common goal of these methods is to capture both semantic/syntactic and sentiment information such that sentimentally similar words have similar vector representations.", "labels": [], "entities": []}, {"text": "They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and negative) given by the training instances.", "labels": [], "entities": []}, {"text": "The use of such sentiment embeddings has improved the performance of binary sentiment classification.", "labels": [], "entities": [{"text": "binary sentiment classification", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.7362156907717387}]}, {"text": "great (7.50) bad (3.24) terrific (7.12) decent nice Ranked by cosine similarity: Example of nearest neighbor ranking.", "labels": [], "entities": []}, {"text": "This study adopts another strategy to obtain both semantic and sentiment word vectors.", "labels": [], "entities": []}, {"text": "Instead of building anew word embedding model from labeled data, we propose a word vector refinement model to refine existing semantically oriented word vectors using sentiment lexicons.", "labels": [], "entities": []}, {"text": "That is, the proposed model can be applied to the pre-trained vectors obtained by any word representation learning models (e.g., and GloVe) as a post-processing step to adapt the pre-trained vectors to sentiment applications.", "labels": [], "entities": []}, {"text": "The refinement model is based on adjusting the pre-trained vector of each affective word in a given sentiment lexicon such that it can be closer to a set of both semantically and sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neighbors (i.e., those with an opposite polarity).", "labels": [], "entities": []}, {"text": "The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and outperform previously proposed sentiment embeddings.", "labels": [], "entities": []}, {"text": "To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) are selected, including convolutional neural networks (CNN), deep averaging network (DAN)) and long-short term memory (LSTM)).", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank (SST)", "start_pos": 80, "end_pos": 113, "type": "DATASET", "confidence": 0.8353512485822042}]}, {"text": "The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed sentiment embeddings to re-run the classification for performance comparison.", "labels": [], "entities": []}, {"text": "The SST is chosen because it can show the effect of using different word embeddings on fine-grained sentiment classification, whereas prior studies only reported binary classification results.", "labels": [], "entities": [{"text": "SST", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8779486417770386}, {"text": "sentiment classification", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.8178359866142273}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the proposed word vector refinement model.", "labels": [], "entities": [{"text": "word vector refinement", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.670714388291041}]}, {"text": "Section 3 presents the evaluation results.", "labels": [], "entities": []}, {"text": "Conclusions are drawn in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section evaluates the proposed refinement model, conventional word embeddings and previously proposed sentiment embeddings using several deep neural network models for binary and fine-grained sentiment classification.", "labels": [], "entities": [{"text": "fine-grained sentiment classification", "start_pos": 184, "end_pos": 221, "type": "TASK", "confidence": 0.6585470736026764}]}, {"text": "SST was adopted as the evaluation corpus).", "labels": [], "entities": [{"text": "SST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5139737725257874}]}, {"text": "The binary classification subtask (positive and negative) contains 6920/872/1821 samples for the train/dev/test sets, while the fine-grained ordinal classification subtask (very negative, negative, neutral, positive, and very positive) contains 8544/1101/2210 samples of the train/dev/test sets.", "labels": [], "entities": []}, {"text": "The word embeddings used for comparison included two conventional word embeddings (GloVe and Word2vec), our refined versions (Re(GloVe) and Re(Word2vec)), and previously proposed sentiment embeddings (HyRank) (.", "labels": [], "entities": []}, {"text": "We used the same dimensionality of 300 for all word embeddings.", "labels": [], "entities": []}, {"text": "\uf06c GloVe and Word2vec: The respective GloVe and Word2vec (skip-gram) were pre-trained on Common Crawl 840B 1 and GoogleNews 2 . 1 http://nlp.stanford.edu/projects/glove/ 2 https://code.google.com/archive/p/word2vec/ \uf06c Re(Glove) and Re(Word2vec): Both the pretrained GloVe and Word2vec were refined using E-ANEW (.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9208816885948181}, {"text": "Common Crawl 840B 1", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.9405344724655151}]}, {"text": "Each affective word was refined by its top k=10 nearest neighbors with parameters of \u03b1:\u03b2=0.1 (1:10) (see Eq.).", "labels": [], "entities": []}, {"text": "\uf06c HyRank: It was trained using SST, NRC Sentiment140 and IMDB datasets.", "labels": [], "entities": [{"text": "\uf06c", "start_pos": 0, "end_pos": 1, "type": "DATASET", "confidence": 0.9058180451393127}, {"text": "HyRank", "start_pos": 2, "end_pos": 8, "type": "DATASET", "confidence": 0.7017694711685181}, {"text": "NRC Sentiment140", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.9021805822849274}, {"text": "IMDB datasets", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9252179563045502}]}, {"text": "We compared this method because its code is publicly accessible 3 . Classifiers.", "labels": [], "entities": []}, {"text": "After the proposed refinement model was applied, both the pre-trained Word2vec and GloVe were improved.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9671752452850342}]}, {"text": "The Re(Word2vec) and Re(GloVe) respectively improved Word2vec and GloVe by 1.7% and 1.5% averaged overall classifiers for binary classification, and both 1.6% for finegrained classification.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9624651074409485}]}, {"text": "In addition, both Re(GloVe) and Re(Word2vec) outperformed the sentiment embeddings HyRank for all classifiers on both binary and fine-grained classification, indicating that the real-valued intensity scores used by the proposed refinement model are more effective than the binary polarity labels used by the previously proposed sentiment embedings.", "labels": [], "entities": []}, {"text": "The proposed method yielded better performance because it can remove semantically similar but sentimentally dissimilar nearest neighbors for the target words by refining their vector representations.", "labels": [], "entities": []}, {"text": "To demonstrate the effect, we define a measure noise@k to calculate the percentage of top k nearest neighbors with an opposite polarity (i.e., noise) to each word in E-ANEW.", "labels": [], "entities": []}, {"text": "For instance, in, the noise@10 for good is 20% because there are two words with an opposite polarity to good among its top 10 nearest neighbors.", "labels": [], "entities": []}, {"text": "shows the average noise@10 for different word embeddings.", "labels": [], "entities": []}, {"text": "For the two semantic-oriented word vectors, GloVe and Word2vec, on average around 24% of the top 10 nearest neighbors for each word are noisy words.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9210411310195923}]}, {"text": "After refinement, both Re(GloVe) and Re(Word2vec) can reduce noise@10 to around 14%.", "labels": [], "entities": []}, {"text": "The HyRank also yielded better performance than both GloVe and Word2vec.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.953971266746521}]}], "tableCaptions": [{"text": " Table 1: Accuracy of different classifiers with  different word embeddings for binary and fine- grained classification.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.965785801410675}]}, {"text": " Table 2: Average percentages of noisy words  in the top 10 nearest neighbors for different  word embeddings.", "labels": [], "entities": []}]}