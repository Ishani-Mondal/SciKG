{"title": [{"text": "Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9125819504261017}]}], "abstractContent": [{"text": "We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet).", "labels": [], "entities": [{"text": "transfer learning in machine comprehension (MC)", "start_pos": 27, "end_pos": 74, "type": "TASK", "confidence": 0.8900134265422821}]}, {"text": "Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs.", "labels": [], "entities": []}, {"text": "Using the proposed SynNet with a pretrained model on the SQuAD dataset, we achieve an F1 measure of 46.6% on the challenging NewsQA dataset, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline by 7.6%, without use of provided annotations.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9762708842754364}, {"text": "F1", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.9997746348381042}, {"text": "NewsQA dataset", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.9904370903968811}, {"text": "F1 measure", "start_pos": 186, "end_pos": 196, "type": "METRIC", "confidence": 0.9848396182060242}]}], "introductionContent": [{"text": "Machine comprehension (MC), the ability to answer questions over a provided context paragraph, is a key task in natural language processing.", "labels": [], "entities": [{"text": "Machine comprehension (MC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7118268847465515}, {"text": "natural language processing", "start_pos": 112, "end_pos": 139, "type": "TASK", "confidence": 0.6295492053031921}]}, {"text": "The rise of high-quality, large-scale human-annotated datasets for this task) has allowed for the training of data-intensive but expressive models such as deep neural networks (.", "labels": [], "entities": []}, {"text": "Moreover, these datasets have the attractive quality that the answer is a short snippet of text within the paragraph, which narrows the search space of possible answer spans.", "labels": [], "entities": []}, {"text": "However, many of these models rely on large amounts of human-labeled data for training.", "labels": [], "entities": []}, {"text": "Yet * Work performed while interning at Microsoft Research.", "labels": [], "entities": []}, {"text": "\u2020 Work performed when the author was at Microsoft Research.", "labels": [], "entities": []}, {"text": "Code will be available at https://github.com/ davidgolub/QuestionGeneration data collection is a time-consuming and expensive task.", "labels": [], "entities": [{"text": "QuestionGeneration data collection", "start_pos": 57, "end_pos": 91, "type": "DATASET", "confidence": 0.7773575981458029}]}, {"text": "Moreover, direct application of a MC model trained on one domain to answer questions over paragraphs from another domain may suffer performance degradation.", "labels": [], "entities": []}, {"text": "While understudied, the ability to transfer a MC model to multiple domains is of great practical importance.", "labels": [], "entities": []}, {"text": "For instance, the ability to quickly use a MC model trained on Wikipedia to bootstrap a question-answering system over customer support manuals or news articles, where there is no labeled data, can unlock a great number of practical applications.", "labels": [], "entities": []}, {"text": "In this paper, we address this problem in MC through a two-stage synthesis network (SynNet).", "labels": [], "entities": []}, {"text": "The SynNet generates synthetic question-answer pairs over paragraphs in anew domain that are then used in place of human-generated annotations to finetune a MC model trained on the original domain.", "labels": [], "entities": []}, {"text": "The idea of generating synthetic data to augment insufficient training data has been explored before.", "labels": [], "entities": []}, {"text": "For example, for the target task of translation, present a method to generate synthetic translations given real sentences to refine an existing machine translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9717304706573486}]}, {"text": "However, unlike machine translation, for tasks like MC, we need to synthesize both the question and answers given the context paragraph.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7303834706544876}]}, {"text": "Moreover, while the question is a syntactically fluent natural language sentence, the answer is mostly a salient semantic concept in the paragraph, e.g., a named entity, an action, or a number, which is often a single word or short phrase.", "labels": [], "entities": []}, {"text": "Since the answer has a very different linguistic structure compared to the question, it maybe more appropri-: Illustration of the two-stage SynNet.", "labels": [], "entities": []}, {"text": "The SynNet is trained to synthesize the answer and the question, given the paragraph.", "labels": [], "entities": []}, {"text": "The first stage of the model, an answer synthesis module, uses a bi-directional LSTM to predict IOB tags on the input paragraph, which mark out key semantic concepts that are likely answers.", "labels": [], "entities": [{"text": "answer synthesis", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.789684385061264}]}, {"text": "The second stage, a question synthesis module, uses a uni-directional LSTM to generate the question, while attending on embeddings of the words in the paragraph and IOB ids.", "labels": [], "entities": [{"text": "question synthesis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7437024414539337}]}, {"text": "Although multiple spans in the paragraph could be identified as potential answers, we pick one span when generating the question.", "labels": [], "entities": []}, {"text": "ate to view answers and questions as two different types of data.", "labels": [], "entities": []}, {"text": "Hence, the synthesis of a (question, answer) tuple is needed.", "labels": [], "entities": []}, {"text": "In our approach, we decompose the process of generating question-answer pairs into two steps, answer generation conditioned on the paragraph, and question generation conditioned on the paragraph and answer.", "labels": [], "entities": [{"text": "question generation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7015904486179352}]}, {"text": "We generate the answer first because answers are usually key semantic concepts, while questions can be viewed as a full sentence composed to inquire the concept.", "labels": [], "entities": []}, {"text": "Using the proposed SynNet, we are able to outperform a strong baseline of directly applying a high-performing MC model trained on another domain.", "labels": [], "entities": []}, {"text": "For example, when we apply our algorithm using a pretrained model on the Stanford Question-Answering Dataset (SQuAD), which consists of Wikipedia articles, to answer questions on the NewsQA dataset (, which consists of CNN/Daily Mail articles, we improve the performance of the SQuAD baseline from 39.0% to 46.6% F1 and approach results of previously published work of (50.0% F1), without use of labeled data in the new domain.", "labels": [], "entities": [{"text": "Stanford Question-Answering Dataset (SQuAD)", "start_pos": 73, "end_pos": 116, "type": "DATASET", "confidence": 0.8212603082259496}, {"text": "NewsQA dataset", "start_pos": 183, "end_pos": 197, "type": "DATASET", "confidence": 0.9909710586071014}, {"text": "F1", "start_pos": 313, "end_pos": 315, "type": "METRIC", "confidence": 0.9981679916381836}, {"text": "F1", "start_pos": 376, "end_pos": 378, "type": "METRIC", "confidence": 0.9915748238563538}]}, {"text": "Moreover, an error analysis reveals that we achieve higher accuracy over the baseline on all common question types.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9987472295761108}]}], "datasetContent": [{"text": "We summarize the datasets we use in our experiments, parameters for our model architectures, and training details.", "labels": [], "entities": []}, {"text": "The SQuAD dataset consists of approximately 100,000 question-answer pairs on Wikipedia, 87,600 of which are used for training, 10,570 for development, and an unknown number in a hidden test set.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7661243081092834}]}, {"text": "The NewsQA dataset consists of 92,549 train, 5,166 development and 5,165 test questions on CNN/Daily Mail news articles.", "labels": [], "entities": [{"text": "NewsQA dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9876469969749451}, {"text": "CNN/Daily Mail news articles", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.8766610225041708}]}, {"text": "Both the domain type (i.e., news) and question types differ between the two datasets.", "labels": [], "entities": []}, {"text": "For example, an analysis of a randomly generated sample of 1,000 questions from both NewsQA and SQuAD reveals that approximately 74.1% of questions in SQuAD require word matching or paraphrasing to retrieve the answer, as opposed to 59.7% in NewsQA.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9735268354415894}, {"text": "SQuAD", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.8280024528503418}, {"text": "word matching", "start_pos": 165, "end_pos": 178, "type": "TASK", "confidence": 0.6696388423442841}, {"text": "NewsQA", "start_pos": 242, "end_pos": 248, "type": "DATASET", "confidence": 0.9887991547584534}]}, {"text": "As our test metrics, we report two numbers, exact match (EM) and F1 score.", "labels": [], "entities": [{"text": "exact match (EM)", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.9110104084014893}, {"text": "F1 score", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9885769784450531}]}, {"text": "We train a BIDAF model on the SQuAD train dataset and use a two-stage SynNet to finetune it on the NewsQA train dataset.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9788877367973328}, {"text": "SQuAD train dataset", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9547922809918722}, {"text": "NewsQA train dataset", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.987233301003774}]}, {"text": "We initialize word-embeddings for the BIDAF model, answer synthesis module, and question synthesis module with 300-dimensional-GloVe vectors () trained on the 840 Billion Words Common Crawl corpus.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.6122342944145203}, {"text": "answer synthesis", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.8499652445316315}, {"text": "question synthesis", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7872070372104645}, {"text": "840 Billion Words Common Crawl corpus", "start_pos": 159, "end_pos": 196, "type": "DATASET", "confidence": 0.6252402464548746}]}, {"text": "We set all embeddings of unknown word tokens to zero.", "labels": [], "entities": []}, {"text": "For both the answer synthesis and question synthesis module, we use a vocabulary of size 110,179.", "labels": [], "entities": [{"text": "answer synthesis", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.7936990559101105}, {"text": "question synthesis", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7343899458646774}]}, {"text": "We use LSTMs with hidden states of size 150 for the answer module vs. those of size 100 for the question module since the answer module is less memory intensive than the question module.", "labels": [], "entities": []}, {"text": "We train both the answer and question module with Adam () and a learning rate of 1e-2.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 64, "end_pos": 77, "type": "METRIC", "confidence": 0.9664272665977478}]}, {"text": "We train a BIDAF model with the default hyperparameters provided in the open-source repository.", "labels": [], "entities": [{"text": "BIDAF", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.8786856532096863}]}, {"text": "To stop training of the question synthesis module, after each epoch, we monitor both the loss as well as the quality of questions generated on the SQuAD development set.", "labels": [], "entities": [{"text": "question synthesis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7554155886173248}, {"text": "SQuAD development set", "start_pos": 147, "end_pos": 168, "type": "DATASET", "confidence": 0.6826158165931702}]}, {"text": "To stop training of the answer synthesis module, we similarly monitor predictions on the SQuAD development set.", "labels": [], "entities": [{"text": "answer synthesis", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.887209415435791}, {"text": "SQuAD development set", "start_pos": 89, "end_pos": 110, "type": "DATASET", "confidence": 0.7648290793100992}]}, {"text": "To train the question synthesis module, we only use the questions provided in the SQuAD train set.", "labels": [], "entities": [{"text": "question synthesis", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7723161280155182}, {"text": "SQuAD train set", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8859116236368815}]}, {"text": "However, to train the answer synthesis module, we further augment the human-annotated labels of each paragraph with tags from a simple NER system 6 because labels of answers provided in the train set are underspecified, i.e., many words in the paragraph that could be potential answers are not labeled.", "labels": [], "entities": [{"text": "answer synthesis", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8678264915943146}, {"text": "NER system 6", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.8579299449920654}]}, {"text": "Therefore, we assume any named entities could also be potential answers of certain questions, in addition to the answers explicitly labeled by annotators.", "labels": [], "entities": []}, {"text": "To generate question-answer pairs on the NewsQA train set using the SynNet, we first run every paragraph through our answer synthesis module.", "labels": [], "entities": [{"text": "NewsQA train set", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.9827345410982767}]}, {"text": "We then randomly sample up to 30 candidate answers extracted by our module, which we feed into the question synthesis module.", "labels": [], "entities": [{"text": "question synthesis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.6783511638641357}]}, {"text": "This results in 250,000 synthetic question-answer pairs that we can use to finetune our MC model.", "labels": [], "entities": []}, {"text": "We report the main results on the NewsQA test set, report brief results on SQuAD, conduct ablation studies, and conduct an error analysis.", "labels": [], "entities": [{"text": "NewsQA test set", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9914516806602478}, {"text": "SQuAD", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.7172450423240662}]}], "tableCaptions": [{"text": " Table 2: Main Results. Exact match (EM) and span F1 scores on the NewsQA test set of a BIDAF  model finetuned with our SynNet. M sq refers to a baseline BIDAF model trained on SQuAD, A gen ,  Q gen refers to using answers generated from our SynNet respectively to finetune the model on NewsQA,  A ner refers to using answers extracted from a standard NER system to generate questions. M  *   sq refers to  using the baseline SQUAD model in the ensemble.", "labels": [], "entities": [{"text": "Exact match (EM)", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.9615381002426148}, {"text": "span", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9629649519920349}, {"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.5927670001983643}, {"text": "NewsQA test set", "start_pos": 67, "end_pos": 82, "type": "DATASET", "confidence": 0.979052722454071}, {"text": "NewsQA", "start_pos": 287, "end_pos": 293, "type": "DATASET", "confidence": 0.9662185311317444}]}, {"text": " Table 4: Ablation Studies. Exact match (EM) and  span F1 results on NewsQA test set of a BIDAF  model finetuned with a 2-stage SynNet. In study  A, we vary k, the number of mini-batches from  SQuAD for every batch in NewsQA. In study B,  we set k = 0, and vary the answer type and how  much of the paragraph we use for question synthe- sis. 2 \u2212 sent refers to using two sentences before  answer span, while all refers to using the entire  paragraph. A ner refers to using an NER system  and A or refers to using the human-annotated an- swers to generate questions.", "labels": [], "entities": [{"text": "Exact match (EM)", "start_pos": 28, "end_pos": 44, "type": "METRIC", "confidence": 0.9477818369865417}, {"text": "span F1", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9259742498397827}, {"text": "NewsQA test", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9602744877338409}, {"text": "NewsQA", "start_pos": 218, "end_pos": 224, "type": "DATASET", "confidence": 0.9638919234275818}]}]}