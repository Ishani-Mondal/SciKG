{"title": [{"text": "Sparse Communication for Distributed Gradient Descent", "labels": [], "entities": [{"text": "Distributed Gradient Descent", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6592745582262675}]}], "abstractContent": [{"text": "We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates.", "labels": [], "entities": [{"text": "distributed stochastic gradient descent", "start_pos": 8, "end_pos": 47, "type": "TASK", "confidence": 0.6123419404029846}]}, {"text": "Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices.", "labels": [], "entities": []}, {"text": "This method can be combined with quan-tization to further improve the compression.", "labels": [], "entities": []}, {"text": "We explore different configurations and apply them to neural machine translation and MNIST image classification tasks.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6527531544367472}, {"text": "MNIST image classification", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8697639306386312}]}, {"text": "Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task.", "labels": [], "entities": [{"text": "MNIST", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.6793451309204102}, {"text": "convergence", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9412969946861267}]}, {"text": "Our experiments show that we can achieve up to 49% speedup on MNIST and 22% on NMT without damaging the final accuracy or BLEU.", "labels": [], "entities": [{"text": "speedup", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9869667887687683}, {"text": "MNIST", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.778407871723175}, {"text": "NMT", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.8657082319259644}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9821385145187378}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9984694123268127}]}], "introductionContent": [{"text": "Distributed computing is essential to train large neural networks on large data sets.", "labels": [], "entities": [{"text": "Distributed computing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8791149258613586}]}, {"text": "We focus on data parallelism: nodes jointly optimize the same model on different parts of the training data, exchanging gradients and parameters over the network.", "labels": [], "entities": []}, {"text": "This network communication is costly, so prior work developed two ways to approximately compress network traffic: 1-bit quantization () and sending sparse matrices by dropping small updates.", "labels": [], "entities": []}, {"text": "These methods were developed and tested on speech recognition and toy MNIST systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8117336630821228}]}, {"text": "In porting these approximations to neural machine translation (NMT)), we find that translation is less tolerant to quantization.", "labels": [], "entities": [{"text": "neural machine translation (NMT))", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8165434102217356}]}, {"text": "Throughout this paper, we compare neural machine translation behavior with a toy MNIST system, chosen because prior work used a similar system (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.614405224720637}]}, {"text": "NMT parameters are dominated by three large embedding matrices: source language input, target language input, and target language output.", "labels": [], "entities": []}, {"text": "These matrices deal with vocabulary words, only a small fraction of which are seen in a mini-batch, so we expect skewed gradients.", "labels": [], "entities": []}, {"text": "In contrast, MNIST systems exercise every parameter in every mini-batch.", "labels": [], "entities": []}, {"text": "Additionally, NMT systems consist of multiple parameters with different scales and sizes, compared to MNIST's 3-layers network with uniform size.", "labels": [], "entities": []}, {"text": "More formally, gradient updates have positive skewness coefficient): most are close to zero but a few are large.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with an image classification task based on MNIST dataset ( and Romanian\u2192English neural machine translation system.", "labels": [], "entities": [{"text": "image classification task", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8101944526036581}, {"text": "MNIST dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9338313341140747}, {"text": "Romanian\u2192English neural machine translation", "start_pos": 77, "end_pos": 120, "type": "TASK", "confidence": 0.5291465918223063}]}, {"text": "For our image classification experiment, we build a fully connected neural network with three 4069-neuron hidden layers.", "labels": [], "entities": [{"text": "image classification", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8090237379074097}]}, {"text": "We use AdaGrad with an initial learning rate of 0.005.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.8865957260131836}]}, {"text": "The mini-batch size of 40 is used.", "labels": [], "entities": []}, {"text": "This setup is identical to.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training speed with various drop ratios.", "labels": [], "entities": [{"text": "drop ratios", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.9515111148357391}]}, {"text": " Table  2. Final BLEU scores are essentially unchanged.", "labels": [], "entities": [{"text": "Final", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.45903411507606506}, {"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9641637206077576}]}, {"text": " Table 2: Summary of BLEU score obtained.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9510866403579712}, {"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9689134657382965}]}]}