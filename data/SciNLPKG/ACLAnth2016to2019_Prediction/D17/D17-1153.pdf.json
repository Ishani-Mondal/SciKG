{"title": [{"text": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", "labels": [], "entities": [{"text": "Reinforcement Learning", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8847478330135345}, {"text": "Bandit Neural Machine Translation", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.5856792479753494}]}], "abstractContent": [{"text": "Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.809366375207901}]}, {"text": "Yet, current neural machine translation training fo-cuses on expensive human-generated reference translations.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6835639476776123}]}, {"text": "We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7815212408701578}]}, {"text": "Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015).", "labels": [], "entities": []}, {"text": "This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.", "labels": [], "entities": [{"text": "corpus-level machine translation", "start_pos": 134, "end_pos": 166, "type": "TASK", "confidence": 0.6878436605135599}]}], "introductionContent": [{"text": "Bandit structured prediction is the task of learning to solve complex joint prediction problems (like parsing or machine translation) under a very limited feedback model: a system must produce a single structured output (e.g., translation) and then the world reveals a score that measures how good or bad that output is, but provides neither a \"correct\" output nor feedback on any other possible output (.", "labels": [], "entities": [{"text": "Bandit structured prediction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7189623514811198}, {"text": "parsing or machine translation)", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.7786147117614746}]}, {"text": "Because of the extreme sparsity of this feedback, a common experimental setup is that one pre-trains a good-but-not-great \"reference\" system based on whatever labeled data is available, and then seeks to improve it overtime using this bandit feedback.", "labels": [], "entities": []}, {"text": "A common motivation for this problem setting is cost.", "labels": [], "entities": []}, {"text": "In the case of translation, bilingual \"experts\" can read a source sentence and a possible translation, and can much more quickly provide a rating of that translation than they can produce a full translation on their own.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9685335755348206}]}, {"text": "Furthermore, one can often collect even less expensive ratings from \"non-experts\" who mayor may not be bilingual (.", "labels": [], "entities": []}, {"text": "Breaking this reliance on expensive data could unlock previously ignored languages and speed development of broad-coverage machine translation systems.", "labels": [], "entities": [{"text": "broad-coverage machine translation", "start_pos": 108, "end_pos": 142, "type": "TASK", "confidence": 0.6212434669335684}]}, {"text": "All work on bandit structured prediction we know makes an important simplifying assumption: the score provided by the world is exactly the score the system must optimize ( \u00a72).", "labels": [], "entities": [{"text": "bandit structured prediction", "start_pos": 12, "end_pos": 40, "type": "TASK", "confidence": 0.6493269701798757}]}, {"text": "In the case of parsing, the score is attachment score; in the case of machine translation, the score is (sentence-level) BLEU.", "labels": [], "entities": [{"text": "attachment score", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.9281012117862701}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7480826079845428}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9714747667312622}]}, {"text": "While this simplifying assumption has been incredibly useful in building algorithms, it is highly unrealistic.", "labels": [], "entities": []}, {"text": "Any time we want to optimize a system by collecting user feedback, we must take into account: 1.", "labels": [], "entities": []}, {"text": "The metric we care about (e.g., expert ratings) may not correlate perfectly with the measure that the reference system was trained on (e.g., BLEU or log likelihood); 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9979901313781738}]}, {"text": "Human judgments might be more granular than traditional continuous metrics (e.g., thumbs up vs. thumbs down); 3.", "labels": [], "entities": []}, {"text": "Human feedback have high variance (e.g., different raters might give different responses given the same system output); 4.", "labels": [], "entities": []}, {"text": "Human feedback might be substantially skewed (e.g., a rater may think all system outputs are poor).", "labels": [], "entities": []}, {"text": "Our first contribution is a strategy to simulate expert and non-expert ratings to evaluate the robustness of bandit structured prediction algorithms in general, in a more realistic environment ( \u00a74).", "labels": [], "entities": [{"text": "bandit structured prediction algorithms", "start_pos": 109, "end_pos": 148, "type": "TASK", "confidence": 0.7004205882549286}]}, {"text": "We construct a family of perturbations to capture three attributes: granularity, variance, and skew.", "labels": [], "entities": [{"text": "variance", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.931186318397522}]}, {"text": "We apply these perturbations on automatically generated scores to simulate noisy human ratings.", "labels": [], "entities": []}, {"text": "To make our simulated ratings as realistic as possible, we study recent human evaluation data () and fit models to match the noise profiles in actual human ratings ( \u00a74.2).", "labels": [], "entities": []}, {"text": "Our second contribution is a reinforcement learning solution to bandit structured prediction and a study of its robustness to these simulated human ratings ( \u00a7 3).", "labels": [], "entities": [{"text": "bandit structured prediction", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.6685775518417358}]}, {"text": "We combine an encoderdecoder architecture of machine translation () with the advantage actor-critic algorithm, yielding an approach that is simple to implement but works on lowresource bandit machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7042492926120758}, {"text": "lowresource bandit machine translation", "start_pos": 173, "end_pos": 211, "type": "TASK", "confidence": 0.7015731632709503}]}, {"text": "Even with substantially restricted granularity, with high variance feedback, or with skewed rewards, this combination improves pre-trained models ( \u00a76).", "labels": [], "entities": []}, {"text": "In particular, under realistic settings of our noise parameters, the algorithm's online reward and final heldout accuracies do not significantly degrade from a noise-free setting.", "labels": [], "entities": [{"text": "final heldout accuracies", "start_pos": 99, "end_pos": 123, "type": "METRIC", "confidence": 0.6351869304974874}]}], "datasetContent": [{"text": "We choose two language pairs from different language families with different typological properties: German-to-English and (De-En) and Chinese-to-English (Zh-En).", "labels": [], "entities": []}, {"text": "We use parallel transcriptions of TED talks for these pairs of languages from the machine translation track of the).", "labels": [], "entities": []}, {"text": "For each language pair, we split its data into four sets for supervised training, bandit training, development and testing.", "labels": [], "entities": []}, {"text": "For English and German, we tokenize and clean sentences using Moses (.", "labels": [], "entities": []}, {"text": "For Chinese, we use the Stanford Chinese word segmenter ( to segment sentences and tokenize.", "labels": [], "entities": [{"text": "Stanford Chinese word segmenter", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6176983714103699}]}, {"text": "We remove all sentences with length greater than 50, resulting in an average sentence length of 18.", "labels": [], "entities": []}, {"text": "We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for bandit training and previous years' development and evaluation data for testing.", "labels": [], "entities": [{"text": "IWSLT 2015 data", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9295474886894226}, {"text": "IWSLT 2014 data", "start_pos": 64, "end_pos": 79, "type": "DATASET", "confidence": 0.9181601802508036}]}, {"text": "For each task, we first use the supervised training set to pre-train a reference NMT model using supervised learning.", "labels": [], "entities": []}, {"text": "On the same training set, we also pre-train the critic model with translations sampled from the pre-trained NMT model.", "labels": [], "entities": []}, {"text": "Next, we enter a bandit learning mode where our models only observe the source sentences of the bandit training set.", "labels": [], "entities": []}, {"text": "Unless specified differently, we train the NMT models with NED-A2C for one pass over the bandit training set.", "labels": [], "entities": [{"text": "NED-A2C", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.7845273613929749}]}, {"text": "If a perturbation function is applied to Per-Sentence BLEU scores, it is only applied in this stage, not in the pre-training stage.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8966606259346008}]}, {"text": "We measure the improvement \u2206S of an evaluation metric S due to bandit training: \u2206S = S A2C \u2212 S ref , where S ref is the metric computed on the reference models and S A2C is the metric computed on models trained with NED-A2C.", "labels": [], "entities": [{"text": "A2C \u2212 S ref", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.8704680949449539}, {"text": "NED-A2C", "start_pos": 216, "end_pos": 223, "type": "DATASET", "confidence": 0.8890178799629211}]}, {"text": "Our primary interest is Per-Sentence BLEU: average sentence-level BLEU of translations that are sampled and scored during the bandit learning pass.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.7307428121566772}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.8823725581169128}]}, {"text": "This metric represents average expert ratings, which we want to optimize for in real-world scenarios.", "labels": [], "entities": []}, {"text": "We also measure Heldout BLEU: corpuslevel BLEU on an unseen test set, where translations are greedily decoded by the NMT models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.8903071880340576}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8085312843322754}]}, {"text": "This shows how much our method improves translation quality, since corpus-level BLEU correlates better with human judgments than sentence-level BLEU.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9677788019180298}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.8424059748649597}]}, {"text": "Because of randomness due to both the random sampling in the model for \"exploration\" as well as the randomness in the reward function, we repeat each experiment five times and report the mean results with 95% confidence intervals.", "labels": [], "entities": []}], "tableCaptions": []}