{"title": [{"text": "Instance Weighting for Neural Machine Translation Domain Adaptation", "labels": [], "entities": [{"text": "Instance Weighting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8894647359848022}, {"text": "Neural Machine Translation Domain Adaptation", "start_pos": 23, "end_pos": 67, "type": "TASK", "confidence": 0.8162897109985352}]}], "abstractContent": [{"text": "Instance weighting has been widely applied to phrase-based machine translation domain adaptation.", "labels": [], "entities": [{"text": "Instance weighting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8600297570228577}, {"text": "phrase-based machine translation domain adaptation", "start_pos": 46, "end_pos": 96, "type": "TASK", "confidence": 0.8268670797348022}]}, {"text": "However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.7865047057469686}]}, {"text": "In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation.", "labels": [], "entities": [{"text": "sentence weighting", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7925970256328583}, {"text": "domain weighting", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.729501873254776}, {"text": "NMT domain adaptation", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.9105689724286398}]}, {"text": "Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points.", "labels": [], "entities": [{"text": "IWSLT English-German/French tasks", "start_pos": 25, "end_pos": 58, "type": "DATASET", "confidence": 0.8306909084320069}, {"text": "NMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.8862367272377014}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.9974263310432434}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9958482980728149}]}], "introductionContent": [{"text": "In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.877490778764089}]}, {"text": "Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection.", "labels": [], "entities": [{"text": "Phrase-based SMT (PBSMT) domain adaptation", "start_pos": 8, "end_pos": 50, "type": "TASK", "confidence": 0.8093167287962777}, {"text": "Data selection", "start_pos": 178, "end_pos": 192, "type": "TASK", "confidence": 0.8388005793094635}]}, {"text": "The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively.", "labels": [], "entities": []}, {"text": "Then select training data by using these ranked scores.", "labels": [], "entities": []}, {"text": "ii) Model Linear Interpolation.", "labels": [], "entities": [{"text": "Model Linear Interpolation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.6694105863571167}]}, {"text": "Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained.", "labels": [], "entities": [{"text": "translation", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9565140604972839}]}, {"text": "These models are then combined to achieve the best performance.", "labels": [], "entities": []}, {"text": "Instance Weighting has been applied to several NLP domain adaptation tasks, such as POS tagging, entity type classification and especially PBSMT (.", "labels": [], "entities": [{"text": "Instance Weighting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8277346789836884}, {"text": "NLP domain adaptation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.7290717959403992}, {"text": "POS tagging", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.8265159428119659}, {"text": "entity type classification", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.7156590223312378}]}, {"text": "They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight.", "labels": [], "entities": []}, {"text": "For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT) domain adaptation", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.831033006310463}]}, {"text": "Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model.", "labels": [], "entities": []}, {"text": "There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (. ii) an NMT further training (fine-tuning) method (.", "labels": [], "entities": []}, {"text": "The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data.", "labels": [], "entities": []}, {"text": "Recently, make an empirical comparison of NMT further training ( and domain control (), which applied word-level domain features to word embedding layer.", "labels": [], "entities": []}, {"text": "This approach provides natural baselines for comparison.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no existing work concerning instance weighting in NMT.", "labels": [], "entities": [{"text": "instance weighting", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7784490585327148}]}, {"text": "The main challenge is that NMT is not a liner model or combination of linear models, where the instance weight can be integrated into directly.", "labels": [], "entities": []}, {"text": "To overcome this difficulty, we try to integrate the instance weight into NMT objective function.", "labels": [], "entities": []}, {"text": "Two technologies, i.e., sentence weighting and domain weighting, are proposed to apply instance weighting to NMT.", "labels": [], "entities": [{"text": "sentence weighting", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7895216345787048}, {"text": "domain weighting", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7079906314611435}]}, {"text": "In addition, we also propose a dynamic weight learning strategy to tune the proposed domain weights.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: IWSLT EN-DE results. The marks (the  same in", "labels": [], "entities": [{"text": "IWSLT EN-DE", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7769459187984467}]}, {"text": " Table 3: IWSLT EN-FR results.", "labels": [], "entities": [{"text": "IWSLT EN-FR", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.7109351754188538}]}, {"text": " Table 4: Further training (", "labels": [], "entities": []}]}