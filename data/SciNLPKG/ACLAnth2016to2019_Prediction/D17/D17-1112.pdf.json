{"title": [{"text": "Speech segmentation with a neural encoder model of working memory", "labels": [], "entities": [{"text": "Speech segmentation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7121727019548416}]}], "abstractContent": [{"text": "We present the first unsupervised LSTM speech segmenter as a cognitive model of the acquisition of words from unseg-mented input.", "labels": [], "entities": [{"text": "LSTM speech segmenter", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.7996952533721924}]}, {"text": "Cognitive biases toward phonological and syntactic predictability in speech are rooted in the limitations of human memory (Baddeley et al., 1998); compressed representations are easier to acquire and retain in memory.", "labels": [], "entities": []}, {"text": "To model the biases introduced by these memory limitations, our system uses an LSTM-based encoder-decoder with a small number of hidden units, then searches fora segmentation that minimizes autoencod-ing loss.", "labels": [], "entities": []}, {"text": "Linguistically meaningful segments (e.g. words) should share regular patterns of features that facilitate de-coder performance in comparison to random segmentations, and we show that our learner discovers these patterns when trained on either phoneme sequences or raw acoustics.", "labels": [], "entities": []}, {"text": "To our knowledge, ours is the first fully unsupervised system to be able to segment both symbolic and acoustic representations of speech.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes anew cognitive model of the acquisition of word-like units from unsegmented input.", "labels": [], "entities": []}, {"text": "The model is intended to describe the process by which pre-linguistic infants learn their earliest words, a stage they pass through during the first year of life.", "labels": [], "entities": []}, {"text": "Our model is based on the standard memory model of in which the listener encodes lexical items into phonological working memory, but represents the entire sentence as a higher-level syntactic structure without phonological detail.", "labels": [], "entities": []}, {"text": "Our model implements this architecture using encoderdecoder LSTMs with limited memory capacity, then searches for word segmentations which make it easy to remember the sentence.", "labels": [], "entities": []}, {"text": "Word learning has been extensively studied in previous research, both with transcribed symbolic input and acoustics.", "labels": [], "entities": [{"text": "Word learning", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7446136772632599}]}, {"text": "Why attempt yet another approach?", "labels": [], "entities": []}, {"text": "Our model has three main advantages.", "labels": [], "entities": []}, {"text": "First, as a cognitive model, it relates the kinds of learning biases used in previous work to the wider literature on working memory.", "labels": [], "entities": []}, {"text": "Second, its sequence-to-sequence neural architecture allows it to handle either one-hot symbolic input or dense vectors of acoustic features.", "labels": [], "entities": []}, {"text": "In contrast, existing models are typically designed for \"clean\" symbolic input, then retrofitted with additional mechanisms to cope with acoustics.", "labels": [], "entities": []}, {"text": "Finally, neural networks have been impressively successful in supervised language processing domains, yet are still underused in unsupervised learning.", "labels": [], "entities": []}, {"text": "Even systems which douse neural nets to model lexical acquisition generally require an auxiliary model for clustering the embeddings, which can make their learning objectives difficult to understand.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.753955215215683}]}, {"text": "Our system uses the well-understood autoencoder objective to perform the segmentation task without requiring auxiliary clustering, and thus suggests anew direction for neural unsupervised learning.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.8861656785011292}]}, {"text": "In an experiment conducted on the widely used Brent corpus, our system achieves performance close to that of, although subsequent systems outperform ours by a wider margin.", "labels": [], "entities": [{"text": "Brent corpus", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9735823571681976}]}, {"text": "We show that memory limitations do indeed drive the performance of the system, with smaller LSTM hidden states outperforming larger ones in the development set.", "labels": [], "entities": []}, {"text": "Ina follow-up experiment designed to ex-plore the flexibility of our model, we deploy the segmenter on acoustic input: the English portion of the Zerospeech 2015 challenge (.", "labels": [], "entities": []}, {"text": "Our model outperforms the winning model from that challenge), although we underperform more recent unsupervised acoustic segmentation systems.", "labels": [], "entities": []}, {"text": "To our knowledge, our system is the first unsupervised LSTM speech segmenter, as well as the first unsupervised speech segmenter to succeed on both symbolic and acoustic representations of speech.", "labels": [], "entities": [{"text": "LSTM speech segmenter", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7557574907938639}]}, {"text": "Our results are of note for several reasons.", "labels": [], "entities": []}, {"text": "First, they provide modeling support for the claim that memory limitations encourage lexical acquisition.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7325430810451508}]}, {"text": "Second, they show that a general strategy of searching for maximally compressible representations can realistically guide lexical acquisition without explicit reference to perceptual biases (c.f. e.g., regardless of input representation.", "labels": [], "entities": [{"text": "lexical acquisition", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.6914817988872528}]}, {"text": "And third, they demonstrate the benefits of our adaptation of neural sequence modeling to unsupervised learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Selected segmentation results on Brent.", "labels": [], "entities": [{"text": "Selected segmentation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5030552893877029}, {"text": "Brent", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.9262122511863708}]}, {"text": " Table 2: Selected word segmentation results on the  the Zerospeech 2015 English corpus.", "labels": [], "entities": [{"text": "Selected word segmentation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.5962254901727041}, {"text": "Zerospeech 2015 English corpus", "start_pos": 57, "end_pos": 87, "type": "DATASET", "confidence": 0.8842526972293854}]}]}