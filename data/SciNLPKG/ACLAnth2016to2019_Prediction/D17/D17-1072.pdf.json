{"title": [], "abstractContent": [{"text": "Traditionally, word segmentation (WS) adopts the single-granularity formalism , where a sentence corresponds to a single word sequence.", "labels": [], "entities": [{"text": "word segmentation (WS)", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.8722416758537292}]}, {"text": "However, Sproat et al.", "labels": [], "entities": []}, {"text": "(1996) show that the inter-native-speaker consistency ratio over Chinese word boundaries is only 76%, indicating single-grained WS (SWS) imposes unnecessary challenges on both manual annotation and statistical modeling.", "labels": [], "entities": []}, {"text": "Moreover, WS results of different gran-ularities can be complementary and beneficial for high-level applications.", "labels": [], "entities": [{"text": "WS", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9793241024017334}]}, {"text": "This work proposes and addresses multi-grained WS (MWS).", "labels": [], "entities": [{"text": "multi-grained WS (MWS)", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.7214677572250366}]}, {"text": "First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets.", "labels": [], "entities": [{"text": "model training and tuning", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7031603306531906}, {"text": "SWS datasets", "start_pos": 131, "end_pos": 143, "type": "DATASET", "confidence": 0.7420856952667236}]}, {"text": "Then we manually annotate 1,500 test sentences with true MWS annotations.", "labels": [], "entities": []}, {"text": "Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6592783778905869}]}, {"text": "Experiments and analysis lead to many interesting findings.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the first processing step of Chinese language processing, word segmentation (WS) has been extensively studied and made great progress during the past decades, thanks to the annotation of large-scale benchmark datasets, among which the most widelyused are Microsoft Research Corpus (MSR) (), Peking University * Correspondence author MSR PPD CTB: An example of annotation heterogeneity: (all) (country) (every) (place) (medical science) (field) (experts) (walk) (out) (people) (great hall).", "labels": [], "entities": [{"text": "Chinese language processing", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6216723223527273}, {"text": "word segmentation (WS)", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.8769070148468018}, {"text": "Microsoft Research Corpus (MSR)", "start_pos": 258, "end_pos": 289, "type": "DATASET", "confidence": 0.8750403126080831}, {"text": "MSR PPD CTB", "start_pos": 336, "end_pos": 347, "type": "DATASET", "confidence": 0.6009437541166941}]}, {"text": "People Daily Corpus (PPD) (, and Penn Chinese Treebank (CTB) ().", "labels": [], "entities": [{"text": "People Daily Corpus (PPD)", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.9747910698254904}, {"text": "Penn Chinese Treebank (CTB)", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.9776203334331512}]}, {"text": "gives an example sentence segmented in different guidelines.", "labels": [], "entities": []}, {"text": "Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries (, to path searching from segmentation graphs based on language modeling scores and other statistics (), to character-based sequence labeling, to shift-reduce incremental parsing (.", "labels": [], "entities": [{"text": "WS", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9857573509216309}, {"text": "character-based sequence labeling", "start_pos": 202, "end_pos": 235, "type": "TASK", "confidence": 0.5939133763313293}]}, {"text": "Recently, neural network models have also achieved success by effectively learning representation of characters and contexts ().", "labels": [], "entities": []}, {"text": "To date, all the labeled datasets adopt the single-granularity formalization, and previous research mainly focuses on single-grained WS (SWS), where one sentence is segmented into a single word sequence.", "labels": [], "entities": [{"text": "WS (SWS)", "start_pos": 133, "end_pos": 141, "type": "TASK", "confidence": 0.6265835613012314}]}, {"text": "Although different WS guidelines share the same high-level criterion of word boundaries -a character string combined closely and used steadily forms a word, people greatly diverge due to individual differences on knowledge and living environments, etc.", "labels": [], "entities": [{"text": "WS", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.946897566318512}]}, {"text": "An anonymous reviewer kindly points out that Vlad\u00edmir Skali\u010dka of the Prague School claimed that unlike the \"isolating\" languages such as French and English, Chinese belongs to the \"polysynthetic\" type, in which compound words are normally produced from indigenous morphemes.", "labels": [], "entities": [{"text": "Prague School", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.9009407162666321}]}, {"text": "The vague distinction between morphemes and compounds also contribute to the cognition divergence on the concept of words.", "labels": [], "entities": []}, {"text": "show that the consensus ratio over word boundaries is only 76% among Chinese native speakers without trained on a common guideline.To fill this gap, WS guidelines need to further group words into many types and provide illustration examples for each type.", "labels": [], "entities": [{"text": "WS", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.939902663230896}]}, {"text": "Nevertheless, it is very challenging even for well-trained annotators to fully grasp the guidelines and to be consistent on uncovered cases.", "labels": [], "entities": []}, {"text": "For example, shows that about 3% characters are inconsistently segmented in the PPD training data used in.", "labels": [], "entities": [{"text": "PPD training data", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.677577277024587}]}, {"text": "We have also observed many inconsistency cases in all MSR/PPD/CTB during this work.", "labels": [], "entities": [{"text": "MSR/PPD/CTB", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.5566044807434082}]}, {"text": "Ina word, SWS imposes great challenge on data annotation, and as aside effect, enforces statistical models to learn subtleness of annotation guidelines rather than the true WS ambiguities.", "labels": [], "entities": [{"text": "SWS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8906778693199158}]}, {"text": "From another perspective, WS results of different granularities maybe complementary in supporting applications such as information retrieval (IR) ( and machine translation (MT) ().", "labels": [], "entities": [{"text": "WS", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9823687672615051}, {"text": "information retrieval (IR)", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.8311316668987274}, {"text": "machine translation (MT)", "start_pos": 152, "end_pos": 176, "type": "TASK", "confidence": 0.839658260345459}]}, {"text": "On the one hand, coarse-grained words enable statistical models to perform more exact matching and analyzing.", "labels": [], "entities": []}, {"text": "On the other hand, fine-grained words are helpful in both reducing data sparseness and supporting deeper understanding of language.", "labels": [], "entities": []}, {"text": "To solve the above two issues for SWS, this paper proposes and addresses multi-grained WS (MWS).", "labels": [], "entities": [{"text": "SWS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9666222333908081}]}, {"text": "Given an input sentence, the goal is to produce a hierarchy structure of all words of different granularities, as illustrated in.", "labels": [], "entities": []}, {"text": "To tackle the lack of labeled data, we build a large-scale pseudo MWS dataset for model training and tuning by automatically converting annotations of three heterogeneous 1 Words in CTB are generally more fine-grained than those in PPD and MSR, probably due to the requirement of annotating syntactic structures.", "labels": [], "entities": []}, {"text": "SWS datasets (i.e. MSR/PPD/CTB) based on the recently proposed coupled sequence labeling approach of.", "labels": [], "entities": [{"text": "SWS datasets", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.7525142729282379}]}, {"text": "In order to fully investigate the problem, we manually annotate 1,500 test sentences with true MWS annotations.", "labels": [], "entities": []}, {"text": "Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling problems.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6468189507722855}, {"text": "sequence labeling", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.5948103070259094}]}, {"text": "Experiments and data analysis lead to many interesting findings.", "labels": [], "entities": []}, {"text": "We will release the newly annotated data and the codes of the benchmark approaches at http://hlt.suda.edu.cn/~zhli.", "labels": [], "entities": []}, {"text": "However, due to the license issue, we may not directly release all the pseudo MWS datasets.", "labels": [], "entities": [{"text": "MWS datasets", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.8339288234710693}]}, {"text": "Instead, we will launch a web service for obtaining MWS annotations given a sentence with one of MSR/PPD/CTB annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data: for MSR, we adopt the training/test datasets of the), and cutoff 10% random training sentences as the dev data following; for PPD and CTB, we follow and directly adopt their datasets and data split.", "labels": [], "entities": [{"text": "MSR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8772608041763306}]}, {"text": "7 Evaluation Metrics: the goal of MWS is to precisely produce all words of different granularities given the input sentence.", "labels": [], "entities": []}, {"text": "Therefore, to reach a balance of both precision (P = #Word gold\u2229sys #Word sys ) and recall (R = #Word gold\u2229sys #Word gold ), we use the F1 score (= 2P RP +R ) as in SWS.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995701909065247}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9996365308761597}, {"text": "F1 score", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9873366057872772}]}, {"text": "Hyper-parameter: we implement all our approaches based on the codes released by, by making extensions such as adding bichar embeddings and.", "labels": [], "entities": []}, {"text": "The dimensions of char and bichar embeddings are both 50 and other hyper-parameters are the same with.", "labels": [], "entities": []}, {"text": "In our preliminary experiments, we observe that under their neural network framework, the MWS performance is quite stable when reruning under random initialization or reasonably altering other hyper-parameters.", "labels": [], "entities": [{"text": "MWS", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.961696445941925}]}, {"text": "Due to time limitation, we leave the use of pre-trained embeddings and more hyper-parameter tuning for future exploration.", "labels": [], "entities": []}, {"text": "Training/test settings: when training the parsing and sequence labeling based MWS models (not SWS aggregation) on MSR/PPD/CTB-train, we adopt the simple corpus weighting strategy used in to balance the contributions of each training dataset.", "labels": [], "entities": []}, {"text": "Before each iteration, we randomly sample 10,000 sentences from each training dataset, and merge and shuffle them for one-iteration training.", "labels": [], "entities": []}, {"text": "We use merged MSR/PPD/CTB-dev as the MWS dev data for model selection.", "labels": [], "entities": [{"text": "MWS dev data", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.7680885195732117}]}, {"text": "For the SWS aggregation model, three SWS models are separately trained on the three training/dev datasets.", "labels": [], "entities": [{"text": "SWS", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9571313261985779}]}, {"text": "For evaluation, three SWS outputs produced independently are merged as one MWS result given a sentence.", "labels": [], "entities": [{"text": "SWS", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9368994832038879}]}, {"text": "In all experiments, training stops when Fscore on the dev data does not improve in 20 consecutive iterations, and we choose the model that performs best on the dev data for final evaluation.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9989805817604065}]}, {"text": "Main results: reports the performance of different approaches on both the pseudo MWS dev data and the manually annotated MWS test data.", "labels": [], "entities": [{"text": "MWS dev data", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.6368267138799032}, {"text": "MWS test data", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.7001858452955881}]}, {"text": "The \"#Word\" column reports the total number of words returned by the corresponding model; the following three columns show the percentages of words of different granularities; the last \"Overlapping\" column gives the percent of words that overlap with other words, which only happens in the \"SWS aggregation\" approach, since no constraint can be applied to the three separate SWS models during testing.", "labels": [], "entities": []}, {"text": "From the results, we can draw the following findings.", "labels": [], "entities": []}, {"text": "First, the results suggest that using pseudo training and dev datasets to build a MWS model is feasible, based on two evidences: 1) our simple benchmark model can reach a high F-score of 96.07% on the manually annotated test data, which is 1.77% higher than directly aggregating outputs of three SWS models; 2) the P/R/F scores on the pseudo dev data and on the manually labeled test data are quite consistent in general, indicating that it is reliable to use the pseudo dev data for model selection and tuning.", "labels": [], "entities": [{"text": "MWS", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.958758533000946}, {"text": "F-score", "start_pos": 176, "end_pos": 183, "type": "METRIC", "confidence": 0.9989860653877258}, {"text": "F", "start_pos": 319, "end_pos": 320, "type": "METRIC", "confidence": 0.508421003818512}]}, {"text": "Second, the parsing approach and the sequence labeling approach (with or without bichar embeddings) achieve very similar performance (within 0.15% vibration), More importantly, the parsing approach produces more words and more multi-grained words than the sequence labeling approach, indicating that it is potentially more proper to model MWS as a parsing problem in order to better capture and represent multi-granularity structures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9692285060882568}, {"text": "sequence labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6872542500495911}, {"text": "MWS as a parsing", "start_pos": 339, "end_pos": 355, "type": "TASK", "confidence": 0.6800850033760071}]}, {"text": "Another possible disadvantage of the sequence labeling approach is that the trained model cannot produce more granularity levels (e.g., four-grained) beyond those in the training data.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.6746423542499542}]}, {"text": "Nevertheless, compared against the manual annotations in, both the parsing and sequence labeling approaches retrieve much less multi-grained words, which is caused by the under-representation issue of the pseudo training data, as discussed in Section 3.", "labels": [], "entities": []}, {"text": "Third, the SWS aggregation approach achieves the best recall at the price of very low precision on both dev/test data.", "labels": [], "entities": [{"text": "SWS", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9080712795257568}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9992579817771912}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9979871511459351}]}, {"text": "We believe the reason is that training three SWS models separately on one of the three training datasets has two disadvantages: 1) connections among different guidelines are totally ignored, leading to many overlapping words (1.0%); 2) smaller training data also degrades the performance of each SWS model.", "labels": [], "entities": []}, {"text": "Finally, using bichar embeddings turns out very helpful for MWS, and leads to 0.97 \u223c 1.18% F-score improvement on dev data and 0.62 \u223c 0.85% on test data, which is consistent with the SWS results in.", "labels": [], "entities": [{"text": "MWS", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9754539132118225}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9989443421363831}, {"text": "SWS", "start_pos": 183, "end_pos": 186, "type": "DATASET", "confidence": 0.4957459270954132}]}], "tableCaptions": [{"text": " Table 3: Data statistics of the MWS test data  before and after manual annotation.", "labels": [], "entities": [{"text": "MWS test data", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8202641606330872}]}, {"text": " Table 4: MWS as sequence labeling. SWS  labels for the same character are organized  fine-to-coarse.", "labels": [], "entities": []}, {"text": " Table 5: Data statistics (in sentence number).  The last column reports the averaged charac- ter number of each word.", "labels": [], "entities": []}, {"text": " Table 6: Performance of different MWS approaches.", "labels": [], "entities": [{"text": "MWS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9863513112068176}]}]}