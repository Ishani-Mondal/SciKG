{"title": [{"text": "Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set", "labels": [], "entities": []}], "abstractContent": [{"text": "We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiper-wasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7472679913043976}]}, {"text": "We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al.", "labels": [], "entities": []}, {"text": "(2011) to produce the first implementation of worst-case Opn 3 q exact decoders for arc-hybrid and arc-eager transition systems.", "labels": [], "entities": []}, {"text": "With our minimal features, we also present Opn 3 q global training methods.", "labels": [], "entities": []}, {"text": "Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the \"second-best-in-class\" result on the English Penn Treebank.", "labels": [], "entities": [{"text": "attachment score reported", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.9160478909810384}, {"text": "Chinese Treebank", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.9903810620307922}, {"text": "English Penn Treebank", "start_pos": 196, "end_pos": 217, "type": "DATASET", "confidence": 0.8617867231369019}]}], "introductionContent": [{"text": "It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding.", "labels": [], "entities": []}, {"text": "But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly ().", "labels": [], "entities": [{"text": "transition-based dependency parsers (TBDPs", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.710625946521759}]}, {"text": "The key efficiency issue for decoding is as follows.", "labels": [], "entities": []}, {"text": "In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration.", "labels": [], "entities": []}, {"text": "But consulting many positions means that although polynomial-time exact-decoding algorithms do exist, having been introduced by and, unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time.", "labels": [], "entities": []}, {"text": "For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn 6 q (originally reported as Opn 7 q) fora length-n sentence.", "labels": [], "entities": []}, {"text": "As an extreme case, use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.", "labels": [], "entities": []}, {"text": "Recently, Kiperwasser and Goldberg (2016a) and applied bidirectional long short-term memory networks (, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well.", "labels": [], "entities": [{"text": "parsing", "start_pos": 152, "end_pos": 159, "type": "TASK", "confidence": 0.9749861359596252}]}, {"text": "Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).", "labels": [], "entities": []}, {"text": "Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to larger sets, and out-performs a single position.", "labels": [], "entities": []}, {"text": "(Details regarding the situation with arc-standard can be found in \u00a72.)", "labels": [], "entities": [{"text": "arc-standard", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.70085608959198}]}, {"text": "Our minimal feature set plugs into Huang and Sagae's ands dynamic program-ming framework to produce the first implementation of Opn 3 q exact decoders for arc-hybrid and arc-eager parsers.", "labels": [], "entities": []}, {"text": "We also enable and implement Opn 3 q global training methods.", "labels": [], "entities": []}, {"text": "Empirically, ensembles containing our minimal-feature, globallytrained and exactly-decoded models produce the best unlabeled attachment score (UAS) reported (to our knowledge) on the Chinese Treebank and the \"second-best-in-class\" result on the English Penn Treebank.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 115, "end_pos": 147, "type": "METRIC", "confidence": 0.8347705801328024}, {"text": "Chinese Treebank", "start_pos": 183, "end_pos": 199, "type": "DATASET", "confidence": 0.9870221614837646}, {"text": "English Penn Treebank", "start_pos": 245, "end_pos": 266, "type": "DATASET", "confidence": 0.8744959433873495}]}, {"text": "Additionally, we provide a slight update to the theoretical connections previously drawn by between TBDPs and the graph-based dependency parsing algorithms of and, including results regarding the arc-eager parsing system.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7033747434616089}]}], "datasetContent": [{"text": "Data and Evaluation We experimented with English and Chinese.", "labels": [], "entities": []}, {"text": "For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (.", "labels": [], "entities": [{"text": "Stanford Dependencies (de Marneffe and Manning, 2008) conversion", "start_pos": 25, "end_pos": 89, "type": "DATASET", "confidence": 0.9354825507510792}, {"text": "Penn Treebank", "start_pos": 129, "end_pos": 142, "type": "DATASET", "confidence": 0.9929037988185883}]}, {"text": "As is standard, we used \u00a72-21 of the Wall Street Journal for training, \u00a722 for development, and \u00a723 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger ().", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.9497549136479696}]}, {"text": "For Chinese, we used the Penn Chinese Treebank 5.1 (, CTB), with the same splits and head-finding rules for conversion to dependencies as.", "labels": [], "entities": [{"text": "Penn Chinese Treebank 5.1", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9792099595069885}]}, {"text": "We adopted the CTB's goldstandard tokenization and POS tags.", "labels": [], "entities": [{"text": "CTB's goldstandard tokenization", "start_pos": 15, "end_pos": 46, "type": "DATASET", "confidence": 0.7447304278612137}]}, {"text": "We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM).", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8057891577482224}, {"text": "sentencelevel unlabeled exact match (UEM)", "start_pos": 47, "end_pos": 88, "type": "METRIC", "confidence": 0.8013889448983329}]}, {"text": "Following prior work, all punctuation is excluded from evaluation.", "labels": [], "entities": []}, {"text": "For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation.", "labels": [], "entities": []}, {"text": "Implementation Details Our model structures reproduce those of.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8600701093673706}]}, {"text": "We use 2-layer bi-directional LSTMs with 256 hidden cell units.", "labels": [], "entities": []}, {"text": "Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors) (English) and pre-trained skipgram-model vectors () (Chinese).", "labels": [], "entities": []}, {"text": "The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh.", "labels": [], "entities": []}, {"text": "We set the dropout rate for the bi-LSTM () and MLP () for each model according to development-set performance.", "labels": [], "entities": [{"text": "MLP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9694958925247192}]}, {"text": "All parameters except the word embed- dings are initialized uniformly.", "labels": [], "entities": []}, {"text": "Approximately 1,000 tokens form a mini-batch for sub-gradient computation.", "labels": [], "entities": []}, {"text": "We train each model for 20 epochs and perform model selection based on development UAS.", "labels": [], "entities": [{"text": "UAS", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.8735164999961853}]}, {"text": "The proposed structured loss function is optimized via Adam (.", "labels": [], "entities": []}, {"text": "The neural network computation is based on the python interface to DyNet, and the exact decoding algorithms are implemented in Cython.", "labels": [], "entities": []}, {"text": "7 Main Results We implement exact decoders for the arc-hybrid and arc-eager systems, and present the test performance of different model configurations in, comparing global models with local models.", "labels": [], "entities": []}, {"text": "All models use the same decoder for testing as during the training process.", "labels": [], "entities": []}, {"text": "Though no global decoder for the arc-standard system has been explored in this paper, its local models are listed for comparison.", "labels": [], "entities": []}, {"text": "We also include an edgefactored graph-based model, which is conventionally trained globally.", "labels": [], "entities": []}, {"text": "The edge-factored model scores bi-LSTM features for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores.", "labels": [], "entities": []}, {"text": "For this model, we use Dozat and Man- See https://github.com/tzshi/dp-parser-emnlp17 . ning's (2017) biaffine scoring model, although in our case the model size is smaller.", "labels": [], "entities": []}, {"text": "8 Analogously to the dev-set results given in \u00a72, on the test data, the minimal feature sets perform as well as larger ones in locally-trained models.", "labels": [], "entities": []}, {"text": "And there exists a clear trend of global models outperforming local models for the two different transition systems on both datasets.", "labels": [], "entities": []}, {"text": "This illustrates the effectiveness of exact decoding and global training.", "labels": [], "entities": []}, {"text": "Of the three types of global models, the arceager arguably has the edge, an empirical finding resonating with our theoretical comparison of their model expressiveness.", "labels": [], "entities": []}, {"text": "Comparison with State-of-the-Art Models compares our algorithms' results with those of the state-of-the-art.", "labels": [], "entities": []}, {"text": "Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach-ing the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches.", "labels": [], "entities": [{"text": "PTB", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.5830768346786499}, {"text": "UAS", "start_pos": 224, "end_pos": 227, "type": "METRIC", "confidence": 0.9927640557289124}, {"text": "CTB dataset", "start_pos": 235, "end_pos": 246, "type": "DATASET", "confidence": 0.9768906831741333}, {"text": "PTB dataset", "start_pos": 287, "end_pos": 298, "type": "DATASET", "confidence": 0.9825606644153595}]}], "tableCaptions": [{"text": " Table 1: Top: English PTB dev-set UAS% for  progressively smaller sets of positional features,  for greedy parsers with different transition sys- tems. The \"double-arrow\" notation indicates vec- tors produced by a bi-directional LSTM. Internal  lines highlight large performance drop-offs when  a feature is deleted. Bottom: sizes of the minimal  feature sets in", "labels": [], "entities": [{"text": "UAS", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.8449102640151978}]}, {"text": " Table 2: Test set performance for different training regimes and feature sets. The models use the same  decoders for testing and training. For each setting, the average and standard deviation across 5 runs with  different random initializations are reported. Boldface: best (averaged) result per dataset/measure.", "labels": [], "entities": []}]}