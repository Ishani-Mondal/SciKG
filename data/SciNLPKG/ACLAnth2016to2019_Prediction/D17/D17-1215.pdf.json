{"title": [{"text": "Adversarial Examples for Evaluating Reading Comprehension Systems", "labels": [], "entities": [{"text": "Evaluating Reading Comprehension", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.8996132810910543}]}], "abstractContent": [{"text": "Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9984983205795288}]}, {"text": "To reward systems with real language understanding abilities , we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD).", "labels": [], "entities": [{"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 115, "end_pos": 158, "type": "DATASET", "confidence": 0.8668244991983686}]}, {"text": "Our method tests whether systems can answer questions about paragraphs that contain adver-sarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans.", "labels": [], "entities": []}, {"text": "In this ad-versarial setting, the accuracy of sixteen published models drops from an average of 75% F1 score to 36%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9992438554763794}, {"text": "F1 score", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.988456517457962}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9955867528915405}]}, {"text": "We hope our insights will motivate the development of new models that understand language more precisely.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quantifying the extent to which a computer system exhibits intelligent behavior is a longstanding problem in AI.", "labels": [], "entities": [{"text": "Quantifying", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9689366817474365}]}, {"text": "Today, the standard paradigm is to measure average error across a held-out test set.", "labels": [], "entities": [{"text": "average error", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.8572499454021454}]}, {"text": "However, models can succeed in this paradigm by recognizing patterns that happen to be predictive on most of the test examples, while ignoring deeper, more difficult phenomena (.", "labels": [], "entities": []}, {"text": "In this work, we propose adversarial evaluation for NLP, in which systems are instead evaluated on adversarially-chosen inputs.", "labels": [], "entities": []}, {"text": "We focus on the The BiDAF Ensemble model originally gets the answer correct, but is fooled by the addition of an adversarial distracting sentence (in blue).", "labels": [], "entities": []}, {"text": "SQuAD reading comprehension task, in which systems answer questions about paragraphs from Wikipedia.", "labels": [], "entities": [{"text": "SQuAD reading comprehension task", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7108496874570847}]}, {"text": "Reading comprehension is an appealing testbed for adversarial evaluation, as existing models appear successful by standard average-case evaluation metrics: the current state-of-the-art system achieves 84.7% F1 score, while human performance is just 91.2%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9894716441631317}]}, {"text": "1 Nonetheless, it seems unlikely that existing systems possess true language understanding and reasoning capabilities.", "labels": [], "entities": []}, {"text": "Carrying out adversarial evaluation on SQuAD requires new methods that adversarially alter reading comprehension examples.", "labels": [], "entities": []}, {"text": "Prior work in computer vision adds imperceptible adversarial perturbations to input images, relying on the fact that such small perturbations cannot change an image's true label (.", "labels": [], "entities": []}, {"text": "In contrast, changing even one word of a paragraph can drastically alter its meaning.", "labels": [], "entities": []}, {"text": "Instead of relying on semantics-preserving perturbations, we create adversarial examples by adding distracting sentences to the input paragraph, as shown in.", "labels": [], "entities": []}, {"text": "We automatically generate these sentences so that they confuse models, but do not contradict the correct answer or confuse humans.", "labels": [], "entities": []}, {"text": "For our main results, we use a simple set of rules to generate a raw distractor sentence that does not answer the question but looks related; we then fix grammatical errors via crowdsourcing.", "labels": [], "entities": []}, {"text": "While adversarially perturbed images punish model oversensitivity to imperceptible noise, our adversarial examples target model overstabilitythe inability of a model to distinguish a sentence that actually answers the question from one that merely has words in common with it.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that no published open-source model is robust to the addition of adversarial sentences.", "labels": [], "entities": []}, {"text": "Across sixteen such models, adding grammatical adversarial sentences reduces F1 score from an average of 75% to 36%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9534890055656433}]}, {"text": "On a smaller set of four models, we run additional experiments in which the adversary adds nongrammatical sequences of English words, causing average F1 score to drop further to 7%.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9741063714027405}]}, {"text": "To encourage the development of new models that understand language more precisely, we have released all of our code and data publicly.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given a model f that takes in paragraph-question pairs (p, q) and outputs an answer\u00e2answer\u02c6answer\u00e2, the standard accuracy over a test set D testis simply where v is the F1 score between the true answer a and the predicted answer f (p, q) (see for details).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9989976286888123}, {"text": "F1 score", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9827747642993927}]}, {"text": "It is noteworthy that the Mnemonic Reader models ( ) outperform the other models by about 6 F1 points.", "labels": [], "entities": [{"text": "Mnemonic Reader", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.6811006665229797}, {"text": "F1", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9970460534095764}]}, {"text": "We hypothesize that Mnemonic Reader's self-alignment layer, which helps model long-distance relationships between parts of the paragraph, makes it better at locating all pieces of evidence that support the correct answer.", "labels": [], "entities": []}, {"text": "Therefore, it can be more confident in the correct answer, compared to the fake answer inserted by the adversary.", "labels": [], "entities": []}, {"text": "To ensure our results are valid, we verified that humans are not also fooled by our adversarial examples.", "labels": [], "entities": []}, {"text": "As ADDANY requires too many model queries to run against humans, we focused on ADDSENT.", "labels": [], "entities": []}, {"text": "We presented each original and adversarial paragraph-question pair to three crowdworkers, and asked them to select the correct answer by copy-and-pasting from the paragraph.", "labels": [], "entities": []}, {"text": "We then took a majority vote over the three responses (if all three responses were different, we picked one at random).", "labels": [], "entities": []}, {"text": "These results are shown in Table 4.", "labels": [], "entities": []}, {"text": "On original examples, our humans are actually slightly better than the reported number of 91.2 F1 on the entire development set.", "labels": [], "entities": [{"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.8951093554496765}]}, {"text": "On ADDSENT, human accuracy drops by 13.1 F1 points, much less than the computer systems.", "labels": [], "entities": [{"text": "ADDSENT", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.7582802772521973}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.996358335018158}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9947983026504517}]}, {"text": "Moreover, much of this decrease can be explained by mistakes unrelated to our adversarial sentences.", "labels": [], "entities": []}, {"text": "Recall that ADDSENT picks the worst case over up to five different paragraph-question pairs.", "labels": [], "entities": [{"text": "ADDSENT", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.6985129714012146}]}, {"text": "Even if we showed the same original example to five sets of three crowdworkers, chances are that at least one of the five groups would make a mistake, just because humans naturally err.", "labels": [], "entities": []}, {"text": "Therefore, it is more meaningful to evaluate humans on ADDONESENT, on which their accuracy drops by only 3.4 F1 points.", "labels": [], "entities": [{"text": "ADDONESENT", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.595999002456665}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9995038509368896}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9970398545265198}]}], "tableCaptions": [{"text": " Table 2: Adversarial evaluation on the Match- LSTM and BiDAF systems. All four systems can  be fooled by adversarial examples.", "labels": [], "entities": [{"text": "Match- LSTM", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.5967167019844055}]}, {"text": " Table 4: Human evaulation on adversarial exam- ples. Human accuracy drops on ADDSENT mostly  due to unrelated errors; the ADDONESENT num- bers show that humans are robust to adversarial  sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.981734037399292}]}, {"text": " Table 5: Transferability of adversarial examples  across models. Each row measures performance  on adversarial examples generated to target one  particular model; each column evaluates one (pos- sibly different) model on these examples.", "labels": [], "entities": []}, {"text": " Table 6: Effect of training the BiDAF Single  model on the original training data alone (first  column) versus augmenting the data with raw  ADDSENT examples (second column).", "labels": [], "entities": []}]}