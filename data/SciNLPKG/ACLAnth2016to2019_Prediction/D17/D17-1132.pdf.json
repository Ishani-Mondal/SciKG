{"title": [], "abstractContent": [{"text": "This paper looks at the task of predicting word association strengths across three datasets; WordNet Evocation (Boyd-Graber et al., 2006), University of Southern Florida Free Association norms (Nel-son et al., 2004), and Edinburgh Associa-tive Thesaurus (Kiss et al., 1973).", "labels": [], "entities": [{"text": "Edinburgh Associa-tive Thesaurus (Kiss et al., 1973)", "start_pos": 221, "end_pos": 273, "type": "DATASET", "confidence": 0.8923255980014801}]}, {"text": "We achieve results of r = 0.357 and \u03c1 = 0.379, r = 0.344 and \u03c1 = 0.300, an \u03c1 = 0.292 and \u03c1 = 0.363, respectively.", "labels": [], "entities": []}, {"text": "We find Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) cosine similarities, as well as vector offsets, to be the highest performing features.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9689982533454895}]}, {"text": "Furthermore , we examine the usefulness of Gaussian embeddings (Vilnis and McCal-lum, 2014) for predicting word association strength, the first work to do so.", "labels": [], "entities": [{"text": "predicting word association strength", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.8660308867692947}]}], "introductionContent": [{"text": "Word embeddings such as) or GloVe () have received increasing attention in the world of natural language processing and computational linguistics.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.6666603485743204}]}, {"text": "Under such embeddings, the semantic relatedness of two words is generally taken to be the cosine similarity of their word vectors.", "labels": [], "entities": []}, {"text": "Although this approach performs well for variety of applications, it is not without its limitations.", "labels": [], "entities": []}, {"text": "First, it defines \"relatedness\" quite narrowly as the extent to which the two words appear in similar contexts.", "labels": [], "entities": []}, {"text": "Second, it fails to capture how humans internally represent words.", "labels": [], "entities": []}, {"text": "Word associations offer a more flexible view of semantic relatedness by leveraging \"lexical knowledge acquired through world experience\" ().", "labels": [], "entities": []}, {"text": "While word embeddings capture distributional relationships, word associations are able to capture more nuanced relationships \"which are based on human perception and experiences are not reflected in common language usage.\")", "labels": [], "entities": []}, {"text": "For example, \"yellow\" is so closely associated with \"banana\" that many people would only specify a banana's colour if it is not yellow.", "labels": [], "entities": []}, {"text": "This is backed up by De Deyne et al.", "labels": [], "entities": []}, {"text": "(2016b) which found word associations performed better than word embeddings across a variety of semantic relatedness tasks.", "labels": [], "entities": []}, {"text": "Furthermore, word associations, unlike cosine similarities, are asymmetric; when presented with the word \"beer\", many people think of the word \"glass\" but when presented with the word \"glass\", few people think of the word \"beer\".", "labels": [], "entities": []}, {"text": "This directionality allows for more fine-grained exploration of semantic links, with applications in word similarity) and computational humour.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7664369642734528}]}, {"text": "Although several word association datasets exist, such as the Edinburgh Associative Thesaurus (EAT,, the University of South Florida Free Association Norms (USF,), or WordNet Evocation), their reliance on human annotations mean they all suffer from coverage issues relating to limited vocabularies or sparse connectivity.", "labels": [], "entities": [{"text": "Edinburgh Associative Thesaurus (EAT", "start_pos": 62, "end_pos": 98, "type": "DATASET", "confidence": 0.9173568248748779}]}, {"text": "Although these issues would be somewhat alleviated by the creation of larger datasets, collecting human judgments for all possible word pairs is impractical.", "labels": [], "entities": []}, {"text": "Therefore, the ability to predict association strengths between arbitrary word pairs represents the best solution to these coverage issues).", "labels": [], "entities": []}, {"text": "Although the prediction of Evocation ratings has attracted some attention, to the best of our knowledge this is the first work to focus on the prediction of USF or EAT strengths.", "labels": [], "entities": [{"text": "prediction of Evocation", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7082871397336324}, {"text": "USF or EAT", "start_pos": 157, "end_pos": 167, "type": "DATASET", "confidence": 0.5884968439737955}]}, {"text": "As described in Sec-tion 2, USF and EAT have several advantages over Evocation, such as the ability to work with ambiguous words instead of WordNet synsets.", "labels": [], "entities": [{"text": "USF", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.7335681319236755}]}, {"text": "Following Hayashi (2016)'s work on Evocation prediction, we frame word association prediction as a supervised regression task and introduce several new and modified features, including the first use of Gaussian embeddings) to better capture the asymmetric nature of word associations.", "labels": [], "entities": [{"text": "Evocation prediction", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9269038140773773}, {"text": "word association prediction", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.7752291560173035}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Individual feature performance after 50  epochs", "labels": [], "entities": []}, {"text": " Table 2: Ablation performance after 50 epochs", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9976032376289368}]}]}