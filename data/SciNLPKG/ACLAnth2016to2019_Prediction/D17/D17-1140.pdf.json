{"title": [{"text": "GrASP: Rich Patterns for Argumentation Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the GrASP algorithm for automatically extracting patterns that characterize subtle linguistic phenomena.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.8554737567901611}]}, {"text": "To that end, GrASP augments each term of input text with multiple layers of linguistic information.", "labels": [], "entities": []}, {"text": "These different facets of the text terms are systematically combined to reveal rich patterns.", "labels": [], "entities": []}, {"text": "We report highly promising experimental results in several challenging text analysis tasks within the field of Argumentation Mining.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.715169757604599}, {"text": "Argumentation Mining", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.8871104121208191}]}, {"text": "We believe that GrASP is general enough to be useful for other domains too.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.8140182495117188}]}], "introductionContent": [{"text": "Many standard text analysis tasks can be addressed relatively well while exploiting simple textual features, e.g., Bag-Of-Words representation and Naive Bayes for document classification.", "labels": [], "entities": [{"text": "Bag-Of-Words representation", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.7750422358512878}, {"text": "document classification", "start_pos": 163, "end_pos": 186, "type": "TASK", "confidence": 0.7857137024402618}]}, {"text": "However, the identification of more subtle linguistic phenomena, that are further reflected via relatively short texts -as opposed to whole documents -may require a wider spectrum of linguistic features.", "labels": [], "entities": []}, {"text": "The main contribution of this work is in outlining a simple method to automatically extract rich linguistic features in the form of patterns, and demonstrate their utility on tasks related to Argumentation Mining (Mochales), although we believe that the proposed approach is not limited to this domain.", "labels": [], "entities": [{"text": "Argumentation Mining (Mochales)", "start_pos": 192, "end_pos": 223, "type": "TASK", "confidence": 0.7771435379981995}]}, {"text": "Argumentation mining involves automatically identifying argumentative structures within a corpus -e.g., claims or conclusions, and evidence instances or premises -as well as their interrelations.", "labels": [], "entities": [{"text": "Argumentation mining", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8613421320915222}]}, {"text": "For instance, each of the following sentences includes a claim fora.", "labels": [], "entities": []}, {"text": "* First two authors contributed equally.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following experiments we used a logistic regression classifier on top of the extracted patterns.", "labels": [], "entities": []}, {"text": "Each pattern is used as a binary feature, which receives value of 1 iff it is matched in the candidate.", "labels": [], "entities": []}, {"text": "To demonstrate the robustness of the this approach, in all experiments we report the results of a single configuration of GrASP parameters, selected based on a quick analysis over a small portion of the claim-sentence detection data (task (a) below).", "labels": [], "entities": [{"text": "claim-sentence detection", "start_pos": 203, "end_pos": 227, "type": "TASK", "confidence": 0.7386440336704254}]}, {"text": "Specifically, we used minimal frequency threshold t 1 = 0.005, correlation threshold t 2 = 0.5, size of the alphabet k 1 = 100, number of patterns in the output k 2 = 100, maximal pattern length maxLen = 5, and window size w = 10.", "labels": [], "entities": [{"text": "correlation threshold t 2", "start_pos": 63, "end_pos": 88, "type": "METRIC", "confidence": 0.9648313224315643}, {"text": "maximal pattern length maxLen", "start_pos": 172, "end_pos": 201, "type": "METRIC", "confidence": 0.8656139373779297}]}, {"text": "This configuration is by no means the optimal one, and we saw that by carefully tuning the parameters per task, results were improved.", "labels": [], "entities": []}, {"text": "We consider three context-dependent argumentation mining tasks: (a) Claim sentence detection ( ), (b) Expert evidence detection, and (c) Study evidence detection.", "labels": [], "entities": [{"text": "context-dependent argumentation mining", "start_pos": 18, "end_pos": 56, "type": "TASK", "confidence": 0.6368358035882314}, {"text": "Claim sentence detection", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6748906075954437}, {"text": "Expert evidence detection", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.6045777400334676}, {"text": "Study evidence detection", "start_pos": 137, "end_pos": 161, "type": "TASK", "confidence": 0.7108244299888611}]}, {"text": "The latter two tasks are described in, where the goal is to detect sentences that can be used as an evidence to support/contest the topic.", "labels": [], "entities": []}, {"text": "The benchmark data for these tasks was extracted from the data released by, consisting of 547 Wikipedia articles in which claims and evidence instances were manually annotated, in the context of 58 debatable topics.", "labels": [], "entities": []}, {"text": "In all tasks the data is highly skewed towards negative examples (only 2.5% of 80.5K instances are positives in task (a), 4% of 55.6K in task (b), and 3.7% of 31.8K in task (c)), making these tasks especially challenging.", "labels": [], "entities": []}, {"text": "As () we use a leave-one-topic-out schema; training over 57 topics, testing over the left out topic.", "labels": [], "entities": []}, {"text": "Our group develops debate supportive technologies which can assist humans to reason, make decisions, or persuade others.", "labels": [], "entities": []}, {"text": "Since in this scenario humans mainly consider top results (similar to information retrieval), precision is more relevant than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9992486834526062}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9979573488235474}]}, {"text": "Thus, we report the macro-averaged Precision@K, where K \u2208 {5, 10, 20}.", "labels": [], "entities": [{"text": "Precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9620954990386963}]}, {"text": "We considered the following baselines: Naive Bayes: over BOW representation, discarding unigrams which appear less than 10 times.", "labels": [], "entities": []}, {"text": "Basic Patterns: a baseline that reflects common practices in the literature where a pattern is a consecutive ordered list of stop words or POS tags.", "labels": [], "entities": []}, {"text": "We add a symbol for topic match (for the context-dependent tasks).", "labels": [], "entities": []}, {"text": "A brute force process generates all possible patterns up to size maxLen and selects top k by the same procedure as GrASP.", "labels": [], "entities": []}, {"text": "We did not examine the Anecdotal type due to the small size of the available benchmark data.", "labels": [], "entities": [{"text": "Anecdotal type", "start_pos": 23, "end_pos": 37, "type": "METRIC", "confidence": 0.9366629719734192}]}, {"text": "for more details see IBM Debating Technologies.", "labels": [], "entities": [{"text": "IBM Debating Technologies", "start_pos": 21, "end_pos": 46, "type": "DATASET", "confidence": 0.8017754952112833}]}, {"text": "For each task we report the best results obtained with k \u2208 {50, 100, .., 400}.", "labels": [], "entities": []}, {"text": "Convolutional Neural Network (CNN): following we used CNN whose input is a concatenation of the topic and the candidate.", "labels": [], "entities": []}, {"text": "The final state vector is fed to a LR soft-max layer.", "labels": [], "entities": []}, {"text": "Cross-entropy loss function was used for training.", "labels": [], "entities": []}, {"text": "The embedding layer was initialized using word2vec vectors (.", "labels": [], "entities": []}, {"text": "Hyper-parameters were tuned on the same portion of the dataset as used by GrASP for tuning.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8910496830940247}]}, {"text": "For these baselines, we are not aware of available methods to incorporate GrASP multi-layered representation.", "labels": [], "entities": []}, {"text": "GrASP alphabet: a simplified version of GrASP which uses the chosen alphabet, or \"patterns\" of length 1.", "labels": [], "entities": [{"text": "GrASP alphabet", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8500193953514099}, {"text": "GrASP", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.900255024433136}]}, {"text": "This baseline does utilize all the information available for GrASP.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8347396850585938}]}, {"text": "shows that Naive Bayes performance is the lowest, demonstrating that a simple representation is not sufficient for such complex tasks.", "labels": [], "entities": []}, {"text": "Using Basic patterns yields better performance, and CNN performs even better.", "labels": [], "entities": []}, {"text": "GrASP alphabet outperforms CNN, indicating the potential of explicitly incorporating linguistic information.", "labels": [], "entities": []}, {"text": "Finally, using the patterns extracted by GrASP outperforms all other methods, emphasizing the added value of constructing patterns over the initial contribution of the multi-layered representation.", "labels": [], "entities": []}, {"text": "GrASP provides an easy way to analyze the importance of each attribute by inspecting its score at the end of the first iteration, the one which determines the alphabet.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8124133944511414}]}, {"text": "For example, PERCENT score was very high in the alphabet for Study evidence patterns (task b), and Person and Organization were ranked high in the alphabet of the Expert evidence (task c).", "labels": [], "entities": [{"text": "PERCENT score", "start_pos": 13, "end_pos": 26, "type": "METRIC", "confidence": 0.9856529831886292}]}, {"text": "Still, these three named enti-  Significant results in comparison to Levy14-rep are marked by * (paired t-test with p < 0.02).", "labels": [], "entities": [{"text": "enti-  Significant", "start_pos": 25, "end_pos": 43, "type": "METRIC", "confidence": 0.8840821186701456}, {"text": "Levy14-rep", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8969694375991821}]}, {"text": "ties were not selected as part of the alphabet for Claim sentences (task a) -reflecting the importance of PERCENT in sentences describing studies and their numeric results, and the importance of authoritative source (either Person or Organization) in evidence based on expert testimonies.", "labels": [], "entities": [{"text": "PERCENT", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.9938001036643982}]}, {"text": "For task (a) we performed two ablation tests, each of them yielded a decrease in performance: (i) not limiting the match of a pattern in a window (a decrease of 10.3 for P@5 and 2.8 for P@20), and (ii) not enforcing the order defined by the pattern (a decrease of 7.6 for P@5 and 2.8 for P@20).", "labels": [], "entities": []}, {"text": "In this evaluation we add GrASP patterns as additional features to the full claim detection system of  to inspect their contribution.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9277750253677368}]}, {"text": "This evaluation is performed on a second claim detection benchmark (on which they have reported results), released by  (1,387 annotated claims associated with 33 topics).", "labels": [], "entities": []}, {"text": "The system of  is comprised of a cascade of three components; (i) detecting sentences which contain claims, (ii) identifying the exact boundaries of the claim part within the sentence, and (iii) ranking the claim candidates.", "labels": [], "entities": []}, {"text": "Each of these components applies a classifier over dedicated features.", "labels": [], "entities": []}, {"text": "Results were reported for the full cascade and for the first component, which is our task (a).", "labels": [], "entities": []}, {"text": "For an idea on how to adapt GrASP for the claim boundaries detection task, see Section 5.", "labels": [], "entities": [{"text": "claim boundaries detection task", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.7260654717683792}]}, {"text": "presents measures reported in  (right hand side) as well as additional measures which reflect the focus of this work on the precision of the top ranked candidates (performance of all systems on P@200 and R@200 were comparable and were omitted due to space limitations).", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9975675344467163}]}, {"text": "The system of , denoted Levy14, and our reproduction of it, denoted Levy14-rep, obtained comparable results.", "labels": [], "entities": []}, {"text": "8 We reproduced their work to perform significant test and report the additional measures.", "labels": [], "entities": []}, {"text": "Evidently, utilizing GrASP patterns alone achieve similar performance as Levy14-rep.", "labels": [], "entities": []}, {"text": "Considering the fact that Levy14-rep is a full system, tailored for claim detection via a lengthy feature engineering process, these results, obtained using only GrASP patterns, are promising.", "labels": [], "entities": [{"text": "Levy14-rep", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.937008798122406}, {"text": "claim detection", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7748033702373505}]}, {"text": "Adding GrASP features to Levy14-rep, denoted Combined, we observe a significant improvement, demonstrating their complementary value.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9672651290893555}, {"text": "Levy14-rep", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8747560381889343}]}], "tableCaptions": [{"text": " Table 2: Macro-averaged precision results for GrASP over three argumentation mining tasks. Significant results in compari-", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9637348055839539}, {"text": "argumentation mining tasks", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.8130345940589905}]}, {"text": " Table 3: Adding GrASP to a full claim detection system.", "labels": [], "entities": [{"text": "GrASP", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.9569764733314514}, {"text": "claim detection", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7369292229413986}]}]}