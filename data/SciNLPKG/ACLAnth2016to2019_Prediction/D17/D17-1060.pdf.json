{"title": [{"text": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "labels": [], "entities": [{"text": "Knowledge Graph Reasoning", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.7721655964851379}]}], "abstractContent": [{"text": "We study the problem of learning to reason in large scale knowledge graphs (KGs).", "labels": [], "entities": []}, {"text": "More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path.", "labels": [], "entities": []}, {"text": "In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9990057349205017}]}, {"text": "Experimentally , we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.", "labels": [], "entities": [{"text": "Never-Ending Language Learning datasets", "start_pos": 147, "end_pos": 186, "type": "DATASET", "confidence": 0.7041276022791862}]}], "introductionContent": [{"text": "In recent years, deep learning techniques have obtained many state-of-the-art results in various classification and recognition problems (.", "labels": [], "entities": [{"text": "classification and recognition", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.8697243928909302}]}, {"text": "However, complex natural language processing problems often require multiple inter-related decisions, and empowering deep learning models with the ability of learning to reason is still a challenging issue.", "labels": [], "entities": []}, {"text": "To handle complex queries where there are no obvious answers, intelligent machines must be able to reason with existing resources, and learn to infer an unknown answer.", "labels": [], "entities": []}, {"text": "More specifically, we situate our study in the context of multi-hop reasoning, which is the task of learning explicit inference formulas, given a large KG.", "labels": [], "entities": [{"text": "multi-hop reasoning", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7483052611351013}]}, {"text": "For example, if the KG includes the beliefs such as Neymar plays for Barcelona, and Barcelona are in the La Liga league, then machines should be able to learn the following formula: playerPlaysForTeam(P,T) \u2227 teamPlaysInLeague(T,L) \u21d2 playerPlaysInLeague(P,L).", "labels": [], "entities": []}, {"text": "In the testing time, by plugging in the learned formulas, the system should be able to automatically infer the missing link between a pair of entities.", "labels": [], "entities": []}, {"text": "This kind of reasoning machine will potentially serve as an essential components of complex QA systems.", "labels": [], "entities": []}, {"text": "In recent years, the Path-Ranking Algorithm (PRA) () emerges as a promising method for learning inference paths in large KGs.", "labels": [], "entities": []}, {"text": "PRA uses a random-walk with restarts based inference mechanism to perform multiple bounded depth-first search processes to find relational paths.", "labels": [], "entities": []}, {"text": "Coupled with elastic-net based learning, PRA then picks more plausible paths using supervised learning.", "labels": [], "entities": [{"text": "PRA", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9819747805595398}]}, {"text": "However, PRA operates in a fully discrete space, which makes it difficult to evaluate and compare similar entities and relations in a KG.", "labels": [], "entities": [{"text": "PRA", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9681379199028015}]}, {"text": "In this work, we propose a novel approach for controllable multi-hop reasoning: we frame the path learning process as reinforcement learning (RL).", "labels": [], "entities": [{"text": "multi-hop reasoning", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7319826185703278}, {"text": "reinforcement learning (RL)", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.6466950893402099}]}, {"text": "In contrast to PRA, we use translationbased knowledge based embedding method) to encode the continuous state of our RL agent, which reasons in the vector space environment of the knowledge graph.", "labels": [], "entities": []}, {"text": "The agent takes incremental steps by sampling a relation to extend its path.", "labels": [], "entities": []}, {"text": "To better guide the RL agent for learning relational paths, we use policy gradient training () with a novel reward function that jointly encourages accuracy, diversity, and efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9983951449394226}]}, {"text": "Empirically, we show that our method outperforms PRA and embedding based methods on a Freebase and a Never-Ending Language) dataset.", "labels": [], "entities": []}, {"text": "Our contributions are three-fold: \u2022 We are the first to consider reinforcement learning (RL) methods for learning relational paths in knowledge graphs; \u2022 Our learning method uses a complex reward function that considers accuracy, efficiency, and path diversity simultaneously, offering better control and more flexibility in the pathfinding process; \u2022 We show that our method can scale up to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 65, "end_pos": 92, "type": "TASK", "confidence": 0.710936713218689}, {"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9962116479873657}]}, {"text": "In the next section, we outline related work in path-finding and embedding methods in KGs.", "labels": [], "entities": []}, {"text": "We describe the proposed method in Section 3.", "labels": [], "entities": []}, {"text": "We show experimental results in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the reasoning formulas found by our RL agent, we explore two standard KG reasoning tasks: link prediction (predicting target entities) and fact prediction (predicting whether an unknown fact holds or not).", "labels": [], "entities": [{"text": "KG reasoning", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.844651609659195}, {"text": "link prediction", "start_pos": 102, "end_pos": 117, "type": "TASK", "confidence": 0.6816181689500809}, {"text": "fact prediction", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.7049005031585693}, {"text": "predicting whether an unknown fact", "start_pos": 168, "end_pos": 202, "type": "TASK", "confidence": 0.778522276878357}]}, {"text": "We compare our method with both path-based methods and embedding based methods.", "labels": [], "entities": []}, {"text": "After that, we further analyze the reasoning paths found by our RL agent.", "labels": [], "entities": []}, {"text": "These highly predictive paths validate the effectiveness of the reward functions.", "labels": [], "entities": []}, {"text": "Finally, we conduct a experiment to investigate the effect of the supervised learning procedure.", "labels": [], "entities": []}, {"text": "To facilitate path finding, we also add the inverse triples.", "labels": [], "entities": [{"text": "path finding", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.8170178532600403}]}, {"text": "For each triple (h, r, t), we append (t, r \u22121 , h) to the datasets.", "labels": [], "entities": []}, {"text": "With these inverse triples, the agent is able to step backward in the KG.", "labels": [], "entities": []}, {"text": "For each reasoning task r i , we remove all the triples with r i or r \u22121 i from the KG.", "labels": [], "entities": []}, {"text": "These removed triples are split into train and test samples.", "labels": [], "entities": []}, {"text": "For the link prediction task, each h in the test triples {(h, r, t)} is considered as one query.", "labels": [], "entities": [{"text": "link prediction task", "start_pos": 8, "end_pos": 28, "type": "TASK", "confidence": 0.8669292529424032}]}, {"text": "A set of candidate target entities are ranked using different methods.", "labels": [], "entities": []}, {"text": "For fact prediction, the true test triples are ranked with some generated false triples.", "labels": [], "entities": [{"text": "fact prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7474706768989563}]}], "tableCaptions": [{"text": " Table 1: Statistics of the Datasets. # Ent. denotes the number  of unique entities and # R. denotes the number of relations", "labels": [], "entities": []}, {"text": " Table 2: Link prediction results (MAP) on two datasets.", "labels": [], "entities": [{"text": "Link prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.7329904139041901}]}, {"text": " Table 3: Fact prediction results (MAP) on two datasets.", "labels": [], "entities": [{"text": "Fact prediction", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.811878889799118}]}, {"text": " Table 4: Number of reasoning paths used by PRA and our RL  model. RL achieved better MAP with a more compact set of  learned paths.", "labels": [], "entities": [{"text": "MAP", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9714420437812805}]}]}