{"title": [{"text": "Neural Machine Translation with Source Dependency Representation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7752798795700073}]}], "abstractContent": [{"text": "Source dependency information has been successfully introduced into statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7326065003871918}]}, {"text": "However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.7942619125048319}]}, {"text": "In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences.", "labels": [], "entities": []}, {"text": "Empirical results on NIST Chinese-to-English translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation task", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.7710029035806656}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9993966817855835}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT)) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.812419613202413}]}, {"text": "Recently, several research works have been proposed to learn richer source representation, such as multisource information, and particularly source syntactic information (, thus improving the performance of NMT.", "labels": [], "entities": []}, {"text": "In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.7862693071365356}]}, {"text": "Actually, source dependency information has been shown greatly effective in * Kehai Chen was an internship research fellow at NICT when conducting this work.", "labels": [], "entities": [{"text": "NICT", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.9111050367355347}]}, {"text": "Statistical Machine Translation (SMT) (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7913378377755483}]}, {"text": "In NMT, there has been a quite recent preliminary exploration, in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel NMT with source dependency representation to improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.9612061977386475}]}, {"text": "Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information.", "labels": [], "entities": [{"text": "vector concatenation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.7274593114852905}]}, {"text": "To this end, we create a dependency unit for each source word to capture long-distance dependency constraints.", "labels": [], "entities": []}, {"text": "Then we design an Encoder with convolutional architecture to jointly learn SDRs (Section 3) and source dependency annotations, thus computing dependency context vectors and hidden states by a novel double-context based Decoder for word prediction (Section 4).", "labels": [], "entities": [{"text": "word prediction", "start_pos": 231, "end_pos": 246, "type": "TASK", "confidence": 0.8055773973464966}]}, {"text": "Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves significant gains over the method by, and thus delivers substantial improvements over the standard attentional NMT (Section 5).", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation task", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.784898117184639}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on NIST Chinese-to-English Translation Task. \"*\" indicates statistically significant  better than \"Sennrich-deponly\" at p-value < 0.05 and \"**\" at p-value < 0.01. AVG = average BLEU  scores for test sets.", "labels": [], "entities": [{"text": "NIST Chinese-to-English Translation Task", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.7272827625274658}, {"text": "AVG", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9993934631347656}, {"text": "BLEU", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9993345141410828}]}]}