{"title": [], "abstractContent": [{"text": "Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications.", "labels": [], "entities": []}, {"text": "Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.8043833076953888}]}, {"text": "However, this work is limited to English, and cannot process dependency graphs, which allow handling complex phenomena such as control.", "labels": [], "entities": []}, {"text": "In this work, we introduce UDEPLAMBDA, a semantic interface for UD, which maps natural language to logical forms in an almost language-independent fashion and can process dependency graphs.", "labels": [], "entities": []}, {"text": "We perform experiments on question answering against Freebase and provide German and Spanish translations of the WebQuestions and GraphQuestions datasets to facilitate multilingual evaluation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8387232422828674}, {"text": "WebQuestions and GraphQuestions datasets", "start_pos": 113, "end_pos": 153, "type": "DATASET", "confidence": 0.6859648823738098}]}, {"text": "Results show that UDEPLAMBDA outperforms strong base-lines across languages and datasets.", "labels": [], "entities": []}, {"text": "For English, it achieves a 4.9 F 1 point improvement over the state-of-the-art on Graph-Questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Universal Dependencies (UD) initiative seeks to develop cross-linguistically consistent annotation guidelines as well as a large number of uniformly annotated treebanks for many languages . Such resources could advance multilingual applications of parsing, improve comparability of evaluation results, enable cross-lingual learning, and more generally support natural language understanding.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 364, "end_pos": 394, "type": "TASK", "confidence": 0.6615709960460663}]}, {"text": "* Work done at the University of Edinburgh Seeking to exploit the benefits of UD for natural language understanding, we introduce UDEP-LAMBDA, a semantic interface for UD that maps natural language to logical forms, representing underlying predicate-argument structures, in an almost language-independent manner.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.6787856618563334}]}, {"text": "Our framework is based on DEPLAMBDA ( ) a recently developed method that converts English Stanford Dependencies (SD) to logical forms.", "labels": [], "entities": [{"text": "DEPLAMBDA", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.6743782162666321}]}, {"text": "The conversion process is illustrated in and discussed in more detail in Section 2.", "labels": [], "entities": [{"text": "conversion", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9890555143356323}]}, {"text": "Whereas DEPLAMBDA works only for English, U-DEPLAMBDA applies to any language for which UD annotations are available.", "labels": [], "entities": []}, {"text": "Moreover, DEP-LAMBDA can only process tree-structured inputs whereas UDEPLAMBDA can also process dependency graphs, which allow to handle complex constructions such as control.", "labels": [], "entities": []}, {"text": "The different treatments of various linguistic constructions in UD compared to SD also require different handling in UDEP-LAMBDA.", "labels": [], "entities": []}, {"text": "Our experiments focus on Freebase semantic parsing as a testbed for evaluating the framework's multilingual appeal.", "labels": [], "entities": [{"text": "Freebase semantic parsing", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7798880934715271}]}, {"text": "We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase.", "labels": [], "entities": []}, {"text": "To facilitate multilingual evaluation, we provide translations of the English WebQuestions () and GraphQuestions (  datasets to German and Spanish.", "labels": [], "entities": []}, {"text": "We demonstrate that UDEPLAMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge.", "labels": [], "entities": []}, {"text": "Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that U-DEPLAMBDA outperforms strong baselines across languages and datasets.", "labels": [], "entities": [{"text": "multilingual semantic parsing", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.7103708585103353}, {"text": "Freebase", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.9430244565010071}]}, {"text": "For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions.", "labels": [], "entities": [{"text": "WebQuestions", "start_pos": 101, "end_pos": 113, "type": "DATASET", "confidence": 0.9566327333450317}]}, {"text": "Our implementation and translated datasets are publicly available at https://github.com/sivareddyg/udeplambda.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (), a widely used benchmark consisting of English questions and their answers, and GraphQuestions ( , a recently released dataset of English questions with both their answers and grounded logical forms.", "labels": [], "entities": [{"text": "question answering", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7302699238061905}]}, {"text": "While WebQuestions is dominated by simple entityattribute questions, GraphQuestions contains a large number of compositional questions involving aggregation (e.g. How many children of Eddard Stark were born in Winterfell?", "labels": [], "entities": []}, {"text": ") and comparison (e.g. In which month does the average rainfall of New York City exceed 86 mm?", "labels": [], "entities": [{"text": "comparison", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.9549349546432495}]}, {"text": "). The number of training, development and test questions is for GraphQuestions.", "labels": [], "entities": []}, {"text": "To support multilingual evaluation, we created translations of WebQuestions and GraphQuestions to German and Spanish.", "labels": [], "entities": []}, {"text": "For WebQuestions two professional annotators were hired per language, while for GraphQuestions we used a trusted pool of 20 annotators per language (with a single annotator per question).", "labels": [], "entities": []}, {"text": "Examples of the original questions and their translations are provided in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example questions and their translations.", "labels": [], "entities": [{"text": "Example questions and their translations", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.6236812829971313}]}, {"text": " Table 2: Structured perceptron k-best entity linking  accuracies on the development sets.", "labels": [], "entities": []}, {"text": " Table 3: F 1 -scores on the test data.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9616652727127075}]}]}