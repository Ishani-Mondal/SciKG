{"title": [{"text": "Dual Tensor Model for Detecting Asymmetric Lexico-Semantic Relations", "labels": [], "entities": [{"text": "Detecting Asymmetric Lexico-Semantic Relations", "start_pos": 22, "end_pos": 68, "type": "TASK", "confidence": 0.8977299779653549}]}], "abstractContent": [{"text": "Detection of lexico-semantic relations is one of the central tasks of computational semantics.", "labels": [], "entities": [{"text": "Detection of lexico-semantic relations", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.9012016355991364}]}, {"text": "Although some fundamental relations (e.g., hypernymy) are asymmetric, most existing models account for asymmetry only implicitly and use the same concept representations to support detection of symmetric and asymmetric relations alike.", "labels": [], "entities": []}, {"text": "In this work, we propose the Dual Tensor model, a neural architecture with which we explicitly model the asymmetry and capture the translation between unspecialized and specialized word embed-dings via a pair of tensors.", "labels": [], "entities": []}, {"text": "Although our Dual Tensor model needs only unspecial-ized embeddings as input, our experiments on hypernymy and meronymy detection suggest that it can outperform more complex and resource-intensive models.", "labels": [], "entities": [{"text": "meronymy detection", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.6638107597827911}]}, {"text": "We further demonstrate that the model can account for polysemy and that it exhibits stable performance across languages.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We evaluate the Dual Tensor model on several datasets for detecting hypernymy and meronymy, two arguably most prominent asymmetric lexicosemantic relations.", "labels": [], "entities": [{"text": "detecting hypernymy", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7600025534629822}]}, {"text": "In all experiments, we compare the model's performance with state-of-the-art results on respective datasets.", "labels": [], "entities": []}, {"text": "Additionally, aiming to quantify the effects that different components of the Dual Tensor model have on prediction performance, we evaluate two reduced models variants.", "labels": [], "entities": []}, {"text": "We evaluate the Dual Tensor model on the following hypernymy and meronymy detection datasets: HypeNet dataset.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9429372847080231}]}, {"text": "Arguing that existing datasets were too small for training their recurrent network,  compiled this dataset for hypernymy detection from several external KBs, taking only pairs of concepts indirect relation (i.e., no transitive closure).", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7613845765590668}]}, {"text": "We additionally evaluate the Dual Tensor model on four smaller datasets for hypernymy detection: (1) BLESS dataset ( WN-Hy and WN-Me datasets.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8216172754764557}, {"text": "BLESS", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9958329796791077}]}, {"text": "We create these datasets by taking concept pairs from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 54, "end_pos": 61, "type": "DATASET", "confidence": 0.9710416197776794}]}, {"text": "We take all instances from the transitive closure of hypernymy (all parts of speech) and meronymy (nouns) relations and couple them with all synonym and antonym relations (all parts of speech), as well as lexical entailment relations (verbs).", "labels": [], "entities": []}, {"text": "For the WN-Hy dataset we designate all hypernymy relations (i.e., both direct and indirect) as positive instances and their inverses (i.e., hyponymy relations) together with all other relations as negative instances.", "labels": [], "entities": [{"text": "WN-Hy dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9610989689826965}]}, {"text": "Finally, we balance the dataset by randomly sampling negative instances to match the number of positive instances.", "labels": [], "entities": []}, {"text": "Analogously, we create the WN-Me dataset by taking meronymy relations as positive instances.", "labels": [], "entities": [{"text": "WN-Me dataset", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.9143010675907135}]}, {"text": "We compile three different WN-Hy datasets: WN-Hy-EN using English WordNet, WNHy-ES using Spanish WordNet (, and WN-Hy-FR using French WordNet (.", "labels": [], "entities": [{"text": "WN-Hy datasets", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.7855193614959717}, {"text": "English WordNet", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.835968554019928}, {"text": "French WordNet", "start_pos": 127, "end_pos": 141, "type": "DATASET", "confidence": 0.8734082281589508}]}, {"text": "To allow for fair comparison of model's performance across languages, we randomly sample two larger dataset (English and French) to match in size the smallest (Spanish).", "labels": [], "entities": []}, {"text": "showed that supervised distributional models for classifying lexico-semantic relations suffer from overfitting in settings with significant lexical overlap between the training and test set.", "labels": [], "entities": []}, {"text": "In such settings models tend to learn properties of individual words (e.g., that a word is a prototypical hypernym) instead of relations between words.", "labels": [], "entities": []}, {"text": "The reported results on such datasets are thus overly optimistic estimates of models' true performance.", "labels": [], "entities": []}, {"text": "WN-Hy-EN 103K (50%) 15K (50%) 30K (50%) WN-Hy-EN 103K (50%) 15K (50%) 30K (50%) WN-Hy-FR 103K (50%) 15K (50%) 30K (50%) WN-Me (rand) 13.9K (50%) 2K (50%) 4K (50%) WN-Me (lex) 7.9K (50%) 208 (50%) 318 (50%): Datasets used in evaluation.", "labels": [], "entities": []}, {"text": "To eliminate the effect of lexical memorization, propose dataset splits with no lexical overlap between the train and test portions.", "labels": [], "entities": []}, {"text": "However, model's performance in a lexically-split setting is an overly pessimistic estimate of models' true performance -in a realistic scenario, the model will occasionally make predictions for pairs involving some of the concepts from the training set.", "labels": [], "entities": []}, {"text": "Because the true model performance is likely between the performance on a randomly-split and performance on a lexically-split dataset, we report models' performance in both of these settings.", "labels": [], "entities": []}, {"text": "We show the sizes of all dataset variants used in our experiments in.", "labels": [], "entities": []}, {"text": "We additionally report the proportion of positive instances (in brackets), as this percentage directly affects some evaluation metrics (precision, F 1 -score, average precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.999489426612854}, {"text": "F 1 -score", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9809374809265137}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.5181595683097839}]}, {"text": "Binary classification is the most straightforward evaluation setting for relation detection models.", "labels": [], "entities": [{"text": "Binary classification", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8591485917568207}, {"text": "relation detection", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.8683300018310547}]}, {"text": "For a pair of concepts, we make the binary asymmetric relation prediction r a (c 1 , c 2 ) simply by thresholding the model's prediction scores, i.e., r a (c 1 , c 2 ) = I{s(c 1 , c 2 ) > 0}, where I is the indicator function.", "labels": [], "entities": []}, {"text": "We first evaluate the DUAL-T model and the baselines on the HypeNet dataset ( . We show the performance of the DUAL-T model in, together with the path-based and hybrid (combination of path-based and distributional signal) variants of the the state-of-the-art RNN model of . On the more challenging, lexically-split dataset DUAL-T model significantly 3 outperforms the more complex hybrid HypeNet model ), an RNN model coupling representations of syntactic paths from a large corpus with  unspecialized concept embeddings.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9217941761016846}]}, {"text": "In both settings DUAL-T outperforms SINGLE-T which, in turn, outperforms BILIN-PROD.", "labels": [], "entities": [{"text": "DUAL-T", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.982853889465332}, {"text": "SINGLE-T", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.8149313926696777}, {"text": "BILIN-PROD", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9966607093811035}]}, {"text": "This empirically justifies both our explicit modeling of asymmetry and relation-specific embedding specialization.", "labels": [], "entities": []}, {"text": "We next evaluate the meronymy classification performance of the models on the WN-Me dataset.", "labels": [], "entities": [{"text": "meronymy classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6883897483348846}, {"text": "WN-Me dataset", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9507316052913666}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Same as in the case of hypernymy classification, DUAL-T significantly outperforms all three baselines, with SINGLE-T outperforming BILIN-PROD.", "labels": [], "entities": [{"text": "hypernymy classification", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8680374026298523}, {"text": "DUAL-T", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9813553690910339}, {"text": "SINGLE-T", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9728111624717712}, {"text": "BILIN-PROD", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.8859833478927612}]}, {"text": "All distributional models we evaluate achieve poorer performance on meronymy than hypernymy detection, especially considering that WN-Me is a balanced dataset, whereas HypeNet is heavily skewed towards negative instances.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7270632088184357}]}, {"text": "Shwartz et al. propose ranking as an alternative evaluation setting for hypernymy detection.", "labels": [], "entities": [{"text": "hypernymy detection", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.8445351421833038}]}, {"text": "The goal is to rank positive relation pairs higher than negative ones.", "labels": [], "entities": []}, {"text": "Our DUAL-T model (and associated baselines) rank the concept pairs in decreasing order of assigned relations scores s(c 1 , c 2 ).", "labels": [], "entities": []}, {"text": "Following, we report performance in terms of overall average precision (AP) and average precision at rank 100 (AP@100).", "labels": [], "entities": [{"text": "overall average precision (AP)", "start_pos": 45, "end_pos": 75, "type": "METRIC", "confidence": 0.6692455361286799}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.7539222836494446}, {"text": "AP", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.9419451951980591}]}, {"text": "We evaluate the ranking performance on four small hypernymy test sets: BLESS, EVALuation, Benotto, and Weeds (cf. Table 1).", "labels": [], "entities": [{"text": "BLESS", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9987888932228088}, {"text": "EVALuation", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9414520263671875}]}, {"text": "As these datasets are not big enough to train neural models, we train all models on the HypeNet dataset.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.9566819965839386}]}, {"text": "For each test set we eliminate the lexical overlap by removing from the HypeNet dataset pairs containing any concept from that test set.", "labels": [], "entities": [{"text": "HypeNet dataset", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9212782382965088}]}, {"text": "displays ranking performance for DUAL-T model, the supervised baselines, and the bestperforming unsupervised hypernymy detection score (BEST-UNSUP, performance taken from ().", "labels": [], "entities": [{"text": "BEST-UNSUP", "start_pos": 136, "end_pos": 146, "type": "METRIC", "confidence": 0.9894261360168457}]}, {"text": "Hypernymy ranking results depict the effectiveness of the DUAL-T model with respect to supervised baselines even more clearly than hypernymy classification results.", "labels": [], "entities": []}, {"text": "All supervised models outperform the best unsupervised model in terms of AP, but only DUAL-T is consistently better when considering only 100 top-ranked pairs (AP@100).", "labels": [], "entities": [{"text": "AP", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9991508722305298}]}, {"text": "This adds to the conclusion that explicit modeling of asymmetry using dual tensors yields crucial performance boost.", "labels": [], "entities": []}, {"text": "We measure the ranking performance for meronymy detection on the WNMe dataset, reporting the results for both randomlyand lexically-split variants of the dataset in.", "labels": [], "entities": [{"text": "meronymy detection", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7478583753108978}, {"text": "WNMe dataset", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9859953820705414}]}, {"text": "Meronymy ranking results are inline with performance figures for hypernymy ranking.", "labels": [], "entities": []}, {"text": "Again, DUAL-T consistently outperforms all three baselines.", "labels": [], "entities": [{"text": "DUAL-T", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.842279851436615}]}, {"text": "Absolute AP scores for meronymy are higher than those we report for hypernymy, but this is merely because WN-Me is a balanced dataset, whereas the hypernymy ranking test sets (with the exception of the Weeds dataset) are substantially skewed in favor of negative concept pairs.", "labels": [], "entities": [{"text": "AP", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.8788561224937439}, {"text": "Weeds dataset", "start_pos": 202, "end_pos": 215, "type": "DATASET", "confidence": 0.9330473244190216}]}], "tableCaptions": [{"text": " Table 2: Hypernymy classification performance.", "labels": [], "entities": [{"text": "Hypernymy classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8427265584468842}]}, {"text": " Table 3: Meronymy classification performance.", "labels": [], "entities": [{"text": "Meronymy classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6947162002325058}]}, {"text": " Table 4: Hypernymy detection, ranking results.", "labels": [], "entities": [{"text": "Hypernymy detection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7084568440914154}]}, {"text": " Table 5: Meronymy detection, ranking results.", "labels": [], "entities": [{"text": "Meronymy detection", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7556273937225342}]}, {"text": " Table 6: Hypernymy classification performance for  different languages.", "labels": [], "entities": [{"text": "Hypernymy classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8083062171936035}]}]}