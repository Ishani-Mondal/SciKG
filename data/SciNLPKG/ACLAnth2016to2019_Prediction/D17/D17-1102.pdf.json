{"title": [{"text": "Video Highlight Prediction Using Audience Chat Reactions", "labels": [], "entities": [{"text": "Video Highlight Prediction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6030418972174326}]}], "abstractContent": [{"text": "Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis.", "labels": [], "entities": []}, {"text": "We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese.", "labels": [], "entities": [{"text": "automatic video highlight prediction", "start_pos": 45, "end_pos": 81, "type": "TASK", "confidence": 0.6370735615491867}]}, {"text": "We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitch.tv channels (will be released for further research), and demonstrate strong results on these using multi-modal, character-level CNN-RNN model architectures.", "labels": [], "entities": []}], "introductionContent": [{"text": "On-line eSports events provide anew setting for observing large-scale social interaction focused on a visual story that evolves over time-a video game.", "labels": [], "entities": []}, {"text": "While watching sporting competitions has been a major source of entertainment for millennia, and is a significant part of today's culture, eSports brings this to anew level on several fronts.", "labels": [], "entities": []}, {"text": "One is the global reach, the same games are played around the world and across cultures by speakers of several languages.", "labels": [], "entities": []}, {"text": "Another is the scale of on-line text-based discourse during matches that is public and amendable to analysis.", "labels": [], "entities": []}, {"text": "One of the most popular games, League of Legends, drew 43 million views for the 2016 world series final matches (broadcast in 18 languages) and a peak concurrent viewership of 14.7 million . Finally, players interact through what they see onscreen while fans (and researchers) can see exactly the same views.", "labels": [], "entities": []}, {"text": "This paper builds on the wealth of interaction around eSports to develop predictive models for match video highlights based on the audience's online chat discourse as well as the visual recordings of matches themselves.", "labels": [], "entities": []}, {"text": "ESports journalists and fans create highlight videos of important moments in matches.", "labels": [], "entities": []}, {"text": "Using these as ground truth, we explore automatic prediction of highlights via multimodal CNN+RNN models for multiple languages.", "labels": [], "entities": [{"text": "automatic prediction of highlights", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7015362232923508}]}, {"text": "Appealingly this task is natural, as the community already produces the ground truth and is global, allowing multilingual multimodal grounding.", "labels": [], "entities": []}, {"text": "Highlight prediction is about capturing the exciting moments in a specific video (a game match in this case), and depends on the context, the state of play, and the players.", "labels": [], "entities": [{"text": "Highlight prediction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8095904886722565}]}, {"text": "This task of predicting the exciting moments is hence different from summarizing the entire match into a story summary.", "labels": [], "entities": []}, {"text": "Hence, highlight prediction can benefit from the available real-time text commentary from fans, which is valuable in exposing more abstract background context, that may not be accessible with computer vision techniques that can easily identify some aspects of the state of play.", "labels": [], "entities": [{"text": "highlight prediction", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8599428832530975}]}, {"text": "As an example, computer vision may not understand why Michael Jordan's dunk is a highlight over that of another player, but concurrent fan commentary might reveal this.", "labels": [], "entities": []}, {"text": "We collect our dataset from Twitch.tv, one of the live-streaming platforms that integrates comments (see), and the largest live-streaming platform for video games.", "labels": [], "entities": []}, {"text": "We record matches of the game League of Legends (LOL), one of the largest eSports game in two subsets, 1) the spring season of the North American League of Legends Championship Series (NALCS), and 2) the League of Legends Master Series (LMS) hosted in Taiwan/Macau/HongKong, with chat comments in English and traditional Chinese respectively.", "labels": [], "entities": []}, {"text": "We use the community created highlights to label each frame of a match as highlight or not.", "labels": [], "entities": []}, {"text": "In addition to our new dataset, we present several experiments with multilingual characterbased models, deep-learning based vision models either per-frame or tied together with a videosequence LSTM-RNN, and combinations of language and vision models.", "labels": [], "entities": []}, {"text": "Our results indicate that while surprisingly the visual models generally outperform language-based models, we can still build reasonably useful language models that help disambiguate difficult cases for vision models, and that combining the two sources is the most effective model (across multiple languages).", "labels": [], "entities": []}], "datasetContent": [{"text": "Training Details In development and ablation studies, we use train and val splits of the data from NALCS to evaluate models in Section 3.", "labels": [], "entities": [{"text": "val splits", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9551926553249359}, {"text": "NALCS", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.9219430088996887}]}, {"text": "For the final results, models are retrained on the combination of train and val data (following major vision benchmarks e.g. PASCAL-VOC and COCO), and performance is measured on the test set.", "labels": [], "entities": [{"text": "val", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9309103488922119}]}, {"text": "We separate the highlight prediction to three different tasks based on using different input data: videos, chats, and videos+chats.", "labels": [], "entities": [{"text": "highlight prediction", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7467544078826904}]}, {"text": "The details of dataset split are in Section 3.", "labels": [], "entities": [{"text": "dataset split", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.6563855707645416}]}, {"text": "Our code is implemented in PyTorch.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9339739084243774}]}, {"text": "To deal with the large number of frames total, we sample only 5k positive and 5k negative examples in each epoch.", "labels": [], "entities": []}, {"text": "We use batch size of 32 and run 60 epochs in all experiments.", "labels": [], "entities": []}, {"text": "Weight decay is 10 \u22124 and learning rate is set as 10 \u22122 in the first 20 epochs and 10 \u22123 after that.", "labels": [], "entities": [{"text": "Weight decay", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9182857573032379}, {"text": "learning rate", "start_pos": 26, "end_pos": 39, "type": "METRIC", "confidence": 0.9682190716266632}]}, {"text": "Cross entropy loss is used.", "labels": [], "entities": []}, {"text": "Highlights are generated by fans and consist of clips.", "labels": [], "entities": []}, {"text": "We match each clip to when it happened in the full match and call this the highlight clip (non-overlapping).", "labels": [], "entities": []}, {"text": "The action of interest (kill, objective control, etc.) often happens in the later part of a highlight clip, while the clip contains some additional context before that action that may help set the stage.", "labels": [], "entities": []}, {"text": "For some of our experimental settings, we used a heuristic of only including the last 25% frames in every highlight clip as positive training examples.", "labels": [], "entities": []}, {"text": "During evaluation, we used all frames in the highlight clip.", "labels": [], "entities": []}, {"text": "shows the performance of each module separately on the dev set.", "labels": [], "entities": []}, {"text": "For the basic L-Char-LSTM and V-CNN models, using only the last 25% of frames in highlight clips in training works best.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of L-Char-LSTM model, we also train a Word-LSTM model by tokenizing all the chats and  only considering the words that appeared more than 10 times, which results in 10019 words.", "labels": [], "entities": []}, {"text": "We use this vocabulary to encode the words to 1-hot vectors.", "labels": [], "entities": []}, {"text": "The L-Char-LSTM outperforms L-Word-LSTM by 22.3%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 lists the numbers of videos in train,  validation, and test subsets.", "labels": [], "entities": []}, {"text": " Table 2: Ablation Study: Effects of various mod- els. C:Chat, V:Video, UF: % of frames Used in  highlight clips as positive training examples; P:  Precision, R: Recall, F: F-score.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9969882369041443}, {"text": "Recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9437094926834106}, {"text": "F-score", "start_pos": 173, "end_pos": 180, "type": "METRIC", "confidence": 0.8564190864562988}]}, {"text": " Table 3: Test Results on the NALCS (English) and  LMS (Traditional Chinese) datasets.", "labels": [], "entities": [{"text": "NALCS (English) and  LMS (Traditional Chinese) datasets", "start_pos": 30, "end_pos": 85, "type": "DATASET", "confidence": 0.8603007251566107}]}]}