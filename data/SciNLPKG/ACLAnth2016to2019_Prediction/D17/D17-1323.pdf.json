{"title": [{"text": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints", "labels": [], "entities": [{"text": "Reducing Gender Bias Amplification", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.718788392841816}]}], "abstractContent": [{"text": "Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web.", "labels": [], "entities": [{"text": "define rich visual recognition", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6265145167708397}]}, {"text": "Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora.", "labels": [], "entities": [{"text": "Structured prediction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7499789893627167}]}, {"text": "In this work, we study data and models associated with multilabel object classification and visual semantic role labeling.", "labels": [], "entities": [{"text": "multilabel object classification", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6140849490960439}, {"text": "visual semantic role labeling", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.6386500224471092}]}, {"text": "We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias.", "labels": [], "entities": []}, {"text": "For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% attest time.", "labels": [], "entities": []}, {"text": "We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference.", "labels": [], "entities": []}, {"text": "Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.", "labels": [], "entities": [{"text": "multilabel classification", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.7685660123825073}, {"text": "visual semantic role labeling", "start_pos": 188, "end_pos": 217, "type": "TASK", "confidence": 0.5896830782294273}]}], "introductionContent": [{"text": "Visual recognition tasks involving language, such as captioning (, visual question answering (, and visual semantic role labeling (, have emerged as avenues for expanding the diversity of information that can be recovered from images.", "labels": [], "entities": [{"text": "Visual recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7714971303939819}, {"text": "visual question answering", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.644785076379776}, {"text": "visual semantic role labeling", "start_pos": 100, "end_pos": 129, "type": "TASK", "confidence": 0.6481992602348328}]}, {"text": "These tasks aim at extracting rich semantics from images and require large quantities of labeled data, predominantly retrieved from the web.", "labels": [], "entities": []}, {"text": "Methods often combine structured prediction and deep learning to model correlations between labels and images to make judgments that otherwise would have weak visual support.", "labels": [], "entities": []}, {"text": "For example, in the first image of, it is possible to predict a spatula by considering that it is a common tool used for the activity cooking.", "labels": [], "entities": []}, {"text": "Yet such methods run the risk of discovering and exploiting societal biases present in the underlying web corpora.", "labels": [], "entities": []}, {"text": "Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models can have the inadvertent effect of magnifying stereotypes.", "labels": [], "entities": []}, {"text": "In this paper, we develop a general framework for quantifying bias and study two concrete tasks, visual semantic role labeling (vSRL) and multilabel object classification (MLC).", "labels": [], "entities": [{"text": "visual semantic role labeling (vSRL)", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.7154576437813895}, {"text": "multilabel object classification (MLC)", "start_pos": 138, "end_pos": 176, "type": "TASK", "confidence": 0.7883669833342234}]}, {"text": "In vSRL, we use the imSitu formalism (, where the goal is to predict activities, objects and the roles those objects play within an activity.", "labels": [], "entities": []}, {"text": "For MLC, we use MS-COCO (, a recognition task covering 80 object classes.", "labels": [], "entities": [{"text": "MLC", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9628309607505798}]}, {"text": "We use gender bias as a running example and show that both supporting datasets for these tasks are biased with respect to a gender binary . Our analysis reveals that over 45% and 37% of verbs and objects, respectively, exhibit bias toward a gender greater than 2:1.", "labels": [], "entities": []}, {"text": "For example, as seen in, the cooking activity in imSitu is a heavily biased verb.", "labels": [], "entities": [{"text": "imSitu", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9117875695228577}]}, {"text": "Furthermore, we show that after training state-of-the-art structured predictors, models amplify the existing bias, by 5.0% for vSRL, and 3.6% in MLC.: Five example images from the imSitu visual semantic role labeling (vSRL) dataset.", "labels": [], "entities": [{"text": "imSitu visual semantic role labeling (vSRL) dataset", "start_pos": 180, "end_pos": 231, "type": "DATASET", "confidence": 0.7865234878328111}]}, {"text": "Each image is paired with a table describing a situation: the verb, cooking, its semantic roles, i.e agent, and noun values filling that role, i.e. woman.", "labels": [], "entities": []}, {"text": "In the imSitu training set, 33% of cooking images have man in the agent role while the rest have woman.", "labels": [], "entities": [{"text": "imSitu training set", "start_pos": 7, "end_pos": 26, "type": "DATASET", "confidence": 0.9572469790776571}]}, {"text": "After training a Conditional Random Field (CRF), bias is amplified: man fills 16% of agent roles in cooking images.", "labels": [], "entities": []}, {"text": "To reduce this bias amplification our calibration method adjusts weights of CRF potentials associated with biased predictions.", "labels": [], "entities": []}, {"text": "After applying our methods, man appears in the agent role of 20% of cooking images, reducing the bias amplification by 25%, while keeping the CRF vSRL performance unchanged.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide details about the two visual recognition tasks we evaluated for bias: visual semantic role labeling (vSRL), and multi-label classification (MLC).", "labels": [], "entities": [{"text": "visual semantic role labeling (vSRL)", "start_pos": 98, "end_pos": 134, "type": "TASK", "confidence": 0.6721381204468864}, {"text": "multi-label classification (MLC)", "start_pos": 140, "end_pos": 172, "type": "TASK", "confidence": 0.8027463793754578}]}, {"text": "We focus on gender, defining G = {man, woman} and focus on the agent role in vSRL, and any occurrence in text associated with the images in MLC.", "labels": [], "entities": [{"text": "vSRL", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8929283022880554}]}, {"text": "Problem statistics are summarized in.", "labels": [], "entities": []}, {"text": "We also provide setup details for our calibration method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of violated constraints, mean  amplified bias, and test performance before and af- ter calibration using RBA. The test performances  of vSRL and MLC are measured by top-1 seman- tic role accuracy and top-1 mean average preci- sion, respectively.", "labels": [], "entities": []}]}