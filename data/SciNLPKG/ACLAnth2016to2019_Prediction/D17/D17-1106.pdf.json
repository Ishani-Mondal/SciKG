{"title": [{"text": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose to directly map raw visual observations and text input to actions for instruction execution.", "labels": [], "entities": []}, {"text": "While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input.", "labels": [], "entities": []}, {"text": "We use reinforcement learning in a contextual bandit setting to train a neural network agent.", "labels": [], "entities": []}, {"text": "To guide the agent's exploration, we use reward shaping with different forms of supervision.", "labels": [], "entities": []}, {"text": "Our approach does not require intermediate representations, planning procedures, or training different models.", "labels": [], "entities": []}, {"text": "We evaluate in a simulated environment , and show significant improvements over supervised learning and common reinforcement learning variants.", "labels": [], "entities": []}], "introductionContent": [{"text": "An agent executing natural language instructions requires robust understanding of language and its environment.", "labels": [], "entities": []}, {"text": "Existing approaches addressing this problem assume structured environment representations (e.g.,., or combine separately trained models (e.g.,), including for language understanding and visual reasoning.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.7429735958576202}]}, {"text": "We propose to directly map text and raw image input to actions with a single learned model.", "labels": [], "entities": []}, {"text": "This approach offers multiple benefits, such as not requiring intermediate representations, planning procedures, or training multiple models.", "labels": [], "entities": []}, {"text": "illustrates the problem in the Blocks environment).", "labels": [], "entities": []}, {"text": "The agent observes the environment as an RGB image using a camera sensor.", "labels": [], "entities": []}, {"text": "Given the RGB input, the agent", "labels": [], "entities": []}], "datasetContent": [{"text": "Environment We use the environment of.", "labels": [], "entities": []}, {"text": "The original task required predicting the source and target positions fora single block given an instruction.", "labels": [], "entities": []}, {"text": "In contrast, we address the task of moving blocks on the plane to execute instructions given visual input.", "labels": [], "entities": []}, {"text": "This requires generating the complete sequence of actions needed to complete the instruction.", "labels": [], "entities": []}, {"text": "The environment contains up to 20 blocks marked with logos or digits.", "labels": [], "entities": []}, {"text": "Each block can be moved in four directions.", "labels": [], "entities": []}, {"text": "Including the STOP action, in each step, the agent selects between 81 actions.", "labels": [], "entities": [{"text": "STOP", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.486375629901886}]}, {"text": "The set of actions is constant and is not limited to the blocks present.", "labels": [], "entities": []}, {"text": "The transition function is deterministic.", "labels": [], "entities": []}, {"text": "The size of each block step is 0.04 of the board size.", "labels": [], "entities": []}, {"text": "The agent observes the board from above.", "labels": [], "entities": []}, {"text": "We adopt a relatively challenging setup with a large action space.", "labels": [], "entities": []}, {"text": "While a simpler setup, for example decomposing the problem to source and target prediction and using a planner, is likely to perform better, we aim to minimize task-specific assumptions and engineering of separate modules.", "labels": [], "entities": []}, {"text": "However, to better understand the problem, we also report results for the decomposed task with a planner.", "labels": [], "entities": []}, {"text": "Data collected a corpus of instructions paired with start and goal states.", "labels": [], "entities": []}, {"text": "The original data includes instructions for moving one block or multiple blocks.", "labels": [], "entities": []}, {"text": "Single-block instructions are relatively similar to navigation instructions and referring expressions.", "labels": [], "entities": []}, {"text": "While they present much of the complexity of natural language understanding and grounding, they rarely display the planning complexity of multi-block instructions, which are beyond the scope of this paper.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.6490727166334788}]}, {"text": "Furthermore, the original data does not include demonstrations.", "labels": [], "entities": []}, {"text": "While generating demonstrations for moving a single block is straightforward, disambiguating action ordering when multiple blocks are moved is challenging.", "labels": [], "entities": []}, {"text": "Therefore, we focus on instructions where a single block changes its position between the start and goal states, and restrict demonstration generation to move the changed block.", "labels": [], "entities": []}, {"text": "The remaining data, and the complexity it introduces, provide an important direction for future work.", "labels": [], "entities": []}, {"text": "To create demonstrations, we compute the shortest paths.", "labels": [], "entities": []}, {"text": "While this process may introduce noise for instructions that specify specific trajectories (e.g., move SRI two steps north and . .", "labels": [], "entities": []}, {"text": ") rather than only describing the goal state, analysis of the data shows this issue is limited.", "labels": [], "entities": []}, {"text": "Out of 100 sampled instructions, 92 describe the goal state rather than the trajectory.", "labels": [], "entities": []}, {"text": "A secondary source of noise is due to discretization of the state space.", "labels": [], "entities": []}, {"text": "As a result, the agent often cannot reach the exact target position.", "labels": [], "entities": []}, {"text": "The demonstrations error illustrates this problem.", "labels": [], "entities": []}, {"text": "To provide task completion reward during learning, we relax the state comparison, and consider states to be equal if the sum of block distances is under the size of one block.", "labels": [], "entities": []}, {"text": "We also report results using ensembles of the three models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the block environment we  use and the SAIL navigation domain.", "labels": [], "entities": [{"text": "SAIL navigation domain", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.6816196441650391}]}, {"text": " Table 2: Mean and median (Med.) development results.", "labels": [], "entities": [{"text": "Mean and median (Med.) development", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.6659273930958339}]}, {"text": " Table 3: Mean and median (Med.) test results.", "labels": [], "entities": [{"text": "Mean and median (", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7916168421506882}, {"text": "Med.) test", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.5683936576048533}]}]}