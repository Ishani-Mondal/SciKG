{"title": [], "abstractContent": [{"text": "We introduce the first end-to-end corefer-ence resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector.", "labels": [], "entities": [{"text": "corefer-ence resolution", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.8084538578987122}]}, {"text": "The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each.", "labels": [], "entities": []}, {"text": "The model computes span em-beddings that combine context-dependent boundary representations with a head-finding attention mechanism.", "labels": [], "entities": []}, {"text": "It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions.", "labels": [], "entities": []}, {"text": "Experiments demonstrate state-of-the-art performance , with again of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.", "labels": [], "entities": [{"text": "F1", "start_pos": 73, "end_pos": 75, "type": "METRIC", "confidence": 0.9961544871330261}, {"text": "OntoNotes benchmark", "start_pos": 83, "end_pos": 102, "type": "DATASET", "confidence": 0.7432296872138977}, {"text": "F1", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9872833490371704}]}], "introductionContent": [{"text": "We present the first state-of-the-art coreference resolution model that is learned end-to-end given only gold mention clusters.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.9540611505508423}]}, {"text": "All recent coreference models, including neural approaches that achieved impressive performance gains (, rely on syntactic parsers, both for head-word features and as the input to carefully hand-engineered mention proposal algorithms.", "labels": [], "entities": []}, {"text": "We demonstrate for the first time that these resources are not required, and in fact performance can be improved significantly without them, by training an end-to-end neural model that jointly learns which spans are entity mentions and how to best cluster them.", "labels": [], "entities": []}, {"text": "Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters.", "labels": [], "entities": []}, {"text": "It includes a span-ranking model that decides, for each span, which of the previous spans (if any) is a good antecedent.", "labels": [], "entities": []}, {"text": "At the core of our model are vector embeddings representing spans of text in the document, which combine context-dependent boundary representations with a head-finding attention mechanism over the span.", "labels": [], "entities": []}, {"text": "The attention component is inspired by parser-derived head-word matching features from previous systems, but is less susceptible to cascading errors.", "labels": [], "entities": []}, {"text": "In our analyses, we show empirically that these learned attention weights correlate strongly with traditional headedness definitions.", "labels": [], "entities": []}, {"text": "Scoring all span pairs in our end-to-end model is impractical, since the complexity would be quartic in the document length.", "labels": [], "entities": []}, {"text": "Therefore we factor the model over unary mention scores and pairwise antecedent scores, both of which are simple functions of the learned span embedding.", "labels": [], "entities": []}, {"text": "The unary mention scores are used to prune the space of spans and antecedents, to aggressively reduce the number of pairwise computations.", "labels": [], "entities": []}, {"text": "Our final approach outperforms existing models by 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble.", "labels": [], "entities": [{"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9973912239074707}, {"text": "OntoNotes benchmark", "start_pos": 64, "end_pos": 83, "type": "DATASET", "confidence": 0.8261244297027588}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9918562173843384}]}, {"text": "It is not only accurate, but also relatively interpretable.", "labels": [], "entities": [{"text": "accurate", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.961377739906311}]}, {"text": "The model factors, for example, directly indicate whether an absent coreference link is due to low mention scores (for either span) or a low score from the mention ranking component.", "labels": [], "entities": []}, {"text": "The head-finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions.", "labels": [], "entities": []}, {"text": "We leverage this overall interpretability to do detailed quantitative and qualitative analyses, providing insights into the strengths and weaknesses of the approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the English coreference resolution data from the CoNLL-2012 shared task () in our experiments.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8117167949676514}, {"text": "CoNLL-2012 shared task", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.8352899750073751}]}, {"text": "This dataset contains 2802 training documents, 343 development documents, and 348 test documents.", "labels": [], "entities": []}, {"text": "The training documents contain on average 454 words and a maximum of 4009 words.", "labels": [], "entities": []}, {"text": "Learning We use ADAM () for learning with a minibatch size of 1.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8879518508911133}]}, {"text": "The LSTM weights are initialized with random orthonormal matrices as described in.", "labels": [], "entities": []}, {"text": "We apply 0.5 dropout to the word embeddings and character CNN outputs.", "labels": [], "entities": []}, {"text": "We apply 0.2 dropout to all hidden layers and feature embeddings.", "labels": [], "entities": []}, {"text": "Dropout masks are shared across timesteps to preserve long-distance information as described in.", "labels": [], "entities": []}, {"text": "The learning rate is decayed by 0.1% every 100 steps.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9538578391075134}]}, {"text": "The model is trained for up to 150 epochs, with early stopping based on the development set.", "labels": [], "entities": []}, {"text": "All code is implemented in TensorFlow ( and is publicly available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test set on the English data from the CoNLL-2012 shared task. The final column  (Avg. F1) is the main evaluation metric, computed by averaging the F1 of MUC, B 3 , and CEAF \u03c6 4 . We  improve state-of-the-art performance by 1.5 F1 for the single model and by 3.1 F1.", "labels": [], "entities": [{"text": "English data from the CoNLL-2012 shared task", "start_pos": 41, "end_pos": 85, "type": "DATASET", "confidence": 0.8000739642551967}, {"text": "Avg. F1)", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9568188190460205}, {"text": "F1", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9977490305900574}, {"text": "MUC", "start_pos": 178, "end_pos": 181, "type": "DATASET", "confidence": 0.7020366787910461}, {"text": "CEAF \u03c6", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.6594291031360626}, {"text": "F1", "start_pos": 252, "end_pos": 254, "type": "METRIC", "confidence": 0.9981622099876404}, {"text": "F1", "start_pos": 287, "end_pos": 289, "type": "METRIC", "confidence": 0.9976578950881958}]}]}