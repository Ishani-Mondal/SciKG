{"title": [{"text": "NITE: A Neural Inductive Teaching Framework for Domain-Specific NER", "labels": [], "entities": [{"text": "NITE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8626298904418945}, {"text": "NER", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.46949252486228943}]}], "abstractContent": [{"text": "In domain-specific NER, due to insufficient labeled training data, deep models usually fail to behave normally.", "labels": [], "entities": [{"text": "NER", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8272621035575867}]}, {"text": "In this paper , we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific NER models into an arbitrary deep neu-ral network in a teacher-student training manner.", "labels": [], "entities": []}, {"text": "NITE is a general framework that builds upon transfer learning and multiple instance learning, which collaboratively not only transfers knowledge to a deep student network but also reduces the noise from teachers.", "labels": [], "entities": [{"text": "NITE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9359403848648071}]}, {"text": "NITE can help deep learning methods to effectively utilize existing resources (i.e., models, labeled and unla-beled data) in a small domain.", "labels": [], "entities": [{"text": "NITE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7690267562866211}]}, {"text": "The experiment resulted on Disease NER proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neu-ral network nearly over 30% in terms of F1-score.", "labels": [], "entities": [{"text": "Disease NER", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.5220323204994202}, {"text": "F1-score", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9985398054122925}]}], "introductionContent": [{"text": "Domain-specific Named Entity Recognition (DNER), which aims to identify domain specific entity mentions and their categories, plays an important role in domain document classification, retrieval and content analysis.", "labels": [], "entities": [{"text": "Domain-specific Named Entity Recognition (DNER)", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7512949662549155}, {"text": "domain document classification", "start_pos": 153, "end_pos": 183, "type": "TASK", "confidence": 0.6381519734859467}, {"text": "content analysis", "start_pos": 199, "end_pos": 215, "type": "TASK", "confidence": 0.6893192678689957}]}, {"text": "It is also a foundation for further level of complex information extraction tasks, serves as cornerstone in the knowledge computing process of transforming data into machine readable knowledge ( . Domain-specific NER is a challenging problem.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7553271353244781}]}, {"text": "For example, in biomedical domain, the number of unseen biomedical entity mentions (such as disease names, chemical names), their abbreviations or acronyms, as well as multiple names of the same entity is growing fast with the rapid increase of biomedical literatures and clinical records.", "labels": [], "entities": []}, {"text": "However, the performance of a learning based NER system relies heavily on data annotation, which is quite expensive.", "labels": [], "entities": []}, {"text": "The situation is even worse in domain-specific NER systems, since their data annotation requires the engage of domain experts.", "labels": [], "entities": []}, {"text": "Therefore, in many special domains, only trained models or APIs are available, while their training data are private and inaccessible.", "labels": [], "entities": []}, {"text": "On the other hand, due to insufficient labeled training data, deep models usually fail to behave normally in such domain, and state-of-the-art methods in these domains are usually dominated by rule based deductive methods or shallow model with hand-crafted features.", "labels": [], "entities": []}, {"text": "However, the way of pre-defining useful domain specific hand-crafted features or rules are usually unavailable to the public.", "labels": [], "entities": []}, {"text": "In this paper, we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing models into an arbitrary deep neural network.", "labels": [], "entities": []}, {"text": "The idea of NITE is mainly borrowed from Transfer learning where previously learned knowledge can aid current situation and solve problems with better solutions.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8832331001758575}]}, {"text": "In NITE, existing NER models behave like inefficient teachers to teach a deep neural network (we called student network) to identify named entities by giving it concrete examples.", "labels": [], "entities": []}, {"text": "The knowledge transferred from these models is their posterior distributions on unlabeled data.", "labels": [], "entities": []}, {"text": "These teachers are inefficient because they transfer not only useful information, but also errors to the student.", "labels": [], "entities": []}, {"text": "The inputs of student network can be twofold, one is a small proportion from human labeled ground truth data (optional, like text book), and another is a large proportion from teachers, which is always noisy and less trustable.", "labels": [], "entities": []}, {"text": "In such case, a student is overwhelmed and often inferior to the teachers, therefore in NITE, we introduced Multiple Instance Learning (MIL) trick ( to reduce the input noise during the model training.", "labels": [], "entities": [{"text": "NITE", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8611510992050171}]}, {"text": "In summary, NITE is a general framework that can help deep learning methods to make the best use of existing resources (i.e., models, labeled and unlabeled data).", "labels": [], "entities": []}, {"text": "The experiment results on Disease NER (DNER) proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neural network, which trained on NCBI training dataset nearly over 30% in terms of F1-score.", "labels": [], "entities": [{"text": "Disease NER (DNER", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6491885632276535}, {"text": "NCBI training dataset", "start_pos": 203, "end_pos": 224, "type": "DATASET", "confidence": 0.8304877281188965}, {"text": "F1-score", "start_pos": 253, "end_pos": 261, "type": "METRIC", "confidence": 0.9985792636871338}]}, {"text": "It also outperformed the teacher model, which proved the correctness of our hypothesis.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we designed several experiments to testify our hypothesis of inductive teaching as well as evaluate our NITE framework.", "labels": [], "entities": [{"text": "NITE framework", "start_pos": 120, "end_pos": 134, "type": "DATASET", "confidence": 0.8102494776248932}]}, {"text": "The experiment's setup is as follows: Our NITE-DNER is trained without any labeled data, we randomly sampled 2,000 unlabeled abstracts of biomedical literature from PubMed as our training data.", "labels": [], "entities": []}, {"text": "The DNorm model is served as the teacher model in the NITE framework.", "labels": [], "entities": [{"text": "NITE framework", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.9285255074501038}]}, {"text": "In student network, we initialized character embeddings with uniform samples from In training procedure we set initial learning rate \u03b7 0 = 0.015 with decay rate \u03c1 = 0.05, the learning rate is updated as where n is the number of epochs.", "labels": [], "entities": [{"text": "initial learning rate \u03b7 0", "start_pos": 111, "end_pos": 136, "type": "METRIC", "confidence": 0.8584883213043213}, {"text": "decay rate \u03c1", "start_pos": 150, "end_pos": 162, "type": "METRIC", "confidence": 0.8672428329785665}]}, {"text": "We use a fixed dropout rate 0.5 at CNN and both input and output vectors of bi-directional LSTM to mitigate overfitting.", "labels": [], "entities": [{"text": "CNN", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8410686254501343}]}, {"text": "For MIL we set the bag size K = 5 with mini-batch size 30.", "labels": [], "entities": [{"text": "MIL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8649560213088989}]}, {"text": "We implemented neural networks on a GeForce GTX 1080 using Theano.", "labels": [], "entities": [{"text": "Theano", "start_pos": 59, "end_pos": 65, "type": "DATASET", "confidence": 0.9201922416687012}]}], "tableCaptions": [{"text": " Table 1: The description of the NCBI corpus as  training, validating and testing sets for the recog- nition of disease named entity", "labels": [], "entities": [{"text": "NCBI corpus", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9012784361839294}, {"text": "recog- nition of disease named entity", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.8427582553454808}]}, {"text": " Table 2.  As shown in", "labels": [], "entities": []}]}