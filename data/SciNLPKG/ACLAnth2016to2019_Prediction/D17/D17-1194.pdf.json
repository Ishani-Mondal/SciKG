{"title": [{"text": "Temporal dynamics of semantic relations in word embeddings: an application to predicting armed conflict participants", "labels": [], "entities": [{"text": "Temporal dynamics of semantic relations", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8834400177001953}, {"text": "predicting armed conflict", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.8806715210278829}]}], "abstractContent": [{"text": "This paper deals with using word embedding models to trace the temporal dynamics of semantic relations between pairs of words.", "labels": [], "entities": []}, {"text": "The setup is similar to the well-known analogies task, but expanded with a time dimension.", "labels": [], "entities": []}, {"text": "To this end, we apply incremental updating of the models with new training texts, including in-cremental vocabulary expansion, coupled with learned transformation matrices that let us map between members of the relation.", "labels": [], "entities": [{"text": "vocabulary expansion", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7421148717403412}]}, {"text": "The proposed approach is evaluated on the task of predicting insurgent armed groups based on geographical locations.", "labels": [], "entities": [{"text": "predicting insurgent armed groups", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.8495797961950302}]}, {"text": "The gold standard data for the time span 1994-2010 is extracted from the UCDP Armed Conflicts dataset.", "labels": [], "entities": [{"text": "UCDP Armed Conflicts dataset", "start_pos": 73, "end_pos": 101, "type": "DATASET", "confidence": 0.9438654780387878}]}, {"text": "The results show that the method is feasible and outperforms the baselines, but also that important work still remains to be done.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To evaluate our approach on all the UCDP data, we again tested how good it is in predicting the future conflicts based on the projection matrices learned from the previous years.", "labels": [], "entities": [{"text": "UCDP data", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.9646212458610535}]}, {"text": "We did this for all the years between 1994 and 2010.", "labels": [], "entities": []}, {"text": "The evaluation metrics are the same as in the Section 3: we calculated the ratio of correctly predicted armed groups names from the conflict pairs, for which the UCDP datasets stated that these conflicts were active in this particular year.", "labels": [], "entities": [{"text": "UCDP datasets", "start_pos": 162, "end_pos": 175, "type": "DATASET", "confidence": 0.9853156507015228}]}, {"text": "As before, the models employed in the experiment were incrementally trained on each successive year with vocabulary expansion.", "labels": [], "entities": []}, {"text": "Words present in the gold standard but absent from the models under analysis were skipped.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9262892305850983}]}, {"text": "At the worst case, 25% of pairs were skipped from the test set; on average, 13% were skipped each year (but seethe note below about the incr. static baseline).", "labels": [], "entities": []}, {"text": "At test time, all the entities were lowercased.", "labels": [], "entities": []}, {"text": "We employ 3 baselines: 1) yearly models trained separately from scratch on the corpora containing texts from each year only (referred to as separate hereafter); 2) yearly models trained from scratch on all the texts from the particular year and the previous years (cumulative hereafter); 3) incrementally trained models without vocabulary expansion (incr.", "labels": [], "entities": [{"text": "vocabulary expansion", "start_pos": 328, "end_pos": 348, "type": "TASK", "confidence": 0.6735241413116455}]}, {"text": "Initially, the linear projections for all models were trained on all the conflict pairs from the past and present years, similar to Section 3.2 (dubbed up-to-now hereafter).", "labels": [], "entities": []}, {"text": "However, the information about conflicts having ended several years before might not be strongly expressed in the model after it was incrementally updated with the data from all the subsequent years.", "labels": [], "entities": []}, {"text": "For example, the 2005 model hardly contains much knowledge about the conflict relations between Mexico and the Popular Revolutionary Army (EPR) which stopped its activities after 1996.", "labels": [], "entities": []}, {"text": "Thus, we additionally conducted a similar experiment, but this time the projections were learned only on the salient pairs (dubbed previous): that is, the pairs active in the last year up to which the model was trained.", "labels": [], "entities": []}, {"text": "presents the results for these experiments, as well as baselines (averaged across 15 years).", "labels": [], "entities": []}, {"text": "dynamic approach, the performance of the previous projections is: Average accuracies of predicting next-year insurgents on the basis of locations, using projections trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9183861613273621}, {"text": "predicting next-year insurgents", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.8437116742134094}]}, {"text": "Results for 3 baselines are shown along with the proposed incremental dynamic approach.", "labels": [], "entities": []}, {"text": "comparable to that of the up-to-now projections on the accuracies @5 and @10, and is even higher on the accuracy @1 (statistically significant with t-test, p < 0.01).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9742274880409241}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.999295711517334}]}, {"text": "Thus, the single-year projections are somewhat more 'focused', while taking much less time to learn, because of less training pairs.", "labels": [], "entities": []}, {"text": "The fact that our models were incrementally updated, not trained from scratch, is crucial.", "labels": [], "entities": []}, {"text": "The results of the separate baseline look more like random jitter.", "labels": [], "entities": []}, {"text": "The cumulative baseline results are slightly better, probably simply because they are trained on more data.", "labels": [], "entities": []}, {"text": "However, they still perform much worse than the models trained using incremental updates.", "labels": [], "entities": []}, {"text": "This is because the former models are not connected to each other, and thus are initialized with a different layout of words in the vector space.", "labels": [], "entities": []}, {"text": "This gives rise to formally different directions of semantic relations in each yearly model (the relations themselves are still there, but they are rotated and scaled differently).", "labels": [], "entities": []}, {"text": "The results for the incr.", "labels": [], "entities": []}, {"text": "static baseline, when tested only on the words present in the test model vocabulary (the left part of the table), seem better than those of the proposed incr.", "labels": [], "entities": []}, {"text": "This stems from the fact that incremental updating with static vocabulary means that we never add new words to the models; thus, they contain only the vocabulary learned from the 1994 texts.", "labels": [], "entities": []}, {"text": "The result is that attest time we skip many more pairs than with the other approaches (about 62% in average).", "labels": [], "entities": []}, {"text": "Subsequently, the projections are tested only on a minor part of the test sets.", "labels": [], "entities": []}, {"text": "Of course, skipping large parts of the data would be a major drawback for any realistic application, so the incr.", "labels": [], "entities": []}, {"text": "static baseline is not really plausible.", "labels": [], "entities": []}, {"text": "For comparison, the right part of provides the accuracies for the setup in which all the pairs are evaluated (for pairs with OOV words the accuracy is always 0).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9990925788879395}]}, {"text": "Other tested approaches are not much affected by this change, but for incr.", "labels": [], "entities": []}, {"text": "static the performance drops drastically.", "labels": [], "entities": []}, {"text": "As a result, for the all pairs scenario, incremental updating with vocabulary expansion outperforms all the baselines (the differences are statistically significant with t-test, p < 0.01).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies for synchronic projections  from locations to armed groups, and vice versa", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9979043006896973}]}, {"text": " Table 3: Average accuracies of predicting next-year insurgents on the basis of locations, using projections  trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous).  Results for 3 baselines are shown along with the proposed incremental dynamic approach.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.8332617878913879}, {"text": "predicting next-year insurgents", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.8604392011960348}]}]}