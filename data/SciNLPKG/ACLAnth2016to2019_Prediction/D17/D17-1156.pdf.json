{"title": [{"text": "Regularization techniques for fine-tuning in neural machine translation", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9736452102661133}, {"text": "neural machine translation", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.6762291789054871}]}], "abstractContent": [{"text": "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7250603040059408}, {"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6599699556827545}]}, {"text": "In this scenario, overfitting is a major challenge.", "labels": [], "entities": []}, {"text": "We investigate a number of techniques to reduce overfitting and improve transfer learning, including regular-ization techniques such as dropout and L2-regularization towards an out-of-domain prior.", "labels": [], "entities": []}, {"text": "In addition, we introduce tuneout, a novel regularization technique inspired by dropout.", "labels": [], "entities": []}, {"text": "We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English\u2192German and English\u2192Russian.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.6212848822275797}, {"text": "IWSLT datasets", "start_pos": 110, "end_pos": 124, "type": "DATASET", "confidence": 0.8781059384346008}]}, {"text": "We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7178255468606949}, {"text": "BLEU score", "start_pos": 180, "end_pos": 190, "type": "METRIC", "confidence": 0.9763731360435486}]}], "introductionContent": [{"text": "Neural machine translation () has established itself as the new state of the art at recent shared translation tasks.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8076473474502563}, {"text": "shared translation tasks", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.7488234241803488}]}, {"text": "In order to achieve good generalization accuracy, neural machine translation, like most other large machine learning systems, requires large amounts of training examples sampled from a distribution as close as possible to the distribution of the inputs seen during execution.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9164761304855347}, {"text": "neural machine translation", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6682858765125275}]}, {"text": "However, in many applications, only a small amount of parallel text is available for the specific application domain, and it is therefore desirable to leverage larger out-domain datasets.", "labels": [], "entities": []}, {"text": "Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning: to continue training an existing model which was trained on out-of-domain data with indomain training data.", "labels": [], "entities": []}, {"text": "This strategy was also found to be very effective for neural machine translation ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6233392357826233}]}, {"text": "Since the amount of in-domain data is typically small, overfitting is a concern.", "labels": [], "entities": []}, {"text": "A common solution is early stopping on a small held-out in-domain validation dataset, but this reduces the amount of in-domain data available for training.", "labels": [], "entities": []}, {"text": "In this paper, we show that we can make finetuning strategies for neural machine translation more robust by using several regularization techniques.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.6611833075682322}]}, {"text": "We consider fine-tuning with varying amounts of in-domain training data, showing that improvements are logarithmic in the amount of indomain data.", "labels": [], "entities": []}, {"text": "We investigate techniques where domain adaptation starts from a pre-trained out-domain model, and only needs to process the in-domain corpus.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7482219636440277}]}, {"text": "Since we do not need to process the large out-domain corpus during adaptation, this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available.", "labels": [], "entities": []}, {"text": "Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features.", "labels": [], "entities": []}, {"text": "These techniques are largely orthogonal to ours 1 and can be used in combination.", "labels": [], "entities": []}, {"text": "In fact, successfully apply fine-tuning in combination with joint training.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate transfer learning on test sets from the IWSLT shared translation task ().", "labels": [], "entities": [{"text": "IWSLT shared translation task", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.7269879877567291}]}], "tableCaptions": [{"text": " Table 1: English-to-German translation BLEU scores  valid  test  System", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9957730174064636}]}, {"text": " Table 2: English-to-Russian translation BLEU scores  valid  test  System", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9948630928993225}]}]}