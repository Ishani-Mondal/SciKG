{"title": [{"text": "Cross-Lingual Word Representations: Induction and Evaluation EMNLP 2017 Tutorial", "labels": [], "entities": [{"text": "Cross-Lingual Word Representations", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5945067703723907}, {"text": "EMNLP 2017 Tutorial", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.8047481775283813}]}], "abstractContent": [{"text": "1 Motivation and Objectives In recent past, NLP as afield has seen tremendous utility of distributional word vector representations as features in downstream tasks.", "labels": [], "entities": []}, {"text": "The fact that these word vectors can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in NLP.", "labels": [], "entities": []}, {"text": "With the increasing use of monolingual word vectors, there is a need for word vectors that can be used as efficiently across multiple languages as monolingually.", "labels": [], "entities": []}, {"text": "Therefore , learning bilingual and multilingual word embeddings/vectors is currently an important research topic.", "labels": [], "entities": []}, {"text": "These vectors offer an elegant and language-pair independent way to represent content across different languages.", "labels": [], "entities": []}, {"text": "This tutorial aims to bring NLP researchers up to speed with the current techniques in cross-lingual word representation learning.", "labels": [], "entities": [{"text": "cross-lingual word representation learning", "start_pos": 87, "end_pos": 129, "type": "TASK", "confidence": 0.7170388475060463}]}, {"text": "We will first discuss how to induce cross-lingual word representations (covering both bilingual and multilingual ones) from various data types and resources (e.g., parallel data, comparable data, non-aligned monolingual data in different languages, dictionaries and thea-suri, or, even, images, eye-tracking data).", "labels": [], "entities": []}, {"text": "We will then discuss how to evaluate such representations , intrinsically and extrinsically.", "labels": [], "entities": []}, {"text": "We will introduce researchers to state-of-the-art methods for constructing cross-lingual word representations and discuss their applicability in abroad range of downstream NLP applications.", "labels": [], "entities": []}, {"text": "We will deliver a detailed survey of the current methods, discuss best training and evaluation practices and use-cases, and provide links to publicly available implementations, datasets, and pre-trained models.", "labels": [], "entities": []}, {"text": "2 Tutorial Overview 2.1 Introduction An overview of the cross-lingual NLP landscape, situating the current work on cross-lingual representation learning and motivating the need for multilingual training and cross-lingual transfer for resource-poor languages.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 207, "end_pos": 229, "type": "TASK", "confidence": 0.7175297290086746}]}, {"text": "A discussion on various types of bilingual resources available: e.g., dictionaries , parallel vs. comparable vs. non-aligned monolingual data.", "labels": [], "entities": []}, {"text": "2.2 Part I: Learning from Word Alignments and Dictionaries In the first part of the tutorial, after important preliminaries (i.e., standard learning techniques in monolingual settings which lend themselves to cross-lingual scenarios: dimensionality reduction , learning from context, etc.), we will present a typology of cross-lingual models roughly clustered according to the cross-lingual signal needed for training (e.g., translation pairs, document-aligned data, images), as well as their ability to exploit both multilingual and more abundant monolingual data in training.", "labels": [], "entities": [{"text": "Learning from Word Alignments and Dictionaries", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.7363172769546509}, {"text": "dimensionality reduction", "start_pos": 234, "end_pos": 258, "type": "TASK", "confidence": 0.7230226099491119}]}, {"text": "We will then zoom in the group of models which learn directly from available dictionaries and word alignment information, drawing comparisons with older baseline work on bil-ingual/multilingual clustering and traditional distri-butional cross-lingual spaces based on one-to-one translation lexicons, and analyzing their modeling assumptions and protocols.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7313995659351349}]}, {"text": "Throughout the tutorial (Part I-Part III), we will demonstrate how to extend the current cross-lingual representation models from bilingual to true multilingual settings (three or more languages), as such extensions are not straightforward for plenty of modeling frameworks.", "labels": [], "entities": []}], "introductionContent": [{"text": "An overview of the cross-lingual NLP landscape, situating the current work on cross-lingual representation learning and motivating the need for multilingual training and cross-lingual transfer for resource-poor languages.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.715404286980629}]}, {"text": "A discussion on various types of bilingual resources available: e.g., dictionaries, parallel vs. comparable vs. non-aligned monolingual data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the final part, we will focus on evaluation and application of cross-lingual word representations.", "labels": [], "entities": []}, {"text": "First, we will discuss the differences between intrinsic and extrinsic evaluations, and current evaluation protocols.", "labels": [], "entities": []}, {"text": "We will demonstrate how crosslingual representations may boost monolingual NLP tasks, and how such representations can support fundamental cross-lingual tasks such as word alignment induction, bilingual lexicon learning, or machine translation.", "labels": [], "entities": [{"text": "word alignment induction", "start_pos": 167, "end_pos": 191, "type": "TASK", "confidence": 0.8595712979634603}, {"text": "bilingual lexicon learning", "start_pos": 193, "end_pos": 219, "type": "TASK", "confidence": 0.6496259570121765}, {"text": "machine translation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.8017288744449615}]}, {"text": "Following that, we will demonstrate the importance of cross-lingual transfer in NLP, and discuss what kind of knowledge (semantic vs. syntactic information) can actually transfer across languages.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7333974540233612}]}, {"text": "We will then show how to use cross-lingual word vectors to accomplish such transfers fora (didactically chosen) selection of NLP downstream tasks.", "labels": [], "entities": []}], "tableCaptions": []}