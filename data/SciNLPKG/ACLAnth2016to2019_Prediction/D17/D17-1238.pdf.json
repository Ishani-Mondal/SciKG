{"title": [{"text": "Why We Need New Evaluation Metrics for NLG", "labels": [], "entities": [{"text": "NLG", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7246031165122986}]}], "abstractContent": [{"text": "The majority of NLG evaluation relies on automatic metrics, such as BLEU.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9107728004455566}, {"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.996862530708313}]}, {"text": "In this paper, we motivate the need for novel, system-and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG.", "labels": [], "entities": []}, {"text": "We also show that metric performance is data-and system-specific.", "labels": [], "entities": []}, {"text": "Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic evaluation measures, such as BLEU), are used with increasing frequency to evaluate Natural Language Generation (NLG) systems: Up to 60% of NLG research published between 2012-2015 relies on automatic metrics (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9984473586082458}, {"text": "Natural Language Generation (NLG)", "start_pos": 93, "end_pos": 126, "type": "TASK", "confidence": 0.814429759979248}]}, {"text": "Automatic evaluation is popular because it is cheaper and faster to run than human evaluation, and it is needed for automatic benchmarking and tuning of algorithms.", "labels": [], "entities": [{"text": "Automatic evaluation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8017737567424774}]}, {"text": "The use of such metrics is, however, only sensible if they are known to be sufficiently correlated with human preferences.", "labels": [], "entities": []}, {"text": "This is rarely the case, as shown by various studies in NLG, as well as in related fields, such as dialogue systems ( , machine translation (MT)), and image captioning.", "labels": [], "entities": [{"text": "machine translation (MT))", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.8459452629089356}, {"text": "image captioning", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.7899543046951294}]}, {"text": "This paper follows on from the above previous work and presents another evaluation study into automatic metrics with the aim to firmly establish the need for new metrics.", "labels": [], "entities": []}, {"text": "We consider this paper to be the most complete study to date, across metrics, systems, datasets and domains, focusing on recent advances in data-driven NLG.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we are the first to: \u2022 Target end-to-end data-driven NLG, where we compare 3 different approaches.", "labels": [], "entities": []}, {"text": "In contrast to NLG methods evaluated in previous work, our systems can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy data.", "labels": [], "entities": []}, {"text": "\u2022 Compare a large number of 21 automated metrics, including novel grammar-based ones.", "labels": [], "entities": []}, {"text": "\u2022 Report results on two different domains and three different datasets, which allows us to draw more general conclusions.", "labels": [], "entities": []}, {"text": "\u2022 Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level.", "labels": [], "entities": []}, {"text": "\u2022 Make all associated code and data publicly available, including detailed analysis results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider the following crowdsourced datasets, which target utterance generation for spoken dialogue systems.", "labels": [], "entities": [{"text": "utterance generation", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8139103055000305}]}, {"text": "shows the number of system outputs for each dataset.", "labels": [], "entities": []}, {"text": "Each data instance consists of one MR and one or more natural language references as produced by humans, such as the following example, taken from the BAGEL dataset: 5 2 https://github.com/shawnwun/RNNLG 3 https://github.com/UFAL-DSG/tgen 4 https://github.com/glampouras/JLOLS_ NLG 5 Note that we use lexicalised versions of SFHOTEL and SFREST and a partially lexicalised version of BAGEL, where proper names and place names are replaced by placeholders (\"X\"), in correspondence with the outputs generated by the MR: inform(name=X, area=X, pricerange=moderate, type=restaurant) Reference: \"X is a moderately priced restaurant in X.\"", "labels": [], "entities": [{"text": "BAGEL dataset", "start_pos": 151, "end_pos": 164, "type": "DATASET", "confidence": 0.9558934569358826}, {"text": "SFHOTEL", "start_pos": 325, "end_pos": 332, "type": "DATASET", "confidence": 0.9440075755119324}, {"text": "SFREST", "start_pos": 337, "end_pos": 343, "type": "DATASET", "confidence": 0.8027429580688477}, {"text": "BAGEL", "start_pos": 383, "end_pos": 388, "type": "DATASET", "confidence": 0.8126307725906372}]}, {"text": "\u2022 SFHOTEL & SFREST) provide information about hotels and restaurants in San Francisco.", "labels": [], "entities": [{"text": "SFHOTEL & SFREST", "start_pos": 2, "end_pos": 18, "type": "DATASET", "confidence": 0.6610487103462219}]}, {"text": "There are 8 system dialogue act types, such as inform, confirm, goodbye etc.", "labels": [], "entities": []}, {"text": "Each domain contains 12 attributes, where some are common to both domains, such as name, type, pricerange, address, area, etc., and the others are domain-specific, e.g. food and kids-allowed for restaurants; hasinternet and dogs-allowed for hotels.", "labels": [], "entities": []}, {"text": "For each domain, around 5K human references were collected with 2.3K unique human utterances for SFHOTEL and 1.6K for SFREST.", "labels": [], "entities": [{"text": "SFHOTEL", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9263812303543091}, {"text": "SFREST", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.9550965428352356}]}, {"text": "The number of unique system outputs produced is 1181 for SFREST and 875 for SFHOTEL.", "labels": [], "entities": [{"text": "SFREST", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8435129523277283}, {"text": "SFHOTEL", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9672073721885681}]}, {"text": "\u2022 BAGEL () provides information about restaurants in Cambridge.", "labels": [], "entities": [{"text": "BAGEL", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9546205997467041}, {"text": "Cambridge", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.9559087753295898}]}, {"text": "The dataset contains 202 aligned pairs of MRs and 2 corresponding references each.", "labels": [], "entities": []}, {"text": "The domain is a subset of SFREST, including only the inform act and 8 attributes.", "labels": [], "entities": [{"text": "SFREST", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7138960361480713}]}, {"text": "We observe that human informativeness ratings follow the same pattern as WBMs, while the average similarity score (SIM) seems to be related to human quality ratings.", "labels": [], "entities": [{"text": "average similarity score (SIM)", "start_pos": 89, "end_pos": 119, "type": "METRIC", "confidence": 0.8748747607072195}]}, {"text": "Looking at GBMs, we observe that they seem to be related to naturalness and quality ratings.", "labels": [], "entities": [{"text": "GBMs", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.7109277844429016}]}, {"text": "Less complex utterances, as measured by readability (RE) and word length (cpw), have higher naturalness ratings.", "labels": [], "entities": [{"text": "readability (RE)", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.6601077690720558}, {"text": "word length (cpw)", "start_pos": 61, "end_pos": 78, "type": "METRIC", "confidence": 0.8492901563644409}]}, {"text": "More complex utterances, as measured in terms of their length (len), number of words (wps), syllables (sps, spw) and polysyllables (pol, ppw), have lower quality evaluation.", "labels": [], "entities": [{"text": "length (len)", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.842183068394661}]}, {"text": "Utterances measured as more grammatical are on average evaluated higher in terms of naturalness.", "labels": [], "entities": []}, {"text": "These initial results suggest a relation between automatic metrics and human ratings at system level.", "labels": [], "entities": []}, {"text": "However, average scores can be misleading, as they do not identify worst-case scenarios.", "labels": [], "entities": []}, {"text": "This leads us to inspect the correlation of human and automatic metrics for each MR-system output pair at utterance level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of NLG system outputs from dif- ferent datasets and systems used in this study.", "labels": [], "entities": []}, {"text": " Table 5: Example pairs of MRs and system outputs from our data, contrasting the average of word- overlap metrics (normalised in the 1-6 range) and semantic similarity (SIM) with human ratings (median  of all measures).", "labels": [], "entities": [{"text": "word- overlap metrics", "start_pos": 92, "end_pos": 113, "type": "METRIC", "confidence": 0.6648462489247322}, {"text": "semantic similarity (SIM)", "start_pos": 148, "end_pos": 173, "type": "METRIC", "confidence": 0.7877479374408722}]}]}