{"title": [{"text": "Inter-Weighted Alignment Network for Sentence Pair Modeling", "labels": [], "entities": [{"text": "Inter-Weighted Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6723023951053619}, {"text": "Sentence Pair Modeling", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8575613498687744}]}], "abstractContent": [{"text": "Sentence pair modeling is a crucial problem in the field of natural language processing.", "labels": [], "entities": [{"text": "Sentence pair modeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9422457416852316}, {"text": "natural language processing", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6423468093077341}]}, {"text": "In this paper, we propose a model to measure the similarity of a sentence pair focusing on the interaction information.", "labels": [], "entities": []}, {"text": "We utilize the word level similarity matrix to discover fine-grained alignment of two sentences.", "labels": [], "entities": []}, {"text": "It should be emphasized that each word in a sentence has a different importance from the perspective of semantic composition, so we exploit two novel and efficient strategies to explicitly calculate a weight for each word.", "labels": [], "entities": []}, {"text": "Although the proposed model only use a sequential LSTM for sentence modeling without any external resource such as syntactic parser tree and additional lexicon features, experimental results show that our model achieves state-of-the-art performance on three datasets of two tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given two pieces of sentences Sand T , sentence pair modeling (SPM) is a fundamental task whose applications include question answering, natural language inference (, paraphrase identification () and sentence completion (  and soon.", "labels": [], "entities": [{"text": "sentence pair modeling (SPM)", "start_pos": 39, "end_pos": 67, "type": "TASK", "confidence": 0.7991302559773127}, {"text": "question answering", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.834993839263916}, {"text": "paraphrase identification", "start_pos": 167, "end_pos": 192, "type": "TASK", "confidence": 0.8390651345252991}, {"text": "sentence completion", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.7548033893108368}]}, {"text": "In general, each of the two sentences are firstly mapped to a representation, and then a model is designed to determine the relation between them.", "labels": [], "entities": []}, {"text": "Traditional methods use lexicon features such as Bag-of-Words(BOW) to map sentences.", "labels": [], "entities": []}, {"text": "As we know, features design and selection are time-consuming and high dimensional features may suffer from sparsity because of the variation of linguistic.", "labels": [], "entities": []}, {"text": "Recently, deep learning tech- * Corresponding author niques have been applied to develop end-to-end models for NLP tasks, such as sentence modeling), relation classification () and machine translation).", "labels": [], "entities": [{"text": "sentence modeling", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7799274623394012}, {"text": "relation classification", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.9120241403579712}, {"text": "machine translation", "start_pos": 181, "end_pos": 200, "type": "TASK", "confidence": 0.8011667430400848}]}, {"text": "These works show that deep learning models can be comparable with hand-crafted features based models and often outperform them.", "labels": [], "entities": []}, {"text": "Existing DNN models are based on pre-trained word embeddings which map each word to one low dimensional vector and compose word embeddings to represent sentence.", "labels": [], "entities": []}, {"text": "Some models are developed directly from the sentence models.", "labels": [], "entities": []}, {"text": "They obtain single vector representation for each sentence separately and then determine the relation based on two vectors ().", "labels": [], "entities": []}, {"text": "Because of the absence of interaction, these models cannot achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "Inspired by attention mechanism in computer vision and machine translation, some elaborate models have been proposed which take interaction information into consideration.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7282320559024811}]}, {"text": "Meanwhile, to grasp the fine-grained information for semantic similarity, some prior works (  firstly compute a word level similarity matrix according to word representation, and utilize multiple convolution layers and extract features from the similarity matrix in a perspective of image recognition.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 283, "end_pos": 300, "type": "TASK", "confidence": 0.7448626458644867}]}, {"text": "In this paper, we focus on solving SPM problem by measuring semantic similarity between two sentences.", "labels": [], "entities": [{"text": "SPM problem", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9296360611915588}]}, {"text": "We propose anew deep learning model based on two facts that previous works always neglected.", "labels": [], "entities": []}, {"text": "As we know, in the aspect of semantic, each word in the sentence is of different importance.", "labels": [], "entities": []}, {"text": "When calculating a sentence representation we should endow each word with a weight indicating its importance.", "labels": [], "entities": []}, {"text": "Taking following sentences as an example: A: a man with a red helmet is riding a motorbike along a roadway.", "labels": [], "entities": []}, {"text": "B: a man is riding a motorbike along a roadway.", "labels": [], "entities": []}, {"text": "C: a man with a red helmet is riding a bicycle along a roadway.", "labels": [], "entities": []}, {"text": "We can see that sentence A is more similar with sentence B than with sentence C while a conventional model probably makes an opposite conclusion because the phrase \"with a red helmet\" will bias the meaning of A to C meanwhile the difference between \"motorbike\" and \"bicycle\" is not large enough.", "labels": [], "entities": []}, {"text": "If the model can realize that the phrase \"with a red helmet\" has little effect on semantic composition, the mistake will be avoided.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.6908186972141266}]}, {"text": "Since we have to analyse a pair of sentences, the weights should be related to not only the sentence itself, but also its partner.", "labels": [], "entities": []}, {"text": "From this point, we propose a novel inter-weighted layer to measure the importance of each word.", "labels": [], "entities": []}, {"text": "On the other hand, the more similar two sentences are, the more probably we can align each word of sentence S with several words of sentence T , and vice versa.", "labels": [], "entities": []}, {"text": "On account of the variety of expression, the position and length of two aligned parts are very likely different, so we apply softalignment mechanism and build an effective alignment layer.", "labels": [], "entities": []}, {"text": "In summary, our contributions are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose an Inter-Weighted Alignment Network (IWAN) for SPM, which builds an alignment layer to compute similarity score according to the degree of alignment.", "labels": [], "entities": [{"text": "SPM", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9741494059562683}]}, {"text": "2. Considering the importance of each word in a sentence is different, we argue that an interweighted layer for evaluating the weight of each word is crucial to semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 161, "end_pos": 181, "type": "TASK", "confidence": 0.7170455008745193}]}, {"text": "We propose two strategies for calculating weights.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate their effectiveness.", "labels": [], "entities": []}, {"text": "3. Experimental results on semantic relatedness benchmark dataset SICK and two answer selection datasets show that proposed model achieves state-of-the-art performances without any external information.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed model, we conduct experiments on two tasks: semantic relatedness and answer selection.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.8657458126544952}]}, {"text": "For semantic relatedness task, we use the Sentences Involving Compositional Knowledge (SICK) dataset), which consists of 9927 sentence pairs in a 4500/500/4927 train/dev/test split.", "labels": [], "entities": []}, {"text": "The sentences are derived from existing image and video description and each sentence pair has a relatedness score y \u2208, where the larger score indicates more similarity between two sentences.", "labels": [], "entities": []}, {"text": "As the goal of this task is to calculate sentence pair similarity, we can directly evaluate our model on SICK.", "labels": [], "entities": []}, {"text": "Following previous works, we use Pearson's Correlation r, Spearman's Correlation \u03c1 and mean square error (MSE) as evaluation metrics.", "labels": [], "entities": [{"text": "Pearson's Correlation r", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.6093997359275818}, {"text": "Spearman's Correlation \u03c1", "start_pos": 58, "end_pos": 82, "type": "METRIC", "confidence": 0.6072104573249817}, {"text": "mean square error (MSE)", "start_pos": 87, "end_pos": 110, "type": "METRIC", "confidence": 0.9376809398333231}]}, {"text": "For answer selection task, we experiment on two datasets: TrecQA and WikiQA.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9228690266609192}, {"text": "TrecQA", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.5979358553886414}, {"text": "WikiQA", "start_pos": 69, "end_pos": 75, "type": "DATASET", "confidence": 0.9586613774299622}]}, {"text": "The TrecQA dataset () from the Text Retrieval Conferences has been widely used for the answer selection task during the past decade.", "labels": [], "entities": [{"text": "TrecQA dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8223236799240112}, {"text": "Text Retrieval Conferences", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6983103354771932}, {"text": "answer selection task", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.9315166672070821}]}, {"text": "The original TrecQA train dataset consists of 1,229 questions with 53,417 question-answer pairs, 82 questions with 1,148 pairs in development set, and 100 questions with 1,517 pairs in test set.", "labels": [], "entities": [{"text": "TrecQA train dataset", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.9304661949475607}]}, {"text": "Recent works (dos) removed questions in development and test set with no answers or with only positive/negative answers, thus there are 65 questions with 1,117 pairs in Clean version development set and 68 questions with 1,442 pairs in Clean version test set. has showed the performances on Original TrecQA and Clean version TrecQA are not comparable.", "labels": [], "entities": [{"text": "Clean version development set", "start_pos": 169, "end_pos": 198, "type": "DATASET", "confidence": 0.8332633823156357}, {"text": "Clean version test set.", "start_pos": 236, "end_pos": 259, "type": "DATASET", "confidence": 0.8815493881702423}, {"text": "Original TrecQA", "start_pos": 291, "end_pos": 306, "type": "DATASET", "confidence": 0.822883129119873}]}, {"text": "Therefore, fora fair comparison, we only display the results on Clean version TrecQA which are posted on the website of Wiki of the Association for Computational Linguistics . The open domain question-answering WikiQA () is constructed from real queries of Bing and Wikipedia.", "labels": [], "entities": [{"text": "TrecQA", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9069372415542603}, {"text": "Wiki", "start_pos": 120, "end_pos": 124, "type": "DATASET", "confidence": 0.9176359176635742}]}, {"text": "We follow to remove all questions with no correct candidate answers.", "labels": [], "entities": []}, {"text": "The excluded WikiQA has 873/126/243 questions and 8627/1130/2351 question-answer pairs for train/dev/test split.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9562017917633057}]}, {"text": "To adapt our model to this task, we use semantic similarity to measure the probability of matching between a question and a candidate answer.", "labels": [], "entities": []}, {"text": "We evaluate models by mean average precision (MAP) and mean reciprocal rank (MRR).", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 22, "end_pos": 50, "type": "METRIC", "confidence": 0.8981971045335134}, {"text": "mean reciprocal rank (MRR)", "start_pos": 55, "end_pos": 81, "type": "METRIC", "confidence": 0.9158120850721995}]}], "tableCaptions": [{"text": " Table 1: Performances of our model with differ- ent strategies in alignment layer on three datasets.", "labels": [], "entities": []}, {"text": " Table 2: Test results on SICK. The symbol  *  indi- cates the models with pre-training. The symbol \u2022  indicates the models with data augmentation strat- egy.", "labels": [], "entities": [{"text": "SICK", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.5403077006340027}]}, {"text": " Table 3: Test results on Clean version TrecQA.", "labels": [], "entities": [{"text": "Clean version TrecQA", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.6255670090516409}]}, {"text": " Table 4: Test results on WikiQA. The symbol \u2022  indicates the models with data augmentation strat- egy.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9282433986663818}]}]}