{"title": [{"text": "Mimicking Word Embeddings using Subword RNNs", "labels": [], "entities": [{"text": "Mimicking Word Embeddings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8569063146909078}]}], "abstractContent": [{"text": "Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using dis-tributional information obtained from un-labeled data.", "labels": [], "entities": []}, {"text": "However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist.", "labels": [], "entities": []}, {"text": "In this paper, we present MIM-ICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings.", "labels": [], "entities": []}, {"text": "Unlike prior work, MIMICK does not require retraining on the original word embedding corpus; instead, learning is performed at the type level.", "labels": [], "entities": [{"text": "MIMICK", "start_pos": 19, "end_pos": 25, "type": "TASK", "confidence": 0.8756077885627747}]}, {"text": "Intrinsic and extrinsic evaluations demonstrate the power of this simple approach.", "labels": [], "entities": []}, {"text": "On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntac-tic attributes.", "labels": [], "entities": []}, {"text": "It is competitive with (and complementary to) a supervised character-based model in low-resource settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6725749969482422}]}, {"text": "These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts.", "labels": [], "entities": []}, {"text": "However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications.", "labels": [], "entities": []}, {"text": "The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods.", "labels": [], "entities": []}, {"text": "These challenges are particularly acute when working with lowresource languages, where even unlabeled data maybe difficult to obtain at scale.", "labels": [], "entities": []}, {"text": "A typical solution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.", "labels": [], "entities": []}, {"text": "We approach this challenge from a quasigenerative perspective.", "labels": [], "entities": []}, {"text": "Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter.", "labels": [], "entities": []}, {"text": "We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task.", "labels": [], "entities": []}, {"text": "We call this model the MIMICK-RNN, for its ability to read a word's spelling and mimick its distributional embedding.", "labels": [], "entities": [{"text": "MIMICK-RNN", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.560263991355896}]}, {"text": "Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features.", "labels": [], "entities": []}, {"text": "As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors.", "labels": [], "entities": [{"text": "OOV abbreviations", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7334728538990021}]}, {"text": "Quantitative evaluation on the Stanford RareWord dataset () provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.", "labels": [], "entities": [{"text": "Stanford RareWord dataset", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.8651886781056722}]}, {"text": "As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes fora diverse set of 23 languages, as provided in the Universal Dependencies dataset).", "labels": [], "entities": [{"text": "Universal Dependencies dataset", "start_pos": 178, "end_pos": 208, "type": "DATASET", "confidence": 0.7457549770673116}]}, {"text": "Our model shows significant improvement across the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task.", "labels": [], "entities": []}, {"text": "In low-resource settings, our approach is particularly effective, and is complementary to supervised character embeddings trained from labeled data.", "labels": [], "entities": []}, {"text": "The MIMICK-RNN therefore provides a useful new tool for tagging tasks in settings where there is limited labeled data.", "labels": [], "entities": [{"text": "tagging tasks", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9182434976100922}]}, {"text": "Models and code are available at www.github.com/ yuvalpinter/mimick .", "labels": [], "entities": []}], "datasetContent": [{"text": "The morphological complexity and compositionality of words varies greatly across languages.", "labels": [], "entities": []}, {"text": "While a morphologically-rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Vietnamese may have not a single polymorphemic or inflected word in it.", "labels": [], "entities": []}, {"text": "To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.818456381559372}]}, {"text": "Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well.", "labels": [], "entities": []}, {"text": "We also considered language selection recommendations from de and.", "labels": [], "entities": [{"text": "language selection", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7778301537036896}]}, {"text": "As stated above, our approach is built on the Polyglot word embeddings.", "labels": [], "entities": []}, {"text": "The intersection of the Polyglot embeddings and the UD dataset (version 1.4) yields 44 languages.", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9575342535972595}]}, {"text": "Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged languages, with the exception of Indonesian.", "labels": [], "entities": []}, {"text": "presents the selected languages and their typological properties.", "labels": [], "entities": []}, {"text": "As an additional proxy for mor-: Languages used in tagging evaluation.", "labels": [], "entities": [{"text": "tagging evaluation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9649859368801117}]}, {"text": "Languages on the right are Indo-European.", "labels": [], "entities": []}, {"text": "*In Vietnamese script, whitespace separates syllables rather than words.", "labels": [], "entities": []}, {"text": "phological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Nearest-neighbor examples for Hebrew (Transcriptions per Sima'an et al. (2001)). 's/y' stands  for 'she/you-m.sg.'; subscripts denote alternative spellings, standard form being 'X' 1 .", "labels": [], "entities": []}, {"text": " Table 3: Similarity results on the RareWord set,  measured as Spearman's \u03c1 \u00d7 100. VarEmbed was  trained on a 20-million token dataset, Polyglot on  a 1.7B-token dataset.", "labels": [], "entities": [{"text": "RareWord set", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9630483388900757}, {"text": "Polyglot", "start_pos": 136, "end_pos": 144, "type": "DATASET", "confidence": 0.9562243223190308}]}, {"text": " Table 5: POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degrada- tion) by McNemar's test, p < .01, comparing MIMICK to \"No-Char\", and \"Both\" to CHAR\u2192TAG.  * For reference, we copy the reported results of Plank et al. (2016)'s analog to CHAR\u2192TAG. Note that  these were obtained on UD 1.2, and without jointly tagging morphosyntactic attributes.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6484591364860535}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9331998229026794}, {"text": "UD 1.4 Test", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.6833752890427908}, {"text": "MIMICK", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.7476379871368408}, {"text": "UD 1.2", "start_pos": 319, "end_pos": 325, "type": "DATASET", "confidence": 0.9057606756687164}]}, {"text": " Table 6: Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant  improvement (degradation) by a bootstrapped Z-test, p < .01, comparing models as in Table 5. Note  that the Kazakh (kk) test set has only 78 morphologically tagged tokens.", "labels": [], "entities": [{"text": "UD 1.4 Test", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.7116160194079081}, {"text": "Kazakh (kk) test set", "start_pos": 215, "end_pos": 235, "type": "DATASET", "confidence": 0.6271903018156687}]}, {"text": " Table 7: Absolute gain in POS tagging accuracy  from using MIMICK for 10,000-token datasets (all  tokens for Tamil and Kazakh). Bold denotes sta- tistical significance (McNemar's test,p < 0.01).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.8160524666309357}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9423292279243469}]}]}