{"title": [{"text": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "labels": [], "entities": [{"text": "Accurate", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9668840765953064}, {"text": "Entity Recognition", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6654392927885056}]}], "abstractContent": [{"text": "Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs.", "labels": [], "entities": []}, {"text": "Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF).", "labels": [], "entities": []}, {"text": "Though expressive and accurate, these models fail to fully exploit GPU par-allelism, limiting their computational efficiency.", "labels": [], "entities": []}, {"text": "This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 191, "end_pos": 212, "type": "TASK", "confidence": 0.7022520899772644}]}, {"text": "Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents.", "labels": [], "entities": [{"text": "O(N) time", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9060122609138489}]}, {"text": "We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9993683695793152}]}, {"text": "Moreover , ID-CNNs trained to aggregate context from the entire document are even more accurate while maintaining 8x faster test time speeds.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to democratize large-scale NLP and information extraction while minimizing our environmental footprint, we require fast, resourceefficient methods for sequence tagging tasks such as part-of-speech tagging and named entity recognition (NER).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7979093492031097}, {"text": "sequence tagging", "start_pos": 160, "end_pos": 176, "type": "TASK", "confidence": 0.6879419237375259}, {"text": "part-of-speech tagging", "start_pos": 191, "end_pos": 213, "type": "TASK", "confidence": 0.7271964102983475}, {"text": "named entity recognition (NER)", "start_pos": 218, "end_pos": 248, "type": "TASK", "confidence": 0.7745184004306793}]}, {"text": "Speed is not sufficient of course: they must also be expressive enough to tolerate the tremendous lexical variation in input data.", "labels": [], "entities": []}, {"text": "The massively parallel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling (.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.6375351995229721}]}, {"text": "While these models are expressive and accurate, they fail to fully exploit the parallelism opportunities of a GPU, and thus their speed is limited.", "labels": [], "entities": []}, {"text": "Specifically, they employ either recurrent neural networks (RNNs) for feature extraction, or Viterbi inference in a structured output model, both of which require sequential computation across the length of the input.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7269767671823502}]}, {"text": "Instead, parallelized runtime independent of the length of the sequence saves time and energy costs, maximizing GPU resource usage and minimizing the amount of time it takes to train and evaluate models.", "labels": [], "entities": []}, {"text": "Convolutional neural networks (CNNs) provide exactly this property).", "labels": [], "entities": []}, {"text": "Rather than composing representations incrementally over each token in a sequence, they apply filters in parallel across the entire sequence at once.", "labels": [], "entities": []}, {"text": "Their computational cost grows with the number of layers, but not the input size, up to the memory and threading limitations of the hardware.", "labels": [], "entities": []}, {"text": "This provides, for example, audio generation models that can be trained in parallel (van den ).", "labels": [], "entities": [{"text": "audio generation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7139072269201279}]}, {"text": "Despite the clear computational advantages of CNNs, RNNs have become the standard method for composing deep representations of text.", "labels": [], "entities": []}, {"text": "This is because a token encoded by a bidirectional RNN will incorporate evidence from the entire input sequence, but the CNN's representation is limited by the effective input width 1 of the network: the size of the input context which is observed, directly or indirectly, by the representation of a token at a given layer in the network.", "labels": [], "entities": []}, {"text": "Specifically, in a network composed of a series of stacked convolutional layers of convolution width w, the number r of context tokens incorporated into a token's representation at a given layer l, is given by r = l(w \u2212 1) + 1.", "labels": [], "entities": []}, {"text": "The number of layers required to incorporate the entire input context grows linearly with the length of the sequence.", "labels": [], "entities": []}, {"text": "To avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 107, "end_pos": 124, "type": "TASK", "confidence": 0.6781174540519714}]}, {"text": "In response, this paper presents an application of dilated convolutions ( for sequence labeling).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.6025150120258331}]}, {"text": "For dilated convolutions, the effective input width can grow exponentially with the depth, with no loss in resolution at each layer and with a modest number of parameters to estimate.", "labels": [], "entities": []}, {"text": "Like typical CNN layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conventional convolutions, the context need not be consecutive; the dilated window skips over every dilation width d inputs.", "labels": [], "entities": []}, {"text": "By stacking layers of dilated convolutions of exponentially increasing dilation width, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: The size of the effective input width fora token at layer l is now given by 2 l+1 \u22121.", "labels": [], "entities": []}, {"text": "More concretely, just four stacked dilated convolutions of width 3 produces token representations with an effective input width of 31 tokens -longer than the average sentence length (23) in the Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.9944200813770294}]}, {"text": "Our overall iterated dilated CNN architecture (ID-CNN) repeatedly applies the same block of dilated convolutions to token-wise representations.", "labels": [], "entities": []}, {"text": "This parameter sharing prevents overfitting and also provides opportunities to inject supervision on intermediate activations of the network.", "labels": [], "entities": []}, {"text": "Similar to models that use logits produced by an RNN, the ID-CNN provides two methods for performing prediction: we can predict each token's label independently, or by running Viterbi inference in a chain structured graphical model.", "labels": [], "entities": []}, {"text": "In experiments on CoNLL 2003 and OntoNotes What we call effective input width here is known as the receptive field in the vision literature, drawing an analogy to the visual receptive field of a neuron in the retina.", "labels": [], "entities": [{"text": "CoNLL 2003", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.8453381657600403}]}, {"text": "5.0 English NER, we demonstrate significant speed gains of our ID-CNNs over various recurrent models, while maintaining similar F1 performance.", "labels": [], "entities": [{"text": "English NER", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.6069769859313965}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.9985200762748718}]}, {"text": "When performing prediction using independent classification, the ID-CNN consistently outperforms a bidirectional LSTM (Bi-LSTM), and performs on par with inference in a CRF with logits from a Bi-LSTM (Bi-LSTM-CRF).", "labels": [], "entities": []}, {"text": "As an extractor of per-token logits fora CRF, our model out-performs the Bi-LSTM-CRF.", "labels": [], "entities": []}, {"text": "We also apply ID-CNNs to entire documents, where independent token classification is as accurate as the Bi-LSTM-CRF while decoding almost 8\u00d7 faster.", "labels": [], "entities": []}, {"text": "The clear accuracy gains resulting from incorporating broader context suggest that these models could similarly benefit many other contextsensitive NLP tasks which have until now been limited by the computational complexity of existing context-rich models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981881976127625}]}], "datasetContent": [{"text": "We describe experiments on two benchmark English named entity recognition datasets.", "labels": [], "entities": [{"text": "benchmark English named entity recognition", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.6717075705528259}]}, {"text": "On CoNLL-2003 English NER, our ID-CNN performs on par with a Bi-LSTM not only when used to produce per-token logits for structured inference, but the ID-CNN with greedy decoding also performs on-par with the Bi-LSTM-CRF while running at more than 14 times the speed.", "labels": [], "entities": [{"text": "CoNLL-2003 English NER", "start_pos": 3, "end_pos": 25, "type": "DATASET", "confidence": 0.9259220163027445}]}, {"text": "We also observe a performance boost in almost all models when broadening the context to incorporate entire documents, achieving an average F1 of 90.65 on CoNLL-2003, out-performing the sentence-level model while still decoding at nearly 8 times the speed of the Bi-LSTM-CRF.", "labels": [], "entities": [{"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9993798732757568}, {"text": "CoNLL-2003", "start_pos": 154, "end_pos": 164, "type": "DATASET", "confidence": 0.934673011302948}]}, {"text": "We evaluate using labeled data from the CoNLL-2003 shared task) and OntoNotes 5.0 ().", "labels": [], "entities": [{"text": "CoNLL-2003 shared task", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.8196781277656555}, {"text": "OntoNotes 5.0", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.8319030702114105}]}, {"text": "Following previous work, we use the same OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task ().", "labels": [], "entities": [{"text": "OntoNotes data split", "start_pos": 41, "end_pos": 61, "type": "DATASET", "confidence": 0.8130544424057007}, {"text": "co-reference resolution", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7163486480712891}]}, {"text": "For both datasets, we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance.", "labels": [], "entities": [{"text": "IOB boundary encoding", "start_pos": 34, "end_pos": 55, "type": "METRIC", "confidence": 0.8095333774884542}, {"text": "BILOU", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9951971173286438}]}, {"text": "As in previous work we evaluate the performance of our models using segment-level micro-averaged F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9406005442142487}]}, {"text": "Hyperparameters that resulted in the best performance on the validation set were selected via grid search.", "labels": [], "entities": []}, {"text": "A more detailed description of the data, evaluation, optimization and data pre-processing can be found in the Appendix.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F1 score of models observing sentence- level context. No models use character embed- dings or lexicons. Top models are greedy, bottom  models use Viterbi inference .", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.97414630651474}]}, {"text": " Table 2: Relative test-time speed of sentence mod- els, using the fastest batch size for each model. 5", "labels": [], "entities": [{"text": "Relative test-time speed", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8556860685348511}]}, {"text": " Table 3: Comparison of models trained with and  without expectation-linear dropout regularization  (DR). DR improves all models.", "labels": [], "entities": []}, {"text": " Table 4: F1 score of models trained to predict  document-at-a-time. Our greedy ID-CNN model  performs as well as the Bi-LSTM-CRF.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9735617935657501}]}, {"text": " Table 7: F1 score of sentence and document mod- els on OntoNotes.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9723848104476929}, {"text": "OntoNotes", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9287521243095398}]}]}