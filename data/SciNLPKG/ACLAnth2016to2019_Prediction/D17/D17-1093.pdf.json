{"title": [{"text": "Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has shown that Tree Kernels (TKs) and Convolutional Neural Networks (CNNs) obtain the state of the art in answer sentence reranking.", "labels": [], "entities": [{"text": "answer sentence reranking", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.772509773572286}]}, {"text": "Additionally, their combination used in Support Vector Machines (SVMs) is promising as it can exploit both the syntactic patterns captured by TKs and the embeddings learned by CNNs.", "labels": [], "entities": [{"text": "Support Vector Machines (SVMs)", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.5911801109711329}]}, {"text": "However, the embeddings are constructed according to a classification function, which is not directly exploitable in the preference ranking algorithm of SVMs.", "labels": [], "entities": []}, {"text": "In this work, we propose anew hybrid approach combining preference ranking applied to TKs and pointwise ranking applied to CNNs.", "labels": [], "entities": []}, {"text": "We show that our approach produces better results on two well-known and rather different datasets: WikiQA for answer sentence selection and SemEval cQA for comment selection in Community Question Answering.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9025112390518188}, {"text": "answer sentence selection", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.7224180499712626}, {"text": "Community Question Answering", "start_pos": 177, "end_pos": 205, "type": "TASK", "confidence": 0.5531129141648611}]}], "introductionContent": [{"text": "Recent work on learning to rank (L2R) has shown that deep learning and kernel methods are two very effective approaches, given their ability of engineering features.", "labels": [], "entities": []}, {"text": "In particular, in question answering (QA), Convolutional Neural Networks (CNN), e.g., can automatically learn the representation of question and answer passage (Q/AP) in terms of word embeddings and their non-linear transformations.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8680762410163879}, {"text": "representation of question and answer passage (Q/AP)", "start_pos": 114, "end_pos": 166, "type": "TASK", "confidence": 0.727143715728413}]}, {"text": "These are then used by the other layers of the network to measure Q/AP relatedness.", "labels": [], "entities": [{"text": "Q/AP relatedness", "start_pos": 66, "end_pos": 82, "type": "METRIC", "confidence": 0.616741493344307}]}, {"text": "In contrast, Convolution Tree Kernels (CTK) can be applied to relational structures built on top of syntactic/semantic structures derived from Q/AP text ().", "labels": [], "entities": []}, {"text": "CNNs as well as CTKs can achieve the state of the art in ranking APs or also questions.", "labels": [], "entities": []}, {"text": "Considering their complementary approach for generating features, studying ways to combine them is very promising.", "labels": [], "entities": []}, {"text": "In (), we investigated the idea of extracting layers from CNNs and using them in a kernel function to be further combined with CTKs in a composite reranking kernel.", "labels": [], "entities": []}, {"text": "This was used in an SVM) model, which obtained a significant improvement over the individual methods.", "labels": [], "entities": []}, {"text": "However, the simple use of CNN layers as vectors in a preference ranking approach is intutively not optimal since such layers are basically learnt in a classification model, thus they are not optimized for SVM In this work, we further compare and investigate different ways of combining CTKs and CNNs in reranking settings.", "labels": [], "entities": []}, {"text": "In particular, we follow the intuition that as CNNs learn the embeddings in a classification setting they should be used in the same way for building the reranking kernel, i.e., we need to use the embeddings in a pointwise reranking fashion.", "labels": [], "entities": []}, {"text": "Therefore, we propose a hybrid preference-pointwise kernel, which consists in (i) a standard reranking kernel based on CTKs applied to the Q/AP structural representations; and (ii) a classification kernel based on the embeddings learned by neural networks.", "labels": [], "entities": []}, {"text": "The intuition about the hybrid models is to add CNN layer vectors, not their difference, to the preference CTK.", "labels": [], "entities": []}, {"text": "That is, CNN layers are still used as they were used in a classification setting whereas CTKs follow the standard SVM Rank approach.", "labels": [], "entities": []}, {"text": "We tested our proposed models on the answer sentence selection benchmark,, and the benchmark from cQA SemEval-2016 Task 3.A 1 corpus.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.6724788943926493}, {"text": "cQA SemEval-2016 Task 3.A 1 corpus", "start_pos": 98, "end_pos": 132, "type": "DATASET", "confidence": 0.9314341843128204}]}, {"text": "We show that the proposed hybrid kernel consistently outperforms standard reranking models in all settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we compare various methods of combining CTKs and CNNs, using standard and our hybrid reranking kernels.", "labels": [], "entities": []}, {"text": "The software for reproducing our experimental results is available at https://github.com/iKernels/ RelTextRank.", "labels": [], "entities": []}, {"text": "WikiQA, sentence selection dataset: this was created for open domain QA.", "labels": [], "entities": [{"text": "WikiQA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9322603344917297}, {"text": "sentence selection", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7339403033256531}]}, {"text": "Text Preprocessing: we used the Illinois chunker) and the Stanford CoreNLP () toolkit, v3.6.0.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.8909834027290344}]}, {"text": "When experimenting with SemEval-2016, we perform preprocessing as in (), e.g., we truncate all the comments to 2000 symbols and sentences to 70 words.", "labels": [], "entities": []}, {"text": "CTKs: we trained our models with SVM-Light-TK 4 using the partial tree kernel (PTK) and the subset tree kernel (STK).", "labels": [], "entities": []}, {"text": "We use PTK for WikiQA and STK for SemEval as suggested in our previous work () with default parameters and the polynomial kernel (P) of degree 3 on all feature vectors, which are embeddings learned as described in Section 3.2.", "labels": [], "entities": [{"text": "PTK", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.6500109434127808}]}, {"text": "Neural Network (CNN) setup: we used the same setup and parameters as (Tymoshenko et al., 2016a): we pre-initialize the word embeddings with skipgram embedding of dimensionality 50 trained on the English Wikipedia dump (.", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 195, "end_pos": 217, "type": "DATASET", "confidence": 0.8499290347099304}]}, {"text": "We used a single non-linear hidden layer (with hyperbolic tangent activation, Tanh), whose size is equal to the size of the previous layer, i.e., the join layer.", "labels": [], "entities": []}, {"text": "The network is trained using SGD with shuffled mini-batches using the Rmsprop update rule).", "labels": [], "entities": []}, {"text": "The model is trained until the validation loss stops improving.", "labels": [], "entities": [{"text": "validation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9350038766860962}]}, {"text": "The size of the sentences embedding (QE and AE) and of the join layer is set as 200.", "labels": [], "entities": [{"text": "AE", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9549948573112488}]}, {"text": "QA metrics: we report our results in terms of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and P@1.", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 46, "end_pos": 74, "type": "METRIC", "confidence": 0.973564992348353}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 76, "end_pos": 102, "type": "METRIC", "confidence": 0.9707200825214386}, {"text": "P@1", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.9599244395891825}]}], "tableCaptions": [{"text": " Table 2: Experimental results on WikiQA and SemEval-2016 Task 3.A corpora", "labels": [], "entities": []}]}