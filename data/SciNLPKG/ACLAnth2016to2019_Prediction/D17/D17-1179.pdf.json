{"title": [{"text": "Semi-Supervised Structured Prediction with Neural CRF Autoencoder", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning of sequential structured prediction problems.", "labels": [], "entities": [{"text": "semi-supervised learning of sequential structured prediction problems", "start_pos": 82, "end_pos": 151, "type": "TASK", "confidence": 0.6322809628077916}]}, {"text": "Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to reconstruct the input.", "labels": [], "entities": []}, {"text": "Our model has a unified structure with different loss functions for labeled and unlabeled data with shared parameters.", "labels": [], "entities": []}, {"text": "We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters.", "labels": [], "entities": []}, {"text": "Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages , show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios .", "labels": [], "entities": [{"text": "Part-of-Speech (POS) tagging task", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.665824164946874}]}], "introductionContent": [{"text": "The recent renaissance of deep learning has led to significant strides forward in several AI fields.", "labels": [], "entities": []}, {"text": "In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms).", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7670626044273376}]}, {"text": "These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision.", "labels": [], "entities": []}, {"text": "Among these models, neural variants of the Conditional Random Fields (CRF) model) are especially popular.", "labels": [], "entities": []}, {"text": "By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks.", "labels": [], "entities": []}, {"text": "Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult.", "labels": [], "entities": [{"text": "structured prediction tasks", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.7152741352717081}]}, {"text": "Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available.", "labels": [], "entities": []}, {"text": "This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data.", "labels": [], "entities": []}, {"text": "Providing it through manual annotation is often a difficult labor-intensive task.", "labels": [], "entities": []}, {"text": "In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning on sequence labeling problems.", "labels": [], "entities": []}, {"text": "An autoencoder is a special type of neural net, modeling the conditional probability P ( \u02c6 X|X), where X is the original input to the model and\u02c6X and\u02c6 and\u02c6X is the reconstructed input.", "labels": [], "entities": []}, {"text": "Autoencoders consist of two parts, an encoder projecting the input to a hidden space, and a decoder reconstructing the input from it.", "labels": [], "entities": []}, {"text": "Traditionally, autoencoders are used for generating a compressed representation of the input by projecting it into a dense low dimensional space.", "labels": [], "entities": []}, {"text": "In our setting the hidden space consists of discrete variables that comprise the output structure.", "labels": [], "entities": []}, {"text": "These generalized settings are described in.", "labels": [], "entities": []}, {"text": "By definition, it is easy to see that the encoder (lower half in can be modeled by a discriminative model describing P (Y |X) directly, while the decoder (upper half in) naturally fits as a generative model, describing P ( \u02c6 X|Y ), where Y is the label.", "labels": [], "entities": []}, {"text": "In our model, illustrated in, the encoder is a CRF model with neural networks as its potential extractors, while the decoder is a generative model, trying to reconstruct the input.", "labels": [], "entities": []}, {"text": "Our model carries the merit of autoencoders, which can exploit valuable information from unlabeled data.", "labels": [], "entities": []}, {"text": "Recent works ( suggested using an autoencoder with a CRF model as an encoder in an unsupervised setting.", "labels": [], "entities": []}, {"text": "We significantly expand on these works and suggest the following contributions: 1.", "labels": [], "entities": []}, {"text": "We propose a unified model seamlessly accommodating both unlabeled and labeled data.", "labels": [], "entities": []}, {"text": "While past work focused on unsupervised structured prediction, neglecting the discriminative power of such models, our model easily supports learning in both fully supervised and semisupervised settings.", "labels": [], "entities": []}, {"text": "We developed a variation of the Expectation-Maximization (EM) algorithm, used for optimizing the encoder and the decoder of our model simultaneously.", "labels": [], "entities": []}, {"text": "2. We increase the expressivity of the traditional CRF autoencoder model using neural networks as the potential extractors, thus avoiding the heavy feature engineering necessary in previous works.", "labels": [], "entities": []}, {"text": "Interestingly, our model's predictions, which unify the discriminative neural CRF encoder and the generative decoder, have led to an improved performance over the highly optimized neural CRF (NCRF) model alone, even when trained in the supervised settings over the same data.", "labels": [], "entities": []}, {"text": "3. We demonstrate the advantages of our model empirically, focusing on the well-known Partof-Speech (POS) tagging problem over 8 different languages, including low resource languages.", "labels": [], "entities": [{"text": "Partof-Speech (POS) tagging problem", "start_pos": 86, "end_pos": 121, "type": "TASK", "confidence": 0.639609103401502}]}, {"text": "In the supervised setting, our NCRF-AE outperformed the highly optimized NCRF.", "labels": [], "entities": []}, {"text": "In the semisupervised setting, our model was able to successfully utilize unlabeled data, improving on the performance obtained when only using the labeled data, and outperforming competing semisupervised learning algorithms.", "labels": [], "entities": []}, {"text": "Furthermore, our newly proposed algorithm is directly applicable to other sequential learning tasks in NLP, and can be easily adapted to other structured tasks such as dependency parsing or constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm.", "labels": [], "entities": [{"text": "dependency parsing or constituent parsing", "start_pos": 168, "end_pos": 209, "type": "TASK", "confidence": 0.6552710592746734}]}, {"text": "All of these tasks can benefit from semisupervised learning algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset We evaluated our model on the POS tagging task, in both the supervised and semisupervised learning settings, over eight different languages from the UD (Universal Dependencies) 1.4 dataset ().", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8401857018470764}, {"text": "UD (Universal Dependencies) 1.4 dataset", "start_pos": 157, "end_pos": 196, "type": "DATASET", "confidence": 0.6880414017609188}]}, {"text": "The task is defined over 17 different POS tags, used across the different languages.", "labels": [], "entities": []}, {"text": "We followed the original English French German Italian Russian Spanish Indonesian: Statistics of different UD languages used in our experiments, including the number of tokens, and the number of sentences in training, development and testing set respectively.", "labels": [], "entities": []}, {"text": "UD division for training, development and testing in our experiments.", "labels": [], "entities": [{"text": "UD division", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9556490778923035}]}, {"text": "The statistics of the data used in our experiments are described in table 1.", "labels": [], "entities": []}, {"text": "The UD dataset includes several low-resource languages which are of particular interest to our semisupervised model.", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8276863694190979}]}], "tableCaptions": [{"text": " Table 1: Statistics of different UD languages used in our experiments, including the number of tokens,  and the number of sentences in training, development and testing set respectively.", "labels": [], "entities": []}, {"text": " Table 2: Supervised learning accuracy of POS tagging on 8 UD languages using different models", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9811243414878845}, {"text": "POS tagging", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8318451344966888}]}, {"text": " Table 3: Semi-supervised learning accuracy of POS tagging on 8 UD languages. HEM means hard-EM,  used as a self-training approach, and OL means only 20% of the labeled data is used and no unlabeled  data is used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9922246932983398}, {"text": "POS tagging", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.8150663375854492}, {"text": "HEM", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9638913869857788}, {"text": "OL", "start_pos": 136, "end_pos": 138, "type": "METRIC", "confidence": 0.9633893966674805}]}]}