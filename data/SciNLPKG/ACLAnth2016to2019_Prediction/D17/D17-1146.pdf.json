{"title": [], "abstractContent": [{"text": "Neural machine translation (NMT) has achieved notable success in recent times, however it is also widely recognized that this approach has limitations with handling infrequent words and word pairs.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8264621396859487}]}, {"text": "This paper presents a novel memory-augmented NMT (M-NMT) architecture, which stores knowledge about how words (usually infrequently encountered ones) should be translated in a memory and then utilizes them to assist the neural model.", "labels": [], "entities": []}, {"text": "We use this memory mechanism to combine the knowledge learned from a conventional statistical machine translation system and the rules learned by an NMT system, and also propose a solution for out-of-vocabulary (OOV) words based on this framework.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6651554306348165}]}, {"text": "Our experiments on two Chinese-English translation tasks demonstrated that the M-NMT architecture out-performed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively.", "labels": [], "entities": [{"text": "Chinese-English translation tasks", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.6767372886339823}, {"text": "BLEU", "start_pos": 144, "end_pos": 148, "type": "METRIC", "confidence": 0.9989877343177795}]}, {"text": "Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods.", "labels": [], "entities": [{"text": "OOV", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.6720923781394958}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has been shown to have highly promising performance, particularly when a large amount of training data is available (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8093741536140442}]}, {"text": "Although there are different model architectures (, the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding.", "labels": [], "entities": []}, {"text": "This 'meaning src. ref.", "labels": [], "entities": []}, {"text": "Humans have 23 pairs of chromosomes.", "labels": [], "entities": []}, {"text": "NMT There are 23-year history of human history.: An example of Chinese-to-English 'meaning drift' with NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.915252149105072}]}, {"text": "encoding' principle leads to a deeper understanding and learning of the translation rules, and hence a better translation than conventional statistic machine translation (SMT) that considers only surface forms, i.e., words and phrases (.", "labels": [], "entities": [{"text": "statistic machine translation (SMT)", "start_pos": 140, "end_pos": 175, "type": "TASK", "confidence": 0.8221337298552195}]}, {"text": "Despite positive results obtained so far, a particular problem of the NMT approach is that it has a tendency towards overfitting to frequent observations (words, word co-occurrences, translation pairs, etc.), but overlooking special cases that are not frequently observed.", "labels": [], "entities": []}, {"text": "For example, NMT is good at learning translation pairs that are frequently observed, and can make use of them well at run-time, but for low-frequency pairs in the training data, the system may 'forget' to use them when they should be.", "labels": [], "entities": []}, {"text": "Unfortunately, rare words are inevitable for all translation tasks due to Zipf's law, and indeed they are often the most important parts of a sentence, e.g., domain-specific entity names.", "labels": [], "entities": []}, {"text": "shows an example, where the word ' ( chromosomes)' is an infrequent word.", "labels": [], "entities": []}, {"text": "As the system does not know (or has effectively 'forgotten') this keyword, it does not translate correctly, and an irrelevant translation is produced, leading to the phenomenon of 'meaning drift'.", "labels": [], "entities": []}, {"text": "This weakness with regard to infrequent words/pairs with NMT has been noticed by a number of researchers, and some studies have been conducted to address this problem, e.g.,;; ;;;.", "labels": [], "entities": []}, {"text": "Superficially, this problem appears to be caused by the imperfect embeddings of infrequent words or the limited vocabulary size of NMT systems, but we argue that the deeper reason should be attributed to the nature of neural models: the translation function, represented by various neural networks, is shared amongst all of the translation pairs, so high-frequency and low-frequency pairs impact each other by adapting their shared parameters.", "labels": [], "entities": []}, {"text": "Due to the overwhelming proportion of high-frequency pairs in the training data, the resulting trained model will naturally be much more focused on these frequently observed pairs.", "labels": [], "entities": []}, {"text": "More seriously, because the translation function is smooth, infrequent pairs tend to be wrongly seen as noise in the training process and so are largely ignored by the model.", "labels": [], "entities": []}, {"text": "In contrast to this, the conventional SMT approach is based on statistics of words and/or phrases, which, in principle, is a symbolic method that uses a discrete model and involves little parameter sharing.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9965683221817017}]}, {"text": "The discrete model means that no matter how infrequently a pair occurs, its probability cannot be smoothed out, and the lack of shared parameters means that the frequent words or pairs have much less impact on infrequent words or pairs.", "labels": [], "entities": []}, {"text": "Essentially, SMT memorizes as many of the observed patterns as possible, usually using a phrase table.", "labels": [], "entities": [{"text": "SMT memorizes", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8875761926174164}]}, {"text": "The respective advantages of SMT and NMT suggest that neither the pure neural approach nor the pure symbolic approach can provide a complete solution for machine translation, and a combined system that exploits the advantages of both approaches would be ideal.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.983688235282898}, {"text": "machine translation", "start_pos": 154, "end_pos": 173, "type": "TASK", "confidence": 0.7667483687400818}]}, {"text": "This idea has been adopted in early research into neural-based MT methods, where neural models were utilized to improve SMT performance ().", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9491564631462097}, {"text": "SMT", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9952743053436279}]}, {"text": "However, this seems to be counterintuitive, as intuitively learning general rules should be the first step, rather than first memorizing special cases and then learning general rules.", "labels": [], "entities": []}, {"text": "This suggests that the combined system should be primarily based on the neural architecture, with symbolic knowledge as a complementary support.", "labels": [], "entities": []}, {"text": "This paper presents such a neural-symbolic architecture, which involves a neural model component to deal with frequently seen patterns, and a memory component to provide knowledge for infrequently used words and pairs.", "labels": [], "entities": []}, {"text": "More specifically, each memory element stores a source-target pair, specifying that a word defined by the source part should be translated to the word defined by the target part.", "labels": [], "entities": []}, {"text": "This knowledge is then used to improve the neural model.", "labels": [], "entities": []}, {"text": "This is analogy to an experienced translator, who can work well inmost cases using their own knowledge (i.e. the neural model aspect), but for unfamiliar and uncommon words that they have little experience of, they will still need to refer to a dictionary (i.e., the memory).", "labels": [], "entities": []}, {"text": "This proposed memory-augmented NMT, or M-NMT, is therefore arguably much more similar to human translators than either NMT or SMT.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the first experiment, the M-NMT architecture combined SMT and NMT by using SMT to construct the memory to assist with NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9378975033760071}]}, {"text": "For comparison purposes, the lexical prediction approach proposed by) was also implemented.", "labels": [], "entities": [{"text": "lexical prediction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7218935787677765}]}, {"text": "This uses the phrase table produced by SMT to improve NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9215684533119202}, {"text": "NMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.6010206341743469}]}, {"text": "Our implementation is a linear combination, and fora fair comparison, the neural model part was kept unchanged.", "labels": [], "entities": []}, {"text": "At each step i, the auxiliary probability provided by the lexical part is P (y i ) = j \u03b1 ij P (y i |x j ), where \u03b1 ij is the attention weight from the neural model, and P (y i |x j ) is obtained from the phrase table.", "labels": [], "entities": []}, {"text": "This can be regarded as a simple memory approach, with memory attention borrowed from the neural model, rather than being learned separately.", "labels": [], "entities": []}, {"text": "shows the BLEU results with different systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.996690034866333}]}, {"text": "Firstly, it can be observed that with the small IWSLT05 dataset, the SMT outperforms the baseline NMT, but with the large NIST dataset, NMT outperforms SMT.", "labels": [], "entities": [{"text": "IWSLT05 dataset", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9622424244880676}, {"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9678887724876404}, {"text": "NIST dataset", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.9635220170021057}, {"text": "SMT", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9589874148368835}]}, {"text": "This is unsurprising as neural models often need more training data.", "labels": [], "entities": []}, {"text": "Secondly, the results show that with both datasets, the lexical approach (NMT-L) can improve NMT performance, showing that using SMT knowledge helps NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9796085953712463}, {"text": "NMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.8866603970527649}]}, {"text": "However, the improvement seems less significant than reported in (.", "labels": [], "entities": []}, {"text": "This is likely to be because our implementation focuses on creating a simple, extensible, and generalizable system, and therefore does not allow retraining the neural model.", "labels": [], "entities": []}, {"text": "The M-NMT system provides significant performance improvement, even with the simplest setting (M-NMT(s, u y )).", "labels": [], "entities": []}, {"text": "More information factors tend to offer better performance, and the best M-NMT system, M-NMT(sy, u xy ), outperforms the baseline NMT by 9.0 and 2.7 BLEU points  on the two datasets respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9986262321472168}]}, {"text": "Notably, the improvement with the IWSLT05 dataset is impressive, the best M-NMT system outperforms even the very strong SMT baseline, which strongly supports our conjecture that NMT must be equipped with a symbolic structure to deal with infrequent words.", "labels": [], "entities": [{"text": "IWSLT05 dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9658917188644409}]}, {"text": "It also suggests that the M-NMT architecture is a promising way to apply neural methods to low-resource tasks.", "labels": [], "entities": []}, {"text": "Here the M-NMT architecture was used to handle OOV words.", "labels": [], "entities": []}, {"text": "The experiments were conducted on the NIST dataset, for which we collected 312 test sentences containing OOV words.", "labels": [], "entities": [{"text": "NIST dataset", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.988917350769043}]}, {"text": "This test set was divided into two subsets: the T-INV set, containing sentences with source OOV words whose translations are NOT OOV in the target language; and the T-OOV set, containing sentences with OOV words that are OOV in both source and translation.", "labels": [], "entities": []}, {"text": "There were 491 source-side OOV words in total, among which 276 words have in-vocabulary translations and 215 words only have OOV translation.", "labels": [], "entities": []}, {"text": "We constructed a translation table with three items for each OOV word: (1) its translation; (2) its similar word; (3) the similar word of its translation, if the translation is also an OOV word.", "labels": [], "entities": []}, {"text": "All the above was designed by hand, and for each OOV word, there was only a single translation.", "labels": [], "entities": []}, {"text": "Although it is not difficult to collect most of this information automatically (e.g., by using an SMT phrase table), we are simulating the scenario where OOV words are newly coined, or where the system is migrated to anew domain, meaning that some words are totally new to the system.", "labels": [], "entities": [{"text": "SMT phrase", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.8431788384914398}]}, {"text": "Handling OOV words of this type is certainly challenging, but it is also practically valuable.", "labels": [], "entities": []}, {"text": "For comparison, the place-holder approach pro-  posed by was also implemented.", "labels": [], "entities": []}, {"text": "Here, OOV words in the target language are substituted by position-aware UNKs, and a post-processing step is used to replace UNKs with the correct translation.", "labels": [], "entities": []}, {"text": "We denote this system as 'NMT-PL'.", "labels": [], "entities": []}, {"text": "For M-NMT, only the best configuration M-NMT(sy, u xy ) was tested in this experiment.", "labels": [], "entities": []}, {"text": "Two scenarios were considered: the original M-NMT system, and the M-NMT system with OOV words involved in the memory (denoted by M-NMT+OOV).", "labels": [], "entities": []}, {"text": "The results with the NIST dataset are shown in.", "labels": [], "entities": [{"text": "NIST dataset", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9882932007312775}]}, {"text": "In addition to the BLEU scores, we also report the OOV recall rates, defined as the proportion of OOV words that are correctly translated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9991483688354492}, {"text": "OOV", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9428926110267639}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.8139539957046509}]}, {"text": "It can be seen that both the basic NMT and M-NMT systems work badly with OOV words: they can only process OOV words whose translations are not OOV in the target language, and the recall rate is very low (0.05 approximately).", "labels": [], "entities": [{"text": "recall rate", "start_pos": 179, "end_pos": 190, "type": "METRIC", "confidence": 0.9907907843589783}]}, {"text": "The placeholder approach (NMT-PL) can address both types of OOV words, but the recall rate is still low.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.985159695148468}]}, {"text": "The M-NMT system with OOV memory, in contrast, is much more effective in OOV word translation, as shown in.", "labels": [], "entities": [{"text": "OOV word translation", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7455640435218811}]}, {"text": "We also implemented the replace-and-restore approach reported by , but found performance to be poor (the BLEU scores are 13.9 on T-INV and 13.3 on T-OOV).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9995173215866089}, {"text": "T-INV", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.831859290599823}, {"text": "T-OOV", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.8068663477897644}]}, {"text": "This maybe due to no re-training of the neural model (again, in order to keep the extensibility and generalizability of the M-NMT system).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores with different translation  systems on the two Chinese-English translation  datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984134435653687}, {"text": "Chinese-English translation  datasets", "start_pos": 69, "end_pos": 106, "type": "DATASET", "confidence": 0.602526585261027}]}, {"text": " Table 4: The OOV recall rates and BLEU scores  on sentences with OOV words. 'T-INV' refers to  the case where the target words of the OOV input  are in-vocabulary, and 'T-OOV' means the case  where the target words are also OOV.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9142943620681763}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9997777342796326}]}, {"text": " Table 5: The translations from different systems  for the Chinese-to-English 'meaning drift' exam- ple.", "labels": [], "entities": []}]}