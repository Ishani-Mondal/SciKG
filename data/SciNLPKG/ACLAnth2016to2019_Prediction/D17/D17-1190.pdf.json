{"title": [{"text": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events", "labels": [], "entities": [{"text": "Classifying Temporal Relations between Intra-Sentence", "start_pos": 23, "end_pos": 76, "type": "TASK", "confidence": 0.8489558935165405}]}], "abstractContent": [{"text": "We present a sequential model for temporal relation classification between intra-sentence events.", "labels": [], "entities": [{"text": "temporal relation classification between intra-sentence events", "start_pos": 34, "end_pos": 96, "type": "TASK", "confidence": 0.7609802832206091}]}, {"text": "The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations.", "labels": [], "entities": []}, {"text": "Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions.", "labels": [], "entities": []}, {"text": "The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models.", "labels": [], "entities": []}, {"text": "The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them.", "labels": [], "entities": []}, {"text": "Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.", "labels": [], "entities": [{"text": "TimeBank corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9280645251274109}]}], "introductionContent": [{"text": "Identifying temporal relations between events is crucial to constructing events timeline.", "labels": [], "entities": []}, {"text": "It has direct application in tasks such as question answering, event timeline generation and document summarization.", "labels": [], "entities": [{"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.9176846742630005}, {"text": "event timeline generation", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.6408248146375021}, {"text": "document summarization", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.6545051634311676}]}, {"text": "Bush said he saw little reason to be optimistic about a settlement of the dispute, which stems from Iraq's invasion of oil-wealthy Kuwait and its subsequent military buildup on the border of Saudi Arabia.", "labels": [], "entities": []}, {"text": "Relations: (dispute af ter rel 1 invasion, invasion ibef ore rel 2 buildup, dispute af ter rel 3 buildup): Example sentence to illustrate the temporal context for event pairs.", "labels": [], "entities": []}, {"text": "Previous works studied this task as the classification problem based on discrete features defined over lexico-syntactic, semantic and discourse features.", "labels": [], "entities": []}, {"text": "However, these features are often derived from local contexts of two events and are only capable of capturing direct evidences indicating the temporal relation.", "labels": [], "entities": []}, {"text": "Specifically, when two events are distantly located or are separated by other events in between, feature based approaches often fail to utilize compositional evidences, which are hard to encode using discrete features.", "labels": [], "entities": []}, {"text": "Consider the example sentence in.", "labels": [], "entities": []}, {"text": "Here, the first two temporal re-lations, dispute after rel 1 invasion and invation ibefore rel 2 buildup, involve events that are close by and discrete features, such as dependency relations and bag-of-words extracted from local contexts of two events, might be sufficient to correctly detect their relations.", "labels": [], "entities": []}, {"text": "However, for the temporal relation dispute after rel 3 buildup, the context between the two events is long, complex and involves another event (invasion) as well, which makes it challenging for any individual feature or feature combinations to capture the temporal relation.", "labels": [], "entities": []}, {"text": "We propose that the overall syntactic structure of in-between contexts including the linear order of words as well as the compositional semantics of multi-word contexts are critical for predicting the temporal relation between two events.", "labels": [], "entities": []}, {"text": "Furthermore, the most important syntactic and semantic structures are derived along dependency paths between two event mentions . This aligns well with the observation that semantic composition relates to grammatical dependency relations.", "labels": [], "entities": []}, {"text": "Our approach defines rules on dependency parse trees to extract temporal relation indicating contexts.", "labels": [], "entities": [{"text": "dependency parse trees", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7601789633433024}]}, {"text": "First, we extract the dependency path between two event mentions.", "labels": [], "entities": []}, {"text": "Then we apply two heuristic rules to enrich extracted dependency paths and deal with complex syntactic structures such as punctuations.", "labels": [], "entities": []}, {"text": "Empirically, we found that parts-of-speech tags (POS) and dependency sequences generated following the dependency path provide evidences to predict the temporal relation as well.", "labels": [], "entities": []}, {"text": "We use neural net sequence models to capture structural and semantic compositionality in describing temporal relations between events.", "labels": [], "entities": []}, {"text": "Specifically, we generate three sequences for each dependency path, the word sequence, the POS tag sequence and the dependency relation sequence.", "labels": [], "entities": []}, {"text": "Using the three types of sequences as input, we train bi-directional LSTM models that consume each of the three sequences and model compositional structural information, both syntactically and semantically.", "labels": [], "entities": []}, {"text": "The evaluation shows that each type of sequences is useful to temporal relation classification between events.", "labels": [], "entities": [{"text": "temporal relation classification between events", "start_pos": 62, "end_pos": 109, "type": "TASK", "confidence": 0.745243227481842}]}, {"text": "Our complete neural net model taking all the three types of sequences per-forms the best, which clearly outperforms feature based models.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model using accuracy which has been used in previous research works for temporal relation classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9994452595710754}, {"text": "temporal relation classification", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.7176728049914042}]}, {"text": "We also compare model performance using per-class F-score and macro Fscore.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.8823254108428955}, {"text": "Fscore", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.647132933139801}]}, {"text": "We briefly describe all the systems we have used for evaluation.", "labels": [], "entities": []}, {"text": "Majority Class: assigns \"after\" relation to all event pairs.", "labels": [], "entities": []}, {"text": "Unidirectional LSTMs: use single LSTM layer to encode each sequence (POS tags, dependency relation and word forms) individually for extracted phrase in forward order.", "labels": [], "entities": []}, {"text": "Bidirectional LSTMs: use two LSTM layers to encode each sequence individually, taken from POS tags, dependency and word forms sequences.", "labels": [], "entities": []}, {"text": "The first layer encodes sequence in forward and second in reverse order.", "labels": [], "entities": []}, {"text": "2 Sequences: bi-directional LSTM based models considering all combinations of two sequences taken from POS tags, dependency and word forms sequences.", "labels": [], "entities": []}, {"text": "Full model: our complete sequential model considering POS, dependency and word forms sequences.", "labels": [], "entities": []}, {"text": "Direct dependency path: the same as Full model except that the two heuristic rules were not applied in extracting sequences.", "labels": [], "entities": []}, {"text": "Baseline I: a neural network classifier using discrete features described in;.", "labels": [], "entities": []}, {"text": "The features used are: POS tag, dependency relation, token and lemma of e 1 (e 2 ); dependency relations between e 1 (e 2 ) and their children; binary features indicating if e 1 and e 2 are related with the 'happensbefore' or the 'similar' relation according to, if e 1 and e 2 have the same POS tag, or if e 1 (e 2 ) is the root and e 1 modifies (or governs) e 2 ; the dependency relation between e 1 and e 2 if they are directly connected in the dependency parse tree; prepositions that modify (or govern) e 1 (e 2 ); signal words) and entity distance between e 1 and e 2 . These features are concatenated and fed into an output neural layer with 14 neurons.", "labels": [], "entities": []}, {"text": "Baseline II: a neural network classifier using POS tags and word forms of words in the surface path as input.", "labels": [], "entities": []}, {"text": "The surface path consists of words that lie in between two event mentions based on the original sentence.", "labels": [], "entities": []}, {"text": "The classifier uses four LSTM layers to encode both POS tag and word sequences in forward and backward order.", "labels": [], "entities": []}, {"text": "The output neural layer and parameters for all LSTM layers are kept the same as the Full model.", "labels": [], "entities": []}, {"text": "Baseline III: a neural network classifier based on event embeddings for both event mentions that were learned using bidirectional LSTMs.", "labels": [], "entities": []}, {"text": "The learning uses two LSTM layers, each with 150 neurons and dropout of 0.2, to embed the forward and backward representations for each event mention.", "labels": [], "entities": []}, {"text": "The input to LSTM layers are sequences of concatenated word embeddings and POS tags; each sequence corresponding to 19 context words to the left or to the right side of an event mention for the forward or the backward LSTM layer respectively.", "labels": [], "entities": []}, {"text": "Event embeddings are then concatenated and fed into an output neural layer with 14 neurons.", "labels": [], "entities": []}, {"text": "All baselines are trained using rmsprop optimizer on an objective function defined by categorical cross entropy and their output layer uses softmax activation function.: Per-class results of our best system and the baseline I. reports accuracy scores for all the systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 235, "end_pos": 243, "type": "METRIC", "confidence": 0.999142050743103}]}, {"text": "We see that simple sequential models outperform the strong feature based system, Baseline I, which used various discrete features.", "labels": [], "entities": []}, {"text": "Note that dependency relation and POS tag sequences alone achieve reasonably high accuracies.", "labels": [], "entities": []}, {"text": "This implies that an important aspect of temporal relation is contained in the syntactic context of event mentions.", "labels": [], "entities": []}, {"text": "Moreover, observed that discrete features based on dependency parse tree did not contribute to improving their classifier's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9949972629547119}]}, {"text": "On the contrary, using the sequence of dependency relations yields a high accuracy in our setting which signifies the advantages of using sequential representations for this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9993656277656555}]}, {"text": "Our Full Model achieves a performance gain of 11.35% over Baseline I.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of temporal relations in  TimeBank v1.2.", "labels": [], "entities": [{"text": "TimeBank v1.2", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8814831078052521}]}, {"text": " Table 2: Temporal relation classification result on  TimeBank corpus.", "labels": [], "entities": [{"text": "Temporal relation classification", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.9309537013371786}, {"text": "TimeBank corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.9812607169151306}]}, {"text": " Table 3: Per-class results of our best system and  the baseline I.", "labels": [], "entities": []}]}