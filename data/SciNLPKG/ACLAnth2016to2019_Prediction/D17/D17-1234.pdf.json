{"title": [], "abstractContent": [{"text": "The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical.", "labels": [], "entities": []}, {"text": "But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly.", "labels": [], "entities": []}, {"text": "Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is un-sustainable.", "labels": [], "entities": []}, {"text": "To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues.", "labels": [], "entities": [{"text": "safety", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9689178466796875}]}, {"text": "For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher.", "labels": [], "entities": []}, {"text": "Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.", "labels": [], "entities": []}], "introductionContent": [{"text": "A task-oriented dialogue system is designed for interacting with humans users to accomplish several predefined domains or tasks (.", "labels": [], "entities": []}, {"text": "Dialogue Manager is the core component in atypical dialogue system, which controls the flow of dialogue by a state tracker and a policy module ().", "labels": [], "entities": []}, {"text": "The state tracker tracks the internal state of the system while the policy module decides the response to the user according to the status of states (; Thomson and * Both authors contributed equally to this work.", "labels": [], "entities": []}, {"text": "The approaches of constructing a policy module can be divided into two categories: rule-based and statistical.", "labels": [], "entities": []}, {"text": "Rule-based policies are usually hand-crafted by domain experts which means they are inconvenient and difficult to be optimized.", "labels": [], "entities": []}, {"text": "In recent mainstream statistical studies, Partially Observable Markov Decision Process (POMDP) framework has been applied to model dialogue management with unobservable states, where policy training can be formulated as a Reinforcement Learning (RL) problem, which enables the policy to be optimized automatically (.", "labels": [], "entities": []}, {"text": "Though RL-based approaches have the potential to improve themselves as they interact more with human users and achieve better performance than rule-based approaches, they are rarely used in real-world applications, especially in on-line scenarios, since the training process is unsustainable.", "labels": [], "entities": []}, {"text": "The main causes of unsustainable on-line dialogue policy learning are two-fold: \u2022 Safety issue: the initial policy trained from scratch may lead to terrible user experience at the early training period, thus fail to attract sufficient users for more dialogues to do further policy training.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7448921799659729}, {"text": "Safety", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9977828860282898}]}, {"text": "\u2022 Efficiency issue: if the progress of policy learning is not so efficient, it will exhaust users' patience before the policy reaches a desirable performance level.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9936155080795288}]}, {"text": "Prior works have mainly focused on improving efficiency, such as Gaussian Processes RL, deep RL (, etc.", "labels": [], "entities": []}, {"text": "For deep RL approaches, recent researches on the student-teacher RL framework have shown prominent acceleration to policy learning process.", "labels": [], "entities": []}, {"text": "In such framework, the teacher agent instructs the student agent by providing suggestions on what actions should betaken next.", "labels": [], "entities": []}, {"text": "For the safety issue, developed several teaching strategies answering \"how\" the human teacher guide the learning process.", "labels": [], "entities": []}, {"text": "However, those previous teaching methods exclude \"when\" to teach from concern.", "labels": [], "entities": []}, {"text": "They simply exhaust all the budget continuously from the beginning, which is wasteful and causes a heavy workload of the human teacher.", "labels": [], "entities": []}, {"text": "An affordable dialogue policy learning with human teaching should require a lighter workload and economically utilize teaching budget.", "labels": [], "entities": []}, {"text": "Furthermore, as for safety and efficiency evaluation, previous works have been observing the training curves and testing curves to tell which one is better, or evaluate policy performance after certain dialogues of training, which are subjective and error prone).", "labels": [], "entities": []}, {"text": "Our contribution is to address the above problems.", "labels": [], "entities": []}, {"text": "We propose a complete framework of companion teaching, and develop various teaching schemes which combine different teaching strategies and teaching heuristics together, to answer the questions of \"how\" and \"when\" to teach to achieve affordable dialogue policy learning (section 2).", "labels": [], "entities": []}, {"text": "Specifically, a novel failure prognosis based teaching heuristic is proposed, where MultiTask Learning (MTL) is utilized to predict the dialogue success reward (section 3).", "labels": [], "entities": []}, {"text": "To avoid the drawbacks of traditional subjective measurements, we propose two evaluation metrics, called Risk Index (RI) and Hitting Time (HT), to quantify the safety and efficiency of on-line policy learning respectively (section 4).", "labels": [], "entities": [{"text": "Risk Index (RI)", "start_pos": 105, "end_pos": 120, "type": "METRIC", "confidence": 0.9425781011581421}, {"text": "Hitting Time (HT)", "start_pos": 125, "end_pos": 142, "type": "METRIC", "confidence": 0.8602644562721252}]}, {"text": "Simulation experiments showed, with the proposed companion teaching schemes, sustainable and affordable on-line dialogue policy learning has been achieved (section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "Three objectives are set for our experiments: (1) Observing the effect of multitask DQN; (2) Contrasting the performances of different teaching schemes (strategies and heuristics) under the companion teaching framework; (3) Observing the safety and efficiency issues under sparse user feedback scenarios.", "labels": [], "entities": []}, {"text": "Our experiments are conducted with the Dialogue State Tracking Challenge 2 (DSTC2) dataset, which is on restaurant information domain).", "labels": [], "entities": [{"text": "Dialogue State Tracking Challenge 2 (DSTC2) dataset", "start_pos": 39, "end_pos": 90, "type": "DATASET", "confidence": 0.6412127812703451}]}, {"text": "The human user is emulated by an agenda-based user simulator with error model (, while the human teacher is emulated by a pre-trained policy model with success rate of about 0.78 through multitask DQN approach without teaching.", "labels": [], "entities": []}, {"text": "A rule-based tracker is used for dialogue state tracking ().", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8272268176078796}]}, {"text": "The semantic parser is implemented according to an SVM-based method proposed by.", "labels": [], "entities": []}, {"text": "The natural language generator is implemented and modified based on an RNNLG toolkit.", "labels": [], "entities": [{"text": "RNNLG toolkit", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8992599844932556}]}, {"text": "In our experiments, all dialogues are limited to twenty turns.", "labels": [], "entities": []}, {"text": "The \"dialogue success\" is judged by the user simulator according to whether all user goals are satisfied.", "labels": [], "entities": []}, {"text": "And for policy learning, we set a small per-turn penalty of one to encourage short interactions, i.e. R turn = \u22121, and a large dialogue success reward of thirty to appeal to successful interactions, i.e. R succ = 30 , and the discount factor \u03b3 is set to one.", "labels": [], "entities": [{"text": "policy learning", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.8927391469478607}, {"text": "R turn = \u22121", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9561005473136902}]}, {"text": "summarizes the heuristics studied in our experiments, together with corresponding configurations which are chosen empirically.", "labels": [], "entities": []}, {"text": "To compare effects of different teaching schemes on safety dimension, we use the Risk Index (RI) in section 4.1 to quantitatively measure each training process.", "labels": [], "entities": [{"text": "Risk Index (RI)", "start_pos": 81, "end_pos": 96, "type": "METRIC", "confidence": 0.9357024431228638}]}, {"text": "We set the empirical safety threshold as 65% here.", "labels": [], "entities": [{"text": "safety threshold", "start_pos": 21, "end_pos": 37, "type": "METRIC", "confidence": 0.8750708401203156}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "As RIs implies, schemes composed with EAPC strategy is much safer than those composed with other strategies.", "labels": [], "entities": [{"text": "RIs", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.5669756531715393}]}, {"text": "As for teaching heuristics, FP-T, SUT and SUT&FPT are three relatively safer heuristic accompanying different strategies.", "labels": [], "entities": [{"text": "FP-T", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8993749022483826}, {"text": "FPT", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9315195083618164}]}, {"text": "One exception is that Early teaching looks more suitable for CA.", "labels": [], "entities": [{"text": "CA", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.960929811000824}]}, {"text": "A possible explanation is that when the teacher gives critique earlier, the student will mind its behavior earlier so that increase safety.", "labels": [], "entities": [{"text": "safety", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9637158513069153}]}, {"text": "shows the training curves of on-line: RIs of learning processes under different teaching schemes.", "labels": [], "entities": [{"text": "RIs", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.8729711174964905}]}, {"text": "The least risky teaching scheme is annotated with * . For comparing different teaching heuristics with fixed teaching strategy, the smallest RIs in each column are bold and underlined, the 2 nd smallest ones are bold only, and the 3 rd smallest ones are underlined only.", "labels": [], "entities": []}, {"text": "See abbreviations of schemes in section 2.1 and 2.2.", "labels": [], "entities": []}, {"text": "learning process under EAPC with various heuristics.", "labels": [], "entities": [{"text": "EAPC", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.9078335165977478}]}, {"text": "Among all 18 teaching schemes, EAPC+SUT is the safest teaching scheme which reduces about 78% risk of no-teaching learning.", "labels": [], "entities": [{"text": "EAPC+SUT", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.5324095586935679}]}, {"text": "We use Hitting Time (HT) in section 4.2 to measure the efficiency of learning process under different teaching schemes.", "labels": [], "entities": [{"text": "Hitting Time (HT)", "start_pos": 7, "end_pos": 24, "type": "METRIC", "confidence": 0.80860515832901}]}, {"text": "The empirical satisfactory target success rate for the student is 70% in our experimental settings.", "labels": [], "entities": []}, {"text": "contains all HTs of learning process under 18 teaching schemes.", "labels": [], "entities": []}, {"text": "Intuitively, The number in the table reflect the number of sessions at which the model achieves target success rate.", "labels": [], "entities": []}, {"text": "As it shows, not any teaching scheme will improve the learning efficiency.", "labels": [], "entities": []}, {"text": "If the teacher intervenes at an improper time, it will distract system or confuse system even with aright guidance.", "labels": [], "entities": []}, {"text": "But teaching when a potential failure exists (F- PT) is always good for improving learning efficiency.", "labels": [], "entities": [{"text": "F- PT)", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9391198009252548}]}, {"text": "EAPC+SUT&FPT is the teaching scheme that leads to the most efficient learning process in our experiments.", "labels": [], "entities": [{"text": "FPT", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.986112654209137}]}, {"text": "gives some example test curves and fitted empirical learning curves of learning process under EAPC with various heuristics.", "labels": [], "entities": [{"text": "EAPC", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.8948685526847839}]}], "tableCaptions": [{"text": " Table 2: RIs of learning processes under differ- ent teaching schemes. The least risky teaching  scheme is annotated with  *  . For comparing differ- ent teaching heuristics with fixed teaching strate- gy, the smallest RIs in each column are bold and  underlined, the 2 nd smallest ones are bold only,  and the 3 rd smallest ones are underlined only. See  abbreviations of schemes in section 2.1 and 2.2.", "labels": [], "entities": []}, {"text": " Table 3: HTs of test curves of different teaching  schemes. The most efficient teaching scheme is  annotated with  *  . For comparing different teach- ing heuristics with fixed strategy on efficiency is- sue, the smallest HTs in each column are bold and  underlined, and the 2 nd smallest bold only. See  abbreviations of schemes in section 2.1 and 2.2.", "labels": [], "entities": []}, {"text": " Table 4: RIs & HTs of learning processes un- der EAPC strategy and different heuristics when  user feedback rate is 30%. See abbreviations of  schemes in section 2.1 and 2.2.", "labels": [], "entities": []}]}