{"title": [{"text": "An Empirical Analysis of Edit Importance between Document Versions", "labels": [], "entities": [{"text": "Edit Importance between Document Versions", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.6564588725566864}]}], "abstractContent": [], "introductionContent": [{"text": "In collaborative content authoring, multiple authors make changes to the same document, which results in the final version being significantly different from the base draft.", "labels": [], "entities": [{"text": "collaborative content authoring", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.723837653795878}]}, {"text": "Often there is a need to review the edits made to the original document, which can be along and arduous task.", "labels": [], "entities": []}, {"text": "Tools like Microsoft Word (mic) and Adobe Acrobat (ado) provide reviewers with a list of edits, in the form of insertions and deletions.", "labels": [], "entities": []}, {"text": "While helpful, these tools do not differentiate between the different types of edits, or consider the varying impact of edits.", "labels": [], "entities": []}, {"text": "For instance, change from numeric '18' to word 'eighteen' maybe a minor change and less crucial for the author to review, as compared to an edit that All authors have equal contribution in this paper.", "labels": [], "entities": []}, {"text": "This work was done as part of an internship at Adobe Research alters the facts of the document.", "labels": [], "entities": []}, {"text": "Thus, in our work, we focus on automatically inferring the impact/change introduced by edits, and predict the perceived importance of such edits by authors.", "labels": [], "entities": []}, {"text": "In this paper, we perform a linguistic analysis of how humans evaluate the significance of edits while reviewing documents.", "labels": [], "entities": []}, {"text": "Our algorithm assigns scores to edits between two versions of a document, which indicate the significance of the specified edit as perceived by the reviewer.", "labels": [], "entities": []}, {"text": "We demonstrate the efficacy of our approach by comparing our algorithm generated edit importance scores with the human perceived ground truth importance, established through a Mechanical Turk survey.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss the various experiments performed, and the results obtained.", "labels": [], "entities": []}, {"text": "Baselines: To the best of our knowledge, our work is the first that attempts to infer importance/impact of text edits between document versions.", "labels": [], "entities": []}, {"text": "Thus, we did not have established baselines to compare against.", "labels": [], "entities": []}, {"text": "Instead we use the following features as baselines: \u2022 Sentence Order -Sentences are ordered according to their position in the document, with the first sentence assigned most importance.", "labels": [], "entities": []}, {"text": "This is also the order in which a reviewer would normally view edits.", "labels": [], "entities": []}, {"text": "\u2022 Readability Score -Sentence edit importance scores are calculated as being proportional to the change in their readability scores.", "labels": [], "entities": [{"text": "Readability Score -Sentence edit importance scores", "start_pos": 2, "end_pos": 52, "type": "METRIC", "confidence": 0.7476042211055756}]}, {"text": "\u2022 Text Rank -We expect sentences with higher TextRank score to have higher edit importance attached to them.", "labels": [], "entities": []}, {"text": "outlines the Spearman \u03c1 correlation of our model and the above baselines with human judgments.", "labels": [], "entities": [{"text": "Spearman \u03c1 correlation", "start_pos": 13, "end_pos": 35, "type": "METRIC", "confidence": 0.9055173397064209}]}, {"text": "We are able to achieve significant improvement over the baselines using the full set of features.", "labels": [], "entities": []}, {"text": "An interesting observation was that sentence position correlates poorly with the human: Performance of each feature group in isolation.", "labels": [], "entities": []}, {"text": "Numbers reflect the performance (Spearman \u03c1) of the model when using only the specified feature group, relative to the performance when using all features.", "labels": [], "entities": [{"text": "Spearman \u03c1)", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9917681415875753}]}, {"text": "This indicates that the order/position of sentences has negligible effect on the perceived significance.", "labels": [], "entities": []}, {"text": "Both readability score and TextRank have reasonable influence on edit importance, though neither of them is able to match the performance of the full set of features.", "labels": [], "entities": [{"text": "TextRank", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.8882003426551819}]}], "tableCaptions": [{"text": " Table 1: Spearman \u03c1 of the predicted impor- tance score with the human annotated importance  scores.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9403186440467834}, {"text": "impor- tance score", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.9058307111263275}]}, {"text": " Table 2: Performance of each feature group in iso- lation. Numbers reflect the performance (Spear- man \u03c1) of the model when using only the specified  feature group, relative to the performance when  using all features.", "labels": [], "entities": [{"text": "Spear- man \u03c1)", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.9848914742469788}]}]}