{"title": [{"text": "An Insight Extraction System on BioMedical Literature with Deep Neural Networks", "labels": [], "entities": [{"text": "Insight Extraction", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.81805419921875}]}], "abstractContent": [{"text": "Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them.", "labels": [], "entities": []}, {"text": "As new scientific findings appear across a large collection of biomedical publications , our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them.", "labels": [], "entities": [{"text": "biomedical knowledge extraction", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.6800355911254883}]}, {"text": "Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature.", "labels": [], "entities": []}, {"text": "Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9987601041793823}, {"text": "relation extraction", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7672147750854492}]}], "introductionContent": [{"text": "Biomedical literature offers a rich set of knowledge sources to discover important facts and find associations among them.", "labels": [], "entities": []}, {"text": "For instance, MED-LINE contains over 18 million references to articles published since 1946 and sourced from over 5500 journals worldwide).", "labels": [], "entities": [{"text": "MED-LINE", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.8291221857070923}]}, {"text": "Two major processing tasks performed on the biomedical text are: (1) identify and classify biomedical entities (NER) into predefined categories such as proteins, genes, or diseases, and (2) infer pair-wise relationships among named entities e.g., protein-protein interaction (), gene-protein, and medical problem-treatment.", "labels": [], "entities": []}, {"text": "This paper presents a system that processes biomedical text to extract two specific types of relationships among biomedical entities: (a) causeeffect and (b) correlation.", "labels": [], "entities": []}, {"text": "This system is motivated by the need to better automate biomedical knowledge extraction and identify important information from them, as new scientific findings appear across a large collection of publications.", "labels": [], "entities": [{"text": "biomedical knowledge extraction", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.7061580419540405}]}, {"text": "For instance, given user sleep patterns, existing biomedical research can be better utilized to provide insights: inform about potential effect (e.g., \"diabetes\", \"obesity\") due to the cause (e.g., \"sleep disorder\") and suggest appropriate treatment.", "labels": [], "entities": []}, {"text": "Since biomedical articles usually have title and abstract summarizing the contents of the full-text article, we focus on extracting the two relationship types from them.", "labels": [], "entities": []}, {"text": "Unfortunately, mining this summary data still poses several key challenges.", "labels": [], "entities": []}, {"text": "Similar to full-text, this data comprises unstructured text with domain-specific vocabulary, issues of synonymy (e.g., \"heart attack\" vs. \"myocardial infarction\"), acronyms, abbreviations and rapidly evolving terminology due to new scientific discoveries.", "labels": [], "entities": []}, {"text": "While the titles are short and informative, they do not contain the key information that would be contained in the abstract.", "labels": [], "entities": []}, {"text": "Many of these challenges are also applicable for biomedical relation extraction.", "labels": [], "entities": [{"text": "biomedical relation extraction", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.81104842821757}]}, {"text": "Further, identifying particular relation types is challenging because relations are expressed as discontinuous spans of text , and the relation types are typically application-specific.", "labels": [], "entities": []}, {"text": "Finally, there is often little consensus on how to best annotate relation types resulting in lack of high quality annotated corpora for training.", "labels": [], "entities": []}, {"text": "In this study, we develop neural networks with novel similarity modeling for better causality/correlation relation extraction, as we map the extraction task into a representational similarity measurement task in the vector space.", "labels": [], "entities": [{"text": "causality/correlation relation extraction", "start_pos": 84, "end_pos": 125, "type": "TASK", "confidence": 0.6695653915405273}]}, {"text": "Our approach innovates in that it explicitly measures both relational and contextual similarity among representations of named entities, entity relations and contexts.", "labels": [], "entities": []}, {"text": "Our system also provides a novel combination of recognizing named entities, predicting relationships (insights) between extracted entities, and ranking the output.", "labels": [], "entities": [{"text": "predicting relationships (insights) between extracted entities", "start_pos": 76, "end_pos": 138, "type": "TASK", "confidence": 0.8767238259315491}]}, {"text": "We conduct human evaluations of the system to show it is able to extract insights with high human acceptance accuracy, and on a SemEval task evaluation its causality/correlation relation extraction compares favorably against previous state-of-the-art work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.740161120891571}]}], "datasetContent": [{"text": "Experiments are conducted on two datasets: our own dataset of medical/health publications annotated on Universal Human Relevance System (UHRS), a crowdsourcing platform for end-to-end system evaluation; and SemEval-2010 task 8 dataset for training and evaluation of our relation extraction component: 1.", "labels": [], "entities": [{"text": "SemEval-2010 task 8 dataset", "start_pos": 207, "end_pos": 234, "type": "DATASET", "confidence": 0.5934990048408508}, {"text": "relation extraction", "start_pos": 270, "end_pos": 289, "type": "TASK", "confidence": 0.7467685043811798}]}, {"text": "The first dataset consists of 100 publications from recent biomedical/health journals, which are then annotated on UHRS to evaluate our system.", "labels": [], "entities": [{"text": "UHRS", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.977744996547699}]}, {"text": "In order to ensure high-quality human annotations, provides an annotation interface on UHRS, which displays instructions, title/abstract texts of publications and a list of top ranked extracted insights from the system output.", "labels": [], "entities": [{"text": "UHRS", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.9503028392791748}]}, {"text": "For fair evaluation the order of extracted insights is randomized then we ask expert annotators with suitable background to verify the correctness of each.", "labels": [], "entities": []}, {"text": "2.) defines 9 relation types between named entities: Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, EntityOrigin, Entity-Destination, Component-Whole, Member-Collection and Message-Topic, and a tenth relation type Other when two named enti-ties do not have the first 9 relations.", "labels": [], "entities": []}, {"text": "SemEval-2010 dataset consists of 10, 717 sentences, with 8, 000 for training and 2, 717 for test.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9225108921527863}]}, {"text": "The dataset is human annotated, and each instance provides one sentence which includes two named entities and a relation type between the two entities.", "labels": [], "entities": []}, {"text": "Since our system focuses on extracting insights, we only use Cause-Effect subset of SemEval-2010 dataset as the positive training/testing examples and treat the remaining 9 categories data such as Content-Container, Message-Topic as negatives.", "labels": [], "entities": [{"text": "SemEval-2010 dataset", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.8904183506965637}]}, {"text": "We use this dataset for training and evaluating our relation extraction component (Sec. 5) only.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8146401643753052}]}, {"text": "Two loss functions are adopted to train relation extraction neural network models.", "labels": [], "entities": [{"text": "relation extraction neural network", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.8323217183351517}]}, {"text": "For contextual similarity model (Sec. 5.3), a hinge loss is used.", "labels": [], "entities": []}, {"text": "The training objective is to minimize the following loss, summed over examples x, y gold : where input x represents an entity pair ( A, B) plus its sentence context, y gold is the ground truth label and y is the model predicted label.", "labels": [], "entities": []}, {"text": "Both y and y gold indicate the relation type with directionality (e.g. directional causality).", "labels": [], "entities": []}, {"text": "w represents weights of contextual similarity model with BiL-STM, function f w (x, y ) outputs the model predicted label value, function f w (x, y gold ) outputs the model ground truth label value, and n is the number of training examples.", "labels": [], "entities": []}, {"text": "For relational similarity model (Sec. 5.2), a Bayesian Personalized Ranking (BPR) loss) is used.", "labels": [], "entities": [{"text": "Bayesian Personalized Ranking (BPR) loss", "start_pos": 46, "end_pos": 86, "type": "METRIC", "confidence": 0.8132542542048863}]}, {"text": "The label of the relational similarity model is binary because the BPR loss ranks positive inputs above negative inputs, thereby requiring the supervision signal to distinguish positives from negatives.", "labels": [], "entities": [{"text": "BPR loss", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9222299754619598}]}, {"text": "Due to BPR loss's ranking nature, each training instance of the relational similarity model include one positive input (x, R + ) and one negative input (x, R \u2212 ).", "labels": [], "entities": []}, {"text": "Given a positive correlation/causality input ( R + ), we generate negative training examples by matching the input x with each of the negative relation labels ( R \u2212 ).", "labels": [], "entities": []}, {"text": "BPR loss is shown to be better tailored for ranking tasks empirically (: where \u03c3 is the sigmoid function, function f w (x, R) represents the relational similarity model with BiLSTM, and outputs a similarity score for ranking purpose (Sec. 5.2).", "labels": [], "entities": [{"text": "BPR loss", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.6243315190076828}]}, {"text": "In all experiments, we perform optimization using RMSProp (Tieleman and Hinton, 2012) with backpropagation (Bottou, 1998) and a learning rate fixed to 10 \u22124 and a momentum parameter 0.9.", "labels": [], "entities": []}, {"text": "We preprocess both datasets with Stanford CoreNLP toolkit).", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.9263395071029663}]}, {"text": "We tokenize, lowercase, sentence split and dependency parse all words of both datasets.", "labels": [], "entities": [{"text": "sentence split", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7014879137277603}, {"text": "dependency parse", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.704250380396843}]}, {"text": "We set LSTM hidden state dim = 500.", "labels": [], "entities": []}, {"text": "Two sets of d = 300-dimension word embeddings are utilized.", "labels": [], "entities": []}, {"text": "The first one is 300-dimension GloVe word embeddings () trained on 840 billion tokens; for better biomedical/health domain adaptation, we also train second word embeddings using the GloVe toolkit on biomedical research articles with over 1 billion tokens.", "labels": [], "entities": [{"text": "biomedical/health domain adaptation", "start_pos": 98, "end_pos": 133, "type": "TASK", "confidence": 0.6577868700027466}]}, {"text": "We do not update word embeddings in all experiments.", "labels": [], "entities": []}, {"text": "During system deployment, we only initialize input words with the medical word embeddings if they do not exist in GloVe embeddings' vocabulary.", "labels": [], "entities": []}, {"text": "We also concatenate embeddings of both input words and their head words on dependency trees as input for relation extraction models.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8071023523807526}]}, {"text": "We follow the task settings and compute F1-score with the official evaluation script only on Cause-Effect subset of SemEval-2010 data, then the best model based on F1 is selected for final system deployment.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9973257780075073}, {"text": "SemEval-2010 data", "start_pos": 116, "end_pos": 133, "type": "DATASET", "confidence": 0.7929977178573608}]}, {"text": "We set a distance limit and do not extract relations between two named entities if the distance is larger than 15.", "labels": [], "entities": []}, {"text": "Human Evaluation of the Entire System.", "labels": [], "entities": []}, {"text": "We firstly provide a full end-to-end evaluation of the system on UHRS with human annotators.", "labels": [], "entities": [{"text": "UHRS", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.9425954818725586}]}, {"text": "For each biomedical publication, top 10 candidate insights from the system are listed for further inspection.", "labels": [], "entities": []}, {"text": "The annotators are required to understand the texts, carefully inspect each insight, finally either accept it if it is one of the article insights or simply reject it  Full System Baseline System: Human evaluation results of the full system and a baseline system on UHRS.", "labels": [], "entities": [{"text": "UHRS", "start_pos": 266, "end_pos": 270, "type": "DATASET", "confidence": 0.9750741720199585}]}, {"text": "We show the acceptance accuracy for each of the top ten positions given both systems' output lists.", "labels": [], "entities": [{"text": "acceptance", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9044644832611084}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.7681756615638733}]}, {"text": "We primarily focus on the first 1 and 3 positions, namely Precision@1 and Precision@3.", "labels": [], "entities": []}, {"text": "We also evaluate the relation extraction component (Sec. 5) on Cause-Effect subset of SemEval-2010 dataset.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8443012535572052}, {"text": "SemEval-2010 dataset", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.8734617829322815}]}, {"text": "Note our causality/correlation relation extraction component is not supposed to be a general purpose one, since our system only focuses on insight extraction of biomedical/health literature.", "labels": [], "entities": [{"text": "causality/correlation relation extraction", "start_pos": 9, "end_pos": 50, "type": "TASK", "confidence": 0.6596405148506165}, {"text": "insight extraction of biomedical/health literature", "start_pos": 139, "end_pos": 189, "type": "TASK", "confidence": 0.836312574999673}]}, {"text": "We compare our relation extraction models against previous work on the CauseEffect subset of the data, shows our relational similarity model, without the use of sparse features or external resources such as WordNet, outperforms recent state-of-the-art treeLSTM model (.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7449848651885986}, {"text": "CauseEffect subset of the data", "start_pos": 71, "end_pos": 101, "type": "DATASET", "confidence": 0.8436593413352966}, {"text": "WordNet", "start_pos": 207, "end_pos": 214, "type": "DATASET", "confidence": 0.9313486814498901}]}, {"text": "It also shows BiGRU model is reasonably competitive on this dataset, which is why we use it in our baseline system for comparison purpose.", "labels": [], "entities": [{"text": "BiGRU", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8614892959594727}]}, {"text": "Equation 8 and 9 from within the contextual similarity model (Sec. 5.3).", "labels": [], "entities": []}, {"text": "Given four sentences in the test set of SemEval-2010 data, the model predicts that all provided entity pairs (in bold) have the causality/correlation relation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 138, "end_pos": 149, "type": "METRIC", "confidence": 0.931018590927124}]}, {"text": "From we observe the model is able to do its expected job: it can recognize important clues words, such as \"result in\", \"produce\", \"generated by\" and \"cause\"; the model produces attention weights (each \u2208) to tell the importance of clue words for causality/correlation relation extraction.", "labels": [], "entities": [{"text": "causality/correlation relation extraction", "start_pos": 245, "end_pos": 286, "type": "TASK", "confidence": 0.6050891816616059}]}, {"text": "We also observe the model tends to focus more on prepositions of clue words, such as \"by\" of \"generated by\" and \"in\" of \"result in\", this is probably because we use head words as extra inputs (Sec. 7) to the model.", "labels": [], "entities": []}, {"text": "We lastly provide case study of our system.", "labels": [], "entities": []}, {"text": "We show two biomedical articles' titles and abstracts as examples, with only necessary omissions to remove irrelevant texts due to the space limit.", "labels": [], "entities": []}, {"text": "Given Case 1, our system outputs the top insight \"the slow negative shift of the DC potential \u2192 increased cortical excitability\" with a score of 0.71.", "labels": [], "entities": []}, {"text": "Given Case 2, our system outputs top 3 insights: \"excessive drinking \u2192 skin cancer\" with a score of 0.55, \"excessive drinking \u2192 alcohol\" with a score of 0.43, and \"excessive drinking \u2192 sunburn\" with a score of 0.31.", "labels": [], "entities": []}, {"text": "The above examples show that our system can provide reasonable insights from biomedical text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Test results (F1 score) on the Cause- Effect subset() of SemEval-2010 dataset. Re- sults are grouped as 1) Top 3 participating teams  in SemEval-2010 competition; 2) Baseline Bi- GRU model; 3) Recent state-of-the-art treeLSTM  model (Miwa and Bansal, 2016); 4) Our work.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9846263229846954}, {"text": "SemEval-2010 dataset", "start_pos": 67, "end_pos": 87, "type": "DATASET", "confidence": 0.932460367679596}]}, {"text": " Table 3: Visualization of model attention weights atten given four SemEval-2010 test sentences.", "labels": [], "entities": [{"text": "SemEval-2010 test sentences", "start_pos": 68, "end_pos": 95, "type": "DATASET", "confidence": 0.7331955631573995}]}]}