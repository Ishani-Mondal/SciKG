{"title": [{"text": "Agent-Aware Dropout DQN for Safe and Efficient On-line Dialogue Policy Learning", "labels": [], "entities": [{"text": "Safe and Efficient On-line Dialogue Policy Learning", "start_pos": 28, "end_pos": 79, "type": "TASK", "confidence": 0.6965294948646}]}], "abstractContent": [{"text": "Hand-crafted rules and reinforcement learning (RL) are two popular choices to obtain dialogue policy.", "labels": [], "entities": [{"text": "reinforcement learning (RL)", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.637887442111969}]}, {"text": "The rule-based policy is often reliable within predefined scope but not self-adaptable, whereas RL is evolvable with data but often suffers from a bad initial performance.", "labels": [], "entities": []}, {"text": "We employ a companion learning framework to integrate the two approaches for on-line dialogue policy learning, in which a pre-defined rule-based policy acts as a teacher and guides a data-driven RL system by giving example actions as well as additional rewards.", "labels": [], "entities": []}, {"text": "A novel agent-aware dropout Deep Q-Network (AAD-DQN) is proposed to address the problem of when to consult the teacher and how to learn from the teacher's experiences.", "labels": [], "entities": []}, {"text": "AAD-DQN, as a data-driven student policy, provides (1) two separate experience memories for student and teacher, (2) an uncertainty estimated by dropout to control the timing of consultation and learning.", "labels": [], "entities": [{"text": "AAD-DQN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6849491596221924}]}, {"text": "Simulation experiments showed that the proposed approach can significantly improve both safety and efficiency of on-line policy optimization compared to other companion learning approaches as well as supervised pre-training using static dialogue corpus.", "labels": [], "entities": [{"text": "safety", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9912130236625671}, {"text": "on-line policy optimization", "start_pos": 113, "end_pos": 140, "type": "TASK", "confidence": 0.7027382254600525}]}], "introductionContent": [{"text": "A task-oriented spoken dialogue system (SDS) is a system that can continuously interact with a human to accomplish a predefined task through speech.", "labels": [], "entities": [{"text": "task-oriented spoken dialogue system (SDS)", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.6812171978609902}]}, {"text": "Dialogue manager, which maintains the dialogue state and decides how to respond, is the core of an SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.946182906627655}]}, {"text": "In this paper, we focus on the dialogue policy.", "labels": [], "entities": []}, {"text": "At the early research, the spoken dialogue systems assume observable dialogue states.", "labels": [], "entities": []}, {"text": "Dialogue policy is simply a set of hand-crafted mapping rules from state to machine action.", "labels": [], "entities": []}, {"text": "This is referred to as rule-based policy, which often has acceptable performance but has no ability of self-adaption.", "labels": [], "entities": []}, {"text": "Nowadays rule-based policy is popular in commercial dialogue systems.", "labels": [], "entities": []}, {"text": "However, in real world scenarios, unpredictable user behavior, inevitable automatic speech recognition, and spoken language understanding errors make it difficult to maintain the true dialogue state and make the decision.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7494766116142273}]}, {"text": "Hence, in recent years, there is a research trend towards statistical dialogue management.", "labels": [], "entities": [{"text": "statistical dialogue management", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.8699478904406229}]}, {"text": "A well-founded theory for this is the partially observable Markov decision process (POMDP) (), which can provide robustness to errors from the input module and automatic policy optimization by reinforcement learning.", "labels": [], "entities": []}, {"text": "Most POMDP based policy learning research is usually carried out using either user simulator or employed users.", "labels": [], "entities": [{"text": "POMDP based policy learning research", "start_pos": 5, "end_pos": 41, "type": "TASK", "confidence": 0.7263152837753296}]}, {"text": "The trained policy is not guaranteed to work well in real world scenarios.", "labels": [], "entities": []}, {"text": "Therefore, on-line policy training has been of great interest).", "labels": [], "entities": []}, {"text": "Recently, proposed two qualitative metrics to measure on-line policy learning: safety and efficiency.", "labels": [], "entities": [{"text": "safety", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9939126372337341}]}, {"text": "Safety reflects whether the initial policy can satisfy the quality-of-service requirement in real-world scenarios during the online policy learning period.", "labels": [], "entities": [{"text": "Safety", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9839658737182617}]}, {"text": "Efficiency reflects how long it takes for the on-line policy training algorithm to reach a satisfactory performance level.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9885631799697876}]}, {"text": "Most traditional RL-based policy training suf-fers poor initial performance, i.e. causes the safety problem.", "labels": [], "entities": [{"text": "RL-based policy", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.9500279128551483}, {"text": "safety", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9629838466644287}]}, {"text": "In light of above, proposed a safe and efficient on-line policy optimization framework, i.e. companion teaching (CT), in which a human teacher is added in the classic POMDP.", "labels": [], "entities": [{"text": "POMDP", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.9021374583244324}]}, {"text": "The teacher has two missions: one is to show example actions, another is to act as a critic to give the student extra reward which can make the learning of policy more efficient.", "labels": [], "entities": []}, {"text": "The example actions not only make the learning safer but also can be directly used by the training of the student policy.", "labels": [], "entities": []}, {"text": "However, there are costs to the teaching of a human teacher.", "labels": [], "entities": []}, {"text": "Based on CT, companion learning (CL) framework is proposed to integrate rule-based policy and RL-based policy, resulting in safe and efficient on-line policy learning.", "labels": [], "entities": []}, {"text": "Here, the rule-based policy acts as a virtual teacher which replaces the human teacher in CT.", "labels": [], "entities": []}, {"text": "There area few differences between these two kinds of teachers.", "labels": [], "entities": []}, {"text": "First, because it has no marginal cost when it's deployed, the rule teacher can be consulted at anytime if needed.", "labels": [], "entities": []}, {"text": "On the other hand, the rule policy is not as good as the human teacher, therefore it's important to determine when and how much the student policy depends on the rule teacher.", "labels": [], "entities": []}, {"text": "Here, we propose an agent-aware dropout Deep Q-Network (AAD-DQN) as the student statistical policy, which provides (1) two separate experience replay pools for student and teacher, (2) an uncertainty estimated by dropout which can be used to control the timing of consultation and learning.", "labels": [], "entities": []}, {"text": "In summary, our main contributions are threefolds: (1) Companion learning (CL) framework was proposed to integrate rule-based policy and RL-based policy.", "labels": [], "entities": []}, {"text": "(2) An agent-aware dropout Deep Q-Network (AAD-DQN) was proposed as the statistical student policy.", "labels": [], "entities": []}, {"text": "(3) Compared with other companion teaching approaches as well as supervised pre-training using static dialogue corpus, CL with AAD-DQN can achieve better performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most previous work on the evaluation of RL-based dialogue policy optimization focuses on the final performance (FP) when the system converges to a steady level.", "labels": [], "entities": [{"text": "RL-based dialogue policy optimization", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.9453359991312027}, {"text": "final performance (FP)", "start_pos": 93, "end_pos": 115, "type": "METRIC", "confidence": 0.7827680230140686}]}, {"text": "However, for on-line policy optimization, it's important to measure the learning process.", "labels": [], "entities": [{"text": "on-line policy optimization", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6479228834311167}]}, {"text": "Except for FP, we proposed two quantitative metrics: safety loss and efficiency loss.", "labels": [], "entities": [{"text": "FP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9160727262496948}, {"text": "safety loss", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9767817854881287}, {"text": "efficiency loss", "start_pos": 69, "end_pos": 84, "type": "METRIC", "confidence": 0.935537725687027}]}, {"text": "Our experiments have three objectives: (1) Comparing our proposed dropout DQN in Algorithm 1 with some baselines when there is no teacher.", "labels": [], "entities": []}, {"text": "Comparing CL with other two baselines when the teacher gets involved, and investigating the benefits of our proposed agent-aware experience replay.", "labels": [], "entities": []}, {"text": "(3) Visually analyzing the differences in behaviors between the rule-based teacher policy and the optimized student policy.", "labels": [], "entities": []}, {"text": "An agenda-based user simulator () with error model () was implemented to emulate the behavior of the human user, and a rule-based policy with 0.695 success rate described in section 3.2 was used as the teacher in our experiments.", "labels": [], "entities": []}, {"text": "The purpose of the user's interacting with SDS is to find restaurant information in the Cambridge (UK) area.", "labels": [], "entities": [{"text": "Cambridge (UK) area", "start_pos": 88, "end_pos": 107, "type": "DATASET", "confidence": 0.9160958409309388}]}, {"text": "This domain has 7 slots of which 4 can be used by the system to constrain the database search.", "labels": [], "entities": []}, {"text": "The summary action space consists of 16 summary actions.", "labels": [], "entities": []}, {"text": "More details are described in Appendix A. For reward, at each turn, an extrinsic reward of \u22120.05 is given to the student policy.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9656776189804077}]}, {"text": "At the end of the dialogue, a reward of +1 is given for dialogue success.", "labels": [], "entities": []}, {"text": "The maximal extra reward \u03b4 is 0.05.", "labels": [], "entities": [{"text": "maximal extra reward \u03b4", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.941221296787262}]}, {"text": "For each set-up, 10000 dialogues are used for training, the moving dialogue success rate is recorded with a window size of 1000.", "labels": [], "entities": []}, {"text": "The final results are the average of 40 runs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The quantitative evaluation results of different methods. Here final performance (FP) is the  success rate of last 2000 dialogues. The FP of Dropout DQN 32 0.749 is used as the ideal performance  S i for computing efficiency loss, and the performance of the rules 0.695 is used as the acceptable  performance S a for computing safety loss.", "labels": [], "entities": [{"text": "final performance (FP)", "start_pos": 73, "end_pos": 95, "type": "METRIC", "confidence": 0.7206737756729126}, {"text": "FP", "start_pos": 145, "end_pos": 147, "type": "METRIC", "confidence": 0.9988414645195007}, {"text": "Dropout DQN 32 0.749", "start_pos": 151, "end_pos": 171, "type": "DATASET", "confidence": 0.8043318092823029}]}, {"text": " Table 2: Evaluation results of different ordered  rules. As a reference, the performance of opti- mised student policy is success rate 0.767, #turn  5.10 and reward 0.5124.", "labels": [], "entities": [{"text": "success rate 0.767", "start_pos": 123, "end_pos": 141, "type": "METRIC", "confidence": 0.9611955285072327}]}]}