{"title": [{"text": "Entity Linking for Queries by Searching Wikipedia Sentences", "labels": [], "entities": [{"text": "Entity Linking for Queries by Searching Wikipedia Sentences", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7163888141512871}]}], "abstractContent": [{"text": "We present a simple yet effective approach for linking entities in queries.", "labels": [], "entities": []}, {"text": "The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query.", "labels": [], "entities": []}, {"text": "Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and related-ness among candidate entities as well as their related entities, to rank the candidates under a regression based framework.", "labels": [], "entities": []}, {"text": "The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result.", "labels": [], "entities": []}, {"text": "First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query.", "labels": [], "entities": []}, {"text": "Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles.", "labels": [], "entities": [{"text": "query sensitive prior probability", "start_pos": 26, "end_pos": 59, "type": "METRIC", "confidence": 0.5879624262452126}]}, {"text": "We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset.", "labels": [], "entities": [{"text": "ERD14 dataset", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9655825197696686}, {"text": "GERDAQ dataset", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9686967134475708}]}, {"text": "Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9996908903121948}, {"text": "ERD14 dataset", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9863659739494324}, {"text": "GERDAQ dataset", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9823672473430634}]}], "introductionContent": [{"text": "Query understanding has been an important research area in information retrieval and natural language processing.", "labels": [], "entities": [{"text": "Query understanding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8698461949825287}, {"text": "information retrieval", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7972428798675537}, {"text": "natural language processing", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.650509754816691}]}, {"text": "A key part of this problem is entity linking, which aims to annotate the entities in the query and link them to a knowledge base such as Freebase and * Contribution during internship at Microsoft Research.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7674202919006348}]}, {"text": "This problem has been extensively studied over the recent years (.", "labels": [], "entities": []}, {"text": "The mainstream methods of entity linking for queries can be summed up in three steps: mention detection, candidate generation, and entity disambiguation.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7194832116365433}, {"text": "mention detection", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.6976500004529953}, {"text": "candidate generation", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.6949960589408875}, {"text": "entity disambiguation", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.7155496031045914}]}, {"text": "The first step is to recognize candidate mentions in the query.", "labels": [], "entities": []}, {"text": "The most common method to detect mentions is to search a dictionary collected by the entity alias in a knowledge base and the human-maintained information in Wikipedia (such as anchors, titles and redirects)).", "labels": [], "entities": []}, {"text": "The second step is to generate candidates by mapping mentions to entities.", "labels": [], "entities": []}, {"text": "It usually uses all possible senses of detected mentions as candidates.", "labels": [], "entities": []}, {"text": "Hereafter, we refer to these two steps of generating candidate entities as entity search.", "labels": [], "entities": []}, {"text": "Finally, they disambiguate and prune candidate entities, which is usually implemented with a ranking framework.", "labels": [], "entities": []}, {"text": "There are two main issues in entity search.", "labels": [], "entities": [{"text": "entity search", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8398122191429138}]}, {"text": "First, a mention maybe linked to many entities.", "labels": [], "entities": []}, {"text": "The methods using entity search usually leverage little context information in the query.", "labels": [], "entities": []}, {"text": "Therefore it may generate many completely irrelevant entities for the query, which brings challenges to the ranking phase.", "labels": [], "entities": []}, {"text": "For example, the mention \"Austin\" usually represents the capital of Texas in the United States.", "labels": [], "entities": []}, {"text": "However, it can also be linked to \"Austin, Western Australia\", \"Austin, Quebec\", \"Austin (name)\", \"Austin College\", \"Austin (song)\" and 31 other entities in the Wikipedia page of \"Austin (disambiguation)\".", "labels": [], "entities": [{"text": "Austin College\"", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.9715256690979004}, {"text": "Austin (song)\"", "start_pos": 117, "end_pos": 131, "type": "DATASET", "confidence": 0.8952609300613403}]}, {"text": "For the query \"blake shelton austin lyrics\", Blake Shelton is a singer and made his debut with the song \"Austin\".", "labels": [], "entities": [{"text": "Austin", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9588074088096619}]}, {"text": "The entity search method detects the mention \"austin\" using the dictionary.", "labels": [], "entities": []}, {"text": "However, while \"Austin (song)\" is most related to the context \"blake shelton\" and \"lyrics\", the mention \"austin\" maybe linked to all the above entities as candidates.", "labels": [], "entities": [{"text": "Austin (song)\"", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9335852861404419}]}, {"text": "Therefore candidate gener-ation with entity search generates too many candidates especially fora common anchor text with a large number of corresponding entities.", "labels": [], "entities": []}, {"text": "Second, it is hard to recognize entities with common surface names.", "labels": [], "entities": []}, {"text": "The common methods usually define a feature called \"link-probability\" as the probability that a mention is annotated in all documents.", "labels": [], "entities": []}, {"text": "There is an issue with this probability being static whatever the query is.", "labels": [], "entities": []}, {"text": "We show an example with the query \"her film\".", "labels": [], "entities": []}, {"text": "\"Her (film)\" is a film while its surface name is usually used as a possessive pronoun.", "labels": [], "entities": []}, {"text": "Since the static link-probability of \"her\" from all Wikipedia articles is very low, \"her\" is usually not treated as a mention linked to the entity \"Her (film)\".", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach to generating candidates by searching sentences from Wikipedia articles and directly using the humanannotated entities as the candidates.", "labels": [], "entities": []}, {"text": "Our approach can greatly reduce the number of candidate entities and obtain the query sensitive prior probability.", "labels": [], "entities": []}, {"text": "We take the query \"blake shelton austin lyrics\" as an example.", "labels": [], "entities": []}, {"text": "Below we show a sentence in the Wikipedia page of \"Austin (song)\".", "labels": [], "entities": [{"text": "Austin (song)\"", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.8943623602390289}]}, {"text": "[: A sentence in the page \"Austin (song)\".", "labels": [], "entities": [{"text": "Austin (song)\"", "start_pos": 27, "end_pos": 41, "type": "DATASET", "confidence": 0.8831967860460281}]}, {"text": "In the above sentence, the mentions \"Austin\" and \"Blake Shelton\" in square brackets are annotated to the entity \"Austin (song)\" and \"Blake Shelton\", respectively.", "labels": [], "entities": []}, {"text": "We generate candidates by searching sentences and thus obtain \"Blake Shelton\" as well as \"Austin (song)\" from this example.", "labels": [], "entities": []}, {"text": "We reduce the number of candidates because many irrelevant entities linked by \"austin\" do not occur in returned sentences.", "labels": [], "entities": []}, {"text": "In addition, as previous methods generate candidates by searching entities without the query information, \"austin\" can be linked to \"Austin, Texas\" with much higher static link-probability than all other senses of \"austin\".", "labels": [], "entities": []}, {"text": "However, the number of returned sentences that contain \"Austin, Texas\" is close to the number of sentences that contain \"Austin (song)\" in our system.", "labels": [], "entities": []}, {"text": "We show another example with the query \"her film\" in.", "labels": [], "entities": []}, {"text": "In this sentence, \"Her\", \"romantic\", \"science fiction\", \"comedy-drama\" and \"Spike Jonze\" are annotated to corresponding entities.", "labels": [], "entities": []}, {"text": "As \"Her\" is annotated to \"Her (film)\" by humans in this example, we have strong evidence to annotate it even if it is usually used as a possessive pronoun with very low static link-probability.", "labels": [], "entities": []}, {"text": "[  We obtain the anchors as well as corresponding entities and map them to the query after searching similar sentences.", "labels": [], "entities": []}, {"text": "Then we build a regression based framework to rank the candidates.", "labels": [], "entities": []}, {"text": "We use a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities.", "labels": [], "entities": []}, {"text": "We evaluate our method on the ERD14 and GERDAQ datasets.", "labels": [], "entities": [{"text": "ERD14", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9207687973976135}, {"text": "GERDAQ datasets", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.9464303553104401}]}, {"text": "Experimental results show that our method outperforms state-ofthe-art systems and yields 75.0% and 56.9% in terms of F1 metric on the ERD14 dataset and the GERDAQ dataset respectively.", "labels": [], "entities": [{"text": "F1 metric", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9810283780097961}, {"text": "ERD14 dataset", "start_pos": 134, "end_pos": 147, "type": "DATASET", "confidence": 0.9807789921760559}, {"text": "GERDAQ dataset", "start_pos": 156, "end_pos": 170, "type": "DATASET", "confidence": 0.9782094359397888}]}], "datasetContent": [{"text": "We conduct experiments on the ERD14 and GER-DAQ datasets.", "labels": [], "entities": [{"text": "ERD14", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9285107254981995}, {"text": "GER-DAQ datasets", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.9604074656963348}]}, {"text": "We compare with several baseline annotators and experimental results show that our method outperforms the baseline on these two datasets.", "labels": [], "entities": []}, {"text": "We also report the parameter selection on each dataset and analyze the quality of the candidates using different methods.", "labels": [], "entities": []}, {"text": "ERD14 3 is a benchmark dataset in the ERD Challenge (), which contains both long-text track and short-text track.", "labels": [], "entities": []}, {"text": "In this paper we only focus on the short-text track.", "labels": [], "entities": []}, {"text": "It contains 500 queries as the development set and 500 queries as the test set.", "labels": [], "entities": []}, {"text": "Due to the lack of training set, we use the development set to do the model training and tuning.", "labels": [], "entities": []}, {"text": "This dataset can be evaluated by both Freebase and Wikipedia as the ERD Challenge Organizers provide the Freebase Wikipedia Mapping with one-to-one correspondence of entities between two knowledge bases.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.8858896493911743}, {"text": "Freebase Wikipedia Mapping", "start_pos": 105, "end_pos": 131, "type": "DATASET", "confidence": 0.893336017926534}]}, {"text": "We use Wikipedia to evaluate our results.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9442524909973145}]}, {"text": "GERDAQ 4 is a benchmark dataset to annotate entities to Wikipedia built by.", "labels": [], "entities": [{"text": "GERDAQ 4", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8724054992198944}]}, {"text": "It contains 500 queries for training, 250 for development, and 250 for test.", "labels": [], "entities": []}, {"text": "The query in this dataset is sampled from the KDD-Cup 2005 and then annotated manually.", "labels": [], "entities": [{"text": "KDD-Cup 2005", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.9437580108642578}]}, {"text": "Both name entities and common concepts are annotated in this dataset.", "labels": [], "entities": []}, {"text": "We use average F1 designed by ERD Challenge () as the evaluation metrics.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9728274941444397}, {"text": "ERD Challenge", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8045527338981628}]}, {"text": "Specifically, given a query q, with labeled entities\u02c6Aentities\u02c6 entities\u02c6A = { \u02c6 E 1 , . .", "labels": [], "entities": []}, {"text": ", \u02c6 E n }.", "labels": [], "entities": []}, {"text": "We define the Fmeasure of a set of hypothesized interpretations A = {E 1 , . .", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9971133470535278}]}, {"text": ", E m } as follows: The average F1 of the evaluation set is the average of the F1 for each query: Following the evaluation guideline in ERD14 and GERDAQ, we define recall to be 1.0 if the gold binding of a query is empty and define precision to be 1.0 if the hypothesized interpretation is empty.", "labels": [], "entities": [{"text": "F1", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9870420694351196}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9547671675682068}, {"text": "ERD14", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.9179773926734924}, {"text": "GERDAQ", "start_pos": 146, "end_pos": 152, "type": "DATASET", "confidence": 0.8737618327140808}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9983758926391602}, {"text": "precision", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9988672733306885}]}, {"text": "AIDA (Hoffart et al., 2011) searches the mention using Stanford NER Tagger based on YAGO2.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8232710957527161}, {"text": "Stanford NER Tagger", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8306016723314921}, {"text": "YAGO2", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.8571373820304871}]}, {"text": "We select AIDA as a representative system aiming to entity linking for documents following the work in.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7295820415019989}]}, {"text": "WAT () is the improved version of TagME", "labels": [], "entities": [{"text": "TagME", "start_pos": 34, "end_pos": 39, "type": "TASK", "confidence": 0.5505096912384033}]}], "tableCaptions": [{"text": " Table 3: Feature Set for Candidate Ranking", "labels": [], "entities": []}, {"text": " Table 4: Results on the ERD dataset. Results  of the baseline systems are taken from Table 8  in Cornolti et al. (2016) and reported by the ERD  organizer (Carmel et al., 2014). We only report the  F1 score as precision and recall are not reported  in previous work. *Significant improvement over  state-of-the-art baselines (t-test, p < 0.05).", "labels": [], "entities": [{"text": "ERD dataset", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.7596102654933929}, {"text": "F1 score", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9828728139400482}, {"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.9992789626121521}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9985180497169495}]}, {"text": " Table 5: Results on the GERDAQ dataset. Results  of the baseline systems are taken from", "labels": [], "entities": [{"text": "GERDAQ dataset", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.944107711315155}]}, {"text": " Table 6: Comparison with different candidate gen- eration methods on the ERD dataset. +RF: in- tegrating ranking features extracted by Sentence  Search.", "labels": [], "entities": [{"text": "ERD dataset", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.7863816320896149}, {"text": "RF", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9834766983985901}]}, {"text": " Table 7: Results for the 398 queries which have at  least one labeled entity on the ERD dataset using  different candidate generation methods. C avg is  the average recall of candidates per query. P avg  and R avg are calculated on the final results.", "labels": [], "entities": [{"text": "ERD dataset", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.8642930090427399}, {"text": "recall", "start_pos": 166, "end_pos": 172, "type": "METRIC", "confidence": 0.9777642488479614}]}]}