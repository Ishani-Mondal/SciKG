{"title": [{"text": "Preserving Distributional Information in Dialogue Act Classification", "labels": [], "entities": [{"text": "Preserving Distributional Information in Dialogue Act Classification", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.8831714391708374}]}], "abstractContent": [{"text": "This paper introduces a novel train-ing/decoding strategy for sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.6903402805328369}]}, {"text": "Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction.", "labels": [], "entities": []}, {"text": "This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding.", "labels": [], "entities": []}, {"text": "Our experiments on dialogue act classification demonstrate the effectiveness of this approach.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7964053551355997}]}, {"text": "Even though our underlying neu-ral network model is relatively simple, it outperforms more complex neural models , achieving state-of-the-art results on the MapTask and Switchboard corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue Act (DA) classification is a sequencelabeling task, where a sequence of utterances is mapped into a sequence of DAs.", "labels": [], "entities": [{"text": "Dialogue Act (DA) classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6016062696774801}]}, {"text": "The DAs are semantic classifications of the utterances, and different corpora usually have their own DA labels.", "labels": [], "entities": []}, {"text": "Two of the most popular DA classification datasets are Switchboard (; Jurafsky et al., 1997) and MapTask (.", "labels": [], "entities": [{"text": "DA classification", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9548635184764862}]}, {"text": "There have been many works on DA classification applied to these two datasets; some focus on textual data), while others explore speech data ().", "labels": [], "entities": [{"text": "DA classification", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9851446747779846}]}, {"text": "The classification methods used can be broadly divided into instancebased methods) and sequence-labeling methods (.", "labels": [], "entities": []}, {"text": "Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines.", "labels": [], "entities": []}, {"text": "Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) () and neural networks).", "labels": [], "entities": []}, {"text": "Stolcke et al. employed an HMM, using a Language Model to produce emission probabilities.", "labels": [], "entities": []}, {"text": "The neural models are particularly successful, posting a higher accuracy on Switchboard than the HMM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9987764954566956}]}, {"text": "Specifically, Kalchbrenner and Blunsom (2013) model a DA sequence with a recurrent neural network (RNN) where sentence representations are constructed by means of a convolutional neural network (CNN); treat the labels as latent variables in a generative RNN; Shen and Lee (2016) employ attentional RNNs for the independent prediction of model the DAs in a conversation by means of a hierarchical RNN.", "labels": [], "entities": []}, {"text": "In this paper, we also rely on RNNs, but our architecture is much simpler than the above neural models, while posting competitive results.", "labels": [], "entities": []}, {"text": "Most neural network models for DA classification employ greedy decoding (, as its speed and simplicity support an on-line decoding process (i.e., producing a label immediately after receiving an utterance).", "labels": [], "entities": [{"text": "DA classification", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.9867501556873322}]}, {"text": "For sequential labeling, the DA label in the current time step is very important (.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8294161260128021}, {"text": "DA label", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.883345365524292}]}, {"text": "However, using a greedy approach to connect the current label directly to the next label may degrade performance, because the current predicted label maybe noisy, which in turn leads to the propagation of errors through the sequence (.", "labels": [], "entities": []}, {"text": "Recently, proposed a technique called Scheduled Sampling that tries to solve the label-bias problem by alternating between the predicted label and the correct label during training.", "labels": [], "entities": [{"text": "Scheduled Sampling", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7758007347583771}]}, {"text": "This makes the model gradually adapt to the noisiness of the predicted label.", "labels": [], "entities": []}, {"text": "However, this method still relies upon a single current label, and, by omitting the distribution over the possible labels, this model loses information about the current stage.", "labels": [], "entities": []}, {"text": "In contrast, we propose to condition the next label on a predicted distribution of the current label.", "labels": [], "entities": []}, {"text": "Specifically, we introduce two variants of this idea: the Uncertainty Propagation model and the Average Embedding model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of different strategies to leverage  the current label.", "labels": [], "entities": []}, {"text": " Table 2: Results on MapTask data.", "labels": [], "entities": [{"text": "MapTask data", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9276297092437744}]}, {"text": " Table 3: Results on Switchboard data.", "labels": [], "entities": [{"text": "Switchboard data", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.840662270784378}]}, {"text": " Table 4: Probability that the models recover from a sequence of n prediction mistakes.", "labels": [], "entities": []}]}