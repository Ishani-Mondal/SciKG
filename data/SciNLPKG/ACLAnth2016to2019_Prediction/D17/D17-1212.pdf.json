{"title": [{"text": "Fine-Grained Citation Span Detection for References in Wikipedia", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.8449885845184326}]}], "abstractContent": [{"text": "Verifiability is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content.", "labels": [], "entities": []}, {"text": "For a Wikipedia article, determining the citation span of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing.", "labels": [], "entities": []}, {"text": "We are the first to address the problem of determining the citation span in Wikipedia articles.", "labels": [], "entities": []}, {"text": "We approach this problem by classifying which textual fragments in an article are covered by a citation.", "labels": [], "entities": []}, {"text": "We propose a sequence classification approach where fora paragraph and a citation, we determine the citation span at a fine-grained level.", "labels": [], "entities": [{"text": "sequence classification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.7746597826480865}]}, {"text": "We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain , where we show improvement for all evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Citations uphold the crucial policy of verifiability in Wikipedia.", "labels": [], "entities": []}, {"text": "This policy requires Wikipedia contributors to support their additions with citations from authoritative external sources (web, news, journal etc.).", "labels": [], "entities": []}, {"text": "In particular, it states that \"articles should be based on reliable, third-party, published sources with a reputation for fact-checking and accuracy\" 1 . Not only are citations essential in maintaining reliability, neutrality and authoritative assessment of content in such a collaboratively edited platform; but lack of citations are At the summit of the climb, carpet tacks were thrown onto the road causing as many as thirty riders to puncture, including Gilbert's teammates Cadel Evans and Steve Cummings, while race leader Bradley Wiggins precaution.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.979935884475708}, {"text": "reliability", "start_pos": 202, "end_pos": 213, "type": "METRIC", "confidence": 0.964429497718811}]}, {"text": "As a result, and eventually soloed his way to a fourth career stage victory at the Tour.", "labels": [], "entities": []}, {"text": "Sagan led home a group of four riders almost a minute behind, behind S\u00e1nchez.", "labels": [], "entities": []}, {"text": "Figure 1: Sub-sentence level span for citation in a citing paragraph in a Wikipedia article.", "labels": [], "entities": []}, {"text": "essential signals for core editors for unreliability checks.", "labels": [], "entities": []}, {"text": "However, there are two problems when it comes to citing facts in Wikipedia.", "labels": [], "entities": []}, {"text": "First, there is along tail of Wikipedia pages where citations are missing and hence facts might be unverified.", "labels": [], "entities": []}, {"text": "Second, citations might have different span granularities, i.e., the text encoding the fact(s), for which a citation is intended, might span less than a sentence (see) to multiple sentences.", "labels": [], "entities": []}, {"text": "We denote the different pieces of text which contain a citation marker as fact statements or simply statements.", "labels": [], "entities": []}, {"text": "For example, shows different statements for several citations.", "labels": [], "entities": []}, {"text": "The aim of this work is to automatically and accurately determine citation spans in order to improve coverage ( and to assist editors in verifying citation quality at a fine-grained level.", "labels": [], "entities": [{"text": "coverage", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.970081627368927}]}, {"text": "Earlier work on span determination is mostly concerned with scientific texts), operates at sentence level and exploits explicit authoring cues specific to scientific text.", "labels": [], "entities": [{"text": "span determination", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8514406085014343}]}, {"text": "Although Wikipedia has well formed text, it does not follow explicit scientific guidelines for placing citations.", "labels": [], "entities": []}, {"text": "Moreover, most statements can only be inferred from the citation text.", "labels": [], "entities": []}, {"text": "In this work, we operate at a sub-sentence level, loosely referred to as text fragments, and take a sequence prediction approach using a linear chain CRF ().", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7093590945005417}]}, {"text": "We limit our work to citations referring to web and news sources, as they are accessible online and present the most prominent sources in.", "labels": [], "entities": []}, {"text": "By using recent work on moving window language models) and the structure of the paragraph that includes a citation, we classify sequences of text fragments as text that belong to a given citation.", "labels": [], "entities": []}, {"text": "We are able to tackle all citation span cases as shown in: Varying degrees of citation span granularity in Wikipedia text.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now outline the experimental setup for evaluating the citation span approach and the competitors for this task.", "labels": [], "entities": []}, {"text": "The data and the proposed approaches are made available at the paper URL 3 .  We evaluate the citation span approaches on a random sample of Wikipedia entities (snapshot of 20/11/2016).", "labels": [], "entities": [{"text": "URL", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.8667898774147034}]}, {"text": "For the sampling process, we first group entities based on the number of web or news citations.", "labels": [], "entities": []}, {"text": "). We then sample from the specific groups.", "labels": [], "entities": []}, {"text": "This is due to the inherent differences in citation spans for entities with different numbers of citations.", "labels": [], "entities": []}, {"text": "For instance, entities with a high number of citations tend to have shorter spans per citation.", "labels": [], "entities": []}, {"text": "shows the distribution of entities from the different groups.", "labels": [], "entities": []}, {"text": "From each sampled entity, we extract all citing paragraphs that contain either a web or news citation.", "labels": [], "entities": []}, {"text": "Our sample consists of 509 citing paragraphs from 134 entities.", "labels": [], "entities": []}, {"text": "Furthermore, since a paragraph may have more than one citation, in our sampled citing paragraphs, we have an average of 4.4 citations per paragraph, which finally resulted in 408 unique paragraphs.", "labels": [], "entities": []}, {"text": "shows the stats of the dataset.", "labels": [], "entities": []}, {"text": "We measure the performance of the citation span approaches through the following metrics.", "labels": [], "entities": []}, {"text": "We will denote with W \u2032 the sampled entities, with p = {p c , . .", "labels": [], "entities": []}, {"text": ".} (p c refers to \u27e8p, c\u27e9) the set of sampled paragraphs from e, and with |p| the total items from e.", "labels": [], "entities": []}, {"text": "Mean Average Precision -M AP . First, we define precision for p c as the ratio P (p c ) = |S \u2032 \u2229 St |/|S \u2032 | of fragments present in S \u2032 \u2229 St over S \u2032 . We measure MAP as in Equation 5.", "labels": [], "entities": [{"text": "Mean Average Precision -M AP", "start_pos": 0, "end_pos": 28, "type": "METRIC", "confidence": 0.7311723927656809}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9991198182106018}, {"text": "MAP", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.8160403966903687}]}, {"text": "Recall -R. We measure the recall for p c as the ratio S \u2032 \u2229 St overall fragments in St , R(p c ) = |S \u2032 \u2229 St |/|S t |.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9990724325180054}]}, {"text": "We average the individual recall scores fore \u2208 W \u2032 for the corresponding p.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9895662069320679}]}, {"text": "We measure the number of extra words or extra sub-sentences (denoted with \u2206 wand \u2206 \u03b4 ) added by text fragments that are not part of the ground-truth St . The ratio is relative to the number of words or sub-sentences in the ground-truth for p c . We compute \u2206 wand \u2206 \u03b4 in Equation 7 and 8, respectively.", "labels": [], "entities": []}, {"text": "6 Results and Discussion shows the results for the different approaches on determining the citation span for all span cases shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Dataset statistics for citing paragraphs,  distinguishing between web and news references,  showing the average number of sentences, frag- ments, and covered fragments.", "labels": [], "entities": [{"text": "frag- ments", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.8995178739229838}]}, {"text": " Table 4: Citation span distribution based on the  number of sub-sentences in the citing paragraph.", "labels": [], "entities": []}, {"text": " Table 6: Evaluation results for the different cita- tion span approaches.", "labels": [], "entities": []}, {"text": " Table 7: Evaluation results for the citation span approaches for the different span cases. For the results  of CSP S we compute the relative increase/decrease of F 1 score compared to the best result (based  on F 1) from the competitors. We mark in bold the best results for the evaluation metrics, and indicate  with ** and * the results which are highly significant (p < 0.001) and significant (p < 0.05) based on  t-test statistics when compared to the best performing baselines (CS, IC, CSW, MRF) based on F1 score,  respectively.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9730354150136312}, {"text": "MRF", "start_pos": 497, "end_pos": 500, "type": "METRIC", "confidence": 0.7386319637298584}, {"text": "F1 score", "start_pos": 511, "end_pos": 519, "type": "METRIC", "confidence": 0.9802123308181763}]}]}