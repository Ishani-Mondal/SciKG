{"title": [{"text": "The strange geometry of skip-gram with negative sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood.", "labels": [], "entities": []}, {"text": "We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors.", "labels": [], "entities": []}, {"text": "We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is generally assumed that the geometry of word embeddings is determined by semantic relatedness.", "labels": [], "entities": []}, {"text": "Vectors are assumed to be distributed throughout a K-dimensional space, with specific regions devoted to specific concepts.", "labels": [], "entities": []}, {"text": "We find that vectors trained with the skip-gram with negative sampling (SGNS) algorithm) are not only influenced by semantics but are also strongly influenced by the negative sampling objective.", "labels": [], "entities": []}, {"text": "In fact, far from spanning the possible space, they exist only in a narrow cone in R K . Nevertheless, SGNS vectors have become a foundational tool in NLP and perform as well or better than numerous methods with similar objectives () with respect to evaluations of intrinsic and extrinsic quality (.", "labels": [], "entities": []}, {"text": "SGNS works by training two sets of embeddings: the \"official\" word embeddings and a second set of context embeddings, with one K-dimensional vector in each set for each word in the vocabulary.", "labels": [], "entities": []}, {"text": "The objective tries to make the word vector and context vector closer fora pair of words that actually occur together than for randomly sampled \"negative\" words.", "labels": [], "entities": []}, {"text": "Following training, the word vectors are typically saved; the context vectors are deleted.", "labels": [], "entities": []}, {"text": "Any difference between these two sets of vectors is puzzling, since the sliding window used in training is symmetrical: a word and its context word reverse roles almost immediately.", "labels": [], "entities": []}, {"text": "Indeed, the superficially similar GloVe algorithm () also defines word and context vectors and by default returns the mean of these two vectors.", "labels": [], "entities": []}, {"text": "Previous work has analyzed what the algorithm might be doing in theory, as an approximation to a matrix factorization ().", "labels": [], "entities": []}, {"text": "Other work has considered the empirical effects of some of the more arbitrary-seeming algorithmic choices (.", "labels": [], "entities": []}, {"text": "But we still have relatively little understanding of how the algorithm actually determines parameter values.", "labels": [], "entities": []}, {"text": "In this work we measure geometric properties of SGNS-trained word vectors and their context vectors.", "labels": [], "entities": [{"text": "SGNS-trained word vectors", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.8249794046084086}]}, {"text": "Although the word vectors appear to span K-dimensional space, we find that the SGNS objective results in vectors that are narrowly clustered in a single orthant, and can be made non-negative without significant loss.", "labels": [], "entities": [{"text": "SGNS", "start_pos": 79, "end_pos": 83, "type": "TASK", "confidence": 0.8857601881027222}]}, {"text": "shows two visualizations of SGNS vectors and context vectors.", "labels": [], "entities": []}, {"text": "The context vectors mirror the \"official\" word vectors, with the angle between vectors effectively controlled by the number of negative samples.", "labels": [], "entities": []}, {"text": "We show that this effect is due to negative sampling and not the general embedding objective.", "labels": [], "entities": []}, {"text": "We note that this relationship between vectors is effectively hidden by the commonly-used t-SNE projection).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}