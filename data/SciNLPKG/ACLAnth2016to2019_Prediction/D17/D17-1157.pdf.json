{"title": [{"text": "Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an empirical study of the phrase-based decoding algorithm proposed by Chang and Collins (2017).", "labels": [], "entities": []}, {"text": "The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order.", "labels": [], "entities": []}, {"text": "Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores.", "labels": [], "entities": [{"text": "speed", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.9988610744476318}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9991452693939209}]}], "introductionContent": [{"text": "Phrase-based models () have until recently been a stateof-the-art method for statistical machine translation, and Moses () is one of the most used phrase-based translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.7247361540794373}, {"text": "phrase-based translation", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.6702769845724106}]}, {"text": "Moses uses abeam search decoder based on a dynamic programming algorithm that constructs the target-language sentence from left to right.", "labels": [], "entities": []}, {"text": "Neural machine translation systems), which have given impressive improvements over phrase-based systems, also typically use models and decoders that construct the target-language string in strictly leftto-right order.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.726149320602417}]}, {"text": "Recently, proposed a phrase-based decoding algorithm that processes the source-language string in strictly left-to-right order.", "labels": [], "entities": []}, {"text": "Reordering is implemented by maintaining multiple sub-strings in the target-language, with phrases being used to extend these sub-strings by various operations (see Section 2 fora full description).", "labels": [], "entities": [{"text": "Reordering", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9498405456542969}]}, {"text": "With a fixed distortion limit on reordering, * On leave from Columbia University.", "labels": [], "entities": []}, {"text": "the time complexity of the algorithm is linear in terms of sentence length, and is polynomial time in other factors.", "labels": [], "entities": []}, {"text": "present the algorithm and give a proof of its time complexity, but do not describe experiments, leaving an open question of whether the algorithm is useful in practice.", "labels": [], "entities": []}, {"text": "This paper complements the original paper by studying the algorithm empirically.", "labels": [], "entities": []}, {"text": "In addition to an exact dynamic programming implementation, we study the use of beam search with the algorithm, and another pruning method that restricts the maximum number of target-language strings maintained at any point.", "labels": [], "entities": []}, {"text": "The experiments show that the algorithm is competitive with Moses in terms of both speed and translation quality (BLEU score).", "labels": [], "entities": [{"text": "speed", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9908339381217957}, {"text": "BLEU score)", "start_pos": 114, "end_pos": 125, "type": "METRIC", "confidence": 0.9772371053695679}]}, {"text": "The new decoding algorithm is of interest fora few reasons.", "labels": [], "entities": []}, {"text": "While the experiments in this paper are with phrase-based translation systems, the method could potentially be extended to neural translation, for example with an attention-based model that is in some sense monotonic (left-toright).", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7686384618282318}, {"text": "neural translation", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7951807081699371}]}, {"text": "The decoder maybe relevant to work on simultaneous translation).", "labels": [], "entities": [{"text": "simultaneous translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6434005051851273}]}, {"text": "The ideas maybe applicable to string-to-string transduction problems other than machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7229528576135635}]}, {"text": "This section gives a sketch of the decoding algorithm of.", "labels": [], "entities": []}, {"text": "We first define the phrase-based decoding problem, and then describe the algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "Required for German-to-English Translation Finally, we investigate empirically how many segments (the maximum value of r) are required for translation from German to English.", "labels": [], "entities": [{"text": "German-to-English Translation", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6627583056688309}, {"text": "translation from German to English", "start_pos": 139, "end_pos": 173, "type": "TASK", "confidence": 0.8469422698020935}]}, {"text": "Ina first experiment, we use the system of Chang and Collins (2011) to give exact search for German-to-English translation under a trigram language model with a distortion limit d = 4, and then look at the maximum value for r for each optimal translation.", "labels": [], "entities": [{"text": "German-to-English translation", "start_pos": 93, "end_pos": 122, "type": "TASK", "confidence": 0.5930026173591614}]}, {"text": "Out of 1,821 sentences, 34.9% have a maximum value of r = 1, 62.4% have r = 2, and 2.69% have r = 3.", "labels": [], "entities": []}, {"text": "No optimal translations require a value of r greater than 3.", "labels": [], "entities": []}, {"text": "It can be seen that very few translations require more than 2 segments.", "labels": [], "entities": []}, {"text": "Ina second experiment, we take the reordering system of and test the maximum value for r on each sentence to capture the reordering rules.", "labels": [], "entities": []}, {"text": "It can be seen that over 99% of sentences require a value of r = 3 or less, again suggesting that for at least this language pair, a choice of r = 3 or r = 4 is large enough to capture the majority of reorderings (assuming that the rules of are comprehensive).", "labels": [], "entities": []}], "tableCaptions": []}