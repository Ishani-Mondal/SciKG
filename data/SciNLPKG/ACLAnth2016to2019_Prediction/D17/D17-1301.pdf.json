{"title": [{"text": "Exploiting Cross-Sentence Context for Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7563454310099283}]}], "abstractContent": [{"text": "In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies.", "labels": [], "entities": [{"text": "translation", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9653987884521484}]}, {"text": "In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 155, "end_pos": 187, "type": "TASK", "confidence": 0.8498077293237051}]}, {"text": "First, this history is summarized in a hierarchical way.", "labels": [], "entities": []}, {"text": "We then integrate the historical representation into NMT in two strategies: 1) a warm-start of en-coder and decoder states, and 2) an auxiliary context source for updating decoder states.", "labels": [], "entities": []}, {"text": "Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7032002409299215}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.999153733253479}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has been rapidly developed in recent years).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8011089662710825}]}, {"text": "The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word byword from the vector representation.", "labels": [], "entities": []}, {"text": "Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 171, "end_pos": 208, "type": "TASK", "confidence": 0.7946338752905527}]}, {"text": "The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of * Corresponding Author: Zhaopeng Tu a word.", "labels": [], "entities": []}, {"text": "Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (.", "labels": [], "entities": []}, {"text": "Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document).", "labels": [], "entities": [{"text": "documentlevel translation", "start_pos": 41, "end_pos": 66, "type": "TASK", "confidence": 0.7277840077877045}]}, {"text": "Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence.", "labels": [], "entities": []}, {"text": "These problems are difficult to alleviate using only limited intra-sentence context.", "labels": [], "entities": []}, {"text": "The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion ( and dialogue modeling (.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 160, "end_pos": 177, "type": "TASK", "confidence": 0.7627883851528168}]}, {"text": "The leverage of global context for NMT, however, has received relatively little attention from the research community.", "labels": [], "entities": [{"text": "NMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9134362936019897}]}, {"text": "In this paper, we propose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document.", "labels": [], "entities": []}, {"text": "Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (.", "labels": [], "entities": []}, {"text": "After obtaining the global context, we design several strategies to integrate it into NMT to translate the current sentence: \u2022 Initialization, that uses the history represen-tation as the initial state of the encoder, decoder, or both; \u2022 Auxiliary Context, that uses the history representation as static cross-sentence context, which works together with the dynamic intrasentence context produced by an attention model, to good effect.", "labels": [], "entities": []}, {"text": "\u2022 Gating Auxiliary Context, that adds agate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed initialization and auxiliary context (w/ or w/o gating) mechanisms significantly improve translation performance individually, and combining them achieves further improvement.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Evaluation of translation quality. \"Init\" denotes Initialization of encoder (\"enc\"), decoder  (\"dec\"), or both (\"enc+dec\"), and \"Auxi\" denotes Auxiliary Context. \" \u2020\" indicates statistically significant  difference (P < 0.01) from the baseline NEMATUS.", "labels": [], "entities": []}, {"text": " Table 2: Translation error statistics.", "labels": [], "entities": [{"text": "Translation error", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7572804093360901}]}]}