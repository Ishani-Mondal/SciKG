{"title": [{"text": "Sentiment Lexicon Expansion Based on Neural PU Learning, Double Dictionary Lookup, and Polarity Association", "labels": [], "entities": []}], "abstractContent": [{"text": "Although many sentiment lexicons in different languages exist, most are not comprehensive.", "labels": [], "entities": []}, {"text": "Ina recent sentiment analysis application, we used a large Chinese sentiment lexicon and found that it missed a large number of sentiment words used in social media.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9153115153312683}]}, {"text": "This prompted us to make anew attempt to study sentiment lexicon expansion.", "labels": [], "entities": [{"text": "sentiment lexicon expansion", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.8738866448402405}]}, {"text": "This paper first formulates the problem as a PU learning problem.", "labels": [], "entities": []}, {"text": "It then proposes anew PU learning method suitable for the problem based on a neural network.", "labels": [], "entities": []}, {"text": "The results are further enhanced with anew dictionary lookup technique and a novel polarity classification algorithm.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.6738705337047577}]}, {"text": "Experimental results show that the proposed approach greatly outper-forms baseline methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment lexicons contain words (such as good, beautiful, bad, and awful) that convey positive or negative sentiments.", "labels": [], "entities": [{"text": "Sentiment lexicons contain words (such as good, beautiful, bad, and awful) that convey positive or negative sentiments", "start_pos": 0, "end_pos": 118, "type": "Description", "confidence": 0.7255970402197405}]}, {"text": "They are instrumental for many sentiment analysis applications.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9790843427181244}]}, {"text": "So far many algorithms have been proposed to generate such lexicons (.", "labels": [], "entities": []}, {"text": "These algorithms are either dictionary-based or corpus-based.", "labels": [], "entities": []}, {"text": "In the dictionary-based approach, one exploits synonym and antonym relations in the dictionary to bootstrap a given seed set of sentiment words (), or learns a classifier to classify the gloss of each word in the dictionary).", "labels": [], "entities": []}, {"text": "The corpus-based approach uses various linguistic rules or patterns).", "labels": [], "entities": []}, {"text": "We will discuss these and other existing methods in the related work section.", "labels": [], "entities": []}, {"text": "Despite many existing studies, the problem is far from being solved.", "labels": [], "entities": []}, {"text": "Ina recent application, we used a popular Chinese sentiment lexicon for sentiment classification of Weibo posts (similar to Twitter), and found that it missed a large number of sentiment words.", "labels": [], "entities": [{"text": "sentiment classification of Weibo posts", "start_pos": 72, "end_pos": 111, "type": "TASK", "confidence": 0.860197901725769}]}, {"text": "As the lexicon was compiled using formal text such as news, it misses a large number of sentiment words used in social media.", "labels": [], "entities": []}, {"text": "Due to the informal nature, many \"low class\" words are used in social media but seldom used informal media.", "labels": [], "entities": []}, {"text": "New words are also created constantly.", "labels": [], "entities": []}, {"text": "Note that new words in Chinese are easier to create from individual characters than in other languages.", "labels": [], "entities": []}, {"text": "Thus many of these words are not in the dictionary.", "labels": [], "entities": []}, {"text": "All these prompted us to make anew attempt to study sentiment lexicon expansion.", "labels": [], "entities": [{"text": "sentiment lexicon expansion", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.8668097456296285}]}, {"text": "In this paper, we solve the problem in two steps: (1) identify sentiment words from a given corpus, and (2) classify their polarity.", "labels": [], "entities": []}, {"text": "We formulate Step 1 as a PU learning problem (learning from positive unlabeled examples).", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first such formulation.", "labels": [], "entities": []}, {"text": "This is important because it gives us a formal model to tackle the problem.", "labels": [], "entities": []}, {"text": "PU learning is stated as follows (): given a set P of examples of a particular class (we also use P to denote the class) and a set U of unlabeled examples which contains hidden instances from both classes P and not-P (called N ), we want to build a classifier using P and U to classify the data in U or future test data into the two classes, i.e., P and N (or not-P).", "labels": [], "entities": [{"text": "PU learning", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7804097533226013}]}, {"text": "In our case, P is the existing sentiment lexicon, and U is a set of candidate words from asocial media corpus.", "labels": [], "entities": []}, {"text": "We identify those words in U that are also sentiment words.", "labels": [], "entities": []}, {"text": "A typical PU learning algorithm works by first identifying a small set of reliable N class examples (RN) from the unlabeled set U and then running a supervised learning method (e.g., SVM) it-eratively to add more and more data to the RN set to finally build a classifier (.", "labels": [], "entities": [{"text": "PU learning", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.8736397325992584}]}, {"text": "In this work, we first adapt a popular such approach to an augmented multilayer perceptron (AMP) method and use it to replace SVM, and show that using SVM as the learning method is inferior to using AMP.", "labels": [], "entities": []}, {"text": "However, we can do much better.", "labels": [], "entities": []}, {"text": "We then propose anew PU learning method, called SE-AMP (Spy-based Elimination of P class instances using AMP), which can better exploit the specific nature of our problem.", "labels": [], "entities": []}, {"text": "SE-AMP goes in the opposite direction to the existing approach.", "labels": [], "entities": [{"text": "SE-AMP", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8613051176071167}]}, {"text": "It starts by treating U as the class N (not-P) data, and then runs the AMP learning method on P and U iteratively to gradually remove potential P class instances from U to purify U so that as iterations progress, fewer and fewer P class instances are still in U . We detail the method in Sec.", "labels": [], "entities": []}, {"text": "Note that SE-AMP is general and not limited to our task of sentiment lexicon expansion.", "labels": [], "entities": [{"text": "sentiment lexicon expansion", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.8254525661468506}]}, {"text": "We also propose anew method based on dictionary lookup, called Double dictionary Lookup (DL), to enhance the results from the proposed PU learning method.", "labels": [], "entities": [{"text": "Double dictionary Lookup (DL)", "start_pos": 63, "end_pos": 92, "type": "METRIC", "confidence": 0.8239028453826904}]}, {"text": "The DL method is also in the framework of PU learning.", "labels": [], "entities": []}, {"text": "Our final proposed method for Step 1 is called SE-AMP-DL.", "labels": [], "entities": []}, {"text": "For polarity classification (Step 2, after sentiment words are found), we propose a novel method that is based on polarity association of individual (Chinese) characters in each word.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7700272798538208}]}, {"text": "In summary, this paper has several innovations: 1.", "labels": [], "entities": []}, {"text": "It formulates Step 1 of sentiment lexicon expansion as a PU learning problem.", "labels": [], "entities": [{"text": "sentiment lexicon expansion", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9533553918202718}]}, {"text": "To the best of our knowledge, this is the first such formulation.", "labels": [], "entities": []}, {"text": "2. It proposes anew neural learning method AMP and shows that AMP outperforms the traditional SVM based PU learning approach.", "labels": [], "entities": []}, {"text": "3. It further proposes anew and general PU learning strategy that works in the opposite direction to the popular existing approach to suit our task.", "labels": [], "entities": []}, {"text": "4. It also proposes a double dictionary lookup technique to improve the result further.", "labels": [], "entities": []}, {"text": "5. It proposes a novel polarity classification method to classify the polarity of each word.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7248828113079071}]}, {"text": "Experimental results show that the proposed approach makes considerable improvement over existing baseline methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed technique SE-AMP-DL to expand an existing Chinese sentiment lexicon, DUTIR (Dalian University of Technology, Information Retrieve Lab) Affective Lexicon Ontology (.", "labels": [], "entities": [{"text": "Information Retrieve Lab) Affective Lexicon Ontology", "start_pos": 138, "end_pos": 190, "type": "TASK", "confidence": 0.730523339339665}]}, {"text": "DUTIR lexicon is perhaps the largest Chinese sentiment lexicon with 27466 words.", "labels": [], "entities": [{"text": "DUTIR lexicon", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9390564858913422}]}, {"text": "Although large, since it was built based on formal text such as news, essays, and novels, it does not contain many sentiment words often used in social media as we will see later.", "labels": [], "entities": []}, {"text": "We will also see that many new sentiment words that we discovered are not even in an authoritative Chinese language dictionary.", "labels": [], "entities": []}, {"text": "Thus, compiling a comprehensive sentiment lexicon is needed.", "labels": [], "entities": []}, {"text": "Below, we first evaluate sentiment words discovery (Step 1) and then polarity classification (Step 2).", "labels": [], "entities": [{"text": "sentiment words discovery", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8929488658905029}, {"text": "polarity classification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.6941163390874863}]}, {"text": "We now present and discuss the results.", "labels": [], "entities": []}, {"text": "The syntactic pattern based DP approach performed very poorly because social media posts are brief and the use of patterns to link sentiment words are quite infrequent.", "labels": [], "entities": []}, {"text": "Thus, we will not include its results.", "labels": [], "entities": []}, {"text": "Below, we first compare PMI, S-SVM and S-AMP, and then the results of the proposed PU learning method SE-AMP (without using DL).", "labels": [], "entities": []}, {"text": "The results of incorporating DL are discussed last.", "labels": [], "entities": [{"text": "DL", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.6810568571090698}]}, {"text": "Existing Approach -PMI vs. S-SVM vs. S-AMP: S-SVM and S-AMP use the traditional PU learning approach described in Sec.", "labels": [], "entities": []}, {"text": "10% of P class examples are sampled as spies to produce the RN set.", "labels": [], "entities": []}, {"text": "shows the results of S-SVM and S-AMP iterations.", "labels": [], "entities": []}, {"text": "We observe that the AMP version performs much better than the SVM version.", "labels": [], "entities": []}, {"text": "The first three iterations improve the results.", "labels": [], "entities": []}, {"text": "But after that, the results deteriorate for both systems.", "labels": [], "entities": []}, {"text": "Thus the table only shows the first few iterations since the results keep deteriorating after iteration 2.", "labels": [], "entities": []}, {"text": "The best results are obtained by S-AMP, which is 0.548 in F score.", "labels": [], "entities": [{"text": "S-AMP", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.730587899684906}, {"text": "F score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9803038835525513}]}, {"text": "S-SVM's best F score is only 0.509, which is poorer.", "labels": [], "entities": [{"text": "F score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9867273569107056}]}, {"text": "We also see that the precision and recall of S-AMP are both consistently better than those of S-SVM.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9998353719711304}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9996699094772339}]}, {"text": "We note that these two algorithms cannot catch the best results when they stop following the algorithm in).", "labels": [], "entities": []}, {"text": "We did not use the validation set hereto find the best stopping criterion because even their best results are poorer than those of SE-AMP.", "labels": [], "entities": []}, {"text": "PMI does similarly to S-AMP.", "labels": [], "entities": [{"text": "PMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8265354037284851}]}, {"text": "Proposed Approach -SE-AMP: shows the results of the proposed PU learning approach (SE-AMP).", "labels": [], "entities": [{"text": "Approach", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9844915270805359}]}, {"text": "As noted above, the iteration stopping criterion \u03b4 is determined using the validation set.", "labels": [], "entities": [{"text": "iteration stopping criterion \u03b4", "start_pos": 20, "end_pos": 50, "type": "METRIC", "confidence": 0.7582286149263382}]}, {"text": "Iteration 0 means the classifier uses all unlabeled examples in U as the N set.", "labels": [], "entities": []}, {"text": "Compared with iteration 0 of the traditional strategy (S-AMP), the F score of SE-AMP improves slightly (from 52.0% to 54.8%), but both are low.", "labels": [], "entities": [{"text": "F score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9932847917079926}, {"text": "SE-AMP", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.8455171585083008}]}, {"text": "This is because the reliable N set RN for the traditional approach is too small (not representative of all N class examples) while for the proposed approach, the N set has too many hidden P class instances.", "labels": [], "entities": []}, {"text": "The traditional PU learning tries to include more and more likely N examples iteratively to move to the P direction while the proposed approach doing the opposite, eliminating likely P instances from the unlabeled set U . As the table shows, the results of SE-AMP gets better and better after each iteration (it stops when the stopping condition is met).", "labels": [], "entities": []}, {"text": "Precision, recall and F score all improve consistently, which result in improvements of 12.0%, 8.5% and 9.6% respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9974634647369385}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "F score", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.9805014133453369}]}, {"text": "Compared with the best result of S-AMP, the precision of SE-AMP improves from 55.4% to 67.4%, recall improves from 51.8% to 59.3% and the F score improves from 53.5% to 63.1%.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9997841715812683}, {"text": "SE-AMP", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.8389868140220642}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9998062252998352}, {"text": "F score", "start_pos": 138, "end_pos": 145, "type": "METRIC", "confidence": 0.9922592639923096}]}, {"text": "We now explain why SE-AMP is better than S-AMP.", "labels": [], "entities": [{"text": "SE-AMP", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9142363667488098}]}, {"text": "We believe that the main reason is the high level of noise in P , i.e., many words in P don't have clear sentiments.", "labels": [], "entities": []}, {"text": "The traditional PU learning (S-AMP) tries to add classified N class instances into the RN set in each iteration.", "labels": [], "entities": []}, {"text": "This works fine for the first few iterations but then goes wrong because the noise in P resulted in a lot of hidden P instances added into the RN set.", "labels": [], "entities": []}, {"text": "Then the results deteriorate as more and more wrong instances are added as iterations progress.", "labels": [], "entities": []}, {"text": "In contrast, the pro-  posed SE-AMP removes likely P instances (including those noisy ones) from the U set to obtain a purer and purer N set.", "labels": [], "entities": []}, {"text": "Due to the very conservative setting of the \u03b8 parameter (see Sec.", "labels": [], "entities": []}, {"text": "3.3), the number of words removed from U in each iteration is small, so is the number of spy words removed from S as we can see in.", "labels": [], "entities": []}, {"text": "Thus, U becomes purer and purer slowly, and the validation set helps find a good stopping iteration.", "labels": [], "entities": []}, {"text": "Incorporating Double Dictionary Lookup (DL) -SE-AMP-DL: The double dictionary lookup (DL) method improves the results further.", "labels": [], "entities": [{"text": "SE-AMP-DL", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9075421094894409}]}, {"text": "DL uses the most authoritative Chinese dictionary: The Contemporary Chinese Dictionary.", "labels": [], "entities": [{"text": "The Contemporary Chinese Dictionary", "start_pos": 51, "end_pos": 86, "type": "DATASET", "confidence": 0.8872736394405365}]}, {"text": "However, only 379 out of the 1000 test words are in the dictionary, among which 87 are sentiment words.", "labels": [], "entities": []}, {"text": "As shows, the DL method alone has a high precision but low recall as a lot of sentiment words are not in the dictionary.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9989674091339111}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9991082549095154}]}, {"text": "After correction of the results from SE-AMP by DL, the F score improves from 63.1 of SE-AMP to 65.6 of SE-AMP-DL.", "labels": [], "entities": [{"text": "F score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9916063249111176}, {"text": "SE-AMP", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.8047084808349609}]}, {"text": "Note: We also tried to cleanup the lexicon first using DL to reduce the noise in the P set   before performing the proposed algorithms.", "labels": [], "entities": []}, {"text": "But after cleaning, only 1968 sentiment words out of 4957 remained.", "labels": [], "entities": []}, {"text": "We inspected the result and found that the cleaning removed a lot of true sentiment words, making the P set too small for our algorithms and thus produced much poorer results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of PMI and the traditional PU  learning approach: S-SVM and S-AMP.", "labels": [], "entities": [{"text": "PMI", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9719257950782776}]}, {"text": " Table 2: Results of the proposed SE-AMP.", "labels": [], "entities": []}, {"text": " Table 3: Results with double dictionary lookup.", "labels": [], "entities": []}, {"text": " Table 4: PMI and CPA classification results.", "labels": [], "entities": [{"text": "PMI", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.4794124662876129}, {"text": "CPA classification", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.617946058511734}]}]}