{"title": [{"text": "Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Monolingual Evaluation of Machine Translation", "start_pos": 45, "end_pos": 90, "type": "TASK", "confidence": 0.6488894701004029}]}], "abstractContent": [{"text": "Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators.", "labels": [], "entities": [{"text": "Monolingual evaluation of Machine Translation (MT)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8343529552221298}]}, {"text": "Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.957206130027771}]}, {"text": "On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however.", "labels": [], "entities": []}, {"text": "We subsequently carryout further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9846889972686768}]}, {"text": "Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.9711119532585144}]}], "introductionContent": [{"text": "Despite it being known for sometime now that automatic metrics, such as BLEU (), provide a less than perfect substitute for human assessment), evaluation in MT more often than not still comprises BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9987461566925049}, {"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9815240502357483}, {"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9985617995262146}]}, {"text": "Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent.", "labels": [], "entities": [{"text": "MT", "start_pos": 115, "end_pos": 117, "type": "TASK", "confidence": 0.9177073836326599}]}, {"text": "In recent Conference on Machine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst.", "labels": [], "entities": [{"text": "Machine Translation (WMT) shared tasks", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.8705262882368905}, {"text": "relative ranking (RR)", "start_pos": 106, "end_pos": 127, "type": "METRIC", "confidence": 0.8208774089813232}, {"text": "MT", "start_pos": 160, "end_pos": 162, "type": "TASK", "confidence": 0.8908567428588867}]}, {"text": "Within this set-up, when presented with the same pair of MT output translations, human assessors often disagree with one another's preference, and even their own previous judgment about which translation is better.", "labels": [], "entities": [{"text": "MT output translations", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.875357985496521}]}, {"text": "Low levels of inter-annotator agreement inhuman evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9464602470397949}, {"text": "MT system evaluations", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.8457966248194376}]}, {"text": "As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9837796092033386}]}, {"text": "Direct assessment (DA) () is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges.", "labels": [], "entities": [{"text": "Direct assessment (DA)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7557720959186554}]}, {"text": "DA collects assessments of translations separately in the form of both fluency and adequacy on a 0-100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments ().", "labels": [], "entities": [{"text": "DA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6650809049606323}]}, {"text": "The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessment.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.944313108921051}]}, {"text": "A reference translation is displayed to the human assessor (rendered in gray) and below it the MT output (in black), with the human judge asked to state the degree to which they agree that The black text adequately expresses the meaning of the gray text in English.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.44869789481163025}]}, {"text": "The motivation behind constructing DA as a monolingual MT evaluation are as follows: \u2022 Monolingual assessment of MT opens up the annotation task to a larger pool of genuinely qualified human assessors; \u2022 Crowd-sourced workers are unlikely to make use of information that is not entirely necessary for completing a given task; and are therefore unlikely to use the source language input if the reference is also displayed or to make use of the source input inconsistently; \u2022 Displaying only the source without a reference greatly increases both the difficulty of the task and the time required to complete each annotation, which is too serious a tradeoff when we wish to carryout human assessment on a very large scale; \u2022 Varying levels of proficiency in the source language across different human assessors could contribute to inconsistency in bilingual MT evaluations.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9009376466274261}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.94075608253479}, {"text": "MT evaluations", "start_pos": 854, "end_pos": 868, "type": "TASK", "confidence": 0.9102567136287689}]}, {"text": "Although DA has been shown to overcome the long-standing challenge of lack of reliability inhuman evaluation of MT, the possibility still exists that, although scores collected with DA have been shown to be almost perfectly reliable in selfreplication experiments, both sets of scores, although consistent with each other, could in fact both be biased in the same way.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.9834927320480347}]}, {"text": "include in the design of DA a number of criteria aimed at minimizing such bias: (i) assessment of individual translations in isolation from others to avoid a given system being scored unfairly low due to its translations being assessed more frequently alongside high quality translations); (ii) elicit assessment scores via a Likert-style question without intermediate labeling, motivated by medical research showing patients' ratings of their own health to be highly dependent on the exact wording of descriptors (; (iii) accurate quality control by assessing the consistency of judges with reference only to their own rating distributions, to accurately remove inconsistent crowdsourced data while avoiding removal of data that legitimately diverges from the scoring strategy of a given expert judge; and (iv) score standardization to avoid bias introduced by legitimate variations in scoring strategies.", "labels": [], "entities": []}, {"text": "Despite efforts to avoid bias in, since DA is a monolingual evaluation of MT that operates via comparison of MT output with a reference translation, it is therefore still possible, while avoiding other sources of bias, that DA incurs reference bias where the level of superficial similarity of translations with reference translations results in an unfair gain, or indeed an unfair disadvantage for systems that yield translations that legitimately deviate from the surface form of reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.9247055649757385}]}, {"text": "Following this intuition, carryout an investigation into bias in monolingual evaluation of MT and conclude that in a monolingual setting, human assessors of MT are strongly biased by the reference translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.8916770815849304}, {"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9778159856796265}]}, {"text": "In this paper, we provide further analysis of experiments originally provided in, in addition to further investigation into the degree to which the intuition about reference bias can be supported.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out using the original 100 Chinese to English translations released by, in addition to 70 English to Spanish MT translations (WMT-13 Quality Estimation Task 1.1).", "labels": [], "entities": [{"text": "WMT-13 Quality Estimation Task 1.1", "start_pos": 151, "end_pos": 185, "type": "DATASET", "confidence": 0.6160142838954925}]}, {"text": "Professional translators, entirely blind to the purpose of the study, were employed to post-edit the MT outputs used in the POST-EDIT setting, and were shown the source input document and the MT output document only (no reference translations).", "labels": [], "entities": [{"text": "MT output document", "start_pos": 192, "end_pos": 210, "type": "DATASET", "confidence": 0.6558935840924581}]}, {"text": "Once post-edits had been created, DA was employed in two separate runs on Amazon Mechani- cal Turk, 11 once for GEN-REF and once for POST-EDIT.", "labels": [], "entities": [{"text": "DA", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.5914985537528992}, {"text": "Amazon Mechani- cal Turk", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.9116787314414978}, {"text": "GEN-REF", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9350329637527466}, {"text": "POST-EDIT", "start_pos": 133, "end_pos": 142, "type": "DATASET", "confidence": 0.8744722604751587}]}, {"text": "Besides employing distinct reference translations in the assessment, all other set-up criteria were identical for both evaluation settings, including the conventional segment-level DA setting, where a minimum of 15 human assessments are combined into a mean DA score fora given translation, after strict quality control measures and score standardization have been applied.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average Kappa coefficients and 99% con- fidence intervals reported in Fomicheva and Spe- cia (2016)", "labels": [], "entities": [{"text": "con- fidence intervals", "start_pos": 45, "end_pos": 67, "type": "METRIC", "confidence": 0.8694374710321426}]}, {"text": " Table 2: Percentage of human annotator pairs  in Fomicheva and Specia (2016) with signifi- cant differences in Kappa coefficients for pairs of  annotators shown the same reference translation  (SAME), different reference translations (DIFF) or  the source language input only (SOURCE), total  numbers of annotator comparisons in each case are  provided within parentheses, numbers of annota- tor pairs was 10 for SOURCE, 40 for SAME and  150 for DIFF.", "labels": [], "entities": []}]}