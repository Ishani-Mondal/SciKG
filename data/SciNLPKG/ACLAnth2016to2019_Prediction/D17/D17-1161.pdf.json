{"title": [{"text": "Joint Concept Learning and Semantic Parsing from Natural Language Explanations", "labels": [], "entities": [{"text": "Semantic Parsing from Natural Language Explanations", "start_pos": 27, "end_pos": 78, "type": "TASK", "confidence": 0.8781877358754476}]}], "abstractContent": [{"text": "Natural language constitutes a predominant medium for much of human learning and pedagogy.", "labels": [], "entities": []}, {"text": "We consider the problem of concept learning from natural language explanations, and a small number of labeled examples of the concept.", "labels": [], "entities": [{"text": "concept learning from natural language explanations", "start_pos": 27, "end_pos": 78, "type": "TASK", "confidence": 0.8274197727441788}]}, {"text": "For example, in learning the concept of a phish-ing email, one might say 'this is a phishing email because it asks for your bank account number'.", "labels": [], "entities": []}, {"text": "Solving this problem involves both learning to interpret open-ended natural language statements, as well as learning the concept itself.", "labels": [], "entities": []}, {"text": "We present a joint model for (1) language interpretation (se-mantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms.", "labels": [], "entities": [{"text": "language interpretation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7173195779323578}, {"text": "concept learning (classification)", "start_pos": 85, "end_pos": 118, "type": "TASK", "confidence": 0.7678305983543396}]}, {"text": "Instead, the model prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for parsing.", "labels": [], "entities": []}, {"text": "On a dataset of email-related concepts, this approach yields across-the-board improvements in classification performance , with a 30% relative improvement in F1 score over competitive classification methods in the low data regime.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.985827624797821}]}], "introductionContent": [{"text": "The ability to automatically learn concepts 1 from examples is a core cognitive ability, with applications across diverse domains.", "labels": [], "entities": []}, {"text": "Examples of such concepts include the concept of a 'negative review' in product reviews, the concept of 'check' over the domain of game states in chess, the concept of 'fraud' in credit history analysis, etc.", "labels": [], "entities": [{"text": "credit history analysis", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.6565060317516327}]}, {"text": "Concept learning is generally approached using classification where a concept is any Boolean function on some domain of instances.", "labels": [], "entities": [{"text": "Concept learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8755869269371033}]}, {"text": "methods that can automatically leverage regularities in large amounts of labeled training data.", "labels": [], "entities": []}, {"text": "However, there are two shortcomings of this paradigm.", "labels": [], "entities": []}, {"text": "First, labeling large amounts of data is unnatural compared to how a person might teach another person (e.g., a human secretary) in a similar situation.", "labels": [], "entities": []}, {"text": "For example, for identifying emails about postdoc positions, a university professor might say 'These inquiries usually seek a postdoc opportunity and include a CV', rather than label scores of examples of such emails.", "labels": [], "entities": []}, {"text": "Second, acquiring large quantities of labeled data maybe infeasible because of along tail of concepts that are highly domain or user specific.", "labels": [], "entities": []}, {"text": "For our example of a busy professor, it might be relevant to teach concepts such as 'postdoc seeking emails', 'course related questions from students', etc.", "labels": [], "entities": []}, {"text": "to an email assistant in order to better manage her/his inbox.", "labels": [], "entities": []}, {"text": "However, these concepts might be irrelevant to a general user.", "labels": [], "entities": []}, {"text": "On the other hand, humans can efficiently learn about new concepts and phenomena through language.", "labels": [], "entities": []}, {"text": "In fact, verbal and written language form the basis for much of human learning and pedagogy, as reflected in text-books, lectures and student-teacher dialogues.", "labels": [], "entities": []}, {"text": "Natural language explanations can be a potent mode of supervision, and can alleviate issues of data sparsity by directly encoding relevant knowledge about concepts.", "labels": [], "entities": [{"text": "Natural language explanations", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5917097727457682}]}, {"text": "showsThis email is spam'), feature labeling (e.g.,The word 'Viagra' indicates spam'), model expectations ('Spam emails rarely come from edu extensions'), etc.", "labels": [], "entities": []}, {"text": "However, here we focus on the ability of natural language to express rich and compositional features for characterizing concepts.", "labels": [], "entities": []}, {"text": "In this paper, we address the task of learning concepts from natural language statements and a small number of labeled examples of the concept.", "labels": [], "entities": []}, {"text": "summarizes the outline of our approach.", "labels": [], "entities": []}, {"text": "We map statements to logical interpretations, which can be evaluated in context of new instances.", "labels": [], "entities": []}, {"text": "In doing this, each statement s effectively acts as a binary feature function {z = f s (x) \u2208 {0, 1}} that fires when the interpretation of a statement sis true for an instance x.", "labels": [], "entities": []}, {"text": "The crux of our approach is that correct interpretations of natural language explanations are more likely to be useful in discriminating concepts, and this observation can be used to guide both semantic interpretation and concept learning 2 . In Section 3, we describe our probabilistic latent variable formulation that learns a semantic parser and a concept classifier from labeled examples of the concept.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 194, "end_pos": 217, "type": "TASK", "confidence": 0.7280772477388382}]}, {"text": "The latent variables correspond to evaluations of natural language statements for different instances, and training proceeds via a generalized EM procedure that iteratively (1) estimates evaluations of explanations (marginalizing overall 2 e.g., a parser may associate multiple incorrect interpretations with the statement in (like stringMatch(attachment stringVal ('usually'))), which are unlikely to help in discriminating instances of the concept.", "labels": [], "entities": []}, {"text": "interpretations), and (2) updates the classification and semantic parsing models.", "labels": [], "entities": [{"text": "classification and semantic parsing", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.7012577131390572}]}, {"text": "The inputs to the method consist of a small number of labeled examples and non-examples of a concept, natural language statements explaining the concept, and a domain specific lexicon.", "labels": [], "entities": []}, {"text": "The method does not require labeling sentences with logical forms.", "labels": [], "entities": []}, {"text": "For our empirical evaluation, we focus on personal emails, a practical example of a domain where target concepts are often highly individualized and labeled data is scarce.", "labels": [], "entities": []}, {"text": "The contributions of this work are: \u2022 We introduce the problem of concept learning from natural language.", "labels": [], "entities": [{"text": "concept learning from natural language", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.8503514647483825}]}, {"text": "We also collect a corpus of emails about common email concepts, along with statements from human users explaining these concepts.", "labels": [], "entities": []}, {"text": "\u2022 We provide a method for concept learning and language understanding that can be trained from a small number of labeled concept instances.", "labels": [], "entities": [{"text": "concept learning", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.873505175113678}, {"text": "language understanding", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7224655747413635}]}, {"text": "Thus, we extend supervised semantic parsing by learning from a weaker form of supervision than has previously been explored.", "labels": [], "entities": [{"text": "supervised semantic parsing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6231666008631388}]}, {"text": "\u2022 We demonstrate that for small labeled data, using natural language statements can achieve substantial gains in classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9099473357200623}]}], "datasetContent": [{"text": "In this section, we evaluate the performance of our approach from the perspectives of concept learn-Concept # of emails Prompt CONTACT 167 \"You are writing an email to yourself to personally keep note of a person contact\" EMPLOYEE 149 \"You area boss writing an email to your employee requesting something to be done\" EVENT 138 \"You are writing an email to a friend asking to meetup at some event\" HUMOR 134 \"You are writing an email to a friend that includes something humorous from the Internet\" MEETING 142 \"You are writing an email to a colleague trying to request a meeting about something\" POLICY 146 \"You are writing an office email regarding announcement of some new policy\" REMINDER 154 \"You are writing an email to yourself as a reminder to do something\": Email concepts used in our experiment, together with the prompts used to describe the concept to workers.", "labels": [], "entities": [{"text": "MEETING", "start_pos": 497, "end_pos": 504, "type": "METRIC", "confidence": 0.9128353595733643}, {"text": "POLICY", "start_pos": 595, "end_pos": 601, "type": "METRIC", "confidence": 0.8308562636375427}, {"text": "REMINDER", "start_pos": 682, "end_pos": 690, "type": "METRIC", "confidence": 0.9048482179641724}]}, {"text": "The same prompt was used in both the Generation and Teaching tasks.", "labels": [], "entities": []}, {"text": "CONTACT: Concept learning performance (F1 scores) using n = 10 labeled examples.", "labels": [], "entities": [{"text": "CONTACT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5530192255973816}, {"text": "F1 scores", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9787169098854065}]}, {"text": "Columns indicate different concept learning tasks defined over emails.", "labels": [], "entities": []}, {"text": "* for the rows corresponding to LNL-NB and LNL-LR denotes statistical significance over the best performing non-LNL modeling as well as semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.7523336410522461}]}, {"text": "We first compare our methods against traditional supervised learning methods on the task of learning email-based concepts described in the previous section.", "labels": [], "entities": []}, {"text": "Our baselines include the following models: Text-only models: \u2022 BoW: A logistic regression (LR) classifier over bag-of-words representation of emails \u2022 BoW tf-idf: LR classifier over bag-of-words representation, with tf-idf weighting \u2022 Para2Vec: LR classifier over a distributed representation of documents, using deep neural network approach by.", "labels": [], "entities": []}, {"text": "\u2022 Bigram: LR model also incorporating bigram features, known to be competitive on several text classification tasks ().", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8181795080502828}]}, {"text": "\u2022 ESA: LR model over ESA (Explicit Semantic Analysis) representations of emails (, which describe a text in terms of its Wikipedia topics.", "labels": [], "entities": []}, {"text": "Models incorporating Statements: \u2022 RTE: This uses a Textual Entailment model (based on features from) that computes a score for aligning of each statement to the text of each email.", "labels": [], "entities": [{"text": "RTE", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.6956213712692261}]}, {"text": "A logistic regression is trained over this representation of the data.", "labels": [], "entities": []}, {"text": "\u2022 Keyword filtering: Filters based on keywords are common in email systems.", "labels": [], "entities": [{"text": "Keyword filtering", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.8393948674201965}]}, {"text": "We add this as a baseline by manually filtering statements referring to occurrences of specific keywords.", "labels": [], "entities": []}, {"text": "Such statements compose nearly 30% of the data.", "labels": [], "entities": []}, {"text": "We train a logistic regression over this representation.", "labels": [], "entities": []}, {"text": "shows classification performance of our approaches for Learning from Natural Language (LNL) against baselines described above for n = 10 labeled examples.", "labels": [], "entities": []}, {"text": "The reported numbers are average F1 scores over 10 data draws.", "labels": [], "entities": [{"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9995132684707642}]}, {"text": "We observe that Bigram and bag-of-word methods are the most competitive among the baselines.", "labels": [], "entities": []}, {"text": "On the other hand, Para2Vec doesn't perform well, probably due to the relatively small scale of the available training data, while ESA fails due to the lack of topical associations in concepts.", "labels": [], "entities": [{"text": "ESA", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.6707541346549988}]}, {"text": "However, most significantly, we observe that both LNL-NB (Naive Bayes) and LNL-LR (Logistic Regression) dramatically outperform all baselines for most concepts (except EM-PLOYEE), and show a 30% relative improvement in average F1 over other methods (p < 0.05, Paired Permutation test).", "labels": [], "entities": [{"text": "F1", "start_pos": 227, "end_pos": 229, "type": "METRIC", "confidence": 0.9986477494239807}]}, {"text": "Interestingly, we note that LNL-NB and LNL-LR show similar performance for most concepts.", "labels": [], "entities": [{"text": "LNL-LR", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.8727481961250305}]}, {"text": "For evaluating semantic parsing of natural language statements, we manually annotated the statements in our dataset using the logical language described in Section 3.4.", "labels": [], "entities": [{"text": "semantic parsing of natural language statements", "start_pos": 15, "end_pos": 62, "type": "TASK", "confidence": 0.8653600414594015}]}, {"text": "In, LNL-Gold denotes the classification performance with using these annotated gold parses.", "labels": [], "entities": [{"text": "LNL-Gold", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6289328932762146}]}, {"text": "This corresponds to the hypothetical case where the classifier knows the correct semantic interpretation of each natural language sentence from an oracle.", "labels": [], "entities": []}, {"text": "While this provides a further 10% relative improvement over our proposed models, the results suggest that our weakly supervised method is quite effective in interpreting natural language statements for concept learning, without explicit supervision.", "labels": [], "entities": [{"text": "interpreting natural language statements", "start_pos": 157, "end_pos": 197, "type": "TASK", "confidence": 0.86247219145298}, {"text": "concept learning", "start_pos": 202, "end_pos": 218, "type": "TASK", "confidence": 0.7391319870948792}]}, {"text": "We also observe that LNL models perform significantly better than Keyword filtering (p < 0.05), indicating that the model leverages the expressiveness of our logical language.", "labels": [], "entities": [{"text": "Keyword filtering", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.676682859659195}]}, {"text": "Finally, the last three rows show performance when the LNL methods also utilize BoW representations of the data.", "labels": [], "entities": []}, {"text": "The further gains over the base LNL models suggest that original feature representations and natural language explanations contain complementary information for many concepts.", "labels": [], "entities": []}, {"text": "A significant motivation for this work is the promise of natural language explanations in facilitating concept learning with a relatively small number of examples.", "labels": [], "entities": [{"text": "concept learning", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.8013687431812286}]}, {"text": "shows the dependence of concept learning performance of LNL(-LR) on the number of labeled training examples (size of training set).", "labels": [], "entities": []}, {"text": "We observe that while our approach consistently outperforms the bag-of-words model (BoW), LNL also requires fewer examples to reach near optimal performance, before it plateaus.", "labels": [], "entities": []}, {"text": "In particular, the generalization performance for LNL is more robust than BoW for n < 10.", "labels": [], "entities": [{"text": "BoW", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7734931707382202}]}, {"text": "The performance trajectory for LNL(-NB) is similar, and omitted in the figure for clarity.: Semantic parsing performance (exact match) for proposed weakly supervised methods vs full supervision (completely labeled logical forms) Parsing performance: We next evaluate the parsing performance of our approach, which learns a semantic parser from only concept labels of examples.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.6533666849136353}]}, {"text": "evaluates parsing performance against the gold annotation logical forms for statements.", "labels": [], "entities": []}, {"text": "For this task, we check for exact match of logical forms.", "labels": [], "entities": []}, {"text": "In the table, full supervision refers to traditional training of a semantic parser using complete annotations of statements with their logical forms.", "labels": [], "entities": []}, {"text": "The results report average accuracy over 10-fold CV, and demonstrate that while not comparable to supervised parsing, our weakly supervised approach is relatively effective in learning semantic parsers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9985539317131042}, {"text": "learning semantic parsers", "start_pos": 176, "end_pos": 201, "type": "TASK", "confidence": 0.6722472409407297}]}, {"text": "Further, exact match to gold annotated logical forms is a restrictive measure.", "labels": [], "entities": [{"text": "exact", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9617061614990234}]}, {"text": "Qualitative analysis revealed that even when the predicted and gold annotation logical forms don't match, predicted logical forms are often strongly correlated in terms of evaluation to gold annotations.", "labels": [], "entities": []}, {"text": "e.g., getPhraseMention( email, stringVal('postdoc')) vs getPhraseMention( body, stringVal('postdoc')).", "labels": [], "entities": []}, {"text": "In about 5% of cases, predicted and gold interpretations are different on the surface, but are semantically equivalent (e.g., stringEquals( sender, recipient) vs stringEquals( recipient, sender)).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Email concepts used in our experiment, together with the prompts used to describe the concept to  workers. The same prompt was used in both the Generation and Teaching tasks.", "labels": [], "entities": []}, {"text": " Table 4: Concept learning performance (F1 scores) using n = 10 labeled examples. Columns indicate  different concept learning tasks defined over emails. * for the rows corresponding to LNL-NB and  LNL-LR denotes statistical significance over the best performing non-LNL model", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9739688336849213}]}, {"text": " Table 5: Semantic parsing performance (exact  match) for proposed weakly supervised methods vs  full supervision (completely labeled logical forms)", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8832569420337677}, {"text": "exact  match)", "start_pos": 40, "end_pos": 53, "type": "METRIC", "confidence": 0.9273154139518738}]}]}