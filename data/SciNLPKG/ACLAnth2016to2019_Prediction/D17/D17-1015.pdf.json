{"title": [{"text": "Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space", "labels": [], "entities": [{"text": "Interpreting Spatial Descriptors", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8577578266461691}]}], "abstractContent": [{"text": "We present a model for locating regions in space based on natural language descriptions.", "labels": [], "entities": []}, {"text": "Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence.", "labels": [], "entities": []}, {"text": "All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation.", "labels": [], "entities": [{"text": "object segmentation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7329885363578796}]}, {"text": "To evaluate our model, we construct and release anew dataset consisting of Minecraft scenes with crowdsourced natural language descriptions.", "labels": [], "entities": []}, {"text": "We achieve a 32% relative error reduction compared to a strong neural baseline.", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 17, "end_pos": 41, "type": "METRIC", "confidence": 0.8104764421780905}]}], "introductionContent": [{"text": "In this work, we present a model for grounding spatial descriptors in 3D scenes.", "labels": [], "entities": []}, {"text": "Consider interpreting the instructions: Take the book and put it on the shelf.", "labels": [], "entities": []}, {"text": "One critical element of being able to interpret this sentence is associating the referring expression the book with the corresponding object in the world.", "labels": [], "entities": []}, {"text": "Another important component of understanding the command above is translating the phrase on the shelf to a location in space.", "labels": [], "entities": []}, {"text": "We call such phrases spatial descriptors.", "labels": [], "entities": []}, {"text": "While spatial descriptors are closely related to referring expressions, they are distinct in that they can refer to locations even when there is nothing there.", "labels": [], "entities": []}, {"text": "An intuitive way to model this is to reason over spatial regions as first-class entities, rather than taking an object-centric approach.", "labels": [], "entities": []}, {"text": "Following along tradition of using game environments for AI, we adopt Minecraft as the setting for our work.", "labels": [], "entities": [{"text": "Minecraft", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.9336937665939331}]}, {"text": "Minecraft has previously been used Misty is hanging in the air next to the wooden shelf with the plant on it. for work on planning and navigation, and we expand on this by using it for grounded language understanding.", "labels": [], "entities": [{"text": "Minecraft", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9558249115943909}, {"text": "planning and navigation", "start_pos": 122, "end_pos": 145, "type": "TASK", "confidence": 0.7243355611960093}, {"text": "grounded language understanding", "start_pos": 185, "end_pos": 216, "type": "TASK", "confidence": 0.6228987177213033}]}, {"text": "As a sandbox game, it can be used to construct a wide variety of environments that capture many interesting aspects of the real world.", "labels": [], "entities": []}, {"text": "At the same time, it is easy to extract machine-interpretable representations from the game.", "labels": [], "entities": []}, {"text": "We construct a dataset of Minecraft scenes with natural-language annotations, and propose a task that evaluates understanding spatial descriptors.", "labels": [], "entities": []}, {"text": "Our task is formulated in terms of locating a pink, cube-shaped character named Misty given a scene, a natural language description, and a set of locations to choose from.", "labels": [], "entities": []}, {"text": "An example from our dataset is shown in.", "labels": [], "entities": []}, {"text": "The Minecraft scene representation does not provide ground-truth information about object identity or segmentation, reflecting the fact that perceptual ambiguity is always present in real-world scenarios.", "labels": [], "entities": []}, {"text": "We do, however, assume the availability of 3D depth information (which, for real-world conditions, can be acquired using depth sensors such as RGBD cameras or Li-DAR).", "labels": [], "entities": []}, {"text": "We propose and evaluate a neural network that combines convolutional layers operating over 3D regions in space with recurrent layers for processing language.", "labels": [], "entities": []}, {"text": "Our model jointly learns to segment objects, associate them with words, and understand spatial relationships -all in an end-to-end manner.", "labels": [], "entities": []}, {"text": "We compare with a strong neural baseline and demonstrate a relative error reduction of 32%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 68, "end_pos": 83, "type": "METRIC", "confidence": 0.9374909400939941}]}, {"text": "The dataset and model described in this paper are available online.", "labels": [], "entities": []}], "datasetContent": [{"text": "In evaluating this task, we would like to use a metric that can provide meaningful comparison of our model with baseline and human performance.", "labels": [], "entities": []}, {"text": "The set of all possible locations for Misty is large enough that it is hard even fora human to guess the correct block on the first try, especially when some descriptions are only precise to within 1 or 2 blocks.", "labels": [], "entities": []}, {"text": "The size of this set also varies from scene to scene.", "labels": [], "entities": []}, {"text": "Therefore for our evaluation, we restrict the set {y 1 , . .", "labels": [], "entities": []}, {"text": ", y n } to 6 possible locations: Misty's true location and 5 distractors.", "labels": [], "entities": []}, {"text": "This represents a less ambiguous problem that is much easier for humans, while also allowing for the evaluation of future models that may require an expensive computation for each candidate location considered.", "labels": [], "entities": []}, {"text": "Our procedure for selecting the distractors is designed to ensure that we test both local and global scene understanding.", "labels": [], "entities": []}, {"text": "Each set of six choices is constructed to consist of three clusters of two candidates each.", "labels": [], "entities": []}, {"text": "Each cluster location is anchored to a landmark -we sample a landmark block adjacent to Misty and two additional landmark blocks from the entire scene, such that the pairwise distances between landmarks are at least 4 units.", "labels": [], "entities": []}, {"text": "We then sample one distractor near Misty's landmark and two distractors near both of the other landmarks.", "labels": [], "entities": []}, {"text": "To make our development and test sets, we construct this six-option variation from a subset of our collected data.", "labels": [], "entities": []}, {"text": "For each such example we crowdsource two human solutions using Mechanical Turk.", "labels": [], "entities": []}, {"text": "Examples where both humans answered correctly are partitioned into a development and a test set.", "labels": [], "entities": []}, {"text": "This filtering procedure serves as our primary method of excluding confusing or uninformative descriptions from the evaluation con-(a) When you come in the door, she's on the floor to the right, just in front of the flower.", "labels": [], "entities": []}, {"text": "(b) Misty is facing to the right of the brown door.", "labels": [], "entities": []}, {"text": "(c) If you were to walk through the door that is on the same wall as the table and plank of floating wood, Misty would be to the left of the door.", "labels": [], "entities": []}, {"text": "She is eye level with the plank of wood and floating in front of it.", "labels": [], "entities": []}, {"text": "(d) Misty is in the ground and she is front of door.", "labels": [], "entities": []}, {"text": "(e) Misty is located under a table that is connected to the wall.", "labels": [], "entities": [{"text": "Misty", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.959475040435791}]}, {"text": "She is at ground level.: Five natural-language descriptions sampled at random from our dataset. ditions.", "labels": [], "entities": []}, {"text": "We also collect a third human solution to each example in the development and test sets to get an independent estimate of human performance on our task.", "labels": [], "entities": []}, {"text": "The final dataset consists of 2321 training examples, 120 dev set examples, and 200 test set examples.", "labels": [], "entities": []}, {"text": "The natural-language descriptions across the full dataset use a vocabulary of 1015 distinct tokens (case-insensitive but including punctuation).", "labels": [], "entities": []}, {"text": "The average description length is 19.02 tokens, with a standard deviation of 10.00 tokens.", "labels": [], "entities": []}, {"text": "The large spread partially reflects the fact that some people gave short descriptions that referenced a few landmarks, while others gave sequences of instructions on how to find Misty.", "labels": [], "entities": [{"text": "find Misty", "start_pos": 173, "end_pos": 183, "type": "TASK", "confidence": 0.7085610628128052}]}, {"text": "As a point of comparison, the ReferIt dataset () has a larger vocabulary of 9124 tokens, but a shorter average description length of 3.52 tokens (with a standard deviation of 2.67 tokens).", "labels": [], "entities": [{"text": "ReferIt dataset", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.8940506875514984}]}, {"text": "A random sampling of descriptions from our dataset is shown in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Success rates for our dataset split. Our model is able  to outperform a strong neural baseline (Seq2Emb).", "labels": [], "entities": [{"text": "Success", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9758111238479614}]}, {"text": " Table 3: Development set results for our full model and three  independent ablations.", "labels": [], "entities": []}]}