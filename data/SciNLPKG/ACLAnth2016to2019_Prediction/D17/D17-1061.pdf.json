{"title": [{"text": "Task-Oriented Query Reformulation with Reinforcement Learning", "labels": [], "entities": [{"text": "Task-Oriented Query Reformulation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5669770836830139}]}], "abstractContent": [{"text": "Search engines play an important role in our everyday lives by assisting us in finding the information we need.", "labels": [], "entities": []}, {"text": "When we input a complex query, however, results are often far from satisfactory.", "labels": [], "entities": []}, {"text": "In this work, we introduce a query reformula-tion system based on a neural network that rewrites a query to maximize the number of relevant documents returned.", "labels": [], "entities": []}, {"text": "We train this neural network with reinforcement learning.", "labels": [], "entities": []}, {"text": "The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9415489435195923}]}, {"text": "We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9991773962974548}]}, {"text": "Furthermore, we present a simple method to estimate a conservative upper-bound performance of a model in a particular environment and verify that there is still large room for improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Search engines help us find what we need among the vast array of available data.", "labels": [], "entities": []}, {"text": "When we request some information using along or inexact description of it, these systems, however, often fail to deliver relevant items.", "labels": [], "entities": []}, {"text": "In this case, what typically follows is an iterative process in which we try to express our need differently in the hope that the system will return what we want.", "labels": [], "entities": []}, {"text": "This is a major issue in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.845252275466919}]}, {"text": "For instance, estimate that 28-52% of all the web queries are modifications of previous ones.", "labels": [], "entities": []}, {"text": "To a certain extent, this problem occurs because search engines rely on matching words in the query with words in relevant documents, to Figure 1: A graphical illustration of the proposed framework for query reformulation.", "labels": [], "entities": [{"text": "query reformulation", "start_pos": 202, "end_pos": 221, "type": "TASK", "confidence": 0.7071228474378586}]}, {"text": "A set of documents D 0 is retrieved from a search engine using the initial query q 0 . Our reformulator selects terms from q 0 and D 0 to produce a reformulated query q which is then sent to the search engine.", "labels": [], "entities": []}, {"text": "Documents Dare returned, and a reward is computed against the set of ground-truth documents.", "labels": [], "entities": []}, {"text": "The reformulator is trained with reinforcement learning to produce a query, or a series of queries, to maximize the expected return.", "labels": [], "entities": []}, {"text": "If there is a mismatch between them, a relevant document maybe missed.", "labels": [], "entities": []}, {"text": "One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.", "labels": [], "entities": []}, {"text": "This technique is known as automatic query reformulation.", "labels": [], "entities": [{"text": "query reformulation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.6828274726867676}]}, {"text": "It typically expands the original query by adding terms from, for instance, dictionaries of synonyms such as WordNet, or from the initial set of retrieved documents (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.9562574625015259}]}, {"text": "This latter type of reformulation is known as pseudo (or blind) relevance feedback (PRF), in which the relevance of each term of the retrieved documents is automatically inferred.", "labels": [], "entities": [{"text": "blind) relevance feedback (PRF)", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6372339853218624}]}, {"text": "The proposed method is built on top of PRF but differs from previous works as we frame the query reformulation problem as a reinforcement learning (RL) problem.", "labels": [], "entities": [{"text": "query reformulation problem", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.7859575847784678}]}, {"text": "An initial query is the natural language expression of the desired goal, and an agent (i.e. reformulator) learns to reformulate an initial query to maximize the expected return (i.e. retrieval performance) through actions (i.e. selecting terms fora new query).", "labels": [], "entities": []}, {"text": "The environment is a search engine which produces anew state (i.e. retrieved documents).", "labels": [], "entities": []}, {"text": "Our framework is illustrated in.", "labels": [], "entities": []}, {"text": "The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.", "labels": [], "entities": []}, {"text": "This opens the possibility of training an agent to use a search engine fora task other than the one it was originally intended for.", "labels": [], "entities": []}, {"text": "To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.", "labels": [], "entities": [{"text": "question answering (Q&A)", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8016228250094822}, {"text": "citation recommendation", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.9456462264060974}, {"text": "passage/snippet retrieval", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6588102728128433}]}, {"text": "As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce anew one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.", "labels": [], "entities": [{"text": "TREC-CAR", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.8669180870056152}, {"text": "MS Academic)", "start_pos": 109, "end_pos": 121, "type": "DATASET", "confidence": 0.8915517727533976}]}, {"text": "Furthermore, we present a method to estimate the upper bound performance of our RL-based model.", "labels": [], "entities": []}, {"text": "Based on the estimated upper bound, we claim that this framework has a strong potential for future improvements.", "labels": [], "entities": []}, {"text": "Here we summarize our main contributions: \u2022 A reinforcement learning framework for automatic query reformulation.", "labels": [], "entities": [{"text": "automatic query reformulation", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.6192295948664347}]}, {"text": "\u2022 A simple method to estimate the upper-bound performance of an RL-based model in a given environment.", "labels": [], "entities": []}, {"text": "\u2022 A new large dataset with hundreds of thousands of query/relevant document pairs.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets and implementation details.", "labels": [], "entities": []}, {"text": "We summarize in the datasets.", "labels": [], "entities": []}, {"text": "TREC -Complex Answer Retrieval (TREC-CAR) This is a publicly available dataset automatically created from Wikipedia whose goal is to encourage the development of methods that respond to more complex queries with longer answers (.", "labels": [], "entities": [{"text": "TREC -Complex Answer Retrieval (TREC-CAR)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.6729353666305542}]}, {"text": "A query is the concatenation of an article title and one of its section titles.", "labels": [], "entities": []}, {"text": "The ground-truth documents are the paragraphs within that section.", "labels": [], "entities": []}, {"text": "For example, a query is \"Sea Turtle, Diet\" and the ground truth documents are the paragraphs in the section \"Diet\" of the \"Sea Turtle\" article.", "labels": [], "entities": [{"text": "Sea Turtle\" article", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.8018115013837814}]}, {"text": "The corpus consists of all the English Wikipedia paragraphs, except the abstracts.", "labels": [], "entities": []}, {"text": "The released dataset has five predefined folds, and we use the first three as the training set and the remaining two as validation and test sets, respectively.", "labels": [], "entities": []}, {"text": "Jeopardy This is a publicly available Q&A dataset introduced by.", "labels": [], "entities": [{"text": "Jeopardy This", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8904180824756622}]}, {"text": "A query is a question from the Jeopardy!", "labels": [], "entities": [{"text": "Jeopardy!", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.8758997619152069}]}, {"text": "TV Show and the corresponding document is a Wikipedia article whose title is the answer.", "labels": [], "entities": [{"text": "TV Show", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9686329364776611}]}, {"text": "For example, a query is \"For the last eight years of his life, Galileo was under house arrest for espousing this mans theory\" and the answer is the Wikipedia article titled \"Nicolaus Copernicus\".", "labels": [], "entities": []}, {"text": "The corpus consists of all the articles in the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8775052428245544}]}, {"text": "Microsoft Academic (MSA) This dataset consists of academic papers crawled from Microsoft Academic API.", "labels": [], "entities": [{"text": "Microsoft Academic (MSA) This dataset", "start_pos": 0, "end_pos": 37, "type": "DATASET", "confidence": 0.8964153443064008}, {"text": "Microsoft Academic API", "start_pos": 79, "end_pos": 101, "type": "DATASET", "confidence": 0.8626907269159952}]}, {"text": "The crawler started at the paper and traversed the graph of references until 500,000 papers were crawled.", "labels": [], "entities": []}, {"text": "We then removed papers that had no reference within or whose abstract had less than 100 characters.", "labels": [], "entities": []}, {"text": "We ended up with 480,000 papers.", "labels": [], "entities": []}, {"text": "A query is the title of a paper, and the groundtruth answer consists of the papers cited within.", "labels": [], "entities": []}, {"text": "Each document in the corpus consists of its title and abstract.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the datasets.", "labels": [], "entities": []}, {"text": " Table 2: Results on Test sets. We use R@40 as a reward to the RL-based models.", "labels": [], "entities": [{"text": "R@40", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9655964573224386}]}, {"text": " Table 3: Percentage of relevant terms over all the  candidate terms according to SL-and RL-Oracle.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9622183442115784}, {"text": "SL-and RL-Oracle", "start_pos": 82, "end_pos": 98, "type": "DATASET", "confidence": 0.567246288061142}]}]}