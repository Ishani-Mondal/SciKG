{"title": [{"text": "Modeling Dialogue Acts with Content Word Filtering and Speaker Preferences", "labels": [], "entities": [{"text": "Modeling Dialogue Acts", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9020384351412455}, {"text": "Speaker Preferences", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6711603850126266}]}], "abstractContent": [{"text": "We present an unsupervised model of dialogue act sequences in conversation.", "labels": [], "entities": []}, {"text": "By modeling topical themes as transitioning more slowly than dialogue acts in conversation , our model de-emphasizes content-related words in order to focus on conversational function words that signal dialogue acts.", "labels": [], "entities": []}, {"text": "We also incorporate speaker tendencies to use some acts more than others as an additional predictor of dialogue act prevalence beyond temporal dependencies.", "labels": [], "entities": []}, {"text": "According to the evaluation presented on two dissimilar corpora, the CNET forum and NPS Chat corpus, the effectiveness of each modeling assumption is found to vary depending on characteristics of the data.", "labels": [], "entities": [{"text": "CNET forum", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9410666227340698}, {"text": "NPS Chat corpus", "start_pos": 84, "end_pos": 99, "type": "DATASET", "confidence": 0.9378580252329508}]}, {"text": "De-emphasizing content-related words yields improvement on the CNET corpus, while utilizing speaker tendencies is advantageous on the NPS corpus.", "labels": [], "entities": [{"text": "CNET corpus", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9454928338527679}, {"text": "NPS corpus", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.9647864997386932}]}, {"text": "The components of our model complement one another to achieve robust performance on both corpora and outperform state-of-the-art baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue acts (DAs), or speech acts, represent the intention behind an utterance in conversation to achieve a conversational goal.", "labels": [], "entities": []}, {"text": "Modeling conversations as structured DA sequences is a step toward the automated understanding of dialogue, useful for dialogue agents) and the processing of informal online conversational data.", "labels": [], "entities": []}, {"text": "Distributions of DAs can also be used as predictors of conversational outcome measures such as student learning in tutoring systems) and engagement in meetings (.", "labels": [], "entities": []}, {"text": "Unsupervised models for DA recognition may substitute or aid costly human annotation.", "labels": [], "entities": [{"text": "DA recognition", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9899880290031433}]}, {"text": "We present an unsupervised model of DA sequences in conversation that overcomes limitations of prior models.", "labels": [], "entities": []}, {"text": "The first improvement our model offers is separating out content-related words to emphasize words more relevant to DAs.", "labels": [], "entities": []}, {"text": "DAs are associated more closely with style and function words such as discourse markers and light verbs than with content words, which are more related to the propositional content.", "labels": [], "entities": []}, {"text": "However, separating out content words is not standard in our field.", "labels": [], "entities": [{"text": "separating out content words", "start_pos": 9, "end_pos": 37, "type": "TASK", "confidence": 0.8767607361078262}]}, {"text": "For example, in some rule-based semantic and pragmatic parsing, the content and function of dialogue acts are not formally distinguished in the formalization), especially in domain-specific applications in dialogue systems.", "labels": [], "entities": [{"text": "rule-based semantic and pragmatic parsing", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.6174779236316681}]}, {"text": "A separation between content and function is useful for making cross-domain or cross-task generalizations about conversational processes.", "labels": [], "entities": []}, {"text": "Our model filters out content words by implementing the assumption that conversations proceed against a backdrop of underlying topics that transition more slowly than DAs or that are constant throughout.", "labels": [], "entities": []}, {"text": "Based on a difference in transition speed, two types of language models are learned: foreground language models that capture DA-related words and background language models for content words.", "labels": [], "entities": []}, {"text": "Although some existing models assume a background or domain-specific language model to filter out words unrelated to DAs (, they either require domain labels or do not learn topics underlying conversations.", "labels": [], "entities": []}, {"text": "The second improvement offered by our model is inclusion of speaker preferences, or tendencies to use some DAs more than others.", "labels": [], "entities": []}, {"text": "Prior mod-els of DAs in conversation often rely on the discourse property of conditional relevance, i.e., tendencies for sequences of conversational DAs such as questions followed by answers, greetings followed by greetings, and invitations followed by acceptances.", "labels": [], "entities": []}, {"text": "Though conditional relevance, which motivates the use of Markov models for inducing DA representations, is one stable signal to discover DAs in discourse data, there are reasons that it is a less strong signal than ultimately desired.", "labels": [], "entities": []}, {"text": "One of the reasons is that the DA of an utterance depends not only on the preceding DA, but also on the speaker's personal style ( or preferences for certain DAs.", "labels": [], "entities": [{"text": "DA", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.8306576609611511}]}, {"text": "Our model explicitly accounts for speaker preferences as a factor in determining the DA of an utterance.", "labels": [], "entities": [{"text": "DA", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.8754895329475403}]}, {"text": "Our model also includes additional structure to account for assumptions about distribution and packaging of observed DAs in running discourse.", "labels": [], "entities": []}, {"text": "First, one utterance can involve more than one DA; for example, asking a question in a forum may involve introducing the speaker, explaining the problem, etc.", "labels": [], "entities": []}, {"text": "Hence, we assume that DAs operate on more than one level simultaneously, and an utterance-level DA is a mixture of finer-grained sentence-level DAs.", "labels": [], "entities": []}, {"text": "Second, online conversations often have multi-level structure, branching into multiple conversational threads using replies.", "labels": [], "entities": []}, {"text": "Our model supports conversations that have such multi-level structure.", "labels": [], "entities": []}, {"text": "To illustrate the generalizability of our model, we evaluate it on two corpora with very different characteristics in terms of utterance length, the number of speakers per conversation, and the domain: CNET and NPS Chat Corpus.", "labels": [], "entities": [{"text": "CNET", "start_pos": 202, "end_pos": 206, "type": "DATASET", "confidence": 0.9436729550361633}, {"text": "NPS Chat Corpus", "start_pos": 211, "end_pos": 226, "type": "DATASET", "confidence": 0.9586052695910136}]}, {"text": "We evaluate the DA recognition accuracy of our model and compare the result with other latest models.", "labels": [], "entities": [{"text": "DA recognition", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.9114686846733093}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8645171523094177}]}, {"text": "As we tune the model parameters for each corpus, we use our model as a lens to understand the relationship between the nature of conversations and effective model components for identifying DAs, which may inform future model design.", "labels": [], "entities": []}, {"text": "For the remainder of the paper, we will discuss prior work on dialogue acts and existing models (Section 2) and explain our model design (Section 3).", "labels": [], "entities": []}, {"text": "Then we will describe our evaluation method and corpora (Section 4) and discuss the lessons learned from our empirical investigation (Section 5).", "labels": [], "entities": []}, {"text": "We conclude the paper in Section 6.", "labels": [], "entities": []}, {"text": "makes a distinction between the illocutionary, social intention of an utterance (as seen in the indirect sentence \"Can you pass the salt?\") and the locutionary act of an utterance, which includes the ostensible surface-level meaning of the words.", "labels": [], "entities": []}, {"text": "DAs are commonly thought of as describing illocutionary actions in talk.", "labels": [], "entities": [{"text": "DAs", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8717074990272522}]}, {"text": "Example DAs used in computational systems include yesno question, statement, backchannel, and opinion (. were some of the first to conceptualize DAs with state transitions as a model for conversation.", "labels": [], "entities": []}, {"text": "Similarly, contemporary unsupervised DA models often use a hidden Markov model (HMM) to structure a generative process of utterance sequences (.", "labels": [], "entities": []}, {"text": "It is commonly assumed that each hidden state corresponds to a DA, but different approaches use different representations for states.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our evaluation method and settings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Descriptions of counter matrices.", "labels": [], "entities": []}, {"text": " Table 4: Dialogue act tags in the corpora.", "labels": [], "entities": [{"text": "Dialogue act tags", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6481466790040334}]}]}