{"title": [{"text": "Towards Debate Automation: a Recurrent Model for Predicting Debate Winners", "labels": [], "entities": [{"text": "Predicting Debate", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.868845671415329}]}], "abstractContent": [{"text": "In this paper we introduce a practical first step towards the creation of an automated debate agent: a state-of-the-art recurrent predictive model for predicting debate winners.", "labels": [], "entities": [{"text": "predicting debate winners", "start_pos": 151, "end_pos": 176, "type": "TASK", "confidence": 0.8729176123936971}]}, {"text": "By having an accurate pre-dictive model, we are able to objectively rate the quality of a statement made at a specific turn in a debate.", "labels": [], "entities": []}, {"text": "The model is based on a recurrent neural network architecture with attention, which allows the model to effectively account for the entire debate when making its prediction.", "labels": [], "entities": []}, {"text": "Our model achieves state-of-the-art accuracy on a dataset of debate transcripts annotated with audience favorability of the debate teams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9984048008918762}]}, {"text": "Finally, we discuss how future work can leverage our proposed model for the creation of an automated debate agent.", "labels": [], "entities": []}, {"text": "We accomplish this by determining the model input that will maximize audience favorability toward a given side of a debate at an arbitrary turn.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversational agents area well-researched area of natural language generation (.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6592152814070383}]}, {"text": "Elsewhere in the field of natural language generation, there is work that seeks to generate persuasive text, which is a logical first step towards creating an automated debate agent.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6750909884770712}]}, {"text": "One major deficiency of existing work in this area is its assessment of how convincing (or compelling) apiece of text is; the approaches use theory-driven models of persuasion, rather than being empirically motivated.", "labels": [], "entities": []}, {"text": "Furthermore, none of these works provide a model that can optimize persuasiveness at an arbitrary point in a conversation.", "labels": [], "entities": []}, {"text": "One of the main reasons fora lack of empirically-driven persuasive generation systems is the absence of labeled data.", "labels": [], "entities": []}, {"text": "In order to alleviate this problem (though not directly for the sake of producing an automated debate agent), have introduced a dataset of debate transcripts from the \"Intelligence Squared\" (IQ2) 1 debates.", "labels": [], "entities": [{"text": "Intelligence Squared\" (IQ2) 1 debates", "start_pos": 168, "end_pos": 205, "type": "DATASET", "confidence": 0.5132183395326138}]}, {"text": "In these debates, two teams are present, arguing either for or against a given topic.", "labels": [], "entities": []}, {"text": "For each debate, an audience poll is conducted both prior to and after the debate.", "labels": [], "entities": []}, {"text": "Whichever team has the largest gain in audience support between the pre/post debate polls is the winner.", "labels": [], "entities": []}, {"text": "This is a natural way to account for the fact that some sides of a debate maybe harder to argue than others, and that audience members maybe initially biased given a debate topic.", "labels": [], "entities": []}, {"text": "Because of the sequential nature of debating, a Recurrent Neural Network (RNN) is an attractive choice for modeling the problem.", "labels": [], "entities": []}, {"text": "Rather than just using the final hidded state for prediction, which likely has lost information from early in the debate, we propose to use an attention mechanism ( ) that creates a weighted sum overall hidden states, and is subsequently used for the final prediction.", "labels": [], "entities": [{"text": "prediction", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.9707569479942322}]}, {"text": "We motivate the use of an RNN, as opposed to a temporally flat classifier, for several reasons.", "labels": [], "entities": []}, {"text": "First, using an RNN allows us to naturally incorporate predicting audience favorability at each turn while explicitly modeling the turn sequence.", "labels": [], "entities": []}, {"text": "Logistic regression, on the other hand, would not allow us to model the sequence explicitly.", "labels": [], "entities": []}, {"text": "Secondly, our model allows us to take raw features as input, without having to compute summary statistics necessary for the fea-tures used in the model of.", "labels": [], "entities": []}, {"text": "Finally, since our end goal is debate automation, an RNN is a natural choice for debate turn generation.", "labels": [], "entities": [{"text": "debate automation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8748137950897217}, {"text": "debate turn generation", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7878222266832987}]}, {"text": "There are two major difficulties dealing with the IQ2 dataset: first, since the construction of the dataset is non-trivial, there are only 108 data points, resulting in Zhang et al.'s proposal for leave-one-out (LOO) evaluation.", "labels": [], "entities": [{"text": "IQ2 dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.729992464184761}]}, {"text": "Second, considering the use of an RNN, the sequences are long, with an average length of 246 (and a standard deviation of 67).", "labels": [], "entities": []}, {"text": "In order to overcome this, we incorporate signals based on implicit audience feedback during the debate into the model's loss function.", "labels": [], "entities": []}, {"text": "Instead of just training the model based on error from the audience's final verdict, propogated through a substantial amount of timesteps, there are intermittent errors propogated backward through the network based on audience reactions, such as applauding or laughing.", "labels": [], "entities": []}, {"text": "These internal signals also help regularize the network.", "labels": [], "entities": []}, {"text": "Ina way, they help generalize the hidden representation of the RNN, allowing it to better contain a distributed representation of the audience's favorability towards a given team.", "labels": [], "entities": []}, {"text": "In our proposed model, the audience's opinion is directly a function of the weighted hidden representations.", "labels": [], "entities": []}, {"text": "Since the previous hidden representations are all fixed at a given timestep, and the current hidden representation is directly a function of these previous hidden representations as well as the current input, the audience's current poll depends directly on the timestep's input.", "labels": [], "entities": []}, {"text": "Therefore, at a given timestep, our framework allows us to determine the input that would maximize the audience's favorability toward the orating team.", "labels": [], "entities": []}, {"text": "This is due to the fact that the inputs are themselves representations of a given team's statement at a particular turn in the debate.", "labels": [], "entities": []}, {"text": "We evaluate our model on the dataset from Zhang et al., posting state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.999544084072113}]}, {"text": "Our results show that our proposed regularization technique is imperative for the RNN-based model to perform competitively with the models previously proposed by Zhang et al..", "labels": [], "entities": [{"text": "regularization", "start_pos": 35, "end_pos": 49, "type": "TASK", "confidence": 0.9713313579559326}]}, {"text": "The attention mechanism also contributes to the best performing system.", "labels": [], "entities": []}, {"text": "Afterward, we show how our model can be used to track audience favorability throughout the debate, as well as the aforementioned input optimization, using it in a case study to instruct a debate team about optimal debate strategy at a given turn.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are conducted on the IQ2 dataset ().", "labels": [], "entities": [{"text": "IQ2 dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9212623238563538}]}, {"text": "We use LOO evaluation, resulting in a training set of 107 examples.", "labels": [], "entities": []}, {"text": "The evaluation metric is simply prediction accuracy for debate winners.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.7225251197814941}]}, {"text": "The winning team is based on audience polling.", "labels": [], "entities": []}, {"text": "Polls are conducted before and after the debate, and audience members can vote as being either for or against a given debate topic, as well as being undecided.", "labels": [], "entities": []}, {"text": "The team that has the highest increase in audience support from the pre to post debate poll is the winning team.", "labels": [], "entities": []}, {"text": "The model trains for 100 epochs.", "labels": [], "entities": []}, {"text": "Once training is complete, we test on the held-out data point.", "labels": [], "entities": []}, {"text": "As Zhang et al. note, there are three debates in the dataset that have a tie between the debate teams.", "labels": [], "entities": []}, {"text": "Following their procedure, we do not test on these data points.", "labels": [], "entities": []}, {"text": "However, we still include these examples in the training sets, because our training objective is to predict polls, not debate winners.", "labels": [], "entities": []}, {"text": "The final test accuracy is averaged across the remaining 105 LOO runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9939252734184265}, {"text": "LOO", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.8411355018615723}]}, {"text": "Furthermore, we note that the dataset is effectively balanced, as there are 53 and 52 examples with the two possible labels.", "labels": [], "entities": []}, {"text": "We implement all our models in TensorFlow (. We use the LSTM cell equipped with peephole connections).", "labels": [], "entities": [{"text": "LSTM cell", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8343673646450043}]}, {"text": "This architecture allows the gates to seethe current cell state, along with the hidden state.", "labels": [], "entities": []}, {"text": "We believe that because of the long sequences present in the dataset, it is important to have all the gates", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of LOO evaluation on the IQ2  dataset. See the beginning of Section 5 for an ex- planation of the models.", "labels": [], "entities": [{"text": "IQ2  dataset", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.9641413390636444}]}]}