{"title": [], "abstractContent": [{"text": "Manual data annotation is a vital component of NLP research.", "labels": [], "entities": [{"text": "Manual data annotation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5952081084251404}]}, {"text": "When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation.", "labels": [], "entities": []}, {"text": "In this paper, we explore sequence effects where annotations of an item are affected by the preceding items.", "labels": [], "entities": []}, {"text": "Having assigned one label to an instance, the annotator maybe less (or more) likely to assign the same label to the next.", "labels": [], "entities": []}, {"text": "During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively.", "labels": [], "entities": []}, {"text": "We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets.", "labels": [], "entities": []}, {"text": "We then recommend a simple way to minimise sequence effects.", "labels": [], "entities": []}], "introductionContent": [{"text": "NLP research relies heavily on annotated datasets for training and evaluation.", "labels": [], "entities": []}, {"text": "The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.", "labels": [], "entities": []}, {"text": "When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (), methods for aggregating annotations (), and intuitive user interfaces ().", "labels": [], "entities": []}, {"text": "Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (, although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers).", "labels": [], "entities": []}, {"text": "In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (.", "labels": [], "entities": []}, {"text": "When crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a \"HIT\" (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes.", "labels": [], "entities": []}, {"text": "The order of items is generally fixed across all annotators of an individual HIT (.", "labels": [], "entities": []}, {"text": "In this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals' assessment of an item.", "labels": [], "entities": []}, {"text": "Since all workers seethe instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).", "labels": [], "entities": []}, {"text": "Possible explanations for sequence effects include: Gambler's fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution.", "labels": [], "entities": []}, {"text": "In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True).", "labels": [], "entities": [{"text": "False", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9867281317710876}]}, {"text": "So if they assign 0 to an item, they may approach the next item with a prior belief that it is more likely to be a 1 than a 0.", "labels": [], "entities": []}, {"text": "showed evidence for the gambler's fallacy in decisions of loan officers, asylum judges, and baseball umpires.", "labels": [], "entities": []}, {"text": "Sequential contrast effects: A high quality item may raise the bar for the next item.", "labels": [], "entities": []}, {"text": "On the other hand, a bad item may make the next item seem better in comparison ( Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item.", "labels": [], "entities": [{"text": "Assimilation and anchoring", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7253283758958181}]}, {"text": "If they focus on similarities between the previous and current instance, the annotations show an assimilation effect).", "labels": [], "entities": []}, {"text": "Anchoring effects may decrease as people gain experience and expertise in the task ().", "labels": [], "entities": [{"text": "Anchoring", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9627158045768738}]}], "datasetContent": [{"text": "We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation.", "labels": [], "entities": [{"text": "event ordering", "start_pos": 177, "end_pos": 191, "type": "TASK", "confidence": 0.7662749290466309}, {"text": "affective text analysis", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.629396398862203}, {"text": "machine translation evaluation", "start_pos": 222, "end_pos": 252, "type": "TASK", "confidence": 0.8680007855097452}]}], "tableCaptions": [{"text": " Table 2: Autocorrelation coefficient \u03b2 1 for the AF- FECTIVE dataset.", "labels": [], "entities": [{"text": "Autocorrelation coefficient \u03b2", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.9507033228874207}, {"text": "AF- FECTIVE dataset", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.6954267397522926}]}, {"text": " Table 3: MT adeq dataset: Autocorrelation coeffi- cient \u03b2 1 , showing sequence bias of good, moderate  and bad workers.", "labels": [], "entities": [{"text": "MT adeq dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8032735784848531}, {"text": "Autocorrelation coeffi- cient \u03b2 1", "start_pos": 27, "end_pos": 60, "type": "METRIC", "confidence": 0.7968587974707285}]}, {"text": " Table 5: MT adeq dataset: Translations following a  low quality translation receive a lower score than  those following a good translation: \"All\" is the  mean score of all sentences in the dataset, where  each sentence score is calculated as the average  of N (standardised) worker scores. \"Low\", \"Mid- dle\", and \"High\" are mean scores of sentences  where the previous sentence annotated is of low,  medium and high quality, resp. \"H \u2212 L\" is the dif- ference between the average high and low scores.", "labels": [], "entities": [{"text": "MT adeq dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.7112102607885996}]}]}