{"title": [{"text": "Earth Mover's Distance Minimization for Unsupervised Bilingual Lexicon Induction", "labels": [], "entities": [{"text": "Earth Mover's Distance Minimization", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6431081175804139}, {"text": "Bilingual Lexicon Induction", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6693370838960012}]}], "abstractContent": [{"text": "Cross-lingual natural language processing hinges on the premise that there exists in-variance across languages.", "labels": [], "entities": []}, {"text": "At the word level, researchers have identified such in-variance in the word embedding semantic spaces of different languages.", "labels": [], "entities": []}, {"text": "However , in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision.", "labels": [], "entities": []}, {"text": "By viewing word embedding spaces as distributions , we propose to minimize their earth mover's distance, a measure of divergence between distributions.", "labels": [], "entities": []}, {"text": "We demonstrate the success on the unsupervised bilingual lexicon induction task.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.7335206717252731}]}, {"text": "In addition , we reveal an interesting finding that the earth mover's distance shows potential as a measure of language difference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite tremendous variation and diversity, languages are believed to share something in common.", "labels": [], "entities": []}, {"text": "Indeed, this belief forms the underlying basis of computational approaches to cross-lingual transfer, inter alia), otherwise it would be inconceivable for the transfer to successfully generalize.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7422691881656647}]}, {"text": "Linguistic universals manifest themselves at various levels of linguistic units.", "labels": [], "entities": []}, {"text": "At the word level, there is evidence that different languages represent concepts with similar structure.", "labels": [], "entities": []}, {"text": "Interestingly, as computational models of word semantics, monolingual word embeddings also exhibit isomorphism across languages ().", "labels": [], "entities": []}, {"text": "This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word embeddings cross-lingually.", "labels": [], "entities": []}, {"text": "Learning such a transformation typically calls for cross-lingual supervision from parallel data.", "labels": [], "entities": []}, {"text": "In this paper, we ask the question: Can we uncover the transformation without any cross-lingual supervision?", "labels": [], "entities": []}, {"text": "At first sight, this task appears formidable, as it would imply that a bilingual semantic space can be constructed by using monolingual corpora only.", "labels": [], "entities": []}, {"text": "On the other hand, the existence of structural isomorphism across monolingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool.", "labels": [], "entities": []}, {"text": "We propose such a tool to answer the above question in the affirmative.", "labels": [], "entities": []}, {"text": "The key insight is to view embedding spaces as distributions, and the desired transformation should make the two distributions close.", "labels": [], "entities": []}, {"text": "This naturally calls fora measure of distribution closeness, for which we introduce the earth mover's distance.", "labels": [], "entities": []}, {"text": "Therefore, our task can be formulated as the minimization of the earth mover's distance between the transformed source embedding distribution and the target one with respect to the transformation.", "labels": [], "entities": []}, {"text": "Importantly, the minimization is performed at the distribution level, and hence no word-level supervision is required.", "labels": [], "entities": [{"text": "minimization", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9722847938537598}]}, {"text": "We demonstrate that the earth mover's distance minimization successfully uncovers the transformation for cross-lingual connection, as evidenced by experiments on the bilingual lexicon induction task.", "labels": [], "entities": [{"text": "earth mover's distance minimization", "start_pos": 24, "end_pos": 59, "type": "TASK", "confidence": 0.6044424831867218}, {"text": "bilingual lexicon induction task", "start_pos": 166, "end_pos": 198, "type": "TASK", "confidence": 0.7040227502584457}]}, {"text": "In fact, as an unsupervised approach, its performance turns out to be highly competitive with supervised methods.", "labels": [], "entities": []}, {"text": "Moreover, as an interesting byproduct, the earth mover's distance provides a distance measure that may quantify a facet of language difference.: An illustration of our earth mover's distance minimization formulation.", "labels": [], "entities": [{"text": "earth mover's distance minimization", "start_pos": 168, "end_pos": 203, "type": "TASK", "confidence": 0.650802731513977}]}, {"text": "The subplots on the left schematically visualize Chinese and English embeddings.", "labels": [], "entities": []}, {"text": "Due to isomorphism, there exists a simple transformation G that aligns the two embedding spaces well, as shown on the right.", "labels": [], "entities": []}, {"text": "We expect to find the transformation G by minimizing the earth mover's distance without the need for cross-lingual word-level supervision, because the earth mover's distance holistically measures the closeness between two sets of weighted points.", "labels": [], "entities": []}, {"text": "It computes the minimal cost of transporting one set of points to the other, whose weights are indicated by the sizes of squares and dots.", "labels": [], "entities": []}, {"text": "We show the transport scheme in the right subplot with arrows, which can be interpreted as word translations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first investigate the learning behavior of our WGAN approach, and then present experiments on the bilingual lexicon induction task, followed by a showcase of the earth mover's distance as a language distance measure.", "labels": [], "entities": [{"text": "bilingual lexicon induction task", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.7098566740751266}]}, {"text": "Details of the data sets and hyperparameters are described in Appendices B and C.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: F 1 scores for bilingual lexicon induction on Chinese-English, Spanish-English, Italian-English,  Japanese-Chinese, and Turkish-English. The supervised methods TM and IA require seeds to train, and  are listed for reference. Our EMDOT approach is initialized with the transformation found by WGAN,  and consistently improves on it, reaching competitive performance with supervised methods.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9749749302864075}, {"text": "IA", "start_pos": 177, "end_pos": 179, "type": "METRIC", "confidence": 0.789777398109436}, {"text": "WGAN", "start_pos": 302, "end_pos": 306, "type": "DATASET", "confidence": 0.954505980014801}]}, {"text": " Table 2: The earth mover's distance (EMD), typology dissimilarity, and geographical distance for  Chinese-English, Spanish-English, Italian-English, Japanese-Chinese, and Turkish-English. The EMD  shows correlation with both factors of linguistic difference.", "labels": [], "entities": [{"text": "earth mover's distance (EMD)", "start_pos": 14, "end_pos": 42, "type": "METRIC", "confidence": 0.7057991325855255}, {"text": "geographical distance", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.8286901116371155}, {"text": "EMD", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.9195551872253418}]}]}