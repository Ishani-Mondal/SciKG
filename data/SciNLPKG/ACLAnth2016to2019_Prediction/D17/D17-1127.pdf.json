{"title": [], "abstractContent": [{"text": "Existing studies on semantic parsing mainly focus on the in-domain setting.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.843993604183197}]}, {"text": "We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain.", "labels": [], "entities": [{"text": "cross-domain semantic parsing", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7635998924573263}]}, {"text": "Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges.", "labels": [], "entities": []}, {"text": "By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7528243064880371}]}, {"text": "We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy.", "labels": [], "entities": []}, {"text": "On the popular OVERNIGHT dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embedding can bring significant improvement.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 15, "end_pos": 32, "type": "DATASET", "confidence": 0.8338676691055298}]}], "introductionContent": [{"text": "Semantic parsing, which maps natural language utterances into computer-understandable logical forms, has drawn substantial attention recently as a promising direction for developing natural language interfaces to computers.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8596415817737579}]}, {"text": "Semantic parsing has been applied in many domains, including querying data/knowledge bases, controlling IoT devices, and communicating with robots (.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8288053274154663}]}, {"text": "Despite the wide applications, studies on semantic parsing have mainly focused on the indomain setting, where both training and testing data are drawn from the same domain.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.8357307612895966}]}, {"text": "How to build semantic parsers that can learn across domains remains an under-addressed problem.", "labels": [], "entities": []}, {"text": "In this work, we study cross-domain semantic parsing.", "labels": [], "entities": [{"text": "cross-domain semantic parsing", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7689048449198405}]}, {"text": "We model it as a domain adaptation problem), where we are given some source domains and a target domain, and the core task is to adapt a semantic parser trained on the source domains to the target domain ().", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7457790672779083}]}, {"text": "The benefits are two-fold: (1) by training on the source domains, the cost of collecting training data for the target domain can be reduced, and (2) the data of source domains may provide information complementary to the data collected for the target domain, leading to better performance on the target domain.", "labels": [], "entities": []}, {"text": "This is a very challenging task.", "labels": [], "entities": []}, {"text": "Traditional domain adaptation) only concerns natural languages, while semantic parsing concerns both natural and formal languages.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.8094493746757507}, {"text": "semantic parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7446260452270508}]}, {"text": "Different domains often involve different predicates.", "labels": [], "entities": []}, {"text": "In, from the source BASKETBALL domain a semantic parser can learn the semantic mapping from natural language to predicates like team and season, but in the target SOCIAL domain it needs to handle predicates like employer instead.", "labels": [], "entities": [{"text": "BASKETBALL", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.8608173131942749}]}, {"text": "Worse still, even for the same predicate, it is legitimate to use arbitrarily different predicate symbols, e.g., other symbols like hired by or even predicate1 can also be used for the employer predicate, reminiscent of the symbol grounding problem.", "labels": [], "entities": []}, {"text": "Therefore, directly transferring the mapping from natural language to predicate symbols learned from source domains to the target domain may not be much beneficial.", "labels": [], "entities": []}, {"text": "Inspired by the recent success of paraphrasing based semantic parsing, we propose to use natural language as an intermediate representation for crossdomain semantic parsing.", "labels": [], "entities": [{"text": "paraphrasing based semantic parsing", "start_pos": 34, "end_pos": 69, "type": "TASK", "confidence": 0.6359596773982048}, {"text": "crossdomain semantic parsing", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.796204129854838}]}, {"text": "As shown in, logical forms are converted into canonical utterances in natural language, and semantic parsing is reduced to paraphrasing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 92, "end_pos": 108, "type": "TASK", "confidence": 0.7599944174289703}]}, {"text": "It is the knowledge of paraphrasing, at lexical, syntactic, and semantic levels, that will be transferred across domains.", "labels": [], "entities": []}, {"text": "Still, adapting a paraphrase model to anew domain is a challenging and under-addressed problem.", "labels": [], "entities": []}, {"text": "To give some idea of the difficulty, for each of the eight domains in the popular OVERNIGHT () dataset, 30% to 55% of the words never occur in any of the other domains, a similar problem observed in domain adaptation for machine translation).", "labels": [], "entities": [{"text": "OVERNIGHT () dataset", "start_pos": 82, "end_pos": 102, "type": "DATASET", "confidence": 0.7120956381162008}, {"text": "machine translation", "start_pos": 221, "end_pos": 240, "type": "TASK", "confidence": 0.698208212852478}]}, {"text": "The paraphrase model therefore can get little knowledge fora substantial portion of the target domain from the source domains.", "labels": [], "entities": []}, {"text": "We introduce pre-trained word embeddings such as WORD2VEC () to combat the vocabulary variety across domains.", "labels": [], "entities": []}, {"text": "Based on recent studies on neural network initialization, we conduct a statistical analysis of pre-trained word embeddings and discover two problems that may hinder their direct use in neural networks: small micro variance, which hurts optimization, and large macro variance, which hurts generalization.", "labels": [], "entities": [{"text": "neural network initialization", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6624051332473755}]}, {"text": "We propose to standardize pre-trained word embeddings, and show its advantages both analytically and experimentally.", "labels": [], "entities": [{"text": "standardize pre-trained word embeddings", "start_pos": 14, "end_pos": 53, "type": "TASK", "confidence": 0.6795524060726166}]}, {"text": "On the OVERNIGHT dataset, we show that crossdomain training under the proposed framework can significantly improve model performance.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.8584761917591095}]}, {"text": "We also show that, compared with directly using pretrained word embeddings or normalization as in previous work, the proposed standardization technique can lead to about 10% absolute improvement inaccuracy.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our model with all the previous methods evaluated on the OVERNIGHT dataset.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 68, "end_pos": 85, "type": "DATASET", "confidence": 0.9344608187675476}]}, {"text": "use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (, to rank logical forms.", "labels": [], "entities": []}, {"text": "use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar.", "labels": [], "entities": []}, {"text": "Similar to ours, also use a Seq2Seq model with bi-directional RNN encoder and attentive decoder, but it is used to predict linearized logical forms.", "labels": [], "entities": []}, {"text": "They also propose a data augmentation technique, which further improves the average accuracy to 77.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9624406695365906}]}, {"text": "But it is orthogonal to this work and can be incorporated in any model including ours, therefore not included.", "labels": [], "entities": []}, {"text": "The above methods are all based on the indomain setting, where a separate parser is trained for each domain.", "labels": [], "entities": []}, {"text": "In parallel of this work, have explored another direction of cross-domain training: they use all of the domains to train a single parser, with a special domain encoding to help differentiate between domains.", "labels": [], "entities": []}, {"text": "We instead model it as a domain adaptation problem, where training on the source and the target domains are separate.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7472594976425171}]}, {"text": "Their model is the same as.", "labels": [], "entities": []}, {"text": "It is the current best-performing method on the OVERNIGHT dataset.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 48, "end_pos": 65, "type": "DATASET", "confidence": 0.9416266083717346}]}, {"text": "We use the standard 80%/20% split of training and testing, and randomly holdout 20% of training for validation.", "labels": [], "entities": []}, {"text": "In cross-domain experiments, for each target domain, all the other domains are combined as the source domain.", "labels": [], "entities": []}, {"text": "Hyper-parameters are selected based on the validation set.", "labels": [], "entities": []}, {"text": "State size of both the encoder and the decoder are set to 100, and word embedding size is set to 300.", "labels": [], "entities": []}, {"text": "Input and output dropout rate of the GRU cells are 0.7 and 0.5, respectively, and mini-batch size is 512.", "labels": [], "entities": [{"text": "Input", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9906067848205566}]}, {"text": "We use Adam with the default parameters suggested in the paper for optimization.", "labels": [], "entities": []}, {"text": "We use gradient clipping with a cap for global norm at 5.0 to alleviate the exploding gradients problem of recurrent neural networks.", "labels": [], "entities": []}, {"text": "Early stopping based on the validation set is used to decide when to stop training.", "labels": [], "entities": []}, {"text": "The selected model is retrained using the whole training set (training + validation).", "labels": [], "entities": []}, {"text": "The   evaluation metric is accuracy, i.e., the proportion of testing examples for which the top prediction yields the correct denotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.99949049949646}]}, {"text": "Our model is implemented in Tensorflow (, and the code can be found at https://github.com/ ysu1989/CrossSemparse.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the word embedding matrix with dif- ferent initialization strategies. Random: random sampling  from U (\u2212  \u221a  3,  \u221a  3), thus unit variance. WORD2VEC: raw  WORD2VEC vectors. ES: per-example standardization. FS:  per-feature standardization. EN: per-example normalization.  Cosine similarity is computed on a randomly selected (but  fixed) set of 1M word pairs.", "labels": [], "entities": [{"text": "FS", "start_pos": 230, "end_pos": 232, "type": "METRIC", "confidence": 0.9846802353858948}]}, {"text": " Table 1. Compared with random initialization,  two characteristics of the WORD2VEC vectors stand  out: (1) Small micro variance. Both the L2 norm  and the micro variance of the WORD2VEC vectors  are much smaller. (2) Large macro variance. The  variance of different WORD2VEC vectors, reflected  by the standard deviation of L2 norm, is much  larger (e.g., the maximum and the minimum L2  norm are 21.1 and 0.015, respectively). Small mi- cro variance can make the variance of neuron acti- vations starts off too small 3 , implying a poor start- ing point in the parameter space. On the other  hand, because of the magnitude difference, large  macro variance may make a model hard to gener-alize to words unseen in training.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the domains in the OVERNIGHT dataset. Pre-trained WORD2VEC embedding covers most of the words in  each domain, paving a way for domain adaptation.", "labels": [], "entities": [{"text": "OVERNIGHT dataset", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9330503046512604}, {"text": "domain adaptation", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7254039943218231}]}, {"text": " Table 3: Main experiment results. We combine the proposed paraphrase model with different word embedding initializations. I:  in-domain, X: cross-domain, EN: per-example normalization, FS: per-feature standardization, ES: per-example standardization.", "labels": [], "entities": [{"text": "FS", "start_pos": 186, "end_pos": 188, "type": "METRIC", "confidence": 0.9794522523880005}, {"text": "ES", "start_pos": 219, "end_pos": 221, "type": "METRIC", "confidence": 0.9787488579750061}]}]}