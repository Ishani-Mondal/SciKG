{"title": [{"text": "Efficient Discontinuous Phrase-Structure Parsing via the Generalized Maximum Spanning Arborescence", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew method for the joint task of tagging and non-projective dependency parsing.", "labels": [], "entities": [{"text": "tagging", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.9621778130531311}, {"text": "non-projective dependency parsing", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.6547340154647827}]}, {"text": "We demonstrate its usefulness with an application to discontinu-ous phrase-structure parsing where decoding lexicalized spines and syntactic derivations is performed jointly.", "labels": [], "entities": [{"text": "discontinu-ous phrase-structure parsing", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.7135176559289297}]}, {"text": "The main contributions of this paper are (1) a reduction from joint tagging and non-projective dependency parsing to the Generalized Maximum Spanning Arborescence problem, and (2) a novel decoding algorithm for this problem through Lagrangian relaxation.", "labels": [], "entities": [{"text": "non-projective dependency parsing", "start_pos": 80, "end_pos": 113, "type": "TASK", "confidence": 0.7858474254608154}, {"text": "Generalized Maximum Spanning Arborescence problem", "start_pos": 121, "end_pos": 170, "type": "TASK", "confidence": 0.7306632399559021}]}, {"text": "We evaluate this model and obtain state-of-the-art results despite strong independence assumptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discontinuous phrase-structure parsing relies either on formal grammars such as LCFRS, which suffer from a high complexity, or on reductions to non-projective dependency parsing with complex labels to encode phrase combinations.", "labels": [], "entities": [{"text": "phrase-structure parsing", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.6590727269649506}]}, {"text": "We propose an alternative approach based on a variant of spinal TAGs, which allows parses with discontinuity while grounding this work on a lexicalized phrase-structure grammar.", "labels": [], "entities": []}, {"text": "Contrarily to previous approaches, (, we do not model supertagging nor spine interactions with a complex label scheme.", "labels": [], "entities": []}, {"text": "We follow but drop projectivity.", "labels": [], "entities": []}, {"text": "We first show that our discontinuous variant of spinal TAG reduces to the Generalized Maximum Spanning Arborescence (GMSA) problem.", "labels": [], "entities": [{"text": "Generalized Maximum Spanning Arborescence (GMSA)", "start_pos": 74, "end_pos": 122, "type": "METRIC", "confidence": 0.7445320316723415}]}, {"text": "Ina graph where vertices are partitioned into clusters, GMSA consists in finding the arborescence of maximum weight incident to exactly one vertex per cluster.", "labels": [], "entities": [{"text": "GMSA", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.6955628991127014}]}, {"text": "This problem is NP-complete even for arc-factored models.", "labels": [], "entities": []}, {"text": "In order to bypass complexity, we resort to Lagrangian relaxation and propose an efficient resolution based on dual decomposition which combines a simple non-projective dependency parser on a contracted graph and a local search on each cluster to find a global consensus.", "labels": [], "entities": []}, {"text": "We evaluated our model on the discontinuous PTB () and the Tiger () corpora.", "labels": [], "entities": [{"text": "PTB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.5358052253723145}]}, {"text": "Moreover, we show that our algorithm is able to quickly parse the whole test sets.", "labels": [], "entities": []}, {"text": "Section 2 presents the parsing problem.", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9855787754058838}]}, {"text": "Section 3 introduces GMSA from which we derive an effective resolution method in Section 4.", "labels": [], "entities": [{"text": "GMSA", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8359925746917725}]}, {"text": "In Section 5 we define a parameterization of the parser which uses neural networks to model local probabilities and present experimental results in Section 6.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran a series of experiments on two corpora annotated with discontinuous constituents.", "labels": [], "entities": []}, {"text": "English We used an updated version of the Wall Street Journal part of the Penn Treebank corpus which introduces discontinuity ().", "labels": [], "entities": [{"text": "Wall Street Journal part of the Penn Treebank corpus", "start_pos": 42, "end_pos": 94, "type": "DATASET", "confidence": 0.8776078754001193}, {"text": "discontinuity", "start_pos": 112, "end_pos": 125, "type": "METRIC", "confidence": 0.9467881917953491}]}, {"text": "Sections 2-21 are used for training, 22 for developpement and 23 for testing.", "labels": [], "entities": [{"text": "developpement", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8334555625915527}]}, {"text": "We used gold and predicted POS tags by the Stanford tagger, 10 trained with 10-jackknifing.", "labels": [], "entities": []}, {"text": "Dependencies are extracted following the head-percolation table of.", "labels": [], "entities": []}, {"text": "German We used the Tiger corpus () with the split defined for the SPMRL 2014 shared task.", "labels": [], "entities": [{"text": "Tiger corpus", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.958814263343811}, {"text": "SPMRL 2014 shared task", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.5961546748876572}]}, {"text": "Following Maier (2015) and, we removed sentences number 46234 and 50224 as they contain annotation errors.", "labels": [], "entities": []}, {"text": "We only used the given gold POS tags.", "labels": [], "entities": [{"text": "POS tags", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.6576676666736603}]}, {"text": "Dependencies are extracted following the head-percolation table distributed with Tulipa (.", "labels": [], "entities": []}, {"text": "We emphasize that long sentences are not filtered out.", "labels": [], "entities": []}, {"text": "Our derivation extraction algorithm is similar to the one proposed in.", "labels": [], "entities": [{"text": "derivation extraction", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.9039691090583801}]}, {"text": "Regarding decoding, we use abeam of size 10 for spines w.r.t.", "labels": [], "entities": []}, {"text": "P \u03bd (s m |m, w) = h P \u03bd (s m |h, m, w) \u00d7 P \u03b1 (h|m, w) but allow every possible adjunction.", "labels": [], "entities": []}, {"text": "The maximum number of iterations of the subgradient descent is set to 500 and the stepsize \u03b7 t is fixed following the rule of.", "labels": [], "entities": []}, {"text": "Parsing results and timing on short sentences only (\u2264 40 words) and full test set using the de-fault discodop 11 eval script are reported on.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9661702513694763}, {"text": "timing", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9847363829612732}]}, {"text": "We report labeled recall (LR), precision (LP), F-measure (LF) and time measured in minutes.", "labels": [], "entities": [{"text": "recall (LR)", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9192955791950226}, {"text": "precision (LP)", "start_pos": 31, "end_pos": 45, "type": "METRIC", "confidence": 0.9710011631250381}, {"text": "F-measure (LF)", "start_pos": 47, "end_pos": 61, "type": "METRIC", "confidence": 0.9780123680830002}]}, {"text": "We also report results published by van for the discontinuous PTB and for Tiger.", "labels": [], "entities": [{"text": "PTB", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9389108419418335}, {"text": "Tiger", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.9817352890968323}]}, {"text": "Moreover, dependency unlabeled attachment scores (UAS) and tagging accuracies (Spine acc.) are given on.", "labels": [], "entities": [{"text": "dependency unlabeled attachment scores (UAS)", "start_pos": 10, "end_pos": 54, "type": "METRIC", "confidence": 0.7277164118630546}]}, {"text": "We achieve significantly better results on the discontinuous PTB, while being roughly 36 times faster together with a low memory footprint.", "labels": [], "entities": [{"text": "PTB", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.8156197667121887}]}, {"text": "On the Tiger corpus, we achieve on par results.", "labels": [], "entities": [{"text": "Tiger corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9885704815387726}]}, {"text": "Note however that rely on a greedy parser combined with beam search.", "labels": [], "entities": []}, {"text": "Fast and efficient parsing of discontinuous constituent is a challenging task.", "labels": [], "entities": [{"text": "parsing of discontinuous constituent", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.8495736569166183}]}, {"text": "Our method can quickly parse the whole test set, without any parallelization or GPU, obtaining an optimality certificate for more than 99% of the sentences in less than 500 iterations of the subgradient descent.", "labels": [], "entities": []}, {"text": "When using anon exact decoding algorithm, such as a greedy transition based method, we may not be able to deduce the best opportunity for improving scores on benchmarks, such as the parameterization method or the decoding algorithm.", "labels": [], "entities": []}, {"text": "Here the behavior maybe easier to interpret and directions for future improvement easier to see.", "labels": [], "entities": []}, {"text": "We stress that our method is able to produce an optimality certificate on more than 99% of the test examples thanks to the enhancement presented in Section 4.4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  and Table 2.", "labels": [], "entities": []}, {"text": " Table 3. We achieve signif- icantly better results on the discontinuous PTB,  while being roughly 36 times faster together with a  low memory footprint.", "labels": [], "entities": [{"text": "PTB", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8446002006530762}]}, {"text": " Table 2: Parsing results and processing time on  the german Tiger corpus. C2017 indicates results  of", "labels": [], "entities": [{"text": "german Tiger corpus", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9033050934473673}, {"text": "C2017", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.7956775426864624}]}, {"text": " Table 3: Dependency parsing and tagging re- sults. Results marked with  \u2020 use predicted part- of-speech tags.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8929904401302338}, {"text": "tagging", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.9557676315307617}]}]}