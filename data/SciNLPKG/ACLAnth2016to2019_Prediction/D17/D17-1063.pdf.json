{"title": [{"text": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "Active learning aims to select a small subset of data for annotation such that a classi-fier learned on the data is highly accurate.", "labels": [], "entities": []}, {"text": "This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets.", "labels": [], "entities": []}, {"text": "To address these shortcomings, we introduce a novel formulation by re-framing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic.", "labels": [], "entities": []}, {"text": "Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages.", "labels": [], "entities": []}, {"text": "We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.", "labels": [], "entities": [{"text": "cross-lingual named entity recognition", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.5940921306610107}]}], "introductionContent": [{"text": "For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck.", "labels": [], "entities": []}, {"text": "Thus active learning has been applied to NLP tasks to minimise the expense of annotating data.", "labels": [], "entities": []}, {"text": "Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9980973601341248}]}, {"text": "There have been many successful applications to NLP, e.g., used an active learning algorithm for CoNLL corpus to get an F 1 score 84% with a reduction of annotation cost of about 48%.", "labels": [], "entities": [{"text": "CoNLL corpus", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9295473992824554}, {"text": "F 1 score", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9781664609909058}]}, {"text": "In prior work most active learning algorithms are designed for English based on heuristics, such as using uncertainty or informativeness.", "labels": [], "entities": []}, {"text": "There has been comparatively little work done about how to learn the active learning strategy itself.", "labels": [], "entities": []}, {"text": "It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (.", "labels": [], "entities": []}, {"text": "Such settings area natural application for active learning, however there is little work to this end.", "labels": [], "entities": []}, {"text": "A potential reason is that most active learning algorithms require a substantial 'seed set' of data for learning a basic classifier, which can then be used for active data selection.", "labels": [], "entities": []}, {"text": "However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible.", "labels": [], "entities": []}, {"text": "In this paper, we propose PAL, short for Policy based Active Learning, a novel approach for learning a dynamic active learning strategy from data.", "labels": [], "entities": []}, {"text": "This allows for the strategy to be applied in other data settings, such as cross-lingual applications.", "labels": [], "entities": []}, {"text": "Our algorithm does not use a fixed heuristic, but instead learns how to actively select data, formalised as a reinforcement learning (RL) problem.", "labels": [], "entities": []}, {"text": "An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network ( . The policy is informed by observations including sentences' content information, the supervised model's classifications and its confidence.", "labels": [], "entities": []}, {"text": "Accordingly, a rich and dynamic policy can be learned for annotating new data based on the past sequence of annotation decisions.", "labels": [], "entities": []}, {"text": "Furthermore, in order to reduce the dependence on the data in the target language, which maybe low resource, we first learn the policy of active learning on another language and then transfer it to the target language.", "labels": [], "entities": []}, {"text": "It is easy to learn a policy on a high resource language, where there is plentiful data, such as English.", "labels": [], "entities": []}, {"text": "We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language.", "labels": [], "entities": []}, {"text": "Our work is different for prior work in active learning for NLP.", "labels": [], "entities": []}, {"text": "Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 55, "end_pos": 64, "type": "TASK", "confidence": 0.9169100224971771}]}, {"text": "Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling.", "labels": [], "entities": []}, {"text": "However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review some related work.", "labels": [], "entities": []}, {"text": "In Section 3, we present active learning algorithms, which cross multiple languages.", "labels": [], "entities": []}, {"text": "The experimental results are presented in Section 4.", "labels": [], "entities": []}, {"text": "We conclude our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language.", "labels": [], "entities": []}, {"text": "We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target languages we only permit a single episode, to mimic a language without existing resources.", "labels": [], "entities": []}, {"text": "We use NER corpora from CoNLL2002/2003 shared tasks, 4 which comprise NER annotated text in English (en), German (de), Spanish (es), and Dutch (nl), each annotated using the IOB1 labelling scheme, which we convert to the IO labeling scheme.", "labels": [], "entities": [{"text": "CoNLL2002/2003 shared tasks", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.9100870370864869}]}, {"text": "We use the existing corpus partions, with train used for policy training, testb used", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results from active learning using the dif- ferent methods, where each approach constructs a  training set of 200 sentences. The three target lan- guages are shown as columns, reporting in each F 1  score (%) and the relative cost reduction to match  the stated performance of the Random strategy.", "labels": [], "entities": [{"text": "F 1  score", "start_pos": 204, "end_pos": 214, "type": "METRIC", "confidence": 0.9521795709927877}]}]}