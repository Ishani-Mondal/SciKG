{"title": [{"text": "Importance sampling for unbiased on-demand evaluation of knowledge base population", "labels": [], "entities": [{"text": "Importance sampling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8667282164096832}]}], "abstractContent": [{"text": "Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations.", "labels": [], "entities": []}, {"text": "Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems.", "labels": [], "entities": [{"text": "KBP evaluation", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9189883172512054}]}, {"text": "We show that this evaluation is problematic: when anew system predicts a previously unseen relation, it is penalized even if it is correct.", "labels": [], "entities": []}, {"text": "This leads to significant bias against new systems, which counterproductively discourages innovation in the field.", "labels": [], "entities": []}, {"text": "Our first contribution is anew importance-sampling based evaluation which corrects for this bias by annotating anew system's predictions on-demand via crowdsourcing.", "labels": [], "entities": []}, {"text": "We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task.", "labels": [], "entities": [{"text": "bias", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9423571825027466}, {"text": "variance", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9889084696769714}, {"text": "2015 TAC KBP task", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.7155509814620018}]}, {"text": "Our second contribution is an implementation of our method made publicly available as an online KBP evaluation service.", "labels": [], "entities": []}, {"text": "We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.", "labels": [], "entities": [{"text": "TAC KBP 2016 corpus", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9592248648405075}]}], "introductionContent": [{"text": "Harnessing the wealth of information present in unstructured text online has been along standing goal for the natural language processing community.", "labels": [], "entities": []}, {"text": "In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus.", "labels": [], "entities": []}, {"text": "Knowledge bases have found many applications including question answering (; * Authors contributed equally.", "labels": [], "entities": [{"text": "question answering", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.942346066236496}]}], "datasetContent": [{"text": "Pooling bias is fundamentally a sampling bias problem where relation instances from new systems are underrepresented in the evaluation dataset.", "labels": [], "entities": [{"text": "Pooling bias", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8727263808250427}]}, {"text": "We could of course sidestep the problem by exhaustively annotating the entire document corpus, by annotating all mentions of entities and checking relations between all pairs of mentions.", "labels": [], "entities": []}, {"text": "However, that would be a laborious and prohibitively expensive task: using the interfaces we've developed (Section 6), it costs about $15 to annotate a single document by non-expert crowdworkers, resulting in an estimated cost of at least $1,350,000 fora reasonably large corpus of 90,000 documents.", "labels": [], "entities": []}, {"text": "The annotation effort would cost significantly more with expert annotators.", "labels": [], "entities": []}, {"text": "In contrast, labeling relation instances from system predictions can bean order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about $0.18 to verify each relation instance compared to $1.60 per instance extracted through exhaustive annotations.", "labels": [], "entities": [{"text": "labeling relation instances from system predictions", "start_pos": 13, "end_pos": 64, "type": "TASK", "confidence": 0.8286231557528178}]}, {"text": "We propose anew paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from systems only when they are underrepresented, thus correcting for pooling bias as it arises.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7929839491844177}]}, {"text": "In this section, we'll formalize the problem solved by ondemand evaluation independent of KBP and describe a cost-effective solution that allows us to accurately estimate evaluation scores without bias using importance sampling.", "labels": [], "entities": []}, {"text": "We'll then instantiate the framework for KBP in Section 5.", "labels": [], "entities": [{"text": "KBP", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.5176208019256592}]}, {"text": "Applying the on-demand evaluation framework to a task requires us to answer three questions: 1.", "labels": [], "entities": []}, {"text": "What is the desired distribution over system predictions pi ? 2. How do we label an instance x, i.e. check if x \u2208 Y?", "labels": [], "entities": []}, {"text": "3. How do we sample from the unknown set of true instances x \u223c p 0 ? In this section, we present practical implementations for knowledge base population.", "labels": [], "entities": []}, {"text": "Let us now see how well on-demand evaluation works in practice.", "labels": [], "entities": []}, {"text": "We begin by empirically studying the bias and variance of the joint estimator proposed in Section 4 and find it is able to correct for pooling bias while significantly reducing variance in comparison with the simple estimator.", "labels": [], "entities": []}, {"text": "We then demonstrate that on-demand evaluation can serve as a practical replacement for the TAC KBP evaluations by piloting anew evaluation service we have developed to evaluate three distinct systems on TAC KBP 2016 document corpus.", "labels": [], "entities": [{"text": "TAC KBP evaluations", "start_pos": 91, "end_pos": 110, "type": "DATASET", "confidence": 0.7909843921661377}, {"text": "TAC KBP 2016 document corpus", "start_pos": 203, "end_pos": 231, "type": "DATASET", "confidence": 0.9547515869140625}]}, {"text": "Separately, we evaluate the efficacy of the adaptive sample selection method described in Section 4.3 through another simulated experiment.", "labels": [], "entities": []}, {"text": "In each trial of this experiment, we evaluate the top 40 systems in random order.", "labels": [], "entities": []}, {"text": "As each subsequent system is evaluated, the number of samples to pick from the system is chosen to meet a target variance and added to the current pool of labeled instances.", "labels": [], "entities": []}, {"text": "To make the experiment more interpretable, we choose the target variance to correspond with the estimated variance of having 500 samples.", "labels": [], "entities": []}, {"text": "plots the results of the experiment.", "labels": [], "entities": []}, {"text": "The number of samples required to estimate systems quickly drops off from the benchmark of 500 samples as the pool of labeled instances covers more systems.", "labels": [], "entities": []}, {"text": "This experiment shows that on-demand evaluation using joint estimation can scale up to an order of magnitude more submissions than a simple estimator for the same cost.", "labels": [], "entities": []}, {"text": "We have implemented the on-demand evaluation framework described here as an evaluation service to which researchers can submit their own system predictions.", "labels": [], "entities": []}, {"text": "As a pilot of the service, we evaluated three relation extraction systems that also participated in the official 2016 TAC KBP competition.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8999624848365784}, {"text": "TAC KBP competition", "start_pos": 118, "end_pos": 137, "type": "DATASET", "confidence": 0.7622419993082682}]}, {"text": "Each system uses Stanford CoreNLP ( ) to identify entities, the Illinois Wikifier) to perform entity linking and a combination of a rule-based system (P), a logistic classifier (L), and a neural network classifier (N) for relation extraction.", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 17, "end_pos": 33, "type": "DATASET", "confidence": 0.9020472466945648}, {"text": "Illinois Wikifier", "start_pos": 64, "end_pos": 81, "type": "DATASET", "confidence": 0.9708947539329529}, {"text": "entity linking", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.7280384004116058}, {"text": "relation extraction", "start_pos": 222, "end_pos": 241, "type": "TASK", "confidence": 0.823514997959137}]}, {"text": "We used 15,000 Newswire documents from the 2016 TAC KBP evaluation as our document corpus.", "labels": [], "entities": [{"text": "Newswire documents from the 2016 TAC KBP evaluation", "start_pos": 15, "end_pos": 66, "type": "DATASET", "confidence": 0.9377708807587624}]}, {"text": "In total, 100 documents were exhaustively annotated for about $2,000 and 500 instances from each system were labeled for about $150 each.", "labels": [], "entities": []}, {"text": "Evaluating all three system only took about 2 hours.", "labels": [], "entities": [{"text": "Evaluating", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9572985172271729}]}, {"text": "reports scores obtained through ondemand evaluation of these systems as well as their corresponding official TAC evaluation scores.", "labels": [], "entities": []}, {"text": "While the relative ordering of systems between the two evaluations is the same, we note that precision and recall as measured through ondemand evaluation are respectively higher and lower than the official scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9996010661125183}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9992339611053467}]}, {"text": "This is to be expected because on-demand evaluation measures precision using each systems output as opposed to an externally defined set of evaluation entities.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9982149600982666}]}, {"text": "Likewise, recall is measured using exhaustive annotations of relations within the corpus instead of annotations from pooled output in the official evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9973819851875305}]}], "tableCaptions": []}