{"title": [{"text": "Dynamic Entity Representations in Neural Language Models", "labels": [], "entities": [{"text": "Dynamic Entity Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7495705882708231}]}], "abstractContent": [{"text": "Understanding along document requires tracking how entities are introduced and evolve overtime.", "labels": [], "entities": []}, {"text": "We present anew type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextu-ally generate their mentions.", "labels": [], "entities": []}, {"text": "Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length.", "labels": [], "entities": []}, {"text": "In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7665552794933319}, {"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.9556046426296234}, {"text": "entity prediction", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.8340677917003632}]}, {"text": "Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding a narrative requires keeping track of its participants over a long-term context.", "labels": [], "entities": []}, {"text": "As a story unfolds, the information a reader associates with each character in a story increases, and expectations about what will happen next change accordingly.", "labels": [], "entities": []}, {"text": "At present, models of natural language do not explicitly track entities; indeed, in today's language models, entities are no more than the words used to mention them.", "labels": [], "entities": []}, {"text": "In this paper, we endow a generative language model with the ability to buildup a dynamic representation of each entity mentioned in the text.", "labels": [], "entities": [{"text": "generative language", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.9087368845939636}]}, {"text": "Our language model defines a probability distribution over the whole text, with a distinct generative story for entity mentions.", "labels": [], "entities": []}, {"text": "It explicitly groups those mentions that corefer and associates with each entity a continuous representation that is updated by every contextualized mention of the entity, and that in turn affects the text that follows.", "labels": [], "entities": []}, {"text": "[John] 1 wanted to go to: ENTITYNLM explicitly tracks entities in a text, including coreferring relationships between entities like 1 and [He] . As a language model, it is designed to predict that a coreferent of [the coffee shop] 2 is likely to follow \"told that,\" that the referring expression will be \"it\", and that \"sold the best beans\" is likely to come next, by using entity information encoded in the dynamic distributed representation.", "labels": [], "entities": []}, {"text": "Our method builds on recent advances in representation learning, creating local probability distributions from neural networks.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.9559580087661743}]}, {"text": "It can be understood as a recurrent neural network language model, augmented with random variables for entity mentions that capture coreference, and with dynamic representations of entities.", "labels": [], "entities": []}, {"text": "We estimate the model's parameters from data that is annotated with entity mentions and coreference.", "labels": [], "entities": []}, {"text": "Because our model is generative, it can be queried in different ways.", "labels": [], "entities": []}, {"text": "Marginalizing everything except the words, it can play the role of a language model.", "labels": [], "entities": []}, {"text": "In \u00a75.1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation ().", "labels": [], "entities": [{"text": "English test set", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.7639393210411072}, {"text": "CoNLL 2012 shared task", "start_pos": 157, "end_pos": 179, "type": "DATASET", "confidence": 0.7529765218496323}, {"text": "coreference evaluation", "start_pos": 183, "end_pos": 205, "type": "TASK", "confidence": 0.6102599054574966}]}, {"text": "The model can also identify entity mentions and coreference relationships among them.", "labels": [], "entities": []}, {"text": "In \u00a75.2, we show that it can easily be used to add a performance boost to a strong coreference resolution system, by reranking a list of k-best candidate outputs.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.9267013370990753}]}, {"text": "On the CoNLL 2012 shared task test set, the reranked outputs are significantly better than the original top choices from the same system.", "labels": [], "entities": [{"text": "CoNLL 2012 shared task test set", "start_pos": 7, "end_pos": 38, "type": "DATASET", "confidence": 0.9192174871762594}]}, {"text": "Fi-nally, the model can perform entity cloze tasks.", "labels": [], "entities": []}, {"text": "As presented in \u00a75.3, it achieves state-of-the-art performance on the InScript corpus ().", "labels": [], "entities": [{"text": "InScript corpus", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.860554963350296}]}], "datasetContent": [{"text": "We evaluate our model in diverse use scenarios: (i) language modeling, (ii) coreference resolution, and (iii) entity prediction.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 52, "end_pos": 69, "type": "TASK", "confidence": 0.736351877450943}, {"text": "coreference resolution", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.9415217638015747}, {"text": "entity prediction", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.831179678440094}]}, {"text": "The evaluation on language modeling shows how the internal entity representation, when marginalized out, can improve the perplexity of language models.", "labels": [], "entities": []}, {"text": "The evaluation on coreference resolution experiment shows how our new language model can improve a competitive coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.9498350620269775}, {"text": "coreference resolution", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.8704541325569153}]}, {"text": "Finally, we employ an entity cloze task to demonstrate the generative performance of our model in predicting the next entity given the previous context.", "labels": [], "entities": []}, {"text": "We use two datasets for the three evaluation tasks.", "labels": [], "entities": []}, {"text": "For language modeling and coreference resolution, we use the English benchmark data from the CoNLL 2012 shared task on coreference resolution ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7443187832832336}, {"text": "coreference resolution", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.961765468120575}, {"text": "English benchmark data from the CoNLL 2012 shared task", "start_pos": 61, "end_pos": 115, "type": "DATASET", "confidence": 0.7350297205977969}, {"text": "coreference resolution", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.9229377806186676}]}, {"text": "We employ the standard training/development/test split, which includes 2,802/343/348 documents with roughly 1M/150K/150K tokens, respectively.", "labels": [], "entities": []}, {"text": "We follow the coreference annotation in the CoNLL dataset to extract entities and ignore the singleton mentions in texts.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9729001224040985}]}, {"text": "For entity prediction, we employ the InScript corpus created by.", "labels": [], "entities": [{"text": "entity prediction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8529948592185974}]}, {"text": "It consists of 10 scenarios, including grocery shopping, taking a flight, etc.", "labels": [], "entities": []}, {"text": "It includes 910 crowdsourced simple narrative texts in total and 18 stories were ignored due to labeling problems ().", "labels": [], "entities": []}, {"text": "On average, each story has 12.4 sentences, 24.9 entities and 217.2 tokens.", "labels": [], "entities": []}, {"text": "Each entity mention is labeled with its entity index.", "labels": [], "entities": []}, {"text": "We use the same training/development/test split as in (, which includes 619, 91, 182 texts, respectively.", "labels": [], "entities": []}, {"text": "In this section, we present the experimental results on the three evaluation tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language modeling evaluation on the test  sets of the English section in the CoNLL 2012  shared task. As mentioned in  \u00a74, the vocabulary  size is 10K. ENTITYNLM does not require any  coreference annotation on the test data.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6919278055429459}, {"text": "CoNLL 2012  shared task", "start_pos": 87, "end_pos": 110, "type": "DATASET", "confidence": 0.9126162528991699}]}, {"text": " Table 2: Coreference resolution scores on the CoNLL 2012 test set. CORT is the best-performing model  of Martschat and Strube (2015) with greedy decoding.", "labels": [], "entities": [{"text": "CoNLL 2012 test set", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9695457816123962}]}, {"text": " Table 3: Entity prediction accuracy on the test set  of the InScript corpus.", "labels": [], "entities": [{"text": "Entity prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.693830206990242}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9576818346977234}, {"text": "InScript corpus", "start_pos": 61, "end_pos": 76, "type": "DATASET", "confidence": 0.889337420463562}]}]}