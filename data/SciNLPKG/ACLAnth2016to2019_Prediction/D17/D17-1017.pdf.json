{"title": [{"text": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts", "labels": [], "entities": []}], "abstractContent": [{"text": "Generating captions for images is a task that has recently received considerable attention.", "labels": [], "entities": [{"text": "Generating captions for images", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9193900674581528}]}, {"text": "In this work we focus on caption generation for abstract scenes, or object layouts where the only information provided is a set of objects and their locations.", "labels": [], "entities": [{"text": "caption generation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9728057980537415}]}, {"text": "We propose OBJ2TEXT, a sequence-to-sequence model that encodes a set of objects and their locations as an input sequence using an LSTM network, and decodes this representation using an LSTM language model.", "labels": [], "entities": []}, {"text": "We show that our model, despite encoding object layouts as a sequence , can represent spatial relationships between objects, and generate descriptions that are globally coherent and semantically relevant.", "labels": [], "entities": []}, {"text": "We test our approach in a task of object-layout captioning by using only object annotations as inputs.", "labels": [], "entities": [{"text": "object-layout captioning", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.7602004408836365}]}, {"text": "We additionally show that our model, combined with a state-of-the-art object detector , improves an image captioning model from 0.863 to 0.950 (CIDEr score) in the test benchmark of the standard MS-COCO Captioning task.", "labels": [], "entities": [{"text": "CIDEr score)", "start_pos": 144, "end_pos": 156, "type": "METRIC", "confidence": 0.8610775470733643}, {"text": "MS-COCO Captioning task", "start_pos": 195, "end_pos": 218, "type": "TASK", "confidence": 0.7843703130880991}]}], "introductionContent": [{"text": "Natural Language generation (NLG) is along standing goal in natural language processing.", "labels": [], "entities": [{"text": "Natural Language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7671536157528559}, {"text": "natural language processing", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6448922355969747}]}, {"text": "There have already been several successes in applications such as financial reporting, or weather forecasts (), however it is still a challenging task for less structured and open domains.", "labels": [], "entities": [{"text": "financial reporting", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7826612293720245}]}, {"text": "Given recent progress in training robust visual recognition models using convolutional neural networks, the task of generating natural language descriptions for ar-: Overview of our proposed model for generating visually descriptive language from object layouts.", "labels": [], "entities": []}, {"text": "The input (a) is an object layout that consists of object categories and their corresponding bounding boxes, the encoder (b) uses a twostream recurrent neural network to encode the input object layout, and the decoder (c) uses a standard LSTM recurrent neural network to generate text.", "labels": [], "entities": []}, {"text": "bitrary images has received considerable attention (.", "labels": [], "entities": []}, {"text": "In general, generating visually descriptive language can be useful for various tasks such as human-machine communication, accessibility, image retrieval, and search.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 137, "end_pos": 152, "type": "TASK", "confidence": 0.7775692343711853}]}, {"text": "However this task is still challenging and it depends on developing both a robust visual recognition model, and a reliable language generation model.", "labels": [], "entities": [{"text": "language generation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.6957798302173615}]}, {"text": "In this paper, we instead tackle a task of describing object layouts where the categories for the objects in an input scene and their corresponding locations are known.", "labels": [], "entities": []}, {"text": "Object layouts are commonly used for story-boarding, sketching, and computer graphics applications.", "labels": [], "entities": [{"text": "Object layouts", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8510727286338806}]}, {"text": "Additionally, using our object layout captioning model on the outputs of an object detector we are also able to improve image caption-ing models.", "labels": [], "entities": [{"text": "object layout captioning", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6997739175955454}]}, {"text": "Object layouts contain rich semantic information, however they also abstract away several other visual cues such as color, texture, and appearance, thus introducing a different set of challenges than those found in traditional image captioning.", "labels": [], "entities": [{"text": "Object layouts", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7717881202697754}]}, {"text": "We propose OBJ2TEXT, a sequence-tosequence model that encodes object layouts using an LSTM network, and decodes natural language descriptions using an LSTM-based neural language model . Natural language generation systems usually consist of two steps: content planning, and surface realization.", "labels": [], "entities": [{"text": "Natural language generation", "start_pos": 186, "end_pos": 213, "type": "TASK", "confidence": 0.7098386685053507}, {"text": "surface realization", "start_pos": 274, "end_pos": 293, "type": "TASK", "confidence": 0.7953104078769684}]}, {"text": "The first step decides on the content to be included in the generated text, and the second step connects the concepts using structural language properties.", "labels": [], "entities": []}, {"text": "In our proposed model, OBJ2TEXT, content planning is performed by the encoder, and surface realization is performed by the decoder.", "labels": [], "entities": [{"text": "OBJ2TEXT", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8696526885032654}]}, {"text": "Our model is trained in the standard MS-COCO dataset (), which includes both object annotations for the task of object detection, and textual descriptions for the task of image captioning.", "labels": [], "entities": [{"text": "MS-COCO dataset", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9486652314662933}, {"text": "object detection", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.7370597422122955}, {"text": "image captioning", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.7308171540498734}]}, {"text": "While most previous research has been devoted to anyone of these two tasks, our paper presents, to our knowledge, the first approach for learning mappings between object annotations and textual descriptions.", "labels": [], "entities": [{"text": "learning mappings between object annotations and textual descriptions", "start_pos": 137, "end_pos": 206, "type": "TASK", "confidence": 0.7550851851701736}]}, {"text": "Using several lesioned versions of the proposed model we explored the effect of object counts and locations in the quality and accuracy of the generated natural language descriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9971771240234375}]}, {"text": "Generating visually descriptive language requires beyond syntax, and semantics; an understanding of the physical word.", "labels": [], "entities": [{"text": "Generating visually descriptive language", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.7676312029361725}]}, {"text": "We also take inspiration from recent work by where the goal was to reconstruct a sentence from a bag-of-words (BOW) representation using a simple surface-level language model based on an encoder-decoder sequence-to-sequence architecture.", "labels": [], "entities": []}, {"text": "In contrast to this previous approach, our model is grounded on visual data, and its corresponding spatial information, so it goes beyond word re-ordering.", "labels": [], "entities": []}, {"text": "Also relevant to our work is which previously explored the task of oracle image captioning by providing a language generation model with a list of manually defined visual concepts known to be present in the image.", "labels": [], "entities": [{"text": "oracle image captioning", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.6977804601192474}]}, {"text": "In addition, our model is able to leverage both quantity and spatial information as additional cues associated with each object/concept, thus allowing it to learn about verbosity, and spatial relations in a supervised fashion.", "labels": [], "entities": []}, {"text": "In summary, our contributions are as follows: \u2022 We demonstrate that despite encoding object layouts as a sequence using an LSTM, our model can still effectively capture spatial information for the captioning task.", "labels": [], "entities": []}, {"text": "We perform ablation studies to measure the individual impact of object counts, and locations.", "labels": [], "entities": []}, {"text": "\u2022 We show that a model relying only on object annotations as opposed to pixel data, performs competitively in image captioning despite the ambiguity of the setup for this task.", "labels": [], "entities": [{"text": "image captioning", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.7160560488700867}]}, {"text": "\u2022 We show that more accurate and comprehensive descriptions can be generated on the image captioning task by combining our OBJ2TEXT model using the outputs of a state-of-the-art object detector with a standard image captioning approach.", "labels": [], "entities": [{"text": "image captioning task", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7853681345780691}]}], "datasetContent": [{"text": "We evaluate the proposed models on the MS-COCO () dataset which is a popular image captioning benchmark that also contains object extent annotations.", "labels": [], "entities": [{"text": "MS-COCO () dataset", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.848089317480723}]}, {"text": "In the object layout captioning task the model uses the groundtruth object extents as input object layouts, while in the image captioning task the model takes raw images as input.", "labels": [], "entities": [{"text": "object layout captioning task", "start_pos": 7, "end_pos": 36, "type": "TASK", "confidence": 0.8121298849582672}, {"text": "image captioning", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.6954611092805862}]}, {"text": "The qualities of generated descriptions are evaluated using both human evaluations and automatic metrics.", "labels": [], "entities": []}, {"text": "We train and validate our models based on the commonly adopted split regime (113,287 training images, 5000 validation and 5000 test images) used in (, and also test our model in the MS-COCO official test benchmark.", "labels": [], "entities": [{"text": "MS-COCO official test benchmark", "start_pos": 182, "end_pos": 213, "type": "DATASET", "confidence": 0.8780115991830826}]}, {"text": "We implement our models based on the open source image captioning system Neuraltalk2 ().", "labels": [], "entities": []}, {"text": "Other configurations including data preprocessing and training hyper-parameters also follow Neuraltalk2.", "labels": [], "entities": [{"text": "Neuraltalk2", "start_pos": 92, "end_pos": 103, "type": "DATASET", "confidence": 0.9168587923049927}]}, {"text": "We trained our models using a GTX1080 GPU with 8GB of memory for 400k iterations using a batch size of 16 and an Adam optimizer with alpha of 0.8, beta of 0.999 and epsilon of 1e-08.", "labels": [], "entities": []}, {"text": "Descriptions of the CNN-RNN approach are generated using the publicly available code and model checkpoint provided by Neuraltalk2 (.", "labels": [], "entities": []}, {"text": "Captions for online test set evaluations are generated using beam search of size 2, but score histories on split validation set are based on captions generated without beam search (i.e. max sampling at each time-step).", "labels": [], "entities": []}, {"text": "Ablation on Object Locations and Counts:.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8960568308830261}]}, {"text": "We setup an experiment where we remove the input locations from the OBJ2TEXT encoder to study the effects on the generated captions, and confirm whether the model is actually using spatial information during surface realization.", "labels": [], "entities": [{"text": "OBJ2TEXT encoder", "start_pos": 68, "end_pos": 84, "type": "DATASET", "confidence": 0.912933349609375}]}, {"text": "In this restricted version of our model the LSTM encoder at each time step only takes the object category embedding vector as input.", "labels": [], "entities": []}, {"text": "The OBJ2TEXT model additionally encodes different instances of the same object category in different time steps, potentially encoding in some of its hidden states information about how many objects of a particular class are in the image.", "labels": [], "entities": []}, {"text": "For example, in the object annotation presented in the input in, there are two instances of \"person\".", "labels": [], "entities": []}, {"text": "We perform an additional experiment where our model does not have access neither to object locations, nor the number of object instances by providing only a set of object categories.", "labels": [], "entities": []}, {"text": "Note that in this set of experiments the object layouts are given as inputs, thus we assume full access to ground-truth object annotations, even in the test split.", "labels": [], "entities": []}, {"text": "In the experimental results section we use the \"-GT\" postfix to indicate that input object layouts are obtained from ground-truth object annotations provided by the MS-COCO dataset.", "labels": [], "entities": [{"text": "MS-COCO dataset", "start_pos": 165, "end_pos": 180, "type": "DATASET", "confidence": 0.9669916927814484}]}, {"text": "Image Captioning Experiment: In this experiment we assess whether the image captioning model OBJ2TEXT-YOLO that only relies on object categories and locations could give comparable performance with a CNN-RNN model based on Neuraltalk2 () that has full access to visual image features.", "labels": [], "entities": [{"text": "Image Captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7094974964857101}, {"text": "image captioning", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7406050860881805}, {"text": "OBJ2TEXT-YOLO", "start_pos": 93, "end_pos": 106, "type": "METRIC", "confidence": 0.5208030939102173}]}, {"text": "We also explore how much does a combined OBJ2TEXT-YOLO + CNN-RNN model could improve over a CNN-RNN model by fusing object counts and location information that is not explicitly encoded in a traditional CNN-RNN approach.", "labels": [], "entities": []}, {"text": "We use a twoalternative forced-choice evaluation (2AFC) ap-proach to compare two methods that generate captions.", "labels": [], "entities": []}, {"text": "For this, we setup a task on Amazon Mechanical Turk where users are presented with an image and two alternative captions, and they have to choose the caption that best describes the image.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.9285937150319418}]}, {"text": "Users are not prompted to use any single criteria but rather a holistic assessment of the captions, including their semantics, syntax, and the degree to which they describe the image content.", "labels": [], "entities": []}, {"text": "In our experiment we randomly sample 500 captions generated by various models for MS COCO online test set images, and use three users per image to obtain annotations.", "labels": [], "entities": [{"text": "MS COCO online test set images", "start_pos": 82, "end_pos": 112, "type": "DATASET", "confidence": 0.8950736622015635}]}, {"text": "Note that three users choosing randomly between two options have a chance of 25% to select the same caption fora given image.", "labels": [], "entities": []}, {"text": "In our experiments comparing method A vs method B, we report the percentage of times A was picked over B (Choice-all), the percentage of times all users selected the same method, either A or B, (Agreement), and the percentage of times A was picked over B only for these cases where all users agreed (Choice-agreement).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of lesioned versions of the proposed model on the MS COCO split test set.", "labels": [], "entities": [{"text": "MS COCO split test set", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.925978708267212}]}, {"text": " Table 2: Human evaluation results using two-alternative forced choice evaluation. Choice-all is percent- age the first alternative was chosen. Choice-agreement is percentage the first alternative was chosen only  when all annotators agreed. Agreement is percentage where all annotators agreed (random is 25%).", "labels": [], "entities": []}]}