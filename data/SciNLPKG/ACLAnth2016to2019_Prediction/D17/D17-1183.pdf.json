{"title": [{"text": "KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs", "labels": [], "entities": [{"text": "KGEval", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8861507773399353}, {"text": "Accuracy Estimation of Automatically Constructed Knowledge Graphs", "start_pos": 8, "end_pos": 73, "type": "TASK", "confidence": 0.6704538719994682}]}], "abstractContent": [{"text": "Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention recently.", "labels": [], "entities": [{"text": "construction of large knowledge graphs (KG)", "start_pos": 10, "end_pos": 53, "type": "TASK", "confidence": 0.6374454163014889}]}, {"text": "Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity and has largely been ignored in prior research.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9919390082359314}]}, {"text": "In this work, we try to fill this gap by proposing KGEval.", "labels": [], "entities": [{"text": "KGEval", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.7239224314689636}]}, {"text": "KGEval uses coupling constraints to bind facts and crowdsource those few that can infer large parts of the graph.", "labels": [], "entities": [{"text": "KGEval", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8553981184959412}]}, {"text": "We demonstrate that the objective optimized by KGEval is submodular and NP-hard, allowing guarantees for our approximation algorithm.", "labels": [], "entities": [{"text": "KGEval", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.7914585471153259}]}, {"text": "Through experiments on real-world datasets, we demonstrate that KGEval best estimates KG accuracy compared to other baselines, while requiring significantly lesser number of human evaluations .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9241384267807007}]}], "introductionContent": [{"text": "Automatic construction of Knowledge Graphs (KGs) from Web documents has received significant interest over the last few years, resulting in the development of several large KGs consisting of hundreds of predicates (e.g., isCity, stadiumLocatedInCity(Stadium, City)) and millions of their instances called beliefs (e.g., (Joe Luis Arena, stadiumLocatedInCity, Detroit)).", "labels": [], "entities": []}, {"text": "Examples of such KGs include NELL ( , Knowledge-Vault () etc.", "labels": [], "entities": []}, {"text": "Due to imperfections in the automatic KG construction process, many incorrect beliefs are also found in these KGs.", "labels": [], "entities": [{"text": "KG construction", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.9127229750156403}]}, {"text": "Knowing accuracy for each predicate in the KG can provide targeted feedback for improvement and highlight its strengths from weaknesses, whereas overall accuracy of a KG can quantify the effectiveness of its constructionprocess.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9986562728881836}, {"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.998256504535675}]}, {"text": "Knowing accuracy at predicate-level granularity is immensely helpful for QuestionAnswering (QA) systems that integrate opinions from multiple KGs ( ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9919388890266418}]}, {"text": "For such systems, being aware that a particular KG is more accurate than others in a certain domain, say sports, helps in restricting the search over relevant and accurate subsets of KGs, thereby improving QA-precision and response time.", "labels": [], "entities": []}, {"text": "In comparison to the large body of recent work focused on construction of KGs, the important problem of accuracy estimation of such large KGs is unexploredwe address this gap in this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9973135590553284}]}, {"text": "True accuracy of a predicate maybe estimated by aggregating human judgments on correctness of each and every belief in the predicate . Even though crowdsourcing marketplaces such as Amazon Mechanical Turk (AMT) provide a convenient way to collect human judgments, accumulating such judgments at the scale of larges KGs is prohibitively expensive.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9986271858215332}]}, {"text": "We shall refer to the task of manually classifying a single belief as true or false as a Belief Evaluation Task (BET).", "labels": [], "entities": []}, {"text": "Thus, the crucial problem is: How can we select a subset of beliefs to evaluate which will best estimate the true (but unknown) accuracy of KG and its predicates?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9968149065971375}]}, {"text": "A naive and popular approach is to evaluate randomly sampled subset of beliefs from the KG.", "labels": [], "entities": []}, {"text": "Since random sampling ignores relationalcouplings present among the beliefs, it usually results in oversampling and poor accuracy estimates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9983863830566406}]}, {"text": "Let us motivate this through an example.", "labels": [], "entities": []}, {"text": "Motivating example: We motivate efficient accuracy estimation through the KG fragment shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9963332414627075}, {"text": "KG fragment", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.8478970527648926}]}, {"text": "Here, each belief is an edge-triple in the graph, for example.", "labels": [], "entities": []}, {"text": "There are six correct and two incorrect beliefs (the two incident on Taj Mahal), resulting in an overall accuracy of 75%(= 6/8) which we would like to estimate.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9994726777076721}]}, {"text": "Additionally, we would also like to estimate accuracies of the predicates: homeStadiumOf, homeCity, stadiumLocatedInCity, cityInState and isA.", "labels": [], "entities": []}, {"text": "We now demonstrate how evaluation labels of beliefs are constrained by each other.", "labels": [], "entities": []}, {"text": "Type consistency is one such coupling constraint.", "labels": [], "entities": []}, {"text": "For instance, we know from KG ontology that the homeStadiumOf predicate connects an entity from Stadium category to another entity in Sports Team category.", "labels": [], "entities": []}, {"text": "Now, if (Joe Louis Arena, homeStadiumOf, Red Wings) is evaluated to be correct, then from these type constraints we can infer that (Joe Louis Arena, isA, Stadium) and (Red Wings, isA, Sports Team) are also correct.", "labels": [], "entities": [{"text": "homeStadiumOf", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.7576158046722412}]}, {"text": "Similarly, by evaluating (Taj Mahal, isA, State) as false, we can infer that (Detroit, cityInState, TajMahal) is incorrect.", "labels": [], "entities": []}, {"text": "Additionally, we have Horn-clause coupling constraints (), such as homeStadiumOf(x, y) \u2227 homeCity(y, z) \u2192 stadiumLocatedInCity(x, z).", "labels": [], "entities": [{"text": "stadiumLocatedInCity", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.8574446439743042}]}, {"text": "By evaluating (Red Wings, homeCity, Detroit) and applying this hornclause to the already evaluated facts mentioned above, we infer that (Joe Louis Arena, stadiumLocatedInCity, Detroit) is also correct.", "labels": [], "entities": [{"text": "homeCity", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9425812363624573}]}, {"text": "We explore generalized forms of these constraints in Section 3.1.", "labels": [], "entities": []}, {"text": "Thus, evaluating only three beliefs, and exploiting constraints among them, we exactly estimate the overall true accuracy as 75% and also coverall predicates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9349576234817505}]}, {"text": "In contrast, the empirical accuracy by randomly evaluating three beliefs, averaged over 5 trials, is 66.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9816900491714478}]}, {"text": "Our contributions in this paper are the following: (1).", "labels": [], "entities": []}, {"text": "Systematic study into the important problem of evaluation of automatically constructed Knowledge Graphs.", "labels": [], "entities": [{"text": "evaluation of automatically constructed Knowledge Graphs", "start_pos": 47, "end_pos": 103, "type": "TASK", "confidence": 0.6590839127699534}]}, {"text": "A novel crowdsourcingbased system KGEval to estimate accuracy of large knowledge graphs (KGs) by exploiting dependencies among beliefs for more accurate and faster KG accuracy estimation..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9909177422523499}, {"text": "KG accuracy estimation.", "start_pos": 164, "end_pos": 187, "type": "TASK", "confidence": 0.7620950937271118}]}, {"text": "Extensive experiments on real-world KGs to demonstrate KGEval's effectiveness and also evaluate its robustness and scalability.", "labels": [], "entities": []}, {"text": "All the data and code used in the paper are available at http://talukdar.net/ mall-lab/KGEval 2 Overview and Problem Statement", "labels": [], "entities": [{"text": "Problem Statement", "start_pos": 109, "end_pos": 126, "type": "TASK", "confidence": 0.6968452036380768}]}], "datasetContent": [{"text": "To combine all beliefs and constraints at a commonplace, for all H and C, we construct a graph with two types of nodes: (1) anode for each BET h \u2208 H, and (2) anode for each constraint Ci \u2208 C. Each Ci node is connected to all h nodes that participate in it.", "labels": [], "entities": []}, {"text": "We call this graph the Evaluation Coupling Graph (ECG), represented as G = (H \u222a C, E) with set of edges Note that ECG is a bipartite factor graph () with h as variable-nodes and Ci as factor-nodes.", "labels": [], "entities": []}, {"text": "To assess the effectiveness of KGEval, we ask the following questions: (1).How effective is KGEval in estimating KG accuracy, both at predicate-level and at overall KG-level?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9131447672843933}]}, {"text": "What is the importance of coupling constraints on its performance?..", "labels": [], "entities": []}, {"text": "And lastly, how robust is KGEval to estimating accuracy of KGs with varying quality?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9658104777336121}]}, {"text": "Initialization: Algorithm 1 requires initial seed set S which we generate by randomly evaluating |S| = 50 beliefs from H.", "labels": [], "entities": []}, {"text": "To maintain fairness, all baselines start from S.", "labels": [], "entities": []}, {"text": "For asserting true (or false) value for beliefs, we set a high soft label confidence threshold at \u03c4 = 0.8 (see Section 3.3).", "labels": [], "entities": [{"text": "soft label confidence threshold", "start_pos": 63, "end_pos": 94, "type": "METRIC", "confidence": 0.6867296621203423}]}, {"text": "Performance of various methods are evaluated using the following two metrics.", "labels": [], "entities": []}, {"text": "To capture accuracy at the predicate level, we define \u2206 predicate as the average of difference between gold and estimated accuracy of each of the R predicates in KG.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9982268214225769}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.8134048581123352}]}, {"text": "We define \u2206 overall as the difference between gold and estimated accuracy over the entire evaluation set.", "labels": [], "entities": [{"text": "\u2206", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9734808206558228}, {"text": "gold", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9673776030540466}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.7734115719795227}]}, {"text": "Above, \u03a6(H) is the overall gold accuracy, \u03a6(H r ) is the gold accuracy of predicate rand l(h) is the label assigned by the currently evaluated method.", "labels": [], "entities": [{"text": "gold accuracy", "start_pos": 27, "end_pos": 40, "type": "METRIC", "confidence": 0.8394046127796173}, {"text": "\u03a6(H r )", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9005732417106629}, {"text": "gold accuracy", "start_pos": 57, "end_pos": 70, "type": "METRIC", "confidence": 0.9293408691883087}]}, {"text": "\u2206 overall treats entire KG as a single bag of BETs whereas \u2206 predicate segregates beliefs based on their type of predicate-relation.", "labels": [], "entities": [{"text": "BETs", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9620576500892639}]}, {"text": "For both metrics, lower is better.", "labels": [], "entities": []}, {"text": "Other Baselines along with Inference: In order to evaluate how Random and Max-degree perform in conjunction with inference mechanism, we replaced KGEval's greedy control mechanism in Line 5 of Algorithm 1 with these two control mechanisms.", "labels": [], "entities": []}, {"text": "In our experiments, we observed that both Random+inference and Maxdegree+inference are able to estimate accuracy more accurately than their control-only variants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9957197308540344}]}, {"text": "Secondly, even though the accuracies estimated by Random+inference and Max-degree+inference were comparable to that of KGEval, they required larger number of crowd-evaluation queries -1.2x and 1.35x more, respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9799920916557312}]}, {"text": "This shows effectiveness of greedy mechanism.", "labels": [], "entities": []}, {"text": "Rate of Coverage: In case of large KGs with scarce budget, it is imperative to have a mechanism which covers greater parts of KG with lesser number of crowdsource queries.", "labels": [], "entities": []}, {"text": "shows the fraction of total beliefs whose evaluations were automatically inferred by different methods as a function of number of crowd-evaluated beliefs.", "labels": [], "entities": []}, {"text": "We observe that KGEval infers evaluation for the largest number of BETs at each supervision level.", "labels": [], "entities": [{"text": "BETs", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.6833751201629639}]}, {"text": "Robustness to Noise: In order to test robustness of the methods in estimating accuracies   of KGs with different gold accuracies, we artificially added noise to H N by flipping a fixed fraction of edges, otherwise following the same evaluation procedure as in Section 3.5.", "labels": [], "entities": []}, {"text": "We analyze \u2206 overall (and not \u2206 predicate ) because flipping edges in KG distorts predicate-relations dependencies and present in.", "labels": [], "entities": []}, {"text": "We evaluated all the methods and observed that while performance of other methods degraded significantly with diminishing KG quality (more noise), KGEval was significantly robust to noise.", "labels": [], "entities": []}, {"text": "Scalability comparisons with MLN: Markov Logic Networks () can serve as a candidate for Inference Mechanism.", "labels": [], "entities": [{"text": "Inference Mechanism", "start_pos": 88, "end_pos": 107, "type": "TASK", "confidence": 0.6784157156944275}]}, {"text": "We compared the runtime performance of KGEval with PSL and MLN as inference engines.", "labels": [], "entities": []}, {"text": "While PSL took 320 seconds to complete one iteration, the MLN implementation (PyMLN) could not finish grounding the rules even after 7 hours.", "labels": [], "entities": [{"text": "PyMLN", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.5584571361541748}]}, {"text": "This justifies our choice of PSL as the inference engine for KGEval.", "labels": [], "entities": [{"text": "KGEval", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7366449236869812}]}], "tableCaptions": [{"text": " Table 2: Details of BET subsets used for accuracy evalua-", "labels": [], "entities": []}, {"text": " Table 3: \u2206 predicate (%) and \u2206 overall (%) estimates (lower", "labels": [], "entities": []}, {"text": " Table 3. (see  Section 4.3)", "labels": [], "entities": []}, {"text": " Table 4: Performance of KGEval with ablated constraint", "labels": [], "entities": [{"text": "KGEval", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.4612789750099182}, {"text": "ablated constraint", "start_pos": 37, "end_pos": 55, "type": "METRIC", "confidence": 0.9584174454212189}]}, {"text": " Table 5: Accuracy estimate (higher is better) over entire", "labels": [], "entities": [{"text": "Accuracy estimate", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9750369191169739}]}]}