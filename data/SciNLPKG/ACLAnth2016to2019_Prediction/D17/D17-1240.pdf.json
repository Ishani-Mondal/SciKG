{"title": [], "abstractContent": [{"text": "In this paper, we present a set of computational methods to identify the likeliness of a word being borrowed, based on the signals from social media.", "labels": [], "entities": []}, {"text": "In terms of Spearman's correlation values, our methods perform more than two times better (\u223c 0.62) in predicting the borrowing like-liness compared to the best performing baseline (\u223c 0.26) reported in literature.", "labels": [], "entities": []}, {"text": "Based on this likeliness estimate we asked annotators to re-annotate the language tags of foreign words in predominantly native contexts.", "labels": [], "entities": []}, {"text": "In 88% of cases the annotators felt that the foreign language tag should be replaced by native language tag, thus indicating a huge scope for improvement of automatic language identification systems.", "labels": [], "entities": [{"text": "automatic language identification", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.6667458812395731}]}], "introductionContent": [{"text": "In social media communication, multilingual people often switch between languages, a phenomenon known as code-switching or codemixing.", "labels": [], "entities": []}, {"text": "This makes language identification and tagging, which is perhaps a prerequisite for almost all other language processing tasks that follow, a challenging problem ().", "labels": [], "entities": [{"text": "language identification and tagging", "start_pos": 11, "end_pos": 46, "type": "TASK", "confidence": 0.6802473962306976}]}, {"text": "In code-mixing people are subconsciously aware of the foreign origin of the codemixed word or the phrase.", "labels": [], "entities": []}, {"text": "A related but linguistically and cognitively distinct phenomenon is lexical borrowing (or simply, borrowing), where a word or phrase from a foreign language say L 2 is used as apart of the vocabulary of native language say L 1 . For instance, in Dutch, the English word \"sale\" is now used more frequently than the Dutch equivalent \"uitverkoop\".", "labels": [], "entities": [{"text": "lexical borrowing (or simply, borrowing)", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.6495040692389011}]}, {"text": "Some English words like \"shop\" are even inflected in Dutch as \"shoppen\" and heavily used.", "labels": [], "entities": []}, {"text": "While it is difficult in general to ascertain whether a foreign word or phrase used in an utterance is borrowed or just an instance of code-mixing ( ), one telltale sign is that only proficient multilinguals can code-mix, while even monolingual speakers can use borrowed words because, by definition, these are part of the vocabulary of a language.", "labels": [], "entities": []}, {"text": "In other words, just because an English speaker understands and uses the word \"tortilla\" does not imply that she can speak or understand Spanish.", "labels": [], "entities": []}, {"text": "A borrowed word from L 2 initially appears frequently in speech, then gradually in print media like newspaper and finally it loses its origin's identity and is used in L 1 resulting in an inclusion in the dictionary of L 1.", "labels": [], "entities": []}, {"text": "Borrowed words often take several years before they formally become part of L 1 dictionary.", "labels": [], "entities": [{"text": "L 1 dictionary", "start_pos": 76, "end_pos": 90, "type": "DATASET", "confidence": 0.7667447725931803}]}, {"text": "This motivates our research question \"is early-stage automatic identification of likely to be borrowed words possible?\".", "labels": [], "entities": [{"text": "early-stage automatic identification of likely to be borrowed words", "start_pos": 41, "end_pos": 108, "type": "TASK", "confidence": 0.7763827509350247}]}, {"text": "This is known to be a hard problem because (i) it is a socio-linguistic phenomenon closely related to acceptability and frequency, (ii) borrowing is a dynamic process; new borrowed words enter the lexicon of a language as old words, both native and borrowed, might slowly fade away from usage, and (iii) it is a population level phenomenon that necessitates data from a large portion of the population unlike standard natural language corpora that typically comes from a very small set of authors.", "labels": [], "entities": []}, {"text": "Automatic identification of borrowed words in social media content (SMC) can improve language tagging by recommending the tagger to tag the language of the borrowed words as L 1 instead of L 2 . The above reasons motivate us to resort to the social media (in particular, Twitter), where a large population of bilingual/multilingual speakers are known to often tweet in code-mixed colloquial languages.", "labels": [], "entities": [{"text": "identification of borrowed words in social media content (SMC)", "start_pos": 10, "end_pos": 72, "type": "TASK", "confidence": 0.8581578948281028}, {"text": "language tagging", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7337518334388733}]}, {"text": "We designed our methodology to work for any pair of languages L 1 and L 2 subject to the availability of sufficient SMC.", "labels": [], "entities": [{"text": "SMC", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9577872157096863}]}, {"text": "In the current study, we consider Hindi as L 1 and English as L 2 . The main stages of our research are as follows: Metrics to quantify the likeliness of borrowing from social media signals: We define three novel and closely similar metrics that serve as social signals indicating the likeliness of borrowing.", "labels": [], "entities": []}, {"text": "We compare the likeliness of borrowing as predicted by our model and a baseline model with that from the ground truth obtained from human judges.", "labels": [], "entities": []}, {"text": "Ground truth generation: We launch an extensive survey among 58 human judges of various age groups and various educational backgrounds to collect responses indicating if each of the candidate foreign word is likely borrowed.", "labels": [], "entities": [{"text": "Ground truth generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7120256721973419}]}, {"text": "Application: We randomly selected some words that have a high, low and medium borrowing likeliness as predicted by our metrics.", "labels": [], "entities": []}, {"text": "Further, we randomly selected one tweet for each of the chosen words.", "labels": [], "entities": []}, {"text": "The chosen words in almost all of these tweets have L 2 as their language tag while a majority of the surrounding words have a tag L 1 . We asked expert annotators to re-evaluate the language tags of the chosen words and indicate if they would prefer to switch this tag from L 2 from L 1 . Finally, our key results are outlined below: 1.", "labels": [], "entities": []}, {"text": "We obtained the Spearman's rank correlation between the ground-truth ranking and the ranking based on our metrics as \u223c 0.62 for all the three variants which is more than double the value (\u223c 0.26) if we use the most competitive baseline ( ) available in the literature.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 16, "end_pos": 43, "type": "METRIC", "confidence": 0.7909435778856277}]}, {"text": "2. Interestingly, the responses of the judges in the age group below 30 seem to correspond even better with our metrics.", "labels": [], "entities": []}, {"text": "Since language change is brought about mostly by the younger population, this might possibly mean that our metrics are able to capture the early signals of borrowing.", "labels": [], "entities": []}, {"text": "3. Those users that mix languages the least in their tweets present the best signals of borrowing in case they do mix the languages (correlation of our metrics estimated from the tweets of these users with that of the ground truth is \u223c 0.65).", "labels": [], "entities": []}, {"text": "4. Finally, we obtain an excellent re-annotation accuracy of 88% for the words falling in the surely borrowed category as predicted by our metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8535556793212891}]}], "datasetContent": [{"text": "In this section we discuss the dataset for our experiments, the evaluation criteria and the ground truth preparation scheme.", "labels": [], "entities": []}, {"text": "In this study, we consider code-mixed tweets gathered from Hindi-English bilingual Twitter users in order to study the effectiveness of our proposed metrics.", "labels": [], "entities": []}, {"text": "In Example I the English word \"film\" is surrounded by mostly Hindi words.", "labels": [], "entities": []}, {"text": "On the other hand, in Example II the English word \"thing\" is surrounded mostly by English words.", "labels": [], "entities": []}, {"text": "Note that the word \"film\" is very commonly used by Hindi monolingual speakers and is therefore highly likely to have been borrowed unlike the English word \"thing\" which is arguably an instance of mixing.", "labels": [], "entities": []}, {"text": "This socio-linguistic difference seems to be very appropriately captured by the language tag of the surrounding words of these two words in the respective tweets.", "labels": [], "entities": []}, {"text": "Based on this hypothesis we arrange the 230 words into contextually similar groups (see supplementary material for the grouping details).", "labels": [], "entities": []}, {"text": "Finally, using the baseline metric log( ) at uniformly at random.", "labels": [], "entities": []}, {"text": "Full set of words (f ull) -Thus, in total we selected 57 target words for the purpose of our evaluation.", "labels": [], "entities": []}, {"text": "We present these words in the box below.", "labels": [], "entities": []}, {"text": "Baseline-biased words -'thing', 'way', 'woman', 'press', 'wrong', 'well', 'matter', 'reason', 'question', 'guy', 'moment', 'week', 'luck', 'president', 'body', 'job', 'car', 'god', 'gift', 'status', 'university', 'lyrics', 'road', 'politics', 'parliament', 'review', 'scene', 'seat', 'film', 'degree' Randomly selected words -'people', 'play', 'house', 'service', 'rest', 'boy', 'month', 'money', 'cool', 'development', 'group', 'friend', 'day', 'performance', 'school', 'blue', 'room', 'interview', 'share', 'request', 'traffic', 'college', 'star', 'class', 'superstar', 'petrol', 'uncle'  We present a four step approach for evaluation as follows.", "labels": [], "entities": []}, {"text": "We measure (i) how well the U U R, UT Rand U PR based ranking of the hlws set, the mws set and the full set correlate with the ground truth ranking (discussed in the next section) in comparison to the rank given by the baseline metric, (ii) how well the different rank ranges obtained from our metric align with the ground truth as compared to the baseline metric, (iii) whether there are some systematic effects of the age group of the survey participants on the rank correspondence, (iv) how our metrics if computed from the tweets of users who (a) rarely mix languages, (b) almost always mix languages and (c) are in between (a) and (b), align with the ground truth.", "labels": [], "entities": []}, {"text": "Rank correlation: We measure the standard Spearman's rank correlation (\u03c1) pairwise between rank lists generated by (i) U U R (ii) UT R (iii) U PR (iv) baseline metric and the ground truth.", "labels": [], "entities": [{"text": "rank correlation (\u03c1)", "start_pos": 53, "end_pos": 73, "type": "METRIC", "confidence": 0.8258657157421112}]}, {"text": "We shall describe the next four measurements taking U U R as the running example.", "labels": [], "entities": [{"text": "U U R", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.6223528981208801}]}, {"text": "The same can be extended verbatim for the other two similar metrics.", "labels": [], "entities": []}, {"text": "Rank ranges: We split each of the three rank lists (U U R, ground truth and baseline) into five different equal-sized ranges as follows -(i) surely borrowed (SB) containing top 20% words from each list, (ii) likely borrowed (LB) containing the next 20% words from each list, (iii) borderline (BL) constituting the subsequent 20% words from each list, (iv) likely mixed (LM) comprising the next 20% words from each list and (v) surely mixed (SM) having the last 20% words from each rank list.", "labels": [], "entities": []}, {"text": "Therefore, we have three sets of five buckets, one set each for U U R, the ground truth and the baseline based rank list.", "labels": [], "entities": []}, {"text": "Next we calculate the bucket-wise correspondence between (i) the U U Rand the ground truth set and (ii) the baseline and the ground truth set in terms of standard precision and recall measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9384827017784119}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9978321194648743}]}, {"text": "For our purpose, we adapt these measures as follows.", "labels": [], "entities": []}, {"text": "G: ground truth bucket set, B b : baseline bucket set, U b : U U R bucket set; BS \u2208 {B b , U b }, T (type of bucket) = {SB, LB, BL, LM, SM}; b t = words in type t bucket from BS, gt = words in type t bucket from G, t \u2208 T ; tp t (no. of true positives) = |b t \u2229 gt |, f pt (no. of false positives) = |b t \u2212 gt |, tn t (no. of true negatives) = |g t \u2212 b t |; Bucket-wise precision and recall therefore: For a given set, we obtained the overall macro precision (recall) by averaging the precision (recall) values over the five buckets.", "labels": [], "entities": [{"text": "precision", "start_pos": 369, "end_pos": 378, "type": "METRIC", "confidence": 0.9310508370399475}, {"text": "recall", "start_pos": 383, "end_pos": 389, "type": "METRIC", "confidence": 0.9991008043289185}, {"text": "precision", "start_pos": 448, "end_pos": 457, "type": "METRIC", "confidence": 0.7741642594337463}, {"text": "recall", "start_pos": 459, "end_pos": 465, "type": "METRIC", "confidence": 0.947062075138092}, {"text": "precision", "start_pos": 484, "end_pos": 493, "type": "METRIC", "confidence": 0.9930203557014465}, {"text": "recall", "start_pos": 495, "end_pos": 501, "type": "METRIC", "confidence": 0.6938814520835876}]}, {"text": "For a given set, we also obtained the overall micro precision by first adding the true positives across all the buckets and then normalizing by the sum of the true and the false positives overall the buckets.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.5839241743087769}]}, {"text": "We take an equivalent approach for obtaining the micro recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.8224968314170837}]}, {"text": "Age group effect: Here we construct two ground truth rank lists one using the responses of the participants with age below 30 (young population) and the other using the responses of the rest of the participants (elderly population).", "labels": [], "entities": []}, {"text": "Next we repeat the above two evaluations considering each of the new ground truth rank lists.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's rank correlation coefficient  (\u03c1) among the different rank lists. Best result is  marked in bold.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient  (\u03c1)", "start_pos": 10, "end_pos": 54, "type": "METRIC", "confidence": 0.7933592163026333}]}, {"text": " Table 2: Number of words falling in each bucket  of three bucket sets.", "labels": [], "entities": []}, {"text": " Table 3: Bucket-wise precision/recall. Best results  are marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9627431035041809}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.99151611328125}]}, {"text": " Table 4: Overall macro and micro precision/recall.  Best results are marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.980506420135498}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9845185279846191}]}, {"text": " Table 5: Spearman's rank correlation across the  two age groups. Best results are marked in bold.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6525095552206039}]}, {"text": " Table 6: Bucket-wise precision (p)/recall (r) for  U U R and the baseline metrics for the two new  ground truths. Best results are marked in bold.", "labels": [], "entities": [{"text": "Bucket-wise", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9242889285087585}, {"text": "precision (p)/recall (r)", "start_pos": 22, "end_pos": 46, "type": "METRIC", "confidence": 0.8705453649163246}]}, {"text": " Table 7: Overall macro and micro precision and  recall for the two new ground truths. Best results  are marked in bold.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9811057448387146}, {"text": "recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9997678399085999}]}, {"text": " Table 8: Spearman's correlation between U U R  and the ground truth in the different user buckets.  Best results are marked in bold.", "labels": [], "entities": []}]}