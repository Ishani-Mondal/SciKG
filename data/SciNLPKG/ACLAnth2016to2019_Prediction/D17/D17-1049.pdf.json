{"title": [{"text": "Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews", "labels": [], "entities": [{"text": "Author-aware Aspect Topic Sentiment", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6007824242115021}, {"text": "Retrieve", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.9572444558143616}]}], "abstractContent": [{"text": "User generated content about products and services in the form of reviews are often diverse and even contradictory.", "labels": [], "entities": []}, {"text": "This makes it difficult for users to know if an opinion in a review is prevalent or biased.", "labels": [], "entities": []}, {"text": "We study the problem of searching for supporting opinions in the context of reviews.", "labels": [], "entities": []}, {"text": "We propose a framework called SURF, that first identifies opinions expressed in a review, and then finds similar opinions from other reviews.", "labels": [], "entities": [{"text": "SURF", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.8087031841278076}]}, {"text": "We design a novel probabilistic graphical model that captures opinions as a combination of aspect, topic and sentiment dimensions, takes into account the preferences of individual authors, as well as the quality of the entity under review, and encodes the flow of thoughts in a review by constraining the aspect distribution dynamically among successive review segments.", "labels": [], "entities": []}, {"text": "We derive a similarity measure that considers both lexical and semantic similarity to find supporting opinions.", "labels": [], "entities": []}, {"text": "Experiments on TripAd-visor hotel reviews and Yelp restaurant reviews show that our model outperforms existing methods for modeling opinions, and the proposed framework is effective in finding supporting opinions.", "labels": [], "entities": []}], "introductionContent": [{"text": "In order to make an informed decision when booking a hotel online, a user will often read through its reviews looking for specific feedbacks.", "labels": [], "entities": []}, {"text": "For example, if he or she plans to do an early check-in and comes across a review that mentions a hasslefree early check-in as shown in, it will be helpful to know whether other guests had similar experiences.", "labels": [], "entities": []}, {"text": "If a review complains about bed: A sample hotel review bugs or noise from construction nearby, then it is important to know if that was an occasional problem based on a single user's experience or happens frequently.", "labels": [], "entities": []}, {"text": "However, it is impossible for an individual to go through the large volume of reviews to verify whether an opinion is prevalent.", "labels": [], "entities": []}, {"text": "In this work, we study the problem of finding supporting sentences from reviews that corroborate the opinions expressed in a target review sentence.", "labels": [], "entities": []}, {"text": "This is useful as it enables users to easily look for appropriate comments on the specific issues they are interested in.", "labels": [], "entities": []}, {"text": "A review is a collection of sentences where each sentence may have multiple segments separated by punctuations or conjunctions.", "labels": [], "entities": []}, {"text": "Each segment expresses an opinion that can be represented as a combination of aspect, topic and sentiment.", "labels": [], "entities": []}, {"text": "An aspect refers to the overall theme of a segment, a topic is the specific subject or issue discussed and the sentiment for each topic can be neutral, positive or negative.", "labels": [], "entities": []}, {"text": "shows the segments and the possible latent aspect, topics and sentiment fora sentence of the review in.", "labels": [], "entities": []}, {"text": "Given an opinion (in a target segment), we say that a review supports the opinion, if it contains some segment whose aspect, topic and sentiment are similar to those in the target segment.", "labels": [], "entities": []}, {"text": "Finding such supporting reviews is a challenge since reviews are typically short unstructured text and discuss a wide range of topics on various aspects with differing sentiments and vocabulary used.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform two sets of experiments to evaluate our proposed framework.", "labels": [], "entities": []}, {"text": "We first compare  Author-ATS with state-of-the-art topic models using perplexity on test data.", "labels": [], "entities": [{"text": "Author-ATS", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9122945070266724}]}, {"text": "Then we evaluate the performance of SURF, for the task of retrieving supporting opinions using human annotation, against keyword based search engine Lucene and a competent word embedding model Word2Vec.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.8965685963630676}, {"text": "Word2Vec", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.9691141247749329}]}, {"text": "We use two real world datasets: (a) hotel reviews from TripAdvisor (, and (b) restaurant reviews from yelp.com.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.945979654788971}, {"text": "yelp.com", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.9737069606781006}]}, {"text": "shows the statistics of the two datasets.", "labels": [], "entities": []}, {"text": "We pre-process both datasets by removing domain independent stopwords 1 . We retain some negation stopwords (e.g.: not, can't, didn't) and join them with the next word (so that 'not good' is treated as a single unit) to help discover sentiment properly.", "labels": [], "entities": []}, {"text": "We use common punctuations like '.', '?', '!'", "labels": [], "entities": []}, {"text": "To further split a sentence into segments we use punctuations used to separate clauses like ',', ';' and conjunctions like 'and', 'however', 'but' as separators.", "labels": [], "entities": []}, {"text": "We use a domain independent subjectivity lexicon 2 to initialize sentiment distributions.", "labels": [], "entities": []}, {"text": "Since aspect words may consist of highly co-occurring words (e.g. 'front-desk', 'walking distance') we use Pointwise Mutual Information (PMI)) to find such collocations.", "labels": [], "entities": []}, {"text": "Bigrams with PMI greater than a threshold (we use 0.05 in our experiments) are treated as a single word.", "labels": [], "entities": [{"text": "PMI", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.8413839340209961}]}, {"text": "To make the discovered aspects understandable and intuitive, we provide a few seed words to the models.", "labels": [], "entities": []}, {"text": "The seeds are only used during initialization and subsequent iterations of Gibbs sampling are not dependent on them.", "labels": [], "entities": []}, {"text": "In this set of experiments, we examine the ability of Author-ATS to capture the opinions in reviews.", "labels": [], "entities": []}, {"text": "Perplexity is derived from the likelihood of unseen test data and is a standard measure for evaluating topic models.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9631784558296204}]}, {"text": "The lower the perplexity, the less confused the model is on seeing new data, implying a better generalization power.", "labels": [], "entities": []}, {"text": "We compare with the following state-of-the-art opinion models: LDA () : A topic model where words are generated from a latent topic dimension.", "labels": [], "entities": []}, {"text": "TAM (Paul and Girju, 2010): A topic model for opinion mining where words are generated from a two-level hierarchy of aspect and topic.", "labels": [], "entities": [{"text": "TAM", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5968800783157349}, {"text": "opinion mining", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7639642953872681}]}, {"text": "JTV (): A topic model especially for contentious documents where each word has a topic and a viewpoint.", "labels": [], "entities": [{"text": "JTV", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8205376863479614}]}, {"text": "We also implement a baseline model ATS based on three-level Aspect-Topic-Sentiment hierarchy.", "labels": [], "entities": []}, {"text": "We use this model to show the performance gain by just considering a hierarchical dependency between these dimensions while capturing an opinion.", "labels": [], "entities": []}, {"text": "For Author-ATS and ATS, we use 6 aspects, 5 topics for each aspect and 3 sentiments.", "labels": [], "entities": [{"text": "ATS", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.4855975806713104}]}, {"text": "For fair comparison, we keep the total number of dimensions as close as possible across models.", "labels": [], "entities": []}, {"text": "We partition our dataset into train (80%) and test (20%) sets and report five fold cross validation results.", "labels": [], "entities": []}, {"text": "shows that ATS outperforms other models in both datasets due to its hierarchical modeling of words.", "labels": [], "entities": []}, {"text": "Author-ATS further improves the performance by considering author and entity characteristics as well as the thought patterns of the authors.", "labels": [], "entities": []}, {"text": "We note that the performance of the non-parametric model is comparable with Author-ATS, making it easier to use the model for any new domain without having much prior knowledge.", "labels": [], "entities": [{"text": "Author-ATS", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.8979549407958984}]}, {"text": "Domain Stopwords TripAdvisor hotel, nice, stay, trip, times, day, place, back Yelp good, place, food, time, order, bit, make: Top words for aspect-topic-sentiments found by Author-ATS for TripAdvisor dataset.", "labels": [], "entities": [{"text": "TripAdvisor dataset", "start_pos": 188, "end_pos": 207, "type": "DATASET", "confidence": 0.9621454775333405}]}, {"text": "For example, the first topic for aspect Room is about in-room experience ('bed','king-size','view'), whereas the second topic seems to be about bathroom ('shower', 'towels', 'tub').", "labels": [], "entities": []}, {"text": "We also observe that the model is able to obtain contextual sentiment terms which are aspect-topic coherent.", "labels": [], "entities": []}, {"text": "For example, words such as 'noise', 'night', 'hear' could be assigned negative sentiment labels for topic 0 of Room due to the context in which they are used, e.g., when describing a room, these words probably indicate a noisy room bothering their sleep at night.", "labels": [], "entities": []}, {"text": "We now evaluate Author-ATS model and LSS measure on retrieving sentences that are relevant to a target sentence.", "labels": [], "entities": [{"text": "LSS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9886007905006409}]}, {"text": "A sentence is considered relevant if it expresses similar opinions as the target sentence.", "labels": [], "entities": []}, {"text": "A sentence with multiple aspects is relevant if it expresses at least one of the opinions in the target sentence.", "labels": [], "entities": []}, {"text": "Precision of the top-k answers are manually determined by three annotators and conflicts are resolved by majority voting.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.947501540184021}]}, {"text": "Recall that LSS considers both lexical and semantic similarity.", "labels": [], "entities": []}, {"text": "The computation of semantic similarity requires the aspect-topic-sentiment distribution which is only available in the baseline ATS and Author-ATS models.", "labels": [], "entities": []}, {"text": "We define a similarity measure called CJSD that can be used by the various topic models to facilitate comparison.", "labels": [], "entities": []}, {"text": "CJSD measures the lexical similarity of two sentences as the cosine similarity of their tf-idf vectors, while the semantic similarity is measured by the similarity of their topic distributions using Jensen-Shannon Divergence(JSD) as follows: CJSD(s1, s2) = \u03bb cosine sim(s1, s2)+(1\u2212\u03bb) JSD(s1, s2) We randomly select 5 hotels from TripAdvisor and 5 restaurants from Yelp datasets.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 329, "end_pos": 340, "type": "DATASET", "confidence": 0.9377574324607849}, {"text": "Yelp datasets", "start_pos": 364, "end_pos": 377, "type": "DATASET", "confidence": 0.9803706705570221}]}, {"text": "For each hotel/restaurant, we randomly pick 10 target sentences and retrieve their supporting sentences.The topic distributions of these sentences are obtained using LDA, TAM, JTV, and the proposed models ATS and Author-ATS.", "labels": [], "entities": [{"text": "TAM", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.8446285128593445}, {"text": "ATS", "start_pos": 205, "end_pos": 208, "type": "METRIC", "confidence": 0.8768001198768616}]}, {"text": "shows the average precision for top 5, 10 and 20 results retrieved using various topic models with similarity measure CJSD.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9951807260513306}, {"text": "CJSD", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.5404826998710632}]}, {"text": "We see that Author-ATS model always outperforms other topic models for the task of retrieving supporting sentences.", "labels": [], "entities": []}, {"text": "This is consistent with the perplexity results of the models obtained previously.", "labels": [], "entities": []}, {"text": "shows the average precision using variants of the proposed model with LSS.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9979372024536133}, {"text": "LSS", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.5711566805839539}]}, {"text": "Clearly, using LSS always yields a better precision compared to using CJSD, with the best performer being the Author-ATS with LSS combination.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.99800044298172}]}, {"text": "SURF framework utilizes this combination for retrieving top-k supporting reviews.", "labels": [], "entities": [{"text": "SURF framework", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.791950911283493}]}, {"text": "Lucene: A popular keyword based ranking method.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8799571990966797}]}, {"text": "We used its default combination of vector space model and boolean model for retrieval.", "labels": [], "entities": []}, {"text": "Word2Vec: (Mikolov et al., 2013) A state-ofthe-art algorithm for word embeddings using neural network.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.93716961145401}]}, {"text": "Supporting sentences are ranked with Word Mover's distance using the word embeddings.", "labels": [], "entities": []}, {"text": "We train on TripAdvisor dataset using CBOW algorithm with context window set to 5 as recommended by the authors.", "labels": [], "entities": [{"text": "TripAdvisor dataset", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.979654848575592}]}, {"text": "We do not train Word2Vec on the Yelp dataset as it is too small.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 16, "end_pos": 24, "type": "DATASET", "confidence": 0.9265727996826172}, {"text": "Yelp dataset", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9612103700637817}]}, {"text": "We set the vector dimension to 500 based on grid search.", "labels": [], "entities": []}, {"text": "We also compare with Word2Vec model pre-trained on the large GoogleNews dataset 3 . shows the average precision for the top 5, 10 and 20 results retrieved using Lucene, Word2Vec and SURF.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9449598789215088}, {"text": "GoogleNews dataset", "start_pos": 61, "end_pos": 79, "type": "DATASET", "confidence": 0.9304161071777344}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9987694621086121}, {"text": "Lucene", "start_pos": 161, "end_pos": 167, "type": "DATASET", "confidence": 0.9643504619598389}, {"text": "Word2Vec", "start_pos": 169, "end_pos": 177, "type": "DATASET", "confidence": 0.9462879300117493}]}, {"text": "Word2Vec performs better when trained on review data, compared to the model trained on general news data.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9382192492485046}]}, {"text": "This confirms that domain knowledge is important.", "labels": [], "entities": []}, {"text": "It is evident from the results that SURF significantly outperforms existing approaches for opinion search.", "labels": [], "entities": [{"text": "SURF", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.8835891485214233}, {"text": "opinion search", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.9006423056125641}]}, {"text": "For evaluating the coherence of retrieved set of supporting reviews for an aspect, we look at their corresponding user given aspect ratings.", "labels": [], "entities": []}, {"text": "For each aspect of each review sentence, we retrieve its topk supporting sentences.", "labels": [], "entities": []}, {"text": "Then we compute the Target Sentence: bedroom had the most comfortable mattress, feather soft pillows as well as firmer ones, they thought of keeping every guest comfortable Supporting Sentences by SURF Supporting Sentences by Lucene Supporting Sentences by Word2Vec Aspect : Room Statement: bill clinton suite was huge with two baths, a wonderful jacuzzi and a comfortable bed bed was very comfortable, as were the large pillows The room had a microwave, coffemaker, hairdryer, bottled water replenished each day (x) Aspect : Room Statement: the beds are the most comfortable of any hotel I have stayed in we were recommending it for our out of town wedding guests, and wanted to make sure they were comfortable (x) It really is a shame because the bed and pillows were super comfortable and we could have had a great night sleep on both nights Aspect : Room Statement: the beds were comfortable and they had good selection of towels who would have imagined that somebody actually thought about where a guest would watch tv (x) They took regular sized hotel rooms and divided them into a sitting room with a bedroom with a door, keeping the bathroom to divide the two areas (x)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics of datasets used", "labels": [], "entities": []}, {"text": " Table 4: Perplexity values for different models.", "labels": [], "entities": []}, {"text": " Table 7: Average precision using CJSD", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9109911918640137}, {"text": "CJSD", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.7315008640289307}]}, {"text": " Table 8: Average precision using LSS", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9354358315467834}, {"text": "LSS", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.40825188159942627}]}, {"text": " Table 9: Comparison with Lucene and Word2Vec", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8565916419029236}]}]}