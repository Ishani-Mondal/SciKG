{"title": [{"text": "Morpheus: A Neural Network for Jointly Learning Contextual Lemmatization and Morphological Tagging", "labels": [], "entities": [{"text": "Morphological Tagging", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7197839617729187}]}], "abstractContent": [{"text": "In this study, we present Morpheus, a joint contextual lemmatizer and morphological tag-ger.", "labels": [], "entities": []}, {"text": "Morpheus is based on a neural sequential architecture where inputs are the characters of the surface words in a sentence and the outputs are the minimum edit operations between surface words and their lemmata as well as the morphological tags assigned to the words.", "labels": [], "entities": []}, {"text": "The experiments on the datasets in nearly 100 languages provided by SigMorphon 2019 Shared Task 2 organizers show that the performance of Morpheus is comparable to the state-of-the-art system in terms of lemmatization.", "labels": [], "entities": [{"text": "SigMorphon 2019 Shared Task 2 organizers", "start_pos": 68, "end_pos": 108, "type": "DATASET", "confidence": 0.834751566251119}]}, {"text": "In morphological tagging, on the other hand, Mor-pheus significantly outperforms the SigMor-phon baseline.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.779076874256134}]}, {"text": "In our experiments, we also show that the neural encoder-decoder architecture trained to predict the minimum edit operations can produce considerably better results than the architecture trained to predict the characters in lemmata directly as in previous studies.", "labels": [], "entities": []}, {"text": "According to the SigMorphon 2019 Shared Task 2 results, Morpheus has placed 3 rd in lemmatization and reached the 9 th place in morphological tagging among all participant teams.", "labels": [], "entities": [{"text": "SigMorphon 2019 Shared Task 2", "start_pos": 17, "end_pos": 46, "type": "DATASET", "confidence": 0.7430511951446533}, {"text": "morphological tagging", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.7615457773208618}]}], "introductionContent": [{"text": "Lemmatization is the process of reducing an inflected word into its dictionary form known as the lemma.", "labels": [], "entities": []}, {"text": "Morphological tagging, on the other hand, is the process of marking up words with their morphological information and part of speech (POS) tags.", "labels": [], "entities": [{"text": "Morphological tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8090786635875702}]}, {"text": "Lemmatization and morphological tagging are essential tasks in natural language processing since they usually represent initial steps of subsequent tasks such as dependency parsing) and semantic role labeling ().", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.7092390656471252}, {"text": "dependency parsing", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7816444039344788}, {"text": "semantic role labeling", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.6556888024012247}]}, {"text": "Morphological information of words is utilized in various tasks including statistical machine translation (, neural machine translation ( and named entity recognition ( to improve the performance.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7046038409074148}, {"text": "neural machine translation", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.7086603244145712}, {"text": "named entity recognition", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.6605908075968424}]}, {"text": "Morphological tagging and lemmatization is crucial especially in morphologically rich languages such as Turkish and Finnish since inflected and derived words carry a substantial amount of information such as number, person, case, tense and aspect.", "labels": [], "entities": [{"text": "Morphological tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8804293870925903}]}, {"text": "Moreover, lexical ambiguities can occur in highly inflectional and derivational languages such as Turkish.", "labels": [], "entities": []}, {"text": "The correct lemma and morphological tags may differ according to the context in which words appear.", "labels": [], "entities": []}, {"text": "As shown in table 1, the Turkish word \"dolar\" may have different lemma and morphological properties according to the context it is used.", "labels": [], "entities": []}, {"text": "To achieve lemmatization and morphological tagging in highly inflectional languages, traditional approaches employ finite state machines which are constructed to model grammatical rules of a language.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.722245991230011}]}, {"text": "Building a state machine for morphological analysis is not a trivial task and requires considerable effort necessitating linguistic knowledge.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7228824347257614}]}, {"text": "Furthermore, morphological analyzers frequently produce multiple analyzes for each word which introduces morphological ambiguities.", "labels": [], "entities": []}, {"text": "Morphological disambiguation which is the process of selecting correct analyzes of words according to the context () is mostly needed after morphological analysis step.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8682767152786255}]}, {"text": "Morphological disambiguation is also a difficult problem due to the fact that it requires the classification of both lemmata and the corresponding labels.", "labels": [], "entities": [{"text": "Morphological disambiguation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9220617115497589}]}, {"text": "Therefore, researchers have studied language-agnostic data-driven solutions for both lemmatization and morphological tagging.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.6674834191799164}]}, {"text": "In most of the studies, applying machine learning or statistical methods over morphologically an-) utilized a Maximum Entropy Classifier to find lemmata and morphological tags of each word in a sentence.", "labels": [], "entities": []}, {"text": "Two separate classifiers are employed in their architecture: one for assigning morphological tags to the words and one for predicting the shortest edit script between the surface word and its lemma.", "labels": [], "entities": []}, {"text": "Shortest edit script is the shortest sequence of instructions (insertions, deletions, and replacements) which transforms a string into another one.", "labels": [], "entities": []}, {"text": "In this way, the system is able to apply lemmatization to out of vocabulary words by predicting the transformation which should be applied to the surface word to obtain the lemma of the word.", "labels": [], "entities": []}, {"text": "More recent work, namely Lemming () has out-performed Morfette by using a Conditional Random Field classifier to classify each candidate sequences of lemmata and morphological tags jointly.", "labels": [], "entities": []}, {"text": "The feature space of Lemming differs from Morfette as Lemming also uses external lexical features such as the occurrences of a candidate lemma in a dictionary.", "labels": [], "entities": []}, {"text": "As deep neural networks gain popularity and lead state-of-the-art results in various natural language processing tasks, sequential neural networks have been successfully employed for lemmatization and morphological tagging in recent studies.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 201, "end_pos": 222, "type": "TASK", "confidence": 0.6994369924068451}]}, {"text": "Promising results are obtained through standard encoder-decoder neural architectures where inputs are the character sequences of the words and outputs are the character sequences of lemmata and morphological tags.", "labels": [], "entities": []}, {"text": "Neural architectures which are designed to predict the edit operations between surface words and lemmata are also proposed in recent works (.", "labels": [], "entities": []}, {"text": "The current state of the art is held by using a neural hard attention mechanism to align the characters of surface words and 1 https://unimorph.github.io/ lemmata.", "labels": [], "entities": []}, {"text": "Morphological tagging and lemmatization are jointly modeled in their architecture and a dynamic programming approach is used to maximize both morphological tagging and lemmatization scores.", "labels": [], "entities": [{"text": "Morphological tagging", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.602509081363678}]}, {"text": "In SigMorphon 2019 workshop, a shared task about morphological tagging and contextual lemmatization in nearly 100 distinct languages is organized (.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6296912431716919}]}, {"text": "In this study, we propose a neural network architecture, namely Morpheus for SigMorphon 2019 Shared Task 2.", "labels": [], "entities": [{"text": "SigMorphon 2019 Shared Task 2", "start_pos": 77, "end_pos": 106, "type": "DATASET", "confidence": 0.8351634025573731}]}, {"text": "Our architecture is inspired by MorphNet (, which has produced promising results in Turkish using an encoderdecoder neural architecture.", "labels": [], "entities": []}, {"text": "In MorphNet, all characters are represented with a vector, and word vectors are generated by using long short term memories (LSTM) over character vectors.", "labels": [], "entities": []}, {"text": "Another bidirectional LSTM is applied over word vectors to obtain context-aware representations of each word in a sentence.", "labels": [], "entities": []}, {"text": "An LSTM based decoder inputs context-aware word representations and produces lemmata and morphological tags, respectively.", "labels": [], "entities": []}, {"text": "Our architecture differs from MorphNet as we use two separate decoders for generating lemmata and morphological tags.", "labels": [], "entities": []}, {"text": "Another difference of our architecture is that we follow the minimum edit script prediction approach considering the promising performance outputs of prior work ().", "labels": [], "entities": [{"text": "minimum edit script prediction", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.6304100826382637}]}, {"text": "The lemma decoder of our network is optimized to predict the minimum edit operations between surface words and lemmata instead of predicting the character sequences of the lemmata as in MorphNet and Lematus.", "labels": [], "entities": []}, {"text": "Our experiments show that predicting the minimum edit operations instead of characters improves the performance significantly on UniMorph dataset, which is provided in SigMorphon 2019 Shared Task 2.", "labels": [], "entities": [{"text": "UniMorph dataset", "start_pos": 129, "end_pos": 145, "type": "DATASET", "confidence": 0.8137048184871674}, {"text": "SigMorphon 2019 Shared Task 2", "start_pos": 168, "end_pos": 197, "type": "DATASET", "confidence": 0.8179517149925232}]}, {"text": "The performance of the proposed architecture is comparable to the current state-of-the-art system (, which is provided as a strong baseline by SigMorphon 2019 organizers.", "labels": [], "entities": []}, {"text": "All of the experiments in this paper are reproducible using the codes we make publicly available 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we train and evaluate the proposed architecture on UniMorph dataset collection  The same settings are used in the training of the architecture for each language.", "labels": [], "entities": [{"text": "UniMorph dataset collection", "start_pos": 71, "end_pos": 98, "type": "DATASET", "confidence": 0.8320287466049194}]}, {"text": "The input character embedding length d a is set to 128 while the length of the word vectors d e is set to 1024 and the length of the context-aware word vectors dc is set to 2048.", "labels": [], "entities": []}, {"text": "The length of character vectors in the minimum edit prediction component d u and the length of the morphological tag vectors d v are set to 256 while the hidden unit sizes in decoder LSTMs d g and d q are set to 1024.", "labels": [], "entities": []}, {"text": "We use Adam optimization algorithm) with learning rate 3e \u2212 4 for minimizing the loss  function.", "labels": [], "entities": []}, {"text": "Dropout rate 0.4 is applied to all connections during model training for regularization.", "labels": [], "entities": [{"text": "Dropout rate 0.4", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9503575563430786}]}, {"text": "All the weights are initialized with Xavier initialization method.", "labels": [], "entities": []}, {"text": "We use an early stop mechanism which stops the training after four consecutive epochs without improvement on validation set.", "labels": [], "entities": []}, {"text": "presents the lemmatization and morphological performances of the proposed method on UniMorph dataset collection.", "labels": [], "entities": [{"text": "UniMorph dataset collection", "start_pos": 84, "end_pos": 111, "type": "DATASET", "confidence": 0.7585107187430064}]}, {"text": "The lemmatization accuracy on a dataset is the proportion of the number of correctly found lemmata over the total lemmata count.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9719399809837341}]}, {"text": "The lemmatization accuracy given in is the average of the accuracies obtained over the validation sets of all languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9036279916763306}, {"text": "accuracies", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9594895243644714}]}, {"text": "The performance of morphological tagging is measured by the F1 score calculated over the predicted and actual individual morphological tags.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.830465704202652}, {"text": "F1 score", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.978816956281662}]}, {"text": "In addition to the performance of the proposed architecture with minimum edit prediction decoder, the performance of the architecture with character prediction decoder is also given.", "labels": [], "entities": []}, {"text": "The performances of SigMorphon 2019 neural baseline, Turku NLP system () which is the best lemmatization performer in and UPP-SALA Uni system which is the best morphological tagging performer in CONLL 2018 are also given.", "labels": [], "entities": [{"text": "SigMorphon 2019 neural baseline", "start_pos": 20, "end_pos": 51, "type": "DATASET", "confidence": 0.8274318724870682}, {"text": "Turku NLP system", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.8204572598139445}]}, {"text": "Although the dataset provided in CONLL 2018 share the same basis with the dataset provided in SigMorphon, important differences exist between them.", "labels": [], "entities": [{"text": "CONLL 2018", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9010968804359436}, {"text": "SigMorphon", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8930359482765198}]}, {"text": "Hence the performances of Turku NLP and UPPSALA Uni are not directly comparable to our systems and SigMorphon baselines.", "labels": [], "entities": [{"text": "Turku NLP", "start_pos": 26, "end_pos": 35, "type": "DATASET", "confidence": 0.9243734180927277}, {"text": "UPPSALA Uni", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9372474551200867}, {"text": "SigMorphon baselines", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.8632864356040955}]}, {"text": "However, we present the performances of those systems averaged on the same languages in SigMorphon dataset to provide an idea of how much improvement is achieved over a year.", "labels": [], "entities": [{"text": "SigMorphon dataset", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.953102171421051}]}, {"text": "According to the results, the proposed architecture, Morpheus performs slightly better than the SigMorphon neural baseline in terms of the average lemmatization accuracy.", "labels": [], "entities": [{"text": "SigMorphon neural baseline", "start_pos": 96, "end_pos": 122, "type": "DATASET", "confidence": 0.872314433256785}, {"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.896362841129303}]}, {"text": "Similarly, for the morphological tagging task, Morpheus with a minimum edit prediction decoder significantly outperforms the baseline and Morpheus with character prediction decoder The experiments show that the performance is improved considerably when the minimum edit prediction decoder is used instead of the character prediction decoder.", "labels": [], "entities": [{"text": "morphological tagging task", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8824038902918497}]}, {"text": "An im-pressive result is that the performance of morphological tagging is also enhanced by employing the minimum edit prediction decoder.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7134325206279755}]}, {"text": "shows the lemmatization and morphological performances of both character prediction and minimum edit prediction models for each language.", "labels": [], "entities": [{"text": "character prediction", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.8032394051551819}]}, {"text": "The performance of the minimum edit prediction model is better than the character prediction model in almost all languages.", "labels": [], "entities": [{"text": "edit prediction", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.6658599823713303}, {"text": "character prediction", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.885733425617218}]}, {"text": "shows that there is a correlation between the size of the training data and the improvement on the performance when the minimum edit prediction decoder is employed.", "labels": [], "entities": []}, {"text": "For instance, the relative lemmatization improvement is extreme in languages with relatively small dataset such as Tagalog-TRG (400 tokens/0.75 relative improvements), Komi-Zyrian (1.1K tokens/0.78 relative improvement) and Akkadian (1.7 tokens/0.44 relative improvement).", "labels": [], "entities": []}, {"text": "On the other hand, in languages with large size dataset such as SpanishAnCora (496K tokens), Catalan-AnCora (480K tokens) and French-GSD (359K tokens), the improvement is relatively low (0.006, 0.007 and 0.01, respectively).", "labels": [], "entities": []}, {"text": "Although improvement magnitude is highly correlated with the training dataset size, there must be other factors specific to the properties of the language.", "labels": [], "entities": []}, {"text": "For instance, the dataset size of the language Marathi-UFAL is small (4.1K tokens).", "labels": [], "entities": []}, {"text": "However, the improvement degree is also small (0.03 relative improvement).", "labels": [], "entities": [{"text": "improvement degree", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.9251990020275116}]}, {"text": "To investigate in which cases the edit prediction model performs better, we explore the outputs of the models for English and Turkish languages.", "labels": [], "entities": [{"text": "edit prediction", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.748084157705307}]}, {"text": "A significant portion of the errors in the character prediction model is observed in unseen words and proper nouns.", "labels": [], "entities": [{"text": "character prediction", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.9187595844268799}]}, {"text": "Some of the errors made by the character prediction based model and corrected by the edit prediction based model are shown in.", "labels": [], "entities": []}, {"text": "A possible reason is that the lemmatization of a singular nominative noun which is rarely seen in the training data is easier to edit prediction model since all of the edit operations are Same and the model should only produce a sequence of Same labels.", "labels": [], "entities": []}, {"text": "Character prediction based model, on the other hand, have to learn to reproduce the word from scratch.", "labels": [], "entities": [{"text": "Character prediction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8117466568946838}]}, {"text": "Additionally, we observe a significant amount of samples in which the edit prediction model produced morphological tags and lemmata more appropriate to the context than the outputs of the character prediction model.", "labels": [], "entities": []}, {"text": "As a result, further research is needed to understand in which cases the edit prediction decoder helps to better learning of morphological properties of a language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average lemmatization and morphological tagging performances of the systems on UniMorph dataset", "labels": [], "entities": [{"text": "UniMorph dataset", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.8446384966373444}]}, {"text": " Table 3: Lemmatization and Morphological Tagging  performances of minimum edit prediction model and  character prediction model on development sets", "labels": [], "entities": [{"text": "Morphological Tagging", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7717986404895782}, {"text": "character prediction", "start_pos": 102, "end_pos": 122, "type": "TASK", "confidence": 0.8152325451374054}]}]}