{"title": [{"text": "Bootstrapping UD treebanks for Delexicalized Parsing", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 14, "end_pos": 26, "type": "DATASET", "confidence": 0.7394292801618576}]}], "abstractContent": [{"text": "Standard approaches to treebanking traditionally employ a waterfall model (Som-merville, 2010), where annotation guidelines guide the annotation process and insights from the annotation process in turn lead to subsequent changes in the annotation guidelines.", "labels": [], "entities": []}, {"text": "This process remains a very expensive step in creating linguistic resources fora target language, necessitates both linguistic expertise and manual effort to develop the annotations and is subject to inconsistencies in the annotation due to human errors.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative approach to treebanking-one that requires writing grammars.", "labels": [], "entities": []}, {"text": "This approach is motivated specifically in the context of Universal Dependencies, an effort to develop uniform and cross-lingually consistent tree-banks across multiple languages.", "labels": [], "entities": []}, {"text": "We show here that a bootstrapping approach to treebanking via interlingual grammars is plausible and useful in a process where grammar engineering and treebanking are jointly pursued when creating resources for the target language.", "labels": [], "entities": []}, {"text": "We demonstrate the usefulness of synthetic treebanks in the task of delexicalized parsing, a task of interest when working with languages with no linguistic resources and corpora.", "labels": [], "entities": [{"text": "delexicalized parsing", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.564150020480156}]}, {"text": "Experiments with three languages reveal that simple models for treebank generation are cheaper than human annotated treebanks, especially in the lower ends of the learning curves for delexicalized parsing, which is relevant in particular in the context of low-resource languages.", "labels": [], "entities": [{"text": "treebank generation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7745576202869415}]}], "introductionContent": [{"text": "Treebanking remains a vital step in the process of creating linguistic resources fora languagea practice that was established in the last 2-3 decades (.", "labels": [], "entities": []}, {"text": "The process of treebanking involves training human annotators in order to obtain high-quality annotations.", "labels": [], "entities": []}, {"text": "This is a human-intensive and costly process where multiple iterations are performed to refine the quality of the linguistic resource.", "labels": [], "entities": []}, {"text": "Grammar engineering is a complementary approach to creating linguistic resources: one that requires a different kind of expertise.", "labels": [], "entities": [{"text": "Grammar engineering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8278751969337463}]}, {"text": "These two approaches have remained orthogonal for obvious reasons: treebanks are primarily necessary to induce abstractions in NLU (Natural Language Understanding) models from data, while grammars are themselves abstractions arising from linguistic knowledge.", "labels": [], "entities": []}, {"text": "Abstractions induced from data have proven themselves to be useful for robust NLU tasks, while grammars are better at precision tasks involving NLG (Natural Language Generation).", "labels": [], "entities": [{"text": "Natural Language Generation)", "start_pos": 149, "end_pos": 177, "type": "TASK", "confidence": 0.6841811165213585}]}, {"text": "Given the resources required for treebanking, synthetic treebanks have been proposed and used as substitute in cross-lingual parsing for languages where treebanks do not exist.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7215345501899719}]}, {"text": "Such treebanks are created using parallel corpora where parse trees in one language are bootstrapped into a target language using alignment information through annotation projection) or using machine translation systems to bootstrap existing treebanks in one or more source language(s) to the target language.", "labels": [], "entities": []}, {"text": "More recently, synthetic treebanks are generated for both real and artificial languages using multilingual treebanks by learning feasible parameter combinations () - show that such treebanks can be useful to select the most similar language to train a parsing model for an unknown language.", "labels": [], "entities": []}, {"text": "At the same time, grammar-based treebanking approaches have been shown to work in monolingual setups-to derive rich linguistic representations defined by explicit grammars).", "labels": [], "entities": []}, {"text": "These approaches are carried out by parsing raw corpora with a target grammar and using an additional human disambiguation phase.", "labels": [], "entities": []}, {"text": "Alternatively, existing treebanks are matched against the target grammar further reducing the human effort in disambiguation: these approaches face a challenge of under-specification in the source treebanks).", "labels": [], "entities": []}, {"text": "In the current paper, we propose a hybrid of these two methods: we use abstract syntax grammars as core linguistic abstraction to generate synthetic treebanks fora grammar that can be translated to target representations with high precision.", "labels": [], "entities": []}, {"text": "The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades.", "labels": [], "entities": []}, {"text": "This question has also been extensively addressed in the context of dependency treebanks.", "labels": [], "entities": []}, {"text": "We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (.", "labels": [], "entities": []}, {"text": "The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency.", "labels": [], "entities": []}, {"text": "We study the efficacy of these dependency treebanks using learning curves of a transition-based parser in a delexicalized parsing setup.", "labels": [], "entities": []}, {"text": "The delexicalized parsing setup allows for generation of parallel UD treebanks in multiple languages with minimal prerequisites on language-specific knowledge.", "labels": [], "entities": []}, {"text": "Another rationale behind the the current work in the context of cross-lingual parsing is while synthetic treebanks offer a \"cheap\" alternative, the signal for the target language is limited by the quality of the MT system.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7302087545394897}, {"text": "MT", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.9529989957809448}]}, {"text": "On the other hand, interlingua grammars provide a high-quality signal about the target language.", "labels": [], "entities": []}, {"text": "High quality using interlingual grammars refers to accurate generation of word-order and morphology -although lexical selection in translation is still a problem.", "labels": [], "entities": []}, {"text": "There have not been previous attempts in cross-lingual parsing to our knowledge studying the effect of these.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7115764319896698}]}, {"text": "This paper is structured as follows: Section 2 gives the relevant background on interlingua grammars and the algorithm used to generate UD trees given treebank derived from an interlingua grammar.", "labels": [], "entities": []}, {"text": "Section 3 describes our algorithm to bootstrap treebanks fora given interlingua grammar and parallel UD treebanks from them along with an intrinsic evaluation of these bootstrapped UD treebanks.", "labels": [], "entities": []}, {"text": "Section 4 shows the parsing setup we use and Section 5 details the results of the parsing experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9777069091796875}, {"text": "parsing", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.9629618525505066}]}], "datasetContent": [{"text": "We ran experiments with 3 languages -English, Swedish and Finnish in this paper.", "labels": [], "entities": []}, {"text": "In addition to the availability of a concrete syntax for the language, our approach also requires concrete configurations for the languages () in order to bootstrap full UD trees.", "labels": [], "entities": []}, {"text": "shows statistics about the concrete configurations for the RGL grammar for the languages.", "labels": [], "entities": []}, {"text": "The probability distribution defined on the RGL was estimated using the GF-Penn treebank of English.", "labels": [], "entities": [{"text": "RGL", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.907759964466095}, {"text": "GF-Penn treebank of English", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.9491503238677979}]}, {"text": "This raises another question -how well does the distribution defined on the abstract syntax of the RGL estimated from monolingual data transfer across other languages.", "labels": [], "entities": []}, {"text": "The bootstrapping algorithm was restricted to generate 20K ASTs of depth less than 10.", "labels": [], "entities": []}, {"text": "We use UDPipe ( to train parsing models, using comparable settings to the baseline systems provided in the CoNLL18 shared task.", "labels": [], "entities": [{"text": "CoNLL18 shared task", "start_pos": 107, "end_pos": 126, "type": "DATASET", "confidence": 0.8485812346140543}]}, {"text": "Gold tokenization and part-of-speech tags are used in both training and testing the parser.", "labels": [], "entities": []}, {"text": "This was done to control for differences in tagging performance across the synthetic and original UD treebanks.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.8492923080921173}]}, {"text": "The models are trained using the primary treebanks from Universal Dependencies v2.3 distribution.", "labels": [], "entities": [{"text": "Universal Dependencies v2.3 distribution", "start_pos": 56, "end_pos": 96, "type": "DATASET", "confidence": 0.8078071624040604}]}, {"text": "We plot the learning curves for parsing: Estimate of the effort required in gf2ud.", "labels": [], "entities": [{"text": "parsing", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.9827337265014648}, {"text": "Estimate", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9909186363220215}]}, {"text": "The abstract configurations are the same for all languages, while the concrete functions and morph-features are defined for each language.", "labels": [], "entities": []}, {"text": "The first column corresponds to configurations for syntactic constructors in the RGL, and second column corresponds to constructors that use syncategorematic words in the linearization.", "labels": [], "entities": [{"text": "RGL", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8949921131134033}]}, {"text": "models in trained on both the original and synthetic treebank data for each use case outlined in Section 4.", "labels": [], "entities": []}, {"text": "The learning curves were plotted using the LAS accuracies obtained on the test set for the three languages using models trained on both the original and the synthetic treebanks.", "labels": [], "entities": [{"text": "LAS accuracies", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9625177979469299}]}, {"text": "It is seen from the learning curves that models trained on the synthetic treebanks do not outperform the models trained using original UD treebanks.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.8227721750736237}]}, {"text": "However, the full learning curves shown in do not tell the complete story.", "labels": [], "entities": []}, {"text": "shows the learning curves (visualized using bar plots) for English, Finnish and Swedish in the setup where less than 1K sentences from UD treemade obsolete in UD v2.3 distribution -with all treebanks being assigned a code.", "labels": [], "entities": []}, {"text": "So, we use the term primary in this paper to refer to EWT for English, TDT for Finnish and Talbanken for Swedish.", "labels": [], "entities": [{"text": "EWT", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.8013601303100586}, {"text": "TDT", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8382906913757324}]}, {"text": "It is clear from the plots for all the three languages that the synthetic treebanks are sub-optimal when directly compared against real treebanks of the same size.", "labels": [], "entities": []}, {"text": "However, what is interesting is that parsing models in this range (i.e. N \u2264 1K) with synthetic treebanks quickly reach comparable accuracies to using real treebank data, with an approximate effective data coefficient of 2.0.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9631911516189575}]}, {"text": "In other words comparable accuracies can be obtained using roughly twice the amount of synthetic data, generated for free by the abstract syntax grammar.", "labels": [], "entities": []}, {"text": "It is interesting to note that the learning curves using the synthetic data for the English parsing models become comparably flat in our setup with less than 5K sentences (shown in).", "labels": [], "entities": [{"text": "English parsing", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.5816294848918915}]}, {"text": "Despite the lower improvements with increasing treebank sizes, there is still a consistent improvement in parsing accuracies with the best accuracy of 65.4 LAS using 10K synthetic samples (shown in).", "labels": [], "entities": [{"text": "parsing", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.9658433198928833}, {"text": "accuracies", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.6533277034759521}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9991451501846313}, {"text": "LAS", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9512555599212646}]}, {"text": "This pattern is consistent across Swedish and Finnish, which allows us to draw the conclusion that while the effective data coefficient is smaller, the synthetic treebanks are still useful to improve parsing accuracies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 200, "end_pos": 207, "type": "TASK", "confidence": 0.9695958495140076}]}], "tableCaptions": [{"text": " Table 1: Entropy values of probability distri- butions P(label-(head-pos)) for different lan- guages estimated from real (P UD ) and boot- strapped (P GF ) treebanks. If a language has more  than one treebank in the UD distribution, we se- lect one treebank as the primary treebank and use  that to estimate the distribution and in the parsing  experiments. Languages for which a UD treebank  does not exist but is included in GF-RGL are listed  towards the bottom of the table.", "labels": [], "entities": []}, {"text": " Table 2: Estimate of the effort required in gf2ud.", "labels": [], "entities": [{"text": "Estimate", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977425336837769}]}]}