{"title": [{"text": "Findings of the WMT 2019 Shared Task on Parallel Corpus Filtering for Low-Resource Conditions", "labels": [], "entities": [{"text": "WMT 2019 Shared Task", "start_pos": 16, "end_pos": 36, "type": "DATASET", "confidence": 0.7611589431762695}]}], "abstractContent": [{"text": "Following the WMT 2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018), we posed the challenge of assigning sentence-level quality scores for very noisy corpora of sentence pairs crawled from the web, with the goal of sub-selecting 2% and 10% of the highest-quality data to be used to train machine translation systems.", "labels": [], "entities": [{"text": "WMT 2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018)", "start_pos": 14, "end_pos": 84, "type": "TASK", "confidence": 0.6748477478822072}, {"text": "machine translation", "start_pos": 305, "end_pos": 324, "type": "TASK", "confidence": 0.7366068065166473}]}, {"text": "This year, the task tackled the low resource condition of Nepali-English and Sinhala-English.", "labels": [], "entities": []}, {"text": "Eleven participants from companies, national research labs, and universities participated in this task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) has experienced significant advances in recent years thanks to improvements in modeling, and in particular neural models (.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8995055198669434}]}, {"text": "Unfortunately, today's neural machine translation models, perform poorly on low-resource language pairs, for which clean, parallel training data is high-quality training data is lacking, by definition (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.738647441069285}]}, {"text": "Improving performance on low resource language pairs is very impactful considering that these languages are spoken by a large fraction of the world population.", "labels": [], "entities": []}, {"text": "This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6995746493339539}]}, {"text": "In face of the scarcity of clean parallel data, learning to translate from any multilingual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl 1 ) is an important option.", "labels": [], "entities": []}, {"text": "1 http://www.paracrawl.eu/ Recently, there is an increased interest in the filtering of noisy parallel corpora to increase the amount of data that can be used to train translation systems ( . While the state-of-the-art methods that use NMT models have proven effective in mining parallel sentences) for high-resource languages, their effectiveness has not been tested in low-resource languages.", "labels": [], "entities": []}, {"text": "The implications of low availability of training data for parallel-scoring methods is not known yet.", "labels": [], "entities": []}, {"text": "The Shared Task on Parallel Corpus Filtering at the Conference for Machine Translation) was organized to promote research to learning from noisy data more viable for low-resource languages.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7045511156320572}]}, {"text": "Compared to last year's edition ( , we only provide about 50-60 million word noisy parallel data, as opposed to 1 billion words.", "labels": [], "entities": []}, {"text": "We also provide only a few million words of clean parallel data of varying quality, instead of over 100 million words of high-quality parallel data.", "labels": [], "entities": []}, {"text": "Participants developed methods to filter web-crawled Nepali-English and Sinhala-English parallel corpora by assigning a quality score for each sentence pair.", "labels": [], "entities": []}, {"text": "These scores are used to filter the web crawled corpora down to fixed sizes (1 million and 5 million English words), trained statistical and neural machine translation systems on these subsets, and measured their quality with the BLEU score on a test set of multi-domain Wikipedia content . This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes and the average sentence length of sub-selected data.", "labels": [], "entities": [{"text": "statistical and neural machine translation", "start_pos": 125, "end_pos": 167, "type": "TASK", "confidence": 0.7392757117748261}, {"text": "BLEU", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.9989093542098999}]}], "datasetContent": [{"text": "The testing setup mirrors the development environment that we provided to the participants.", "labels": [], "entities": []}, {"text": "Given a selected subset of a given size fora system submission, we built statistical (SMT) and neural machine translation (NMT) systems to evaluate the quality of the selected sentence pairs.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.8017393151919047}]}, {"text": "NMT For neural machine translation, we used fairseq (Ott et al., 2019) transformer model with the parameter settings shown in.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8989846110343933}, {"text": "neural machine translation", "start_pos": 8, "end_pos": 34, "type": "TASK", "confidence": 0.6492880880832672}, {"text": "fairseq (Ott et al., 2019) transformer", "start_pos": 44, "end_pos": 82, "type": "DATASET", "confidence": 0.8438938988579644}]}, {"text": "Preprocessing was done with sentence piece fora 5000 subword vocabulary on tokenized text using the Moses tokenizer (but no truecasing was used).", "labels": [], "entities": [{"text": "Preprocessing", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.89969801902771}]}, {"text": "Decoding was done with beam size 5 and length normalization 1.2.", "labels": [], "entities": [{"text": "length normalization 1.2", "start_pos": 39, "end_pos": 63, "type": "METRIC", "confidence": 0.9244483113288879}]}, {"text": "Training a system for the 1 million, and 5 million subsets took about 3, and 13 hours, respectively, on a single GTX 1080ti GPU.", "labels": [], "entities": []}, {"text": "Scores on the test sets were computed with Sacrebleu.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Provided clean parallel data for Nepali.", "labels": [], "entities": []}, {"text": " Table 2: Provided clean parallel data for Sinhala.", "labels": [], "entities": []}, {"text": " Table 3: Provided clean monolingual data.", "labels": [], "entities": []}, {"text": " Table 4: Hindi corpora released as related language  data from the IIT Bombay English-Hindi Corpus.", "labels": [], "entities": [{"text": "IIT Bombay English-Hindi Corpus", "start_pos": 68, "end_pos": 99, "type": "DATASET", "confidence": 0.8893777132034302}]}, {"text": " Table 5: Noisy parallel data to be filtered (de- duplicated raw output Paracrawl pipeline).", "labels": [], "entities": []}, {"text": " Table 6: Examples of good sentence pairs from the noisy corpus for Nepali-English and Sinhala-English.", "labels": [], "entities": []}, {"text": " Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsam- pled data sets. Word counts are obtained with wc on tokenized text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7332209348678589}, {"text": "subsam- pled data sets", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.8065924048423767}]}, {"text": " Table 9: Results for Nepali: BLEU scores are reported for systems trained on 1, 2, and 5 million word subsets of  the data, subsampled based on the quality scores provided by the participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9936832189559937}]}, {"text": " Table 10: Results for Sinhala: BLEU scores are reported for systems trained on 1, 2, and 5 million word subsets  of the data, subsampled based on the quality scores provided by the participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9967315196990967}]}, {"text": " Table 11: Number of sentences and the corresponding  average sentence length (counting English words) for  Nepali.", "labels": [], "entities": []}, {"text": " Table 12: Number of sentences and the corresponding  average sentence length (counting English words) for  Sinhala.", "labels": [], "entities": []}, {"text": " Table 13: Overlap for Nepali, 1 million word data condition. For each submission, a row in the table lists the  total number of sentence pairs, the ratio of unique sentence pairs that are in included in no other submission, and  the ratio of sentence pairs shared with each of the other submissions.  Submissions from different participants share up to 67.9% of sentence pairs (NRC ensemble and Facebook main).", "labels": [], "entities": [{"text": "NRC ensemble", "start_pos": 379, "end_pos": 391, "type": "DATASET", "confidence": 0.9726066589355469}]}, {"text": " Table 15: Overlap for Sinhala, 1 million word data condition. For each submission, a row in the table lists the  total number of sentence pairs, the ratio of unique sentence pairs that are in included in no other submission, and  the ratio of sentence pairs shared with each of the other submissions.  There is less overlap between submissions, compared to Nepali. The submissions share almost always below half  of the sentence pairs.", "labels": [], "entities": []}]}