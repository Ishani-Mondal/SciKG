{"title": [{"text": "Comparing linear and neural models for competitive MWE identification", "labels": [], "entities": [{"text": "MWE", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9620130062103271}]}], "abstractContent": [{"text": "In this paper, we compare the use of linear versus neural classifiers in a greedy transition system for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.9847264885902405}]}, {"text": "Both our linear and neural models achieve anew state-of-the-art on the PARSEME 1.1 shared task data sets, comprising 20 languages.", "labels": [], "entities": [{"text": "PARSEME 1.1 shared task data sets", "start_pos": 71, "end_pos": 104, "type": "DATASET", "confidence": 0.8446534971396128}]}, {"text": "Surprisingly, our best model is a simple feed-forward network with one hidden layer, although more sophisticated (recurrent) architectures were tested.", "labels": [], "entities": []}, {"text": "The feedback from this study is that tuning a SVM is rather straightforward, whereas tuning our neural system revealed more challenging.", "labels": [], "entities": []}, {"text": "Given the number of languages and the variety of linguistic phenomena to handle for the MWE identification task, we have designed an accurate tuning procedure, and we show that hyper-parameters are better selected by using a majority-vote within random search configurations rather than a simple best configuration selection.", "labels": [], "entities": [{"text": "MWE identification task", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.9627887010574341}]}, {"text": "Although the performance is rather good (better than both the best shared task system and the average of the best per-language results), further work is needed to improve the generalization power, especially on unseen MWEs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-word expressions (MWE) are composed of several words (more precisely of elements that are words in some contexts) that exhibit irregularities at the morphological, syntactic and/or semantic level.", "labels": [], "entities": [{"text": "Multi-word expressions (MWE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7053101122379303}]}, {"text": "For instance, \"prendre la porte\" is a French verbal expression with semantic and morphological idiosyncrasy because (1) its idiomatic meaning (\"to leave the room\") differs from its literal meaning (\"to take the door\") and (2) the idiomatic reading would be lost if \"la porte\" were used in the plural.", "labels": [], "entities": []}, {"text": "Identifying MWE is known to be challenging ( , due to the highly lexical nature of the MWE status, the various degrees of the MWE irregularities and the various linguistic levels in which these show.", "labels": [], "entities": [{"text": "Identifying MWE", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8804791569709778}]}, {"text": "In this paper we focus on the task of identifying verbal MWEs, which have been the focus of two recent shared tasks, accompanied by data sets for 20 languages: PARSEME shared task ST.0 ( and ST.1 (.", "labels": [], "entities": [{"text": "identifying verbal MWEs", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.6707826058069865}, {"text": "PARSEME shared task ST.0", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.5082832723855972}]}, {"text": "Verbal MWEs are rather rare (one every 4 sentences overall in ST1.1 data sets) but being predicates, they are crucial to downstream semantic tasks.", "labels": [], "entities": [{"text": "Verbal MWEs", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7471102476119995}, {"text": "ST1.1 data sets", "start_pos": 62, "end_pos": 77, "type": "DATASET", "confidence": 0.8149170974890391}]}, {"text": "They are unfortunately even more difficult to identify than other categories of MWEs: they are more likely to be discontinuous sequences and to exhibit morphological and structural variation, if only the verb generally shows full inflectional variation, allows adverbial modification and in some cases syntactic reordering such as relativization.", "labels": [], "entities": []}, {"text": "Our starting point to address the MWE identification task is to reuse the system of Al, an enhanced version of the winning system of ST.0, a transition system using a linear (SVM) model.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.9857822954654694}]}, {"text": "Our objective has been to incorporate neural methods, which are overwhelming in current NLP systems.", "labels": [], "entities": []}, {"text": "Neural networks have brought substantial performance improvements on a large variety of NLP tasks including transitionbased parsing (e.g. or), in particular thanks to the use of distributed representations of atomic labels, their ability to capture contextual information.", "labels": [], "entities": [{"text": "transitionbased parsing", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.5755873620510101}]}, {"text": "Moreover, neural methods supposedly learn combinations from simple feature templates, as an alternative to hand-crafted task-specific feature engineering.", "labels": [], "entities": []}, {"text": "Yet, using neural methods for our task is challenging, the sizes of the available corpus are relatively modest (no ST.1 language has more than 5000 instances of training MWEs), albeit neural models generally have more parameters to learn than linear models.", "labels": [], "entities": []}, {"text": "Indeed, the best systems at the shared tasks ST.0 and ST.1) (in closed track) are not neural and surpassed some neural approaches.", "labels": [], "entities": []}, {"text": "In this paper, we carefully describe and compare the development and tuning of linear versus neural classifiers, to use in the transition system for MWE identification proposed in Al, which itself built on the joint syntactic / MWE analyzer of.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.9782586395740509}]}, {"text": "We set ourselves the constraints (i) of building systems that are robust across languages, hence using the same hyperparameter configuration for all languages and (ii) of using lemma and POS information but not syntactic parses provided in the PARSEME data sets, so that the resulting systems require limited preprocessing.", "labels": [], "entities": [{"text": "PARSEME data sets", "start_pos": 244, "end_pos": 261, "type": "DATASET", "confidence": 0.922089139620463}]}, {"text": "We report a systematic work on designing and tuning linear and neural transition classifiers, including the use of resampling, vocabulary generalization and several strategies for the selection of the best hyperparameter configuration.", "labels": [], "entities": [{"text": "vocabulary generalization", "start_pos": 127, "end_pos": 152, "type": "TASK", "confidence": 0.7095753699541092}]}, {"text": "We address both the open and closed tracks of the PARSEME ST.1, i.e with and without external resources (which in our case amount to pre-trained word embeddings).", "labels": [], "entities": [{"text": "PARSEME ST.1", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.6303884387016296}]}, {"text": "The contributions of our work are: \u2022 anew state-of-the art for the MWE identification task on the PARSEME ST1.1 data sets.", "labels": [], "entities": [{"text": "MWE identification task", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.9428841272989908}, {"text": "PARSEME ST1.1 data sets", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.9100717455148697}]}, {"text": "We discuss the related work in Section 2, data sets in Section 3 and the transition system in Section 4.", "labels": [], "entities": []}, {"text": "Linear and neural models are described in Sections 5 and 6, and the tuning methodology in Section 7.", "labels": [], "entities": []}, {"text": "We present experiments and discuss results in Sections 8 and 9, and conclude in Section 10.", "labels": [], "entities": []}], "datasetContent": [{"text": "Comparing the results of the open and the closed track, we can observe that the use of pretrained word embeddings has no significant impact on the MLP results.", "labels": [], "entities": [{"text": "MLP", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9291266798973083}]}, {"text": "This might mean that static embeddings are not well-suited for representing tokens both when used literally and within MWE.", "labels": [], "entities": [{"text": "MWE", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.8812601566314697}]}, {"text": "This tendency would deserve more investigation using other word embedding types, in particular contextualized ones.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of Sentences, Tokens and MWEs in", "labels": [], "entities": []}, {"text": " Table 2: Linear model feature hyperparameters. First col-", "labels": [], "entities": []}, {"text": " Table 4: MWE-based F-scores for ST.1 languages on test", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.642387330532074}, {"text": "F-scores", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.6150606274604797}]}]}