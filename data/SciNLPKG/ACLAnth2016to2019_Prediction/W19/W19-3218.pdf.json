{"title": [{"text": "Neural Network to identify personal health experience mention in tweets using BioBERT embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the system developed by team ASU-NLP for the Social Media Mining for Health Applications(SMM4H) shared task 4.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications(SMM4H) shared task 4", "start_pos": 66, "end_pos": 130, "type": "TASK", "confidence": 0.8075303907195727}]}, {"text": "We extract feature embeddings from the BioBERT (Lee et al., 2019) model which has been fine-tuned on the training dataset and use that as inputs to a dense fully connected neu-ral network.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.7427176237106323}]}, {"text": "We achieve above average scores among the participant systems with the overall F1-score, accuracy, precision, recall as 0.8036, 0.8456, 0.9783, 0.6818 respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9990442395210266}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9995180368423462}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9996111989021301}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9996604919433594}]}], "introductionContent": [{"text": "There has been an increase in the use of social media worldwide in recent years, which provides an abundance of data available and an exciting opportunity to build and improve biomedical and public health applications.", "labels": [], "entities": []}, {"text": "The Social Media Mining for Health Applications (SMM4H) proposed four tasks.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H)", "start_pos": 4, "end_pos": 55, "type": "TASK", "confidence": 0.7259138027826945}]}, {"text": "We have focused on task 4, which was the most interesting.", "labels": [], "entities": []}, {"text": "The task is to classify whether the tweet contains personal health mention as opposed to a general discussion of the topic.", "labels": [], "entities": []}, {"text": "The training data consisted of tweets related to the flu.", "labels": [], "entities": []}, {"text": "The system is evaluated on tweets related to flu and a second health domain across two contexts.", "labels": [], "entities": []}], "datasetContent": [{"text": "Language models like BERT ( and OpenAI GPT-2 ( have achieved state of the art performances in various NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9761728048324585}, {"text": "OpenAI GPT-2", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.9176797270774841}]}, {"text": "Such models that are trained on large datasets can be fine-tuned on smaller datasets to achieve good scores on various NLP tasks.", "labels": [], "entities": []}, {"text": "BioBERT (), a domainspecific language representation model designed for biomedical text, is built using BERT architecture.", "labels": [], "entities": [{"text": "BERT", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9011054039001465}]}, {"text": "Our system is built using transfer learning approach by fine tuning on the given dataset using the BioBERT model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy, F1 score, Precision and Recall re- sults on training data using different models and em- beddings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995447993278503}, {"text": "F1 score", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.984893798828125}, {"text": "Precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9996552467346191}, {"text": "Recall re- sults", "start_pos": 44, "end_pos": 60, "type": "METRIC", "confidence": 0.8353733420372009}]}, {"text": " Table 2: Final Accuracy, F1 score, Precision and Re- call scores on the test set for the best performing run  submitted.", "labels": [], "entities": [{"text": "Final", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9906981587409973}, {"text": "Accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8763730525970459}, {"text": "F1 score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9879536330699921}, {"text": "Precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9992977380752563}, {"text": "Re- call scores", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9727370291948318}]}]}