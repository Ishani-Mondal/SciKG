{"title": [{"text": "Segmentation of Argumentative Texts with Contextualised Word Representations", "labels": [], "entities": [{"text": "Segmentation of Argumentative Texts", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8642587810754776}]}], "abstractContent": [{"text": "The segmentation of argumentative units is an important subtask of argument mining, which is frequently addressed at a coarse granular-ity, usually assuming argumentative units to be no smaller than sentences.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7606449723243713}]}, {"text": "Approaches focus-ing at the clause-level granularity, typically address the task as sequence labeling at the token level, aiming to classify whether a token begins , is inside, or is outside of an argumentative unit.", "labels": [], "entities": []}, {"text": "Most approaches exploit highly engineered , manually constructed features, and algorithms typically used in sequential tagging-such as Conditional Random Fields, while more recent approaches try to exploit manually constructed features in the context of deep neural networks.", "labels": [], "entities": [{"text": "sequential tagging-such", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.6580058634281158}]}, {"text": "In this context, we examined to what extend recent advances in sequential labelling allow to reduce the need for highly sophisticated, manually constructed features, and whether limiting features to embeddings, pre-trained on large corpora is a promising approach.", "labels": [], "entities": []}, {"text": "Evaluation results suggest the examined models and approaches can exhibit comparable performance, minimising the need for feature engineering.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argument mining involves the automatic discovery of argument components (such as claims, premises, etc.) and the argumentative relations (i.e. support, attack, etc.) among these components in texts.", "labels": [], "entities": [{"text": "Argument mining involves the automatic discovery of argument components (such as claims, premises, etc.) and the argumentative relations (i.e. support, attack, etc.) among these components in texts", "start_pos": 0, "end_pos": 197, "type": "Description", "confidence": 0.7282916273389544}]}, {"text": "Primarily aiming to extract arguments from texts in order to provide structured data for computational models of argument and reasoning engines (, argument mining has additionally the potential to support applications in various research fields, such as opinion mining (, stance detection (), policy modelling (), legal information systems (, fact checking (, etc.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 147, "end_pos": 162, "type": "TASK", "confidence": 0.7486314177513123}, {"text": "opinion mining", "start_pos": 254, "end_pos": 268, "type": "TASK", "confidence": 0.8427013754844666}, {"text": "stance detection", "start_pos": 272, "end_pos": 288, "type": "TASK", "confidence": 0.8928093314170837}, {"text": "policy modelling", "start_pos": 293, "end_pos": 309, "type": "TASK", "confidence": 0.7400577664375305}, {"text": "fact checking", "start_pos": 343, "end_pos": 356, "type": "TASK", "confidence": 0.864398181438446}]}, {"text": "The identification of argumentative discourse structures typically consists of two main tasks: 1) the identification of the locations in text and the type of the argument components, and 2) the identification of how these argument components related to each other (.", "labels": [], "entities": []}, {"text": "As a result, argument mining is usually addressed as a pipeline of several sub-tasks.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.818214476108551}]}, {"text": "Typically the first sub-task is the separation between argumentative and non-argumentative text units, which can be performed at various granularity levels, from clauses to several sentences, usually depending on corpora characteristics.", "labels": [], "entities": []}, {"text": "Detection of argumentative units (AU) 1 , as discussed in Section 2, is typically modeled as a fully-supervised classification task, either a binary one, where units are separated in argumentative and non-argumentative ones with argumentative ones to be subsequently classified in major claims, claims, premises, etc.", "labels": [], "entities": [{"text": "Detection of argumentative units (AU) 1", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8765157088637352}]}, {"text": "as a second step, or as a multi-class one, where identification of argumentative units and their classification into claims and premises are performed as a single step.", "labels": [], "entities": []}, {"text": "Typically the granularity of this task is coarse, with most approaches considering sentences as the smallest argumentative unit (, although some works focused on the most difficult task of detecting units at the clause level.", "labels": [], "entities": []}, {"text": "According to a recent survey (, the performance of proposed approaches depends on highly engineered and sophisticated, manually constructed, features.", "labels": [], "entities": []}, {"text": "Approaches focusing at the clause-level granu-larity, typically address the task as sequence labeling at the token level, aiming to classify whether a token begins, is inside, or is outside of an argumentative unit through the IOB format.", "labels": [], "entities": []}, {"text": "Most of the approaches employ Conditional Random Fields (CRFs) () with hand-crafted features (), as CRFs are the prominent and most reliable algorithm for many sequential labelling tasks (, and have been applied to a wide range of segmenting tasks, from namedentity recognition and shallow parsing, to aspectbased sentiment analysis ().", "labels": [], "entities": [{"text": "namedentity recognition", "start_pos": 254, "end_pos": 277, "type": "TASK", "confidence": 0.7264626324176788}, {"text": "shallow parsing", "start_pos": 282, "end_pos": 297, "type": "TASK", "confidence": 0.5949191004037857}, {"text": "aspectbased sentiment analysis", "start_pos": 302, "end_pos": 332, "type": "TASK", "confidence": 0.7199527621269226}]}, {"text": "Sequence labeling algorithms take as input a set of features for each token in a sequence (such as a sentence) and learn to predict an optimal sequence of labels for all tokens in the input sequence, while performance depends on the provided (typically manually engineered features) and how well these features can help the model predicting the likelihood of every label in the sequence.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9041677415370941}]}, {"text": "However, as deep learning is slowly replacing CRFs for sequence labelling (i.e. (), it is interesting to examine whether these hand-crafted features are still important, or comparative levels of performance can be achieved without them.", "labels": [], "entities": []}, {"text": "In this paper we examine whether a \"CRFinspired\" neural model without the hand-crafted features, can be applied to the task of argumentative unit segmentation at the clause level, and whether its performance is comparable to approaches exploiting such features.", "labels": [], "entities": [{"text": "argumentative unit segmentation", "start_pos": 127, "end_pos": 158, "type": "TASK", "confidence": 0.6555991868178049}]}, {"text": "In addition, we study whether contextualised word representations can help in this task, and provide an alternative to hand-crafted features.", "labels": [], "entities": []}, {"text": "These can be reflected in the following two questions: 1.", "labels": [], "entities": []}, {"text": "Can approaches that do not use manually engineered features achieve performances comparable to approaches that exploit such features?", "labels": [], "entities": []}, {"text": "2. Can contextualised word representations (pre-trained in large corpora) replace manually engineered features in argument mining?", "labels": [], "entities": [{"text": "argument mining", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.7274307012557983}]}, {"text": "The motivation behind the work presented in this paper originates from the advances performed in the state of art of named-entity recognition by Bidirectional LSTM-CRF Models for Sequence Tagging (, a variation of Long Short-Term Memory (LSTM) based models with a decoding layer that considers relations between neighbouring labels and jointly decodes the optimal sequence of labels fora given input sequence (, using a Conditionally Random Field.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7260527014732361}, {"text": "Sequence Tagging", "start_pos": 179, "end_pos": 195, "type": "TASK", "confidence": 0.7788679599761963}]}, {"text": "Recognising a similar evolution pattern also in the area of argument mining segmentation -starting with CRF's and manually constructed features, then employing word embeddings as features in CRFs () and subsequently applying bi-directional LSTMs () on manually engineered features -poses the question if a similar advancement can be achieved by introducing the currently missing pieces (LSTM-CRF models or contextualised word representations such as (), in an attempt to eliminate -or reduce the need for -manually engineered features.", "labels": [], "entities": [{"text": "argument mining segmentation", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.8254579504330953}]}, {"text": "In order to approach our research questions we have used the second version of the Argument Annotated Essay Corpus, a collection of 402 essays, which has been manually annotated with major claims (one per essay), claims and premises at the clause level.", "labels": [], "entities": [{"text": "Argument Annotated Essay Corpus", "start_pos": 83, "end_pos": 114, "type": "DATASET", "confidence": 0.7130125612020493}]}, {"text": "In addition, the corpus contains manual annotations of argumentative relations, where the claims and premises are linked, while claims are linked to the major claim either with a support or an attack relation.", "labels": [], "entities": []}, {"text": "We have applied LSTM-CRF models (using the implementation reported in) employing various word embeddings (including contextualised word representations like \"ELMo\" (Peters et al., 2018), \"Flair\" and \"BERT\").", "labels": [], "entities": [{"text": "BERT", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.992087185382843}]}, {"text": "Evaluation results suggest that all studied approaches are comparable or slightly better to the current state of art.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of documents, tokens per class, and average number of tokens per document.", "labels": [], "entities": [{"text": "average number of", "start_pos": 53, "end_pos": 70, "type": "METRIC", "confidence": 0.93949294090271}]}]}