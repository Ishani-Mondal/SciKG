{"title": [{"text": "Cmu livemedqa at trec 2017 liveqa: A consumer health question answering system", "labels": [], "entities": [{"text": "Cmu livemedqa at trec 2017 liveqa", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.8355495929718018}, {"text": "consumer health question answering", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.6042990684509277}]}], "abstractContent": [{"text": "In medical domain, given a medical question, it is difficult to manually select the most relevant information from a large number of search results.", "labels": [], "entities": []}, {"text": "BioNLP 2019 proposes Question Answering (QA) task, which encourages the use of text mining technology to automatically judge whether a search result is an answer to the medical question.", "labels": [], "entities": [{"text": "BioNLP 2019", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9648197889328003}, {"text": "Question Answering (QA) task", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8294665416081747}]}, {"text": "The main challenge of QA task is how to mine the semantic relation between question and answer.", "labels": [], "entities": [{"text": "QA task", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.8576631844043732}]}, {"text": "We propose BioBERT Transformer model to tackle this challenge, which applies Transformers to extract semantic relation between different words in questions and answers.", "labels": [], "entities": []}, {"text": "Furthermore, BioBERT is utilized to encode medical domain-specific contextualized word representations.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.7354998588562012}]}, {"text": "Our method has reached the accuracy of 76.24% and spearman of 17.12% on the BioNLP 2019 QA task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9997596144676208}, {"text": "BioNLP 2019 QA task", "start_pos": 76, "end_pos": 95, "type": "DATASET", "confidence": 0.7224988490343094}]}], "introductionContent": [{"text": "In medical field, the professional vocabulary is large and the semantics are complex, which makes manually selecting answers to a medical question from search results time consuming.", "labels": [], "entities": []}, {"text": "The question answering (QA) task proposed by BioNLP 2019) aims to automatically extract answers to a medical question by using text mining technology.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.884001205364863}, {"text": "BioNLP 2019", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.8822761476039886}]}, {"text": "This task consists of two objectives: one is to determine whether each candidate answer can be used as the correct answer to a question, and the other is to rank the retrieved answers according to the relevance to a question.", "labels": [], "entities": []}, {"text": "The nature of QA task is to match the meaning rather than only match words between question and answer sentences.", "labels": [], "entities": [{"text": "QA task", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.8968091905117035}]}, {"text": "Several QA approaches based on syntax information have been developed to match the meaning between question and answer.", "labels": [], "entities": []}, {"text": "propose a statistical syntaxbased model that softly aligns a question sentence with a candidate answer sentence.", "labels": [], "entities": []}, {"text": "encode semantic knowledge directly into syntactic tree representations of a pair of questions and answers for answers ranking.", "labels": [], "entities": []}, {"text": "However, all these models rely on dependency parsers, suffering from error propagation.", "labels": [], "entities": []}, {"text": "Neural network-based methods can automatically learn the inherent semantic features and have achieved good performance on QA task.", "labels": [], "entities": []}, {"text": "employ an attentional encoder-decoder model based on long short-term memory (LSTM) for answer ranking, and their model achieves the best performance of 63.7% average score on the TREC LiveQA 2017 challenge (.", "labels": [], "entities": [{"text": "answer ranking", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.8814693987369537}, {"text": "TREC LiveQA 2017 challenge", "start_pos": 179, "end_pos": 205, "type": "DATASET", "confidence": 0.9363840073347092}]}, {"text": "use a convolutional neural network (CNN) model to classify a question into a restricted set of 10 question types and crawl relevant online web pages to find the answers.", "labels": [], "entities": []}, {"text": "However, all the models described above neglect the long range dependency between words in question and answer, limiting their capacity when question and answer sequences are long.", "labels": [], "entities": []}, {"text": "Transformer () is a model based entirely on attention mechanisms and has achieved success on several natural language processing (NLP) tasks, such as machine translation () and language understanding Recently, language models (LM) based on largescale corpus pre-training have made great progress in several NLP tasks, such as machine translation and natural language inference (NLI).) learns two unidirectional LMs based on LSTM networks which is able to capture both sub-word information and contextual clues.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7963934242725372}, {"text": "language understanding", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.7747677862644196}, {"text": "machine translation", "start_pos": 326, "end_pos": 345, "type": "TASK", "confidence": 0.7676974534988403}]}, {"text": "OpenAI GPT () uses a left-toright Transformer (), which introduces minimal task-specific parameters and is trained on the downstream tasks by simply finetuning the pre-trained parameters.", "labels": [], "entities": [{"text": "OpenAI GPT", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9271045029163361}]}, {"text": "The major limitation of pre-trained model above is that they are unidirectional, which limits the choice of architectures that can be used during pre-training.", "labels": [], "entities": []}, {"text": "BERT) employs a bidirectional Transformer encoder to fuse both the left and the right context and can explicitly model the relationship of a pair of text.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8422802090644836}]}, {"text": "Thus, it can make progress in paired NLP tasks, such as NLI and QA.", "labels": [], "entities": []}, {"text": "Based on the BERT architecture, BioBERT () is a domain-specific language representation model pre-trained on large-scale biomedical corpora and effectively transfers the knowledge from biomedical texts to biomedical text mining models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9361289739608765}]}, {"text": "Corpus of QA task proposed by BioNLP 2019 contains answers with long text, which requires models to capture the long range dependency information across words in both question and answer sentences.", "labels": [], "entities": [{"text": "BioNLP 2019", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.8403691947460175}]}, {"text": "Thus, we propose BioBERT Transformer (BBERT-T) model based on Transformer to model the associations between question and answer.", "labels": [], "entities": [{"text": "BioBERT Transformer (BBERT-T)", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.736032772064209}]}, {"text": "Specifically, question and answer sequences are first passed to BioBERT to generate medical domain-specific contextualized representations.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8380392789840698}]}, {"text": "Then, the question and answer representations are fed into two Transformers, respectively, to capture the long range dependency information and semantic relation between question and answer.", "labels": [], "entities": []}, {"text": "Finally, a weighted cross entropy loss is applied to further improve the performance.", "labels": [], "entities": []}, {"text": "Our method achieves accuracy of 76.24% and spearman of 17.12% on the BioNLP 2019 QA task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997348189353943}, {"text": "BioNLP 2019 QA task", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.7460801750421524}]}, {"text": "1 https://github.com/huggingface/pytorch-pretrained-BERT", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset: MEDIQA2019-Task3-QA task contains dataset of medical questions and the associated answers retrieved by CHiQA 2 . describes the details of statistics of the dataset.", "labels": [], "entities": []}, {"text": "Evaluation Metrics: For BioNLP 2019 QA task, organizers employ two measurements: accuracy and spearman.", "labels": [], "entities": [{"text": "BioNLP 2019 QA task", "start_pos": 24, "end_pos": 43, "type": "DATASET", "confidence": 0.658021479845047}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9996296167373657}]}, {"text": "The evaluation is reported by official evaluation toolkit 3 , and accuracy is the main metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9997063279151917}]}, {"text": "For each experiment, we report the mean values with corresponding standard deviations over 3 repetitions.", "labels": [], "entities": []}, {"text": "The BioBERT we use includes 12 layers (i.e., Transformer blocks), and the dimension of hidden size is 768.", "labels": [], "entities": []}, {"text": "The Transformer we use has 3 blocks, each of which contains 16 heads.", "labels": [], "entities": []}, {"text": "For each head, the mapped Q, K, and V dimensions are 48.", "labels": [], "entities": []}, {"text": "Thus, the input and output dimension of Transformer is 768.", "labels": [], "entities": [{"text": "Transformer", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.7293429374694824}]}, {"text": "We use Adam () with 1 \uf062 = 0.9, 2 \uf062 = 0.999 for optimization.", "labels": [], "entities": []}, {"text": "The learning rate is 2e-5.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9752141237258911}]}, {"text": "The dropout rate is 0.5.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9104240834712982}]}, {"text": "The batch size is set to 4.", "labels": [], "entities": []}, {"text": "The BBERT-T  PyTorch .We use the BioBERT module () without modifying.", "labels": [], "entities": [{"text": "BBERT-T  PyTorch", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.5000529140233994}]}, {"text": "The Transformer is developed by ourselves.", "labels": [], "entities": []}, {"text": "Computations are run on a single server computer equipped with a GPU.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of dataset of QA task.", "labels": [], "entities": []}, {"text": " Table 2: Comparisons with baselines, * stands for  using ensemble by averaging the last 4 epoch  output probabilities. \u00b1 denotes standard deviation,  and bold font indicates best performance. Time", "labels": [], "entities": [{"text": "Time", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9243329167366028}]}]}