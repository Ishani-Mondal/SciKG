{"title": [{"text": "Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques", "labels": [], "entities": [{"text": "Equalizing Gender Bias", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9337606827418009}, {"text": "Neural Machine Translation", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.6593573192755381}]}], "abstractContent": [{"text": "Neural machine translation has significantly pushed forward the quality of the field.", "labels": [], "entities": [{"text": "Neural machine translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7267082134882609}]}, {"text": "However , there are remaining big issues with the output translations and one of them is fairness.", "labels": [], "entities": []}, {"text": "Neural models are trained on large text corpora which contain biases and stereotypes.", "labels": [], "entities": []}, {"text": "As a consequence, models inherit these social biases.", "labels": [], "entities": []}, {"text": "Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings.", "labels": [], "entities": []}, {"text": "We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6956772208213806}, {"text": "neural machine translation", "start_pos": 139, "end_pos": 165, "type": "TASK", "confidence": 0.7611347436904907}]}, {"text": "We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point.", "labels": [], "entities": [{"text": "WMT English-Spanish benchmark task", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.7259377390146255}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9994276165962219}]}, {"text": "As for the gender bias evaluation , we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language is one of the most interesting and complex skills used in our daily life, and may even betaken for granted on our ability to communicate.", "labels": [], "entities": []}, {"text": "However, the understanding of meanings between lines in natural languages is not straightforward for the logic rules of programming languages.", "labels": [], "entities": [{"text": "understanding of meanings between lines", "start_pos": 13, "end_pos": 52, "type": "TASK", "confidence": 0.8025902032852172}]}, {"text": "Natural language processing (NLP) is a subfield of artificial intelligence that focuses on making natural languages understandable to computers.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7659113357464472}]}, {"text": "Similarly, the translation between different natural languages is a task for Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.8478823840618134}]}, {"text": "Neural MT has shown significant improvements on performance using deep learning techniques, which are algorithms that learn abstractions from data.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.832756370306015}]}, {"text": "In recent years, these deep learning techniques have shown promising results in narrowing the gap between human-like performance with sequence-to-sequence learning approaches in a variety of tasks (), improvements in combination of approaches such as attention () and translation systems algorithms like the Transformer (.", "labels": [], "entities": []}, {"text": "One downside of models trained with human generated corpora is that social biases and stereotypes from the data are learned (.", "labels": [], "entities": []}, {"text": "A systematic way of showing this bias is by means of word embeddings, a vector representation of words.", "labels": [], "entities": []}, {"text": "The presence of biases, such as gender bias, is studied for these representations and evaluated on crowd-sourced tests (.", "labels": [], "entities": []}, {"text": "The presence of biases in the data can directly impact downstream applications () and are at risk of being amplified (.", "labels": [], "entities": []}, {"text": "The objective of this work is to study the presence of gender bias in MT and give insight on the impact of debiasing in such systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9882986545562744}]}, {"text": "An example of this gender bias is the word \"friend\" in the English sentence \"She works in a hospital, my friend is a nurse\" would be correctly translated to \"amiga\" (girl friend in Spanish) in Spanish, while \"She works in a hospital, my friend is a doctor\" would be incorrectly translated to \"amigo\" (boy friend in Spanish) in Spanish.", "labels": [], "entities": []}, {"text": "We consider that this translation contains gender bias since it ignores the fact that, for both cases, \"friend\" is a female and translates by focusing on the occupational stereotypes, i.e. translating doctor as male and nurse as female.", "labels": [], "entities": []}, {"text": "The main contribution of this study is providing progress on the recent detected problem which is gender bias in MT ().", "labels": [], "entities": [{"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9725574851036072}]}, {"text": "The progress towards reducing gender bias in MT is made in two directions: first, we define a frame-work to experiment, detect and evaluate gender bias in MT fora particular task; second, we propose to use debiased word embeddings techniques in the MT system to reduce the detected bias.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9720489978790283}, {"text": "MT", "start_pos": 155, "end_pos": 157, "type": "TASK", "confidence": 0.9133443236351013}, {"text": "MT", "start_pos": 249, "end_pos": 251, "type": "TASK", "confidence": 0.9599716663360596}]}, {"text": "This is the first study in proposing debiasing techniques for MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9956996440887451}]}, {"text": "The rest the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports material relevant to the background of the study.", "labels": [], "entities": []}, {"text": "Section 3 presents previous work on the bias problem.", "labels": [], "entities": []}, {"text": "Section 4 reports the methodology used for experimentation and section 5 details the experimental framework.", "labels": [], "entities": []}, {"text": "The results and discussion are included in section 6 and section 7 presents the main conclusions and ideas for further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the experimental framework.", "labels": [], "entities": []}, {"text": "We report details on the training of the word embeddings and the translation system.", "labels": [], "entities": [{"text": "translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9629216194152832}]}, {"text": "We describe the data related to the training corpus and test sets and the parameters.", "labels": [], "entities": []}, {"text": "Also, we comment on the use of computational resources.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: English-Spanish data set.", "labels": [], "entities": [{"text": "English-Spanish data set", "start_pos": 10, "end_pos": 34, "type": "DATASET", "confidence": 0.8265496293703715}]}, {"text": " Table 3: Word Embeddings Parameters.", "labels": [], "entities": []}, {"text": " Table 4. Pre-trained embeddings are used  for training in three scenarios: in the encoder side  (Enc.), in the decoder side (Dec.) and in both  the encoder and decoder sides (Enc./Dec.). These  pre-trained embeddings are updated during train- ing. We are comparing several pre-trained embed- dings against a baseline system ('Baseline' in Ta- ble 4) which does not include pre-trained embed- dings (neither on the encoder nor the decoder", "labels": [], "entities": [{"text": "Baseline' in Ta- ble 4", "start_pos": 327, "end_pos": 349, "type": "DATASET", "confidence": 0.6762260837214333}]}]}