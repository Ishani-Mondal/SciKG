{"title": [{"text": "WMD O : Fluency-based Word Mover's Distance for Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8275079131126404}]}], "abstractContent": [{"text": "We propose WMD O , a metric based on distance between distributions in the semantic vector space.", "labels": [], "entities": [{"text": "WMD O", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.6232370287179947}]}, {"text": "Matching in the semantic space has been investigated for translation evaluation , but the constraints of a translation's word order have not been fully explored.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.9813574552536011}]}, {"text": "Building on the Word Mover's Distance metric and various word embeddings, we introduce a fragmentation penalty to account for fluency of a translation.", "labels": [], "entities": []}, {"text": "This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current metrics to automatically evaluate machine translations, such as the popular BLEU (), are heavily based on string matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9918265342712402}]}, {"text": "They claim to account for adequacy by checking for overlapping words between the machine translation output and reference translation, and fluency by rewarding matches in sequences of more than one word.", "labels": [], "entities": []}, {"text": "This way of viewing adequacy is very limiting; comparing strings makes it harder to evaluate any deviation from the semantics of the original text in the reference or machine translation.", "labels": [], "entities": []}, {"text": "Meteor () relaxes this constraint by allowing matching of lemmas, synonyms or paraphrases.", "labels": [], "entities": []}, {"text": "However, this requires linguistic resources to lemmatise the data or lexical databases to fetch synonyms/paraphrases, which do not exist for most languages.", "labels": [], "entities": []}, {"text": "Character-based metrics like chrF and CharacTER () also relax the exact word match constraint by allowing the matching of characters.", "labels": [], "entities": []}, {"text": "However, they ultimately still assume a surface-level similarity between reference and machine translation output.", "labels": [], "entities": []}, {"text": "presented a number of experiments where both translation and reference sentences are compared in the embedding space rather than at surface level.", "labels": [], "entities": []}, {"text": "They however simply extract these two embedding representations and measure the (cosine) similarity between them, which may account for some overall semantic similarity, but ignores other aspects of translation quality.", "labels": [], "entities": []}, {"text": "A version of Meteor has been proposed that also performs matches at the word embedding space (.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9199327230453491}]}, {"text": "Two words are considered to match if their cosine distance in the embedding space is above a certain threshold.", "labels": [], "entities": []}, {"text": "In other words, the embeddings are only used to provide this binary decision, rather than to measure overall semantic distance between two sentences.", "labels": [], "entities": []}, {"text": "Ina similar vein, bleu2vec and ngram2vec) area direct modification of BLEU where fuzzy matches are added to strict matches.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9906816482543945}]}, {"text": "The fuzzy match score is implemented via token and n-gram embedding similarities.", "labels": [], "entities": []}, {"text": "As we show in Section 4, these metrics do not perform well.", "labels": [], "entities": []}, {"text": "MEANT 2.0 (Lo, 2017) also relies on matching of words in the embedding space, but this is only used to score the similarity between pairs of words that have already been aligned based on their semantic roles, rather than to find the alignments between words.", "labels": [], "entities": [{"text": "MEANT 2.0 (Lo, 2017)", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.7444917219025748}]}, {"text": "We suggest a more general way of using distributional representations of words, where distance in the semantic space is viewed as a global decision between the entire machine and reference translations.", "labels": [], "entities": []}, {"text": "More specifically, we propose an adaptation of a powerful and flexible metric that operates on the semantic space: Word Mover's Distance (WMD) (.", "labels": [], "entities": [{"text": "Word Mover's Distance (WMD)", "start_pos": 115, "end_pos": 142, "type": "METRIC", "confidence": 0.5669056517737252}]}, {"text": "WMD is an instance of the Earth Mover's Distance transportation problem that calculates the most efficient way to transform one distribution onto another.", "labels": [], "entities": [{"text": "WMD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.660144031047821}, {"text": "Earth Mover's Distance transportation problem", "start_pos": 26, "end_pos": 71, "type": "TASK", "confidence": 0.6879524290561676}]}, {"text": "Adjustments to EMD have been used previously to create evaluation metrics based on word embeddings and word positions.", "labels": [], "entities": []}, {"text": "Likewise, using vector word embeddings as an indicator of similarity and the word embeddings of each text as a distribution, WMD gives the optimal method of transforming the words of one document to the words of another document.", "labels": [], "entities": []}, {"text": "WMD does not take word order into account and rather focuses on semantic similarity of word meanings.", "labels": [], "entities": [{"text": "WMD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6027097702026367}]}, {"text": "WMD has been recently used for the evaluation of image captioning models (.", "labels": [], "entities": [{"text": "WMD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6272044777870178}, {"text": "image captioning", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.6932387799024582}]}, {"text": "It proved promising for image captioning evaluation, where word order is less relevant.", "labels": [], "entities": [{"text": "image captioning evaluation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9007081389427185}]}, {"text": "The same image can be described similarly using different word orders as it is constrained by the image itself.", "labels": [], "entities": []}, {"text": "We note that in machine translation evaluation, word order is more important, since the order is constrained by that of the source sentence.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8085852464040121}]}, {"text": "In this paper, we propose WMD O -an extension to WMD that incorporates word order.", "labels": [], "entities": [{"text": "WMD O", "start_pos": 26, "end_pos": 31, "type": "TASK", "confidence": 0.6044278740882874}]}, {"text": "We show that this metric outperforms the standard WMD and performs on par or better than most state-ofthe-art evaluation metrics.", "labels": [], "entities": [{"text": "WMD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.6130022406578064}]}], "datasetContent": [{"text": "We performed experiments to verify the performance of the proposed metric, comparing the metric's results against human annotations to measure a level of correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 154, "end_pos": 165, "type": "METRIC", "confidence": 0.9512145519256592}]}, {"text": "We used the PyEMD wrapper (Mayner, 2019) for calculating the WMD, based on ().", "labels": [], "entities": [{"text": "PyEMD wrapper (Mayner, 2019)", "start_pos": 12, "end_pos": 40, "type": "DATASET", "confidence": 0.8934285129819598}, {"text": "WMD", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.5666288733482361}]}, {"text": "We did not remove any stopwords as these are important to fluency.", "labels": [], "entities": []}, {"text": "We also use Cosine rather than Euclidean distance to calculate distance between word embeddings as magnitude of the vectors is not as important in such high dimensions.", "labels": [], "entities": []}, {"text": "We used the WMT17 segment-level into-English datasets for our experiments (.", "labels": [], "entities": [{"text": "WMT17 segment-level into-English datasets", "start_pos": 12, "end_pos": 53, "type": "DATASET", "confidence": 0.86479252576828}]}, {"text": "This has data from seven different source languages, with 560 different texts each.", "labels": [], "entities": []}, {"text": "Every text carries a reference translation and a machine translation, with a human annotation labelling how closely the machine translation relates to the reference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance of different embeddings on standard WMD, including OOV rate.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9502153098583221}]}, {"text": " Table 4: Performance of different metrics in the WMT17 shared task against the two proposed metrics. Our  metrics are highlighted in blue. Trained/ensemble metrics are highlighted in grey. Bolded values signify the best  performing non-trained metric for each language pair.", "labels": [], "entities": [{"text": "WMT17 shared task", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.573773721853892}]}]}