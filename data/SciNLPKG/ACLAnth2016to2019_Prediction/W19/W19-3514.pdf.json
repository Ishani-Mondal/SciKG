{"title": [{"text": "Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context", "labels": [], "entities": [{"text": "Preemptive Toxic Language Detection", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7121029496192932}]}], "abstractContent": [{"text": "We address the task of automatically detecting toxic content in user generated texts.", "labels": [], "entities": [{"text": "automatically detecting toxic content in user generated texts", "start_pos": 23, "end_pos": 84, "type": "TASK", "confidence": 0.7964831180870533}]}, {"text": "We focus on exploring the potential for preemptive moderation, i.e., predicting whether a particular conversation thread will, in the future, incite a toxic comment.", "labels": [], "entities": []}, {"text": "Moreover, we perform preliminary investigation of whether a model that jointly considers all comments in a conversation thread outperforms a model that considers only individual comments.", "labels": [], "entities": []}, {"text": "Using an existing dataset of conversations among Wikipedia contributors as a starting point, we compile anew large-scale dataset for this task consisting of labeled comments and comments from their conversation threads.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to the ever-growing amount of user generated content online, manual moderation of such content is becoming increasingly difficult to scale up.", "labels": [], "entities": []}, {"text": "On the other hand, the relative anonymity and lack of personal contact between participants of web conversations lowers inhibitions and increases the risk of toxic behavior, making adequate moderation increasingly important.", "labels": [], "entities": []}, {"text": "Consequently, automated detection of toxic language in user generated content is an increasingly important area of research.", "labels": [], "entities": [{"text": "automated detection of toxic language in user generated content", "start_pos": 14, "end_pos": 77, "type": "TASK", "confidence": 0.8462165726555718}]}, {"text": "While automated classification models are unlikely to ever fully replace human moderators, they can make their task easier by suggesting which content to prioritize for moderation.", "labels": [], "entities": []}, {"text": "The typical way to approach this problem is via supervised machine learning, where an input to a model is a user-generated text, and the output is a classification decision (toxic or non-toxic) or a numerical toxicity score.", "labels": [], "entities": []}, {"text": "In this paper, we explore two possible extensions of this approach: preemptive classification and thread-level models.", "labels": [], "entities": [{"text": "preemptive classification", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6462069004774094}]}, {"text": "While practically very useful, standard models are only applicable in a post-hoc scenario, i.e., to detect a toxic comment after if has already been posted.", "labels": [], "entities": []}, {"text": "An alternate approach would be to have models detect situations that are likely to lead to toxic comments.", "labels": [], "entities": []}, {"text": "If successful, such models would enable moderators to preemptively focus on potentially problematic discussion threads and then either intervene and guide the discussion away from conflict or respond in near real-time after the toxic comment is posted.", "labels": [], "entities": []}, {"text": "Large-scale implementation of such near real-time moderation might be unnecessary and require too many moderators.", "labels": [], "entities": []}, {"text": "However, for limited parts of discussions that are known to pertain to specially vulnerable social groups, this might be a feasible approach.", "labels": [], "entities": []}, {"text": "Our first research question is whether such preemptive toxic comment detection is viable.", "labels": [], "entities": [{"text": "toxic comment detection", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6443486909071604}]}, {"text": "The second research question pertains to the benefits of including thread-level information when detecting toxic comments.", "labels": [], "entities": []}, {"text": "Namely, most existing models consider every comment in isolation, therefore ignoring the context provided by the other comments in the discussion.", "labels": [], "entities": []}, {"text": "For post-hoc models, while useful, this additional information may not be crucial, as the main indicators of toxicity are most present in the text of the comment being classified rather than in the rest of the thread.", "labels": [], "entities": []}, {"text": "In the preemptive scenario, however, the model has access only to comments that appeared before a toxic one.", "labels": [], "entities": []}, {"text": "We hypothesize that considering the entire thread of comments might be of greater importance in this case.", "labels": [], "entities": []}, {"text": "The contribution of this paper is threefold.", "labels": [], "entities": []}, {"text": "First, using a large data set of conversations among Wikipedia contributors, we compile and make publicly available anew dataset with complete discussion threads and with semi-automatically generated toxicity labels.", "labels": [], "entities": []}, {"text": "Secondly, we explore the viability of models for the preemptive toxic detection task.", "labels": [], "entities": [{"text": "preemptive toxic detection task", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6868542432785034}]}, {"text": "Third, we investigate the potential benefits of including thread-level information into models.", "labels": [], "entities": []}], "datasetContent": [{"text": "At the time of writing we were not aware of the data set from (.", "labels": [], "entities": []}, {"text": "At first we considered using the dataset from (), but found it rather small (\u223c1200 examples) for deep learning approaches.", "labels": [], "entities": []}, {"text": "Furthermore, this dataset was constructed using a very carefully designed methodology fora specific experiment -detecting whether a toxic comment will appear given a courteous initial exchange of two comments.", "labels": [], "entities": []}, {"text": "We are interested in a more general case, where conversation threads might be longer and not necessarily start in a courteous manner.", "labels": [], "entities": []}, {"text": "Moreover, we aimed at a setting which would better reflect the realistic working conditions in which our models would be used and allow us to measure their practical impact.", "labels": [], "entities": []}, {"text": "Consequently, we decided to create anew dataset from the data collected by.", "labels": [], "entities": []}, {"text": "It contains the entire conversational history of comments on Wikipedia modeled as a graph of actions.", "labels": [], "entities": []}, {"text": "The possible actions are Creation, Addition, Modification, Deletion, and Restoration.", "labels": [], "entities": [{"text": "Addition", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9535221457481384}]}, {"text": "Automatically derived toxicity scores are also provided for each example.", "labels": [], "entities": []}, {"text": "We apply the following steps to this dataset: Step 1.", "labels": [], "entities": []}, {"text": "Filter the data to remove all threads with less than 2 different participants.", "labels": [], "entities": []}, {"text": "This leaves \u223c8.7M threads.", "labels": [], "entities": []}, {"text": "Apply all Modification actions, to update the comments to their most recent version.", "labels": [], "entities": []}, {"text": "Flag comments that were deleted.", "labels": [], "entities": []}, {"text": "A comment is considered deleted if there is a Deletion action on it, without a subsequent Restoration action that would undo the effect.", "labels": [], "entities": []}, {"text": "Split the threads into the train (70%), dev (15%), and test (15%) sets.", "labels": [], "entities": []}, {"text": "The split is done across time: the test set contains the most recent threads, while the train set contains the least recent.", "labels": [], "entities": []}, {"text": "Semi-automatically label the examples for toxicity.", "labels": [], "entities": []}, {"text": "An example is considered toxic if its toxicity or severe toxicity scores are above 0.64 or 0.92, respectively.", "labels": [], "entities": []}, {"text": "1 and it was deleted by a person who is not the comment author.", "labels": [], "entities": []}, {"text": "This heuristic takes into account two signals: (1) the fact that a toxic classifier has high confidence fora comment and (2) the fact the comment was deleted.", "labels": [], "entities": []}, {"text": "Considering only deleted examples as toxic would yield noisy labels, as comments are often deleted for reasons other than being toxic.", "labels": [], "entities": []}, {"text": "Manual inspection of the silver-labeled dataset reveals that the combination of the toxicity classifier and observed deletion is effective in identifying some of the toxic comments.", "labels": [], "entities": []}, {"text": "However, this approach fails to identify those toxic comments which were not deleted or those for which the toxicity classifier failed.", "labels": [], "entities": []}, {"text": "The former issue is not problematic, as it was shown by that toxic comments on Wikipedia get deleted by the community very quickly.", "labels": [], "entities": []}, {"text": "Thus toxic comments that are not deleted are quite rare.", "labels": [], "entities": []}, {"text": "The latter issue, however, represents a limitation of our work.", "labels": [], "entities": []}, {"text": "Our results apply only to those types of toxic language that are detectable by current posthoc models.", "labels": [], "entities": []}, {"text": "Extending this data set to account for more complex types of toxic language would require considerable annotation effort and we leave it as a possibility for future work.", "labels": [], "entities": []}, {"text": "Generate examples from each thread in the train/dev/test set for the (1) preemptive scenario and (2) post-hoc scenario as shown in.", "labels": [], "entities": []}, {"text": "For the post-hoc scenario, examples are generated from all comments in the tree.", "labels": [], "entities": []}, {"text": "Positive examples are those comments that are labeled as toxic, while all other comments are negative examples.", "labels": [], "entities": []}, {"text": "In the preemptive scenario, examples are generated from all comments that are not leaves of the tree.", "labels": [], "entities": []}, {"text": "Positive examples are those comments that have a toxic child and no toxic ancestors, while all other comments are negative examples.", "labels": [], "entities": []}, {"text": "ceding comments available for each comment in this data can vary from 0 to over 100 (median is 2).", "labels": [], "entities": []}, {"text": "Consequently, to differentiate from the setting in the next step, we will refer to this setting as the real-context setting.", "labels": [], "entities": []}, {"text": "While the previous setting is more realistic, in order to better assess their full potential, we wished to evaluate the thread level models in a setting where the context provided by preceding comments is always available.", "labels": [], "entities": []}, {"text": "To this end, we filter the examples from the previous step such that only those are left that have at least L min = 2 comments on the path to the root.", "labels": [], "entities": []}, {"text": "We will refer to the datasets obtained in this step as being in the rich-context setting.", "labels": [], "entities": []}, {"text": "As the label distribution of the examples obtained in this way is extremely skewed (positive examples account for 0.5 to 1 percent of the data, depending on the setting and scenario), we undersample the negative class.", "labels": [], "entities": []}, {"text": "For completeness, we also retain the non-undersampled versions of the dev and test sets for some of the experiments.", "labels": [], "entities": []}, {"text": "Lastly, to additionally evaluate the quality of the silver labels we manually labeled 100 examples from the balanced version of the data set.", "labels": [], "entities": []}, {"text": "We found that on these examples the silver labels have 0.51 precision and 1.00 recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.991486668586731}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9651117920875549}]}, {"text": "This yields 0.67 F1 measure and is somewhat lower than the expected 0.85 obtained for this classifier in (.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.987106055021286}]}, {"text": "The difference indicates that the thresholds from () obtained on non-deleted comments from Wikipedia may not perform equally well on deleted comments.", "labels": [], "entities": []}, {"text": "To address this and increase the quality of the labels, more deleted comments should be manually labeled and thresholds retuned using, e.g., the same error rate method of (.", "labels": [], "entities": []}, {"text": "Some statistics of the finally generated data set are given in, and some examples in.", "labels": [], "entities": []}, {"text": "We make the dataset and the code for generating it available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results in various settings and scenarios. Ran- dom is the expected performance of choosing a random  class with uniform probability across the classes. The  numbers are F1-scores on perfectly balanced test data.", "labels": [], "entities": [{"text": "Ran- dom", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9771872758865356}, {"text": "F1-scores", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9943559765815735}]}]}