{"title": [{"text": "Neural Models for Detecting Binary Semantic Textual Similarity for Algerian and MSA", "labels": [], "entities": [{"text": "Detecting Binary Semantic Textual Similarity", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.8752676844596863}, {"text": "MSA", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.7317038178443909}]}], "abstractContent": [{"text": "We explore the extent to which neural networks can learn to identify semantically equivalent sentences from a small variable dataset using an end-to-end training.", "labels": [], "entities": []}, {"text": "We collect anew noisy non-standardised user-generated Alge-rian (ALG) dataset and also translate it to Modern Standard Arabic (MSA) which serves as its regularised counterpart.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 103, "end_pos": 131, "type": "DATASET", "confidence": 0.9041683574517568}]}, {"text": "We compare the performance of various models on both datasets and report the best performing configurations.", "labels": [], "entities": []}, {"text": "The results show that relatively simple models composed of 2 LSTM layers outperform by far other more sophisticated attention-based architectures, for both ALG and MSA datasets.", "labels": [], "entities": [{"text": "MSA datasets", "start_pos": 164, "end_pos": 176, "type": "DATASET", "confidence": 0.66527059674263}]}], "introductionContent": [{"text": "Detecting Semantic Textual Similarity (STS) aims to predict a relationship between a pair of sentences based on a semantic similarity score.", "labels": [], "entities": [{"text": "Detecting Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7728412193911416}]}, {"text": "It is a well-established problem which deals with text comprehension and which has been framed and tackled differently.", "labels": [], "entities": []}, {"text": "In this work we focus on deep learning approach.", "labels": [], "entities": []}, {"text": "For example, frame the problem as a sentence-pair scoring using binary or graded scores indicating the degree to which a pair of sentences are related.", "labels": [], "entities": []}, {"text": "Solutions to detecting semantic similarity benefit from the recent success of neural models applied to NLP and have achieved new state-of-theart performance.", "labels": [], "entities": [{"text": "detecting semantic similarity", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.8378007213274637}]}, {"text": "However, so far it has been explored only on fairly large well-edited labelled data in English.", "labels": [], "entities": []}, {"text": "This paper explores a largely unexplored question which concerns the application of neural models to detect binary STS from small labelled datasets.", "labels": [], "entities": [{"text": "detect binary STS from small labelled datasets", "start_pos": 101, "end_pos": 147, "type": "TASK", "confidence": 0.7778743590627398}]}, {"text": "We take the case of the language used in Algeria (ALG) which is an under-resourced language with several linguistic challenges.", "labels": [], "entities": []}, {"text": "ALG is a collection of local colloquial varieties with a heavy use of code-switching between different languages and language varieties including Modern Standard Arabic (MSA), non-standardised local colloquial Arabic, and other languages like French and Berber, all written in Arabic script normally without the vowels.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 146, "end_pos": 174, "type": "DATASET", "confidence": 0.8375350137551626}]}, {"text": "ALG and MSA are two Arabic varieties which differ lexically, morphologically, syntactically, etc., and therefore represent different challenges for NLP.", "labels": [], "entities": []}, {"text": "For instance, ALG and MSA share some morphological features, but at the same time the same morphological forms have different meanings.", "labels": [], "entities": []}, {"text": "For instance, a verb in the 1 st person singular in ALG is the same 1 st person plural in MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9159659743309021}]}, {"text": "The absence of morpho-syntactic analysers for ALG makes it challenging to analyse such texts, especially when ALG is mixed with MSA.", "labels": [], "entities": []}, {"text": "Furthermore, this language is not documented, i.e., it does not have lexicons, standardised orthography, and written morpho-syntactic rules describing how words are formed and combined to form larger units.", "labels": [], "entities": []}, {"text": "The nonexistence of lexicons to disambiguate the senses of a word based on its language or language variety makes resolving lexical ambiguity challenging for NLP because relying on exact word form matching is misleading.", "labels": [], "entities": []}, {"text": "(1) a. b. I spent one week at my parents' house and when I came back I found that my son made a big mess.", "labels": [], "entities": []}, {"text": "After that my husband changed his opinion and never allowed me to stay overnight (at my parents' house).", "labels": [], "entities": []}, {"text": "(2) a. b. In Mawlid we prepare Couscous for lunch, and you what will you prepare (for lunch)?", "labels": [], "entities": [{"text": "Mawlid", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9150334000587463}]}, {"text": "In many cases, while the same word form has several meanings depending on its context, different word forms have the same meaning.", "labels": [], "entities": []}, {"text": "As an illustration, consider examples (1) and (2) which are user-generated texts taken from our corpus (Section 3.1.1).", "labels": [], "entities": []}, {"text": "In (1), the same word form \"\" occurs three times with different meanings: \"house\", \"made\", and \"changed\" respectively.", "labels": [], "entities": []}, {"text": "Whereas in (2), the different word forms \" \" and \" \" mean both \"lunch\".", "labels": [], "entities": []}, {"text": "We mention these examples to provide a basic background fora better understanding of the challenges faced while processing this kind of real-world data using the current NLP approaches and systems that are designed and trained mainly on well-edited standardised monolingual corpora.", "labels": [], "entities": []}, {"text": "We could, for instance, distinguish the meanings of \"\" in (1) if we knew that the 1 st occurrence is a noun and the two others are verbs.", "labels": [], "entities": []}, {"text": "Likewise, if we had a tool to distinguish between ALG and MSA, it were easier to detect the meaning of \" \" as \"lunch\" in ALG rather than the MSA meaning \"tomorrow\".", "labels": [], "entities": []}, {"text": "Traditional models for detecting STS cannot be applied on such data because they require existing resources and tools, such as tokeniser, stemmer, PoS tagger, etc.", "labels": [], "entities": [{"text": "detecting STS", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.943486362695694}]}, {"text": "to pre-process the data and extract useful features assuming that the data is correctly spelled (standardised orthography).", "labels": [], "entities": []}, {"text": "Thus using deep neural networks (DNNs) is promising because representations can be learned in an unsupervised way.", "labels": [], "entities": []}, {"text": "In particular, when trained end-toend, inputs are mapped directly to the desired outputs without the need to handcraft features.", "labels": [], "entities": []}, {"text": "Nevertheless, this learning approach based on pattern matching requires lot of data to learn useful patterns.", "labels": [], "entities": [{"text": "pattern matching", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7269786149263382}]}, {"text": "Besides there are only a few cleaned and labelled textual corpora available for some languages and creating new ones is labour intensive.", "labels": [], "entities": []}, {"text": "Our contributions are as follows.", "labels": [], "entities": []}, {"text": "(i) We introduce a newly built (small) ALG dataset for STS.", "labels": [], "entities": [{"text": "ALG dataset", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7044537961483002}, {"text": "STS", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8903910517692566}]}, {"text": "(ii) We compare the performance of different DNN configurations on this dataset, namely: various combinations of Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), pre-training of embeddings, including a replication of two new state-of-the art attention models.", "labels": [], "entities": []}, {"text": "(iii) We test whether increasing the dataset size helps.", "labels": [], "entities": []}, {"text": "(iv) We test whether language regularisation helps.", "labels": [], "entities": [{"text": "language regularisation", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6953360289335251}]}, {"text": "For this purpose, we run the same experiments on a regularised and comparable MSA translation of the ALG dataset.", "labels": [], "entities": [{"text": "ALG dataset", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.9281112253665924}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we briefly review some STS applications.", "labels": [], "entities": [{"text": "STS", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9724379777908325}]}, {"text": "In Section 3, we describe our experimental setup including data and models.", "labels": [], "entities": []}, {"text": "In Section 4, we discuss the results and conclude with our future plans in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Average accuracy of the models (%). Acc is accuracy with non-augmented", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9392217397689819}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9306218028068542}, {"text": "Acc", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9996849298477173}, {"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9988139867782593}]}, {"text": " Table 3: Average performance of the models per class trained on the ALG augmented data.", "labels": [], "entities": [{"text": "ALG augmented data", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7701922059059143}]}, {"text": " Table 4: Average performance of the models per class trained on the MSA augmented data.", "labels": [], "entities": [{"text": "MSA augmented data", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.7157185872395834}]}]}