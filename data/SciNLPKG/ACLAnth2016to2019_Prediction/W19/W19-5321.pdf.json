{"title": [{"text": "Microsoft Translator at WMT 2019: Towards Large-Scale Document-Level Neural Machine Translation", "labels": [], "entities": [{"text": "WMT 2019", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.6282842457294464}, {"text": "Large-Scale Document-Level Neural Machine Translation", "start_pos": 42, "end_pos": 95, "type": "TASK", "confidence": 0.6092052102088928}]}], "abstractContent": [{"text": "This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German.", "labels": [], "entities": [{"text": "WMT19 news translation shared task", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.7945866227149964}]}, {"text": "Our main focus is document-level neural machine translation with deep transformer models.", "labels": [], "entities": [{"text": "document-level neural machine translation", "start_pos": 18, "end_pos": 59, "type": "TASK", "confidence": 0.6224177181720734}]}, {"text": "We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with trans-lationese input.", "labels": [], "entities": []}, {"text": "We explore fine-tuning techniques , deeper models and different ensem-bling strategies to counter these effects.", "labels": [], "entities": []}, {"text": "Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models.", "labels": [], "entities": []}, {"text": "We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries.", "labels": [], "entities": []}, {"text": "We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems.", "labels": [], "entities": [{"text": "BERT-objective", "start_pos": 120, "end_pos": 134, "type": "METRIC", "confidence": 0.9887744188308716}]}, {"text": "Based on preliminary human evaluation results, evalu-ators strongly prefer the document-level systems over our comparable sentence-level system.", "labels": [], "entities": []}, {"text": "The document-level systems also seem to score higher than the human references in source-based direct assessment.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task () for English-German.", "labels": [], "entities": [{"text": "WMT19 news translation shared task", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.797226083278656}]}, {"text": "Our main focus is document-level neural machine translation with deep transformer models.", "labels": [], "entities": [{"text": "document-level neural machine translation", "start_pos": 18, "end_pos": 59, "type": "TASK", "confidence": 0.6224177181720734}]}, {"text": "We first explore strong sentence-level systems, trained on large-scale data created via data-filtering and noisy back-translation and investigate the interaction of both with the translation direction of the development sets.", "labels": [], "entities": []}, {"text": "We find that backtranslation seems to mainly help with translationese input.", "labels": [], "entities": [{"text": "translationese input", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.9061953723430634}]}, {"text": "Next, we explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects.", "labels": [], "entities": []}, {"text": "Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models.", "labels": [], "entities": []}, {"text": "We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries.", "labels": [], "entities": []}, {"text": "We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder, and two-pass decoding for combinations of sentence-level and document-level systems.", "labels": [], "entities": [{"text": "BERT-objective", "start_pos": 120, "end_pos": 134, "type": "METRIC", "confidence": 0.9917356371879578}]}, {"text": "We find that current transformer models are perfectly capable of translating whole documents with up to 1000 subword segments with improved quality over comparable sentence-level systems.", "labels": [], "entities": []}, {"text": "Deeper models seem to benefit more from the added context.", "labels": [], "entities": []}, {"text": "Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over comparable sentence-level systems.", "labels": [], "entities": []}, {"text": "The document-level systems also seem to score higher than the human references in source-based direct assessment.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train our document-level models with similar hyper-parameters as our sentence-level models, increasing the maximum allowed training sequence length to 1024.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: SacreBLEU results for sentence-level systems on new devset (concatenated test2016, test2017, test2018)  split by source language and combined. 6-layers denotes transformer models with 6 blocks, 12-layers with 12  blocks. For comparison, we also provide results on the original test sets although we did not use these numbers to  inform our choices. Results have been computed for a single chosen model and may vary with different random  initializations, but generally follow this pattern.", "labels": [], "entities": []}, {"text": " Table 2: SacreBLEU results for various ensembles of 12-layer sentence-level systems on new dev set (concatenated  test2016, test2017, test2018) split by source language and combined. Ensembles are weighted equally when no  weights are shown. (a) refers to a single model trained on filtered parallel data only, (c) refers to a models trained  with back-translated data, fine-tuned on filtered parallel data.", "labels": [], "entities": []}, {"text": " Table 3: SacreBLEU results for document-level systems on new devset. Missing numbers marked as * were not  computed during our experiments.", "labels": [], "entities": []}, {"text": " Table 4: SacreBLEU results for second-pass decoding of single fine-tuned sentence-level model (c) and best  sentence-level ensemble. We pass both sentence level models through two second pass models. Missing num- bers marked as * were not computed during our experiments.", "labels": [], "entities": []}, {"text": " Table 5: SacreBLEU results various for ensembles of 12-layer document-level systems on new devset", "labels": [], "entities": []}, {"text": " Table 6: Results from the WMT-Matrix on test 2019 for  our submitted systems. We also include BLEU scores  for our split dev set for comparison.", "labels": [], "entities": [{"text": "WMT-Matrix", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.8043516874313354}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.999460756778717}]}, {"text": " Table 7: Preliminary human evaluation results shared  by the organizers. Our system submissions are marked  with bold font. There was a total of 23 submissions,  we selected highest and lowest scoring systems in each  cluster and systems surrounding our own submissions.", "labels": [], "entities": []}]}