{"title": [{"text": "Approaching SMM4H with Merged Models and Multi-task Learning", "labels": [], "entities": [{"text": "Approaching SMM4H", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.4901784807443619}]}], "abstractContent": [{"text": "We describe our submissions to the 4th edition of the Social Media Mining for Health Applications (SMM4H) shared task.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) shared task", "start_pos": 54, "end_pos": 117, "type": "TASK", "confidence": 0.7953015755523335}]}, {"text": "Our team (UZH) participated in two sub-tasks: Automatic classifications of adverse effects mentions in tweets (Task 1) and Generaliz-able identification of personal health experience mentions (Task 4).", "labels": [], "entities": [{"text": "Automatic classifications of adverse effects mentions in tweets", "start_pos": 46, "end_pos": 109, "type": "TASK", "confidence": 0.7742592133581638}, {"text": "Generaliz-able identification of personal health experience mentions", "start_pos": 123, "end_pos": 191, "type": "TASK", "confidence": 0.9293946368353707}]}, {"text": "For our submissions , we exploited ensembles based on a pre-trained language representation with a neu-ral transformer architecture (BERT) (Tasks 1 and 4) and a CNN-BiLSTM(-CRF) network within a multi-task learning scenario (Task 1).", "labels": [], "entities": [{"text": "BERT", "start_pos": 133, "end_pos": 137, "type": "METRIC", "confidence": 0.9531082510948181}]}, {"text": "These systems are placed on top of a carefully crafted pipeline of domain-specific pre-processing steps.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Social Media Mining for Health Applications (SMM4H) shared task 2019) focused on classical natural-languageprocessing (NLP) problems applied to Twitter microposts (tweets).", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) shared task", "start_pos": 4, "end_pos": 67, "type": "TASK", "confidence": 0.7030303478240967}]}, {"text": "Our team participated in two tasks of binary text classification: tweets are labeled positive if they contain an Adverse Drug Reaction (ADR) in Task 1 or a Personal Health Mention (PHM) in Task 4.", "labels": [], "entities": []}, {"text": "Task 1 (automatic classifications of adverse effects mentions in tweets) is a re-run of the ADR task from previous editions of the SMM4H shared task.", "labels": [], "entities": [{"text": "classifications of adverse effects mentions in tweets)", "start_pos": 18, "end_pos": 72, "type": "TASK", "confidence": 0.7801891937851906}, {"text": "SMM4H shared task", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.8705806136131287}]}, {"text": "Task 4 (generalizable identification of personal health experience mentions) was run for the first time.", "labels": [], "entities": [{"text": "generalizable identification of personal health experience mentions)", "start_pos": 8, "end_pos": 76, "type": "TASK", "confidence": 0.8873590230941772}]}, {"text": "This task consists in deciding if a tweet contains personal health mentions, as opposed to mentions of general awareness of a health issue.", "labels": [], "entities": []}, {"text": "Here, the main challenge is to generalize from the health contexts given by the two datasets provided as training data (i.e. flu vaccination and flu infection) to other, possibly very different, health contexts.", "labels": [], "entities": []}], "datasetContent": [{"text": "For Task 1, we experimented with two different systems, separately and in combination.", "labels": [], "entities": []}, {"text": "The first system (labeled MTL) is a CNN+BiLSTM neural network with multi-task-learning (MTL) capabilities).", "labels": [], "entities": [{"text": "CNN+BiLSTM neural network", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.799974524974823}]}, {"text": "The multi-task architecture allows tackling multiple tasks (datasets) in a single model, based on the idea that complementary information from different tasks can lead to mutual benefit when they are trained jointly (see e.g.).", "labels": [], "entities": []}, {"text": "The architecture distinguishes shared layers, where parameters are updated for all tasks during training, and taskspecific layers with parameters dedicated to a single task.", "labels": [], "entities": []}, {"text": "Our MTL architecture is able to handle different types of tasks, such as sequence labeling and document classification, in the same model.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.6333147287368774}, {"text": "document classification", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.7638116478919983}]}, {"text": "In the present configuration, the model was trained on data from Task 1, Task 2, and the CADEC corpus (, where the latter two served as helper tasks, solving the problem of span detection for ADRs.", "labels": [], "entities": [{"text": "CADEC corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9041430354118347}, {"text": "span detection", "start_pos": 173, "end_pos": 187, "type": "TASK", "confidence": 0.782568484544754}]}, {"text": "In the shared part of the model, character embeddings are combined with pre-trained word embeddings () into a bidirectional Long-Short Term Memory (BiLSTM) layer.", "labels": [], "entities": []}, {"text": "In the task-specific layer, the sequence-labeling tasks are modeled with Conditional Random Fields (CRF), whereas the textlevel classifier for Task 1 is based on the final state of the BiLSTM layer directly.", "labels": [], "entities": []}, {"text": "Additionally, the Task-1 classifier uses a lexicon feature based on a fuzzy-match lookup in the MedDRA vocabulary.", "labels": [], "entities": [{"text": "MedDRA vocabulary", "start_pos": 96, "end_pos": 113, "type": "DATASET", "confidence": 0.9390771687030792}]}, {"text": "We trained 10 different models in a crossvalidation setting, using a held-out set to prevent overfitting through early stopping.", "labels": [], "entities": []}, {"text": "The predicted labels are based on the mean of the scores of all folds (transformed by softmax).", "labels": [], "entities": []}, {"text": "We based the second system (labeled BERT) for Task 1 on BERT, a pre-trained language representation with a neural transformer architecture (Devlin et al., 2018).", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9534408450126648}, {"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9739065766334534}]}, {"text": "Our system merged parameters of 20 models (originating from 10-fold cross validation trained once for four epochs and once with early stopping 3 ) into a single model.", "labels": [], "entities": []}, {"text": "For this, we calculated the weighted sum of parameters across models: we weighted parameters of each model by their performance on the respective testing fold (measured as F-score and transformed by softmax).", "labels": [], "entities": [{"text": "F-score", "start_pos": 172, "end_pos": 179, "type": "METRIC", "confidence": 0.9838233590126038}]}, {"text": "By applying this method, we first separately merged the systems resulting from training with early stopping and from training for 4 fixed epochs, and subsequently, merged the two resulting systems into a single system.", "labels": [], "entities": []}, {"text": "For this last merging step, we gave the system resulting from merging early stopping systems nine times the weight of the other system which resulted from merging systems trained fora fixed number of epochs.", "labels": [], "entities": []}, {"text": "For the last run (MTL+BERT), we combined predictions from all 20 BERT systems with the first system and a second MTL configuration which uses different word embeddings (    different types of BERT-based ensemble systems.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9674360752105713}]}, {"text": "Our first system (labeled Merge) is similar to the second system (BERT) of Task 1.", "labels": [], "entities": [{"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9955333471298218}]}, {"text": "We trained two systems using 10-fold cross validation: one for infection and one for vaccination.", "labels": [], "entities": []}, {"text": "Subsequently, we first merged the resulting systems across folds and, in a second step, we merged the two resulting systems into one single system, giving nine times the weight to the system resulting from training on the infection dataset.", "labels": [], "entities": []}, {"text": "This run has ranked first among all systems participating in the task.", "labels": [], "entities": []}, {"text": "The second run (labeled Average) is again trained on both datasets separately using 10-fold cross validation, resulting in 20 independent systems.", "labels": [], "entities": [{"text": "Average", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.8936721086502075}]}, {"text": "Labels are determined by averaging label probabilities returned by all 20 systems.", "labels": [], "entities": []}, {"text": "Finally, the third run (Join) is trained on both datasets jointly but giving twice as much weight to all data points from the infection dataset, again using 10-fold cross validation, and probabilities were averaged across these 10 systems.", "labels": [], "entities": []}, {"text": "For both tasks, our BERT classifiers are based on the PyTorch implementation of BERT 5 and fine-tune the pre-trained model provided by Google research as BERT-Base, Uncased shows official results on the test set.", "labels": [], "entities": [{"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9197201132774353}, {"text": "PyTorch implementation of BERT 5", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8058123588562012}, {"text": "BERT-Base", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.8069698214530945}]}, {"text": "The official unlabeled test sets for Tasks 1 and 4 comprise 4575 and 285 tweets, respectively.", "labels": [], "entities": []}, {"text": "Apart from an overall evaluation, systems submitted for Task 4 were also evaluated with respect to three different health contexts (also: health concerns), which were still undisclosed by the time we wrote this system description.", "labels": [], "entities": []}, {"text": "For our best performing system (Merge), results for each health context can be found in.", "labels": [], "entities": []}, {"text": "In Task 1, the BERT-based model clearly outperformed our competing MTL-based approach.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 15, "end_pos": 25, "type": "METRIC", "confidence": 0.996159553527832}]}, {"text": "After the submission deadline, we used the evaluation interface to obtain test set evaluation scores fora BERT system, which for Task 1 only includes the systems trained with early stopping (i.e. we excluded the system which was trained for 4 fixed epochs).", "labels": [], "entities": [{"text": "BERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9961715340614319}]}, {"text": "This still gave us a considerable improvement.", "labels": [], "entities": []}, {"text": "Besides merging the 10 models into one, we also experimented with voting ensembles but found that merging models in fact gave us the best performance, with the weighted version still achieving a slight improvement compared to the unweighted version.", "labels": [], "entities": []}, {"text": "Results for Task 1 postsubmission runs can be found in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tweets provided for each task as  training data. Task 4 includes data from the health con- text of Vaccination (Vacc) and Infection (Inf ). Unique  tweets are counted after pre-processing followed by re- moval of duplicates.", "labels": [], "entities": []}, {"text": " Table 2: Official scores for our submissions, compared  to mean scores of all participating systems (best results  in bold).", "labels": [], "entities": []}, {"text": " Table 3: Official scores for Task 4, System 1  (Merged BERT models across contexts) by Health  Context/Health Concern (HC).", "labels": [], "entities": [{"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9391982555389404}]}]}