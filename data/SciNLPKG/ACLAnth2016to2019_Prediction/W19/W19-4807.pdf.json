{"title": [{"text": "Do Human Rationales Improve Machine Explanations?", "labels": [], "entities": [{"text": "Human Rationales Improve Machine Explanations", "start_pos": 3, "end_pos": 48, "type": "TASK", "confidence": 0.813331127166748}]}], "abstractContent": [{"text": "Work on \"learning with rationales\" shows that humans providing explanations to a machine learning system can improve the system's pre-dictive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9058331251144409}]}, {"text": "However, this work has not been connected to work in \"explainable AI\" which concerns machines explaining their reasoning to humans.", "labels": [], "entities": []}, {"text": "In this work, we show that learning with rationales can also improve the quality of the machine's explanations as evaluated by human judges.", "labels": [], "entities": []}, {"text": "Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using \"supervised attention\" are judged superior to explanations generated using normal unsupervised attention.", "labels": [], "entities": [{"text": "CNN-based text classification", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6136264304320017}]}], "introductionContent": [{"text": "Recently, the need for explainable artificial intelligence (XAI) has become a major concern due to the increased use of machine learning in automated decision making.", "labels": [], "entities": [{"text": "explainable artificial intelligence (XAI)", "start_pos": 23, "end_pos": 64, "type": "TASK", "confidence": 0.650243490934372}]}, {"text": "On the other hand, work on \"learning with rationales\" ( has shown that humans providing explanatory information supporting their supervised classification labels can improve the accuracy of machine learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9967674016952515}]}, {"text": "These human annotations that can explain classification labels are called rationales.", "labels": [], "entities": [{"text": "classification labels", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.8916822075843811}]}, {"text": "In particular, for text categorization, humans select phrases or sentences from a document that most support their decision as rationales.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.770330935716629}]}, {"text": "However, there is no work connecting \"learning from rationales\" with improving XAI, although they are clearly complementary problems.", "labels": [], "entities": []}, {"text": "Contribution We explore whether learning from human explanations actually improves a system's ability to explain its decisions to human users.", "labels": [], "entities": []}, {"text": "Specifically, we show that for explanations for text classification in the form of selected passages that best support a decision, training on human rationales improves the quality of a system's explanations as judged by human evaluators.", "labels": [], "entities": [{"text": "text classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7035360932350159}]}, {"text": "Attention mechanisms () have become standard practice in computer vision and text classification ().", "labels": [], "entities": [{"text": "text classification", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.769891083240509}]}, {"text": "In both computer vision and textbased tasks, learned attention weights have been shown through human evaluation to be useful explanations fora model's decisions; however, attention's explanatory power has come into question in recent work, which we discuss in Section 2.", "labels": [], "entities": []}, {"text": "Traditional attention mechanisms are unsupervised; however, recent work has shown that supervising attention with human annotated rationales can improve learning for text classification based on Convolutional Neural Networks (CNNs) (.", "labels": [], "entities": [{"text": "text classification", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.7317898273468018}]}, {"text": "While this work alludes to improved explainability using supervised attention, it does not explicitly evaluate this claim.", "labels": [], "entities": []}, {"text": "We extend this work by evaluating whether supervised attention using human rationales, rather than unsupervised attention, actually improves explanation.", "labels": [], "entities": []}, {"text": "Explanations from both models are full sentences that the model has weighted as being most important to the document's final classification.", "labels": [], "entities": []}, {"text": "While automated evaluations of explanations (e.g. comparing them to human gold-standard explanations () can be somewhat useful, we argue that because the goal of machine explanations is to help users, they should be directly evaluated by human judges.", "labels": [], "entities": []}, {"text": "Machine explanations can be different from human ones, but still provide good justification fora decision (.", "labels": [], "entities": []}, {"text": "This opinion is shared by other researchers in the area), but human evaluation is often avoided due to the time required and difficulty of conducting human trials.", "labels": [], "entities": [{"text": "human evaluation", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.760572761297226}]}, {"text": "We believe it is a necessary element of explainability research, and in this work, we compare the explanations from the two models through human evaluation on Mechanical Turk and find that the model trained with human rationales is judged to generate explanations that better support its decisions.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 159, "end_pos": 174, "type": "DATASET", "confidence": 0.8675336241722107}]}], "datasetContent": [{"text": "We evaluate the explanations for both models on the movie review dataset from.", "labels": [], "entities": [{"text": "movie review dataset", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.7095549801985422}]}, {"text": "It contains 1,000 positive reviews and 1,000 negative reviews where 900 of each are annotated with human rationales.", "labels": [], "entities": []}, {"text": "Each review is a document consisting of 32 sentences on average, and each annotated document contains about 8 rationale sentences.", "labels": [], "entities": []}, {"text": "We use the 1,800 annotated documents as the training set, and the remaining 200 documents without extra annotation as test.", "labels": [], "entities": []}, {"text": "The human rationales are used as supervision in RA-CNN but not in AT-CNN.", "labels": [], "entities": [{"text": "RA-CNN", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.8688873648643494}, {"text": "AT-CNN", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9337354898452759}]}, {"text": "We use Amazon Mechanical Turk (AMT) to evaluate the explanations from both AT-CNN and RA-CNN.", "labels": [], "entities": [{"text": "AT-CNN", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.823282778263092}, {"text": "RA-CNN", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.8752070069313049}]}, {"text": "Doc-CNN AT-CNN RA-CNN 86.00% 88.50% 90.00%: Classification accuracy for movie reviews.", "labels": [], "entities": [{"text": "Doc-CNN AT-CNN RA-CNN", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.7644954522450765}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8893559575080872}]}, {"text": "there area few deft touches of filmmaking that are simply outstanding , and joanne woodward' narration is exquisite.", "labels": [], "entities": []}, {"text": "where the models each correctly classified the document.", "labels": [], "entities": []}, {"text": "The statistics presented are the percentage of times reliable workers agreed that one model's explanations better supported the document's classification or were equal.", "labels": [], "entities": []}, {"text": "Overall, it is clear that RA-CNN is providing better explanations for the plurality of test documents (43.47%).", "labels": [], "entities": [{"text": "RA-CNN", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.7311843633651733}]}, {"text": "The explanations are considered equal 36.14% of the time, and the remaining 20.48% of the documents were better explained by AT-CNN.", "labels": [], "entities": [{"text": "AT-CNN", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.9735410809516907}]}, {"text": "After seeing these results, we decided to run another baseline test to ensure that AT-CNN explanations are reasonable and can at least beat a weak baseline.", "labels": [], "entities": []}, {"text": "The results from comparing AT-CNN explanations to randomly sampled sentences from the test document are in.", "labels": [], "entities": []}, {"text": "From these results we can see that AT-CNN is beating the random baseline the majority of the time, demonstrating that attention, even without human supervision, can provide helpful explanations fora model's decision.", "labels": [], "entities": []}, {"text": "To further understand the differences between the explanations from AT-CNN and RA-CNN, we calculated statistics to find the amount of overlap in the top three explanatory sentences from each model.", "labels": [], "entities": [{"text": "RA-CNN", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.8398736715316772}]}, {"text": "In 33.5% of the test documents, the models share no explanation sentences, in 43.1% they share one explanation sentence, in 22.2% they share two explanation sentences, and they share all three in 1.2%.", "labels": [], "entities": []}, {"text": "When considering just the most highly weighted sentence, or top explanation, the models agree 18.6 % of the time.", "labels": [], "entities": []}, {"text": "So while it is relatively rare for the models to produce the same top explanatory sentence, we chose to show humans three explanatory sentences per test document to provide insight even in those matching cases.", "labels": [], "entities": []}, {"text": "contains the top 3 explanations from each model for two test documents.", "labels": [], "entities": []}, {"text": "In both examples, AT-CNN extracts sentences that are more plot related and give less insight into the reviewer's opinion as compared to RA-CNN.", "labels": [], "entities": [{"text": "RA-CNN", "start_pos": 136, "end_pos": 142, "type": "DATASET", "confidence": 0.8957305550575256}]}, {"text": "These sentences are generally less helpful for understanding the classification of the movie review.", "labels": [], "entities": [{"text": "classification of the movie review", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.8089559435844421}]}, {"text": "In the second example, both models have identified a good explanatory sentence: \"watching the film clean and sober, you are bound to recognize how truly awful it is.\"", "labels": [], "entities": []}, {"text": "However, AT-CNN ranks it as less important than two sentences that primarily describe the plot of the film while RA-CNN only ranks another, equally explanatory sentence as more important.", "labels": [], "entities": [{"text": "AT-CNN", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.762108564376831}, {"text": "RA-CNN", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.6658746004104614}]}, {"text": "An interesting future avenue for evaluation is to compare explanations from when the models make incorrect predictions.", "labels": [], "entities": []}, {"text": "We found a trend in the explanations for test documents that both models misclassified where RA-CNN produced explanations that supported the misclassification while AT-CNN produced more explanations that supported the correct classification, despite the model's decision.", "labels": [], "entities": []}, {"text": "While this analysis is too small scale to be conclusive, this raises the question for future work: Do we want our explanation systems to offer the best support for the chosen decision or would it be more beneficial if they provide an explanation that brings the decision into question?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 3 explanations from both models for both a positive and negative correctly classified test document.", "labels": [], "entities": []}, {"text": " Table 4: AMT results comparing AT-CNN to the ran- dom baseline.", "labels": [], "entities": [{"text": "AMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.48853105306625366}, {"text": "AT-CNN", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.8619688153266907}]}]}