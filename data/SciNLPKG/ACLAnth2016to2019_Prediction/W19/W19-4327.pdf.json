{"title": [{"text": "Best Practices for Learning Domain-Specific Cross-Lingual Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages.", "labels": [], "entities": []}, {"text": "They area crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages.", "labels": [], "entities": []}, {"text": "A common approach to learning cross-lingual em-beddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary.", "labels": [], "entities": []}, {"text": "While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the best practices for constructing the seed dictionary fora specific domain.", "labels": [], "entities": []}, {"text": "We evaluate the embeddings on the sequence labelling task of Curriculum Vi-tae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence the performance.", "labels": [], "entities": [{"text": "Curriculum Vi-tae parsing", "start_pos": 61, "end_pos": 86, "type": "TASK", "confidence": 0.6854169766108195}]}, {"text": "We also show that the less training data is available in the low-resource language , the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.", "labels": [], "entities": []}], "introductionContent": [{"text": "Expanding Natural Language Processing (NLP) models to new languages typically involves creating completely new data sets for each language which comes with challenges such as acquiring and annotating the data.", "labels": [], "entities": []}, {"text": "To avoid these tedious and costly tasks, one can use cross-lingual embeddings to enable knowledge transfer from languages with sufficient training data to lowresource languages.", "labels": [], "entities": []}, {"text": "Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages.", "labels": [], "entities": []}, {"text": "Based on the assumption that the embedding spaces of different languages exhibit a similar structure ( , previous work proposed to learn a linear transformation which projects independently learned monolingual spaces into a single shared space, using a seed translation dictionary.", "labels": [], "entities": []}, {"text": "Although more advanced techniques involving jointly optimising monolingual and crosslingual objectives were proposed, most of these solutions require some form of cross-lingual supervision via parallel data (.", "labels": [], "entities": []}, {"text": "However, for applications targeting a specific domain (in our case, human resources) there is often little to no parallel data available, so simple alignment-based methods relying on only a small translation dictionary remain an attractive choice.", "labels": [], "entities": []}, {"text": "We adopt the Multilingual CCA framework (, and evaluate the crosslingual embedding on a sequence labelling task in Curriculum Vitae parsing domain.", "labels": [], "entities": [{"text": "Curriculum Vitae parsing domain", "start_pos": 115, "end_pos": 146, "type": "TASK", "confidence": 0.7572101354598999}]}, {"text": "We use this framework as it only requires an easier to acquire seed dictionary.", "labels": [], "entities": []}, {"text": "Previous work has shown that the quality of this dictionary influences the crosslingual embeddings.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, there has been no extensive research on the choice of a seed dictionary in a non-generic domain.", "labels": [], "entities": []}, {"text": "In addition, little attention was paid to how the quality of the bilingual dictionary affects performance as some labelled data from the target language is added.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the best practices to create a seed dictionary for training domainspecific cross-lingual embeddings.", "labels": [], "entities": []}, {"text": "We measure the impact of different choices of the dictionary creation on the downstream task: the dictionary size, the source of the words and their frequency, in both zero-shot and joint training scenarios.", "labels": [], "entities": []}], "datasetContent": [{"text": "As extrinsic evaluation metric of the cross-lingual embeddings, we use the average F1 score across the 2 entities we extract (job title and organisation name).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9820190668106079}]}, {"text": "As intrinsic evaluation metric, we use the precision at 1 (P@1) measured on the MUSE test sets consisting of 1,500 translation pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9981611371040344}, {"text": "MUSE test sets", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.9250333507855734}]}, {"text": "presents our results on how the 3 bilingual dictionary factors influence the downstream task performance and the precision@1 score.", "labels": [], "entities": [{"text": "precision@1 score", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.9547001421451569}]}, {"text": "We start with the best practices from previous work (top 5k frequent words) and change one factor at a time choosing the best performing setting when moving to the next factor.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average F1 and precision@1 score for bilingual dictionary experiments. Joint training uses 200 documents  from the low resource language.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.988955020904541}, {"text": "precision@1 score", "start_pos": 25, "end_pos": 42, "type": "METRIC", "confidence": 0.9149957746267319}]}, {"text": " Table 2: Gain from knowledge transfer, averaged F1 score. Full set is 1363 for German and 1678 for Dutch.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6771651953458786}, {"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9726192951202393}, {"text": "Full set", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8800550401210785}]}, {"text": " Table 2. For  these experiments, we use the best performing  seed dictionary (5k high-frequency words from  domain corpus). The results demonstrate that  with a strong English-only CV parsing model and", "labels": [], "entities": [{"text": "English-only CV parsing", "start_pos": 169, "end_pos": 192, "type": "TASK", "confidence": 0.48167510827382404}]}]}