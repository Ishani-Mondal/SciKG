{"title": [{"text": "Artificial Error Generation with Fluency Filtering", "labels": [], "entities": [{"text": "Artificial Error Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.603507806857427}]}], "abstractContent": [{"text": "The quantity and quality of training data plays a crucial role in grammatical error correction (GEC).", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 66, "end_pos": 100, "type": "TASK", "confidence": 0.7444620331128439}]}, {"text": "However, due to the fact that obtaining human-annotated GEC data is both time-consuming and expensive, several studies have focused on generating artificial error sentences to boost training data for grammatical error correction, and shown significantly better performance.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 200, "end_pos": 228, "type": "TASK", "confidence": 0.6351697345574697}]}, {"text": "The present study explores how fluency filtering can affect the quality of artificial errors.", "labels": [], "entities": [{"text": "fluency filtering", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8126440048217773}]}, {"text": "By comparing artificial data filtered by different levels of fluency, we find that artificial error sentences with low fluency can greatly facilitate error correction, while high fluency errors introduce more noise.", "labels": [], "entities": [{"text": "error correction", "start_pos": 150, "end_pos": 166, "type": "TASK", "confidence": 0.6532749980688095}]}], "introductionContent": [{"text": "Grammatical Error Correction (GEC), a NLP task of automatically detecting and correcting grammatical errors in text, has received much attention in the past few years, because of an ever-growing demand for reliable and quick feedback to facilitate the progress of English learners.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.865384504199028}, {"text": "automatically detecting and correcting grammatical errors in text", "start_pos": 50, "end_pos": 115, "type": "TASK", "confidence": 0.7663977332413197}]}, {"text": "Ina typical GEC task, an error sentence such as I follows his advice needs to be corrected to a grammatical sentence I follow his advice, while a grammatical sentence She follows his advice should output the same sentence without any modification.", "labels": [], "entities": [{"text": "GEC task", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.7008646130561829}]}, {"text": "Currently, neural machine translation (NMT) systems using sequence-to-sequence (seq2seq) learning () that \"translate\" incorrect sentences into correct ones, have shown to be promising in grammatical error correction, and several recent NMT approaches have obtained the state-of-the-art results in GEC (e.g.,.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8373143970966339}, {"text": "grammatical error correction", "start_pos": 187, "end_pos": 215, "type": "TASK", "confidence": 0.6114124655723572}, {"text": "GEC", "start_pos": 297, "end_pos": 300, "type": "DATASET", "confidence": 0.7807297110557556}]}, {"text": "While designing a GEC-oriented seq2seq architecture is one important aspect to achieve high performance in grammatical error correction, the quantity and quality of data also plays a crucial role in the NMT approach to GEC, as NMT parameters cannot learn and generalize well with limited training data.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6032681465148926}, {"text": "GEC", "start_pos": 219, "end_pos": 222, "type": "TASK", "confidence": 0.7929954528808594}]}, {"text": "Due to the fact that obtaining human-annotated GEC data is both timeconsuming and expensive, several studies have focused on generating artificial error sentences to boost training data for grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 190, "end_pos": 218, "type": "TASK", "confidence": 0.6573873162269592}]}, {"text": "One main approach is to extract errors and their surrounding context (the context window approach) from available annotated data, and then apply the errors to error-free sentences naively or probabilistically).", "labels": [], "entities": []}, {"text": "The other approach uses machine back-translation, which switches the source-target sentence pairs in GEC and learns to \"translate\" correct sentences into their incorrect counterparts (.", "labels": [], "entities": [{"text": "GEC", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8578709363937378}]}, {"text": "While the first approach may not generalize well to unseen errors, and the second one may have no control over what kind of error is produced, artificial error sentences generated from both approaches contribute to better performance in grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 237, "end_pos": 265, "type": "TASK", "confidence": 0.6955729325612386}]}, {"text": "In this paper, we do not focus on which approach is superior in artificial error generation.", "labels": [], "entities": [{"text": "artificial error generation", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.6543342669804891}]}, {"text": "Rather, given that both approaches can generate multiple error candidates for each correct sentence, we investigate how to select the best ones that can boost GEC performance the most.", "labels": [], "entities": [{"text": "GEC", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.8794326186180115}]}, {"text": "Although previous studies have shown that artificial errors that match the real error distributions tend to generate better results, we propose an alternative framework that incorporates fluency filtering based on language models.", "labels": [], "entities": [{"text": "fluency filtering", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.7566901445388794}]}, {"text": "We evaluate four strategies of artificial error selection using different fluency ranges (from lowest to highest) on the recent W&I+LOCNESS test set.", "labels": [], "entities": [{"text": "artificial error selection", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6311516364415487}, {"text": "W&I+LOCNESS test set", "start_pos": 128, "end_pos": 148, "type": "DATASET", "confidence": 0.7708417347499302}]}, {"text": "Our results show that three of the four strategies lead to evident improvement over the original baseline, which is inline with previous findings that in general GEC benefits from artifi-cial error data.", "labels": [], "entities": [{"text": "GEC", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.6167413592338562}]}, {"text": "The model trained with artificial error sentences with the lowest fluency obtains the highest recall among the four settings, while the one trained with error sentences with the median fluency achieves the highest performance in terms of F 0.5 , with an absolute increase of 5.06% over the baseline model.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9988967180252075}, {"text": "F 0.5", "start_pos": 238, "end_pos": 243, "type": "METRIC", "confidence": 0.9827329218387604}]}], "datasetContent": [{"text": "We used the four datasets -FCE, NUCLE, W&I+LOCNESS and Lang-8 -provided in the BEA 2019 Shared Task on GEC 1 as the training data for our baseline model (in total about 1.1M sentence pairs).", "labels": [], "entities": [{"text": "FCE", "start_pos": 27, "end_pos": 30, "type": "DATASET", "confidence": 0.7325026392936707}, {"text": "NUCLE", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.7511513829231262}, {"text": "BEA 2019 Shared Task on GEC 1", "start_pos": 79, "end_pos": 108, "type": "DATASET", "confidence": 0.8912616627556937}]}, {"text": "shows the summary of the four datasets.", "labels": [], "entities": []}, {"text": "There are slightly over half a million error-contained sentences in these datasets, where we extracted 1.3M correct-incorrect fragments.", "labels": [], "entities": []}, {"text": "We applied our artificial error injection procedure to the remaining 0.6M error-free sentences, and over 0.4M of them received replacements.", "labels": [], "entities": []}, {"text": "We trained a 3-gram language model on all the correct-side sentences using KenLM).", "labels": [], "entities": []}, {"text": "The language model was used to calculate perplexity of artificial error sentences.", "labels": [], "entities": []}, {"text": "From the 0.4M sentences with error injections, we created four different artificial datasets: one with the highest fluency error sentences among the candidates of each correct sentence, one with the lowest, one with the median, and the last one was  We used the 7-layer convolutional seq2seq model proposed in Chollampatt and Ng (2018) for grammatical error correction with minimal modification.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 340, "end_pos": 368, "type": "TASK", "confidence": 0.6149622599283854}]}, {"text": "The only difference to is that the word embedding dimensions in both encoders and decoders were set to 300 rather than 500, and the word embeddings were trained separately using the error and correct side training data instead of external corpora.", "labels": [], "entities": []}, {"text": "Other parameters were set as recommended in, including the top 30K BPE tokens as the vocabularies of input and output, 1,024 \u00d7 3 hidden layers in the encoders and decoders, Nesterov Accelerated Gradient as the optimizer with a momentum of 0.99, dropout rate of 0.2, initial learning rate of 0.25, and minimum learning rate of 10 \u22124 . A beam size of 10 was used during model inference.", "labels": [], "entities": [{"text": "initial learning rate", "start_pos": 266, "end_pos": 287, "type": "METRIC", "confidence": 0.8230581680933634}, {"text": "minimum learning rate", "start_pos": 301, "end_pos": 322, "type": "METRIC", "confidence": 0.8615386684735616}]}, {"text": "No spellchecker was incorporated in the present study, either as pre-processing or postprocessing.", "labels": [], "entities": []}, {"text": "shows the results for our baseline and models trained with different fluency-filtered artificial errors.", "labels": [], "entities": []}, {"text": "The model trained on the baseline data, which include 0.6M correct sentence pairs, performs the worst in terms of recall (18.85%), because the large proportion of the same sentences makes the model too conservative to make corrections.", "labels": [], "entities": [{"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9993875026702881}]}, {"text": "Indeed, true positive for the baseline model is only 749, which is about half of that in the lowest fluency condition.", "labels": [], "entities": []}, {"text": "All the four models with artificial errors obtain higher recall (over 26%), but at the expense of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9997439980506897}, {"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9989773035049438}]}, {"text": "The model with error sentences that have the highest fluency among candidate sentences, in particular, drops over 15% in precision compared to the baseline, making it the worst model in terms of F 0.5 (42.86%).", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9995402097702026}, {"text": "F 0.5", "start_pos": 195, "end_pos": 200, "type": "METRIC", "confidence": 0.9816104173660278}]}, {"text": "Error sentences with the lowest fluency lead to the highest recall (32.96%) and second highest F 0.5 (48.68%) among all the models, while the model in the median fluency condition achieves a good balance between precision drop and recall gain, resulting in the highest F 0.5 (49.03%).", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9993113279342651}, {"text": "F 0.5", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.9911468327045441}, {"text": "precision drop", "start_pos": 212, "end_pos": 226, "type": "METRIC", "confidence": 0.9865918457508087}, {"text": "recall gain", "start_pos": 231, "end_pos": 242, "type": "METRIC", "confidence": 0.9884614050388336}, {"text": "F 0.5", "start_pos": 269, "end_pos": 274, "type": "METRIC", "confidence": 0.9790523946285248}]}], "tableCaptions": [{"text": " Table 1: An example of an error-free sentence and its error injected candidate sentences with three levels of fluency.", "labels": [], "entities": []}, {"text": " Table 2: Summary of training data.", "labels": [], "entities": []}, {"text": " Table 3: Performance of multi-layer CNNs for GEC on  W&I+LOCNESS test set with different error data from  different fluency filtering.", "labels": [], "entities": [{"text": "GEC", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.6024538278579712}, {"text": "W&I+LOCNESS test set", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.8704135247639248}]}]}