{"title": [{"text": "LSTM Networks Can Perform Dynamic Counting", "labels": [], "entities": [{"text": "Perform Dynamic Counting", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.8861685593922933}]}], "abstractContent": [{"text": "In this paper, we systematically assess the ability of standard recurrent networks to perform dynamic counting and to encode hierarchical representations.", "labels": [], "entities": [{"text": "dynamic counting", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.6775288283824921}]}, {"text": "All the neural models in our experiments are designed to be smallsized networks both to prevent them from memorizing the training sets and to visualize and interpret their behaviour attest time.", "labels": [], "entities": []}, {"text": "Our results demonstrate that the Long ShortTerm Memory (LSTM) networks can learn to recognize the well-balanced parenthesis language (Dyck-1) and the shuffles of multiple Dyck-1 languages, each defined over different parenthesis-pairs, by emulating simple realtime k-counter machines.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this work is the first study to introduce the shuffle languages to analyze the computational power of neural networks.", "labels": [], "entities": []}, {"text": "We also show that a single-layer LSTM with only one hidden unit is practically sufficient for recognizing the Dyck-1 language.", "labels": [], "entities": []}, {"text": "However, none of our recurrent networks was able to yield a good performance on the Dyck-2 language learning task, which requires a model to have a stack-like mechanism for recognition.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recurrent Neural Networks (RNNs) are known to capture long-distance and complex dependencies within sequential data.", "labels": [], "entities": []}, {"text": "In recent years, RNNbased architectures have emerged as a powerful and effective architecture choice for language modeling (.", "labels": [], "entities": []}, {"text": "When equipped with infinite precision and rational state weights, RNN models are known to be theoretically Turingcomplete (.", "labels": [], "entities": []}, {"text": "However, there still remain some fundamental questions regarding the practical computational expressivity of RNNs with finite precision.", "labels": [], "entities": []}, {"text": "have recently demonstrated that Long Short-Term Memory (LSTM) models), a popular variant of RNNs, can, theoretically, emulate a simple real-time k-counter machine, which can be described as a finite state controller with k separate counters, each containing integer values and capable of manipulating their content by adding \u00b11 or 0 at each time step.", "labels": [], "entities": []}, {"text": "The authors further tested their theoretical result by training the LSTM networks to learn an b n and an b n c n . Their examination of the cell state dynamics of the models exhibited the existence of simple counting mechanisms in the cell states.", "labels": [], "entities": []}, {"text": "Nonetheless, these two formal languages can be captured by a particularly simple form of automaton, a deterministic one-turn two-counter automaton.", "labels": [], "entities": []}, {"text": "Hence, there is still an open question of whether the LSTMs can empirically learn to emulate more general finite-state automata equipped with multiple counters capable of performing an arbitrary number of turns.", "labels": [], "entities": []}, {"text": "In the present paper, we answer this question in the affirmative.", "labels": [], "entities": []}, {"text": "We assess the empirical performance of three types of recurrent networksElman-RNNs (or RNNs, in short), LSTMs, and Gated Recurrent Units (GRUs)-to perform dynamic counting by training them to learn the Dyck-1 language.", "labels": [], "entities": [{"text": "dynamic counting", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.756761759519577}]}, {"text": "Our results demonstrate that the LSTMs with only a single hidden unit perform with perfect accuracy on the Dyck-1 learning task, and successfully generalize far beyond the training set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9988394379615784}]}, {"text": "Furthermore, we show that the LSTMs can learn the shuffles of multiple Dyck-1 languages, defined over disjoint parenthesis-pairs, which require the emulation of multiple-counter arbitraryturn machines.", "labels": [], "entities": []}, {"text": "Our results corroborate the theoretical findings of, while extending their empirical observations.", "labels": [], "entities": []}, {"text": "On the other hand, when trained to learn the Dyck-2 language, which is a strictly context-free language, all our recurrent models failed to learn the language.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the capability of RNN-based architectures to perform dynamic counting and to encode hierarchical representations, we conducted experiments on four different synthetic sequence prediction tasks.", "labels": [], "entities": [{"text": "dynamic counting", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.6703517138957977}, {"text": "synthetic sequence prediction", "start_pos": 169, "end_pos": 198, "type": "TASK", "confidence": 0.7013207972049713}]}, {"text": "Each task was designed to highlight some particular feature of recurrent networks.", "labels": [], "entities": []}, {"text": "All the tasks were formulated as supervised learning problems with discrete k-hot targets and meansquared-error loss under the sequence prediction framework, defined next.", "labels": [], "entities": [{"text": "meansquared-error loss", "start_pos": 94, "end_pos": 116, "type": "METRIC", "confidence": 0.9487750828266144}, {"text": "sequence prediction", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.6682637631893158}]}, {"text": "We repeated each experiment ten times but used the same random seed across each run for each of the tasks to ensure comparability of RNN, GRU, and LSTM models.", "labels": [], "entities": []}, {"text": "The first language, Dyck-1 (or D 1 ), consists of well-balanced sequences of opening and closing parentheses.", "labels": [], "entities": []}, {"text": "Recall that a neural network need not be equipped with a stack-like mechanism to recognize the Dyck-1 language under the sequence prediction paradigm; a single counter DCA 1 is sufficient.", "labels": [], "entities": [{"text": "sequence prediction paradigm", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7845764557520548}]}, {"text": "However, dynamic counting is required to capture the language.", "labels": [], "entities": []}, {"text": "The next two languages are the shuffles of two and six Dyck-1 languages, each defined over disjoint parentheses; we refer to these two languages as Shuffle-2 and Shuffle-6, respectively.", "labels": [], "entities": []}, {"text": "These two tasks are formulated to investigate whether recurrent networks can emulate deterministic k-counter automata by performing dynamic counting, separately counting the number of opening and closing parentheses for each of the distinct parenthesispairs and predicting the closing parentheses for the pairs for which the counters are non-zero, in addition to the opening parentheses.", "labels": [], "entities": []}, {"text": "In contrast, the final language, Dyck-2, is a context-free language which cannot be captured by a simple counting mechanism; a model capable of recognizing the Dyck-2 language must contain a stack-like component.", "labels": [], "entities": []}, {"text": "provide example input-output pairs for the four languages under the sequenceprediction task.", "labels": [], "entities": []}, {"text": "For purposes of presentation only, we use a simple binary encoding of the output sets to concisely represent the output.", "labels": [], "entities": []}, {"text": "In all of the languages we investigate in this paper, the open parentheses are always allowable as next symbol; we assign the set of open parentheses the number 0.", "labels": [], "entities": []}, {"text": "Each closing parenthesis is assigned a different power of 2: ) is assigned to 1, ] to 2, } to 4, to 8, to 16, and to 32.", "labels": [], "entities": []}, {"text": "We note that even though an input sequence might appear in two different languages, it might have different target representations.", "labels": [], "entities": []}, {"text": "This observation is important especially when making comparisons between the Dyck-2 and the Shuffle-2 languages.", "labels": [], "entities": [{"text": "Shuffle-2 languages", "start_pos": 92, "end_pos": 111, "type": "DATASET", "confidence": 0.8264239132404327}]}, {"text": "For instance, the output sequence for ( [ ] ) in the Dyck-2 language is 1 2 1 0, whereas the output sequence for ( [ ] ) in the Shuffle-2 language is 1 3 1 0.", "labels": [], "entities": []}], "tableCaptions": []}