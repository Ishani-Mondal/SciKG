{"title": [{"text": "Verb Argument Structure Alternations in Word and Sentence Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Verbs occur in different syntactic environments , or frames.", "labels": [], "entities": []}, {"text": "We investigate whether artificial neural networks encode grammatical distinctions necessary for inferring the idiosyn-cratic frame-selectional properties of verbs.", "labels": [], "entities": []}, {"text": "We introduce five datasets, collectively called FAVA, containing in aggregate nearly 10k sentences labeled for grammatical acceptability, illustrating different verbal argument structure alternations.", "labels": [], "entities": [{"text": "FAVA", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.6333405375480652}]}, {"text": "We then test whether models can distinguish acceptable English verb-frame combinations from unacceptable ones using a sentence embedding alone.", "labels": [], "entities": []}, {"text": "For converging evidence , we further construct LaVA, a corresponding word-level dataset, and investigate whether the same syntactic features can be extracted from word embeddings.", "labels": [], "entities": []}, {"text": "Our models perform reliable classifications for some verbal alternations but not others, suggesting that while these representations do encode fine-grained lexical information, it is incomplete or can be hard to extract.", "labels": [], "entities": []}, {"text": "Further, differences between the word-and sentence-level models show that some information present in word embeddings is not passed onto the downstream sentence embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Artificial neural networks (ANNs) are powerful computational models that are able to implicitly learn syntactic and semantic features necessary fora variety of natural language tasks.", "labels": [], "entities": []}, {"text": "These empirical results raise a deeper scientific question: to what extent do the features learned by ANNs resemble the linguistic competence of humans?", "labels": [], "entities": []}, {"text": "Studying the linguistic competence of ANNs, in addition to its intrinsic value for model evaluation, can help resolve outstanding scientific questions in linguistics about the role of prior grammatical bias * The first three authors contributed equally and are listed in alphabetical order.", "labels": [], "entities": []}, {"text": "suggests that the acquisition of rich grammatical distinctions is facilitated by an innate universal grammar (UG), which imparts specific grammatical knowledge to the learner.", "labels": [], "entities": []}, {"text": "This proposal crucially depends on the poverty of the stimulus argument, which holds that the acquisition of certain linguistic features by purely domain-general datadriven learning should not be possible.", "labels": [], "entities": []}, {"text": "Studying the ability of low-bias learners like ANNs to acquire specific grammatical knowledge can provide evidence relevant to this argument.", "labels": [], "entities": []}, {"text": "In this work, we evaluate ANNs' treatment of verbs; verbs contribute to the overall meaning of sentences by encoding information about how entities are related to, and participate in, events.", "labels": [], "entities": [{"text": "ANNs' treatment of verbs; verbs contribute to the overall meaning of sentences by encoding information about how entities are related to, and participate in, events", "start_pos": 26, "end_pos": 190, "type": "Description", "confidence": 0.7777902153985841}]}, {"text": "Concretely, we investigate if ANNs acquire the specific grammatical distinctions necessary for inferring the frame-selectional properties of verbs.", "labels": [], "entities": []}, {"text": "Cross-linguistically, the lexical entry of a verb is associated with a set of syntactic contexts or syntactic frames in which it can appear.", "labels": [], "entities": []}, {"text": "This information is lexically idiosyncratic, i.e., even verbs that are intuitively very similar in meaning may vary as to which syntactic frames they can appear in: (1) a.", "labels": [], "entities": []}, {"text": "Sharon sprayed water on the plants. b. Sharon sprayed the plants with water. c. Carla poured lemonade into the pitcher. d. *Carla poured the pitcher with lemonade.", "labels": [], "entities": []}, {"text": "Certain verbs, e.g., spray, select multiple related frames and are therefore known as alternating verbs.", "labels": [], "entities": []}, {"text": "In contrast, other semantically similar verbs, e.g., pour, select only a single frame and are thus not alternating.", "labels": [], "entities": []}, {"text": "Information about whether a given verb alternates (as well as which frames it In this paper, stars mark ungrammatical sentences.", "labels": [], "entities": []}, {"text": "can appear in) has been described and classified in several verb lexica).", "labels": [], "entities": []}, {"text": "Knowledge about verb frames and their alternations is part of a human speaker's linguistic competence, and as such, should potentially be learned by ANNs.", "labels": [], "entities": []}, {"text": "We present two datasets and two experiments that compare ANNs' knowledge of verb frame alternations at the word level and the sentence level, respectively.", "labels": [], "entities": []}, {"text": "First, we ask if a verb's word embedding can be used to predict which frames that verb can licitly appear in.", "labels": [], "entities": []}, {"text": "We construct a dataset of verbs, the Lexical Verb-frame Alternations dataset (LaVA), based on, and train a multi-class classifier to identify the licit syntactic frames associated with a verb from its word embedding alone (if successful, the classifier should be able to determine, e.g., that sprayed alternates and can appear in sentences with with-alternants like (1-b), but that poured cannot (1-d)).", "labels": [], "entities": []}, {"text": "Second, we ask whether sentence embeddings encode the frame-selectional properties of their main verb.", "labels": [], "entities": []}, {"text": "The main verb's frame-selectional properties have consequences for grammaticality at the sentence level; to give an example, (1-d) is not grammatical, because poured cannot participate in this frame alternation.", "labels": [], "entities": []}, {"text": "To exploit this, we semi-automatically generate sentences in such away to ensure that the main verb's frame alternation information is the only information determining the (un)grammaticality of the sentence.", "labels": [], "entities": []}, {"text": "For a portion of the sentences, the main verb can participate in a given verb frame alternation, and for another portion it cannot; if the main verb cannot participate in the alternation, then one of the sentences in the pair will be ungrammatical.", "labels": [], "entities": []}, {"text": "Using this dataset, the Frames and Alternations of Verbs Acceptability dataset (FAVA), we train a binary classifier to judge the acceptability of sentences containing verbs in various syntactic contexts using the sentence embeddings alone.", "labels": [], "entities": []}, {"text": "We find that verb frame information is extractable from both word embeddings and sentence embeddings, but that these two complementary methods differ in performance.", "labels": [], "entities": []}, {"text": "The LaVA and FAVA datasets are available under https: //nyu-mll.github.io/CoLA for future research and model evaluation.", "labels": [], "entities": [{"text": "LaVA", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9204203486442566}, {"text": "FAVA datasets", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8446719646453857}, {"text": "CoLA", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.839531421661377}]}], "datasetContent": [{"text": "In this section, we describe in detail our wordlevel dataset, which we call the Lexical Verbframe Alternations dataset (LaVA); and the corresponding sentence-level dataset, which we call the Frames and Alternations of Verbs Acceptability dataset (FAVA).", "labels": [], "entities": []}, {"text": "Five argument structure alternations are chosen and verbs that evoke at least one frame of the alternation are included in our lexical corpus.", "labels": [], "entities": []}, {"text": "These verbs are subsequently used to semi-automatically create a sentence acceptability corpus for our second experiment.", "labels": [], "entities": []}, {"text": "We describe our selected argument structure alternations in the remainder of this section and introduce our corpora.", "labels": [], "entities": []}, {"text": "In our first experiment, we aim at classifying acceptable syntactic frames, given embeddings for each of the verbs.", "labels": [], "entities": []}, {"text": "Linguists are able to arrive at a classification of a verb according to its syntactic frames by interrogating whether sentences with a given verb and frame are acceptable.", "labels": [], "entities": []}, {"text": "Analogously, we can observe whether a verb's frame-selectional properties can be extracted from a sentence embedding by training an acceptability classifier to distinguish sentences with acceptable from sentences with unacceptable verb-frame combinations.", "labels": [], "entities": []}, {"text": "If a classifier is able to reliably classify all minimal pairs of several verbs with different frameselectional properties from a sentence embedding alone, we can infer that the sentence embedding contains enough information to distinguish both the frame-selectional properties of the verbs and the relevant syntactic frames.", "labels": [], "entities": []}, {"text": "Model Our acceptability classifier is again an MLP with a single hidden layer.", "labels": [], "entities": []}, {"text": "We model the probability that a that sentence S is acceptable as: Here x is the input, a sentence embedding obtained from the real/fake sentence encoder described in Section 4, W 1 and W 2 are weight matrices, \u03c3 denotes the sigmoid function, and tanh is the hyperbolic tangent activation function.", "labels": [], "entities": []}, {"text": "We use a threshold of 0.5 to map the model's predictions to binary outputs.", "labels": [], "entities": []}, {"text": "Training Details To select hyperparameters, we train 20 acceptability classifiers on each of the five datasets, and an additional 20 classifiers on a dataset produced by aggregating all the datasets.", "labels": [], "entities": []}, {"text": "We repeat all experiments augmenting each dataset with the more than 10k sentences: Results from Experiment 1 for CoLA-style embeddings (top) and GloVe embeddings (bottom); \"Majority BL\" denotes the majority baseline.", "labels": [], "entities": [{"text": "BL", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.7333301305770874}]}, {"text": "Bolded MCC values represent reasonably strong correlations (above 0.45).", "labels": [], "entities": [{"text": "MCC", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9941178560256958}]}, {"text": "Results for the majority baselines differ due to different words not having a vector representation within the respective embeddings.", "labels": [], "entities": []}, {"text": "The corpus does not contain negative examples for caus. and there frames (parenthetical); these results cannot be interpreted and are only included for completeness. from the corpus of linguistic acceptability (CoLA) built by.", "labels": [], "entities": []}, {"text": "Hyperparameters are chosen by random search within the following ranges: hidden size \u2208, learning rate \u2208 [10 2 , 10 5 ], and dropout rate \u2208 {0.2, 0.5}.", "labels": [], "entities": []}, {"text": "All models are trained using early stopping with a patience of 20 epochs.", "labels": [], "entities": [{"text": "patience", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9792494177818298}]}, {"text": "shows results for acceptability classification on the verb-frame datasets.", "labels": [], "entities": [{"text": "acceptability classification", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.873774915933609}]}, {"text": "These results lead us to conclude that the sentence encoder we test does reliably encode some fine-grained lexical information, but fails to do so in all cases.", "labels": [], "entities": []}, {"text": "Our models are able to perform reliable acceptability classifications on several of the alternations featured in FAVA, achieving a moderate correlation (0.5-0.7) in 5 out of 12 experiments, and a strong correlation (>0.7) in one experiment.", "labels": [], "entities": [{"text": "FAVA", "start_pos": 113, "end_pos": 117, "type": "TASK", "confidence": 0.5037972331047058}]}, {"text": "Most classifiers achieve a correlation above 0.3.", "labels": [], "entities": [{"text": "correlation", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9800262451171875}]}, {"text": "Across all verb classes, augmenting the training data with CoLA examples lowers MCC.", "labels": [], "entities": [{"text": "MCC", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9977108240127563}]}, {"text": "However, when evaluating on the aggregate dataset augmenting the training data with CoLA improves MCC.", "labels": [], "entities": [{"text": "CoLA", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.778765082359314}, {"text": "MCC", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9989620447158813}]}, {"text": "One explanation for this might be that the distribution from which the test set is drawn does not resemble the training distribution: for instance, in the CAUSATIVE-INCHOATIVE with CoLA set, training examples illustrating the relevant alternation are outnumbered about 20:1 by CoLA examples that illustrate mostly unrelated syntactically or semantically complicated phenomena.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overview of the lexical dataset. \"Positive\" refers to the number of verbs that evoke each frame (i.e., will  yield a grammatical sentence) and \"negative\" refers to the number of verbs which do not evoke those frames (i.e.,  will yield an ungrammatical sentence). Causative and there sentence frames have no negative examples (i.e., every  verb participating in the alternation can instantiate these frames).", "labels": [], "entities": []}, {"text": " Table 3: Sentence counts for our acceptability corpus.  \"% Positive\" is the percentage of sentences that count  as acceptable, i.e., as positive examples.", "labels": [], "entities": []}, {"text": " Table 4: Results from Experiment 1 for CoLA-style embeddings (top) and GloVe embeddings (bottom); \"Majority  BL\" denotes the majority baseline. Bolded MCC values represent reasonably strong correlations (above 0.45).", "labels": [], "entities": [{"text": "BL", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.7022901773452759}]}, {"text": " Table 5: Results from Experiment 2. \"w/o CoLA\" are models trained on datasets not augmented with CoLA; \"w/  CoLA\" are models trained on augmented datasets; \"Comb.\" refers to an aggregate dataset. Bolded MCC values  represent moderate correlations (above 0.45).", "labels": [], "entities": []}]}