{"title": [{"text": "Similar Minds Post Alike: Assessment of Suicide Risk by Hybrid Language and Behavioral Model", "labels": [], "entities": [{"text": "Assessment of Suicide Risk", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8187091648578644}]}], "abstractContent": [{"text": "This paper describes our system submission for the CLPsych 2019 shared task B on suicide risk assessment.", "labels": [], "entities": [{"text": "CLPsych 2019 shared task B", "start_pos": 51, "end_pos": 77, "type": "DATASET", "confidence": 0.8096370220184326}, {"text": "suicide risk assessment", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7289137244224548}]}, {"text": "We approached the problem with three separate models: a behaviour model; a language model and a hybrid model.", "labels": [], "entities": []}, {"text": "For the behavioral model approach, we model each user's behaviour and thoughts with four groups of features: posting behaviour, sentiment , motivation, and content of the user's posting.", "labels": [], "entities": []}, {"text": "We use these features as an input in a support vector machine (SVM).", "labels": [], "entities": []}, {"text": "For the language model approach, we trained a language model for each risk level using all the posts from the users as the training corpora.", "labels": [], "entities": []}, {"text": "Then, we computed the perplexity of each user's posts to determine how likely his/her posts were to belong to each risk level.", "labels": [], "entities": []}, {"text": "Finally, we built a hybrid model that combines both the language model and the behavioral model, which demonstrates the best performance in detecting the suicide risk level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Every year, there are over 800,000 people who die of suicide.", "labels": [], "entities": []}, {"text": "Although healthcare systems play a major role in assessment of suicide risk, given limited time, clinicians are unable to assess thoroughly all the risk factors.", "labels": [], "entities": [{"text": "assessment of suicide risk", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.771252766251564}]}, {"text": "One of the most important warning signs for suicide is the expressions of suicidal thoughts.", "labels": [], "entities": []}, {"text": "The standard practice of clinicians asking people about suicidal thoughts cannot effectively predict and prevent suicide, because most patients who died of suicide did not report any suicidal thoughts when asked by a doctor, therefore, many of them were assessed to have a low or moderate risk before their suicide attempts ().", "labels": [], "entities": []}, {"text": "The CLpsych 2019 shared task B ( attempts to address the challenge of automatic suicide risks asssessment using people's forum postings.", "labels": [], "entities": [{"text": "CLpsych 2019 shared task B", "start_pos": 4, "end_pos": 30, "type": "DATASET", "confidence": 0.8913063883781434}]}, {"text": "The aim of the task is to distinguish the levels of suicide risks among users who posted any contents in the suicide watch (SW) subreddit.", "labels": [], "entities": []}, {"text": "The dataset includes all the posts (N = 31,553) in any subreddit from 621 users who had posted on SW.", "labels": [], "entities": []}, {"text": "One of the four risk levels ranging from \"No Risk\" to \"Severe Risk\" was assigned to each user according to their SW posts.", "labels": [], "entities": [{"text": "Severe Risk\"", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9292342861493429}]}, {"text": "The annotation process is described in.", "labels": [], "entities": []}, {"text": "We treat the task as a multi-classification problem.", "labels": [], "entities": []}, {"text": "We approach it with three models: a behavioural model (BM), a suicide language model (SLM) and a hybrid model (HM BM SLM ) that combines the (BM) and (SLM) models.", "labels": [], "entities": []}, {"text": "The SLM offers good classification accuracy, but it does not provide any human interpretable reason for its classification decisions.", "labels": [], "entities": [{"text": "classification", "start_pos": 20, "end_pos": 34, "type": "TASK", "confidence": 0.9519495368003845}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9357378482818604}]}, {"text": "Hence, we define a collection of features to better capture users' posting behaviours and thoughts, then we use these features in the BM.", "labels": [], "entities": [{"text": "BM", "start_pos": 134, "end_pos": 136, "type": "DATASET", "confidence": 0.8064151406288147}]}, {"text": "The overall results show that the hybrid model (HM BM SLM ) performs the best in identifying the risk level with a f1 score 38% for the CLPsych task B.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used for training the models is provided by the CLPsych shared task B (.", "labels": [], "entities": [{"text": "CLPsych shared task B", "start_pos": 60, "end_pos": 81, "type": "DATASET", "confidence": 0.7570227831602097}]}, {"text": "It contains 621 reddit users who had posted on SW with an overall of 31,553 posts.", "labels": [], "entities": []}, {"text": "The users are labeled as \"no risk\" (class A), \"low risk\" (class B), \"moderate risk\" (class C), and \"severe risk\" (class D).", "labels": [], "entities": []}, {"text": "Dataset statistics is presented in table 1.", "labels": [], "entities": []}, {"text": "From the training set, it is shown that nearly half of the posts were labeled as \"severe risk\", class B only accounts for less than 10% of the posts.", "labels": [], "entities": []}, {"text": "Nearly half of the posts in both the training and testing sets did not have any contents in the post body.", "labels": [], "entities": []}, {"text": "In the SLM, for each document, the model with the lowest perplexity is assigned to the document.", "labels": [], "entities": [{"text": "SLM", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9497666954994202}]}, {"text": "Perplexity is the inverse probability of a test set, normalized by the number of words, a low perplexity indicates that the probability distribution is good at predicting the sentence.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9677342772483826}]}, {"text": "Given a sample test, we calculate its likelihood for all the models, and select the model with the best score.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9597747325897217}]}, {"text": "In the BM, we use random forest to select the top 300 features to use in the final prediction.", "labels": [], "entities": [{"text": "BM", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.49814149737358093}]}, {"text": "We validate our BM features on the multiclassification problem using support vector machines (SVM) in scikitlearn . We use the 5-fold cross validation on training data and grid-search parameters to explore both the kernels and margin of the hyperplane (C parameter).", "labels": [], "entities": []}, {"text": "Furthermore, we construct a hybrid model based on our observations on the prediction results from the SLM and the BM.", "labels": [], "entities": []}, {"text": "In the training process, we observe the BM is weak in distinguishing classes B and C, but the SLM is better in identifying class B.", "labels": [], "entities": [{"text": "BM", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.9774020910263062}]}, {"text": "Therefore, we adopt the class B results from the SLM.", "labels": [], "entities": []}, {"text": "We also find that some posts in class A are suicide experiences from someone associated with the authors, but not the authors themselves.", "labels": [], "entities": []}, {"text": "The BM is better than the language model in identifying these cases, so we use the BM for class A.", "labels": [], "entities": [{"text": "BM", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9734896421432495}]}, {"text": "However, if the confidence score is lower than 0.4, the SLM becomes better at identifying class A.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 16, "end_pos": 32, "type": "METRIC", "confidence": 0.9594904780387878}, {"text": "SLM", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9538037776947021}]}, {"text": "Therefore, we replace the results with confidence score lower than 0.4 with those from the SLM model.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.9720053672790527}]}, {"text": "https://scikit-learn.org/stable/ shows the test set results of the three models.", "labels": [], "entities": []}, {"text": "shows f1 for flagged vs. nonflagged and urgent vs. non-urgent.", "labels": [], "entities": [{"text": "f1", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.9740079045295715}]}, {"text": "Flagged vs. non-flagged distinguished class A from the rest of the classes.", "labels": [], "entities": []}, {"text": "Urgent vs. non-urgent distinguished classes A, B with classes C, D.", "labels": [], "entities": []}, {"text": "The hybrid model had the best average f1 macro in the risk assessment task.", "labels": [], "entities": []}, {"text": "In our test set result, we find that SLM is overfitting.", "labels": [], "entities": [{"text": "SLM", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9277189373970032}]}, {"text": "SLM classifies most of the posts to class D in the testing set.", "labels": [], "entities": []}, {"text": "Whereas, the BM has consistent good performances on classes A and D, but poor performances on classes B and C.", "labels": [], "entities": [{"text": "BM", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.5970668196678162}]}], "tableCaptions": [{"text": " Table 1: Basic Statistics for train and test set", "labels": [], "entities": []}, {"text": " Table 2: Results for risk assessment task", "labels": [], "entities": []}, {"text": " Table 3: Results for flagged and urgent cases", "labels": [], "entities": []}]}