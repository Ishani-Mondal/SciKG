{"title": [{"text": "Incorporating Source Syntax into Transformer-Based Neural Machine Translation", "labels": [], "entities": [{"text": "Transformer-Based Neural Machine Translation", "start_pos": 33, "end_pos": 77, "type": "TASK", "confidence": 0.6984518617391586}]}], "abstractContent": [{"text": "Transformer-based neural machine translation (NMT) has recently achieved state-of-the-art performance on many machine translation tasks.", "labels": [], "entities": [{"text": "Transformer-based neural machine translation (NMT)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8464261378560748}, {"text": "machine translation tasks", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.814569354057312}]}, {"text": "However, recent work (Ra-ganato and Tiedemann, 2018; Tang et al., 2018; Tran et al., 2018) has indicated that Transformer models may not learn syntactic structures as well as their recurrent neu-ral network-based counterparts, particularly in low-resource cases.", "labels": [], "entities": []}, {"text": "In this paper, we incorporate constituency parse information into a Transformer NMT model.", "labels": [], "entities": [{"text": "constituency parse", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8157182335853577}]}, {"text": "We leverage linearized parses of the source training sentences in order to inject syntax into the Transformer architecture without modifying it.", "labels": [], "entities": []}, {"text": "We introduce two methods: a multi-task machine translation and parsing model with a single encoder and decoder, and a mixed encoder model that learns to translate directly from parsed and unparsed source sentences.", "labels": [], "entities": [{"text": "multi-task machine translation and parsing", "start_pos": 28, "end_pos": 70, "type": "TASK", "confidence": 0.7650233030319213}]}, {"text": "We evaluate our methods on low-resource translation from English into twenty target languages , showing consistent improvements of 1.3 BLEU on average across diverse target languages for the multi-task technique.", "labels": [], "entities": [{"text": "low-resource translation from English", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7700282782316208}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9991174340248108}]}, {"text": "We further evaluate the models on full-scale WMT tasks, finding that the multi-task model aids low-and medium-resource NMT but degenerates high-resource English\u2192German translation .", "labels": [], "entities": [{"text": "WMT tasks", "start_pos": 45, "end_pos": 54, "type": "TASK", "confidence": 0.9263488948345184}]}], "introductionContent": [{"text": "Transformer-based neural machine translation (NMT) ( has recently outperformed recurrent neural network (RNN)-based models () in many tasks (.", "labels": [], "entities": [{"text": "Transformer-based neural machine translation (NMT)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.7907519595963615}]}, {"text": "However, there is still room for improvement for NMT, particularly for low-and moderate-resource language pairs.", "labels": [], "entities": [{"text": "NMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9007094502449036}]}, {"text": "Enriching NMT with syntactic information has the potential to improve generalization in low-resource scenarios, and adding syntax to Transformer-based NMT is currently an underexplored research area.", "labels": [], "entities": []}, {"text": "Transformer-based NMT may in fact stand to benefit even more from explicit syntactic annotations than RNN-based NMT, particularly in lowresource settings.", "labels": [], "entities": []}, {"text": "On the one hand, the Transformer model already learns some syntax without explicit supervision in high-resource cases.", "labels": [], "entities": []}, {"text": "visualized a few encoder self-attentions in a trained NMT model and found that they seemed to capture syntactic structure.", "labels": [], "entities": []}, {"text": "This was formalized by, who found that Transformer encoders trained on high-resource NMT tasks were able to perform reasonably well at part-of-speech tagging, chunking, and other tasks.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.7066000699996948}]}, {"text": "However, for Transformers trained on low-resource NMT, the results on these tasks were not as strong.", "labels": [], "entities": []}, {"text": "Additionally, found that an RNN language model did better at predicting subject-verb agreement than a Transformer language model; saw similar results for Transformer vs. RNN NMT models.", "labels": [], "entities": [{"text": "predicting subject-verb agreement", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.8625521262486776}]}, {"text": "Thus, the goal of this paper is to improve Transformer-based NMT using source-side syntactic supervision.", "labels": [], "entities": [{"text": "Transformer-based NMT", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.7186778783798218}]}, {"text": "We propose two methods that incorporate source-side linearized constituency parses into Transformer-based NMT.", "labels": [], "entities": [{"text": "source-side linearized constituency parses", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.5609677657485008}]}, {"text": "The first, multi-task, uses the Transformer to learn to parse and translate the source sentence simultaneously.", "labels": [], "entities": []}, {"text": "The second, mixed encoder, learns to translate directly from both parsed and unparsed source sentences.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 This is one of the first attempts at using syntax to improve Transformer-based NMT \u2022 We introduce two methods for adding syntax perm\u00edtanme utilizar una comparaci\u00f3n . (ROOT (S (NP ) (VP (VP (VP ) ) ) ) ) . .", "labels": [], "entities": [{"text": "ROOT", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.972785234451294}]}, {"text": "(a) Multi-task syntactic NMT model.", "labels": [], "entities": []}, {"text": "The system is trained to translate (<TR>) and parse (<PA>) source sentences using the same architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our multi-task and mixed encoder models compared to a standard (non-syntactic) Transformer baseline on translation from English into 21 target languages.", "labels": [], "entities": []}, {"text": "Sections 4.1 and 5.1 contain detailed information on the target languages and data used.", "labels": [], "entities": []}, {"text": "All models are implemented in Sockeye ().", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9157599210739136}]}, {"text": "For hyperparameter settings, we follow the recommendations of.", "labels": [], "entities": []}, {"text": "We preprocess our data for all experiments as follows.", "labels": [], "entities": []}, {"text": "First, we tokenize and truecase the data using the Moses scripts ().", "labels": [], "entities": []}, {"text": "We then train separate subword vocabularies for the source and target languages, with 30k merge operations per language.", "labels": [], "entities": []}, {"text": "We use the Stanford CoreNLP parser () to generate constituency parses of the source (English) sentences, and linearize and format the parses as described in section 2.1.", "labels": [], "entities": []}, {"text": "We do not use any monolingual training data; however, our proposed models are amenable to adding monolingual data, and we expect that BLEU scores would strongly increase if monolingual training data were used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.998893678188324}]}], "tableCaptions": [{"text": " Table 4: BLEU scores on the test set for small- scale cross-lingual experiments for the baseline (base),  mixed encoder (mixed enc.), and multi-task models.  Difference with the baseline is shown in parentheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988638162612915}]}, {"text": " Table 5: BLEU scores (and improvement over the  baseline) for EN\u2192TR on the test (newstest2017) and  held-out (newstest2018) datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995867609977722}, {"text": "TR", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.6029722094535828}, {"text": "newstest2018) datasets", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.8086678584416708}]}, {"text": " Table 6: BLEU scores (and improvement over the  baseline) for EN\u2192RO on the test set (newstest2016).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994996786117554}, {"text": "RO", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.5584698915481567}]}, {"text": " Table 7: BLEU scores (and difference with the base- line) for EN\u2192DE on the test (newstest2016) and held- out (newstest2017) datasets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995253086090088}, {"text": "DE", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.8549294471740723}, {"text": "newstest2017) datasets", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.8092201153437296}]}, {"text": " Table 8: Percent of valid parses of the parses generated  by the Europarl multi-task systems.", "labels": [], "entities": [{"text": "Percent", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9731595516204834}, {"text": "Europarl", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9721828699111938}]}]}