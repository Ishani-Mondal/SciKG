{"title": [], "abstractContent": [{"text": "In the current work, we present a description of the system submitted to WMT 2019 News Translation Shared task.", "labels": [], "entities": [{"text": "WMT 2019 News Translation Shared task", "start_pos": 73, "end_pos": 110, "type": "TASK", "confidence": 0.744117816289266}]}, {"text": "The system was created to translate news text from Lithuanian to English.", "labels": [], "entities": []}, {"text": "To accomplish the given task, our system used a Word Embedding based Neu-ral Machine Translation model to post edit the outputs generated by a Statistical Machine Translation model.", "labels": [], "entities": [{"text": "Neu-ral Machine Translation", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.5930282473564148}, {"text": "Statistical Machine Translation", "start_pos": 143, "end_pos": 174, "type": "TASK", "confidence": 0.5788635412851969}]}, {"text": "The current paper documents the architecture of our model, descriptions of the various modules and the results produced using the same.", "labels": [], "entities": []}, {"text": "Our system garnered a BLEU score of 17.6.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9804185926914215}]}], "introductionContent": [{"text": "Machine Translation (MT) is automated translation of one natural language to another using a computer.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8797544479370117}]}, {"text": "Translation, itself, is a very tough task for both humans as well as a computer.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.957587718963623}]}, {"text": "It requires a thorough understanding of the syntax and semantics of both the languages under consideration.", "labels": [], "entities": []}, {"text": "For producing good translations, a MT system needs good quality and sufficient amount of parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9888604283332825}]}, {"text": "In the modern context, MT systems can be categorized into Statistical Machine Translation (SMT) and Neural Machine Translation (NMT).", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9903337359428406}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 58, "end_pos": 95, "type": "TASK", "confidence": 0.7656196306149164}, {"text": "Neural Machine Translation (NMT)", "start_pos": 100, "end_pos": 132, "type": "TASK", "confidence": 0.8211412926514944}]}, {"text": "SMT has had its share in making MT very popular among the masses.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8294828534126282}, {"text": "MT", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.9902133345603943}]}, {"text": "It includes creating statistical models, whose input parameters are derived from the analysis of bilingual text corpora, created by professional translators.", "labels": [], "entities": []}, {"text": "The state-of-art for SMT is Moses Toolkit 1 , created by, incorporates subcomponents like Language Model generation, Word Alignment and Phrase Table generation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9946486353874207}, {"text": "Language Model generation", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.638676385084788}, {"text": "Word Alignment", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.7038044780492783}, {"text": "Phrase Table generation", "start_pos": 136, "end_pos": 159, "type": "TASK", "confidence": 0.6949190497398376}]}, {"text": "Various works have been done in SMT and it has shown good results for many language pairs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9932793974876404}]}, {"text": "1 http://www.statmt.org/moses/ On the other hand NMT ( ), though relatively new, has shown considerable improvements in the translation results when compared to SMT).", "labels": [], "entities": [{"text": "translation", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.9327782392501831}, {"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9728111624717712}]}, {"text": "This includes better fluency of the output and better handling of the Out-of-Vocabulary problem.", "labels": [], "entities": []}, {"text": "Unlike SMT, it doesnt depend on alignment and phrasal unit translations).", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9901856184005737}]}, {"text": "On the contrary, it uses an EncoderDecoder approach incorporating Recurrent Neural Cells ( ).", "labels": [], "entities": []}, {"text": "As a result, when given sufficient amount of training data, it gives much more accurate results when compared to SMT ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.974631130695343}]}, {"text": "For the given task 2 , we attempted to create a MT system that can translate sentences from Lithuanian to English.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9669877290725708}]}, {"text": "Since, using only SMT or NMT models leads to some or the other disadvantages, we tried to use both in a pipeline.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9045332670211792}]}, {"text": "This leads to an improvement of the results over the individual usage of either SMT or NMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9519866108894348}]}, {"text": "The main idea was to train a SMT model for translating Lithuanian language to English.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9917945265769958}, {"text": "translating Lithuanian language to English", "start_pos": 43, "end_pos": 85, "type": "TASK", "confidence": 0.848253870010376}]}, {"text": "Thereafter, a test set was translated using this model.", "labels": [], "entities": []}, {"text": "Then, a word embedding based NMT model was trained to learn the mappings between the SMT output (in English) and the gold standard data (in English).", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9795642495155334}]}, {"text": "The organizers provided the required parallel corpora, consisting of 9,62,022 sentence pairs, for training the translation model.", "labels": [], "entities": [{"text": "translation", "start_pos": 111, "end_pos": 122, "type": "TASK", "confidence": 0.9632996320724487}]}, {"text": "Among this, 7,62,022 pairs was used to train the SMT system and 2,00,000 pairs were used to test the SMT system and then train the NMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.6006856560707092}, {"text": "NMT", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8530740141868591}]}, {"text": "The statistics of the parallel corpus is depicted in 1.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 will describe the methodology of creating the SMT and the NMT model and will in-: Statistics of the Lithuanian-English parallel corpus provided by the organizers.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9831794500350952}]}, {"text": "\"Lt\" and \"En\" depict Lithuanian and English, respectively.", "labels": [], "entities": [{"text": "En", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9855263233184814}]}, {"text": "\"vocab\" means vocabulary of unique tokens.", "labels": [], "entities": []}, {"text": "clude the preprocessing steps, a brief summary of the encoder-decoder approach and the architecture of our system.", "labels": [], "entities": []}, {"text": "This will be followed by the results and conclusion in Section 3 and 4, respectively.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.981346845626831}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracy scores calculated using various  autmoated evaluation metrics.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988648891448975}]}]}