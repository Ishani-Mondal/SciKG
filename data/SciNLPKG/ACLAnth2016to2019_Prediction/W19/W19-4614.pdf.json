{"title": [{"text": "Assessing Arabic Weblog Credibility via Deep Co-learning", "labels": [], "entities": [{"text": "Assessing Arabic Weblog", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.801251232624054}]}], "abstractContent": [{"text": "Assessing the credibility of online content has garnered a lot of attention lately.", "labels": [], "entities": [{"text": "Assessing the credibility of online content", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8282446463902792}]}, {"text": "We focus on one such type of online content, namely we-blogs or blogs for short.", "labels": [], "entities": []}, {"text": "Some recent work attempted the task of automatically assessing the credibility of blogs, typically via machine learning.", "labels": [], "entities": []}, {"text": "However, in the case of Arabic blogs, there are hardly any datasets available that can be used to train robust machine learning models for this difficult task.", "labels": [], "entities": []}, {"text": "To overcome the lack of sufficient training data, we propose deep co-learning, a semi-supervised end-to-end deep learning approach to assess the credibility of Arabic blogs.", "labels": [], "entities": []}, {"text": "In deep co-learning, multiple weak deep neural network classifiers are trained using a small labeled dataset, and each using a different view of the data.", "labels": [], "entities": []}, {"text": "Each one of these classifiers is then used to classify unlabeled data, and its prediction is used to train the other classifiers in a semi-supervised fashion.", "labels": [], "entities": []}, {"text": "We evaluate our deep co-learning approach on an Arabic blogs dataset, and we report significant improvements in performance compared to many baselines including fully-supervised deep learning models as well as ensemble models.", "labels": [], "entities": [{"text": "Arabic blogs dataset", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.8679958581924438}]}], "introductionContent": [{"text": "Weblogs, also known as blogs, are gaining popularity, as alternative sources of news and information.", "labels": [], "entities": []}, {"text": "The size of the blogosphere is exponentially increasing.", "labels": [], "entities": []}, {"text": "For instance, as of October 2018, the popular blogging website Tumblr estimates the total number of blogs on the website to be above 450 million blogs with over 167 billion blog posts . With the surge in misinformation, disinformation and fake news on the Web, and their adverse effects on spreading rumors, tampering with election results and promoting propaganda, an important research question is how to assess the credibil-1 https://www.tumblr.com/about ity of blog posts.", "labels": [], "entities": []}, {"text": "This is particularly crucial in the case of the Arabic speaking world given its recent and constant turmoil.", "labels": [], "entities": []}, {"text": "There has been thus an increased interest in the machine learning and data mining communities to tackle the problem of fake news) and the credibility of content in social media in general.", "labels": [], "entities": []}, {"text": "Some works also focused on the credibility of blog posts ().", "labels": [], "entities": []}, {"text": "Most such approaches relied on careful feature-engineering.", "labels": [], "entities": []}, {"text": "In this paper, we propose to utilize end-to-end deep learning to assess the credibility of Arabic blog posts.", "labels": [], "entities": []}, {"text": "Deep Learning is a type of machine learning that uses deep neural networks to automatically learn features without spending an undue effort to engineer these features as is custom in traditional machine learning.", "labels": [], "entities": []}, {"text": "It has been shown to perform significantly better than any other approaches for various NLP tasks.", "labels": [], "entities": []}, {"text": "However, deep learning models require a large amount of training data.", "labels": [], "entities": []}, {"text": "Assessing the credibility of blog posts is a difficult task and one that has not yet received enough attention from the research community.", "labels": [], "entities": [{"text": "Assessing the credibility of blog posts", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7933223843574524}]}, {"text": "This has led to only scarce datasets of blogs that are labeled for credibility.", "labels": [], "entities": []}, {"text": "This is again particularly true in the case of Arabic blogs, with hardly any such datasets available, with the exception of (Al, which only consists of few hundreds of annotated blog posts.", "labels": [], "entities": []}, {"text": "To overcome the lack of sufficient training data, we propose a semi-supervised deep learning approach, which we refer to as deep co-learning.", "labels": [], "entities": []}, {"text": "Deep co-learning is based on co-training, an approach first introduced by) that utilizes multiple classifiers that learn from each other using different views (i.e., features) of the data.", "labels": [], "entities": []}, {"text": "In particular, the classifiers are all initially trained in a completely supervised manner using a small training dataset.", "labels": [], "entities": []}, {"text": "Each trained classifier is then used to label some unlabeled data, and this automatically labeled data by each classifier is then used to re-train the other classifiers in a semi-supervised fashion.", "labels": [], "entities": []}, {"text": "In our approach, we use a small fully-labeled dataset to train two deep learning models for assessing the credibility of Arabic blog posts.", "labels": [], "entities": []}, {"text": "The two classifiers are based on a convolutional neural network (CNN) architecture.", "labels": [], "entities": []}, {"text": "The first model uses continuous bag of words (CBOW) word embeddings as features, while the second uses characterlevel embeddings.", "labels": [], "entities": []}, {"text": "We then iteratively retrain our classifiers by applying each classifier on an unlabeled dataset of Arabic blog posts and use the output of each classifier to re-train the other classifier.", "labels": [], "entities": []}, {"text": "We evaluate our approach on an Arabic blogs dataset and compare it to various baselines.", "labels": [], "entities": [{"text": "Arabic blogs dataset", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.8026762008666992}]}, {"text": "Our contributions can be summarized as follows: \u2022 We build an end-to-end deep learning model to assess the credibility of Arabic blog posts \u2022 We utilize semi-supervised learning to train our model even in the lack of sufficient training data \u2022 We evaluate our approach on an Arabic blogs dataset) and demonstrate its effectiveness compared to many baselines The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by reviewing related work, then describe our deep colearning approach for assessing the credibility of blog posts.", "labels": [], "entities": []}, {"text": "We then present our experimental results where we evaluate our approach on a publicly available Arabic blogs dataset.", "labels": [], "entities": [{"text": "Arabic blogs dataset", "start_pos": 96, "end_pos": 116, "type": "DATASET", "confidence": 0.837075412273407}]}, {"text": "Finally, we conclude and present future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our deep co-learning approach, we use a dataset of Arabic blog posts constructed by Al).", "labels": [], "entities": []}, {"text": "It consists of 268 Arabic blog posts.", "labels": [], "entities": []}, {"text": "The collected blog posts were based on trendy topics at the time of construction, such as Lebanese parliament elections, FIFA world cup, Lebanese residential elections, the Gaza war, the Syrian war, and conflicts in Egypt.", "labels": [], "entities": []}, {"text": "To annotate the blogs for credibility, the authors relied on crowdsourcing and the annotators had to label each blog post as credible, fairly credible, or not credible.", "labels": [], "entities": []}, {"text": "Note that to the best of our knowledge, this is the only dataset that is publicly available and contains credibility assessment for Arabic blog posts.", "labels": [], "entities": []}, {"text": "We divided the dataset described above as follows: 60% training, 20% validation, and 20% testing.", "labels": [], "entities": [{"text": "training", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9513566493988037}]}, {"text": "The data was split in a stratified fashion reserving the percentage of samples for each class.", "labels": [], "entities": []}, {"text": "Our two deep learning models were bootstrapped using the fully-annotated training dataset, which was used to initially train the co-learning models in the first iteration of the deep co-learning algorithm.", "labels": [], "entities": []}, {"text": "We then used the validation dataset to tune the different hyperparameters of our approach.", "labels": [], "entities": []}, {"text": "These included the number of instances m we picked at each iteration of the deep co-learning algorithm and the number of instances k with the highest scores.", "labels": [], "entities": []}, {"text": "It also included the low-level hyperparameters of the neural networks such as the number of neurons, epochs, and batch size.", "labels": [], "entities": []}, {"text": "In addition to the labeled dataset, we created a large corpus of unlabeled data, which was used to re-train our two deep learning models as described in the previous section.", "labels": [], "entities": []}, {"text": "We developed a script to download a set of blog posts from Al Arabiya Blogs 2 and Al Hudood . This dataset consists of 20392 blogs.", "labels": [], "entities": [{"text": "Al Arabiya Blogs 2", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9367697685956955}, {"text": "Al Hudood", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.8315288722515106}]}, {"text": "We compared our deep co-learning approach to various baselines.", "labels": [], "entities": []}, {"text": "The first baseline is a lin- Algorithm 1: Deep Co-learning Algorithm ear SVM that is trained using the TF-IDF scores of the words in the blog posts, and we set the soft-margin weight C to 5 based on the validation set.", "labels": [], "entities": [{"text": "soft-margin weight C", "start_pos": 164, "end_pos": 184, "type": "METRIC", "confidence": 0.8489948113759359}]}, {"text": "This baseline is used to evaluate the effectiveness of a deep-learning approach such as ours compared to a more simple model such as SVM.", "labels": [], "entities": []}, {"text": "The second and third baselines are wordlevel convolution neural networks (Word-CNN), and a character-level convolution neural networks (Char-CNN), respectively.", "labels": [], "entities": [{"text": "Word-CNN", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9091145396232605}]}, {"text": "The last baseline we We trained all supervised models (i.e., the first two baselines and the initial models of the deep colearning approaches) for 500 epochs with a batch size of 16, a dropout of 0.2 after each hidden layer, and we used Adagrad ( the optimization algorithm.", "labels": [], "entities": []}, {"text": "All experiments were run on a Ubuntu machine with a 24 GB RAM, a CPU Intel Core I7 and a GPU NVIDIA GeForce GTX 1080 Ti 11GB.", "labels": [], "entities": []}, {"text": "For the deep co-learning approaches, we repeated the process of co-learning for 50 times since retraining the models was taking significant time which is around 24 hours.", "labels": [], "entities": []}, {"text": "In each iteration of the co-learning algorithm, we randomly picked 1000 sentences from the unlabeled data and used the top-50 scored sentences to retrain the other model.", "labels": [], "entities": []}, {"text": "All the other parameters were adjusted using the validation set.", "labels": [], "entities": []}, {"text": "Note that we also experimented with variations of the above, but we only report here the best performing ones based on validation data.", "labels": [], "entities": []}, {"text": "shows the results of our deep colearning approach and the baselines on the testing dataset.", "labels": [], "entities": []}, {"text": "We observe that an SVM model trained with TF-IDF scores as features has an F1-score of 0.57, which is higher than all the fully supervised deep learning approaches.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994770884513855}]}, {"text": "This can be mainly attributed to the small size of the training dataset, which makes it harder to train more complex models such as the fully-supervised deep learning models.", "labels": [], "entities": []}, {"text": "Comparing the fully-supervised deep learning models to each other, we observe that the deep learning model trained on character-level representations has an F1-Score of 0.54, while the deep learning model trained on word-level representations has a lower F1-score of 0.52.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9992406368255615}, {"text": "F1-score", "start_pos": 255, "end_pos": 263, "type": "METRIC", "confidence": 0.9973439574241638}]}, {"text": "The advantage of character-level models over wordlevel models is that they can learn misspellings, emoticons, and n-grams.", "labels": [], "entities": []}, {"text": "Interestingly, the ensemble model of Word-CNN and Char-CNN (Ensemble CNN in) performs worse than all other models.", "labels": [], "entities": [{"text": "Word-CNN", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9306433796882629}]}, {"text": "This indicates that with the lack of enough training data, even ensemble models are notable of generalizing well.", "labels": [], "entities": []}, {"text": "On the contrary, our deep co-learning approach, which combines the best of both worlds, the complexity of deep learning approaches and the ability to generalize well even when no sufficient training data is available through semi-supervision, significantly outperforms all the baselines with an F1-Measure of 0.63.", "labels": [], "entities": [{"text": "F1-Measure", "start_pos": 295, "end_pos": 305, "type": "METRIC", "confidence": 0.999045193195343}]}], "tableCaptions": []}