{"title": [], "abstractContent": [{"text": "The embedding of words and documents in compact, semantically meaningful vector spaces is a crucial part of modern information systems.", "labels": [], "entities": []}, {"text": "Deep Learning models are powerful but their hyperpa-rameter selection is often complex and they are expensive to train, and while pre-trained models are available, embeddings trained on general corpora are not necessarily well-suited to domain specific tasks.", "labels": [], "entities": []}, {"text": "We propose a novel embedding method which extends random projection by weighting and projecting raw term embeddings orthogonally to an average language vector, thus improving the discriminating power of resulting term embeddings, and build more meaningful document embeddings by assigning appropriate weights to individual terms.", "labels": [], "entities": [{"text": "random projection", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.778814971446991}]}, {"text": "We describe how updating the term embed-dings online as we process the training data results in an extremely efficient method, in terms of both computational and memory requirements.", "labels": [], "entities": []}, {"text": "Our experiments show highly competitive results with various state-of-the-art embedding methods on different tasks, including the standard STS benchmark and a subject prediction task, at a fraction of the computational cost.", "labels": [], "entities": [{"text": "subject prediction task", "start_pos": 159, "end_pos": 182, "type": "TASK", "confidence": 0.7336365977923075}]}], "introductionContent": [{"text": "Modern information systems rely extensively on the embedding of words and documents in compact, semantically meaningful vector spaces, where semantic similarity/relatedness can be computed and used efficiently.", "labels": [], "entities": []}, {"text": "Various embedding methods are essentially all based on the Distributional Hypothesis, and rely on co-occurrence evidence found in a corpus -whether computed globally or in a local context.", "labels": [], "entities": []}, {"text": "The recent success of local context predictive models such as) have initiated the development of more complex and powerful deep learning models ().", "labels": [], "entities": []}, {"text": "The resulting embeddings combine compactness and discriminating ability, but the associated computational requirements are substantial and the optimal hyperparameter settings are not easy to find.", "labels": [], "entities": []}, {"text": "It is, therefore, more common that embeddings are pre-trained on large corpora and plugged into a variety of downstream tasks (sentiment analysis, classification, translation, etc.).", "labels": [], "entities": [{"text": "sentiment analysis, classification, translation", "start_pos": 127, "end_pos": 174, "type": "TASK", "confidence": 0.7038910587628683}]}, {"text": "However, such transfer learning might fail to capture crucial domain-specific semantics.", "labels": [], "entities": []}, {"text": "Revisiting the methods based on global co-occurrence counts, high dimensional spaces built from the raw global co-occurrence counts are normally mapped to a more compact, lower-dimensional space of embeddings, using dimensionality reduction methods such as Principal Component Analysis, Locally Linear Embeddings), and Random Projection.", "labels": [], "entities": [{"text": "Principal Component Analysis", "start_pos": 257, "end_pos": 285, "type": "TASK", "confidence": 0.5922740697860718}]}, {"text": "The latter has the unique advantage of being computationally cheap in the creation of the low-dimensional space, while being a linear projection method for which no optimisation is required attest time.", "labels": [], "entities": []}, {"text": "On the flip side, the lack of optimisation means separation of datapoints is obtained by a comparatively larger increase of the dimensionality, and the linearity of the method further limits how compact the low-dimensional representation can be.", "labels": [], "entities": []}, {"text": "However, thanks to the simplicity of the model, very efficient optimisations can be made to the algorithms and the resulting embeddings can be made to be effective even in high-dimensional spaces.", "labels": [], "entities": []}, {"text": "In our approach, we use a two-step process where we first reduce the dimensionality of the term vectors by an extremely efficient implementation of random projection.", "labels": [], "entities": []}, {"text": "We then project the term vectors on the hyperplane orthogonal to the average language vector, which improves how discriminative the vector representations are, and assign appropriate weights for building document embeddings.", "labels": [], "entities": []}, {"text": "As we show in our experiment section, the resulting method is highly competitive with the state-ofthe-art, in terms of sentence similarity computation and downstream classification task, and has much lower computational space and time requirements.", "labels": [], "entities": [{"text": "sentence similarity computation", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.7819961309432983}, {"text": "downstream classification", "start_pos": 155, "end_pos": 180, "type": "TASK", "confidence": 0.7061488330364227}]}], "datasetContent": [{"text": "We performed three experimental evaluations: in Experiment 1, we compare our method to the state-ofthe-art on the standard STS benchmark; in Experiment 2, we qualitatively evaluate the weights assigned to terms; in Experiment 3, we evaluate our embedding method in terms of subject prediction.", "labels": [], "entities": [{"text": "STS benchmark", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.6472296416759491}, {"text": "subject prediction", "start_pos": 274, "end_pos": 292, "type": "TASK", "confidence": 0.6975995004177094}]}, {"text": "For our experiments, we implemented a parallelised version of the algorithm in C.", "labels": [], "entities": []}, {"text": "All experiments were carried out on the same server with 2 Intel Xeon Silver 4109T 8-core processors and 384GB memory.", "labels": [], "entities": []}, {"text": "For meaningful evaluation, we trained all methods on the same two datasets using publicly available code for the state-of-the-art methods, and compared the resulting models on the standard STS benchmark.", "labels": [], "entities": [{"text": "STS benchmark", "start_pos": 189, "end_pos": 202, "type": "DATASET", "confidence": 0.7622220516204834}]}, {"text": "One dataset is the generic Simple English Wikipedia.", "labels": [], "entities": [{"text": "Simple English Wikipedia", "start_pos": 27, "end_pos": 51, "type": "DATASET", "confidence": 0.778154045343399}]}, {"text": "The other domain-specific one is a subset of the MEDLINE database that consists of 10 6 MEDLINE articles, randomly selected from WorldCat.org.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.9387435913085938}, {"text": "WorldCat.org", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.9765931367874146}]}, {"text": "In the latter dataset, each article is written in English and has a title and an abstract, to ensure sufficient textual information for computing the word embeddings.", "labels": [], "entities": []}, {"text": "This dataset is of interest to our research and provides an interesting use case, as it consists of scientific articles and contains an above-average proportion of technical terms and jargon.", "labels": [], "entities": []}, {"text": "Very rare terms carry critical meaning and make the task of word embedding particularly challenging.", "labels": [], "entities": []}, {"text": "The Semantic Textual Similarity (STS) Benchmark 5 is a SemEval task organized between 2012 and 2017.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) Benchmark", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7746140445981707}, {"text": "SemEval task organized", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8852445483207703}]}, {"text": "It consists of 8628 pairs of English sentences, selected from image captions, news headlines and user forums.", "labels": [], "entities": []}, {"text": "The similarity between these sentence pairs was annotated using a five point scale via crowdsourcing (.", "labels": [], "entities": []}, {"text": "Participating systems calculate the similarity between these sentence pairs and are evaluated based on their Pearson correlation with the gold standard STS annotations.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 109, "end_pos": 128, "type": "METRIC", "confidence": 0.982405960559845}]}, {"text": "We trained our method on the September 2018 datadump of Simple English Wikipedia, where we applied a sliding window of 80 terms, with 50% overlap.", "labels": [], "entities": [{"text": "September 2018 datadump of Simple English Wikipedia", "start_pos": 29, "end_pos": 80, "type": "DATASET", "confidence": 0.8340949160712106}]}, {"text": "This resulted in 628,382 windows, each being considered as a separate document for co-occurrence counting.", "labels": [], "entities": []}, {"text": "A total of 199,430 unique 256-dimensional term vectors were obtained, their weights were calculated (Section 3.4), and used to embed the sentences in the STS benchmark (Section 3.5).", "labels": [], "entities": [{"text": "STS benchmark", "start_pos": 154, "end_pos": 167, "type": "DATASET", "confidence": 0.7724794745445251}]}, {"text": "The results are listed in, together with several state-of-the-art methods on the STS benchmark as published by.) and trained them on the two datasets using the same machine.", "labels": [], "entities": [{"text": "STS benchmark", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.8217589557170868}]}, {"text": "The common hyperparameters were chosen to generally maximise the different methods' performance for this task, and are: a vector size of 256, a minimal number of word occurrences of 10, number of negative samples of 10, window size of 10, using hierarchical softmax, a learning rate of 1.0 and a number of threads of 16.", "labels": [], "entities": []}, {"text": "All the other parameters were kept to their default values.", "labels": [], "entities": []}, {"text": "When trained on the MEDLINE dataset, for word embedding only, it took Word2Vec and GloVe 29 and 35 minutes, respectively.", "labels": [], "entities": [{"text": "MEDLINE dataset", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.9570150375366211}, {"text": "Word2Vec", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9797642230987549}]}, {"text": "As sentencelevel embedding methods, Doc2Vec cost more than 2 hours to train.", "labels": [], "entities": []}, {"text": "FastText and Sent2Vec also required more than 3 hours.", "labels": [], "entities": [{"text": "FastText", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9690483808517456}]}, {"text": "In contrast, our combined term and document embedding -which includes the 10 6 Medline articles and 340\u00d710 3 unique terms -requires only 43 s.", "labels": [], "entities": [{"text": "Medline articles", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.9272666275501251}]}, {"text": "In analysing the STS benchmark results, it is apparent that our method substantially outperforms all baseline methods when trained on the same dataset.", "labels": [], "entities": []}, {"text": "Also notice how the training dataset has a clear impact on each method's performance, and even though the Simple English Wikipedia dataset is more limited, both in vocabulary and in size, than the datasets used for publication by the other methods, our method still outperforms the published results of the other baselines in terms of STS scores, and is very competitive with Sent2Vec.", "labels": [], "entities": [{"text": "Simple English Wikipedia dataset", "start_pos": 106, "end_pos": 138, "type": "DATASET", "confidence": 0.8301199078559875}, {"text": "STS", "start_pos": 335, "end_pos": 338, "type": "METRIC", "confidence": 0.968042254447937}]}, {"text": "All methods suffer a drop of performance on the generic STS benchmark when trained on the MEDLINE dataset, as a consequence of the domain-specific nature of the dataset, but this drop is least pronounced in the case of our method.", "labels": [], "entities": [{"text": "MEDLINE dataset", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.9322333335876465}]}, {"text": "This suggests that a careful search fora more appropriate training set would improve the method's performance even further.", "labels": [], "entities": []}, {"text": "Finally, we should emphasise how low the train times are for our method.", "labels": [], "entities": []}, {"text": "Since we do not require any iterative optimisation of the model parameters, our method's results are deterministically determined by the training data, they do not depend on parameter initialisation, and training is orders of magnitudes faster than the other methods.", "labels": [], "entities": []}, {"text": "At the other end of the spectrum, less frequent terms are likely to carry discriminative information for representing the semantics of the whole documents; however not all equally infrequent words have equally high weights.", "labels": [], "entities": []}, {"text": "For example, \"comprised\" and \"clarify\" have much lower weights than \"cytometry,\" \"spleen,\" \"cox\" and \"embryos\" which are expected to be key topics for documents which contain them.", "labels": [], "entities": []}, {"text": "The orthogonal projection and weighting help to give discriminative terms a boost when calculating the document embedding, no matter how frequently these terms are used.", "labels": [], "entities": []}, {"text": "In addition, we observed that the average cosine similarity between all documents is smaller by orders of magnitude when orthogonal projection and weighting is performed compared to when it is not, suggesting the documents are distributed in more compact clusters.", "labels": [], "entities": []}, {"text": "That being said, without a proper evaluation with domain experts, it is not easy to evaluate the genuine validity of such operation.", "labels": [], "entities": [{"text": "validity", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9466577768325806}]}, {"text": "Our future work will include conducting such user-in-the-loop evaluation.", "labels": [], "entities": []}, {"text": "In most digital library catalogs, bibliographic records are indexed using controlled vocabularies or thesauri to improve the discoverability of the content.", "labels": [], "entities": []}, {"text": "These vocabularies are either generic, such as Library of Congress Subject Headings (LCSH), or domain-specific, such as Medical Subject Headings (MeSH) which is used for indexing articles in the MEDLINE database.", "labels": [], "entities": [{"text": "Medical Subject Headings (MeSH)", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6860667765140533}, {"text": "MEDLINE database", "start_pos": 195, "end_pos": 211, "type": "DATASET", "confidence": 0.9404637515544891}]}, {"text": "Traditionally, assigning a most relevant subset of subject headings to describe a record is done manually by professional taxonomists.", "labels": [], "entities": [{"text": "assigning a most relevant subset of subject headings", "start_pos": 15, "end_pos": 67, "type": "TASK", "confidence": 0.6122413128614426}]}, {"text": "However, such manual assignment is very time-consuming and can no longer keep up with the speed at which new records are produced.", "labels": [], "entities": []}, {"text": "Therefore automatically assigning a set of relevant subjects to articles becomes increasingly important.", "labels": [], "entities": []}, {"text": "We evaluated our embedding method on the use case of subject prediction.", "labels": [], "entities": [{"text": "subject prediction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.720602884888649}]}, {"text": "This remains a difficult problem and is a form of Extreme Multi-label Text Classification (XMTC) (, where the prediction space normally consists of hundreds of thousands to millions of labels and data sparsity and scalability are the major challenges.", "labels": [], "entities": [{"text": "Extreme Multi-label Text Classification (XMTC)", "start_pos": 50, "end_pos": 96, "type": "TASK", "confidence": 0.6874973092760358}]}, {"text": "In our MEDLINE dataset, there are more than 324,619 MeSH headings indexing 896,300 articles (the other articles do not have any subjects) with on average 16 headings per article.", "labels": [], "entities": [{"text": "MEDLINE dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9827748537063599}]}, {"text": "However, only 102,484 MeSH headings are used to index more than 10 articles.", "labels": [], "entities": []}, {"text": "We propose to treat the MeSH headings as terms in the documents they are associated with, so that terms, documents and MeSH headings are all embedded in the same D-dimensional semantic space.", "labels": [], "entities": []}, {"text": "Our assumption is that an article would be indexed by its most related subject headings, i.e., the MeSH headings with the highest cosine similarities to the document itself.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.8391438722610474}]}, {"text": "To evaluate this, we computed embeddings for term and MeSH headings using the training dataset (previously selected 10 6 MEDLINE articles).", "labels": [], "entities": [{"text": "MeSH headings", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.5123151689767838}]}, {"text": "We then prepared a separate testing dataset which contains 10 4 articles randomly selected from WorldCat.org.", "labels": [], "entities": [{"text": "WorldCat.org", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.9845569133758545}]}, {"text": "The articles in the testing dataset all have an abstract and are indexed by at least one MeSH heading.", "labels": [], "entities": [{"text": "MeSH heading", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.880685955286026}]}, {"text": "For each of these articles, we computed the document embedding using the terms in its title and abstract, following Eq.", "labels": [], "entities": []}, {"text": "3. We then computed their most similar MeSH headings and compared them with the actual ones.", "labels": [], "entities": [{"text": "MeSH headings", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.7987284362316132}]}, {"text": "Notice how this method is, therefore, not biased towards predicting the more common (and often less informative) subjects.", "labels": [], "entities": []}, {"text": "For FastText and Sent2Vec, we did the same, i.e., using the document-subject similarities to select the potential candidates.", "labels": [], "entities": []}, {"text": "Since FastText and Sent2Vec can be used to train a supervised text classifier (, we additionally trained a classifier where each article's title and abstract were concatenated as a text, and their actual MeSH subject headings were used as the labels to predict.", "labels": [], "entities": []}, {"text": "We trained a separate FastText and Sent2Vec text classifier, which we used to predict the most likely subjects for the documents in the testing dataset, based on their title and abstract.", "labels": [], "entities": []}, {"text": "The parameters for training a classifier were exactly the same as those for generating word embeddings, but the train time was dramatically shorter, less than 5 minutes with the same machine.", "labels": [], "entities": []}, {"text": "All candidate subjects were ranked, by their similarities to the document or by the probabilities according to the corresponding classifiers, as appropriate.", "labels": [], "entities": []}, {"text": "shows the precision@n and recall@n for different methods/settings.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9981558918952942}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9988237023353577}]}, {"text": "Both FastText and Sent2Vec perform much worse than our method if using document-subject similarities for subject prediction.", "labels": [], "entities": [{"text": "subject prediction", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7110176533460617}]}, {"text": "As multi-label classifiers, their performance are nearly identical to each other and the quality of the predicted subjects are comparable with our similarity-based prediction.", "labels": [], "entities": []}, {"text": "Their precision@n is higher than our method for low values of n while it Recall at n fasttext (similarity) sent2vec (similarity) our method (similarity) fasttext (classification) sent2vec (classification): The performance comparison when predicting subjects quickly decreases to be almost the same as ours.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.99745112657547}]}, {"text": "Up to top 20 candidates, the recall for three methods are more or less the same, but our method is able to predict more actual subjects at lower ranks, where the recall outperforms FastText and Sent2Vec.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9982572197914124}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9923344254493713}]}, {"text": "lists the 23 actual MeSH headings of an example article.", "labels": [], "entities": [{"text": "MeSH headings", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.7197621166706085}]}, {"text": "The MeSH terms that reflect the major points of this article are marked with an asterisk (*).", "labels": [], "entities": [{"text": "MeSH", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.832198441028595}]}, {"text": "The 25 most relevant MeSH headings predicted by three methods are also listed.", "labels": [], "entities": []}, {"text": "It is not surprising that subjects such as \"Humans\" and \"Female\" are predicted first by FastText and Sent2Vec, because they are the most frequent ones used in the training dataset.", "labels": [], "entities": [{"text": "FastText", "start_pos": 88, "end_pos": 96, "type": "DATASET", "confidence": 0.9265597462654114}]}, {"text": "In fact, many of the subjects predicted by these two classifiers are very common (see their document counts in).", "labels": [], "entities": []}, {"text": "These classifiers have trouble finding subjects which describe the articles more precisely, while our method ranks specific subjects such as \"Lens Capsule, Crystalline/Surgery\" high in the list, even though fewer than 100 articles in the training set are indexed by this subject.", "labels": [], "entities": []}, {"text": "We realise that this evaluation has its limitations.", "labels": [], "entities": []}, {"text": "As shown in, highly related MeSH headings such as \"Lenses Intraocular\" and \"Phacoemulsification Methods\" are predicted as good candidates for this article, both of which are reasonable and potentially useful.", "labels": [], "entities": []}, {"text": "But since they are not the subject headings that the professional taxonomists have chosen, their value cannot be easily assessed.", "labels": [], "entities": []}, {"text": "This illustrates how precision/recall may not be a very meaningful evaluation metric in this application.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9990648627281189}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9294717311859131}]}, {"text": "It also shows how this method could provide good recommendations to cataloguers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: STS scores and train times of different methods and different settings.  Method  Dev Test Dataset used Train time  Doc2Vec (PV-DBOW)  72.2 64.9 AP-NEWS  Unknown  31.0 27.0 S.E. Wikipedia 23m  53.7 49.8 MEDLINE  2h3m", "labels": [], "entities": [{"text": "Method  Dev Test Dataset", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.5919962674379349}, {"text": "Train time  Doc2Vec (PV-DBOW)  72.2 64.9 AP-NEWS  Unknown  31.0 27.0 S.E. Wikipedia 23m  53.7 49.8", "start_pos": 113, "end_pos": 211, "type": "DATASET", "confidence": 0.6764099796613058}, {"text": "MEDLINE  2h3m", "start_pos": 212, "end_pos": 225, "type": "METRIC", "confidence": 0.5706814229488373}]}, {"text": " Table 2: STS scores and train times w.r.t. K  K Dev Test Train time(s) #terms  5 73.9 60.5  55s  753,422  10 73.6 58.9  43s  339,729  20 71.5 57.8  37s  183,058  40 69.0 55.2  35s  109,662  80 65.7 52.1  32s  68,461", "labels": [], "entities": [{"text": "STS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8644711971282959}]}, {"text": " Table 3: Examples of term counts and their adjusted weights", "labels": [], "entities": []}]}