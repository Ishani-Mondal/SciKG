{"title": [{"text": "Pitfalls in the Evaluation of Sentence Embeddings", "labels": [], "entities": [{"text": "Evaluation of Sentence Embeddings", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7729264348745346}]}], "abstractContent": [{"text": "Deep learning models continuously break new records across different NLP tasks.", "labels": [], "entities": []}, {"text": "At the same time, their success exposes weaknesses of model evaluation.", "labels": [], "entities": [{"text": "model evaluation", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.6696731597185135}]}, {"text": "Here, we compile several key pitfalls of evaluation of sentence embed-dings, a currently very popular NLP paradigm.", "labels": [], "entities": []}, {"text": "These pitfalls include the comparison of em-beddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks.", "labels": [], "entities": []}, {"text": "Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research.", "labels": [], "entities": []}, {"text": "Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of natural language processing (NLP) is currently in upheaval.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.8119112054506937}]}, {"text": "A reason for this is the success story of deep learning, which has led to ever better reported performances across many different NLP tasks, sometimes exceeding the scores achieved by humans.", "labels": [], "entities": []}, {"text": "These fanfares of victory are echoed by isolated voices raising concern about the trustworthiness of some of the reported results.", "labels": [], "entities": [{"text": "victory", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.8976045250892639}]}, {"text": "For instance, find that neural language models have been misleadingly evaluated and that, under fair conditions, standard LSTMs outperform more recent innovations.", "labels": [], "entities": []}, {"text": "find that reporting single performance scores is insufficient for comparing nondeterministic approaches such as neural networks.", "labels": [], "entities": []}, {"text": "holds that neural MT systems are unfairly compared in the literature using different variants of the BLEU score metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.8369492888450623}, {"text": "BLEU score metric", "start_pos": 101, "end_pos": 118, "type": "METRIC", "confidence": 0.967782735824585}]}, {"text": "In an even more general context, detect several current \"troubling trends\" in machine learning scholarship, some of which refer to evaluation.", "labels": [], "entities": []}, {"text": "Sentence encoders () are one particularly hot deep learning topic.", "labels": [], "entities": [{"text": "Sentence encoders", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8768738508224487}]}, {"text": "Generalizing the popular word-level representations () to the sentence level, they are valuable in a variety of contexts: (i) clustering of sentences and short texts; (ii) retrieval tasks, e.g., retrieving answer passages fora question; and (iii) when task-specific training data is scarce-i.e., when the full potential of task-specific word-level representation approaches cannot be leveraged (.", "labels": [], "entities": [{"text": "clustering of sentences and short texts", "start_pos": 126, "end_pos": 165, "type": "TASK", "confidence": 0.8670872847239176}]}, {"text": "The popularity of sentence encoders has led to a large variety of proposed techniques.", "labels": [], "entities": []}, {"text": "These range from 'complex' unsupervised RNN models predicting context sentences () to supervised RNN models predicting semantic relationships between sentence pairs (.", "labels": [], "entities": [{"text": "predicting semantic relationships between sentence pairs", "start_pos": 108, "end_pos": 164, "type": "TASK", "confidence": 0.8105575640996298}]}, {"text": "Even more complex models learn sentence embeddings in a multi-task setup.", "labels": [], "entities": []}, {"text": "In contrast, 'simple' encoders compute sentence embeddings as an elementary function of word embeddings.", "labels": [], "entities": []}, {"text": "They compute a weighted average of word embeddings and then modify these representations via principal component analysis (SIF) (; average n-gram embeddings (Sent2Vec) (; consider generalized pooling mechanisms; or combine word embeddings via randomly initialized projection matrices.", "labels": [], "entities": []}, {"text": "The embeddings of different encoders vary across various dimensions, the most obvious being their size.", "labels": [], "entities": []}, {"text": "E.g., the literature has proposed embeddings ranging from 300d average word embeddings to 700d n-gram embeddings, to 4096d InferSent embeddings, to 24k dimensional random embeddings (.", "labels": [], "entities": []}, {"text": "Unsurprisingly, comparing embeddings of different sizes is unfair when size itself is crucially related to performances in downstream tasks, as has been highlighted before).", "labels": [], "entities": []}, {"text": "We compile several pitfalls when evaluating and comparing sentence encoders.", "labels": [], "entities": []}, {"text": "These relate to (i) the embedding sizes, (ii) normalization of embeddings before feeding them to classifiers, and (iii) unsupervised semantic similarity evaluation.", "labels": [], "entities": []}, {"text": "We also discuss (iv) the choice of classifier used on top of sentence embeddings and (v) divergence in performance results which compare downstream tasks and so-called probing tasks ( . Our motivation is to assemble diverse observations from different published works regarding problematic aspects of the emerging field of sentence encoders.", "labels": [], "entities": []}, {"text": "We do so in order to provide future research with an easy-to-access reference about issues that may not (yet) be widely known.", "labels": [], "entities": []}, {"text": "We also want to provide the newcomer to sentence encoders a guide for avoiding pitfalls that even experienced researchers have fallen prey to.", "labels": [], "entities": []}, {"text": "We also recommend best practices, from our viewpoint.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Sentence encoders used in this work, together  with the sizes of the resulting sentence embeddings.", "labels": [], "entities": []}, {"text": " Table 2: Unsupervised cosine similarity + Pearson cor- relation (UCP) on STSBench (test data). \u2206 in pp.", "labels": [], "entities": [{"text": "Pearson cor- relation (UCP)", "start_pos": 43, "end_pos": 70, "type": "METRIC", "confidence": 0.9578605549676078}, {"text": "STSBench (test data)", "start_pos": 74, "end_pos": 94, "type": "DATASET", "confidence": 0.8845795750617981}]}]}