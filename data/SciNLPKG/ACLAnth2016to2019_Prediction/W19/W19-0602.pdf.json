{"title": [{"text": "Visual TTR Modelling Visual Question Answering in Type Theory with Records", "labels": [], "entities": [{"text": "TTR Modelling Visual Question Answering in Type Theory", "start_pos": 7, "end_pos": 61, "type": "TASK", "confidence": 0.782909382134676}]}], "abstractContent": [{"text": "In this paper, I will describe a system that was developed for the task of Visual Question Answering.", "labels": [], "entities": [{"text": "Visual Question Answering", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7205604116121928}]}, {"text": "The system uses the rich type universe of Type Theory with Records (TTR) to model utterances about the image, the image itself, and classifications made relating the outcomes of these two tasks.", "labels": [], "entities": []}, {"text": "At its most basic, the decision of whether any given predicate can be assigned to an object in the image is delegated to a CNN.", "labels": [], "entities": []}, {"text": "Consequently, images can betaken as evidence for propositional judgments.", "labels": [], "entities": []}, {"text": "The end result is a model whose application of perceptual classifiers to a given image is guided by the accompanying utterance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Visual question answering is a recent popular task in the field of computer vision.", "labels": [], "entities": [{"text": "Visual question answering", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6639410257339478}]}, {"text": "However, the extent to which formal linguistics is needed to solve the task has been a point of contention.", "labels": [], "entities": []}, {"text": "This paper details an approach that utilizes both a rule-based approach to parsing utterances about an image and a deep neural model to supply perceptual meaning.", "labels": [], "entities": [{"text": "parsing utterances about an image", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.8312989711761475}]}, {"text": "TTR) offers a powerful semantic framework for modelling natural language.", "labels": [], "entities": []}, {"text": "TTR has been used to model more coarse-grained linguistic phenomena, many of them related to dialogue.", "labels": [], "entities": [{"text": "TTR", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.40626442432403564}]}, {"text": "However, this paper is concerned with relatively basic phenomena.", "labels": [], "entities": []}, {"text": "The challenge here is to model a multimodal world, namely a visual and linguistic one.", "labels": [], "entities": []}, {"text": "This project builds on a previous VQA model using TTR which is detailed in . Both projects utilize pyTTR, a python implementation of TTR.", "labels": [], "entities": []}, {"text": "This previous implementation features a pipeline that includes object recognition in the form of You Only Look Once (YOLO,), representation of the image and question in TTR and, subsequently, evaluation of the utterance with respect to the image.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7200458347797394}, {"text": "TTR", "start_pos": 169, "end_pos": 172, "type": "METRIC", "confidence": 0.6899384260177612}]}, {"text": "The TTR representation of the image consists of a record type that contains an individual variable and bounding box for every detected object, as well as the predicates that apply to them.", "labels": [], "entities": []}, {"text": "Furthermore, it uses the predicate loc to link individual variables to their bounding boxes.", "labels": [], "entities": []}, {"text": "This predicate simply signifies that the individual with this name is located at this position in the image.", "labels": [], "entities": []}, {"text": "I refine the TTR modelling of the image and object classification and replace YOLO with a set of binary word classifiers.", "labels": [], "entities": [{"text": "TTR modelling", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7552286088466644}, {"text": "object classification", "start_pos": 44, "end_pos": 65, "type": "TASK", "confidence": 0.7183815538883209}, {"text": "YOLO", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9767722487449646}]}, {"text": "In Visual TTR, predicates do not need to be added explicitly to the TTR representation of the image.", "labels": [], "entities": []}, {"text": "Instead, links between the image and the question are made where appropriate.", "labels": [], "entities": []}, {"text": "For example, if a question contains a reference to a dog, the system will try to find suitable objects by running the dog classifier on every annotated entity in the image.", "labels": [], "entities": []}, {"text": "If the classifier returns a sufficiently high score for any of the objects, these objects are considered instances of the dog predicate (type).", "labels": [], "entities": []}, {"text": "These technical changes enable a change to the order in which the model performs its sub-tasks.", "labels": [], "entities": []}, {"text": "Where the original system runs an object recognition algorithm on the image and translates the result to a TTR representation of the image, the question is now parsed first, and guides the perceptual classification part of the architecture.", "labels": [], "entities": [{"text": "object recognition", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7137613445520401}]}, {"text": "In section 3, I make recommendations for appropriate training data and classifier design.", "labels": [], "entities": []}, {"text": "Based on the classifier score, likely candidate regions will be considered instances (or witnesses) of the predicate type.", "labels": [], "entities": []}, {"text": "In the case of polar questions, this classified record of the image is a witness of the question type iff the answer to the question is yes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}