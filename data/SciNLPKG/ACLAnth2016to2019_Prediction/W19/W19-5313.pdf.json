{"title": [], "abstractContent": [{"text": "In this paper, we describe our supervised neural machine translation (NMT) systems that we developed for the news translation task for Kazakh\u2194English, Gujarati\u2194English, Chinese\u2194English, and English\u2192Finnish translation directions.", "labels": [], "entities": [{"text": "supervised neural machine translation", "start_pos": 31, "end_pos": 68, "type": "TASK", "confidence": 0.7328266799449921}, {"text": "news translation task", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.807341476281484}, {"text": "English\u2192Finnish translation directions", "start_pos": 190, "end_pos": 228, "type": "TASK", "confidence": 0.6558628916740418}]}, {"text": "We focused on leveraging multilingual transfer learning and back-translation for the extremely low-resource language pairs: Kazakh\u2194English and Gujarati\u2194English translation.", "labels": [], "entities": [{"text": "Gujarati\u2194English translation", "start_pos": 143, "end_pos": 171, "type": "TASK", "confidence": 0.605923593044281}]}, {"text": "For the Chinese\u2194English translation, we used the provided parallel data augmented with a large quantity of back-translated monolingual data to train state-of-the-art NMT systems.", "labels": [], "entities": []}, {"text": "We then employed techniques that have been proven to be most effective, such as back-translation, fine-tuning, and model en-sembling, to generate the primary submissions of Chinese\u2194English.", "labels": [], "entities": []}, {"text": "For English\u2192Finnish, our submission from WMT18 remains a strong baseline despite the increase in parallel corpora for this year's task.", "labels": [], "entities": [{"text": "WMT18", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8463078737258911}]}], "introductionContent": [{"text": "Neural machine translation (NMT) ( has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PB-SMT) (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7656561980644861}, {"text": "word alignments", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7408780455589294}, {"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 226, "end_pos": 279, "type": "TASK", "confidence": 0.6844834131853921}]}, {"text": "NMT performs well in resource-rich scenarios but badly in resource-poor ones (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6213566660881042}]}, {"text": "With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted ().", "labels": [], "entities": []}, {"text": "Furthermore, unsupervised NMT () has enabled * equal contribution translation in a scenario where only monolingual corpora are available.", "labels": [], "entities": []}, {"text": "In this paper, we describe all the systems for Kazakh\u2194English, Gujarati\u2194English, Chinese\u2194English, and English\u2192Finnish, that we developed and submitted for WMT 2019 under the team name \"NICT.\"", "labels": [], "entities": [{"text": "WMT 2019", "start_pos": 155, "end_pos": 163, "type": "TASK", "confidence": 0.5587856769561768}, {"text": "NICT", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.9313802123069763}]}, {"text": "In particular our observations can be summarized as follows: Kazakh\u2192English translation heavily benefits from the existence of Russian as a pivot language in the form of a Russian-Kazakh corpus which can be used to generate a pseudo-parallel Kazakh-English corpus from the Russian-English corpus.", "labels": [], "entities": [{"text": "Kazakh\u2192English translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.7311718165874481}]}, {"text": "Gujarati\u2192English translation can be drastically improved by training a robust Hindi\u2192English model and fine tuning it on the Gujarati-English corpus.", "labels": [], "entities": [{"text": "Gujarati\u2192English translation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6621216461062431}]}, {"text": "Chinese\u2194English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data.", "labels": [], "entities": [{"text": "Chinese\u2194English translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5414702743291855}]}, {"text": "English\u2192Finnish translation generated by our WMT18's NMT system remains a strong baseline despite the availability of larger bilingual corpora for training this year.", "labels": [], "entities": [{"text": "WMT18's NMT system", "start_pos": 45, "end_pos": 63, "type": "DATASET", "confidence": 0.8438983708620071}]}, {"text": "Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations.", "labels": [], "entities": []}, {"text": "Kindly refer to the overview paper ( for additional details about the tasks, comparisons to other submissions, human analyses and insights.", "labels": [], "entities": []}], "datasetContent": [{"text": "The training data for the Chinese\u2194English (ZH\u2194EN) translation tasks consists of two parts: 1) we selected the first 10 million lines of the News Crawl 2016 English corpus according to our last year's finding), 2) the corresponding synthetic data was generated through back-translation (: Results for ZH\u2194EN translation task.", "labels": [], "entities": [{"text": "Chinese\u2194English (ZH\u2194EN) translation", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.4809470540947384}, {"text": "News Crawl 2016 English corpus", "start_pos": 140, "end_pos": 170, "type": "DATASET", "confidence": 0.9570036172866822}, {"text": "ZH\u2194EN translation task", "start_pos": 300, "end_pos": 322, "type": "TASK", "confidence": 0.5742298543453217}]}, {"text": "\"Single model\" denotes that it was trained by only using the first 10M lines of the News Crawl-2016 English corpus as training data.", "labels": [], "entities": [{"text": "News Crawl-2016 English corpus", "start_pos": 84, "end_pos": 114, "type": "DATASET", "confidence": 0.975409060716629}]}, {"text": "These scores are simply copied from the official runs list.", "labels": [], "entities": [{"text": "official runs list", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.6578121682008108}]}, {"text": "et al., 2007) to the English sentences.", "labels": [], "entities": []}, {"text": "Jieba 12 was used to tokenize the Chinese sentence.", "labels": [], "entities": [{"text": "Jieba 12", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8577119708061218}, {"text": "tokenize", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9835188984870911}]}, {"text": "For cleaning, we filtered out sentences longer than 80 tokens in the training data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for Kazakh\u2194English and Gujarati\u2194English tasks. These scores are simply copied from the  official runs list.", "labels": [], "entities": []}, {"text": " Table 2: Results for ZH\u2194EN translation task. \"Single model\" denotes that it was trained by only using the first  10M lines of the News Crawl-2016 English corpus as training data. These scores are simply copied from the  official runs list.", "labels": [], "entities": [{"text": "ZH\u2194EN translation task", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.5545615404844284}, {"text": "News Crawl-2016 English corpus", "start_pos": 131, "end_pos": 161, "type": "DATASET", "confidence": 0.9742114990949631}]}]}