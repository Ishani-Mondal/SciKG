{"title": [{"text": "Linguistic evaluation of German-English Machine Translation using a Test Suite", "labels": [], "entities": [{"text": "German-English Machine Translation", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.529803454875946}]}], "abstractContent": [{"text": "We present the results of the application of a grammatical test suite for German\u2192English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories.", "labels": [], "entities": [{"text": "German\u2192English MT", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.49430421739816666}, {"text": "WMT19", "start_pos": 120, "end_pos": 125, "type": "DATASET", "confidence": 0.918390691280365}]}, {"text": "The systems still translate wrong one out of four test items in average.", "labels": [], "entities": []}, {"text": "Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency.", "labels": [], "entities": []}, {"text": "When compared to last year , there has been a improvement of function words, nonverbal agreement and punctuation.", "labels": [], "entities": []}, {"text": "More detailed conclusions about particular systems and phenomena are also presented.", "labels": [], "entities": []}], "introductionContent": [{"text": "For decades, the development of Machine Translation (MT) has been based on either automatic metrics or human evaluation campaigns with the main focus on producing scores or comparisons (rankings) expressing a generic notion of quality.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.8577966511249542}]}, {"text": "Through the years there have been few examples of more detailed analyses of the translation quality, both automatic (HTER (), Hjerson (Popovi\u00b4cPopovi\u00b4c, 2011)) and human).", "labels": [], "entities": []}, {"text": "Nevertheless, these efforts have not been systematic and they have only focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort.", "labels": [], "entities": []}, {"text": "Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9896639883518219}]}, {"text": "In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites).", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9790982007980347}]}, {"text": "The test suites are assembled in away that allows testing particular issues which are the focus of the evaluation.", "labels": [], "entities": []}, {"text": "The evaluation of the systems is not based on generic text samples, but from the perspective of fulfilling a priori quality requirements.", "labels": [], "entities": []}, {"text": "In this paper we use the DFKI test suite for German\u2192English MT) in order to analyze the performance of the 16 MT Systems that took part at the translation task of the Fourth Conference of Machine Translation.", "labels": [], "entities": [{"text": "DFKI test suite", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.7982301115989685}, {"text": "translation task of the Fourth Conference of Machine Translation", "start_pos": 143, "end_pos": 207, "type": "TASK", "confidence": 0.8662316799163818}]}, {"text": "The evaluation focuses on 107 mostly grammatical phenomena organized in 14 categories.", "labels": [], "entities": []}, {"text": "In order to apply the test suite, we follow a semiautomatic methodology that benefits from regular expressions, followed by minimal human refinement (Section 3).", "labels": [], "entities": []}, {"text": "The application of the suite allows us to form conclusions on the particular grammatical performance of the systems and perform several comparisons (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "In the evaluation presented in the paper, MT outputs are obtained from the 16 systems that are part of the news translation task of the Fourth Conference on Machine Translation (WMT19).", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9938587546348572}, {"text": "news translation task of the Fourth Conference on Machine Translation (WMT19)", "start_pos": 107, "end_pos": 184, "type": "TASK", "confidence": 0.6757392814526191}]}, {"text": "According to the details that the developers have published by the time this paper is written, 10 of the systems are declared to be Neural Machine Translation (NMT) systems and 9 of them confirm that they follow the Transformer paradigm, whereas for the rest 6 systems no details were given.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 132, "end_pos": 164, "type": "TASK", "confidence": 0.8370153705279032}]}, {"text": "For the evaluation of the MT outputs the software TQAutoTest () was used.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9724494814872742}, {"text": "TQAutoTest", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.5808265805244446}]}, {"text": "After processing the MT output for the 5560 items of the test suite, the automatic application of the regular expressions resulted to about 10% warnings.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9107785820960999}]}, {"text": "Consequently, one human annotator (student of linguistics) committed about 70 hours of work in order to reduce the warnings to 3%.", "labels": [], "entities": []}, {"text": "The final results were calculated using 5393 test items, which, after the manual inspection, did not have any warning for any of the respective MT-outputs.", "labels": [], "entities": [{"text": "MT-outputs", "start_pos": 144, "end_pos": 154, "type": "DATASET", "confidence": 0.5477755069732666}]}, {"text": "Since we applied the same test suite as last year, this year's automatic evaluation is profiting from the manual refinement of the regular expressions that took place then.", "labels": [], "entities": []}, {"text": "The first application of the test suite in 2018 resulted in about 10-45% of warnings depending on the system, whereas after this year's application, we only had 8-28%.", "labels": [], "entities": [{"text": "warnings", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9195736050605774}]}, {"text": "This year's results are therefore based on 16% more valid test items, as compared to last year.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Percentage (%) of accuracy improvement or deterioration between WMT18 and WMT19 for all the  systems submitted (averaged in last column) and the systems submitted with the same name", "labels": [], "entities": [{"text": "accuracy improvement", "start_pos": 28, "end_pos": 48, "type": "METRIC", "confidence": 0.98379847407341}, {"text": "WMT18", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8684161901473999}, {"text": "WMT19", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.8191704750061035}]}, {"text": " Table 3: Accuracies of successful translations for 16 systems and 14 categories. Boldface indicates significantly best systems in each row", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.995884358882904}]}]}