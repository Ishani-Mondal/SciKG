{"title": [{"text": "Discourse Relation Prediction: Revisiting Word Pairs with Convolutional Networks", "labels": [], "entities": [{"text": "Discourse Relation Prediction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8131805658340454}]}], "abstractContent": [{"text": "Word pairs across argument spans have been shown to be effective for predicting the discourse relation between them.", "labels": [], "entities": []}, {"text": "We propose an approach to distill knowledge from word pairs for discourse relation classification with convolutional neural networks by incorporating joint learning of implicit and explicit relations.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.6395638485749563}]}, {"text": "Our novel approach of representing the input as word pairs achieves state-of-the-art results on four-way classification of both implicit and explicit relations as well as one of the binary classification tasks.", "labels": [], "entities": []}, {"text": "For explicit relation prediction, we achieve around 20% error reduction on the four-way task.", "labels": [], "entities": [{"text": "explicit relation prediction", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6779785652955373}, {"text": "error reduction", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.9739126563072205}]}, {"text": "At the same time, compared to a two-layered Bi-LSTM-CRF model, our model is able to achieve these results with half the number of learnable parameters and approximately half the amount of training time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Implicit discourse relation identification is the task of recognizing the relationship between text segments without the use of an explicit connective indicating the relationship.", "labels": [], "entities": [{"text": "Implicit discourse relation identification", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8181903213262558}]}, {"text": "For instance, while a connective such as \"because\" may indicate a causal relationship when present between sentences, it is not necessary for causality (as in Example 1).", "labels": [], "entities": []}, {"text": "Without the explicit connective, automatically identifying the relationship is much more difficult.", "labels": [], "entities": []}, {"text": "Improvement in identifying implicit discourse relations will also improve performance in downstream tasks such as question answering, textual inference (for determining relationships between text segments), machine translation and other multi-lingual tasks (for transferring discourse information between languages).", "labels": [], "entities": [{"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8631114661693573}, {"text": "machine translation", "start_pos": 207, "end_pos": 226, "type": "TASK", "confidence": 0.829269140958786}]}, {"text": "The Penn Discourse Tree Bank (PDTB) theory of discourse relations) defines a shallow discourse representation between adjacent or nearby segments.", "labels": [], "entities": [{"text": "Penn Discourse Tree Bank (PDTB) theory", "start_pos": 4, "end_pos": 42, "type": "DATASET", "confidence": 0.9425392374396324}]}, {"text": "As a result, the span of the arguments participating in the discourse relation is often the most important input to a classifier.", "labels": [], "entities": []}, {"text": "Initial approaches used linguistically informed features derived from the arguments as inputs to traditional machine learning methods.", "labels": [], "entities": []}, {"text": "More recently, the application of neural methods has resulted in the best performance on this task, modeling the relationship between words in the arguments in context (.", "labels": [], "entities": []}, {"text": "A common approach in prior work is to use pairs of words from across the arguments as features (.", "labels": [], "entities": []}, {"text": "Consider the example: I am late for the meeting because the train was delayed.", "labels": [], "entities": []}, {"text": "(1) The words \"late\" and \"delayed\" are semantically related and (absent the connective) one might hypothesize that their presence is what triggers a causal relation.", "labels": [], "entities": []}, {"text": "Therefore, pairs of words across discourse arguments should be useful features for identifying discourse relations.", "labels": [], "entities": []}, {"text": "However, learning these specific word pairings requires leveraging large text corpora to observe them in the relevant discourse context.", "labels": [], "entities": []}, {"text": "Furthermore, as the number of possible word pairs grows quadratically with the size of the vocabulary, representing word pairs discretely results in very sparse feature sets.", "labels": [], "entities": []}, {"text": "Since a continuous representation of the word pairs allows for better generalization to unseen pairs, we thus use a Convolutional Neural Network (CNN) to embed word pairs from the arguments in a dense vector representation.", "labels": [], "entities": []}, {"text": "We also extend this idea of word pairs beyond a single pair of words by using larger filter sizes.", "labels": [], "entities": []}, {"text": "Our results show that these word pairs provide improved performance in transferring knowledge from explicit relations, indicating less sensitivity to word ordering.", "labels": [], "entities": [{"text": "transferring knowledge from explicit relations", "start_pos": 71, "end_pos": 117, "type": "TASK", "confidence": 0.8143285155296326}]}, {"text": "Finally, an additional advantage is that our architecture based on convolution layers allows for additional improvement in the speed of training through parallel processing unlike sequential models based on LSTMs.", "labels": [], "entities": []}, {"text": "Our primary contributions are as follows: \u2022 A novel application of convolutional neural networks to model word pairs in the arguments in a discourse relation \u2022 A demonstration that joint learning of implicit and explicit relations with both word pairs and n-grams improves performance over learning implicit relations only \u2022 State-of-the-art results on four-way classification for both implicit and explicit relations, reducing the error by 20% in the latter case \u2022 A model with half the number of learnable parameters compared to a state-of-theart two-layered Bi-LSTM-CRF model along with approximately half the training time", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Spacy to tokenize and annotate POS tags for the individual arguments.", "labels": [], "entities": []}, {"text": "To learn WP-k features, we limit the Cartesian product to a maximum of 500 word pairs per relation.", "labels": [], "entities": []}, {"text": "For n-gram features, we limit the arguments to a maximum of 100 words.", "labels": [], "entities": []}, {"text": "For word embeddings, we used pre-trained embeddings.", "labels": [], "entities": []}, {"text": "However, for words not found in word2vec, we back-off to embeddings trained on the raw WSJ articles.", "labels": [], "entities": [{"text": "WSJ articles", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.9618774652481079}]}, {"text": "We fix the word embeddings during training.", "labels": [], "entities": []}, {"text": "We also concatenate one hot POS embeddings to the fixed word embeddings.", "labels": [], "entities": []}, {"text": "We use 100 and 50 feature maps per filter size for learning WP-k and n-grams respectively.", "labels": [], "entities": []}, {"text": "For WP-k, we use filters of size 2, 4, 6 and 8.", "labels": [], "entities": []}, {"text": "For n-grams, we use filters of size 2, 3, 4 and 5.", "labels": [], "entities": []}, {"text": "For all dense layers and gate layers, we set the output dimension of the weight matrices to 300.", "labels": [], "entities": []}, {"text": "For regularization, we use dropout) of 0.5 after convolution operations and before the softmax layers.", "labels": [], "entities": [{"text": "regularization", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9724897146224976}]}, {"text": "We also use L2 regularization with a coefficient of 0.0001 and early stopping to prevent over-fitting.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 63, "end_pos": 77, "type": "METRIC", "confidence": 0.9224258363246918}]}, {"text": "For training, we minimize multi-class cross-entropy loss using the Adam optimizer () with a learning rate of 0.0005 and batch size of 200.", "labels": [], "entities": []}, {"text": "Our architecture is implemented in Theano deep learning framework.", "labels": [], "entities": [{"text": "Theano deep learning framework", "start_pos": 35, "end_pos": 65, "type": "DATASET", "confidence": 0.9329747408628464}]}], "tableCaptions": [{"text": " Table 2: Results of four-way classification experiments on implicit relations. The numbers in the parenthesis  correspond to average of 10 runs", "labels": [], "entities": []}, {"text": " Table 4: Results of binary classification experiments. The numbers in the parenthesis correspond to average of 10  runs", "labels": [], "entities": [{"text": "binary classification", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7500551640987396}]}, {"text": " Table 5: Ensemble results of four-way classification  experiments. JL : \"Joint Learning\" and IO : \"Implicit  Only\"", "labels": [], "entities": [{"text": "JL", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.6766470074653625}, {"text": "IO", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9763233065605164}]}, {"text": " Table 6: Results of fifteen-way task on CoNLL 2016  test and blind test sets", "labels": [], "entities": [{"text": "CoNLL 2016  test and blind test sets", "start_pos": 41, "end_pos": 77, "type": "DATASET", "confidence": 0.9051691549164909}]}, {"text": " Table 7: Comparison of model complexity. Gate 1 &  Gate 2 have output of size 300. LSTMs have hidden  state of size 300.", "labels": [], "entities": []}]}