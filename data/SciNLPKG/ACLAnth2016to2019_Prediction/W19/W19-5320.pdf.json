{"title": [{"text": "The MLLP-UPV Supervised Machine Translation Systems for WMT19 News Translation Task", "labels": [], "entities": [{"text": "MLLP-UPV Supervised Machine Translation", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.5659754499793053}, {"text": "WMT19 News Translation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7352891763051351}]}], "abstractContent": [{"text": "This paper describes the participation of the MLLP research group of the Universi-tat Polit\u00e8cnica de", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe the supervised Statistical Machine Translation (MT) systems developed by the MLLP research group of the Universitat Polit\u00e8cnica deVa\u00ec encia for the News Translation Shared Task of the ACL 2019 Fourth Conference on Machine Translation (WMT19).", "labels": [], "entities": [{"text": "Statistical Machine Translation (MT)", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.8276657660802206}, {"text": "News Translation Shared Task of the ACL 2019 Fourth Conference on Machine Translation (WMT19)", "start_pos": 174, "end_pos": 267, "type": "TASK", "confidence": 0.8790144845843315}]}, {"text": "For this year's edition, we participated in both directions of the German \u2194 English and German \u2194 French language pairs, using Neural Machine Translation (NMT) models following the Transformer ( architecture.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 126, "end_pos": 158, "type": "TASK", "confidence": 0.8245893319447836}]}, {"text": "Following the lessons learned from last year, we have continued working on data filtering, and we have experimented with additional synthetic data techniques and bigger neural network architectures trained with multi-GPU machines.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.8384855091571808}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the data processing steps (including data filtering and synthetic data generation) carried out prior to system training.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7570460438728333}]}, {"text": "Section 3 describes the architecture and settings used for our NMT models, and the different experiments and evaluations performed are detailed in Section 4.", "labels": [], "entities": []}, {"text": "Our conclusions for this shared task are outlined in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments and evaluation carried out for each of the language directions, with special emphasis placed in the German \u2194 English systems.", "labels": [], "entities": []}, {"text": "For the German \u2194 English systems, we have used newstest2017 as dev set, and newstest2018 as test set.", "labels": [], "entities": []}, {"text": "Additionally, we report results on this year's test set, newstest 2019.", "labels": [], "entities": []}, {"text": "For the German \u2194 French systems, we splitted in half the supplied euelections dev set into two sets, dev 1 and dev 2, and used the former as dev set and the latter as test set.", "labels": [], "entities": []}, {"text": "We also report the results obtained in the official test set newstest2019.", "labels": [], "entities": [{"text": "official test set newstest2019", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8160632997751236}]}, {"text": "We report BLEU scores () computed using SacreBLEU (Post, 2018).", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9712212979793549}, {"text": "SacreBLEU (Post, 2018)", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.8519170085589091}]}, {"text": "shows the results obtained by our systems trained for the German \u2192 English direction.", "labels": [], "entities": [{"text": "German \u2192 English direction", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.4876992106437683}]}, {"text": "As baselines, we take our WMT18 system, trained with 1 GPU (this is the configuration that was used for our WMT18 submission), and the same setup trained with 3 GPUs.", "labels": [], "entities": [{"text": "WMT18", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8044625520706177}, {"text": "WMT18 submission", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.5830245316028595}]}, {"text": "The increase in effective batch size from 3000 to 9000 tokens results in an improvement of 1.7 BLEU in newstest2018 and 2.0 BLEU in newstest2019 without any other change in hyperparameters.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9984915256500244}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9981222748756409}]}], "tableCaptions": [{"text": " Table 1: Evaluation results of German \u2192 English systems", "labels": [], "entities": []}, {"text": " Table 2: Evaluation results of English \u2192 German systems", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results of German \u2192 French sys- tems", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results of French \u2192 German sys- tems", "labels": [], "entities": []}]}