{"title": [], "abstractContent": [{"text": "We present our submission to the IWCS 2019 shared task on semantic parsing, a transition-based parser that uses explicit word-meaning pairings, but no explicit representation of syntax.", "labels": [], "entities": [{"text": "IWCS 2019 shared task", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6937463730573654}, {"text": "semantic parsing", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.6824126094579697}]}, {"text": "Parsing decisions are made based on vector representations of parser states, encoded via stack-LSTMs (Ballesteros et al., 2017), as well as some heuristic rules.", "labels": [], "entities": []}, {"text": "Our system reaches 70.88% f-score in the competition.", "labels": [], "entities": [{"text": "f-score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.991985559463501}]}], "introductionContent": [{"text": "Anyone who complains about arguing over \"semantics\" has never seen how boring it can be to argue over syntax.", "labels": [], "entities": []}, {"text": "-isaacs (@izs).", "labels": [], "entities": []}, {"text": "A spectrum is haunting semantic parsing-the spectrum ranging from traditional semantic grammars on one end to recent sequence-to-sequence methods on the other.", "labels": [], "entities": [{"text": "semantic parsing-the", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7229283154010773}]}, {"text": "Examples of the former include the LKB system for Minimal Recursion Semantics, and Boxer for Discourse Representation Theory (DRT).", "labels": [], "entities": [{"text": "LKB system", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.9240843951702118}, {"text": "Minimal Recursion Semantics", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6671144862969717}, {"text": "Boxer for Discourse Representation Theory (DRT)", "start_pos": 83, "end_pos": 130, "type": "TASK", "confidence": 0.7947385013103485}]}, {"text": "Examples of the latter include van  for Abstract Meaning Representations (AMR) and for DRT.", "labels": [], "entities": [{"text": "Abstract Meaning Representations (AMR)", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.7866726070642471}, {"text": "DRT", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7972306609153748}]}, {"text": "The approach in the present paper aims to occupy a useful middle ground on this spectrum.", "labels": [], "entities": []}, {"text": "On the one hand, we emphasize the usefulness of an explicitly specified lexicon of word-meaning pairs, amenable to tweaking by linguists and engineers, and to interfacing with rule-based components.", "labels": [], "entities": []}, {"text": "On the other hand, we aim to minimize the amount of grammar engineering required, and rely on neural networks to learn to assemble word meanings into sentence meanings.", "labels": [], "entities": []}, {"text": "We describe a system that follows this approach and apply it to the IWCS 2019 shared task on DRS parsing (.", "labels": [], "entities": [{"text": "IWCS 2019 shared task", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.8733788132667542}, {"text": "DRS parsing", "start_pos": 93, "end_pos": 104, "type": "TASK", "confidence": 0.7208617031574249}]}, {"text": "The challenge is to map raw input sentences (plain text, not tokenized or otherwise annotated) to discourse representation structures (DRSs).", "labels": [], "entities": []}, {"text": "DRSs represent meaning as a hierarchy of nested boxes containing referents and conditions.", "labels": [], "entities": []}, {"text": "They can be represented as a flat set of clauses, where referent identity and special conditions encode the structure.", "labels": [], "entities": []}, {"text": "For example, in (top right), all clauses belonging inbox b2 are marked with the b2 prefix, and that the referent e1 is introduced by box b2 is expressed by the special condition b2 REF e1.", "labels": [], "entities": [{"text": "REF", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9455605149269104}]}, {"text": "Our system is inspired by the AMR parser of Ballesteros and Al-Onaizan (2017) and, by extension, the non-projective dependency parsing algorithm of Nivre (2009): it uses a transition sytem to process tokens from left to right, and stack-LSTMs to create vector representations of parser states to make transition decisions.", "labels": [], "entities": []}, {"text": "To apply this approach to DRT, we replace atomic node labels by lexical clause lists (LCLs) and edge labels by sets of referent address pairs (RAPs), which encode decisions to unify specific discourse referents.", "labels": [], "entities": [{"text": "DRT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9129906296730042}]}, {"text": "We also factor the lexicon to address data sparseness and apply various preprocessing and postprocessing steps to ease learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The most frequent bind actions generated from the shared task gold training data.", "labels": [], "entities": []}, {"text": " Table 3: Ablation results on the development data, with one component removed at a time. \"Epochs\"  indicates the number of training epochs needed to reach the indicated f-score.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9955183267593384}, {"text": "Epochs", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9720985889434814}]}]}