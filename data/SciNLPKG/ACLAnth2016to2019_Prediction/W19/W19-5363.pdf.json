{"title": [{"text": "System Description -the submission of FOKUS to the WMT 19 robustness task", "labels": [], "entities": [{"text": "System Description", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6908979415893555}, {"text": "FOKUS", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.524163007736206}, {"text": "WMT 19 robustness", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.7811376055081686}]}], "abstractContent": [{"text": "This paper describes the systems of Fraun-hofer FOKUS for the WMT 2019 machine translation robustness task.", "labels": [], "entities": [{"text": "Fraun-hofer", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.7385072708129883}, {"text": "FOKUS", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.5105078220367432}, {"text": "WMT 2019 machine translation robustness task", "start_pos": 62, "end_pos": 106, "type": "TASK", "confidence": 0.7552739183108012}]}, {"text": "We have made submissions to the EN-FR, FR-EN, and JA-EN language pairs.", "labels": [], "entities": []}, {"text": "The first two were made with a baseline translator, trained on clean data for the WMT 2019 biomedical translation task.", "labels": [], "entities": [{"text": "WMT 2019 biomedical translation task", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.7185054779052734}]}, {"text": "These baselines improved over the baselines from the MTNT paper by 2 to 4 BLEU points, but where not trained on the same data.", "labels": [], "entities": [{"text": "MTNT paper", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.883240282535553}, {"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9990222454071045}]}, {"text": "The last one used the same model class and training procedure, with induced typos in the training data to increase the model robustness.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our submissions to the robustness task () aimed to investigate two questions: a) how robust are well-performing models that are trained on clean text and b) does making small intentional \"typos\" in the training data lead to robust models?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores of our submissions, contrasted  with the results of the same models on the biomedical  translation task, except for JA-EN, where the result on  the closest language pair is given, Chinese to English", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992107152938843}, {"text": "biomedical  translation task", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.7825049062569936}]}]}