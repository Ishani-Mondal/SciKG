{"title": [{"text": "Multiclass Text Classification on Unbalanced, Sparse and Noisy Data", "labels": [], "entities": [{"text": "Multiclass Text Classification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6097905139128367}]}], "abstractContent": [{"text": "This paper discusses methods to improve the performance of text classification on data that is difficult to classify due to a large number of unbalanced classes with noisy examples.", "labels": [], "entities": [{"text": "text classification", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7768738269805908}]}, {"text": "A variety of features are tested, in combination with three different neural-network-based methods with increasing complexity.", "labels": [], "entities": []}, {"text": "The classifiers are applied to a songtext-artist dataset which is large, unbalanced and noisy.", "labels": [], "entities": []}, {"text": "We come to the conclusion that substantial improvement can be obtained by removing unbalancedness and sparsity from the data.", "labels": [], "entities": []}, {"text": "This fulfils a classification task unsatisfactorily-however, with contemporary methods, it is a practical step towards fairly satisfactory results.", "labels": [], "entities": [{"text": "classification task", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8859190344810486}]}], "introductionContent": [{"text": "Text classification tasks are omnipresent in natural language processing (NLP).", "labels": [], "entities": [{"text": "Text classification tasks", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8196695248285929}, {"text": "natural language processing (NLP)", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.7746106485525767}]}, {"text": "Various classifiers may perform better or worse, depending on the data they are given (eg..", "labels": [], "entities": []}, {"text": "However, there is data where one would expect to find a correlation between (vectorised) texts and classes, but the expectation is not met and the classifiers achieve poor results.", "labels": [], "entities": []}, {"text": "One example for such data are songtexts with the corresponding artists being classes.", "labels": [], "entities": []}, {"text": "A classification task on this data is especially hard due to multiple handicaps: First, the number of classes is extraordinarily high (compared to usual text classification tasks).", "labels": [], "entities": [{"text": "classification task", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.8798363208770752}, {"text": "text classification", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7117352187633514}]}, {"text": "Second, the number of samples fora class varies between a handful and more than a hundred.", "labels": [], "entities": []}, {"text": "And third, songtexts are structurally and stylistically more diverse than, e.g., newspaper texts, as they maybe organised in blocks of choruses and verses, exhibit rhyme, make use of slang language etc.", "labels": [], "entities": []}, {"text": "In addition, we try to predict something latent, since there is no direct mapping between artists and their songtexts.", "labels": [], "entities": []}, {"text": "A lot of the texts are not written by the singers themselves, but by professional songwriters.", "labels": [], "entities": []}, {"text": "Hence the correlation that a classifier should capture is between songtexts that the writers think to fit a specific artist and the artist.", "labels": [], "entities": []}, {"text": "All these points make the task difficult; still, it is a task needed for nowadays' NLP systems, e.g., in a framework that suggests new artists to a listener based on the songtexts s/he likes.", "labels": [], "entities": []}, {"text": "Thus, to tackle the challenges given is a helpful step for any task in the field of NLP that might come with similarly difficult data.", "labels": [], "entities": []}, {"text": "For the artist classification, we investigate three neural-network-based methods: a one-layer perceptron, a two-layer Doc2Vec model and a multilayer perceptron.", "labels": [], "entities": [{"text": "artist classification", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7664558291435242}]}, {"text": "(A detailed description of the models shall follow in section 2.)", "labels": [], "entities": []}, {"text": "Besides the model, the representation of the instances in a feature space is important for classification, thus we also aim to find expressive features for the particular domain of songtexts.", "labels": [], "entities": [{"text": "classification", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.960986852645874}]}, {"text": "(See section 4 fora list of our features.)", "labels": [], "entities": []}], "datasetContent": [{"text": "This section lists the concrete parameter settings of the methods described in section 2.", "labels": [], "entities": []}, {"text": "Since our models can only predict classes which have been encountered during training, only the 612 artists occurring in all subsets are kept for all evaluations.", "labels": [], "entities": []}, {"text": "For another series of experiments, only the 40 unique artists with more songs than 140 in the training set are kept to reduce the impact of unbalancedness and sparsity (numbers in.", "labels": [], "entities": []}, {"text": "Each feature group uses multiple stacked layers that are then merged with a concatenation layer.", "labels": [], "entities": []}, {"text": "The sizes of the dense layers are manually selected by trial and error.", "labels": [], "entities": []}, {"text": "Several dropout layers with a constant probability of 0.2 are included.", "labels": [], "entities": []}, {"text": "In contrast, MLP uses only the noun count vectors (as Perceptron) and thus only one input branch.", "labels": [], "entities": [{"text": "MLP", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.6802246570587158}]}, {"text": "For both models, Adadelta, an optimisation See https://deeplearning4j.org/docs/ latest/deeplearning4j-nlp-doc2vec fora quick example.", "labels": [], "entities": []}, {"text": "8 Since deeplearning4j does not document the possibility to get intermediate evaluation results during training, 10 models are trained separately for 10, 20, 30 etc.", "labels": [], "entities": []}, {"text": "epochs to obtain data points for the learning progress analysis.", "labels": [], "entities": []}, {"text": "function with adaptive learning rate, a batch size of 32 and categorical cross-entropy as the loss function are used.", "labels": [], "entities": []}, {"text": "For the activation functions, rectified linear units are used in the hidden layer and softmax in the output layer.", "labels": [], "entities": []}, {"text": "The model trains for 250 epochs and stores the weights that led to the best accuracy on the validation set through a checkpoint mechanism.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9989355206489563}, {"text": "validation", "start_pos": 92, "end_pos": 102, "type": "TASK", "confidence": 0.9554263353347778}]}, {"text": "Since F macro gives every class the same weight, but we deal with an unbalanced dataset, we choose F micro as evaluation measure and only show F macro in some graphs for comparison.", "labels": [], "entities": []}, {"text": "shows the micro-averaged F -score for all models.", "labels": [], "entities": [{"text": "F -score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9719292521476746}]}, {"text": "MLP+ performs best, followed by Perceptron and Doc2Vec.", "labels": [], "entities": [{"text": "MLP+", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9297176003456116}]}, {"text": "The use of additional features significantly decreases the performance of the perceptron (Perceptron+), but increases it for the multi-layer network (MLP+).", "labels": [], "entities": []}, {"text": "This observation is discussed in section 5.4.", "labels": [], "entities": []}, {"text": "show the performance of Perceptron, Doc2Vec and MLP+ in dependence of the number of training epochs.", "labels": [], "entities": [{"text": "Doc2Vec", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9029260277748108}]}, {"text": "The Perceptron shows a generally increasing learning curve, i.e. more epochs lead to better results.", "labels": [], "entities": [{"text": "Perceptron", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8370084762573242}]}, {"text": "Peaks like the one after the 51 st epoch are ignored since the model uses a fixed number of 100 training epochs.", "labels": [], "entities": []}, {"text": "Doc2Vec () reaches its best performance with 20 epochs and does not show any learning progress after that, even if trained for 100 epochs.", "labels": [], "entities": [{"text": "Doc2Vec", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9631190299987793}]}, {"text": "The MLP+ model: Micro-averaged F -score for each model on the test set after training (final) and during training (best), together with the corresponding training epochs.", "labels": [], "entities": [{"text": "MLP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.6721422672271729}, {"text": "F -score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9635729988416036}]}, {"text": "Lower part: only artists with more songs than (MST) 140 are kept in the training and the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of unique artists (classes), num- ber of songtexts and average number of songtexts  per artist (standard deviation in parentheses) for  each dataset.", "labels": [], "entities": [{"text": "num- ber", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.8763492107391357}]}, {"text": " Table 2: Number of unique artists (classes), num- ber of songtexts and average number of songtexts  per artist (standard deviation in parentheses) for  each dataset, keeping only the 40 unique artists  with more songs than 140 in the training set.", "labels": [], "entities": [{"text": "num- ber of songtexts", "start_pos": 46, "end_pos": 67, "type": "METRIC", "confidence": 0.9071687698364258}]}, {"text": " Table 3: Micro-averaged F -score for each model  on the test set after training (final) and during  training (best), together with the corresponding  training epochs. Lower part: only artists with  more songs than (MST) 140 are kept in the train- ing and the test set.", "labels": [], "entities": [{"text": "F -score", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9563107490539551}]}]}