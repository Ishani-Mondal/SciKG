{"title": [{"text": "DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks", "labels": [], "entities": [{"text": "Grounded Response Generation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6953261892000834}]}], "abstractContent": [{"text": "Recent advances in neural sequence-to-sequence models have led to promising results for several language generation-based tasks, including dialogue response generation, summarization, and machine translation.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 139, "end_pos": 167, "type": "TASK", "confidence": 0.8152808547019958}, {"text": "summarization", "start_pos": 169, "end_pos": 182, "type": "TASK", "confidence": 0.987680196762085}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.8271905481815338}]}, {"text": "However, these models are known to have several problems, especially in the context of chitchat based dialogue systems: they tend to generate short and dull responses that are often too generic.", "labels": [], "entities": []}, {"text": "Furthermore, these models do not ground conversational responses on knowledge and facts, resulting in turns that are not accurate, informative and engaging for the users.", "labels": [], "entities": []}, {"text": "In this paper, we propose and experiment with a series of response generation models that aim to serve in the general scenario wherein addition to the dialogue context, relevant unstructured external knowledge in the form of text is also assumed to be available for models to harness.", "labels": [], "entities": []}, {"text": "Our proposed approach extends pointer-generator networks (See et al., 2017) by allowing the decoder to hierarchically attend and copy from external knowledge in addition to the dialogue context.", "labels": [], "entities": []}, {"text": "We empirically show the effectiveness of the proposed model compared to several baselines including (Ghazvininejad et al., 2018; Zhang et al., 2018) through both automatic evaluation metrics and human evaluation on CONVAI2 dataset.", "labels": [], "entities": [{"text": "CONVAI2 dataset", "start_pos": 215, "end_pos": 230, "type": "DATASET", "confidence": 0.9480690062046051}]}], "introductionContent": [{"text": "Recently, deep neural networks have achieved stateof-the-art results in various tasks including computer vision, natural language and speech processing.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7179678231477737}]}, {"text": "Specifically, neural sequence-to-sequence models) have led to great progress in important downstream NLP tasks like text summarization; * Work done while interning at Google AI., machine translation (, and reading comprehension (.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7906808853149414}, {"text": "machine translation", "start_pos": 179, "end_pos": 198, "type": "TASK", "confidence": 0.8465644717216492}]}, {"text": "However, achieving satisfactory performance on dialogue still remains an open problem.", "labels": [], "entities": []}, {"text": "This is because dialogues can have multiple valid responses with varying semantic content.", "labels": [], "entities": []}, {"text": "This is vastly different from the aforementioned tasks, where the generation is more conveniently and uniquely constrained by the input source.", "labels": [], "entities": []}, {"text": "Although neural models appear to generate meaningful responses when trained with sufficiently large datasets in the chit-chat setting, such generic chit-chat models reveal several weaknesses that were reported by previous research.", "labels": [], "entities": []}, {"text": "Most common problems include inconsistency in personality, dull and generic responses, and unawareness of long-term dialogue context.", "labels": [], "entities": []}, {"text": "To alleviate these limitations, we turn our focus on a different problem setting for dialogue response generation where the model is provided a set of relevant textual facts (speaker persona descriptions) and is allowed to harness this knowledge when generating responses in a multi-turn dialogue.", "labels": [], "entities": [{"text": "dialogue response generation", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.8605350653330485}]}, {"text": "To handle the personality inconsistency issue, we ground our dialogue generation model on external knowledge facts which area list of persona descriptions in our application ().", "labels": [], "entities": []}, {"text": "We explicitly use the dialogue history as memory for the model to condition on which potentially encourages a more natural conversation flow.", "labels": [], "entities": []}, {"text": "Towards encouraging generation of more specific and appropriate responses while avoiding generic and dull ones, we use a hierarchical pointer network in our model such that it can copy content from two sources: current dialogue history and persona descriptions.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel and general ar-chitecture DEEPCOPY that extends the attentional sequence-to-sequence model with a hierarchical pointer network that enables the decoder to jointly attend and copy tokens from any of the facts available as external knowledge in addition to the dialogue context (encoder input).", "labels": [], "entities": []}, {"text": "This is achieved entirely in an end-to-end fashion through factoring the whole copy mechanism into following three hierarchies/components: (i) a token-level attention mechanism over the dialogue context to determine the probability of copying a token from the dialogue context, (ii) A hierarchical pointer network to determine the probability of copying a token from each fact, and (iii) An inter-source meta attention over the input sources dialogue context and external knowledge, which combines the two copying probabilities.", "labels": [], "entities": []}, {"text": "Using these components, a single copying probability distribution over the unique tokens appearing in the model input is computed exploiting the well-defined hierarchy among them.", "labels": [], "entities": []}, {"text": "In addition, the model is equipped with a soft switch mechanism between copying and generation modes similar to (, which allows us to softly combine the copying probabilities with the decoder's generation probabilities over a fixed vocabulary into a final output probability distribution over an extended vocabulary.", "labels": [], "entities": []}, {"text": "We empirically show the effectiveness of the proposed DEEPCOPY model compared to several baselines including) on CONVAI2 challenge.", "labels": [], "entities": [{"text": "CONVAI2 challenge", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.7924897372722626}]}], "datasetContent": [{"text": "In this section, we describe the details of dataset, training process, evaluation metrics, and the performance results of DEEPCOPY model in comparison to proposed and existing baselines.", "labels": [], "entities": []}, {"text": "We perform experiments for our problem setup on the recently released CONVAI2 conversational AI challenge dataset, which is an extended version of PERSONACHAT ().", "labels": [], "entities": [{"text": "CONVAI2 conversational AI challenge dataset", "start_pos": 70, "end_pos": 113, "type": "DATASET", "confidence": 0.7288681626319885}]}, {"text": "The conversations in CONVAI2 are obtained by asking a pair of crowdworkers to chat with each other naturally based on their randomly assigned personas (from a set of 1155 personas) towards getting to know each other.", "labels": [], "entities": []}, {"text": "Personas are created by a different set of crowdworkers, and they consist of ~5 natural language sentences, each describing an aspect of a person that can range from common hobbies like \"I like to play basketball\" to very specific facts like \"I have a pet parrot named Tasha\", reflecting a wide range of different personalities.", "labels": [], "entities": []}, {"text": "The dataset contains ~11000 dialogues with ~160000 utterances, and 2000 dialogues with non-overlapping personas are used for validation and test.", "labels": [], "entities": [{"text": "validation", "start_pos": 125, "end_pos": 135, "type": "TASK", "confidence": 0.966510534286499}]}, {"text": "For our setting, we use personas as external knowledge sources that models can ground on while generating responses.", "labels": [], "entities": []}, {"text": "In, we present our results in comparison with the existing and proposed baseline models.", "labels": [], "entities": []}, {"text": "We report the performance of each model across several metrics commonly used for evaluation of text generation models including perplexity, corpus BLEU (), ROUGE-L (),.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9558254480361938}, {"text": "ROUGE-L", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9782527089118958}]}, {"text": "As expected, SEQ2SEQ + BESTFACTRE-SPONSE model and its +COPY version outperform all the other models across all the evaluation metrics.", "labels": [], "entities": [{"text": "BESTFACTRE-SPONSE", "start_pos": 23, "end_pos": 40, "type": "METRIC", "confidence": 0.9568765759468079}, {"text": "COPY", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9925998449325562}]}, {"text": "This model pinpoints the importance of selecting the most suitable fact in the persona for the response to be generated at each turn, justifying our underlying motivation for conducting this experiment as highlighted in Section 3.2.1.", "labels": [], "entities": []}, {"text": "However, the most suitable fact for the response is not available in the real application scenario, where the models are responsible for picking the useful pieces of information pertaining to the current dialogue turn to generate meaningful responses.", "labels": [], "entities": []}, {"text": "Our proposed SEQ2SEQ + BESTFACTCONTEXT model and its +COPY version, on the other hand, are valid baselines for this scenario where the best fact is selected completely based on the dialogue context without relying on the ground-truth response.", "labels": [], "entities": [{"text": "BESTFACTCONTEXT", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9691442251205444}, {"text": "COPY", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9908993244171143}]}, {"text": "This model outperforms the previously proposed memory network based model MEMNET () for knowledge grounded response generation on all the evaluation metrics, demonstrating its effectiveness despite the fact that it does not have access to all the facts unlike ().", "labels": [], "entities": [{"text": "knowledge grounded response generation", "start_pos": 88, "end_pos": 126, "type": "TASK", "confidence": 0.5893828198313713}]}, {"text": "However, this approach has the following potential weaknesses: (i) if the best persona fact selected w.r.t dialogue context is wrong (irrelevant) for the ground-truth response, the generated response might be drastically misinforming, and furthermore it is difficult for model to recover from this error because it has no access to other facts, (ii) selecting the best fact w.r.t dialogue context based: Main results on CONVAI2 dataset.", "labels": [], "entities": [{"text": "CONVAI2 dataset", "start_pos": 420, "end_pos": 435, "type": "DATASET", "confidence": 0.9783304631710052}]}, {"text": "Evaluation metrics on last three columns are better the higher.", "labels": [], "entities": []}, {"text": "Perplexity is lower the better.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9934855103492737}]}, {"text": "The results of the proposed approach are presented in bold.", "labels": [], "entities": []}, {"text": "* indicates that the corresponding model should be considered as a kind of ORACLE because it has access to the fact that is most relevant to the ground-truth response during the inference/test time as defined in Section 3.2.1.", "labels": [], "entities": [{"text": "ORACLE", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9764103889465332}]}, {"text": "\u2020 indicates that the improvement of DEEPCOPY in automatic evaluation metrics over each of the other models (except S2SC-3) is statistically significant with p-value of less than 0.001 on the paired t-test.", "labels": [], "entities": [{"text": "DEEPCOPY", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9921789169311523}]}, {"text": "on tf-idf similarity may result in poor fact selection when the lexical overlap between context and response is small which might be a common case especially for the CONVAI2 dataset as the focus of conversation may often change swiftly across the dialogue turns.", "labels": [], "entities": [{"text": "CONVAI2 dataset", "start_pos": 166, "end_pos": 181, "type": "DATASET", "confidence": 0.9252117276191711}]}, {"text": "The latter might be the reason why copying does not help much for this model since it might end up copying irrelevant tokens in the scenario mentioned above.", "labels": [], "entities": [{"text": "copying", "start_pos": 35, "end_pos": 42, "type": "TASK", "confidence": 0.9669023752212524}]}, {"text": "Our proposed DEEPCOPY model is designed to effectively address the aforementioned issues, where it has access to the entire set of persona facts per dialogue from which it is expected to include the useful pieces of information in the response.", "labels": [], "entities": []}, {"text": "DEEPCOPY model outperforms all the models reported in except for SEQ2SEQ + BEST-CONTEXTRESPONSE models, which we already deem as kind of an upper bound because it has access to the most relevant fact to the response.", "labels": [], "entities": [{"text": "DEEPCOPY", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8251031041145325}, {"text": "BEST-CONTEXTRESPONSE", "start_pos": 75, "end_pos": 95, "type": "METRIC", "confidence": 0.9702629446983337}]}, {"text": "This justifies the effectiveness of DEEPCOPY model compared to the existing works () and the additional baselines we explored in this work.", "labels": [], "entities": []}, {"text": "On the other hand, MULTISEQ2SEQ performs considerably worse than the DEEPCOPY model despite the fact they both have access to the entire set of facts and employ the same encoder-decoder architecture except for the copy mechanism.", "labels": [], "entities": []}, {"text": "This further justifies the effectiveness of incorporating the proposed hierarchical pointer networks in DEEPCOPY because integrating the external knowledge simply by employing multi-source attention as in does not yield to a good solution with competitive results, performing even worse than SEQ2SEQ + NOFACT on 3 of the metrics.", "labels": [], "entities": [{"text": "DEEPCOPY", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.859102189540863}, {"text": "NOFACT", "start_pos": 302, "end_pos": 308, "type": "METRIC", "confidence": 0.6677192449569702}]}, {"text": "Although automatic metrics provide tangible information regarding the performance of the models, we augment them with human evaluations fora more comprehensive analysis of the resulting model generated responses.", "labels": [], "entities": []}, {"text": "Towards this end, we randomly sample 100 examples from test data and ask human raters to evaluate the candidate model generated responses in terms of appropriateness.", "labels": [], "entities": []}, {"text": "Each example is rated by 3 raters, who are shown a dialog history along with a set of persona facts (of the person in turn), and asked to rate each response based on its appropriateness in the dialogue context with a score from 1 (worst) to 5 (best).", "labels": [], "entities": []}, {"text": "In, we present the results of human evaluation under the appropriateness column.", "labels": [], "entities": []}, {"text": "Since each response is rated by 3 different human raters, we report the average rating along with the standard deviation in parenthesis.", "labels": [], "entities": []}, {"text": "We observe that DEEPCOPY outperforms both the existing memorynetwork baselines and the proposed sequence-tosequence baselines on the appropriateness evaluation.", "labels": [], "entities": [{"text": "DEEPCOPY", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9333512187004089}]}, {"text": "It also achieves a performance that is close to the oracle model (S2SC-3), which has a leverage of having an access to the fact that is most relevant to the ground-truth response during the inference time.", "labels": [], "entities": []}, {"text": "Overall, human evaluation of the responses in terms of appropriateness further justifies the promise and effectiveness of our proposed DEEPCOPY model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Main results on CONVAI2 dataset. Evaluation metrics on last three columns are better the higher. Perplexity is lower", "labels": [], "entities": [{"text": "CONVAI2 dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9539583623409271}, {"text": "Perplexity", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.989997386932373}]}, {"text": " Table 2: Lexical diversity and fact inclusion analysis results. Model names are abbreviated according to Table 1. F.Inc denotes", "labels": [], "entities": [{"text": "Lexical diversity", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8812276124954224}, {"text": "fact inclusion analysis", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7992871304353079}, {"text": "F.Inc", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.8058653473854065}]}]}