{"title": [{"text": "Investigating sub-word embedding strategies for the morphologically rich and free phrase-order Hungarian", "labels": [], "entities": []}], "abstractContent": [{"text": "For morphologically rich languages, word em-beddings provide less consistent semantic representations due to higher variance in word forms.", "labels": [], "entities": []}, {"text": "Moreover, these languages often allow for less constrained word order, which further increases variance.", "labels": [], "entities": [{"text": "variance", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9690561294555664}]}, {"text": "For the highly agglu-tinative Hungarian, semantic accuracy of word embeddings measured on word analogy tasks drops by 50-75% compared to English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9679829478263855}, {"text": "word analogy tasks", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7932197550932566}]}, {"text": "We observed that embeddings learn morphosyntax quite well instead.", "labels": [], "entities": []}, {"text": "Therefore, we explore and evaluate several sub-word unit based embedding strategies-character n-grams, lemmatization provided by an NLP-pipeline, and segments obtained in unsupervised learning (morfessor)-to boost semantic consistency in Hungarian word vectors.", "labels": [], "entities": []}, {"text": "The effect of changing embedding dimension and context window size have also been considered.", "labels": [], "entities": []}, {"text": "Morphological analysis based lemmatization was found to be the best strategy to improve embeddings' semantic accuracy , whereas adding character n-grams was found consistently counterproductive in this regard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9691969752311707}]}], "introductionContent": [{"text": "Word embeddings show amazing capabilities in representing semantic relations, which has been demonstrated in analogical reasoning tasks.", "labels": [], "entities": []}, {"text": "They are also capable of learning morphosyntax, showing again a consistent mapping of grammatical operations, i.e. inflections (see Section 2).", "labels": [], "entities": []}, {"text": "Word embeddings obtain such semantic and syntactic capabilities by matching the words to their observed contexts (or vice versa).", "labels": [], "entities": []}, {"text": "Since the size of the word vector table is the vocabulary size times the embedding dimension, for languages with rich morphology (especially agglutinative ones), this results in huge matrices.", "labels": [], "entities": []}, {"text": "The vocabulary needs to be increased for morphologically rich languages to ensure a high enough coverage for the overall occurring words.", "labels": [], "entities": []}, {"text": "Furthermore, to obtain a reliable estimate of word vectors, a larger training corpus is required so that theoretically the same convergence of the estimation can be reached than fora non agglutinative language.", "labels": [], "entities": [{"text": "convergence", "start_pos": 128, "end_pos": 139, "type": "METRIC", "confidence": 0.9619061946868896}]}, {"text": "Finally, morphologically rich languages can express grammatical relations through suffixes (i.e. case endings) and hence let the word order becoming less constrained than in configurational languages.", "labels": [], "entities": []}, {"text": "This can result in higher context variability, which translates again into less accurate estimates (i.e. the effect of migrating words outside the context window can be imagined as a kind of smoothing, making representation more blurred).", "labels": [], "entities": []}, {"text": "Augmenting the size of the context window is not a effective counter-measure, as it will result again in higher variability of the context.", "labels": [], "entities": []}, {"text": "proposes character level enhancement for word embeddings to overcome difficulties caused by unseen or rare words.", "labels": [], "entities": []}, {"text": "It is demonstrated fora large set of languages that adding character n-grams to the embeddings can be a powerful way of generating word vectors for unseen words, and this augments both semantic and syntactic consistency (and accuracy) of the embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.9990963935852051}]}, {"text": "However, tests no highly agglutinative language for their embeddings' syntactic and semantic accuracies with and without n-grams.", "labels": [], "entities": []}, {"text": "We conduct proper evaluation on an analogy set for Hungarian) designed according to the standard, and show that the already weak baseline semantic accuracy consistently decreases when character ngrams are added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9600986838340759}]}, {"text": "On the other hand, embeddings learn the complex Hungarian morphosyntax quite well.", "labels": [], "entities": []}, {"text": "Our ambition in this work is to address these issues emerging from large vocabulary and less constrained word order.", "labels": [], "entities": []}, {"text": "We systematically investigate and analyze sub-word embedding strategies for the very highly agglutinating Hungarian language.", "labels": [], "entities": []}, {"text": "We are basically interested in benchmarking syntactic and semantic accuracies with each of the methods, therefore we are primarily engaged in testing morphological analysis, lemmatization and stemming based alternatives.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Embedding vector trainer parameters.", "labels": [], "entities": []}]}