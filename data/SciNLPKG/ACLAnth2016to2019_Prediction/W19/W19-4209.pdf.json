{"title": [{"text": "Cross-lingual morphological inflection with explicit alignment", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes two related systems for cross-lingual morphological inflection for SIGMORPHON 2019 Shared Task participation.", "labels": [], "entities": [{"text": "cross-lingual morphological inflection", "start_pos": 45, "end_pos": 83, "type": "TASK", "confidence": 0.7649505933125814}, {"text": "SIGMORPHON 2019 Shared Task participation", "start_pos": 88, "end_pos": 129, "type": "TASK", "confidence": 0.6944329977035523}]}, {"text": "Both sets of results submitted to the shared task for evaluation are obtained using a simple approach of predicting transducer actions based on initial alignments on the training set, where cross-lingual transfer is limited to only using the high-resource language data as additional training set.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.7316048741340637}]}, {"text": "The performance of the system does not reach the performance of the top two systems in the competition.", "labels": [], "entities": []}, {"text": "However, we show that results can be improved with further tuning.", "labels": [], "entities": []}, {"text": "We also present further analyses showing that the cross-lingual gain is rather modest.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological inflection generation is the task of generating a word based on its lemma and morphological features.", "labels": [], "entities": [{"text": "Morphological inflection generation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9162482023239136}]}, {"text": "For example, given the German lemma aufgeben 'to give up' and the morphological tags {V.PTCP, PST}, the task is to predict the inflected form aufgegeben (morphological tags are described in.", "labels": [], "entities": []}, {"text": "Traditionally, finitestate methods are used for morphological generation (and analysis).", "labels": [], "entities": [{"text": "morphological generation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.746286004781723}]}, {"text": "Since such systems typically require man-months of expert work, and difficult to maintain and adapt to changes in the language, data driven approaches to inflection generation have recently become popular).", "labels": [], "entities": [{"text": "inflection generation", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.7497225701808929}]}, {"text": "The task is further popularized by the past three SIGMOR-PHON morphological (re)inflection shared tasks (.", "labels": [], "entities": [{"text": "SIGMOR-PHON morphological (re)inflection shared tasks", "start_pos": 50, "end_pos": 103, "type": "TASK", "confidence": 0.6720910519361496}]}, {"text": "The primary focus of the task tackled in this paper, the task 1 of the present SIGMORPHON shared task, is the cross-lingual transfer learning of the inflection generation.", "labels": [], "entities": [{"text": "SIGMORPHON shared task", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.5731221636136373}, {"text": "cross-lingual transfer learning", "start_pos": 110, "end_pos": 141, "type": "TASK", "confidence": 0.7251158654689789}]}, {"text": "The dominant approach to morphological inflection has been sequence-to-sequence neural networks with attention (e.g.,).", "labels": [], "entities": [{"text": "morphological inflection", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8289395272731781}]}, {"text": "Furthermore, there seems to be a shift from soft attention models towards models with monotonic attention, which indicate that the predictions of the decoder benefit most from a (short) window in the output.", "labels": [], "entities": []}, {"text": "Although we do not use an encoder-decoder architecture, the simple systems presented here are similar to hard-monotonic attention models in the sense that they predict the transduction actions based on a window on the input and output.", "labels": [], "entities": []}, {"text": "The method presented here is much simpler, however.", "labels": [], "entities": []}, {"text": "The predictions are not conditioned on any hidden (continuous or discrete) state or variable.", "labels": [], "entities": []}, {"text": "A particular reason of interest for data-driven approaches to morphological inflection generation is to avoid the considerable amount of expert time required for building rule-based finite-state systems.", "labels": [], "entities": [{"text": "morphological inflection generation", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.7839388449986776}]}, {"text": "This is particularly important for lowresource languages, where experts, and maybe even native speakers, are hard to come by.", "labels": [], "entities": []}, {"text": "As past SIGMORPHON shared tasks demonstrated, however, satisfactory results in the morphological inflection task requires relatively large amount of data.", "labels": [], "entities": [{"text": "SIGMORPHON shared tasks", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.8266385396321615}]}, {"text": "The low-resource settings in earlier SIG-MORPHON shared tasks often resulted in much worse accuracy compared to the high-resource settings.", "labels": [], "entities": [{"text": "SIG-MORPHON shared tasks", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.8035326401392618}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9987180233001709}]}, {"text": "A potential solution to this problem, the focus of the current inflection shared task, is cross-lingual or transfer learning, which has been demonstrated to be useful a number of language processing tasks (e.g.,.", "labels": [], "entities": []}, {"text": "In cross-lingual learn-ing, the data or resources that exist fora related language are leveraged to improve the learning in low-resource setting.", "labels": [], "entities": []}, {"text": "The method we use for cross-lingual learning is rather simple.", "labels": [], "entities": []}, {"text": "We only use the (related) high-resource language as additional training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The shared task data used in this study consists of 100 language pairs, which is described in detail in.", "labels": [], "entities": []}, {"text": "Here we only provide a basic overview that is relevant to our discussion below.", "labels": [], "entities": []}, {"text": "All language pairs feature a high-resource training set from the source language, a low-resource training set and a development set, both from the target language.", "labels": [], "entities": []}, {"text": "Number of unique source and target languages are both 44.", "labels": [], "entities": []}, {"text": "The number of source languages fora target language, and number of target languages fora source language differ.", "labels": [], "entities": []}, {"text": "Some languages also appear as both source and target languages in dif-ferent pairs.", "labels": [], "entities": []}, {"text": "Most source languages have 10 000 training instances, with a few exceptions (notably Uzbek with only 1 060 word forms).", "labels": [], "entities": []}, {"text": "The number of training instances for all target languages is 100, with a single exception of Telugu with 61 word forms.", "labels": [], "entities": []}, {"text": "Development set sizes vary more between 50, 100 and 1 000 word forms.", "labels": [], "entities": []}, {"text": "The number of unique lemmas and tag combinations also vary among different training and development sets.", "labels": [], "entities": []}, {"text": "The relation of language pairs also differ.", "labels": [], "entities": []}, {"text": "Most pairs have shared ancestry, ranging from very close (e.g., Turkish-Azeri) to rather far modern relatives (e.g., Russian-Portuguese), or historical relatives (e.g., Polish-Old Church Slavonic).", "labels": [], "entities": []}, {"text": "There are also a few pairs where the relation is rather through geographical contact (e.g., ItalianMaltese).", "labels": [], "entities": [{"text": "ItalianMaltese", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.9654601216316223}]}, {"text": "As noted above, one obstacle for crosslingual learning is the different writing systems used in these languages.", "labels": [], "entities": [{"text": "crosslingual learning", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8604879677295685}]}, {"text": "The data set includes 11 different scripts, and 30 of language pairs do not use the same script.", "labels": [], "entities": []}, {"text": "It should be noted, however, that the use of common script does not necessarily solve all the problems regarding mapping character sequences across languages reliably.", "labels": [], "entities": []}, {"text": "Even when they use the same script, e.g., Latin or Cyrillic, the differences adopted in the writing tradition of each languages may still introduce difficulties.", "labels": [], "entities": []}, {"text": "To overcome the differences in scripts, we transliterated source language data in eight pairs (Bashkir-Azeri, Bashkir-Crimean-Tatar, Bashkir-Tatar, Bashkir-Turkmen, TurkishKazakh, Turkish-Khakas, Uzbek-Kazakh, and Uzbek-Khakas) into the script used by the target language.", "labels": [], "entities": []}, {"text": "The transliteration method used tries to maximize the similarity of the transliterations with the writing system of the target language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Overall results obtained by our systems in  comparison to the official state-of-the-art baseline (Wu  and Cotterell, 2019). The scores are word-form accu- racy and mean edit distance (MED) averaged over all  100 language pairs. The rows marked with asterisk in- dicate post-evaluation scores obtained using the linear  predictor, after fixing a bug and further tuning.", "labels": [], "entities": [{"text": "mean edit distance (MED) averaged", "start_pos": 174, "end_pos": 207, "type": "METRIC", "confidence": 0.8814282161848885}]}]}