{"title": [{"text": "hULMonA ( ): The Universal Language Model in Arabic", "labels": [], "entities": []}], "abstractContent": [{"text": "Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis.", "labels": [], "entities": [{"text": "text classification", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7643742263317108}, {"text": "sentiment analysis", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.9585466980934143}]}, {"text": "The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English.", "labels": [], "entities": [{"text": "transfer learning (TL)", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7953181147575379}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9957685470581055}, {"text": "text classification", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.7605023980140686}]}, {"text": "TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets.", "labels": [], "entities": []}, {"text": "In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English.", "labels": [], "entities": [{"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9679601788520813}]}, {"text": "In this paper, we hypothesize that similar success can be achieved for Arabic.", "labels": [], "entities": []}, {"text": "The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hUL-MonA-meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multilingual BERT can also be used for Arabic.", "labels": [], "entities": [{"text": "BERT", "start_pos": 237, "end_pos": 241, "type": "METRIC", "confidence": 0.9391824007034302}]}, {"text": "We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis.", "labels": [], "entities": [{"text": "ULM", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9763317108154297}, {"text": "Arabic sentiment analysis", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.5889331797758738}]}, {"text": "Experiment results show that the developed hULMonA and multilingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets.", "labels": [], "entities": [{"text": "Arabic Sentiment Analysis", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.5868893365065256}]}], "introductionContent": [{"text": "Transfer learning (TL) with universal language models (ULMs) have recently shown to achieve state of the art accuracy for several natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "Transfer learning (TL", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9209125638008118}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.998309850692749}]}, {"text": "ULMs are trained unsupervised to provide an intrinsic representation of the language using large corpora that do not require annotations.", "labels": [], "entities": []}, {"text": "These models can then be fine-tuned in a supervised mode with much smaller annotated training data to achieve a particular NLP task.", "labels": [], "entities": []}, {"text": "The established success in English with limited data sets makes ULMs an attractive option for Arabic consideration since Arabic has limited amount of annotated resources.", "labels": [], "entities": []}, {"text": "Early language models focused on vector embeddings for words and provided word-level vector representations (, sentence embeddings (, and paragraph embeddings ().", "labels": [], "entities": []}, {"text": "These early models were able to achieve success comparable to models that were trained only on specific tasks.", "labels": [], "entities": []}, {"text": "More recently, the language model representation was extended to cover a broader representation for text.,, and OpenAI GPT () are examples of such new pre-trained language models and which were able to achieve state of the art results in many NLP tasks.", "labels": [], "entities": [{"text": "OpenAI GPT", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.8929563164710999}]}, {"text": "However, in the field of Arabic NLP, such ULMs have not been explored yet.", "labels": [], "entities": [{"text": "Arabic NLP", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.4885557144880295}]}, {"text": "The use of transfer learning in Arabic has been mainly focused on word embedding models ().", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8958941698074341}]}, {"text": "Among the recently, developed ULM models, BERT) built a multilingual language version using 104 languages including Arabic but this model has only been tested on Arabic \"sentence contradiction\" task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9592697620391846}]}, {"text": "One advantage of the multi-lingual BERT is that it can be used for many languages.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.8902919888496399}]}, {"text": "However, one important limitation is that it was constrained to parallel multi-lingual corpora and did not take advantage of much larger corpora set available for Arabic, making its intrinsic representation limited for Arabic.", "labels": [], "entities": []}, {"text": "As a result, there is an opportunity to further improve the potential for ULM success by developing an Arabic specific ULM.", "labels": [], "entities": [{"text": "ULM", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9865504503250122}]}, {"text": "In this paper, we aim at advancing performance and generalization capabilities of Arabic NLP tasks by developing new ULMs for Arabic.", "labels": [], "entities": []}, {"text": "We develop the first Arabic specific ULM model, called hULMonA.", "labels": [], "entities": []}, {"text": "Furthermore, we show how pre-trained multi-lingual BERT can be fine tuned and applied for Arabic classification tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.8971164226531982}, {"text": "Arabic classification tasks", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.7695367832978567}]}, {"text": "We also conduct a benchmark study to evaluate the success potentials for the ULMs with Arabic sentiment analysis.", "labels": [], "entities": []}, {"text": "We consider several datasets in the evaluation and show the superiority of the methods' generalization handling both MSA and dialects.", "labels": [], "entities": []}, {"text": "The results show the superiority of the models compared to state of the art.", "labels": [], "entities": []}, {"text": "We further show that even though the multi-lingual BERT was not trained for dialects, it still achieves state of the art for some of the dialect data sets.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.8764050006866455}]}, {"text": "In summary, our contributions are: 1.", "labels": [], "entities": []}, {"text": "The development of hULMonA, the first Arabic specific ULM, 2.", "labels": [], "entities": []}, {"text": "the fine tuning of multi-lingual BERT ULM for Araic sentiment analysis, and 3.", "labels": [], "entities": [{"text": "BERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9693177938461304}, {"text": "Araic sentiment analysis", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9056891202926636}]}, {"text": "the collection of a benchmark dataset for ULM evaluation with sentiment analysis The rest of the paper is organized as follows: Section 2 provides a survey of previous work in language development for English and Arabic.", "labels": [], "entities": [{"text": "ULM evaluation", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9506209194660187}, {"text": "sentiment analysis", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.854292243719101}]}, {"text": "Section 3 presents a description of the methodologies to develop the targeted ULMs and the description of the benchmark data set.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiment results.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To provide credible evaluation for the performance of the two ULM's, we catalog a benchmark dataset for Arabic which can also be used for future research benchmark evaluations.", "labels": [], "entities": []}, {"text": "The data sets vary in size allowing us to demonstrate the ULM's abilities to fine tune with little data and achieve high performance.", "labels": [], "entities": []}, {"text": "The benchmark data set is summarized in table 1 along with statistics on its content.", "labels": [], "entities": []}, {"text": "In this section, we discuss in detail the experiments that were conducted to evaluate the development of hULMonA, fine-tuning of hULMonA and BERT, and and testing the performance of the models with sentiment analysis.", "labels": [], "entities": [{"text": "BERT", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.9979377388954163}, {"text": "sentiment analysis", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.8514632880687714}]}, {"text": "The benchmark data set was used to fine tune both models and provide different evaluations.", "labels": [], "entities": []}, {"text": "We evaluate our work on four widely-studied Arabic sentiment analysis datasets, with varying numbers of sentences and dialects.", "labels": [], "entities": [{"text": "Arabic sentiment analysis", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6858577032883962}]}, {"text": "All used datasets are described in details in section 3.3, and datasets statistics are shown in table 1.", "labels": [], "entities": []}, {"text": "Following previous works, 20% of the data was held out for testing for some datasets, while other datasets were tested on 10%.", "labels": [], "entities": []}, {"text": "To perform sentiment analysis, we fine-tuned the pretrained ULMs on a target dataset; meaning we  resume training the language model to predict the next token but with a sentiment dataset instead of Wikipedia.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.9508792757987976}]}, {"text": "Fine-tuning improved the model by adapting to new words (e.g., dialects) or words that may convey several meanings.", "labels": [], "entities": []}, {"text": "Fine-tuning was done on each of the data sets in the aforementioned benchmark data separately and utilizing different learning rates for different layers, ranging from 2e-5 to 1e-3.", "labels": [], "entities": []}, {"text": "Finally, after adding a classification layer, the network was trained by unfreezing one layer after each epoch, starting from the output layer.", "labels": [], "entities": []}, {"text": "Results are reported in table 4.", "labels": [], "entities": []}, {"text": "Note that hULMonA outperformed the state-ofthe-art in four Arabic sentiment analysis datasets, demonstrating the benefit of transferring knowledge from a large corpus into small and dialectal datasets.", "labels": [], "entities": [{"text": "Arabic sentiment analysis", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.6564084688822428}]}, {"text": "The results obtained are compared to state-ofthe-art models and presented in.", "labels": [], "entities": []}, {"text": "Eventhough BERT achieved state-of-the-art results on two benchmark datasets, during the evaluation, we noticed that the BERT multilingual tokenizer failed to tokenize Arabic sentences as seen in.", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.877395510673523}, {"text": "BERT", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.7811864018440247}, {"text": "tokenize Arabic sentences", "start_pos": 158, "end_pos": 183, "type": "TASK", "confidence": 0.8494940201441447}]}, {"text": "This tokenizer could have limited the model's accuracy and compromised the model's Arabic pre-training.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9983123540878296}]}], "tableCaptions": [{"text": " Table 2: generating text using the pretrained Arabic language model", "labels": [], "entities": []}, {"text": " Table 4: Comparison of results (F1-Accuracy) obtained using hULMonA and other state-of-the-art models", "labels": [], "entities": [{"text": "F1-Accuracy", "start_pos": 33, "end_pos": 44, "type": "METRIC", "confidence": 0.9989959597587585}]}, {"text": " Table 5: Learning rate and number of epochs used for  training each dataset", "labels": [], "entities": []}]}