{"title": [{"text": "Learning Multilingual Word Embeddings Using Image-Text Data", "labels": [], "entities": []}], "abstractContent": [{"text": "There has been significant interest recently in learning multilingual word embeddings-in which semantically similar words across languages have similar embeddings.", "labels": [], "entities": []}, {"text": "State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings.", "labels": [], "entities": []}, {"text": "In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data.", "labels": [], "entities": []}, {"text": "In particular , we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text.", "labels": [], "entities": []}, {"text": "Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.", "labels": [], "entities": [{"text": "crosslingual semantic similarity tasks", "start_pos": 198, "end_pos": 236, "type": "TASK", "confidence": 0.6900794804096222}]}], "introductionContent": [{"text": "Recent advances in learning distributed representations for words (i.e., word embeddings) have resulted in improvements across numerous natural language understanding tasks ().", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 136, "end_pos": 172, "type": "TASK", "confidence": 0.7489285171031952}]}, {"text": "These methods use unlabeled text corpora to model the semantic content of words using their co-occurring context words.", "labels": [], "entities": []}, {"text": "Key to this is the observation that semantically similar words have similar contexts, thus leading to similar word embeddings.", "labels": [], "entities": []}, {"text": "A limitation of these word embedding approaches is that they only produce monolingual embeddings.", "labels": [], "entities": []}, {"text": "This is because word cooccurrences are very likely to be limited to being within language rather than across language in text corpora.", "labels": [], "entities": []}, {"text": "Hence semantically similar words across languages are unlikely to have similar word embeddings.", "labels": [], "entities": []}, {"text": "To remedy this, there has been recent work on learning multilingual word embeddings, in which semantically similar words within and across languages have similar word embeddings.", "labels": [], "entities": []}, {"text": "Multilingual embeddings are not just interesting as an interlingua between multiple languages; they are useful in many downstream applications.", "labels": [], "entities": []}, {"text": "For example, one application of multilingual embeddings is to find semantically similar words and phrases across languages (.", "labels": [], "entities": []}, {"text": "Another use of multilingual embeddings is in enabling zero-shot learning on unseen languages, just as monolingual word embeddings enable predictions on unseen words.", "labels": [], "entities": []}, {"text": "In other words, a classifier using pretrained multilingual word embeddings can generalize to other languages even if training data is only in English.", "labels": [], "entities": []}, {"text": "Interestingly, multilingual embeddings have also been shown to improve monolingual task performance).", "labels": [], "entities": []}, {"text": "Consequently, multilingual embeddings can be very useful for low-resource languages -they allow us to overcome the scarcity of data in these languages.", "labels": [], "entities": []}, {"text": "However, as detailed in Section 2, most work on learning multilingual word embeddings so far has heavily relied on the availability of expensive resources such as word-aligned / sentencealigned parallel corpora or bilingual lexicons.", "labels": [], "entities": []}, {"text": "Unfortunately, this data can be prohibitively expensive to collect for many languages.", "labels": [], "entities": []}, {"text": "Furthermore even for languages with such data available, the coverage of the data is a limiting factor that restricts how much of the semantic space can be aligned across languages.", "labels": [], "entities": []}, {"text": "Overcoming this data bottleneck is a key contribution of our work.", "labels": [], "entities": []}, {"text": "We investigate the use of cheaply available, weakly-supervised image-text data for learning multilingual embeddings.", "labels": [], "entities": []}, {"text": "Images area rich, language-agnostic medium that can provide abridge across languages.", "labels": [], "entities": []}, {"text": "For example, the English word \"cat\" might be found on webpages containing images of cats.", "labels": [], "entities": []}, {"text": "Similarly, the German word \"katze\" (meaning cat) is likely to be found on other webpages containing similar (or perhaps identical) images of cats.", "labels": [], "entities": []}, {"text": "Thus, images can be used to learn that these words have similar semantic content.", "labels": [], "entities": []}, {"text": "Importantly, image-text data is generally available on the internet even for low-resource languages.", "labels": [], "entities": []}, {"text": "As image data has proliferated on the internet, tools for understanding images have advanced considerably.", "labels": [], "entities": []}, {"text": "Convolutional neural networks (CNNs) have achieved roughly human-level or better performance on vision tasks, particularly classification (.", "labels": [], "entities": [{"text": "classification", "start_pos": 123, "end_pos": 137, "type": "TASK", "confidence": 0.9661481976509094}]}, {"text": "During classification of an image, CNNs compute intermediate outputs that have been used as generic image features that perform well across a variety of vision tasks).", "labels": [], "entities": []}, {"text": "We use these image features to enforce that words associated with similar images have similar embeddings.", "labels": [], "entities": []}, {"text": "Since words associated with similar images are likely to have similar semantic content, even across languages, our learned embeddings capture crosslingual similarity.", "labels": [], "entities": []}, {"text": "There has been other recent work on reducing the amount of supervision required to learn multilingual embeddings (cf. Section 2).", "labels": [], "entities": []}, {"text": "These methods take monolingual embeddings learned using existing methods and align them post-hoc in a shared embedding space.", "labels": [], "entities": []}, {"text": "A limitation with posthoc alignment of monolingual embeddings, first noticed by, is that doing training of monolingual embeddings and alignment separately may lead to worse results than joint training of embeddings in one step.", "labels": [], "entities": []}, {"text": "Since the monolingual embedding objective is distinct from the multilingual embedding objective, monolingual embeddings are not required to capture all information helpful for post-hoc multilingual alignment.", "labels": [], "entities": []}, {"text": "Post-hoc alignment loses out on some information, whereas joint training does not.", "labels": [], "entities": []}, {"text": "observe improved results using a joint training method compared to a similar post-hoc method.", "labels": [], "entities": []}, {"text": "Thus, a joint training approach is desirable.", "labels": [], "entities": []}, {"text": "To our knowledge, no previous method jointly learns multilingual word embeddings using weakly-supervised data available for low-resource languages.", "labels": [], "entities": []}, {"text": "To summarize: In this paper we propose an approach for learning multilingual word embeddings using image-text data jointly across all languages.", "labels": [], "entities": []}, {"text": "We demonstrate that even a bag-of-words based embedding approach achieves performance competitive with the state-of-the-art on crosslingual semantic similarity tasks.", "labels": [], "entities": []}, {"text": "We present experiments for understanding the effect of using pixel data as compared to co-occurrences alone.", "labels": [], "entities": []}, {"text": "We also provide a method for training and making predictions on multilingual word embeddings even when the language of the text is unknown.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our learned multilingual embeddings using six crosslingual semantic similarity tasks, two multilingual document classification tasks, and 13 monolingual semantic similarity tasks.", "labels": [], "entities": [{"text": "document classification", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.7053476274013519}]}, {"text": "We adapt code from and for evaluation.", "labels": [], "entities": []}, {"text": "Crosslingual Semantic Similarity This task measures how well multilingual embeddings capture semantic similarity of words, as judged by human raters.", "labels": [], "entities": []}, {"text": "The task consists of a series of crosslingual word pairs.", "labels": [], "entities": []}, {"text": "For each word pair in the task, human raters judge how semantically similar the words are.", "labels": [], "entities": []}, {"text": "The model also predicts how similar the words are, using the cosine similarity between the embeddings.", "labels": [], "entities": []}, {"text": "The score on the task is the Spearman correlation between the human ratings and the model predictions.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9033287763595581}]}, {"text": "The specific six subtasks we use are part of the Rubenstein-Goodenough dataset and detailed by.", "labels": [], "entities": [{"text": "Rubenstein-Goodenough dataset", "start_pos": 49, "end_pos": 78, "type": "DATASET", "confidence": 0.6928458511829376}]}, {"text": "We also include an additional task aggregating the six subtasks.", "labels": [], "entities": []}, {"text": "Multilingual Document Classification In this task, a classifier built on top of learned multilingual embeddings is trained on the RCV corpus of newswire text as in and.", "labels": [], "entities": [{"text": "Multilingual Document Classification", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7600561777750651}, {"text": "RCV corpus of newswire text", "start_pos": 130, "end_pos": 157, "type": "DATASET", "confidence": 0.9669426083564758}]}, {"text": "The corpus consists of documents in seven languages on four topics, and the classifier predicts the topic.", "labels": [], "entities": []}, {"text": "The score on the task is test accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9905602335929871}]}, {"text": "Note that each document is monolingual, so this task measures performance within languages for multiple languages (as opposed to crosslingual performance).", "labels": [], "entities": []}], "tableCaptions": []}