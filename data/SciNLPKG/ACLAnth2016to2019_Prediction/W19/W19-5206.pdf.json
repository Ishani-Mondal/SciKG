{"title": [], "abstractContent": [{"text": "Recent work in Neural Machine Translation (NMT) has shown significant quality gains from noised-beam decoding during back-translation, a method to generate synthetic parallel data.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.8140731354554495}]}, {"text": "We show that the main role of such synthetic noise is not to diversify the source side, as previously suggested, but simply to indicate to the model that the given source is synthetic.", "labels": [], "entities": []}, {"text": "We propose a simpler alternative to noising techniques, consisting of tagging back-translated source sentences with an extra token.", "labels": [], "entities": []}, {"text": "Our results on WMT outperform noised back-translation in English-Romanian and match performance on English-German, redefining state-of-the-art in the former.", "labels": [], "entities": [{"text": "WMT", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.7343916893005371}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) has made considerable progress in recent years (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8300722440083822}]}, {"text": "Traditional NMT has relied solely on parallel sentence pairs for training data, which can bean expensive and scarce resource.", "labels": [], "entities": []}, {"text": "This motivates the use of monolingual data, usually more abundant).", "labels": [], "entities": []}, {"text": "Approaches using monolingual data for machine translation include language model fusion for both phrasebased ( and neural MT (, backtranslation (, unsupervised machine translation (, dual learning (), and multi-task learning.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7413367927074432}, {"text": "language model fusion", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.6389886736869812}]}, {"text": "We focus on back-translation (BT), which, despite its simplicity, has thus far been the most effective technique (.", "labels": [], "entities": [{"text": "back-translation (BT)", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.6333161219954491}]}, {"text": "Backtranslation entails training an intermediate targetto-source model on genuine bitext, and using this model to translate a large monolingual corpus from the target into the source language.", "labels": [], "entities": []}, {"text": "This allows training a source-to-target model on a mixture of genuine parallel data and synthetic pairs from back-translation.", "labels": [], "entities": []}, {"text": "We build upon and, who investigate BT at the scale of hundreds of millions of sentences.", "labels": [], "entities": [{"text": "BT", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.7130935788154602}]}, {"text": "Their work studies different decoding/generation methods for back-translation: in addition to regular beam search, they consider sampling and adding noise to the one-best hypothesis produced by beam search.", "labels": [], "entities": []}, {"text": "They show that sampled BT and noisedbeam BT significantly outperform standard BT, and attribute this success to increased source-side diversity (sections 5.2 and 4.4).", "labels": [], "entities": []}, {"text": "Our work investigates noised-beam BT (NoisedBT) and questions the role noise is playing.", "labels": [], "entities": [{"text": "noised-beam BT (NoisedBT)", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.7393434047698975}]}, {"text": "Rather than increasing source diversity, our work instead suggests that the performance gains come simply from signaling to the model that the source side is back-translated, allowing it to treat the synthetic parallel data differently than the natural bitext.", "labels": [], "entities": []}, {"text": "We hypothesize that BT introduces both helpful signal (strong target-language signal and weak cross-lingual signal) and harmful signal (amplifying the biases of machine translation).", "labels": [], "entities": [{"text": "BT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.5510206818580627}]}, {"text": "Indicating to the model whether a given training sentence is back-translated should allow the model to separate the helpful and harmful signal.", "labels": [], "entities": []}, {"text": "To support this hypothesis, we first demonstrate that the permutation and word-dropping noise used by do not improve or significantly degrade NMT accuracy, corroborating that noise might act as an indicator that the source is back-translated, without much loss in mutual information between the source and target.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9317708611488342}]}, {"text": "We then train models on WMT English-German (EnDe) without BT noise, and instead explicitly tag the synthetic data with a reserved token.", "labels": [], "entities": [{"text": "WMT English-German (EnDe)", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.8711490154266357}]}, {"text": "We call this technique \"Tagged Back-Translation\" (TaggedBT).", "labels": [], "entities": []}, {"text": "These models achieve equal to slightly higher performance than the noised variants.", "labels": [], "entities": []}, {"text": "We repeat these experiments with WMT English-Romanian (EnRo), where NoisedBT underperforms standard BT and TaggedBT improves over both techniques.", "labels": [], "entities": [{"text": "WMT English-Romanian (EnRo)", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8178598761558533}, {"text": "BT", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.8050915598869324}]}, {"text": "We demonstrate that TaggedBT also allows for effective iterative back-translation with EnRo, a technique which saw quality losses when applied with standard back-translation.", "labels": [], "entities": []}, {"text": "To further our understanding of TaggedBT, we investigate the biases encoded in models by comparing the entropy of their attention matrices, and look at the attention weight on the tag.", "labels": [], "entities": []}, {"text": "We conclude by investigating the effects of the backtranslation tag at decoding time.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents our datasets, evaluation protocols and model architectures.", "labels": [], "entities": []}, {"text": "It also describes our back-translation procedure, as well as noising and tagging strategies.", "labels": [], "entities": []}, {"text": "We rely on BLEU score () as our evaluation metric.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9860458076000214}]}, {"text": "While well established, any slight difference in post-processing and BLEU computation can have a dramatic impact on output values.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9933550357818604}]}, {"text": "For example, Lample and Conneau (2019) report 33.3 BLEU on EnRo using unsupervised NMT, which at first seems comparable to our reported 33.4 SacreBLEU from iterative TaggedBT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9950792789459229}, {"text": "EnRo", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9531726241111755}]}, {"text": "However, when we use their preprocessing scripts and evaluation protocol, our system achieves 39.2 BLEU on the same data, which is close to 6 points higher than the same model evaluated by SacreBLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9837452173233032}]}, {"text": "We therefore report strictly SacreBLEU 1 , using the reference implementation from, which aims to standardize BLEU evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9670646786689758}]}], "tableCaptions": [{"text": " Table 2: SacreBLEU degradation as a function of the  proportion of bitext data that is noised.", "labels": [], "entities": []}, {"text": " Table 3: SacreBLEU on Newstest EnDe for different types of noise, with back-translated data either sampled down  to 24M or using the full set of 216M sentence pairs.", "labels": [], "entities": [{"text": "Newstest EnDe", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.8236546814441681}]}, {"text": " Table 4: Comparing SacreBLEU scores for differ- ent flavors of BT for WMT16 EnRo. Previous  works' scores are reported in italics as they use  detok.multi-bleu instead of SacreBLEU, so are  not guaranteed to be comparable. In this case, how- ever, we do see identical BLEU on our systems when  we score them with detok.multi-bleu, so we be- lieve it to be a fair comparison.", "labels": [], "entities": [{"text": "BT", "start_pos": 64, "end_pos": 66, "type": "DATASET", "confidence": 0.8589171171188354}, {"text": "WMT16 EnRo", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.8652679920196533}, {"text": "BLEU", "start_pos": 269, "end_pos": 273, "type": "METRIC", "confidence": 0.9956244826316833}]}, {"text": " Table 5: Results on WMT15 EnFr, with bitext, BT, NoisedBT, and TaggedBT.", "labels": [], "entities": [{"text": "WMT15 EnFr", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9133053719997406}, {"text": "BT", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.8547149896621704}]}, {"text": " Table 6: Attention sink ratio on the first and last to- ken and entropy (at decoder layer 5) for the models in", "labels": [], "entities": [{"text": "Attention sink ratio", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9806778232256571}]}, {"text": " Table 3.a, averaged over all sentences in newstest14.", "labels": [], "entities": [{"text": "newstest14", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9695215821266174}]}, {"text": " Table 7: Comparing standard decoding with decoding as if the input were back-translated data, meaning that it is  tagged (for the TaggedBT model) or noised (for the NoisedBT model) .", "labels": [], "entities": []}, {"text": " Table 9: Source-target overlap for both back-translated  data with decoding newstest as if it were bitext or BT  data. Model decodes are averaged over newstest2010- newstest2017.", "labels": [], "entities": [{"text": "BT  data", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.814471036195755}]}]}