{"title": [{"text": "GTCOM Neural Machine Translation Systems for WMT19", "labels": [], "entities": [{"text": "GTCOM Neural Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.806513711810112}, {"text": "WMT19", "start_pos": 45, "end_pos": 50, "type": "TASK", "confidence": 0.7902942895889282}]}], "abstractContent": [{"text": "This paper describes the Global Tone Communication Co., Ltd.'s submission of the WMT19 shared news translation task.", "labels": [], "entities": [{"text": "Global Tone Communication Co.", "start_pos": 25, "end_pos": 54, "type": "DATASET", "confidence": 0.7543171048164368}, {"text": "WMT19 shared news translation task", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.688551950454712}]}, {"text": "We participate in six directions: English to (Gujarati, Lithuanian and Finnish) and (Gujarati, Lithuanian and Finnish) to English.", "labels": [], "entities": []}, {"text": "Further, we get the best BLEU scores in the directions of English to Gujarati and Lithuanian to English (28.2 and 36.3 respectively) among all the participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989901185035706}]}, {"text": "The submitted systems mainly focus on back-translation, knowledge distillation and rerank-ing to build a competitive model for this task.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.768144428730011}]}, {"text": "Also, we apply language model to filter mono-lingual data, back-translated data and parallel data.", "labels": [], "entities": []}, {"text": "The techniques we apply for data filtering include filtering by rules, language models.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8222646117210388}]}, {"text": "Besides, We conduct several experiments to validate different knowledge distillation techniques and right-to-left (R2L) reranking.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated in the WMT shared news translation task and focus on the bidirections: English and Gujarati, English and Lithuanian, as well as English and Finnish.", "labels": [], "entities": [{"text": "WMT shared news translation task", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.7283025860786438}]}, {"text": "Our neural machine translation system is developed as transformer () architecture and the toolkit we used is Marian.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.7566934625307719}]}, {"text": "Since BLEU () is the main ranking index for all submitted systems, we apply BLEU as the evaluation matrix for our translation system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9972696900367737}, {"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9980019927024841}]}, {"text": "In addition to data filtering, which is basically the same as the techniques we applied in WMT 2018 last year, we verify different knowledge distillation and reranking techniques to improve the performance of all our systems.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.7563420832157135}, {"text": "WMT 2018 last year", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.9169064313173294}]}, {"text": "For data preprocessing, the basic methods include punctuation normalization, tokenization, truecase and byte pair encoding(BPE)).", "labels": [], "entities": [{"text": "punctuation normalization", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.6400564312934875}]}, {"text": "Besides, human rules and language model are also involved to clean English parallel data, monolingual data and synthetic data.", "labels": [], "entities": []}, {"text": "Regard to the techniques on model training, backtranslation (), knowledge distillation and R2L reranking) are applied to verify whether these techniques could improve the performance of our systems.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6820893287658691}]}, {"text": "In order to explore the application of knowledge distillation technology in the field of neural machine translation, we conduct a number of experiments for sequence-level knowledge distillation and sequence-level interpolation.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.7418943047523499}, {"text": "neural machine translation", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.7254080176353455}, {"text": "sequence-level knowledge distillation", "start_pos": 156, "end_pos": 193, "type": "TASK", "confidence": 0.602138896783193}]}, {"text": "Another, R2L reranking didn't get the better performance in last year experiment.", "labels": [], "entities": []}, {"text": "In order to improve the performance of R2L reranking, we increase the beam size step by step, and explore the effect of any combination for R2L models with every step.", "labels": [], "entities": []}, {"text": "This paper is arranged as follows.", "labels": [], "entities": []}, {"text": "We firstly describe the task and provided data information, then introduce the method of data filtering, mainly in the application of language model.", "labels": [], "entities": [{"text": "data filtering", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.7162172198295593}]}, {"text": "After that, we describe the techniques on transformer architecture and show the conducted experiments in detail of all directions, including data preprocessing, model architecture, back-translation and knowledge distillation.", "labels": [], "entities": [{"text": "transformer architecture", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.8664552271366119}, {"text": "knowledge distillation", "start_pos": 202, "end_pos": 224, "type": "TASK", "confidence": 0.7376431524753571}]}, {"text": "At last, we analyze the results of experiments and draw the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the all experiments we conducted and illustrates how we get the evaluation result step by step.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The main model configuration.", "labels": [], "entities": []}, {"text": " Table 4: The main training parameters.", "labels": [], "entities": []}, {"text": " Table 5: The case-insensitive BLEU score of English  to Lithuanian.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9699963927268982}]}, {"text": " Table 6: The case-insensitive BLEU score of Lithua- nian to English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9720335006713867}]}, {"text": " Table 7: The case-insensitive BLEU score of English  to Gujarati.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.971350371837616}]}, {"text": " Table 8: The case-insensitive BLEU score of Gujarati  to English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9702313542366028}]}, {"text": " Table 9: The case-insensitive BLEU score of English  to Finnish.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9686621427536011}]}, {"text": " Table 10: The case-insensitive BLEU score of Finnish  to English.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9677509367465973}]}]}