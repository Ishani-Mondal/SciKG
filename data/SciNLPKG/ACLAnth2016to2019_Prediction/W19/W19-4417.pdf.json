{"title": [{"text": "The CUED's Grammatical Error Correction Systems for BEA-2019", "labels": [], "entities": [{"text": "CUED", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9097867608070374}, {"text": "BEA-2019", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.46617576479911804}]}], "abstractContent": [{"text": "We describe two entries from the Cambridge University Engineering Department to the BEA 2019 Shared Task on grammatical error correction.", "labels": [], "entities": [{"text": "Cambridge University Engineering Department", "start_pos": 33, "end_pos": 76, "type": "DATASET", "confidence": 0.8708312213420868}, {"text": "BEA 2019 Shared Task", "start_pos": 84, "end_pos": 104, "type": "DATASET", "confidence": 0.8628069311380386}, {"text": "grammatical error correction", "start_pos": 108, "end_pos": 136, "type": "TASK", "confidence": 0.5598628024260203}]}, {"text": "Our submission to the low-resource track is based on prior work on using finite state transducers together with strong neural language models.", "labels": [], "entities": []}, {"text": "Our system for the restricted track is a purely neural system consisting of neural language models and neural machine translation models trained with back-translation and a combination of checkpoint averaging and fine-tuning-without the help of any additional tools like spell checkers.", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 271, "end_pos": 285, "type": "TASK", "confidence": 0.7464886605739594}]}, {"text": "The latter system has been used inside a separate system combination entry in cooperation with the Cambridge University Computer Lab.", "labels": [], "entities": [{"text": "Cambridge University Computer Lab", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.9604341387748718}]}], "introductionContent": [{"text": "The automatic correction of errors in text [In a such situaction \u2192 In such a situation] is receiving more and more attention from the natural language processing community.", "labels": [], "entities": [{"text": "automatic correction of errors in text", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7622356414794922}]}, {"text": "A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task (, the CoNLL-2014 shared task (, and finally the BEA 2019 shared task.", "labels": [], "entities": [{"text": "grammatical error correction (GEC)", "start_pos": 45, "end_pos": 79, "type": "TASK", "confidence": 0.7931911995013555}, {"text": "BEA 2019 shared task", "start_pos": 157, "end_pos": 177, "type": "TASK", "confidence": 0.6433645635843277}]}, {"text": "This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop.", "labels": [], "entities": [{"text": "GEC competition at the BEA 2019 workshop", "start_pos": 105, "end_pos": 145, "type": "DATASET", "confidence": 0.7135673633643559}]}, {"text": "We submitted systems to two different tracks.", "labels": [], "entities": []}, {"text": "The low-resource track did not permit the use of parallel training data except a small development set with around 4K sentence pairs.", "labels": [], "entities": []}, {"text": "For our low-resource system we extended our prior work on finite state transducer based GEC ( ) to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words.", "labels": [], "entities": []}, {"text": "For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences.", "labels": [], "entities": []}, {"text": "Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 90, "end_pos": 118, "type": "TASK", "confidence": 0.7852848172187805}]}, {"text": "We confirm the results of and report substantial gains by applying back-translation () to GEC -a data augmentation technique common in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.7555099427700043}]}, {"text": "Furthermore, we noticed that large parts of the training data do not match the target domain.", "labels": [], "entities": []}, {"text": "We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training.", "labels": [], "entities": []}, {"text": "Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.8153097530206045}]}, {"text": "Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by.", "labels": [], "entities": [{"text": "Cambridge University Computer Lab", "start_pos": 72, "end_pos": 105, "type": "DATASET", "confidence": 0.9562663584947586}]}], "datasetContent": [{"text": "We use neural LMs and neural machine translation (NMT) models in our restricted track entry.: Impact of identity removal on BASE models.", "labels": [], "entities": [{"text": "identity removal", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7273647636175156}]}, {"text": "(i.e. source and target sentences are equal) from the training corpora ().", "labels": [], "entities": []}, {"text": "We note that removing these identity mappings can be seen as measure to control the balance between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9992654919624329}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9938817024230957}]}, {"text": "7, removing identities encourages the model to make more corrections and thus leads to higher recall but lower precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9993757605552673}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9975704550743103}]}, {"text": "It depends on the test set whether this results in an improvement in F 0.5 score.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9881237745285034}]}, {"text": "For the subsequent experiments we found that removing identities in the parallel training corpora but not in the back-translated synthetic data works well in practice.", "labels": [], "entities": []}, {"text": "The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search.", "labels": [], "entities": []}, {"text": "In GEC, this means that the reverse model learns to introduce errors into a correct English sentence.", "labels": [], "entities": [{"text": "GEC", "start_pos": 3, "end_pos": 6, "type": "DATASET", "confidence": 0.7740359306335449}]}, {"text": "Back-translation has been applied successfully to GEC by.", "labels": [], "entities": [{"text": "GEC", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.7913663387298584}]}, {"text": "We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation.", "labels": [], "entities": [{"text": "GEC", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6999313235282898}, {"text": "grammatical error correction", "start_pos": 127, "end_pos": 155, "type": "TASK", "confidence": 0.6288459698359171}, {"text": "machine translation", "start_pos": 160, "end_pos": 179, "type": "TASK", "confidence": 0.7468146085739136}]}, {"text": "Our experiments with back-translation are summarized in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9627933502197266}]}, {"text": "8. Adding 1M synthetic sentences to the training data already yields very substantial gains on both test sets.", "labels": [], "entities": []}, {"text": "We achieve our best results with 5M synthetic sentences (+8.44 on BEA-2019 Dev).", "labels": [], "entities": [{"text": "BEA-2019 Dev", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.6981325447559357}]}, {"text": "In machine translation, it is important to maintain a balance between authentic and synthetic data).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.770042210817337}]}, {"text": "Over-sampling the real data is a common practice to rectify that ratio if large amounts of synthetic data are available.", "labels": [], "entities": []}, {"text": "Interestingly, over-sampling real data in GEC hurts performance (row 3 vs. 5 in Tab.", "labels": [], "entities": [{"text": "GEC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9002159833908081}]}, {"text": "8), and it is possible to mix real and synthetic sentences at a ratio of 1:7.9 (last three rows in Tab. 8).", "labels": [], "entities": [{"text": "Tab. 8", "start_pos": 99, "end_pos": 105, "type": "DATASET", "confidence": 0.9224269688129425}]}, {"text": "We will proceed with the 5M setup for the remainder of this paper.", "labels": [], "entities": []}, {"text": "Fine-tuning As explained previously, we oversample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets.", "labels": [], "entities": [{"text": "W&I+LOCNESS corpus", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.7359224061171213}, {"text": "BEA-2019 dev and test sets", "start_pos": 146, "end_pos": 172, "type": "DATASET", "confidence": 0.9165364503860474}]}, {"text": "To further adapt our system to the target domain, we fine-    tune the NMT models on W&I+LOCNESS after convergence on the full training set.", "labels": [], "entities": [{"text": "LOCNESS", "start_pos": 89, "end_pos": 96, "type": "METRIC", "confidence": 0.46809327602386475}]}, {"text": "We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass.", "labels": [], "entities": [{"text": "W&I+", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.6921952068805695}, {"text": "LOCNESS", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.4366070330142975}]}, {"text": "plots the F 0.5 score on the BEA-2019 dev set for two different setups.", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9708300034205118}, {"text": "BEA-2019 dev set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.9705219070116679}]}, {"text": "For the red curve, we average all checkpoints (Junczys-Dowmunt et al., 2016) (including the last unadapted checkpoint) up to a certain training iteration.", "labels": [], "entities": []}, {"text": "Checkpoints are dumped every 500 steps.", "labels": [], "entities": []}, {"text": "The green curve does not use any checkpoint averaging.", "labels": [], "entities": []}, {"text": "Checkpoint averaging helps to smooth out fluctuations in F 0.5 score, and also generalizes better to CoNLL-2014 (Tab. 9).", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.9810787240664164}, {"text": "CoNLL-2014 (Tab. 9)", "start_pos": 101, "end_pos": 120, "type": "DATASET", "confidence": 0.7949959516525269}]}, {"text": "10 contains our experiments with the BIG configuration.", "labels": [], "entities": [{"text": "BIG", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.5311113595962524}]}, {"text": "In addition to W&I+LOCNESS over-sampling, back-translation with 5M sentences, and fine-tuning with checkpoint averaging, we report further gains by adding the language models from our low-resource system (Sec. 2.2) and ensembling.", "labels": [], "entities": []}, {"text": "Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set (.", "labels": [], "entities": [{"text": "M2", "start_pos": 64, "end_pos": 66, "type": "METRIC", "confidence": 0.8519694805145264}, {"text": "CoNLL-2014", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9159530997276306}]}, {"text": "However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets.", "labels": [], "entities": [{"text": "BEA-2019 dev set", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8822530508041382}, {"text": "CoNLL-2013 or CoNLL-2014 test sets", "start_pos": 91, "end_pos": 125, "type": "DATASET", "confidence": 0.8098191618919373}]}, {"text": "As we argued in Sec.", "labels": [], "entities": []}, {"text": "2.4, our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 201, "end_pos": 211, "type": "DATASET", "confidence": 0.8831272721290588}]}, {"text": "We also note that unlike the system of, our system for the restricted track does not use spell checkers or other NLP tools but relies solely on neural sequence models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: List of tokens R that can be deleted by the  deletion transducer D in Fig. 2.", "labels": [], "entities": []}, {"text": " Table 2: Results on the low-resource track. The \u03bb-parameters are tuned on the BEA-2019 dev set.", "labels": [], "entities": [{"text": "BEA-2019 dev set", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.9050217270851135}]}, {"text": " Table 3: Number of correction types in CoNLL-2014  and BEA-2019 Dev references.", "labels": [], "entities": [{"text": "correction", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.746334969997406}, {"text": "CoNLL-2014", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.9055325388908386}, {"text": "BEA-2019 Dev references", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.8443174759546915}]}, {"text": " Table 4: NMT setups BASE and BIG used in our exper- iments for the restricted track.", "labels": [], "entities": [{"text": "BASE", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9892025589942932}, {"text": "BIG", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9995741248130798}]}, {"text": " Table 5: BEA-2019 parallel training data with and  without removing pairs where source and target sen- tences are the same.", "labels": [], "entities": [{"text": "BEA-2019 parallel training data", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.7919817566871643}]}, {"text": " Table 6: Over-sampling the BEA-2019 in-domain corpus W&I+LOCNESS under BASE models. The second  column contains the ratio of W&I+LOCNESS samples to training samples from the other corpora.", "labels": [], "entities": [{"text": "BEA-2019 in-domain corpus W", "start_pos": 28, "end_pos": 55, "type": "DATASET", "confidence": 0.8369306325912476}, {"text": "BASE", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9662994742393494}]}, {"text": " Table 7: Impact of identity removal on BASE models.", "labels": [], "entities": [{"text": "identity removal", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7119813114404678}, {"text": "BASE", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.5389092564582825}]}, {"text": " Table 8: Using back-translation for GEC (BASE models). The third column contains the ratio between real and  synthetic sentence pairs.", "labels": [], "entities": []}, {"text": " Table 9: Fine-tuning through continued training on W&I+LOCNESS and checkpoint averaging with a BASE model  with 5M back-translated sentences.", "labels": [], "entities": [{"text": "Fine-tuning", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9698309302330017}, {"text": "W&I+LOCNESS", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.554911732673645}, {"text": "checkpoint averaging", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.6138577610254288}, {"text": "BASE", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9916207790374756}]}]}