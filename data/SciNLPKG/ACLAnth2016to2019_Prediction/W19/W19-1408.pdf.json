{"title": [{"text": "Neural Machine Translation Between Myanmar (Burmese) and Rakhine (Arakanese)", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7719588677088419}]}], "abstractContent": [{"text": "This work explores neural machine translation between Myanmar (Burmese) and Rakhine (Arakanese).", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.6833523313204447}]}, {"text": "Rakhine is a language closely related to Myanmar, often considered a dialect.", "labels": [], "entities": []}, {"text": "We implemented three prominent neural machine translation (NMT) systems: recurrent neural networks (RNN), transformer, and con-volutional neural networks (CNN).", "labels": [], "entities": []}, {"text": "The systems were evaluated on a Myanmar-Rakhine parallel text corpus developed by us.", "labels": [], "entities": [{"text": "Myanmar-Rakhine parallel text corpus", "start_pos": 32, "end_pos": 68, "type": "DATASET", "confidence": 0.7385990619659424}]}, {"text": "In addition, two types of word seg-mentation schemes for word embeddings were studied: Word-BPE and Syllable-BPE segmentation.", "labels": [], "entities": [{"text": "Syllable-BPE segmentation", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7878553569316864}]}, {"text": "Our experimental results clearly show that the highest quality NMT and statistical machine translation (SMT) performances are obtained with Syllable-BPE segmentation for both types of translations.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 71, "end_pos": 108, "type": "TASK", "confidence": 0.7447763135035833}]}, {"text": "If we focus on NMT, we find that the transformer with Word-BPE segmentation outperforms CNN and RNN for both Myanmar-Rakhine and Rakhine-Myanmar translation.", "labels": [], "entities": [{"text": "Word-BPE segmentation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.6604427397251129}]}, {"text": "However, CNN with Syllable-BPE segmentation obtains a higher score than the RNN and transformer .", "labels": [], "entities": [{"text": "Syllable-BPE segmentation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6070900708436966}]}], "introductionContent": [{"text": "The Myanmar language includes a number of mutually intelligible Myanmar dialects, with a largely uniform standard dialect used by most Myanmar standard speakers.", "labels": [], "entities": []}, {"text": "Speakers of the standard Myanmar may find the dialects hard to follow.", "labels": [], "entities": []}, {"text": "The alternative phonology, morphology, and regional vocabulary cause some problems in communication.", "labels": [], "entities": []}, {"text": "Machine translation (MT) has so far neglected the importance of properly handling the spelling, lexical, and grammar divergences among language varieties.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8831754088401794}]}, {"text": "In the Republic of the Union of Myanmar, there are many ethnical groups, and dialectal varieties exist within the standard Myanmar language.", "labels": [], "entities": []}, {"text": "To address this problem, we are developing a Myanmar and Rakhine dialectal corpus with monolingual and parallel text.", "labels": [], "entities": [{"text": "Myanmar and Rakhine dialectal corpus", "start_pos": 45, "end_pos": 81, "type": "DATASET", "confidence": 0.6793164312839508}]}, {"text": "We conducted statistical machine translation (SMT) experiments and obtained results similar to previous research (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8058416942755381}]}, {"text": "Deep learning revolution brings rapid and dramatic change to the field of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7690033316612244}]}, {"text": "The main reason for moving from SMT to neural machine translation (NMT) is that it achieved the fluency of translation that was a huge step forward compared with the previous models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9942086935043335}, {"text": "neural machine translation (NMT)", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.8089343011379242}]}, {"text": "Ina trend that carries over from SMT, the strongest NMT systems benefit from subtle architecture modifications and hyperparameter tuning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.985325813293457}]}, {"text": "NMT models have advanced the state of the art by building a single neural network that can learn representations better).", "labels": [], "entities": []}, {"text": "Other authors () conducted experiments with different NMTs for less-resourced and morphologically rich languages, such as Estonian and Russian.", "labels": [], "entities": []}, {"text": "They compared the multi-way model performance to one-way model performance, by using different NMT architectures that allow achieving state-of-the-art translation.", "labels": [], "entities": []}, {"text": "For the multiway model trained using the transformer network architecture, the reported improvement over the baseline methods was +3.27 bilingual evaluation understudy (BLEU) points.", "labels": [], "entities": [{"text": "bilingual evaluation understudy (BLEU)", "start_pos": 136, "end_pos": 174, "type": "METRIC", "confidence": 0.8104897240797678}]}, {"text": "( proposed solutions for the machine translation of a family of dialects, Swiss German, for which parallel corpora are scarcee.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.666455864906311}]}, {"text": "The authors presented three strategies for normalizing Swiss German input to address the regional and spelling diversity.", "labels": [], "entities": [{"text": "normalizing Swiss German input", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.6799291372299194}]}, {"text": "The results show that character-based neural machine translation was the most promising strategy for text normalization and that in combination with phrase-based statistical machine translation it achieved 36% BLEU score.", "labels": [], "entities": [{"text": "character-based neural machine translation", "start_pos": 22, "end_pos": 64, "type": "TASK", "confidence": 0.6256242096424103}, {"text": "text normalization", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8533282279968262}, {"text": "phrase-based statistical machine translation", "start_pos": 149, "end_pos": 193, "type": "TASK", "confidence": 0.6267546713352203}, {"text": "BLEU score", "start_pos": 210, "end_pos": 220, "type": "METRIC", "confidence": 0.9773591458797455}]}, {"text": "In their study, NMT outperformed SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9894497990608215}]}, {"text": "In our study, we performed the first comparative NMT analysis of Myanmar dialectal language with three prominent architectures: recurrent neural network (RNN), convolutional neural network (CNN), and transformer.", "labels": [], "entities": [{"text": "NMT analysis", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.9131719768047333}]}, {"text": "We investigated the translation quality of the corresponding hyper-parameters (batch size, learning rate, cell type, and activation function) in machine translation between the standard Myanmar and national varieties of the same group of languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.761769711971283}]}, {"text": "In addition, we used two types of segmentation schemes: word byte pair encoding (Word-BPE) segmentation and syllable byte pair encoding (Syllable-BPE) segmentation.", "labels": [], "entities": [{"text": "word byte pair encoding (Word-BPE) segmentation", "start_pos": 56, "end_pos": 103, "type": "TASK", "confidence": 0.5556525252759457}]}, {"text": "We compared the performance of this method to SMT and NMT experiments with the RNN, transformer, and CNN.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9766771793365479}]}, {"text": "We found that the transformer with Word-BPE segmentation outperformed both CNN and RNN for both Myanmar-Rakhine and Myanmar-Rakhine translations.", "labels": [], "entities": [{"text": "Word-BPE segmentation", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6967928856611252}, {"text": "CNN", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.9491205215454102}, {"text": "RNN", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.7293920516967773}]}, {"text": "We also found that CNN with Syllable-BPE segmentation obtained a higher score compared with RNN and the transformer.", "labels": [], "entities": [{"text": "Syllable-BPE segmentation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.6328727602958679}]}], "datasetContent": [{"text": "We used automatic criteria to evaluate the machine translation output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.6948168873786926}]}, {"text": "The metric BLEU () measures the adequacy of the translation between language pairs, such as Myanmar and English.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.973249614238739}]}, {"text": "The Higher BLEU scores are better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9993565678596497}]}, {"text": "Before computing BLEU, the translations were decomposed into their constituent syllables to ensure that the results are cross-comparable.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.976995587348938}]}], "tableCaptions": [{"text": " Table 1: BLEU scores of Syllable-BPE segmentation with different batch sizes for three NMT models", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990010857582092}, {"text": "Syllable-BPE segmentation", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.7903150618076324}]}, {"text": " Table 2: BLEU scores of Word-BPE segmentation with different batch sizes for three NMT models", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990776777267456}, {"text": "Word-BPE segmentation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7919265329837799}]}, {"text": " Table 3: BLEU scores for batch size 256 of Syllable-BPE segmentation with different learning rates and  two memory cell types on RNN and the transformer", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981499910354614}, {"text": "Syllable-BPE segmentation", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.7263754904270172}]}, {"text": " Table 4: BLEU scores for batch sizes 128 and 256 of Syllable-BPE segmentation with different learning  rates and two activation functions on CNN", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981265664100647}, {"text": "Syllable-BPE segmentation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.754741758108139}]}, {"text": " Table 5: Comparison of SMT and NMT (top BLEU scores) on two segmentation schemes", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9560181498527527}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9877771139144897}]}]}