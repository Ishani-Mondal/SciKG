{"title": [{"text": "Pay \"Attention\" to Your Context when Classifying Abusive Language", "labels": [], "entities": [{"text": "Classifying Abusive Language", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.771995743115743}]}], "abstractContent": [{"text": "The goal of any social media platform is to facilitate healthy and meaningful interactions among its users.", "labels": [], "entities": []}, {"text": "But more often than not, it has been found that it becomes an avenue for wanton attacks.", "labels": [], "entities": []}, {"text": "We propose an experimental study that has three aims: 1) to provide us with a deeper understanding of current datasets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language and personal attacks); 2) to investigate what type of attention mechanism (con-textual vs. self-attention) is better for abusive language detection using deep learning archi-tectures; and 3) to investigate whether stacked architectures provide an advantage oversimple architectures for this task.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 372, "end_pos": 398, "type": "TASK", "confidence": 0.6744983990987142}]}], "introductionContent": [{"text": "Any social interaction whether in online forums, comment sections or micro-blogging platforms such as Twitter often involves an exchange of ideas or beliefs.", "labels": [], "entities": []}, {"text": "Unfortunately, we often see that users resort to verbal abuse to win an argument or overshadow someone's opinion.", "labels": [], "entities": []}, {"text": "Natural Language Processing (NLP) could aid in the process of detecting and flagging abusive language and thus signaling abusive behaviour online.", "labels": [], "entities": []}, {"text": "This is a particularly challenging task due to the noisiness of user-generated text and the diverse types of abusive language ranging from racism, sexism, and hate speech to harassment and personal attacks.", "labels": [], "entities": []}, {"text": "point out that different types of abusive language can be reduced to two primary factors: 1.", "labels": [], "entities": []}, {"text": "Obama is kinder to islam than any other future western leader is likely to be 2.", "labels": [], "entities": [{"text": "islam", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.6195052862167358}]}, {"text": "you cannot even imagine how i think because i cannot imagine how anyone would take such a vile religion as islam: Tweets where the word \"islam\" is used in two separate contexts: the top tweet is labeled as None while the bottom as Racism ().", "labels": [], "entities": []}, {"text": "\u2022 Is the language directed towards a specific individual or entity or is it directed towards a generalized group?", "labels": [], "entities": []}, {"text": "\u2022 Is the abusive content explicit or implicit?", "labels": [], "entities": []}, {"text": "shows two examples of tweets from the first large-scale Twitter abusive language detection dataset, where the second tweet expresses racism, while the first one does not (.", "labels": [], "entities": [{"text": "Twitter abusive language detection", "start_pos": 56, "end_pos": 90, "type": "TASK", "confidence": 0.6487073749303818}]}, {"text": "The usage of words in a particular context is important in determining the author's intended meaning.", "labels": [], "entities": []}, {"text": "For example, the contexts of the word \"islam\" in the two tweets in are different (a non-racist vs. a racist use of the word, respectively).", "labels": [], "entities": []}, {"text": "Traditional bag-of-words models or simple deep learning models often cannot distinguish and handle such differences.", "labels": [], "entities": []}, {"text": "This motivates us to explore deep learning models that use contextual attention for detecting abusive language and compare their performance against models with selfattention.", "labels": [], "entities": []}, {"text": "We make the following contributions: \u2022 Conduct an empirical study to deepen our understanding of current datasets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language and personal attacks).", "labels": [], "entities": []}, {"text": "Show that our stacked Bidirectional Long Short Term Memory architecture with contextual attention is comparable to or out-performs state of the art approaches on all the existing datasets.", "labels": [], "entities": []}, {"text": "\u2022 Investigate what type of attention mechanism in deep learning architectures (contextual attention vs. self-attention) is better for abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.636499414841334}]}, {"text": "We show that contextual attention models outperform selfattention models on most cases (datasets and architectures), and present a thorough error analysis showing how contextual attention works better than self-attention particularly when it comes to modeling implicit abusive content.", "labels": [], "entities": []}, {"text": "\u2022 Investigate whether stacked architectures are better than simple architectures for abusive language detection when using Biderectional Long Short Term Memory (Bi-LSTM) networks.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.6695531209309896}]}, {"text": "We show that stacked architectures are better than simple architectures on all datasets.", "labels": [], "entities": []}, {"text": "In addition, we discuss the importance of pre-trained word embeddings for deep learning models.", "labels": [], "entities": []}, {"text": "We make the code and all the experimental setups available in https://github.com/ tuhinjubcse/ALW3-ACL2019.", "labels": [], "entities": []}], "datasetContent": [{"text": "Abusive language can be of different types, and previous literature and datasets have focused on some of these types.", "labels": [], "entities": []}, {"text": "Before introducing the existing datasets we use in our study, we provide the definitions for the types of abusive language used in existing work and examples for each type): \u2022 Racism: a belief that race is the primary determinant of human traits and capacities and that racial differences produce an inherent superiority of a particular race.", "labels": [], "entities": []}, {"text": "\u2022 Sexism: prejudice or discrimination based on sex; especially: discrimination against women.", "labels": [], "entities": []}, {"text": "\u2022 Hate Speech: is a language that is used to expresses hatred towards a targeted group or Type Example Racism The only reason the overall numbers increase is because Muslims breed like rats, just like their prophet told them to do.", "labels": [], "entities": []}, {"text": "#Islam Sexism Don't ever let women drive, they'll break your arm!", "labels": [], "entities": []}, {"text": "Hate Speech #westvirginia is full of white trash Offensive Lang I probably wouldnt mind school as much if we didnt have to deal with bitch ass teachers.", "labels": [], "entities": [{"text": "Hate Speech #westvirginia", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.6110200881958008}]}, {"text": "Harassment yes ! whites who do not want to be a minority and will not accept being blended out of existence need to be shot ! #whitegenocide.", "labels": [], "entities": [{"text": "Harassment", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9341191053390503}]}, {"text": "Personal Attack what to do with elitist assholes who do not allow anybody else to edit certain pages?", "labels": [], "entities": [{"text": "Personal Attack", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7269168794155121}]}, {"text": "We must get rid of elitism, Wikipedia is a democracy for the contribution of ideas. is intended to be derogatory, to humiliate, or to insult the members of the group ( ).", "labels": [], "entities": []}, {"text": "\u2022 Offensive Language: is a kind of abuse that causes someone to feel hurt, angry, or upset.", "labels": [], "entities": []}, {"text": "It is usually rude or insulting and often very unpleasant.", "labels": [], "entities": []}, {"text": "\u2022 Harassment: is a type of abuse that is constructed with the identity of sincerely wishing to be part of the group in question, including professing, or conveying pseudosincere intentions, but its real intention(s) is/are to cause disruption and/or to trigger or exacerbate conflict for the purposes of amusement ().", "labels": [], "entities": []}, {"text": "\u2022 Personal Attack: is a type of abuse that usually involves insulting or belittling one's opponent to invalidate his or her argument, but can also involve pointing out factual but ostensible character flaws or actions which are irrelevant to the opponent's argument.", "labels": [], "entities": [{"text": "Personal Attack", "start_pos": 2, "end_pos": 17, "type": "TASK", "confidence": 0.6655360460281372}]}, {"text": "We experiment with four benchmark datasets currently used in the related work on abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6987225015958151}]}, {"text": "Three of them are from Twitter) and the fourth one from Wikipedia (Table 4), and together they showcase all the above mentioned types of abusive language.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.9324817061424255}]}, {"text": "\u2022 D1 ( -This is the first large-scale dataset for abusive tweet detection.", "labels": [], "entities": [{"text": "abusive tweet detection", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6659183998902639}]}, {"text": "Each of the 15, 844 tweets in the dataset is classified into three classes: RACISM, SEXISM, and NONE.", "labels": [], "entities": [{"text": "RACISM", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9238197803497314}, {"text": "SEXISM", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9726362228393555}, {"text": "NONE", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.6347578763961792}]}, {"text": "bootstrapped the corpus collection by performing an initial manual search of common slurs.", "labels": [], "entities": []}, {"text": "(2017) (2017) settled on the following list of search terms (\"#whitegenocide\", \"#fuckniggers\", \"#WhitePower\", \"#WhiteLivesMatter\", \"you fucking nigger\", \"fucking muslim\", \"fucking faggot\", \"religion of hate\", \"the jews\", \"feminist\").", "labels": [], "entities": []}, {"text": "Though it produced a higher rate of tweets from alt-right / white nationalist tweeters, they were willing to accept a corpus that was not necessarily representative of all harassing content in order to achieve higher density.", "labels": [], "entities": []}, {"text": "\u2022 D4 ( shows the class-wise distribution for the three Twitter datasets D1, D2 and D3, respectively.", "labels": [], "entities": [{"text": "Twitter datasets D1", "start_pos": 55, "end_pos": 74, "type": "DATASET", "confidence": 0.8002159893512726}]}], "tableCaptions": [{"text": " Table 3: Statistics of the Twitter datasets (D1, D2, D3).", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 28, "end_pos": 44, "type": "DATASET", "confidence": 0.8849077820777893}]}, {"text": " Table 4: Statistics of the Wikipedia dataset (D4).", "labels": [], "entities": [{"text": "Wikipedia dataset (D4)", "start_pos": 28, "end_pos": 50, "type": "DATASET", "confidence": 0.960451853275299}]}, {"text": " Table 5: Weighted F1 scores on all datasets for all  models.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9521779716014862}]}, {"text": " Table 6: Examples correctly classified by : Context Attention (CA) but mis-classified by Self Attention (SA)", "labels": [], "entities": []}, {"text": " Table 7: F1 scores of RACISM and SEXISM on D1 on  one of the test splits", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9995712637901306}, {"text": "RACISM", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.4554644227027893}, {"text": "SEXISM", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9058880805969238}]}, {"text": " Table 8: F1 scores of OFFENSIVE LANGUAGE (OL)  and HATE SPEECH (HS) and NONE on D2", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9996705055236816}, {"text": "OFFENSIVE LANGUAGE (OL)", "start_pos": 23, "end_pos": 46, "type": "METRIC", "confidence": 0.8850784063339233}, {"text": "HATE SPEECH (HS)", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.890326464176178}, {"text": "NONE", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9895374774932861}]}, {"text": " Table 9: Weighted F1 scores comparing pre-trained  embeddings on the Twitter datasets.", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9798587560653687}, {"text": "Twitter datasets", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8444705009460449}]}, {"text": " Table 10: Cross-datasets training (same CV test splits  of D3)", "labels": [], "entities": []}, {"text": " Table 11: Comparison of our best model with state-of- the-art models on the three Twitter datasets.  \u2020 Results  as reported in the respective papers.", "labels": [], "entities": [{"text": "Twitter datasets", "start_pos": 83, "end_pos": 99, "type": "DATASET", "confidence": 0.7319038212299347}]}, {"text": " Table 12: Comparisons with state-of-the-art models on  D4 DEV and TEST.", "labels": [], "entities": [{"text": "D4 DEV", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.8048734664916992}, {"text": "TEST", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.47250404953956604}]}]}