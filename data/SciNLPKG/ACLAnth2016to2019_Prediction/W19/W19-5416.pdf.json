{"title": [], "abstractContent": [{"text": "For this round of the WMT 2019 APE shared task, our submission focuses on addressing the \"over-correction\" problem in APE.", "labels": [], "entities": [{"text": "WMT 2019 APE shared task", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6788233280181885}, {"text": "APE", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.773166835308075}]}, {"text": "Over-correction occurs when the APE system tends to rephrase an already correct MT output, and the resulting sentence is penalized by a reference-based evaluation against human post-edits.", "labels": [], "entities": []}, {"text": "Our intuition is that this problem can be prevented by informing the system about the predicted quality of the MT output or, in other terms, the expected amount of needed corrections.", "labels": [], "entities": [{"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9716865420341492}]}, {"text": "For this purpose, following the common approach in multilingual NMT, we prepend a special token to the beginning of both the source text and the MT output indicating the required amount of post-editing.", "labels": [], "entities": []}, {"text": "Following the best submissions to the WMT 2018 APE shared task, our backbone architecture is based on multi-source Transformer to encode both the MT output and the corresponding source text.", "labels": [], "entities": [{"text": "WMT 2018 APE shared task", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6274191677570343}]}, {"text": "We participated both in the English-German and English-Russian sub-tasks.", "labels": [], "entities": []}, {"text": "In the first subtask, our best submission improved the original MT output quality up to +0.98 BLEU and-0.47 TER.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8656995892524719}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9967697858810425}, {"text": "TER", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9991983771324158}]}, {"text": "In the second subtask, where the higher quality of the MT output increases the risk of over-correction, none of our submitted runs was able to improve the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9279493093490601}, {"text": "MT", "start_pos": 155, "end_pos": 157, "type": "TASK", "confidence": 0.9734237790107727}]}], "introductionContent": [{"text": "Automatic Post-Editing (APE) is the task of correcting the possible errors in the output of a Machine Translation (MT) system.", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5038350105285645}, {"text": "Machine Translation (MT)", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8058134078979492}]}, {"text": "It is usually considered as a supervised sequence-to-sequence task, which aims to map the output of MT system to a better translation i.e. post-edited output, by leveraging a three-way parallel corpus containing (source text, mt output, post-edited output).", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.929660439491272}]}, {"text": "Considering the MT output as a source sentence and the post-edited output as a target sentence, this problem can be cast as a monolingual translation task and be addressed with different MT solutions (.", "labels": [], "entities": [{"text": "MT output", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.836870402097702}, {"text": "MT", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9541720151901245}]}, {"text": "However, it has been proven that better performance can be obtained by not only using the raw output of the MT system but also by leveraging the source text ().", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.8883289098739624}]}, {"text": "In the last round of the APE shared task (), the top-ranked systems were based on Transformer (, the state-of-the-art architecture in neural MT (NMT), with two encoders to encode both source text and MT output.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8623164097468058}]}, {"text": "Although using these systems to postedit the output of Phrase-Based Statistical Machine Translation (PBSMT) system resulted in a large boost in performance, smaller improvements were observed over neural MT outputs.", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation (PBSMT)", "start_pos": 55, "end_pos": 107, "type": "TASK", "confidence": 0.7042255827358791}]}, {"text": "Indeed, the good performance of the NMT systems leaves less room for improvement and poses the risk of over-correcting the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.8452685475349426}]}, {"text": "Over-correction occurs when the APE system rephrases an already correct MT output.", "labels": [], "entities": []}, {"text": "Although the post-edited output can still be a correct translation, it is penalized in terms of reference-based evaluation metrics, since it differs from the reference post-edited output.", "labels": [], "entities": []}, {"text": "With the steady improvement of NMT technology on the one side, and the adoption of referencebased evaluation metrics that penalizes correct but unnecessary corrections on the other side, tacking this problem has become a priority.", "labels": [], "entities": [{"text": "tacking", "start_pos": 187, "end_pos": 194, "type": "TASK", "confidence": 0.9932889342308044}]}, {"text": "In order to respond to this priority, for this round of the shared task our submission focuses on addressing the over-correction problem.", "labels": [], "entities": []}, {"text": "Over-correction has been already addressed before by integrating Quality Estimation (QE) and APE system in three different ways, namely: i) as an activator, to decide whether to apply postediting or not, using a threshold on the estimated quality of the MT output, ii) as a guidance, to post-edit only the parts of a text that have poor estimated quality, iii) as a selector, to select the best output by comparing the estimated quality of the MT output and the automatically post-edited output.", "labels": [], "entities": [{"text": "APE", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9325219988822937}]}, {"text": "Our approach is a mixture of the first two.", "labels": [], "entities": []}, {"text": "While in all previous scenarios the decision is made externally to the APE system, we allow the APE system to implicitly make the decision and in a softer manner.", "labels": [], "entities": [{"text": "APE", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.7425890564918518}, {"text": "APE", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.7361190319061279}]}, {"text": "Instead of choosing between \"do\" and \"do not\" post-edit, we let the system decide which post-editing strategy to apply, choosing between three strategies: no postediting (i.e. leaving the sentence untouched), light post-editing (i.e. a conservative modification) and heavy post-editing (i.e. an aggressive modification).", "labels": [], "entities": []}, {"text": "To this aim, similar to the idea of multilingual NMT, a special token is added to the beginning of both the source text and the MT output indicating the required amount of post-editing.", "labels": [], "entities": []}, {"text": "Similar to last year's submission (, we use Transformer architecture with two encoders for encoding the source text and the MT output, while we share the parameters of the two encoders and tie the embeddings and decoder's softmax layer weights.", "labels": [], "entities": []}, {"text": "We participated in both the APE sub-tasks proposed this year, which respectively consist in postediting the output of English-German and EnglishRussian NMT systems.", "labels": [], "entities": []}, {"text": "Our experiments show that, on the development sets for both language directions, prepending the special token can improve the performance of the APE system up to 0.5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9990978240966797}]}, {"text": "However, predicting the correct token attest time, when the quality of the MT output is unknown, is still challenging and can harm the systems' performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9539152979850769}]}, {"text": "In the English-German subtask, our top system improves the MT output up to -0.47 TER and +0.98 BLEU points.", "labels": [], "entities": [{"text": "MT output", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.7363777756690979}, {"text": "TER", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9970996379852295}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9973335266113281}]}, {"text": "In the English-Russian subtask, due to the high quality of the MT segments, none of our submitted systems was able to improve the MT output, emphasizing the need for further research towards more reliable solutions to the over-correction problem.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.959784984588623}, {"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.975632905960083}]}], "datasetContent": [{"text": "We use two different evaluation metrics to assess the quality of our APE systems: i) TER (), the official metric for the task, computed based on the edit distance between the given hypothesis and the reference and ii) BLEU (), as the geometric average of n\u2212gram precisions in the given hypothesis multiplied by the brevity penalty.", "labels": [], "entities": [{"text": "TER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9988277554512024}, {"text": "BLEU", "start_pos": 218, "end_pos": 222, "type": "METRIC", "confidence": 0.9991806149482727}]}], "tableCaptions": [{"text": " Table 1: Performance of the APE systems, on the  English-German development set.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6672870516777039}, {"text": "English-German development set", "start_pos": 50, "end_pos": 80, "type": "DATASET", "confidence": 0.8255995512008667}]}, {"text": " Table 2: Performance of the APE systems, on the  English-Russian development set.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7477333545684814}, {"text": "English-Russian development set", "start_pos": 50, "end_pos": 81, "type": "DATASET", "confidence": 0.8066088358561198}]}, {"text": " Table 3: Performance of the APE systems, on the  English-German test set.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.6646308302879333}, {"text": "English-German test set", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.8794764677683512}]}, {"text": " Table 4: Performance of the APE systems, on the  English-Russian test set.", "labels": [], "entities": [{"text": "APE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7631177306175232}, {"text": "English-Russian test set", "start_pos": 50, "end_pos": 74, "type": "DATASET", "confidence": 0.8572617570559183}]}]}