{"title": [{"text": "Clinical Concept Extraction for Document-Level Coding", "labels": [], "entities": [{"text": "Clinical Concept Extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5372261703014374}, {"text": "Document-Level Coding", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7852811813354492}]}], "abstractContent": [{"text": "The text of clinical notes can be a valuable source of patient information and clinical assessments.", "labels": [], "entities": []}, {"text": "Historically, the primary approach for exploiting clinical notes has been information extraction: linking spans of text to concepts in a detailed domain ontology.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7624826729297638}]}, {"text": "However , recent work has demonstrated the potential of supervised machine learning to extract document-level codes directly from the raw text of clinical notes.", "labels": [], "entities": []}, {"text": "We propose to bridge the gap between the two approaches with two novel syntheses: (1) treating extracted concepts as features, which are used to supplement or replace the text of the note; (2) treating extracted concepts as labels, which are used to learn a better representation of the text.", "labels": [], "entities": []}, {"text": "Unfortunately , the resulting concepts do not yield performance gains on the document-level clinical coding task.", "labels": [], "entities": [{"text": "document-level clinical coding task", "start_pos": 77, "end_pos": 112, "type": "TASK", "confidence": 0.6732707321643829}]}, {"text": "We explore possible explanations and future research directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clinical decision support from raw-text notes taken by clinicians about patients has proven to be a valuable alternative to state-of-the-art models built from structured EHRs.", "labels": [], "entities": []}, {"text": "Clinical notes contain valuable information that the structured part of the EHR does not provide, and do not rely on expensive and time-consuming human annotation (; American Academy of Professional Coders, 2019).", "labels": [], "entities": [{"text": "EHR", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.9336754679679871}]}, {"text": "Impressive advances using deep learning have allowed for modeling on the raw text alone (.", "labels": [], "entities": []}, {"text": "However, there exist some shortcomings to these approaches: clinical text is noisy, and often contains heavy amounts of abbreviations and acronyms, a challenge for machine reading.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 164, "end_pos": 179, "type": "TASK", "confidence": 0.7625179886817932}]}, {"text": "Additionally, rare words replaced with \"UNK\" tokens for better generalization maybe crucial for predicting rare labels.", "labels": [], "entities": [{"text": "predicting rare labels", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.8799909551938375}]}, {"text": "Clinical concept extraction tools abstract over the noise inherent in surface representations of clinical text by linking raw text to standardized concepts in clinical ontologies.", "labels": [], "entities": [{"text": "Clinical concept extraction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6421657204627991}]}, {"text": "The Apache clinical Text Analysis Knowledge Extraction System (cTAKES,) is the most widelyused such tool, with over 1000 citations.", "labels": [], "entities": [{"text": "clinical Text Analysis Knowledge Extraction", "start_pos": 11, "end_pos": 54, "type": "TASK", "confidence": 0.6592874467372895}]}, {"text": "Based on rules and non-neural machine learning methods and engineered for almost a decade, cTAKES provides an easily-obtainable source of human-encoded domain knowledge, although it cannot leverage deep learning to make document-level predictions.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to maximize the predictive power of clinical notes by bridging the gap between information extraction and deep learning models.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.7460756301879883}]}, {"text": "We address the following research questions: how can we best leverage tools such as cTAKES on clinical text?", "labels": [], "entities": []}, {"text": "Can we show the value of these tools in linking unstructured data to structured codes in an existing ontology for downstream prediction?", "labels": [], "entities": [{"text": "downstream prediction", "start_pos": 114, "end_pos": 135, "type": "TASK", "confidence": 0.6666641235351562}]}, {"text": "We explore two novel hybrids of these methods: data augmentation (augmenting text with extracted concepts) and multi-task learning (learning to predict the output of cTAKES).", "labels": [], "entities": []}, {"text": "Unfortunately, in neither case does cTAKES improve downstream performance on the document-level clinical coding task.", "labels": [], "entities": [{"text": "document-level clinical coding task", "start_pos": 81, "end_pos": 116, "type": "TASK", "confidence": 0.6469167768955231}]}, {"text": "We probe this negative result through an extensive series of ablations, and suggest possible explanations, such as the lack of word variation captured through concept assignment.", "labels": [], "entities": []}], "datasetContent": [{"text": "Metrics In addition to the metrics reported in prior work, we report average precision score (AP), which is a preferred metric to AUC for imbalanced classes ().", "labels": [], "entities": [{"text": "average precision score (AP)", "start_pos": 69, "end_pos": 97, "type": "METRIC", "confidence": 0.8901228904724121}, {"text": "AUC", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.6953141093254089}]}, {"text": "We report both macro-and micro-metrics, with the former being more favorable toward rare labels by weighting all classes equally.", "labels": [], "entities": []}, {"text": "We additionally focus on the precision-atk (P@k) metric, representing the fraction of the k highest-scored predicted labels that are present in the ground truth.", "labels": [], "entities": [{"text": "precision-atk (P@k) metric", "start_pos": 29, "end_pos": 55, "type": "METRIC", "confidence": 0.9468744226864406}]}, {"text": "Both macro-metrics and P@k are useful in a computer-assisted coding use-case, where the desired outcome is to correctly identify needle-in-the-haystack labels as opposed to more frequent ones, and to accurately suggest a small subset of codes with the highest confidence as annotation suggestions ().", "labels": [], "entities": []}, {"text": "Baselines Along with CAML, we evaluate on a raw codes baseline where the ICD9 annotations generated by cTAKES c 1 , c 2 , . .", "labels": [], "entities": [{"text": "ICD9", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.909753143787384}]}, {"text": ", c N are used directly as the document-level predictions.", "labels": [], "entities": []}, {"text": "Formally,  Results are presented in for ICD9 annotations.", "labels": [], "entities": [{"text": "ICD9", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.7013483643531799}]}, {"text": "Overall, the cTAKES spanprediction task does more to hurt than help performance on the main task.", "labels": [], "entities": [{"text": "cTAKES spanprediction task", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.7145110766092936}]}, {"text": "Tying the model weights at a higher layer (post-convolution as opposed to preconvolution) results in worse performance, even though the model fits the auxiliary task well.", "labels": [], "entities": []}, {"text": "This indicates either that the model may not have enough capacity to adequately fit both tasks, or that the cTAKES prediction task as formulated may actually misguide the clinical coding task slightly in parameter search space.", "labels": [], "entities": [{"text": "cTAKES prediction", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.6828982979059219}]}, {"text": "We additionally remark that increasing the weight of the auxiliary task generally lowers performance on the clinical coding task, and tuning \u03bb on the dev set does not result in more optimal performance (we include results with \u03bb = 1 here; see in the Appendix).", "labels": [], "entities": [{"text": "clinical coding task", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7075509826342264}]}, {"text": "Notably, for even very small values of \u03bb, we achieve very high validation accuracy on the auxiliary task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9271959662437439}]}, {"text": "This performance does not change with larger weightings, indicating that the auxiliary task may not be difficult enough to result in effective knowledge transfer.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7479220926761627}]}, {"text": "11 We found similar results using SNOMED annotations.", "labels": [], "entities": []}, {"text": "While the models in Sections 4 did not introduce new hyperparameters to the baseline architecture, hyperparameters for this architecture were selected by human intuition.", "labels": [], "entities": []}, {"text": "Room for future work includes more extensive tuning (see in Appendix A).", "labels": [], "entities": [{"text": "tuning", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9646189212799072}, {"text": "Appendix", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.8604446053504944}]}, {"text": "Data Following, we use the same train/test/validation splits for the MIMIC-III dataset, and concatenate all supplemental text fora patient discharge summary into one record.", "labels": [], "entities": [{"text": "MIMIC-III dataset", "start_pos": 69, "end_pos": 86, "type": "DATASET", "confidence": 0.9488006234169006}]}, {"text": "We use the authors' provided data processing pipeline to preprocess the corpus.", "labels": [], "entities": []}, {"text": "The vocubulary includes all words occurring in at least 3 training documents.", "labels": [], "entities": []}, {"text": "See for descriptive statistics of the dataset.", "labels": [], "entities": []}, {"text": "We construct a concept vocabulary for embedding initialization following the same specification as the word vocabulary: any concept which does not occur in at least 3 training documents is replaced with an UNK token.", "labels": [], "entities": []}, {"text": "Details on the size of the vocabulary can be found in  Training We train with the same specifications as unless otherwise specified, with dropout performed after concept augmentation for the models in Sections 4, and early stopping with a patience of 10 epochs on the precision at 8 metric, fora maximum of 200 epochs (note that in the multi-task learning models the stopping criterion is only a function of performance on the clinical coding task).", "labels": [], "entities": [{"text": "early stopping", "start_pos": 217, "end_pos": 231, "type": "METRIC", "confidence": 0.9264627397060394}, {"text": "precision", "start_pos": 268, "end_pos": 277, "type": "METRIC", "confidence": 0.9834023714065552}]}, {"text": "Unlike previous work, we reduce the batch size to 12 in order to allow each batch to fit on a single GPU, and we do not use pretrained embeddings as we find this improves performance.", "labels": [], "entities": []}, {"text": "All models are trained on a single NVIDIA Titan X GPU with 12,189 MiB of RAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9520343542098999}]}, {"text": "We port the optimal hyperparameters reported in to our experiments.", "labels": [], "entities": []}, {"text": "With more extensive hyperparameter tuning, we may expect to see a potential increase in the performance of our models over the baseline.", "labels": [], "entities": []}, {"text": "See for hyperparameters and other details specific to our proposed model architectures.", "labels": [], "entities": []}, {"text": "All neural models are implemented using PyTorch", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.7322341799736023}]}], "tableCaptions": [{"text": " Table 1. Note the difference in number of anno- tations provided by using the SNOMED ontology  compared to ICD9. 4", "labels": [], "entities": [{"text": "ICD9", "start_pos": 108, "end_pos": 112, "type": "DATASET", "confidence": 0.8693212866783142}]}, {"text": " Table 2: Test set results using the augmentation methods.", "labels": [], "entities": []}, {"text": " Table 3: Test set results of ablation experiments on the MIMIC-III dataset, using ICD9 concept annotations.", "labels": [], "entities": [{"text": "MIMIC-III dataset", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9649700224399567}, {"text": "ICD9 concept annotations", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.8059866031010946}]}, {"text": " Table 4: Test set results of ablation experiments on the MIMIC-III dataset, using SNOMED concept annotations.", "labels": [], "entities": [{"text": "MIMIC-III dataset", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.9532386958599091}]}, {"text": " Table 5: Test set performance on the ICD9 coding task for \u03bb = 1 and using ICD9 annotations.", "labels": [], "entities": [{"text": "ICD9 coding task", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.5595181087652842}]}, {"text": " Table 6: Dev set performance on the auxiliary task for  \u03bb = 1 and using ICD9 annotations. Relatively high  task performance is achieved even after one epoch with  a simple model.", "labels": [], "entities": [{"text": "ICD9", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.8343771696090698}]}, {"text": " Table 9: The effect of tuning \u03bb on dev set performance on the ICD9 coding task, for the pre-convolution model  with a linear auxiliary layer and ICD9 annotations. We select \u03bb = 1 for reporting test results; there isn't a clear  value which produces strictly better performance.", "labels": [], "entities": [{"text": "ICD9 coding task", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.5677911241849264}]}]}