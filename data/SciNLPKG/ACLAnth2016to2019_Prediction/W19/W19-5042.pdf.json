{"title": [{"text": "DoubleTransfer at MEDIQA 2019: Multi-Source Transfer Learning for Natural Language Understanding in the Medical Domain", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.7967246174812317}, {"text": "Natural Language Understanding", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.7163404623667399}]}], "abstractContent": [{"text": "This paper describes our competing system to enter the MEDIQA-2019 competition.", "labels": [], "entities": [{"text": "MEDIQA-2019 competition", "start_pos": 55, "end_pos": 78, "type": "DATASET", "confidence": 0.715248316526413}]}, {"text": "We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN (Liu et al., 2019b) and SciBERT (Beltagy et al., 2019) to natural language understanding tasks in the medical domain.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 145, "end_pos": 181, "type": "TASK", "confidence": 0.7442003041505814}]}, {"text": "For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance.", "labels": [], "entities": [{"text": "transfer learning fine-tuning", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8865399956703186}]}, {"text": "The proposed methods are proved effective for natural language understanding in the medical domain , and we rank the first place on the QA task.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.6570577720801035}, {"text": "QA task", "start_pos": 136, "end_pos": 143, "type": "TASK", "confidence": 0.7980768978595734}]}], "introductionContent": [], "datasetContent": [{"text": "In,domain Datasets D k  MedNLI: Since the MEDIQA shared task uses a different test set than the original MedNLI dataset, we merge the original MedNLI development set into the training set and use evaluation performance on the original MedNLI test set.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.8960010707378387}, {"text": "MedNLI test set", "start_pos": 235, "end_pos": 250, "type": "DATASET", "confidence": 0.930692195892334}]}, {"text": "Furthermore, MedNLI and MNLI are the same NLI tasks, thus, we shared final-layer classifiers for these two tasks.", "labels": [], "entities": [{"text": "MNLI", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8995583057403564}]}, {"text": "For MedNLI, we find that each consecutive 3 samples in all the training set contain the same premise with different hypothesizes, and contains exactly 1 entail, 1 neutral and 1 contradiction.", "labels": [], "entities": []}, {"text": "To the end, in our prediction, we constrain the three predictions to be one of each kind, and use the most likely prediction from the model prediction probabilities.", "labels": [], "entities": []}, {"text": "RQE: We use the clinical question as the premise and question from FAQ as the hypothesis.", "labels": [], "entities": [{"text": "RQE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.4733896255493164}, {"text": "FAQ", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.843177080154419}]}, {"text": "We find that the test data distribution is quite different from the train data distribution.", "labels": [], "entities": []}, {"text": "To mitigate this effect, we randomly shuffle half of the evaluation data into the training set and evaluate on the remaining half.", "labels": [], "entities": []}, {"text": "QA: We use the answer as the premise and the question as the hypothesis.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5123398900032043}]}, {"text": "The QA task is cast as both a ranking task and a classification task.", "labels": [], "entities": []}, {"text": "Each question is associated with a relevance score in {1, 2, 3, 4}, and an additional rank overall the answers fora specific question is given.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.953239917755127}]}, {"text": "We use a modified score to incorporate both information: suppose there are m questions with relevance score s \u2208 {1, 2, 3, 4}.", "labels": [], "entities": []}, {"text": "Then the i-th most relevant answer in these m questions get modified score s \u2212 i\u22121 m . In this way the scores are uniformly distributed in.", "labels": [], "entities": []}, {"text": "We shift all scores by \u22122 so that a positive score leads to a correct answer and vice versa.", "labels": [], "entities": []}, {"text": "We also tried pairwise losses to incorporate the ranking but did not find it to boost the performance very much.", "labels": [], "entities": []}, {"text": "We find that the development set distribution is inconsistent with test data -the training and test set consist of both LiveQAMed and Alexa questions, whereas the development set seems to only contain LiveQAMed questions.", "labels": [], "entities": []}, {"text": "We shuffle the training and development set to make them similar: We use the last 25 questions in original development set (LiveQAMed questions) and the last 25 Alexa questions (from the original training set) as our development set, and use the remaining questions as our training set.", "labels": [], "entities": []}, {"text": "This results in 1,504 training pairs and 431 validation pairs.", "labels": [], "entities": []}, {"text": "Due to the limited size of the QA dataset, we use cross-validation that divides all pairs into 5 slices and train 5 models by using each slice as a validation set.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.8169199228286743}]}, {"text": "We train MT-DNN and SciBERT on both these 5 setups and obtain 10 models, and ensemble all the 10 models obtained.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.6385769248008728}]}, {"text": "MedQuAD: We use 10,109 questions from MedQuAD because the remaining questions are not available due to copyright issues.", "labels": [], "entities": [{"text": "MedQuAD", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9653497934341431}]}, {"text": "The original MedQuAD dataset only contains positive question pairs.", "labels": [], "entities": [{"text": "MedQuAD dataset", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.9253323376178741}]}, {"text": "We add negative samples to the dataset by randomly sampling an answer from the same web page.", "labels": [], "entities": []}, {"text": "For each positive QA pair, we add two negative samples.", "labels": [], "entities": []}, {"text": "The resulting 30,327 pairs are randomly divided into 27,391 training pairs and 2,936 evaluation pairs.", "labels": [], "entities": []}, {"text": "Then we use the same method as QA to train MedQuAD; we also share the same answer module between QA and MedQuAD.", "labels": [], "entities": [{"text": "MedQuAD", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.8400500416755676}]}], "tableCaptions": [{"text": " Table 1: The leaderboard for MedNLI task (link).  Scores are accuracy(%). Our method ranked the 3rd  on the leaderboard. Previous SOTA method was from  (Romanov and Shivade, 2018), on the original MedNLI  test set (used as dev set here).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9996670484542847}, {"text": "MedNLI  test set", "start_pos": 198, "end_pos": 214, "type": "DATASET", "confidence": 0.9546477794647217}]}, {"text": " Table 2: The leaderboard for RQE task (link). Scores  are accuracy(%). Our method ranked the 7th on the  leaderboard.", "labels": [], "entities": [{"text": "RQE task", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.6630525588989258}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9997346997261047}]}, {"text": " Table 3: The leaderboard for QA task (link). Our  method ranked #1 on the leaderboard in terms of Acc  (accuracy). The Spearman score is not consistent with  other scores in the leaderboard.", "labels": [], "entities": [{"text": "QA task", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.7467291951179504}, {"text": "Acc  (accuracy)", "start_pos": 99, "end_pos": 114, "type": "METRIC", "confidence": 0.87428118288517}]}, {"text": " Table 4: Comparison of ensembles from different  sources. Avg.Acc stands for average accuracy, the nu- merical average of each individual model's accuracy.  Esm.Acc stands for ensemble accuracy, the accuracy  of the resulting ensemble model. For ensembles, MT- DNN means all the three models are from MT-DNN,  and similarly for SciBERT; MultiSource denotes the  ensemble models come from two different sources.", "labels": [], "entities": [{"text": "Avg.Acc", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9846625328063965}, {"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.601239800453186}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9825076460838318}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.6051385402679443}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9945796728134155}, {"text": "MT-DNN", "start_pos": 302, "end_pos": 308, "type": "DATASET", "confidence": 0.8376632928848267}]}, {"text": " Table 5: Single model performance on MedNLI de- veloplment data. Nai\u00a8\u0131veNai\u00a8\u0131ve means simply integrating all  medical-domain data; Ratio means using MedNLI as  in-domain data and other medical domain data as ex- ternal data; Ratio+MNLI means using medical domain  data as in-domain and MNLI as external.", "labels": [], "entities": [{"text": "MedNLI de- veloplment data", "start_pos": 38, "end_pos": 64, "type": "DATASET", "confidence": 0.5955376088619232}, {"text": "MNLI", "start_pos": 232, "end_pos": 236, "type": "DATASET", "confidence": 0.8039889931678772}]}]}