{"title": [], "abstractContent": [{"text": "This paper describes the University of Mary-land's submission to the WMT 2019 Kazakh to English news translation task.", "labels": [], "entities": [{"text": "WMT 2019 Kazakh to English news translation task", "start_pos": 69, "end_pos": 117, "type": "TASK", "confidence": 0.7260107547044754}]}, {"text": "We study the impact of transfer learning from another low-resource but related language.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.9059505760669708}]}, {"text": "We experiment with different ways of encoding lexical units to maximize lexical overlap between the two language pairs, as well as back-translation and ensembling.", "labels": [], "entities": []}, {"text": "The submitted system improves over a Kazakh-only baseline by +5.45 BLEU on newstest2019.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9996621608734131}]}], "introductionContent": [{"text": "Neural Machine Translation (NMT) outperforms traditional phrase-based statistical machine translation provided that large amounts of parallel data are available (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7792844772338867}, {"text": "phrase-based statistical machine translation", "start_pos": 57, "end_pos": 101, "type": "TASK", "confidence": 0.6200739592313766}]}, {"text": "However, it performs poorly under low-resource conditions.", "labels": [], "entities": []}, {"text": "While much work addresses this problem via semi-supervised learning from monolingual text (, we focus on transfer learning from another language pair (.", "labels": [], "entities": []}, {"text": "In this setting, an NMT system is firstly trained using auxiliary parallel data from a so-called \"parent\" language pair and then the trained model is used to initialize a \"child\" model which is further trained on a low-resource language pair.", "labels": [], "entities": []}, {"text": "Similar approaches that support cross-lingual transfer learning for Multi-lingual NMT train a model on the concatenation of all data instead of employing sequential training (.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 32, "end_pos": 63, "type": "TASK", "confidence": 0.7966908613840739}]}, {"text": "Transfer learning has been found effective in submissions to WMT in previous years:  reported improvements of +2.4 BLEU on the low-resource Estonian\u2192English translation task by transfer learning from Finnish\u2192English.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9458239376544952}, {"text": "WMT", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.5725271105766296}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9936929941177368}, {"text": "Estonian\u2192English translation task", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.6376536428928375}, {"text": "transfer learning from Finnish\u2192English", "start_pos": 177, "end_pos": 215, "type": "TASK", "confidence": 0.7951829532782236}]}, {"text": "Interestingly,  observed that the transfer learning approach is still effective when there is no relatedness between the \"child\" and \"parent\" language-pairs and also hypothesize that the size of the parent training set is the most important factor leading to translation quality improvements.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.8985098898410797}]}, {"text": "However, previous work has also empirically validated that transfer learning benefits most when \"child\"-\"parent\" languages belong to the same or linguistically similar language family (.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.9220638275146484}]}, {"text": "Specifically, showed consistent improvements in two Turkic languages via transfering from another related, low-resource language.", "labels": [], "entities": []}, {"text": "Taking those recent results into consideration, our main focus at WMT19 is to examine transfer learning for the Kazakh-English language pair using additional parallel data from Turkish-English.", "labels": [], "entities": [{"text": "WMT19", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.7997868061065674}, {"text": "transfer learning", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.8884529173374176}]}, {"text": "While using distinct writing systems, both source languages belong to the Turkic language family and preserve many morphological and syntactic features common for that group.", "labels": [], "entities": []}, {"text": "As a result, they constitute a suitable \"child\"-\"parent\" language-pair choice for exploring transfer learning between related lowresource languages.", "labels": [], "entities": []}, {"text": "In this direction, we conduct experiments to address the following questions: \u2022 How can we represent lexical units to exploit vocabulary overlap between languages?", "labels": [], "entities": []}, {"text": "We compare bilingual and monolingual byte-pair encoding models with the recently proposed soft decoupled encoding model.", "labels": [], "entities": []}, {"text": "\u2022 How can we leverage both \"child\" and \"parent\" parallel data to obtain synthetic backtranslated data from monolingual resources?", "labels": [], "entities": []}], "datasetContent": [{"text": "Starting from Baseline BPE-based NMT systems trained using only the Kazakh data provided by the competition, we conduct the following experiments.", "labels": [], "entities": []}, {"text": "presents our results of 3 runs using {32, 64}K merge operations in total for each experiment.", "labels": [], "entities": []}, {"text": "Generally, both Joint and Separate BPE segmentation strategies, with and without romanization improve BLEU over the Baseline.", "labels": [], "entities": [{"text": "Separate BPE segmentation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.678090512752533}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9986866116523743}]}, {"text": "Previous empirical results on transfer learning for extremely low-resource languages indicated that training the BPE operations separately for the \"child\" and \"parent\" languages has a large positive effect on the performance of the model (.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.9155900180339813}]}, {"text": "By contrast, JBPEs and SBPEs perform comparably well in almost all configurations here.", "labels": [], "entities": []}, {"text": "This could be attributed to our less imbalanced setting where the ratio of \"child\"-\"parent\" data is 1 : 2, and the child language therefore contributes more to sub-word segmentation rules.", "labels": [], "entities": []}, {"text": "The best BLEU score is achieved using 32K JBPEs on the romanized data which is consistent with the configuration with the largest vocabulary overlap, according to.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9732981324195862}]}, {"text": "However, using {32, 64}K SBPEs on the original data only hurts BLEU by 0.5 and 1.24, despite the lack of lexical overlap.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9989452958106995}]}, {"text": "This suggests that most of the improvement does not come from the shared encoder vocabulary.: SDE Experiments using 64K n-grams of the concatenated corpora.", "labels": [], "entities": []}, {"text": "The last line refers to the best BLEU score using 64K BPEs for comparison.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9702058136463165}, {"text": "BPEs", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.5962567329406738}]}], "tableCaptions": [{"text": " Table 1: Examples of words sharing significant lexical overlap in Kazakh and Turkish among with their corre- sponding sub-words segmentations.", "labels": [], "entities": []}, {"text": " Table 3: Kazakh \u2192 English BLEU score results on news-test2019 for different BPE configurations and versions  of data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9670010507106781}]}, {"text": " Table 4: SDE Experiments using 64K n-grams of the  concatenated corpora. The last line refers to the best  BLEU score using 64K BPEs for comparison.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.998621940612793}]}]}