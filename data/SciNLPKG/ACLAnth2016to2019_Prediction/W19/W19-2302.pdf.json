{"title": [{"text": "DAL: Dual Adversarial Learning for Dialogue Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "In open-domain dialogue systems, genera-tive approaches have attracted much attention for response 1 generation.", "labels": [], "entities": [{"text": "response 1 generation", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.6476543148358663}]}, {"text": "However, existing methods are heavily plagued by generating safe responses and unnatural responses.", "labels": [], "entities": []}, {"text": "To alleviate these two problems, we propose a novel framework named Dual Adversarial Learning (DAL) for high-quality response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7632800340652466}]}, {"text": "DAL innovatively utilizes the duality between query generation and response generation to avoid safe responses and increase the diversity of the generated responses.", "labels": [], "entities": [{"text": "query generation", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7044747471809387}, {"text": "response generation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.6623489558696747}]}, {"text": "Additionally , DAL uses adversarial learning to mimic human judges and guides the system to generate natural responses.", "labels": [], "entities": [{"text": "DAL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.8563791513442993}]}, {"text": "Experimental results demonstrate that DAL effectively improves both diversity and overall quality of the generated responses.", "labels": [], "entities": [{"text": "DAL", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9130034446716309}]}, {"text": "DAL outperforms state-of-the-art methods regarding automatic met-rics and human evaluations.", "labels": [], "entities": [{"text": "DAL", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.5307485461235046}]}], "introductionContent": [{"text": "In recent years, open-domain dialogue systems are gaining much attention owing to their great potential in applications such as educational robots, emotional companion, and chitchat.", "labels": [], "entities": []}, {"text": "The existing approaches for open-domain dialogue systems can be divided into two categories: retrieval-based approaches () and generative approaches.", "labels": [], "entities": []}, {"text": "The retrieval-based approaches are based on conventional information retrieval techniques and strongly rely on the underlying corpus (.", "labels": [], "entities": []}, {"text": "Since the capability of retrieval-based approaches is strongly limited by corpus, generative approaches are attracting more attention in the field of open-domain dialogue research.", "labels": [], "entities": []}, {"text": "The de facto backbone of generative approaches is the Seq2Seq model () , which is essentially an encoderdecoder neural network architecture.", "labels": [], "entities": [{"text": "generative", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9643634557723999}]}, {"text": "Despite their success, Seq2Seq model and its variants ( are heavily plagued by safe responses (generic and dull responses such as \"I don't know\" or \"Me too\") and unnatural responses (such as \"I want to go, but I don't want to go\").", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel framework named Dual Adversarial Learning (DAL) to alleviate the aforementioned two problems.", "labels": [], "entities": []}, {"text": "DAL consists of two generative adversarial networks (GANs): one for query generation and the other for response generation.", "labels": [], "entities": [{"text": "query generation", "start_pos": 68, "end_pos": 84, "type": "TASK", "confidence": 0.7272774875164032}, {"text": "response generation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7105038613080978}]}, {"text": "The response generation model is used to transfer from the query domain Q to the response domain R, while the query generation model is for transformation from R to Q.", "labels": [], "entities": [{"text": "response generation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7120769023895264}, {"text": "query generation", "start_pos": 110, "end_pos": 126, "type": "TASK", "confidence": 0.6909805834293365}]}, {"text": "Here we consider the response generation task and the query generation task as dual tasks.", "labels": [], "entities": [{"text": "response generation task", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7609075506528219}, {"text": "query generation task", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.7723859449227651}]}, {"text": "The generators of these two GANs are connected through the duality constraint.", "labels": [], "entities": []}, {"text": "As such, in DAL, there are two kinds of signals that jointly instruct the optimization of generators: (1) the dual signal from the duality constraint between these two generators; (2) the adversarial signal from the discriminators.", "labels": [], "entities": []}, {"text": "The dual signal is utilized to model the mutual relation between query generation and response generation.", "labels": [], "entities": [{"text": "query generation and response generation", "start_pos": 65, "end_pos": 105, "type": "TASK", "confidence": 0.743483018875122}]}, {"text": "We use an instance to better illustrate this mutual relation: fora given query \"Where to have dinner?\", compared with a safe response \"I dont know\", a more diverse and specific response \"The Indian cuisine around the corner is great\" usually has a higher probability of being transformed back to the given query.", "labels": [], "entities": []}, {"text": "DAL takes full advantage of this intuition via dual learning, which avoids generating safe responses and improves the diversity of the generated responses.", "labels": [], "entities": [{"text": "DAL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5846639275550842}]}, {"text": "Additionally, in order to make the generated responses as natural as possible, the adversarial signal in DAL mimics human judges to alle-viate unnatural responses.", "labels": [], "entities": []}, {"text": "We compare DAL with state-of-the-art methods through extensive experiments, and DAL demonstrates superior performance regarding automatic metrics, human evaluations, and efficiency.", "labels": [], "entities": []}, {"text": "There are crucial differences between our dual approach and Maximum Mutual Information (MMI) () though both utilize the reverse dependency to improve the diversity of the generated responses.", "labels": [], "entities": []}, {"text": "Due to the challenging mutual information objective, the distribution p(r|q) is same as that in vanilla Seq2Seq in MMI.", "labels": [], "entities": []}, {"text": "More specifically, p(r|q) in MMI is trained only by maximum likelihood estimation (MLE) objective at training time (we use p(r|q) to denote the probability distribution of predicting the response r given the query q).", "labels": [], "entities": [{"text": "MMI", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9553635716438293}, {"text": "maximum likelihood estimation (MLE) objective", "start_pos": 52, "end_pos": 97, "type": "METRIC", "confidence": 0.8144475732530866}]}, {"text": "The mutual information in MMI is utilized only at inference time, and the inference process is not only time-consuming but also inaccurate in MMI.", "labels": [], "entities": [{"text": "MMI", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8990142941474915}, {"text": "MMI", "start_pos": 142, "end_pos": 145, "type": "TASK", "confidence": 0.9019433259963989}]}, {"text": "However, p(r|q) in our dual approach is trained by not only the maximum likelihood estimation objective but also the diversity objective (duality constraint) at training time.", "labels": [], "entities": []}, {"text": "Since the dual approach directly incorporates the reverse dependency information at the training time, it can avoid the time-consuming inference plaguing MMI.", "labels": [], "entities": [{"text": "MMI", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.9007493257522583}]}, {"text": "Additionally, the dual approach does not need to maintain a large size optional response set for the time-consuming reranking strategy in MMI-bidi (one variant of MMI).", "labels": [], "entities": []}, {"text": "The dual approach shows its efficiency superiority over MMI in real-life applications, which is shown in our efficiency experiment.", "labels": [], "entities": []}, {"text": "Our dual approach is quite different from the reinforcement learning based structure having two Seq2Seq models in ( 2 . In (, G 1 , which generates a respons\u00ea r given a query q, uses the conditional probability P 2 (q|\u02c6rq|\u02c6r) calculated by G 2 as the coherence measure to guide G 1 in the reinforcement learning process.", "labels": [], "entities": [{"text": "reinforcement learning process", "start_pos": 289, "end_pos": 319, "type": "TASK", "confidence": 0.8769252101580302}]}, {"text": "Similarly, G 2 , which generates a query\u02c6qquery\u02c6 query\u02c6q given a response r, uses the conditional probability P 1 (r|\u02c6qr|\u02c6q) calculated by G 1 as the coherence measure to guide G 2 in the reinforcing learning process.", "labels": [], "entities": []}, {"text": "However, in our work, we utilize the joint probability p(q, r) to connect these two Seq2Seq models and thus avoid unstable and time-consuming reinforcement learning in the dual approach.", "labels": [], "entities": []}, {"text": "Besides, our DAL framework is strongly different from previous structures that are composed of two GANs, such as CycleGAN ( , and.", "labels": [], "entities": [{"text": "CycleGAN", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.8963654041290283}]}, {"text": "Those works can only be utilized on the image translation task and two generators are connected by cycle consistency, i.e., for each image x in domain X , the image translation cycle is supposed to bring x to the original image: However, cycle consistency is difficult to be applied into the text generation task.", "labels": [], "entities": [{"text": "image translation task", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.828572690486908}, {"text": "text generation task", "start_pos": 292, "end_pos": 312, "type": "TASK", "confidence": 0.8206758499145508}]}, {"text": "In our paper, we use the joint distribution of query-response pairs rather than cycle consistency to enforce the duality between these two dual generators.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: \u2022 To the best of our knowledge, this is the first work that adopts the duality to avoid safe responses for dialogue generation.", "labels": [], "entities": [{"text": "dialogue generation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.8500843346118927}]}, {"text": "It sheds light on the utility of query generation in improving the performance of response generation.", "labels": [], "entities": [{"text": "query generation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7264859229326248}, {"text": "response generation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7721788585186005}]}, {"text": "\u2022 DAL is a novel framework that integrates dual learning and adversarial learning, which complementary and jointly contributes to generating both diverse and natural responses.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The related work is firstly reviewed.", "labels": [], "entities": []}, {"text": "The DAL framework is introduced in Section 3 and the training of DAL is described in Section 4.", "labels": [], "entities": []}, {"text": "Experimental results are shown in Section 5, followed by the conclusion of this paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "A Sina Weibo dataset ( ) is employed to train the models.", "labels": [], "entities": [{"text": "Sina Weibo dataset", "start_pos": 2, "end_pos": 20, "type": "DATASET", "confidence": 0.8802764614423116}]}, {"text": "We treat each query-response pair as a single-turn conversation.", "labels": [], "entities": []}, {"text": "Attention mechanism () is applied in all the methods to enhance the performance.", "labels": [], "entities": [{"text": "Attention", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9540579319000244}]}, {"text": "All the methods are implemented based on the open source tools Pytorch( and OpenNMT (.", "labels": [], "entities": [{"text": "Pytorch", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.8773914575576782}]}, {"text": "1,000,565 query-response pairs are employed as the training data, 3,000 pairs as the validation data.", "labels": [], "entities": []}, {"text": "The test data is another unique 10,000 query-response pairs.", "labels": [], "entities": []}, {"text": "The length of all the dialogue utterances in the training corpus ranges from 5 to 50.", "labels": [], "entities": []}, {"text": "Batch size is set to 64.", "labels": [], "entities": []}, {"text": "The vocabulary size is set to 50,000.", "labels": [], "entities": []}, {"text": "The dimension of word embedding is set to 500.", "labels": [], "entities": []}, {"text": "All the methods adopt abeam size of 5 in the decoding phase.", "labels": [], "entities": []}, {"text": "The maximum length of the target sequence is set to 50.", "labels": [], "entities": []}, {"text": "Gradient clipping strategy is adopted when the norm exceeds a threshold of 5.", "labels": [], "entities": [{"text": "Gradient clipping", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.7005028128623962}]}, {"text": "There are 2 fully-connected layers (1000*500, 500*1) in the discriminator structure of DAL-DuAd.", "labels": [], "entities": []}, {"text": "The vanilla Seq2Seq, MMI-anti and MMI-bidi use SGD as the optimizer, whose initial learning rate is 1.0.", "labels": [], "entities": []}, {"text": "Adver-REIN, GAN-AEL, DAL-Dual, and DAL-DuAd use Adam ( as the optimizer, whose initial learning rate is 0.001, \u03b2 1 = 0.9, and \u03b2 2 = 0.999.", "labels": [], "entities": []}, {"text": "Both Adam and SGD used in all the methods adopt a decay rate of 0.5 after the 8th epoch.", "labels": [], "entities": []}, {"text": "The dropout () probability is set to 0.5.", "labels": [], "entities": [{"text": "dropout () probability", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9257904688517252}]}, {"text": "\u03bb dual is set to 0.025 for both G \u03b8qr and G \u03b8rq . \u03bb dual is set to 0.025 and \u03bb gan is set to 1 for both G \u03b8qr and G \u03b8rq . In Algorithm 1, dis set to 1 and g is set to 5.", "labels": [], "entities": []}, {"text": "In MMI-bidi, the size of the N-best list is set to 5.", "labels": [], "entities": []}, {"text": "In MMI-anti, \u03b3 is set to 0.15 and \u03bb is set to 0.3.", "labels": [], "entities": []}, {"text": "We firstly evaluate DAL on the task of generating of diverse responses.", "labels": [], "entities": [{"text": "DAL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9353007078170776}]}, {"text": "Then we resort to human annotators to evaluate the overall quality of the generated responses.", "labels": [], "entities": []}, {"text": "Finally, we present several cases generated by all the involved method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of diversity evaluation.", "labels": [], "entities": [{"text": "diversity evaluation", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.9426532089710236}]}, {"text": " Table 2: Results of human elevation: response quality.", "labels": [], "entities": []}]}