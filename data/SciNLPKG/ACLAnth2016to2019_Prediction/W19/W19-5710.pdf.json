{"title": [], "abstractContent": [{"text": "The distribution of sentence length in ordinary language is not well captured by the existing models.", "labels": [], "entities": []}, {"text": "Here we survey previous models of sentence length and present our random walk model that offers both a better fit with the data and a better understanding of the distribution.", "labels": [], "entities": []}, {"text": "We develop a generalization of KL divergence, discuss measuring the noise inherent in a corpus, and present a hyperparameter-free Bayesian model comparison method that has strong conceptual ties to Minimal Description Length modeling.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9113274216651917}, {"text": "Minimal Description Length modeling", "start_pos": 198, "end_pos": 233, "type": "TASK", "confidence": 0.7715504169464111}]}, {"text": "The models we obtain require only a few dozen bits, orders of magnitude less than the naive nonparametric MDL models would.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, statistical properties of sentence length distribution were investigated with the goal of settling disputed authorship.", "labels": [], "entities": [{"text": "sentence length distribution", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.7463618020216624}, {"text": "settling disputed authorship", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.8468790451685587}]}, {"text": "Simple models, such as a \"monkeys and typewriters\" Bernoulli process do not fit the data well, and this problem is inherited from n-gram Markov to n-gram Hidden Markov models, such as found in standard language modeling tools like SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 231, "end_pos": 236, "type": "DATASET", "confidence": 0.8356273174285889}]}, {"text": "Today, length modeling is used more often as a downstream task to probe the properties of sentence vectors, but the problem is highly relevant in other settings as well, in particular for the current generation of LSTM/GRU-based language models that generally use an ad hoc cutoff mechanism to regulate sentence length.", "labels": [], "entities": [{"text": "length modeling", "start_pos": 7, "end_pos": 22, "type": "TASK", "confidence": 0.7824269235134125}]}, {"text": "This is also evident inmost of the cumulative percentage frequency distributions of sentence-lengths plotted on logprobability paper by.", "labels": [], "entities": []}, {"text": "The sweep of the curves drawn through the plotted observations is concave upwards which means that we deal with sub-lognormal populations.", "labels": [], "entities": []}, {"text": "In other words, most of the observed sentencelength distributions, after logarithmic transformation, are negatively skew.", "labels": [], "entities": []}, {"text": "Finally, a mathematical distribution model which cannot fit real data -as shown up by the conventional \u03c7 2 test-cannot claim serious attention.", "labels": [], "entities": []}, {"text": "Sichel's own model is a mixture of Poisson distributions given as where K \u03b3 is the modified Bessel function of the second kind of order \u03b3.", "labels": [], "entities": []}, {"text": "As Sichel notes, \"a number of known discrete distribution functions such as the Poisson, negative binomial, geometric, Fisher's logarithmic series in its original and modified forms, Yule, Good, Waring and Riemann distributions are special or limiting forms of (1)\".", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distribution of short and long sentences", "labels": [], "entities": [{"text": "Distribution of short and long sentences", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.7099408010641733}]}, {"text": " Table 2: Sentence length datasets. For tolerance see  subsection 3.2", "labels": [], "entities": []}, {"text": " Table 3: Optimal models for artificially generated data  (1.k3) for various n values.", "labels": [], "entities": []}, {"text": " Table 6: Optimal models without tolerance. Ill-fitting  models are marked with italics.", "labels": [], "entities": []}]}