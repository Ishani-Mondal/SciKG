{"title": [], "abstractContent": [{"text": "The net is rife with rumours that spread through microblogs and social media.", "labels": [], "entities": []}, {"text": "Not all the claims in these can be verified.", "labels": [], "entities": []}, {"text": "However, recent work has shown that the stances alone that commenters take toward claims can be sufficiently good indicators of claim veracity, using e.g. an HMM that takes conversational stance sequences as the only input.", "labels": [], "entities": []}, {"text": "Existing results are monolingual (English) and mono-platform (Twitter).", "labels": [], "entities": []}, {"text": "This paper introduces a stance-annotated Reddit dataset for the Danish language, and describes various implementations of stance classification models.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.7008780986070633}, {"text": "stance classification", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.7821052670478821}]}, {"text": "Of these, a Linear SVM provides predicts stance best, with 0.76 accuracy / 0.42 macro F 1.", "labels": [], "entities": [{"text": "predicts", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8409071564674377}, {"text": "stance", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.49835899472236633}, {"text": "accuracy / 0.42 macro F 1", "start_pos": 64, "end_pos": 89, "type": "METRIC", "confidence": 0.7809791167577108}]}, {"text": "Stance labels are then used to predict veracity across platforms and also across languages, training on conversations held in one language and using the model on conversations held in another.", "labels": [], "entities": []}, {"text": "In our experiments, monolinugal scores reach stance-based veracity accuracy of 0.83 (F 1 0.68); applying the model across languages predicts veracity of claims with an accuracy of 0.82 (F 1 0.67).", "labels": [], "entities": [{"text": "veracity accuracy", "start_pos": 58, "end_pos": 75, "type": "METRIC", "confidence": 0.6104685962200165}, {"text": "F 1 0.68)", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.93956059217453}, {"text": "accuracy", "start_pos": 168, "end_pos": 176, "type": "METRIC", "confidence": 0.9944847226142883}]}, {"text": "This demonstrates the surprising and powerful viability of transferring stance-based ve-racity prediction across languages.", "labels": [], "entities": [{"text": "stance-based ve-racity prediction", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.579790323972702}]}], "introductionContent": [{"text": "Social media has come to play a big role in our everyday lives as we use it to connect with our social network, but also to connect with the world.", "labels": [], "entities": []}, {"text": "It is common to catch upon news through Facebook, or to be alerted with emerging events through * : These authors contributed to the paper equally.", "labels": [], "entities": []}, {"text": "However these phenomena create a platform for the spread of rumours, that is, stories with unverified claims, which mayor may not be true.", "labels": [], "entities": []}, {"text": "This has lead to the concept of fake news, or misinformation, where the spreading of a misleading rumour is intentional ().", "labels": [], "entities": []}, {"text": "Can we somehow automatically predict the veracity of rumours?", "labels": [], "entities": [{"text": "predict the veracity of rumours", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8025098443031311}]}, {"text": "Research has tried to tackle this problem), but automated rumour veracity prediction is still maturing (.", "labels": [], "entities": [{"text": "rumour veracity prediction", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6947261889775594}]}, {"text": "This project investigates stance classification as a step for automatically determining the veracity of a rumour.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9340795278549194}, {"text": "automatically determining the veracity of a rumour", "start_pos": 62, "end_pos": 112, "type": "TASK", "confidence": 0.632032322032111}]}, {"text": "Previous research has shown that the stance of a crowd is a strong indicator for veracity (, but that it is a difficult task to build a reliable classifier . Moreover a study has shown that careful feature engineering can have substantial influence on the accuracy of a classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9982886910438538}]}, {"text": "A system able to verify or refute rumours is typically made up of four components: rumour detection, rumour tracking, stance classification, and veracity classification (.", "labels": [], "entities": [{"text": "rumour detection", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7969924509525299}, {"text": "rumour tracking", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.8141986727714539}, {"text": "stance classification", "start_pos": 118, "end_pos": 139, "type": "TASK", "confidence": 0.8664522171020508}, {"text": "veracity classification", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.8491919338703156}]}, {"text": "This project will mainly be concerned with stance classification and rumour veracity classification.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.9700420498847961}, {"text": "rumour veracity classification", "start_pos": 69, "end_pos": 99, "type": "TASK", "confidence": 0.7423021793365479}]}, {"text": "Current research is mostly concerned with the English language, and in particular data from Twitter is used as data source because of its availability and relevant news content).", "labels": [], "entities": []}, {"text": "To our knowledge no research within this area has been carried out in a Danish context.", "labels": [], "entities": []}, {"text": "To perform automated rumour veracity prediction for the Danish language following the components in, a number of problems must be solved.", "labels": [], "entities": [{"text": "rumour veracity prediction", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8752493262290955}]}, {"text": "(1) to facilitate Danish stance classification a Danish dataset must be generated and annotated for stance.", "labels": [], "entities": [{"text": "Danish stance classification", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6471236447493235}, {"text": "Danish dataset", "start_pos": 49, "end_pos": 63, "type": "DATASET", "confidence": 0.8825181126594543}]}, {"text": "(2) developing a good stance classifier is difficult, especially given the unknown domain of the Danish language.", "labels": [], "entities": [{"text": "stance classifier", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6128794997930527}]}, {"text": "Therefore experiments must be performed to investigate what approach to apply to Danish stance classification.", "labels": [], "entities": [{"text": "Danish stance classification", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.6741507053375244}]}, {"text": "(3) given rumourous data, and aided by stance classification, a rumour veracity prediction component should be able to determine whether it is true or false.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.9376461505889893}]}], "datasetContent": [{"text": "Because of various limitations on big social media platforms including Facebook and Twitter, Reddit is used as platform for the dataset.", "labels": [], "entities": []}, {"text": "3 This is a novel approach; prior research has typically relied on Twitter ().", "labels": [], "entities": []}, {"text": "Data sampling: The data gathering process consists of two approaches: to manually identify interesting submissions on Reddit, and; to issue queries to the Reddit API 4 on specific topics.", "labels": [], "entities": [{"text": "Data sampling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7433509528636932}]}, {"text": "An example of a topic could be \"Peter Madsen\" refer-ring to the submarine murder case, starting from August 2017.", "labels": [], "entities": [{"text": "Peter Madsen\" refer-ring to the submarine murder case", "start_pos": 32, "end_pos": 85, "type": "TASK", "confidence": 0.5932285288969675}]}, {"text": "A query would as such be constructed of the topic \"Peter Madsen\" as search text, a time window and a minimum amount of Reddit upvotes.", "labels": [], "entities": []}, {"text": "A minimum-upvotes filter is applied to limit the amount of data returned by the query.", "labels": [], "entities": []}, {"text": "Moreover the temporal filters are to ensure a certain amount of relevance to the case, specifically when the event initially unfolded.", "labels": [], "entities": []}, {"text": "Several submissions prior or subsequent to the given case may match a search term such as \"ub\u00e5d\" (submarine).", "labels": [], "entities": []}, {"text": "Four Danish Subreddits were browsed, including \"Denmark, denmark2, DKpol, and GammelDansk\", 6 although all relevant data turned out to be from the \"Denmark\" Subreddit.", "labels": [], "entities": [{"text": "Denmark", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9649240970611572}, {"text": "denmark2", "start_pos": 57, "end_pos": 65, "type": "DATASET", "confidence": 0.692814290523529}, {"text": "DKpol", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8110615611076355}, {"text": "GammelDansk", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8014546036720276}, {"text": "Denmark\" Subreddit", "start_pos": 148, "end_pos": 166, "type": "DATASET", "confidence": 0.9284266630808512}]}, {"text": "The submission IDs found manually and returned by the queries are used to download all posts from each submission using the praw 7 and psaw 8 Python libraries.", "labels": [], "entities": []}, {"text": "The submission data is subsequently stored in a JSON format, one JSON file per submission, consisting of submission data and a list of comment data.", "labels": [], "entities": []}, {"text": "These files include submission post text and comment text, as well as metainformation about the following: submission post, submitter user, Subreddit, comments, and commenting users.", "labels": [], "entities": []}, {"text": "Annotation: One widely used annotation scheme for stance on Twitter is the SDQC approach from.", "labels": [], "entities": []}, {"text": "Twitter differs from Reddit in the way conversations are structured.", "labels": [], "entities": []}, {"text": "Each tweet spawns a conversation which can have nested replies, and as such creates branches.", "labels": [], "entities": []}, {"text": "Reddit implements the same mechanism, but a set of conversations are tied to a specific submission, which is initiated by a submission post.", "labels": [], "entities": []}, {"text": "The Reddit structure is depicted in, illustrating a conversation (in green) and two respective branches (in respectively red and purple).", "labels": [], "entities": []}, {"text": "Note that branches share at least one comment.", "labels": [], "entities": []}, {"text": "Thus, away to annotate data from the Reddit platform with the annotation scheme from is by regarding a submission post as a source, instead of each top-level comment for the conversations.", "labels": [], "entities": []}, {"text": "The stance of the source/submission post is taken into account when annotating the stance for: The structure of a Reddit submission replying posts of top-level posts.", "labels": [], "entities": []}, {"text": "As stance annotations are relative to some target, each post does not have one single stance annotation: each post is annotated for the stance targeted towards the submission and the stance targeted towards the direct parent of the post.", "labels": [], "entities": []}, {"text": "The double-annotation should facilitate away to infer the stance for individual posts.", "labels": [], "entities": []}, {"text": "For instance, if the source post supports a rumour, and a nested reply supports its parent post, which in turn denies the source, then the nested reply is implicitly denying the rumour.", "labels": [], "entities": []}, {"text": "Further, a majority of submissions have no text, but a title and a link to an article, image or another website, with content related to the title of the submission.", "labels": [], "entities": []}, {"text": "If this is the case and the title of the submission bears no significant stance, it is assumed that the author of the submission takes the same stance as the content which is attached to the submission.", "labels": [], "entities": []}, {"text": "Annotation tool: A custom web-based annotation tool was built to facilitate the annotation process of the data.", "labels": [], "entities": []}, {"text": "C# and MySQL technologies were used to build the tool in order to support rapid development.", "labels": [], "entities": []}, {"text": "The tool enables annotators to partition datasets into events and upload submissions in the JSON form from the gathering Reddit data to each event.", "labels": [], "entities": []}, {"text": "Further the tool allows fora branch view of each submission in the event and facilitates annotation following the SDQC scheme, as well as certainty and evidentiality as presented by.", "labels": [], "entities": [{"text": "certainty", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9942514896392822}]}, {"text": "Any annotation conflicts are highlighted by the tool, which will cause the annotators to discuss and re-annotate the post with a conflict.", "labels": [], "entities": []}, {"text": "A screenshot of the annotation page for During annotation of the first \u223c 500 posts, annotators disagreed upon labels for around 40-50% of posts.", "labels": [], "entities": []}, {"text": "However after the initial annotation work this rate dropped to around 25%.", "labels": [], "entities": []}, {"text": "Annotation conflicts were handled in collaboration between the annotators after annotation of every \u223c 100 posts.", "labels": [], "entities": []}, {"text": "DAST: The result of the data sampling and annotation process is the Danish stance-annotated Reddit dataset (DAST).", "labels": [], "entities": [{"text": "DAST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7874098420143127}, {"text": "Danish stance-annotated Reddit dataset (DAST)", "start_pos": 68, "end_pos": 113, "type": "DATASET", "confidence": 0.9213790467807225}]}, {"text": "The dataset consists of a total of 11 events with 3,007 stance-annotated Reddit posts across 33 submissions with a total of 1,161 branches.", "labels": [], "entities": []}, {"text": "Information on DAST is presented in  The \"querying\" label is rare with a total of 81 annotations out of the 3,007 posts.", "labels": [], "entities": []}, {"text": "The \"supporting\" and \"denying\" labels are almost equally distributed with a total of respectively 273 \"supporting\" and \"300\" denying posts.", "labels": [], "entities": []}, {"text": "The \"commenting\" class is the absolute dominant one, with a total of 2,353 annotations.", "labels": [], "entities": []}, {"text": "Although rumours with known truth value would be optimal for veracity classification, this might reflect reality as the truth value of rumours may stay unverified.", "labels": [], "entities": [{"text": "veracity classification", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.9275768399238586}]}, {"text": "The amount of unverified rumours does however warrant more investigation in order to use all of the rumourous submissions for rumour veracity classification.", "labels": [], "entities": [{"text": "rumour veracity classification", "start_pos": 126, "end_pos": 156, "type": "TASK", "confidence": 0.8919278780619303}]}, {"text": "Further details about the approach to unverified rumours are covered in Section 4.", "labels": [], "entities": []}, {"text": "In total the dataset contains 3,007 Reddit posts distributed across 33 submissions respectively grouped into 16 events.", "labels": [], "entities": []}, {"text": "The tools and annotated corpora (Lillie and Middelboe, 2019a) are openly released with this paper in GDPR-compliant, non-identifying format.", "labels": [], "entities": []}, {"text": "See appendix for data statement).", "labels": [], "entities": []}, {"text": "Most of the related work report results with accuracy as scoring metric, which expresses the ratio of number of correct predictions to the total number of input samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9994090795516968}]}, {"text": "However, this becomes quite uninteresting if the input samples have imbalanced class distributions, which is the case for our dataset.", "labels": [], "entities": []}, {"text": "What is interesting to measure is how well the models are at predicting the correct class labels.", "labels": [], "entities": []}, {"text": "As such, in addition to reporting accuracy we will also use the F 1 scoring metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9762283563613892}, {"text": "F 1 scoring metric", "start_pos": 64, "end_pos": 82, "type": "METRIC", "confidence": 0.9608814865350723}]}, {"text": "In particular we will use an unweighted macro-averaged F 1 score for the case of multi-class classification.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9271275997161865}, {"text": "multi-class classification", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.7480234503746033}]}], "tableCaptions": [{"text": " Table 1: SDQC stance labels per event", "labels": [], "entities": []}, {"text": " Table 5: Stance-only veracity prediction, cross- validated over the Danish-language DAST corpus.", "labels": [], "entities": [{"text": "Stance-only veracity prediction", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6568920811017355}, {"text": "Danish-language DAST corpus", "start_pos": 69, "end_pos": 96, "type": "DATASET", "confidence": 0.8182288209597269}]}, {"text": " Table 6: Veracity prediction from stance only,  training on English/German PHEME rumour dis- cussions and testing on Danish-language DAST.", "labels": [], "entities": [{"text": "Veracity prediction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8218932747840881}, {"text": "English/German PHEME rumour dis- cussions", "start_pos": 61, "end_pos": 102, "type": "TASK", "confidence": 0.5590340122580528}, {"text": "Danish-language DAST", "start_pos": 118, "end_pos": 138, "type": "DATASET", "confidence": 0.7765528857707977}]}, {"text": " Table 7: Training on the PHEME dataset and test- ing on automatic stance labels generated for DAST  with \"Unverified\" rumours treated as \"False\".", "labels": [], "entities": [{"text": "PHEME dataset", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.9440777003765106}, {"text": "DAST", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.5221973657608032}, {"text": "False", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.9838587641716003}]}, {"text": " Table 9: Training and testing on PHEME data, with  \"Unverified\" rumours treated as \"True\".", "labels": [], "entities": [{"text": "PHEME data", "start_pos": 34, "end_pos": 44, "type": "DATASET", "confidence": 0.8639343678951263}]}]}