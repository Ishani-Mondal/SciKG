{"title": [{"text": "The Second Cross-Lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages", "labels": [], "entities": [{"text": "Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages", "start_pos": 38, "end_pos": 135, "type": "TASK", "confidence": 0.7299913721425193}]}], "abstractContent": [{"text": "We describe the Second Multilingual Named Entity Challenge in Slavic languages.", "labels": [], "entities": []}, {"text": "The task is recognizing mentions of named entities in Web documents, their normalization, and cross-lingual linking.", "labels": [], "entities": [{"text": "recognizing mentions of named entities in Web documents", "start_pos": 12, "end_pos": 67, "type": "TASK", "confidence": 0.8668826818466187}, {"text": "cross-lingual linking", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.7500779628753662}]}, {"text": "The Challenge was organized as part of the 7th Balto-Slavic Natural Language Processing Workshop, co-located with the ACL-2019 conference.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing Workshop", "start_pos": 47, "end_pos": 96, "type": "TASK", "confidence": 0.6151097655296326}]}, {"text": "Eight teams participated in the competition, which covered four languages and five entity types.", "labels": [], "entities": []}, {"text": "Performance for the named entity recognition task reached 90% F-measure, much higher than reported in the first edition of the Challenge.", "labels": [], "entities": [{"text": "named entity recognition task", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.7172302901744843}, {"text": "F-measure", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9996732473373413}]}, {"text": "Seven teams covered all four languages, and five teams participated in the cross-lingual entity linking task.", "labels": [], "entities": [{"text": "cross-lingual entity linking task", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.7585266083478928}]}, {"text": "Detailed evaluation information is available on the shared task web page.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to rich inflection and derivation, free word order, and other morphological and syntactic phenomena exhibited by Slavic languages, analysis of named entities (NEs) in these languages poses a challenging problem.", "labels": [], "entities": [{"text": "analysis of named entities (NEs)", "start_pos": 135, "end_pos": 167, "type": "TASK", "confidence": 0.8511210594858442}]}, {"text": "Fostering research on detection and normalization of NEs-and on the closely related problem of cross-lingual, crossdocument entity linking-is of paramount importance for improving multilingual and cross-lingual information access in these languages.", "labels": [], "entities": [{"text": "detection and normalization of NEs-and", "start_pos": 22, "end_pos": 60, "type": "TASK", "confidence": 0.7123192131519318}]}, {"text": "This paper describes the Second Shared Task on multilingual NE recognition (NER), which aims at addressing these problems in a systematic way.", "labels": [], "entities": [{"text": "NE recognition (NER)", "start_pos": 60, "end_pos": 80, "type": "TASK", "confidence": 0.8963092565536499}]}, {"text": "The shared task was organized in the context of the 7th Balto-Slavic Natural Language Processing Workshop co-located with the ACL 2019 conference.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing Workshop", "start_pos": 56, "end_pos": 105, "type": "TASK", "confidence": 0.5984510898590087}, {"text": "ACL 2019 conference", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.7297131021817526}]}, {"text": "The task covers four languages-Bulgarian, Czech, Polish and Russian-and five types of NE: person, location, organization, product, and event.", "labels": [], "entities": [{"text": "NE", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9379903674125671}]}, {"text": "The input text collection consists of documents collected from the Web, each collection centered on a certain \"focal\" entity.", "labels": [], "entities": []}, {"text": "The rationale of such a setup is to foster the development of \"all-round\" NER and cross-lingual entity linking solutions, which are not tailored to specific, narrow domains.", "labels": [], "entities": []}, {"text": "This paper also serves as an introduction and a guide for researchers wishing to explore these problems using the training and test data.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 3 describes the task; Section 4 describes the annotation of the dataset.", "labels": [], "entities": []}, {"text": "The evaluation methodology is introduced in Section 5.", "labels": [], "entities": []}, {"text": "Participant systems are described in Section 6 and the results obtained by these systems are presented in Section 7.", "labels": [], "entities": []}, {"text": "Conclusions and lessons learned are discussed in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The NER task (exact case-insensitive matching) and Name Normalization (or \"lemmatization\") were evaluated in terms of precision, recall, and F1-measure.", "labels": [], "entities": [{"text": "NER task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6313410699367523}, {"text": "Name Normalization", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.6667171865701675}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9995232820510864}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9994025230407715}, {"text": "F1-measure", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9983083009719849}]}, {"text": "For NER, two types of evaluations were carried out: \u2022 Relaxed: An entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless of whether the extracted mention is in base form); \u2022 Strict: The system response should include exactly one annotation for each unique form of a named mention of an entity in a given document, i.e., identifying all variants of an entity is required.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8578850030899048}, {"text": "Relaxed", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9795280694961548}]}, {"text": "In relaxed evaluation we additionally distinguish between exact and partial matching: in the latter case, an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one partial match of a named mention of this entity.", "labels": [], "entities": []}, {"text": "We evaluate systems at several levels of granularity: we measure performance for (a) all NE types and all languages, (b) each given NE type and all languages, (c) all NE types for each language, and (d) each given NE type per language.", "labels": [], "entities": []}, {"text": "In the name normalization task, we take into account only correctly recognized entity mentions and only those that were normalized (on both the annotation and system's sides).", "labels": [], "entities": [{"text": "name normalization task", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.8660386403401693}]}, {"text": "Formally, let N correct denote the number of all correctly recognized entity mentions for which the system returned a correct base form.", "labels": [], "entities": []}, {"text": "Let N key denote the number of all normalized entity mentions in the gold-standard answer key and N response denote the number of all normalized entity mentions in the system's response.", "labels": [], "entities": []}, {"text": "We define precision and recall for the name normalization task as: In evaluating document-level, single-language and cross-lingual entity linking we adopted the Link-Based Entity-Aware metric (LEA), which considers how important the entity is and how well it is resolved.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9990919828414917}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992436170578003}, {"text": "name normalization task", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8454477985699972}, {"text": "cross-lingual entity linking", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.660753051439921}, {"text": "Link-Based Entity-Aware metric (LEA)", "start_pos": 161, "end_pos": 197, "type": "METRIC", "confidence": 0.6468710501988729}]}, {"text": "LEA is defined as follows.", "labels": [], "entities": [{"text": "LEA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.6857225894927979}]}, {"text": "Let K = {k 1 , k 2 , . .", "labels": [], "entities": []}, {"text": ", k |K| } denote the set of key entities and R = {r 1 , r 2 , . .", "labels": [], "entities": []}, {"text": ", r |R| } the set of response entities, i.e., k i \u2208 K (r i \u2208 R) stand for set of mentions of the same entity in the key entity set (response entity set).", "labels": [], "entities": []}, {"text": "LEA recall and precision are then defined as follows: where imp and res denote the measure of importance and the resolution score for an entity, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.7290179133415222}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9991694688796997}, {"text": "resolution score", "start_pos": 113, "end_pos": 129, "type": "METRIC", "confidence": 0.9668105244636536}]}, {"text": "In our setting, we define imp(e) = log 2 |e| for an entity e (in K or R), |e| is the number of mentions of e-i.e., the more mentions an entity has the more important it is.", "labels": [], "entities": []}, {"text": "To avoid biasing the importance of the more frequent entities log is used.", "labels": [], "entities": []}, {"text": "The resolution score of key entity k i is computed as the fraction of correctly resolved coreference links of k i : where link (e) = (|e| \u00d7 (|e| \u2212 1))/2 is the number of unique co-reference links in e.", "labels": [], "entities": [{"text": "resolution score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9702803194522858}]}, {"text": "For each k i , LEA checks all response entities to check whether they are partial matches fork i . Analogously, the resolution score of response entity r i is computed as the fraction of co-reference links in r i that are extracted correctly: LEA brings several benefits.", "labels": [], "entities": [{"text": "resolution score", "start_pos": 116, "end_pos": 132, "type": "METRIC", "confidence": 0.967309296131134}]}, {"text": "For example, LEA considers resolved co-reference relations instead of resolved mentions and has more discriminative power than other metrics for co-reference resolution (.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.7457073032855988}]}, {"text": "It is important to note at this stage that the evaluation was carried out in \"case-insensitive\" mode: all named mentions in system response and test corpora were lower-cased.", "labels": [], "entities": []}, {"text": "Five teams submitted results for cross-lingual entity linking.", "labels": [], "entities": [{"text": "cross-lingual entity linking", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6981672048568726}]}, {"text": "The best results for each team, averaged across two corpora, are presented in, and in  The best performing model, IIUWR.PL, yields Fmeasure 45%.", "labels": [], "entities": [{"text": "IIUWR.PL", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.782284140586853}, {"text": "Fmeasure", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9985491633415222}]}, {"text": "As seen from the plot, for this task it is harder to balance recall and precision: the first two models obtain much higher precision, while the last three obtain much higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9993564486503601}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995213747024536}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9982352256774902}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9972461462020874}]}, {"text": "The two best-performing models used rule-based entity linking.", "labels": [], "entities": [{"text": "rule-based entity linking", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6311454872290293}]}, {"text": "Note that in our setting the performance on entity linking depends on performance on name recognition and normalization: a system had to link entities that it extracted from documents upstream, rather than link a correct set of entities.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7128022760152817}, {"text": "name recognition", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.757996141910553}]}, {"text": "present the F-measure for all tasks, split by language, for the RYANAIR and NORD STREAM datasets; shows performance on the final phase-cross-lingual entity linking.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9970867037773132}, {"text": "RYANAIR", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.6614817976951599}, {"text": "NORD STREAM datasets", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.7501380741596222}]}, {"text": "We show one top-performing model for each team.", "labels": [], "entities": []}, {"text": "For recognition, we present only the relaxed evaluation, since results obtained on the three evaluation schemes are correlated, as can be seen from.", "labels": [], "entities": [{"text": "recognition", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9773650765419006}]}, {"text": "The tables indicate that the test corpora present approximately the same level of difficulty for the participating systems, since the values in both tables are similar.", "labels": [], "entities": []}, {"text": "The only exception is singlelanguage document linking, which seems to be much harder for the RYANAIR dataset, especially for Russian.", "labels": [], "entities": [{"text": "singlelanguage document linking", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6066502134005228}, {"text": "RYANAIR dataset", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9026834070682526}]}, {"text": "This needs to be investigated further.", "labels": [], "entities": []}, {"text": "In we present the results of the evaluation by entity type.", "labels": [], "entities": []}, {"text": "As seen in the table, performance was higher overall for LOC and PER, and substantially lower for ORG and PRO, which corresponds with our findings from the First shared task, where ORG and MISC were the most problematic categories (.", "labels": [], "entities": [{"text": "ORG", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9845313429832458}]}, {"text": "The PRO category also exhibits higher variation across languages and corpora than other categories, which might point to some annotation artefacts.", "labels": [], "entities": []}, {"text": "The results for the EVT category are less informative, since there are few examples of this category in the dataset, as seen in", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the training and test datasets.", "labels": [], "entities": []}, {"text": " Table 3: F-measure results for the RYANAIR corpus", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.985313355922699}, {"text": "RYANAIR", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.6099762320518494}]}, {"text": " Table 4: Evaluation results (F-measure) for the NORD STREAM 2 corpus", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9970415234565735}, {"text": "NORD STREAM 2", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.5112966100374857}]}, {"text": " Table 5: Recognition F-measure (relaxed partial) by  entity type-best-performing systems for each lan- guage.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.7420377731323242}]}]}