{"title": [], "abstractContent": [{"text": "Generic responses frequently generated by neural models area critical problem for user engagement in dialogue systems.", "labels": [], "entities": []}, {"text": "For a more engaging chitchat experience, we propose a response generation model motivated by the interpersonal process model for intimacy.", "labels": [], "entities": [{"text": "response generation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.753568708896637}]}, {"text": "It generates responsive and self-expressive replies, which are implemented as domain-awareness and sentiment-richness, respectively.", "labels": [], "entities": []}, {"text": "Experiments empirically confirmed that our model outperformed the sequence-to-sequence model; 68.1% of our responses were domain-aware with sentiment polarities, which was only 2.7% for responses generated by the sequence-to-sequence model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialogue systems that conduct non-goal-oriented chat, i.e., chit-chat, is an active research area.", "labels": [], "entities": []}, {"text": "The sequence-to-sequence model (SEQ2SEQ) () is commonly used for implementation, however, recent studies, e.g.,), point out that SEQ2SEQ frequently generates overly generic responses.", "labels": [], "entities": []}, {"text": "Among different approaches to address this problem, previous studies propose to generate more engaging responses by reacting to topics in users' utterances ( or embodying emotions (.", "labels": [], "entities": []}, {"text": "Herein we make a step further to generate responsive and self-expressive replies simultaneously.", "labels": [], "entities": []}, {"text": "The interpersonal process model for intimacy ( indicates that conversational responsiveness, i.e., showing concern for what was said, and self-expression, i.e., sharing thoughts and feelings, are primary factors to create intimacy.", "labels": [], "entities": []}, {"text": "Motivated by this theory, we believe that the con- In this study, we focus on single-turn conversations, i.e., generating a response to a single utterance from the user.", "labels": [], "entities": []}, {"text": "versational responsiveness and self-expression are also valid fora dialogue system to generate engaging responses.", "labels": [], "entities": []}, {"text": "We implement the conversational responsiveness as domain-awareness because it effectively conveys an impression that the dialogue agent is listening to the user by responding about mentioned topics.", "labels": [], "entities": []}, {"text": "Also, we implement the self-expression as sentiment-richness by representing sentiment polarity to generate subjective responses with feelings.", "labels": [], "entities": []}, {"text": "Specifically, the encoder predicts the domain of a user's utterance and integrates domain and utterance representations to tell the decoder the target domain explicitly.", "labels": [], "entities": []}, {"text": "Then the decoder embodies sentiment polarity in its generation process.", "labels": [], "entities": [{"text": "sentiment polarity", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.7773682475090027}]}, {"text": "shows real responses generated by our model.", "labels": [], "entities": []}, {"text": "You may find that our responses react to the domains of input utterances while showing salient sentiments.", "labels": [], "entities": []}, {"text": "On the other hand, SEQ2SEQ ends up generating generic responses.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first study that simultaneously achieved both domain- aware and sentiment-rich response generation.", "labels": [], "entities": [{"text": "sentiment-rich response generation", "start_pos": 106, "end_pos": 140, "type": "TASK", "confidence": 0.7410717606544495}]}, {"text": "First, we achieve these features in a simple architecture integrating existing methods on top of SEQ2SEQ in order to make it easily reproducible in existing dialogue systems.", "labels": [], "entities": []}, {"text": "Second, our model utilizes fine-tuning to compensate for the training data scarcity, which is essential because there is a limited amount of domain-dependent and sentiment-rich dialogues.", "labels": [], "entities": []}, {"text": "Our codes and scripts are publicly available.", "labels": [], "entities": []}, {"text": "Evaluation results empirically confirmed that our model significantly outperformed SEQ2SEQ from the human perspective.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.6913760304450989}]}, {"text": "Annotators judged that responses generated by our model are consistent with the utterances' domains and show salient sentiments for 89% and 72% of cases while preserving fluency and consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 182, "end_pos": 193, "type": "METRIC", "confidence": 0.9664249420166016}]}, {"text": "Furthermore, they judged 68.1% responses by our model as both domain-aware and sentiment-rich, which was only 2.7% for responses by SEQ2SEQ.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.934836745262146}]}], "datasetContent": [{"text": "Because the effectiveness of each component for embodying emotions have been evaluated in (, we focus on evaluating whether both domain-awareness and sentiment-richness are achieved simultaneously by our model compared to SEQ2SEQ.", "labels": [], "entities": []}, {"text": "Because each utterance has many appropriate responses, an automatic evaluation scheme has yet to be established.", "labels": [], "entities": []}, {"text": "To assess the quality of the generated responses from the human perspective, we designed two evaluation tasks.", "labels": [], "entities": []}, {"text": "Task 1 evaluates the overall quality of our model compared to SEQ2SEQ from the perspectives of domain-awareness and sentiment-richness.", "labels": [], "entities": []}, {"text": "Task 2 evaluates if an intended sentiment is embodied as desired without being affected by domainawareness.", "labels": [], "entities": []}, {"text": "We recruited five graduate students majoring in computer science that are Japanese native speakers (hereafter called annotators).", "labels": [], "entities": []}, {"text": "After an instruction session to explain judgment standards, they annotated Task 1 and Task 2.", "labels": [], "entities": []}, {"text": "As a token of appreciation, each annotator received a small stipend.", "labels": [], "entities": []}, {"text": "Test Set Creation To exclude external factors, e.g., word segmentation failures, that may affect the evaluation results, we manually created a test set consisting of 300 utterances in the baseball domain and another 300 utterances in the Pok\u00e9mon Go domain.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6979288905858994}, {"text": "Pok\u00e9mon Go domain", "start_pos": 238, "end_pos": 255, "type": "DATASET", "confidence": 0.8617779413859049}]}, {"text": "First, we crawled new conversational pairs from the same Facebook Groups from November to December 2017.", "labels": [], "entities": []}, {"text": "Next, we manually excluded conversations in the general domain (e.g., greetings).", "labels": [], "entities": []}, {"text": "We then cleaned sentences in the same manner with the general and in-domain corpora.", "labels": [], "entities": []}, {"text": "Besides, we manually replaced OOV words within vocabulary words that preserve the original meanings of sentences.", "labels": [], "entities": []}, {"text": "Slang and uncommon expressions were also manually converted to standard expressions to avoid impacting the accuracy of word segmentation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9983698725700378}, {"text": "word segmentation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7095464468002319}]}, {"text": "Half of the test set (150 conversations for each domain) was used for Task 1 and the other half was used for Task 2.", "labels": [], "entities": []}, {"text": "Note that all annotators annotated the same conversations, in total 600 pairs of utterances and responses.", "labels": [], "entities": []}, {"text": "Task 1: Overall Evaluation Annotators judged triples of an input utterance and responses by our model and by SEQ2SEQ.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 109, "end_pos": 116, "type": "DATASET", "confidence": 0.8743640780448914}]}, {"text": "The order of responses was randomly shuffled to ensure a fair evaluation.", "labels": [], "entities": []}, {"text": "Annotators assessed the following aspects: \u2022 Fluency: Annotators judged if a response is fluent and at an acceptable level to understand its meaning (1 = fluent, 0 = influent).", "labels": [], "entities": [{"text": "Fluency", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9482938051223755}]}, {"text": "\u2022 Consistency: Annotators evaluated whether a response is semantically consistent with the utterance (1 = consistent, 0 = inconsistent).", "labels": [], "entities": []}, {"text": "Generic responses can be regarded as consistent if they are acceptable forgiven utterances.", "labels": [], "entities": []}, {"text": "Responses judged as influent are automatically annotated as inconsistent.", "labels": [], "entities": []}, {"text": "\u2022 Domain-awareness: Annotators compared the two responses and determined which one better matched the domain of the input utterance (1 = model that generated the better response, 0 = the other model).", "labels": [], "entities": []}, {"text": "\u2022 Sentiment-richness: Annotators compared the two responses and determined one showing salient sentiments like Domain-awareness annotation.", "labels": [], "entities": []}, {"text": "Only positive or negative responses were considered for our model.", "labels": [], "entities": []}, {"text": "For Domain-awareness and Sentiment-richness, we conduct a pairwise comparison of our model and SEQ2SEQ, which enables reliable judgments for subjective annotations (, rather than independently judging different models.", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.661916196346283}]}, {"text": "Task 2: Evaluation of Sentiment Control Our model takes a sentiment label that is desired to be expressed in a generated response as input, which we refer to as intended sentiment.", "labels": [], "entities": [{"text": "Evaluation of Sentiment Control", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.7170128971338272}]}, {"text": "This task evaluates if such an intended sentiment is embodied in a response by comparing the intended sentiment and a sentiment that annotators perceive in practice.", "labels": [], "entities": []}, {"text": "Annotators were shown a pair of input utterance and generated response by our model, and then asked to judge if the response was positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "We evaluated the agreement between the intended and perceived sentiments.", "labels": [], "entities": []}, {"text": "As an automatic evaluation measure, we computed the BLEU score () following evaluations in (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9856853485107422}]}, {"text": "Our model achieved the higher BLEU score (1.54) than SEQ2SEQ (1.39).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9804486036300659}, {"text": "SEQ2SEQ", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.4686238765716553}]}, {"text": "However, as discussed in (, current automatic evaluation measures show either weak or no correlation with human judgements, or worse, they tend to favor generic responses.", "labels": [], "entities": []}, {"text": "Hence, we focus on human evaluation in the following.", "labels": [], "entities": []}, {"text": "First of all, the agreement level of annotations is examined based on Fleiss' \u03ba.", "labels": [], "entities": []}, {"text": "All annotations have reasonable agreements (\u03ba \u2265 0.37) except the annotation of fluency for SEQ2SEQ whose \u03ba value is as low as 0.21 (all the \u03ba values are shown in Sec. C).", "labels": [], "entities": [{"text": "SEQ2SEQ", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.837741494178772}]}, {"text": "This phenomenon maybe because SEQ2SEQ tends to output generic responses that are less dependent on the utterances, making judgments difficult due to the limited clues to evaluate fluency.", "labels": [], "entities": []}, {"text": "shows the macro-averages and the 95% confidence intervals of the scores obtained by the Metrics SEQ2SEQ Our model Fluency 0.995 \u00b1 0.006 0.955 \u00b1 0.023 Consistency 0.773 \u00b1 0.094 0.753 \u00b1 0.127 Domainawareness 0.109 \u00b1 0.044 0.890 \u00b1 0.044", "labels": [], "entities": [{"text": "Fluency", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.982550323009491}]}], "tableCaptions": [{"text": " Table 4: Detailed statistics of training data", "labels": [], "entities": []}, {"text": " Table 5: Fleiss' \u03ba on annotation results", "labels": [], "entities": []}]}