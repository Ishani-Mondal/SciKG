{"title": [{"text": "Noisy Parallel Corpus Filtering through Projected Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a very simple method for parallel text cleaning of low-resource languages, based on projection of word embeddings trained on large monolingual corpora in high-resource languages.", "labels": [], "entities": [{"text": "parallel text cleaning", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6212876538435618}]}, {"text": "In spite of its simplicity, we approach the strong baseline system in the downstream machine translation evaluation.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 85, "end_pos": 115, "type": "TASK", "confidence": 0.8067945043245951}]}], "introductionContent": [{"text": "With the advent of web-scale parallel text mining, quality estimation and filtering is becoming an increasingly important step in multilingual NLP.", "labels": [], "entities": [{"text": "text mining", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.663717970252037}, {"text": "quality estimation", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.5983995646238327}]}, {"text": "Existing methods focus on languages with relatively large amounts of parallel text available, but scaling down to languages with limited amounts of parallel text poses new challenges.", "labels": [], "entities": []}, {"text": "We present a method based on projecting word embeddings learned from a monolingual corpus in a highresource language, to the target low-resource language through whatever parallel text is available.", "labels": [], "entities": []}, {"text": "The goal of participants in the WMT 2019 parallel corpus filtering shared task is to select the 5 million words of parallel sentences producing the highest-quality machine translation system, given a set of automatically crawled sentence candidates of varying quality.", "labels": [], "entities": [{"text": "WMT 2019 parallel corpus filtering shared task", "start_pos": 32, "end_pos": 78, "type": "TASK", "confidence": 0.7644511801855904}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7434803247451782}]}, {"text": "It is the continuation of the last year's task (, except that this year two low-resource languages are used: Nepali and Sinhalese.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word and sentences counts of the \"clean\" par- allel text", "labels": [], "entities": []}, {"text": " Table 2: Result of pre-filtering the \"clean\" parallel data.", "labels": [], "entities": []}, {"text": " Table 3: Result of pre-filtering the noisy data.", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9027109742164612}]}, {"text": " Table 5: Word and sentence counts in the 1 million and 5 million sub-samples according to our model. Numbers  in parenthesis refer to the counts of the baseline system (", "labels": [], "entities": []}]}