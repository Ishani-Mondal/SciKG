{"title": [], "abstractContent": [], "introductionContent": [{"text": "This paper compares the accuracy of Authorship Verification (AV) in five Modern Standard Arabic (MSA) genres using four popular distance measures: Manhattan Distance, Canberra Distance, Cosine Distance, and Jaccard distance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994750618934631}, {"text": "Authorship Verification (AV)", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.4710021436214447}, {"text": "Canberra Distance", "start_pos": 167, "end_pos": 184, "type": "DATASET", "confidence": 0.5988254547119141}]}, {"text": "The genres in question are fiction and non-fiction books, and articles on economics, politics, and newspaper columns.", "labels": [], "entities": []}, {"text": "Authorship Verification (AV) is a type of authorship analysis problem that addresses the question of whether a question document is written by a known author, given a corpus of authentic documents known to be written by that author.", "labels": [], "entities": [{"text": "Authorship Verification (AV)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.762956577539444}, {"text": "authorship analysis", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.6987779289484024}]}, {"text": "AV is often compared to Authorship Attribution (AA), where there is a set of known candidate authors, and the task is to determine which one of them is the author of the question document.", "labels": [], "entities": [{"text": "Authorship Attribution (AA)", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.786178606748581}]}, {"text": "Both AV and AA are relevant in the areas of corpus linguistics, stylistic and literary analysis, Digital Humanities, and forensic linguistics.", "labels": [], "entities": [{"text": "AV", "start_pos": 5, "end_pos": 7, "type": "METRIC", "confidence": 0.814981997013092}, {"text": "AA", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9327639937400818}, {"text": "stylistic and literary analysis", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.5943236872553825}, {"text": "forensic linguistics", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.7687405645847321}]}, {"text": "This paper is organized as follows: section 2 gives an overview of relevant literature and outlines the research question.", "labels": [], "entities": []}, {"text": "Section 3 describes the corpus used and feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7474561333656311}]}, {"text": "Section 4 outlines the Authorship Verification method and distance measured used in the experiments.", "labels": [], "entities": [{"text": "Authorship Verification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.4612238258123398}, {"text": "distance", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9774755239486694}]}, {"text": "The results are described and discussed in sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Example of features extracted from an  input word.", "labels": [], "entities": []}, {"text": " Table 4: Best performing feature/distance measure  per domain.", "labels": [], "entities": []}]}