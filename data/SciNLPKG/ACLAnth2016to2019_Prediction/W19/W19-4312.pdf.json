{"title": [{"text": "Towards Robust Named Entity Recognition for Historic German", "labels": [], "entities": [{"text": "Towards Robust Named Entity Recognition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6580891489982605}, {"text": "Historic German", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.6411179900169373}]}], "abstractContent": [{"text": "Recent advances in language modeling using deep neural networks have shown that these models learn representations, that vary with the network depth from morphology to semantic relationships like co-reference.", "labels": [], "entities": []}, {"text": "We apply pre-trained language models to low-resource named entity recognition for Historic Ger-man.", "labels": [], "entities": [{"text": "low-resource named entity recognition", "start_pos": 40, "end_pos": 77, "type": "TASK", "confidence": 0.6607871055603027}]}, {"text": "We show on a series of experiments that character-based pre-trained language models do not run into trouble when faced with low-resource datasets.", "labels": [], "entities": []}, {"text": "Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6%.", "labels": [], "entities": [{"text": "F1 score performance", "start_pos": 131, "end_pos": 151, "type": "METRIC", "confidence": 0.9725511074066162}]}, {"text": "Our pre-trained language and NER models are publicly available 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Named entity recognition (NER) is a central component in natural language processing tasks.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.791195367773374}]}, {"text": "Identifying named entities is a key part in systems e.g. for question answering or entity linking.", "labels": [], "entities": [{"text": "Identifying named entities", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.9126229286193848}, {"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9113056361675262}, {"text": "entity linking", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.7250083982944489}]}, {"text": "Traditionally, NER systems are built using conditional random fields (CRFs).", "labels": [], "entities": []}, {"text": "Recent systems are using neural network architectures like bidirectional LSTM with a CRF-layer ontop and pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "Pre-trained word embeddings have been shown to be of great use for downstream NLP tasks ().", "labels": [], "entities": []}, {"text": "Many recently proposed approaches go beyond these pre-trained embeddings.", "labels": [], "entities": []}, {"text": "Recent works have proposed methods that produce different representations for the same word depending on its contextual usage (.", "labels": [], "entities": []}, {"text": "These methods have shown to be very powerful in the fields of named entity recognition, coreference resolution, part-ofspeech tagging and question answering, especially in combination with classic word embeddings.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6644685864448547}, {"text": "coreference resolution", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.9577218890190125}, {"text": "part-ofspeech tagging", "start_pos": 112, "end_pos": 133, "type": "TASK", "confidence": 0.7974101305007935}, {"text": "question answering", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.9084745049476624}]}, {"text": "Our paper is based on the work of.", "labels": [], "entities": []}, {"text": "They showed how to build a model for German named entity recognition (NER) that performs at the state of the art for both contemporary and historical texts.", "labels": [], "entities": [{"text": "German named entity recognition (NER)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.695270700114114}]}, {"text": "Labeled historical texts for German named entity recognition area low-resource domain.", "labels": [], "entities": [{"text": "German named entity recognition", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.5085276141762733}]}, {"text": "In order to achieve robust state-of-the-art results for historical texts they used transfer-learning with labeled data from other high-resource domains like) or GermEval ().", "labels": [], "entities": []}, {"text": "They showed that using Bi-LSTM with a CRF as the top layer and word embeddings outperforms CRFs with hand-coded features in a big-data situation.", "labels": [], "entities": []}, {"text": "We buildup upon their work and use the same low-resource datasets for Historic German.", "labels": [], "entities": [{"text": "Historic German", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.9181129038333893}]}, {"text": "Furthermore, we show how to achieve new state-ofthe-art results for Historic German named entity recognition by using only unlabeled data via pretrained language models and word embeddings.", "labels": [], "entities": [{"text": "Historic German named entity recognition", "start_pos": 68, "end_pos": 108, "type": "TASK", "confidence": 0.6840209603309632}]}, {"text": "We also introduce a novel language model pretraining objective, that uses only contemporary texts for training to achieve comparable state-ofthe-art results on historical texts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the same two datasets for Historic German as used by.", "labels": [], "entities": [{"text": "Historic German", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.9034053385257721}]}, {"text": "These datasets are based on historical texts that were extracted from the Europeana collection of historical newspapers 3 . The first corpus is the collection of Tyrolean periodicals and newspapers from the Dr Friedrich Temann Library (LFT).", "labels": [], "entities": [{"text": "Europeana collection of historical newspapers", "start_pos": 74, "end_pos": 119, "type": "DATASET", "confidence": 0.9439594745635986}, {"text": "Tyrolean periodicals and newspapers from the Dr Friedrich Temann Library (LFT)", "start_pos": 162, "end_pos": 240, "type": "DATASET", "confidence": 0.718309576694782}]}, {"text": "The LFT corpus consists of approximately 87,000 tokens from 1926.", "labels": [], "entities": [{"text": "LFT corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8590589761734009}]}, {"text": "The second corpus is a collection of Austrian newspaper texts from the Austrian National Library (ONB).", "labels": [], "entities": [{"text": "Austrian newspaper texts from the Austrian National Library (ONB)", "start_pos": 37, "end_pos": 102, "type": "DATASET", "confidence": 0.7890191538767382}]}, {"text": "The ONB corpus consists of approximately 35,000 tokens from texts created between 1710 and 1873.", "labels": [], "entities": [{"text": "ONB corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.970892995595932}]}, {"text": "The tagset includes locations (LOC), organizations (ORG), persons (PER) and the remaining entities as miscellaneous (MISC).", "labels": [], "entities": []}, {"text": "contain an overview of the number of named entities of the two datasets.", "labels": [], "entities": []}, {"text": "No miscellaneous entities (MISC) are found in the ONB dataset and only a few are annotated in the LFT dataset.", "labels": [], "entities": [{"text": "ONB dataset", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9752886593341827}, {"text": "LFT dataset", "start_pos": 98, "end_pos": 109, "type": "DATASET", "confidence": 0.9327580630779266}]}, {"text": "The two corpora pose three challenging problems: they are relatively small compared to contemporary corpora like CoNLL-2003 or GermEval.", "labels": [], "entities": [{"text": "CoNLL-2003", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.93272864818573}, {"text": "GermEval", "start_pos": 127, "end_pos": 135, "type": "DATASET", "confidence": 0.8522866368293762}]}, {"text": "They also have a different language variety (German and Austrian) and they include a high rate of OCR errors 4 since they were originally printed in Gothic type-face (Fraktur), a low resource font, which has not been the main focus of recent OCR research.", "labels": [], "entities": [{"text": "OCR errors", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.8838372528553009}, {"text": "OCR", "start_pos": 242, "end_pos": 245, "type": "TASK", "confidence": 0.9584758877754211}]}, {"text": "In the first experiment we use different types of embeddings on the two datasets: (a) FastText embeddings trained on German Wikipedia articles, (b) FastText embeddings trained on Common Crawl and (c) character embeddings, as proposed by.", "labels": [], "entities": []}, {"text": "We use pre-trained FastText embeddings 5 without subword information, as we found out that subword information could harm performance (0.8 to 1.5%) of our system in some cases.", "labels": [], "entities": []}, {"text": "73.31% (with transfer-learning) 78.56%: Results on LFT and ONB dataset with different configurations.", "labels": [], "entities": [{"text": "ONB dataset", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9271934628486633}]}, {"text": "Wikipedia and Common Crawl are pretrained FastText word embeddings.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.954680860042572}]}, {"text": "The best configurations reported by used Wikipedia or Europeana word embeddings with subword information and character embeddings.", "labels": [], "entities": []}, {"text": "(74.33%), who used transfer-learning with more labeled data.", "labels": [], "entities": []}, {"text": "also shows the same effect for ONB: combining Wikipedia and Common Crawl embeddings leads to 79.46% and adding character embeddings marginally improves the result to 80.48%.", "labels": [], "entities": [{"text": "ONB", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.861299455165863}]}, {"text": "This result is also higher than the reported one by (78.56%).", "labels": [], "entities": []}, {"text": "For the next experiments we train contextualized string embeddings as proposed by shows the temporal overlap for the language model corpora and the datasets used in the downstream task.", "labels": [], "entities": []}, {"text": "There is a huge temporal overlap between the ONB dataset and the WZ corpus, whereas the overlap between the LFT dataset and the HHA corpus is relatively small.", "labels": [], "entities": [{"text": "ONB dataset", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9608172476291656}, {"text": "WZ corpus", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9329832792282104}, {"text": "LFT dataset", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.8893078863620758}, {"text": "HHA corpus", "start_pos": 128, "end_pos": 138, "type": "DATASET", "confidence": 0.9524849355220795}]}, {"text": "Additionally we use the BERT model, that was trained on Wikipedia for 104 languages 6 for comparison.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9945684671401978}]}, {"text": "We perform a per-layer analysis of the multi-lingual BERT model on the development set to find the best layer for our task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9578840136528015}]}, {"text": "For the German language model, we use the same pre-trained language model for German as used in.", "labels": [], "entities": []}, {"text": "This model was trained on various sources (Wikipedia, OPUS) with a training data set size of half a billion tokens.", "labels": [], "entities": [{"text": "OPUS", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8727609515190125}]}, {"text": "shows that the temporal aspect of training data for the language models has deep impact on the performance.", "labels": [], "entities": []}, {"text": "On LFT (1926) the language model trained on the HHA corpus leads to a F1 score of 77.51%, which is anew state-of-the art result on this dataset.", "labels": [], "entities": [{"text": "LFT (1926)", "start_pos": 3, "end_pos": 13, "type": "DATASET", "confidence": 0.9386599361896515}, {"text": "HHA corpus", "start_pos": 48, "end_pos": 58, "type": "DATASET", "confidence": 0.9344514608383179}, {"text": "F1 score", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9865365922451019}]}, {"text": "The result is 3.18% better than the result reported by, which uses transferlearning with more labeled training data.", "labels": [], "entities": []}, {"text": "The language model trained on the WZ corpus (1703-1875) only achieves a F1 score of 75.60%, likely because the time period of the data used for pretraining (19th century) is too far removed from  We also consider the masked language modeling (MLM) objective of.", "labels": [], "entities": [{"text": "WZ corpus (1703-1875)", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9507171034812927}, {"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9896062612533569}, {"text": "masked language modeling (MLM)", "start_pos": 217, "end_pos": 247, "type": "TASK", "confidence": 0.7774842778841654}]}, {"text": "However, this technique cannot be directly used, because they use a subword-based language model, in contrast to our character-based language model.", "labels": [], "entities": []}, {"text": "We introduce a novel masked language modeling technique, synthetic masked language modeling (SMLM) that randomly adds noise during training.", "labels": [], "entities": [{"text": "masked language modeling", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6353207429250082}, {"text": "synthetic masked language modeling (SMLM)", "start_pos": 57, "end_pos": 98, "type": "TASK", "confidence": 0.7834298525537763}]}, {"text": "The main motivation for using SMLM is to transfer a corpus from one domain (e.g. \"clean\" contemporary texts) into another (e.g. \"noisy\" historical texts).", "labels": [], "entities": [{"text": "SMLM", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9889695644378662}]}, {"text": "SMLM uses the vocabulary (characters) from the target domain and injects them into the source domain.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9606481194496155}]}, {"text": "With this technique it is possible to create a synthetic corpus, that \"emulates\" OCR errors or spelling mistakes without having any data from the target domain (except all possible characters as vocabulary).", "labels": [], "entities": []}, {"text": "Furthermore, SMLM can also be seen as a kind of domain adaption.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9833871722221375}]}, {"text": "To use SMLM we extract all vocabulary (characters) from the ONB and LFT datasets.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 7, "end_pos": 11, "type": "TASK", "confidence": 0.9726985692977905}, {"text": "ONB and LFT datasets", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.7241607904434204}]}, {"text": "We refer to these characters as target vocabulary.", "labels": [], "entities": []}, {"text": "Then we obtained a corpus consisting of contemporary texts from Leipzig Corpora Collection (Goldhahn et al., 2012) for German.", "labels": [], "entities": [{"text": "Leipzig Corpora Collection (Goldhahn et al., 2012)", "start_pos": 64, "end_pos": 114, "type": "DATASET", "confidence": 0.9333743453025818}]}, {"text": "The resulting corpus has 388,961,352 tokens.", "labels": [], "entities": []}, {"text": "During training, the following SMLM objective is used: Iterate overall characters in the contemporary corpus.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 31, "end_pos": 35, "type": "TASK", "confidence": 0.9923266172409058}]}, {"text": "Leave the character unchanged in 90% of the time.", "labels": [], "entities": []}, {"text": "For the remaining 10% we employ the following strategy: in 20% of the time replace the character with a masked character, that does not exist in the target vocabulary.", "labels": [], "entities": []}, {"text": "In 80% of the time we randomly replace the character by a symbol from the target vocabulary.", "labels": [], "entities": []}, {"text": "shows that the language model trained with SMLM achieves the second best result on LFT with 77.16%.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 43, "end_pos": 47, "type": "TASK", "confidence": 0.9664300680160522}]}, {"text": "The ONB corpus is more challenging for SMLM, because it includes texts from a totally different time period.", "labels": [], "entities": [{"text": "ONB corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8941770195960999}, {"text": "SMLM", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.9916995763778687}]}, {"text": "SMLM achieves the third best result with a F-Score of 82.15%.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.577235996723175}, {"text": "F-Score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9996498823165894}]}, {"text": "This result is remarkable, because the language model itself has never seen texts from the 18-19th century.", "labels": [], "entities": []}, {"text": "The model was trained on contemporary texts with SMLM only.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 49, "end_pos": 53, "type": "TASK", "confidence": 0.9611126184463501}]}, {"text": "The usage of pre-trained character-based language models boosts performance for both LFT and ONB datasets.", "labels": [], "entities": [{"text": "ONB datasets", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.8559944331645966}]}, {"text": "The results in table 4 show, that the selection of the language model corpus plays an important role: a corpus with a large degree of temporal overlap with the downstream task performs better than corpus with little to no temporal overlap.", "labels": [], "entities": []}, {"text": "In order to compare our trained language models with each other, we measure both the perplexity of the forward language model and the backward language model on the test dataset for LFT and ONB.", "labels": [], "entities": [{"text": "ONB", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.8348401188850403}]}, {"text": "The perplexity for each sentence in the test dataset is calculated and averaged.", "labels": [], "entities": []}, {"text": "The results for LFT and ONB are shown in table 5.", "labels": [], "entities": [{"text": "LFT", "start_pos": 16, "end_pos": 19, "type": "DATASET", "confidence": 0.5288383960723877}, {"text": "ONB", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5441106557846069}]}, {"text": "For all language models (except one) there is a clear correlation between overall perplexity and F1 score on the test dataset: lower perplexity (both for forward and backward language model) yields better performance in terms of the F1 score on the downstream NER tasks.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9870290160179138}, {"text": "F1", "start_pos": 233, "end_pos": 235, "type": "METRIC", "confidence": 0.9987037181854248}]}, {"text": "But this assumption does not hold for the language model that was trained on synthetic data via SMLM objective: The perplexity for this language model (both forward and backward) is relatively high compared to other language models, but the F1 score results are better than some other language models with lower perplexity.", "labels": [], "entities": [{"text": "SMLM", "start_pos": 96, "end_pos": 100, "type": "TASK", "confidence": 0.9821678400039673}, {"text": "F1 score", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9861607849597931}]}, {"text": "This variation can be observed both on LFT and ONB test data.", "labels": [], "entities": [{"text": "ONB test data", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8140528202056885}]}, {"text": "We leave this anomaly here as an open question: Is perplexity a good measure for comparing language models and a useful indicator for their results on downstream tasks?", "labels": [], "entities": []}, {"text": "We train all NER models with IOBES) tagging scheme.", "labels": [], "entities": [{"text": "IOBES", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9001036882400513}]}, {"text": "In the prediction step we convert IOBES tagging scheme to IOB, in order to use the offical CoNLL-2003 evaluation script . For all NER models we train and evaluate 3 runs and report an averaged F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 193, "end_pos": 201, "type": "METRIC", "confidence": 0.9813261032104492}]}], "tableCaptions": [{"text": " Table 1: Number of named entities in ONB dataset.", "labels": [], "entities": [{"text": "ONB dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9771901667118073}]}, {"text": " Table 2: Number of named entities in LFT dataset.", "labels": [], "entities": [{"text": "LFT dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8586075901985168}]}, {"text": " Table 3: Results on LFT and ONB dataset with different configurations. Wikipedia and Common Crawl are pre- trained FastText word embeddings. The best configurations reported by", "labels": [], "entities": [{"text": "ONB dataset", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.8645142018795013}, {"text": "Wikipedia", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9659124612808228}]}, {"text": " Table 5: Averaged perplexity for all sentences in the  test dataset for LFT for all pre-trained language mod- els.", "labels": [], "entities": []}, {"text": " Table 6: Parameters used for language model pre- training.", "labels": [], "entities": []}]}