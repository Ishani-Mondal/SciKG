{"title": [{"text": "Faithful Multimodal Explanation for Visual Question Answering", "labels": [], "entities": [{"text": "Faithful Multimodal Explanation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5947359800338745}, {"text": "Visual Question Answering", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6170410315195719}]}], "abstractContent": [{"text": "AI systems' ability to explain their reasoning is critical to their utility and trustworthiness.", "labels": [], "entities": []}, {"text": "Deep neural networks have enabled significant progress on many challenging problems such as visual question answering (VQA).", "labels": [], "entities": [{"text": "visual question answering (VQA)", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.7766516556342443}]}, {"text": "However , most of them are opaque black boxes with limited explanatory capability.", "labels": [], "entities": []}, {"text": "This paper presents a novel approach to developing a high-performing VQA system that can elucidate its answers with integrated textual and visual explanations that faithfully reflect important aspects of its underlying reasoning process while capturing the style of comprehen-sible human explanations.", "labels": [], "entities": []}, {"text": "Extensive experimental evaluation demonstrates the advantages of this approach compared to competing methods using both automated metrics and human evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep neural networks have made significant progress on visual question answering (VQA), the challenging AI problem of answering naturallanguage questions about an image (.", "labels": [], "entities": [{"text": "visual question answering (VQA)", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.8068091968695322}, {"text": "answering naturallanguage questions about an image", "start_pos": 118, "end_pos": 168, "type": "TASK", "confidence": 0.757208506266276}]}, {"text": "However successful systems () based on deep neural networks are difficult to comprehend because of many layers of abstraction and a large number of parameters.", "labels": [], "entities": []}, {"text": "This makes it hard to develop user trust.", "labels": [], "entities": []}, {"text": "Partly due to the opacity of current deep models, there has been a recent resurgence of interest in explainable AI, systems that can explain their reasoning to human users.", "labels": [], "entities": []}, {"text": "In particular, there has been some recent development of explainable VQA systems (.", "labels": [], "entities": []}, {"text": "One approach to explainable VQA is to generate visual explanations, which highlight image regions that most contributed to the system's answer, as determined by attention mechanisms (Lu Question: What sport is pictured?", "labels": [], "entities": [{"text": "Lu Question: What sport is pictured?", "start_pos": 183, "end_pos": 219, "type": "TASK", "confidence": 0.5071499645709991}]}, {"text": "Explanation: Because the man is riding a wave on a surfboard.", "labels": [], "entities": []}, {"text": "et al., 2016) or gradient analysis (.", "labels": [], "entities": []}, {"text": "However, such simple visualizations do not explain how these regions support the answer.", "labels": [], "entities": []}, {"text": "An alternate approach is to generate a textual explanation, a natural-language sentence that provides reasons for the answer.", "labels": [], "entities": []}, {"text": "Some recent work has generated textual explanations for VQA by training a recurrent neural network (RNN) to directly mimic examples of human explanations.", "labels": [], "entities": []}, {"text": "A multimodal approach that integrates both a visual and textual explanation provides the advantages of both.", "labels": [], "entities": []}, {"text": "Words and phrases in the text can point to relevant regions in the image.", "labels": [], "entities": []}, {"text": "An illustrative explanation generated by our system is shown in 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section experimentally evaluates both the textual and visual aspects of our multimodal explanations, including comparisons to competing methods and ablations that study the impact of the various components of our overall system.", "labels": [], "entities": []}, {"text": "Finally, we present metrics and evaluation for the faithfulness of our explanations.", "labels": [], "entities": []}, {"text": "reports our performance, including ablations.", "labels": [], "entities": []}, {"text": "In particular, \"Justification\" denotes training on the entire or randomly sampled VQA-X dataset and \"Explanation\" denotes 2 Due to duplicated segments, we use a lower threshold.", "labels": [], "entities": [{"text": "VQA-X dataset", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.9380210638046265}, {"text": "Explanation", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9509626626968384}]}, {"text": "training only on the remaining faithful explanations.", "labels": [], "entities": []}, {"text": "We outperform the current state-of-the-art PJ-X model) on all automated metrics by a clear margin with only about half the explanation training data.", "labels": [], "entities": []}, {"text": "This indicates that constructing explanations that faithfully reflect the VQA process can actually generate explanations that match human explanations better than just training to directly match human explanations, possibly by avoiding over-fitting and focusing more on important aspects of the test images.", "labels": [], "entities": []}, {"text": "In this section, we present the evaluations of our model on both visual and multimodal aspects.", "labels": [], "entities": []}, {"text": "Automated Evaluation: As in previous work (, we first used Earth Mover Distance (EMD)) to compare the image regions highlighted in our explanation to image regions highlighted by human judges.", "labels": [], "entities": []}, {"text": "In order to fairly compare to prior results, we resize all the images in the entire test split to 14\u21e514 and adjust the segmentation in the images accordingly using bi-linear interpolation.", "labels": [], "entities": []}, {"text": "Next, we sum up the multiplication of attention values and source identifiers' values in Eq, 2 overtime (t) and assign the accumulated attention weight to each corresponding segmentation region.", "labels": [], "entities": [{"text": "Eq, 2 overtime", "start_pos": 89, "end_pos": 103, "type": "METRIC", "confidence": 0.8045760691165924}]}, {"text": "We then normalize attention weights over the 14 \u21e5 14 resized images to sum to 1, and finally compute the EMD between the normalized attentions and the ground truth.", "labels": [], "entities": [{"text": "EMD", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9990079998970032}]}, {"text": "As shown in the Visual results in, our approach matches human attention maps more  closely than PJ-X ().", "labels": [], "entities": []}, {"text": "We attribute this improvement to the following reasons.", "labels": [], "entities": []}, {"text": "First, our approach uses detailed image segmentation which avoids focusing on background and is much more precise than bounding-box detection.", "labels": [], "entities": [{"text": "image segmentation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7278869599103928}, {"text": "bounding-box detection", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.6944582015275955}]}, {"text": "Second, our visual explanation is focused by textual explanation where the segmented visual objects must be linked to specific words in the textual explanation.", "labels": [], "entities": []}, {"text": "Therefore, the risk of attending to unnecessary objects in the images is significantly reduced.", "labels": [], "entities": []}, {"text": "As a result, we filter out most of the noisy attention in a purely visual explanation like that in PJ-X.", "labels": [], "entities": []}, {"text": "Human Evaluation: We also asked AMT workers to evaluate our final multimodal explanations that link words in the textual explanation directly to segments in the image.", "labels": [], "entities": []}, {"text": "Specifically, we randomly selected 1,000 correctly answered question and asked workers \" How well do the highlighted image regions support the answer to the question?\" and provided them a Likert-scale set of possible answers: \"Very supportive\", \"Supportive\", \"Neutral\", 'Unsupportive\" and \"Completely unsupportive\".", "labels": [], "entities": []}, {"text": "The second task was to evaluate the quality of the links between words and image regions in the explanations.", "labels": [], "entities": []}, {"text": "We asked workers \"How well do the colored image segments highlight the appropriate regions for the corresponding colored words in the explanation?\" with the Like-scale choices: \"Very Well\", \"Well\", \"Neutral\", \"Not Well\", \"Poorly\".", "labels": [], "entities": []}, {"text": "We assign five questions in each AMT HIT with one \"validation\" item to control the HIT's qualities.", "labels": [], "entities": [{"text": "AMT HIT", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.5883857905864716}]}, {"text": "As shown in, in both cases, about 70% of the evaluations are positive and about 45% of them are strongly positive.", "labels": [], "entities": []}, {"text": "This indicates that our multimodal explanations provide good connections among visual explanations, textual explanations, and the VQA process.", "labels": [], "entities": []}, {"text": "presents some sample positively-rated multimodal explanations.", "labels": [], "entities": []}, {"text": "In this section, we measure the faithfulness of our explanations, i.e. how well they reflect the underlying VQA system's reasoning.", "labels": [], "entities": [{"text": "VQA system", "start_pos": 108, "end_pos": 118, "type": "DATASET", "confidence": 0.8785814940929413}]}, {"text": "First, we measured how many words in a generated explanation are actually linked to a visual segmentation in the image.", "labels": [], "entities": []}, {"text": "We analyzed the explanations from 1,000 correctly answered questions from the test data.", "labels": [], "entities": []}, {"text": "On average, our model is able to link 1.6 words in an explanation to an image segment, indicating that the textual explanation is actually grounded in objects detected by our VQA system.", "labels": [], "entities": []}, {"text": "We use the model-agnostic explainer LIME ( to determine the segmented objects that most influenced a particular answer, and measure how well the objects referenced in our explanation match these influential segments.", "labels": [], "entities": [{"text": "LIME", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.976213812828064}]}, {"text": "We regard all the detected visual segments as the \"interpretable\" units used by LIME to explain decisions.", "labels": [], "entities": []}, {"text": "Using these interpretable units, LIME applies LASSO with the regularization path ( to learn a linear model of the local decision boundary around the example to be explained.", "labels": [], "entities": [{"text": "LASSO", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9932215213775635}]}, {"text": "In particular, we collect 256 points around the example by randomly blinding each segment's features with a probability of 0.4.", "labels": [], "entities": []}, {"text": "The highly weighted features in this model are claimed to provide a faithful explanation of the decision on this example (.", "labels": [], "entities": []}, {"text": "The complexity of the explanation is controlled by the number of units, K, that can be used in this linear model.", "labels": [], "entities": []}, {"text": "Using the coefficients w of LIME's weighted linear model, we compare the object segments selected by LIME to the set of objects that are actually linked to words in our explanations.", "labels": [], "entities": []}, {"text": "Specifically, we define our faithfulness metric as: where vi denotes the visual feature of the i-th segmented object and the L denotes the set of explanation-linked objects.", "labels": [], "entities": []}, {"text": "For each object in the LIME explanation, it finds the closest object in our explanation and multiplies its LIME weight by this similarity.", "labels": [], "entities": []}, {"text": "The normalized sum of these matches is used to measure the similarity of the two explanations.", "labels": [], "entities": [{"text": "similarity", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9842355251312256}]}, {"text": "We collect all correctly answered questions in the VQA-X test set, and reports the average score for their explanations using models trained on 15K training explanations with different numbers of interpretable units K.", "labels": [], "entities": [{"text": "VQA-X test set", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9571628570556641}]}, {"text": "The influential objects recognized by LIME match objects that are linked to words in our explanations with an average cosine similarity around 0.7.", "labels": [], "entities": []}, {"text": "This indicates that the explanations are faithfully making reference to visual segmentations that actually influenced the decision of the underlying VQA system.", "labels": [], "entities": []}, {"text": "Also, we observe that training with faithful human explanation outperforms purely mimicking human explanations in terms of our faithful metric, and further enforcing the local faithfulness achieves a better result.: Evaluation of LIME-based faithfulness scores for different numbers of interpretable units K using 15K training explanations.", "labels": [], "entities": []}, {"text": "\"Random\" means the training explanations are randomly sampled from the train set, and \"Filtered\" means the models are trained using the remaining faithful explanations.", "labels": [], "entities": [{"text": "Filtered", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.975634753704071}]}, {"text": "We also evaluated the consistency between the Grad-CAM visual explanation vectors from the textual explanation and the predicted answer using the faithful score S f defined in Eq.", "labels": [], "entities": [{"text": "consistency", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9749720692634583}]}, {"text": "7. reports the results from using filtered verses randomly sampled explanations for training.", "labels": [], "entities": []}, {"text": "We observe that with faithful human explanations, the average faithfulness evaluation score increases 7% over training with randomly sampled explanations.", "labels": [], "entities": [{"text": "faithfulness evaluation score", "start_pos": 62, "end_pos": 91, "type": "METRIC", "confidence": 0.8963257670402527}]}, {"text": "Moreover, with the faithfulness loss L f , the model can better align the visual explanation for the textual explanation with that for the predicted answer, leading to a further 11% increase.", "labels": [], "entities": []}, {"text": "We also report the distribution of the generated explanations' cosine similarity between their visual explanation and the visual explanation of the: Average faithfulness evaluation score using different explanations models.", "labels": [], "entities": []}, {"text": "\"Random\" means the training explanations are randomly sampled from the train set, and \"Filtered\" means the models are trained using the remaining faithful explanations.", "labels": [], "entities": [{"text": "Filtered", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.975634753704071}]}, {"text": "The fraction of the faithfulness scores between the interval [0.0, 0.1] is significantly decreased by over 17% when using the faithful human explanations for supervision and further enforcing the local faithfulness with the faithfulness loss, L f .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of LIME-based faithfulness scores  for different numbers of interpretable units K using  15K training explanations. \"Random\" means the train- ing explanations are randomly sampled from the train  set, and \"Filtered\" means the models are trained using  the remaining faithful explanations.", "labels": [], "entities": []}]}