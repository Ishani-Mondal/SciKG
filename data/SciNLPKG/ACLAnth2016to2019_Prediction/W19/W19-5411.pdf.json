{"title": [{"text": "SOURCE: SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation", "labels": [], "entities": [{"text": "Machine Translation Quality Estimation", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.8651452213525772}]}], "abstractContent": [{"text": "Quality estimation (QE) of machine translation (MT) systems is a task of growing importance.", "labels": [], "entities": [{"text": "Quality estimation (QE) of machine translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.760951578617096}]}, {"text": "It reduces the cost of post-editing, allowing machine-translated text to be used informal occasions.", "labels": [], "entities": []}, {"text": "In this work, we describe our submission system in WMT 2019 sentence-level QE task.", "labels": [], "entities": [{"text": "WMT 2019 sentence-level QE task", "start_pos": 51, "end_pos": 82, "type": "DATASET", "confidence": 0.7920246720314026}]}, {"text": "We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy.", "labels": [], "entities": []}, {"text": "The strategy is similar to ELMo, but additionally conditions on source sentences.", "labels": [], "entities": []}, {"text": "Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE.", "labels": [], "entities": [{"text": "WMT QE dataset", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.9245109756787618}, {"text": "QE", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9272408485412598}]}, {"text": "In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.", "labels": [], "entities": [{"text": "WMT-2019 QE task", "start_pos": 3, "end_pos": 19, "type": "DATASET", "confidence": 0.868053674697876}, {"text": "En-De NMT dataset", "start_pos": 62, "end_pos": 79, "type": "DATASET", "confidence": 0.9605031212170919}, {"text": "En-Ru NMT dataset", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9699281255404154}]}], "introductionContent": [{"text": "The quality of machine translation systems have been significantly improved over the past few years (, especially with the development of neural machine translation (NMT) models).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7775571048259735}, {"text": "neural machine translation (NMT)", "start_pos": 138, "end_pos": 170, "type": "TASK", "confidence": 0.8383401930332184}]}, {"text": "Despite such inspiring improvements, some machine translated texts are still error-prone and unreliable compared to those by professional humans.", "labels": [], "entities": []}, {"text": "It is often desirable to have human experts perform post-editing on machine-translated text to achieve a balance between cost and correctness.", "labels": [], "entities": []}, {"text": "Correspondingly, we may also want to develop automatic quality estimation systems to judge the quality of machine translation outputs, leading to the development of the Machine Translation Quality Estimation task.", "labels": [], "entities": [{"text": "machine translation outputs", "start_pos": 106, "end_pos": 133, "type": "TASK", "confidence": 0.7564354638258616}, {"text": "Machine Translation Quality Estimation task", "start_pos": 169, "end_pos": 212, "type": "TASK", "confidence": 0.8598429799079895}]}, {"text": "The task of QE aims to evaluate the output of a machine translation system without access to reference translations.", "labels": [], "entities": [{"text": "QE", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.8591355681419373}, {"text": "machine translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6856623142957687}]}, {"text": "It would allow human experts to concentrate * equal contribution on translations that are estimated of low-quality, further reducing post-editing cost.", "labels": [], "entities": []}, {"text": "In this work, we focus on sentence-level QE and describe our submission to the WMT 2019 QE task.", "labels": [], "entities": [{"text": "QE", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.6523493528366089}, {"text": "WMT 2019 QE task", "start_pos": 79, "end_pos": 95, "type": "TASK", "confidence": 0.5311488136649132}]}, {"text": "Sentence-level QE aims to predict a score for the entire source-translation pair that indicates the effort required for further post-editing.", "labels": [], "entities": [{"text": "Sentence-level QE", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6692613959312439}]}, {"text": "The goals of the task are two-fold: 1) to predict the required post-editing cost, measured in HTER (); 2) to rank all sentence pairs in descending translation quality.", "labels": [], "entities": [{"text": "HTER", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.8349612951278687}]}, {"text": "In previous works, including the participating systems in previous WMT shared tasks, there have been various methods to tackle this problem.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8212793668111166}]}, {"text": "Traditional linear models are based on handcrafted features, while recent state-of-the-art systems adopt end-to-end neural models.", "labels": [], "entities": []}, {"text": "The neural systems are usually composed of two modules: the bottom part is an MT-like source-target encoding model pre-trained with large parallel corpora, stacked with a top-level QE scorer based on the neural features extracted by the bottom model.", "labels": [], "entities": [{"text": "MT-like source-target encoding", "start_pos": 78, "end_pos": 108, "type": "TASK", "confidence": 0.6794869502385458}]}, {"text": "Especially,  adopted the \"Bilingual Expert\" model ) for pre-training the bottom model and obtained several best results in WMT 2018.", "labels": [], "entities": [{"text": "WMT", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.8475556969642639}]}, {"text": "In this work, we improve the \"Bilingual Expert\" model with a SOURceConditional ELMo-style (SOURCE) strategy: instead of predicting target words based on contexts from both sides, we train two conditioned language (translation) models, each restricted to context from one side only.", "labels": [], "entities": []}, {"text": "This harder setting may force the model to condition more on the source.", "labels": [], "entities": []}, {"text": "Experiments show that this strategy can bring improvements for QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.8701744675636292}]}, {"text": "Gold HTER Score: The architecture of our QE system, which consists of two modules: 1) the MT Module encodes the bilingual information and can be pre-trained with large parallel data, 2) the QE Module adopts the source and target representations from the MT Module and further encodes those information followed by a final linear layer for QE scoring.", "labels": [], "entities": [{"text": "Gold HTER Score", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.6263938943545023}, {"text": "MT Module", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.8709528148174286}, {"text": "QE scoring", "start_pos": 339, "end_pos": 349, "type": "TASK", "confidence": 0.8455334007740021}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of the parallel data and QE data.", "labels": [], "entities": [{"text": "QE data", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.6757294088602066}]}, {"text": " Table 2: Evaluation results on the test sets of WMT-18 En-De-SMT and WMT-18 NMT. The two leading teams  only provide ensemble results on 2018 test data. We re-implement Alibaba's single model (Bilingual Expert) and  achieved similar results on 2017 data as reported in their paper. We test that Bilingual Expert on 2018 data to  make a fair comparison for single model. With limited computational resource, we only run the ensemble for  En-De-NMT, because in WMT2019 they only requires NMT submission.", "labels": [], "entities": [{"text": "WMT-18 En-De-SMT", "start_pos": 49, "end_pos": 65, "type": "DATASET", "confidence": 0.812717080116272}, {"text": "WMT-18 NMT", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.8973493874073029}, {"text": "WMT2019", "start_pos": 460, "end_pos": 467, "type": "DATASET", "confidence": 0.9559525847434998}]}, {"text": " Table 3: Evaluation results on the WMT-19 QE sentence-level shared task. Here we only show the top four teams.", "labels": [], "entities": [{"text": "WMT-19 QE sentence-level shared task", "start_pos": 36, "end_pos": 72, "type": "DATASET", "confidence": 0.7850685358047486}]}]}