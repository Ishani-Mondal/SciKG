{"title": [{"text": "Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.8010744253794352}]}], "abstractContent": [{"text": "This paper describes Meteor++ 2.0, our submission to the WMT19 Metric Shared Task.", "labels": [], "entities": [{"text": "WMT19 Metric Shared Task", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.5701026171445847}]}, {"text": "The well known Meteor metric improves machine translation evaluation by introducing paraphrase knowledge.", "labels": [], "entities": [{"text": "Meteor metric", "start_pos": 15, "end_pos": 28, "type": "DATASET", "confidence": 0.833828866481781}, {"text": "machine translation evaluation", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.901330848534902}]}, {"text": "However, it only fo-cuses on the lexical level and utilizes consecutive n-grams paraphrases.", "labels": [], "entities": []}, {"text": "In this work, we take into consideration syntactic level paraphrase knowledge, which sometimes maybe skip-grams.", "labels": [], "entities": []}, {"text": "We describe how such knowledge can be extracted from Paraphrase Database (PPDB) and integrated into Meteor-based met-rics.", "labels": [], "entities": []}, {"text": "Experiments on WMT15 and WMT17 evaluation datasets show that the newly proposed metric outperforms all previous versions of Meteor.", "labels": [], "entities": [{"text": "WMT15", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.9529920816421509}, {"text": "WMT17 evaluation datasets", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9111506144205729}, {"text": "Meteor", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.9572027325630188}]}], "introductionContent": [{"text": "Accurate evaluation of machine translation plays an important role in measuring improvement in system performance.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7590073645114899}]}, {"text": "Since human evaluation is time-consuming and expensive, automatic metrics for MT have received significant attention in the past few years.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9967467784881592}]}, {"text": "A lot of MT evaluation metrics from different perspective have been proposed to measure how close machine-generated translations are to professional human translations such as BLEU (), Meteor (Banerjee and), TER () etc.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.8940129280090332}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9951358437538147}, {"text": "TER", "start_pos": 208, "end_pos": 211, "type": "METRIC", "confidence": 0.8811617493629456}]}, {"text": "Because Meteor has the ability to employ various linguistic language features and resources easily, a lot of improved versions has been put forward continuously.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.8653149604797363}]}, {"text": "The Meteor-Next extends the Meteor to phrase-level with the support of paraphrase tables.", "labels": [], "entities": []}, {"text": "It's clear that this knowledge incorporated into matching procedure do help the metric reach a higher correlation with the human scores.", "labels": [], "entities": []}, {"text": "In previous work, phrases in paraphrase table are defaulted to be consecutive n-grams which mainly draw on the lexical level.", "labels": [], "entities": []}, {"text": "What's more, skip n-gram () paraphrases whose components need not be consecutive also capture many meaning-preserving syntactic transformations.", "labels": [], "entities": []}, {"text": "The original Meteor-based metrics only pay attention to consecutive string matching, they perform badly when reference-hypothesis pairs contain skip n-grams.", "labels": [], "entities": []}, {"text": "Using the pair (protect...from, protect...against) for an example, the two different prepositions from and against will bring a miss-matching and then have a negative effect on the Meteor score.", "labels": [], "entities": [{"text": "Meteor score", "start_pos": 181, "end_pos": 193, "type": "DATASET", "confidence": 0.7862811386585236}]}, {"text": "Obviously, these two words are equivalent when appearing simultaneously with the verb protect.", "labels": [], "entities": []}, {"text": "What's more, from and against here mainly support the sentence structure and contribute little on semantic expression.", "labels": [], "entities": []}, {"text": "In this paper, we seek to directly address the problem mentioned before by adopting a syntactic-level language resource into Meteor.", "labels": [], "entities": []}, {"text": "Taking advantage of the large Paraphrase Database (, we automatically extract a subset of syntax PPDB which contains skip n-grams.", "labels": [], "entities": []}, {"text": "To demonstrate the efficacy of this knowledge, we raise an improved version of the Meteor incorporated with that via an extra parallel syntax stage.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9705697298049927}]}, {"text": "Our extended metric, Meteor++ 2.0, shows an improvement in the correlation with the human scores on most of the language pairs.", "labels": [], "entities": []}, {"text": "We organize the remainder of the paper as follows: Section 2 describes the traditional Meteor scoring.", "labels": [], "entities": [{"text": "Meteor scoring", "start_pos": 87, "end_pos": 101, "type": "TASK", "confidence": 0.5183462351560593}]}, {"text": "Section 3 presents the syntactic level paraphrase table acquisition and model details.", "labels": [], "entities": [{"text": "syntactic level paraphrase table acquisition", "start_pos": 23, "end_pos": 67, "type": "TASK", "confidence": 0.5631904602050781}]}, {"text": "Section 4 is devoted to the experiments and results.", "labels": [], "entities": []}, {"text": "The conclusions follow in the final section.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Comparison of segment-level Pearson correlation between Meteor and Meteor++ 2.0 (syntax) on WMT15  and WMT17 evaluation datasets. The weight of Syntax stage in Meteor++ 2.0 is set to be 0.4, other parameters are  consistent with the Meteor Universal.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.8860056698322296}, {"text": "WMT15", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.9746758341789246}, {"text": "WMT17 evaluation datasets", "start_pos": 113, "end_pos": 138, "type": "DATASET", "confidence": 0.9069084127744039}]}, {"text": " Table 4: Comparison of segment-level Pearson correlation between Meteor++ (copy) and Meteor++ 2.0 (copy +  syntax) on WMT15 and WMT17 evaluation datasets. The weight of Syntax stage in Meteor++ 2.0 is set to be 0.4,  other parameters are consistent with the Meteor Universal.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 38, "end_pos": 57, "type": "METRIC", "confidence": 0.9027804732322693}, {"text": "WMT15", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9556882381439209}, {"text": "WMT17 evaluation datasets", "start_pos": 129, "end_pos": 154, "type": "DATASET", "confidence": 0.8737171689669291}, {"text": "Meteor Universal", "start_pos": 259, "end_pos": 275, "type": "DATASET", "confidence": 0.8930191993713379}]}]}