{"title": [], "abstractContent": [{"text": "Summarization Evaluation and Short-Answer Grading share the challenge of automatically evaluating content quality.", "labels": [], "entities": [{"text": "Summarization Evaluation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9520569145679474}, {"text": "Short-Answer Grading", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6801711022853851}]}, {"text": "Therefore, we explore the use of ROUGE, a well-known Summarization Evaluation method, for Short-Answer Grading.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9925957322120667}, {"text": "Summarization Evaluation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.8985274434089661}, {"text": "Short-Answer Grading", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7757655382156372}]}, {"text": "We find a reliable ROUGE parametrization that is robust across corpora and languages and produces scores that are significantly correlated with human short-answer grades.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.747043788433075}]}, {"text": "ROUGE adds no information to Short-Answer Grading NLP-based machine learning features in a by-corpus evaluation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.35624369978904724}, {"text": "Short-Answer Grading NLP-based machine learning", "start_pos": 29, "end_pos": 76, "type": "TASK", "confidence": 0.7888639688491821}]}, {"text": "However, on a question-by-question basis, we find that the ROUGE Recall score may outperform standard NLP features.", "labels": [], "entities": [{"text": "ROUGE Recall score", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.8724154233932495}]}, {"text": "We therefore suggest to use ROUGE within a framework for per-question feature selection or as a reliable and reproducible baseline for SAG.", "labels": [], "entities": [{"text": "per-question feature selection", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6377039055029551}, {"text": "SAG", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.9331768155097961}]}], "introductionContent": [{"text": "Teachers use short free-text questions both in second-language teaching (to evaluate reading comprehension and writing skills) and in content instruction (to probe content understanding and the ability to apply knowledge).", "labels": [], "entities": []}, {"text": "Reducing the time needed for grading the answers greatly lightens teacher workloads and allows flexible self-study.", "labels": [], "entities": []}, {"text": "Short-Answer Grading (SAG) is the corresponding NLP task of predicting grades for student answers containing up to three sentences.", "labels": [], "entities": [{"text": "Short-Answer Grading (SAG)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7494873702526093}]}, {"text": "The most difficult formulation of the SAG problem, which occurs frequently in real-world teaching, is the processing of completely unseen questions and their answers.", "labels": [], "entities": [{"text": "SAG problem", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.9362382292747498}]}, {"text": "The prevailing strategy in this situation is to compare student and reference answers and base the grade prediction on any similarities.", "labels": [], "entities": []}, {"text": "While very shallow baselines like bagof-word models are strong for SAG (, they fail to cover deeper levels of meaning.", "labels": [], "entities": [{"text": "SAG", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9129215478897095}]}, {"text": "Therefore, features on different levels of language processing have been proposed to solve the central problem of comparing the meaning of two different texts (see).", "labels": [], "entities": []}, {"text": "Other NLP tasks facing a similar challenge are Machine Translation evaluation, Natural Language Generation evaluation and Summarization evaluation.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.9444956382115682}, {"text": "Natural Language Generation evaluation", "start_pos": 79, "end_pos": 117, "type": "TASK", "confidence": 0.7175189331173897}, {"text": "Summarization evaluation", "start_pos": 122, "end_pos": 146, "type": "TASK", "confidence": 0.9835711717605591}]}, {"text": "Of the three, Summarization evaluation is most closely related to SAG: When determining the quality of an automatic summary, the standard evaluation method ROUGE (derived from Translation evaluation's BLEU) compares candidate summaries against manually created references, with the goal of comparing the meaning of the two texts with string-level evaluation tools.", "labels": [], "entities": [{"text": "Summarization evaluation", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.9004436135292053}, {"text": "ROUGE", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9742711186408997}, {"text": "BLEU", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.9694297909736633}]}, {"text": "points out that the parameter space of ROUGE is not trivial and that for individual tasks and/or data sets different parameter combinations might give the best results.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the similarities of the tasks by applying ROUGE to SAG.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9941577911376953}]}, {"text": "We evaluate on four corpora from the content assessment domain, in English and German.", "labels": [], "entities": []}, {"text": "We begin by determining an appropriate, robust set of parameters for ROUGE and by analyzing how well the metric is correlated with the gold grades in the different corpora.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.46222442388534546}]}, {"text": "We then goon to compare ROUGE with standard SAG features for machine learning.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.8629841208457947}]}, {"text": "We find that ROUGE is a robust predictor on its own (and could therefore serve as a standardized baseline) and on the question level can outperform the Recently, neural network approaches have also been explored for educational scoring in general, e.g., and SAG in particular (", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9532957673072815}, {"text": "SAG", "start_pos": 258, "end_pos": 261, "type": "TASK", "confidence": 0.8034346103668213}]}], "datasetContent": [{"text": "We experimentally determine the set of ROUGE parameters that yields the best correlation of ROUGE scores against human SAG grades across corpora.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 92, "end_pos": 97, "type": "METRIC", "confidence": 0.9314066767692566}]}, {"text": "As detailed in there is a wide range of possible combinations.", "labels": [], "entities": []}, {"text": "Therefore, our first step is a parameter sweeping experiment to determine the best settings for the following parameters 5 : Stemming yes/no Stopwords yes/no ROUGE variant unigrams to 4-grams, longest common subsequence (LCS) and skip ngrams (S*)  Stemming and stopwords are options for text pre-processing, intended as rough measures to normalize the input and focus on content words.", "labels": [], "entities": [{"text": "Stemming yes/no Stopwords yes/no ROUGE", "start_pos": 125, "end_pos": 163, "type": "TASK", "confidence": 0.7293439772393968}, {"text": "longest common subsequence (LCS)", "start_pos": 193, "end_pos": 225, "type": "METRIC", "confidence": 0.7057025531927744}]}, {"text": "The ROUGE measure itself can be calculated in different variants: Four are based on plain n-grams (uni-up to 4-grams), and there are the longest common subsequence (LCS) and skip bigrams model (S*, initially with a skip interval of 4), giving a total of 6 scores.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.95904541015625}]}, {"text": "We do not consider ROUGE-W* as it rarely produced stable results.", "labels": [], "entities": [{"text": "ROUGE-W", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.982653021812439}]}, {"text": "The evaluation basis can be either ROUGE for all the tokens in the document or the average over sentence ROUGE scores; raw counts can also be output independent of ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9883040189743042}]}, {"text": "ROUGE usually evaluates against a number of samples -in a SAG context, this corresponds to having multiple reference answers.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.703546404838562}]}, {"text": "The evaluation can then be reported using the average results across all the reference samples for Precision, Recall and F-Score, or just for the best sample.", "labels": [], "entities": [{"text": "Precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9965399503707886}, {"text": "Recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.981940507888794}, {"text": "F-Score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9903558492660522}]}, {"text": "We follow Summarization evaluation practice and experiment with Recall and F-Score, with different weightings of Precision and Recall.", "labels": [], "entities": [{"text": "Summarization evaluation", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8614766597747803}, {"text": "Recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9926243424415588}, {"text": "F-Score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9907885193824768}, {"text": "Precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9980925917625427}, {"text": "Recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9823186993598938}]}, {"text": "Finally, we varied the required confidence interval between 0.99 and 0.95.", "labels": [], "entities": [{"text": "confidence interval", "start_pos": 32, "end_pos": 51, "type": "METRIC", "confidence": 0.9670809805393219}]}, {"text": "ROUGE proved quite robust to many parameter instantiations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.5721640586853027}]}, {"text": "There were results for 75% (864) of parametrizations on the Beetle data, and for all parametrizations on SEB.", "labels": [], "entities": [{"text": "Beetle data", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.9733618795871735}, {"text": "SEB", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.953874945640564}]}, {"text": "In contrast, though, only 168 (14.5%) out of 1152 possible parameter combinations yielded results for ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 102, "end_pos": 106, "type": "TASK", "confidence": 0.6450991034507751}]}, {"text": "Beetle and ASAP evaluations both failed for all runs which use raw counts as the basis of evaluation.", "labels": [], "entities": [{"text": "Beetle", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9406660795211792}]}, {"text": "This result is unproblematic in practice, since the raw scores are not a standard evaluation tool and are not in focus here.", "labels": [], "entities": []}, {"text": "ASAP evaluations additionally failed for all runs that evaluated across all models, and yielded no results in the 0.99 confidence interval.", "labels": [], "entities": []}, {"text": "The reason for the difficulties on ASAP maybe that the model answers are quite long.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.6646174192428589}]}, {"text": "The questions ask students to give multiple aspects or key points, and the model answers aim to list many possible correct aspects.", "labels": [], "entities": []}, {"text": "However, any given student will answer with just the required number of aspects, so there is usually a relatively large difference between the student answers and the models they are compared to.", "labels": [], "entities": []}, {"text": "Despite this drawback, we find throughout that the ROUGE output performs similarly for ASAP as for the other corpora, so it appears justified to use the ASAP data.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9714113473892212}, {"text": "ASAP data", "start_pos": 153, "end_pos": 162, "type": "DATASET", "confidence": 0.8115063011646271}]}, {"text": "shows the optimal parameters for the three corpora.", "labels": [], "entities": []}, {"text": "While the ROUGE tool was brittle on ASAP, this corpus shows the largest correlation of ROUGE results and human ratings.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7489858269691467}]}, {"text": "Inversely, the correlation is lowest for SEB, the corpus without any failed ROUGE runs.", "labels": [], "entities": [{"text": "correlation", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.9728683829307556}, {"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8210277557373047}]}, {"text": "During parameter sweeping, the largest drops in \u03c4 compared to the optima are observed (in order) by changing the ROUGE variant, the F weighting and the combination of stemming and stop words (for all three corpora).", "labels": [], "entities": [{"text": "parameter sweeping", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7657982409000397}, {"text": "ROUGE", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.994183361530304}, {"text": "F weighting", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.9357945919036865}]}, {"text": "Worst case, changing to ROUGE-4 on ASAP costs \u2206\u03c4 = 0.39, and \u2206\u03c4 = 0.27 on Beetle.", "labels": [], "entities": [{"text": "ROUGE-4", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9936931133270264}, {"text": "ASAP", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.8852797150611877}, {"text": "Beetle", "start_pos": 74, "end_pos": 80, "type": "DATASET", "confidence": 0.9852190017700195}]}, {"text": "This is inline with observations from the Summarization community, where the numerically highest scores are usually achieved using ROUGE-1 and the lowest using ROUGE-4.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9734917879104614}, {"text": "ROUGE-1", "start_pos": 131, "end_pos": 138, "type": "METRIC", "confidence": 0.8946127891540527}]}, {"text": "This pattern is ultimately due to sparse data caused by linguistic variation, which greatly reduces the chance of finding exactly matching 4-grams in two different documents compared to unigrams.", "labels": [], "entities": []}, {"text": "The changes in F weighting and stemming/stop words cause much smaller drops in the range between \u2206\u03c4 = 0.1 and 0.01, underscoring again the robustness of ROUGE performance to variations in parameter settings.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 153, "end_pos": 158, "type": "METRIC", "confidence": 0.6941865682601929}]}, {"text": "We found several more, stable patterns across parametrizations that helped inform our choice of final parametrization.", "labels": [], "entities": []}, {"text": "For each pattern, we also discuss its plausibility in a SAG context.", "labels": [], "entities": [{"text": "SAG context", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8887774348258972}]}, {"text": "To begin with the pre-processing steps, stopwords alone are detrimental for all three corpora.", "labels": [], "entities": []}, {"text": "In combination with stemming, they work well for SEB, but not at all for Beetle and not optimally for ASAP.", "labels": [], "entities": [{"text": "stemming", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.973041832447052}, {"text": "SEB", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.43269625306129456}, {"text": "Beetle", "start_pos": 73, "end_pos": 79, "type": "DATASET", "confidence": 0.9112274646759033}, {"text": "ASAP", "start_pos": 102, "end_pos": 106, "type": "TASK", "confidence": 0.4917345643043518}]}, {"text": "This possibly points to a domain dependence of stopword lists.", "labels": [], "entities": []}, {"text": "Stemming without stopwords is the best setting for ASAP and the second best by a small margin for Beetle and SEB.", "labels": [], "entities": [{"text": "Stemming without stopwords", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7703927357991537}, {"text": "ASAP", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.48228585720062256}, {"text": "Beetle", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.8620147109031677}, {"text": "SEB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.8213213086128235}]}, {"text": "Since stemming is a step away from pure string comparison, this result is plausible for SAG.", "labels": [], "entities": [{"text": "SAG", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9124263525009155}]}, {"text": "ROUGE-S* using the standard skip of 4 tokens between the elements of a bigram works best for ASAP and Beetle, while LCS outperforms it slightly for SEB.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9587361812591553}, {"text": "ASAP", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.6609314680099487}, {"text": "Beetle", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.8440631031990051}, {"text": "SEB", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.8484001159667969}]}, {"text": "In addition to the standard skip of 4 tokens, we also experimented with 2 and 6 tokens, but found the performance using a skip of size 4 to achieve the best numeric results.", "labels": [], "entities": []}, {"text": "As mentioned above, ROUGE-4 is consistently the worst choice across corpora while ROUGE-S* proved to be quite robust.", "labels": [], "entities": [{"text": "ROUGE-4", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.8880297541618347}]}, {"text": "Ina SAG context, this result is plausible, as ROUGE-S* flexibly allows paraphrases.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.9360266327857971}]}, {"text": "In contrast, ROUGE-4 looks fora specific, fairly long sequence.", "labels": [], "entities": [{"text": "ROUGE-4", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.5952976942062378}]}, {"text": "With short answers of 2 to 3 sentences, the probability to find matching 4-grams drops considerably due to linguistic variation.", "labels": [], "entities": []}, {"text": "ROUGE-1 fails to show optimal performance, but yields robust slightly lower results across the remaining parameters, inline with observations in Summarization evaluation.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8563598394393921}, {"text": "Summarization", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.9430034756660461}]}, {"text": "There is a small preference for sentences as evaluation unit, while tokens perform just as well for SEB and Beetle.", "labels": [], "entities": [{"text": "Beetle", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.8532230257987976}]}, {"text": "Raw scores, tested for the sake of completeness, lower the correlation for SEB and evaluation on raw scores breaks down for Beetle and ASAP.", "labels": [], "entities": [{"text": "completeness", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.9299926161766052}, {"text": "SEB", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.8957551121711731}, {"text": "Beetle", "start_pos": 124, "end_pos": 130, "type": "DATASET", "confidence": 0.901972234249115}, {"text": "ASAP", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.5825404524803162}]}, {"text": "The standard SAG setting of using the Best Model, i.e., using the highest score produced by comparison to any reference is consistent with Beetle and SEB and optimal for ASAP.", "labels": [], "entities": [{"text": "Beetle", "start_pos": 139, "end_pos": 145, "type": "DATASET", "confidence": 0.9310207962989807}, {"text": "SEB", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.41182729601860046}, {"text": "ASAP", "start_pos": 170, "end_pos": 174, "type": "TASK", "confidence": 0.4505084156990051}]}, {"text": "While correlations of F 0.5 -Scores with the human grades often are numerically slightly higher than correlations of Recall and human grades, the Recall predictions are much more robust across different combinations of parameters.", "labels": [], "entities": [{"text": "F 0.5 -Scores", "start_pos": 22, "end_pos": 35, "type": "METRIC", "confidence": 0.9823296070098877}]}, {"text": "This is plausible for the SAG task, since the recall of n-gram overlap between the student answer and reference answer shows how much of the reference answer content the student replicated.", "labels": [], "entities": [{"text": "SAG task", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9289173483848572}, {"text": "recall of n-gram overlap", "start_pos": 46, "end_pos": 70, "type": "METRIC", "confidence": 0.8943164795637131}]}, {"text": "Precision would correspond to predicting a high human grade if the student only produced correct answer portions (but maybe missed important parts of the answer).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9724284410476685}]}, {"text": "The chosen confidence interval did not make a difference to the results for SEB and Beetle, but there were no results in the 0.99 interval for ASAP (probably due to the form of the model answers).: Correlations between ROUGE predictions and manual grades for seen (dev) and unseen (test) corpus portions.", "labels": [], "entities": [{"text": "SEB", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7938013672828674}, {"text": "Beetle", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.6334725618362427}, {"text": "ASAP", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.4150896966457367}]}, {"text": "All correlations significant at p < 0.001.", "labels": [], "entities": []}, {"text": "Given the optimal parametrizations and our general observations for the English data, we chose the parameters that work for the majority of corpora.", "labels": [], "entities": [{"text": "English data", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.7893493175506592}]}, {"text": "The only departure from this rule is our use of Recall, which yields slightly lower figures, but seems overall more robust than F.", "labels": [], "entities": [{"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.8131207823753357}]}, {"text": "We use stemming without stopwords, S* with gaps of up to four intervening words, and evaluate on the sentence level using the best model.", "labels": [], "entities": []}, {"text": "Incidentally, this is the optimal parametrization for ASAP, and causes only a small drop in \u03c4 for Beetle and SEB (see the bottom line in).", "labels": [], "entities": [{"text": "ASAP", "start_pos": 54, "end_pos": 58, "type": "TASK", "confidence": 0.9275039434432983}, {"text": "Beetle", "start_pos": 98, "end_pos": 104, "type": "DATASET", "confidence": 0.8921838402748108}]}, {"text": "These parameters hardly differ from the most commonly used settings in Summarization evaluation (i.e. as used in DUC 2004).", "labels": [], "entities": [{"text": "Summarization evaluation", "start_pos": 71, "end_pos": 95, "type": "TASK", "confidence": 0.9671545326709747}, {"text": "DUC 2004", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9211778938770294}]}, {"text": "The only deviation from that standard is that we do not include unigrams in the skip-bigram (ROUGE-S*) calculation.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.8651947379112244}]}, {"text": "This underlines the similarities between the summarization and SAG tasks.", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9756937623023987}, {"text": "SAG tasks", "start_pos": 63, "end_pos": 72, "type": "TASK", "confidence": 0.8126211166381836}]}, {"text": "From a SAG perspective, the resulting parameters are also plausible given previous work, as discussed above.", "labels": [], "entities": [{"text": "SAG", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.8755707740783691}]}, {"text": "We next test the generalizability of these parameters for new data sets and anew language.", "labels": [], "entities": []}, {"text": "We first try the test sets of the three English corpora and then the German corpus.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9139112234115601}]}, {"text": "For the German data, instead of the stemming step we externally performed lemmatization (using the TreeTagger,) to do more justice to German morphology.", "labels": [], "entities": [{"text": "German data", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.7855275571346283}]}, {"text": "presents results for the optimal parameter setting determined in Exp.1.", "labels": [], "entities": [{"text": "Exp.1", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.9183814525604248}]}, {"text": "The top three rows of the table repeat the development set results for the final parametrization for the three English corpora and show performance on the unseen test sets.", "labels": [], "entities": []}, {"text": "For all three data sets, performance drops, as must be expected.", "labels": [], "entities": []}, {"text": "The most affected data set is ASAP.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.6089438796043396}]}, {"text": "This was the most brittle corpus in parameter sweeping, so the optimal parameters possibly overfit the training data used for parameter setting.", "labels": [], "entities": [{"text": "parameter sweeping", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7675828635692596}]}, {"text": "Least affected is SEB, which showed the highest drop between optimal and final parameters.", "labels": [], "entities": [{"text": "SEB", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9946293234825134}]}, {"text": "All correlations remain highly significant (and recall that \u03c4 is a conservative measure).", "labels": [], "entities": []}, {"text": "For the German corpus, which was not used in parameter sweeping, the correlation is numerically the strongest of all.", "labels": [], "entities": [{"text": "German corpus", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9427177608013153}, {"text": "parameter sweeping", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7274798154830933}, {"text": "correlation", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9707627892494202}]}, {"text": "This allows us to conclude that the parameter set can be ported to another language with a similar outcome as porting to the unseen test portion of the development data.", "labels": [], "entities": []}, {"text": "The method is clearly robust when using language-specific preprocessing tools.", "labels": [], "entities": []}, {"text": "In sum, we find that the ROUGE parameters we have determined on the training sets of three English SAG corpora are stable across corpora and languages.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9883422255516052}]}, {"text": "However, we find signs of brittleness and overfitting for our largest English corpus, ASAP, which are probably due to the nature of the available model answers.", "labels": [], "entities": [{"text": "English corpus, ASAP", "start_pos": 70, "end_pos": 90, "type": "DATASET", "confidence": 0.773392528295517}]}, {"text": "We therefore expect the identified parameters to be portable to new corpora, especially if model and students answers are comparable (as for SEB, Beetle and CSSAG).", "labels": [], "entities": []}, {"text": "Our final experiments evaluate the usefulness of the ROUGE predictions in combination with existing features for grade prediction by machine learning.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8009512424468994}, {"text": "grade prediction", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.6235804706811905}]}, {"text": "We use the system from Pad\u00f3 (2016), which extracts features on the basis of a range of levels of linguistic analysis, such as n-grams, textual similarity, dependency parses, semantic representations and textual entailment.", "labels": [], "entities": [{"text": "dependency parses", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.6957850754261017}]}, {"text": "We experiment with an SVM and a Random Forest (RF) learner for the correct-incorrect decision.", "labels": [], "entities": []}, {"text": "All the corpora we work on provide several target labels representing partial credit.", "labels": [], "entities": []}, {"text": "Prediction tasks with many target labels are harder than predicting a small number of labels.", "labels": [], "entities": []}, {"text": "Our corpora have nine labels (CSSAG), five labels (ASAP) and two labels (SEB and Beetle, two-task annotation).", "labels": [], "entities": [{"text": "ASAP", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.962547242641449}]}, {"text": "In order to standardize the difficulty of the annotation task, we normalize the annotation of ASAP and CSSAG to a binary correct-incorrect annotation by labeling as correct all student answers that receive at least the middle label (50% of points).", "labels": [], "entities": [{"text": "ASAP", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.6458848714828491}]}, {"text": "We report F scores as the standard measure for classification tasks and in accordance with previous work for SEB, Beetle and CSSAG (.", "labels": [], "entities": [{"text": "F scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9803256392478943}, {"text": "classification tasks", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.9119628071784973}, {"text": "SEB", "start_pos": 109, "end_pos": 112, "type": "DATASET", "confidence": 0.849140465259552}, {"text": "Beetle", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.8390159606933594}, {"text": "CSSAG", "start_pos": 125, "end_pos": 130, "type": "DATASET", "confidence": 0.5020297765731812}]}, {"text": "As mentioned in the Introduction, we consider the hardest instantiation of the label prediction task, the unseen question setting, where any questions in the test set are completely unseen (so no question-specific models can be trained).", "labels": [], "entities": [{"text": "label prediction task", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.810100793838501}]}, {"text": "In order to achieve this, we use leaveone-question-out evaluation on the training portion of ASAP (the provided test data is for seen questions) and on the full (previously unused) CSSAG data.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 93, "end_pos": 97, "type": "TASK", "confidence": 0.44692718982696533}, {"text": "CSSAG data", "start_pos": 181, "end_pos": 191, "type": "DATASET", "confidence": 0.9302046298980713}]}, {"text": "SEB and Beetle have test sets with unseen data.", "labels": [], "entities": [{"text": "SEB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9720059633255005}, {"text": "Beetle", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9456307291984558}]}, {"text": "8 shows evidence of the high unigram baseline for SAG at at least F=59.7 (RF on SEB; F=65.1 SVM) and up to F=86.7 (RF on ASAP).", "labels": [], "entities": [{"text": "unigram baseline", "start_pos": 29, "end_pos": 45, "type": "METRIC", "confidence": 0.953853964805603}, {"text": "SAG", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.8184713125228882}, {"text": "F", "start_pos": 66, "end_pos": 67, "type": "METRIC", "confidence": 0.9991119503974915}, {"text": "RF", "start_pos": 74, "end_pos": 76, "type": "METRIC", "confidence": 0.9407652616500854}, {"text": "F=65.1 SVM)", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.8885877251625061}, {"text": "F", "start_pos": 107, "end_pos": 108, "type": "METRIC", "confidence": 0.9986408352851868}, {"text": "ASAP", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.6960309743881226}]}, {"text": "We also report the majority baseline (the performance of a hypothetical classifier that always predicts the more frequent class) as a learning algorithm-independent (low) baseline.", "labels": [], "entities": []}, {"text": "The majority baseline is easy to beat for all classifiers and feature sets, but it highlights the strong label imbalance for ASAP, which is mirrored in its high numerical prediction results throughout.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 125, "end_pos": 129, "type": "TASK", "confidence": 0.561913788318634}]}, {"text": "Over all feature sets, the RF classifier deals better with the data than the SVM.", "labels": [], "entities": []}, {"text": "The ROUGE scores alone perform robustly, but below the unigram baseline inmost cases.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9919434785842896}]}, {"text": "They beat it numerically for RF on SEB and CSSAG.", "labels": [], "entities": [{"text": "RF", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.848251461982727}, {"text": "SEB", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8914164900779724}, {"text": "CSSAG", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.7809058427810669}]}, {"text": "This verifies that ROUGE is predictive for the SAG task, and quite strongly in some configurations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.9954148530960083}, {"text": "SAG task", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.8896394371986389}]}, {"text": "The deeper features in the NLP feature set generally numerically improve performance over the baseline (except for CSSAG and ASAP RF).", "labels": [], "entities": [{"text": "ASAP RF", "start_pos": 125, "end_pos": 132, "type": "TASK", "confidence": 0.4811072051525116}]}, {"text": "Using ROUGE scores as features in addition to the NLPbased features yields no significant improvement and mixed trends.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.944005012512207}]}, {"text": "Results for CSSAG improve numerically.", "labels": [], "entities": [{"text": "CSSAG", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7707386612892151}]}, {"text": "On the other hand, we see a small drop for both learners on the Beetle data and for ASAP and SEB, we observe a decrease for one learner, but an increase for the other.", "labels": [], "entities": [{"text": "Beetle data", "start_pos": 64, "end_pos": 75, "type": "DATASET", "confidence": 0.9866008758544922}, {"text": "ASAP", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.6633222103118896}, {"text": "SEB", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.7373325228691101}]}, {"text": "This indicates that ROUGE incorporates information also found in the standard NLP features.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.983878493309021}]}, {"text": "Since we work with ROUGE-S* skip n-grams, we assume that the shared information can be found in the uni-, bi-and trigrams in the standard NLP features.", "labels": [], "entities": []}, {"text": "We further investigate the impact of ROUGE by  This intriguing picture of a light-weight stand-in for our range of NLP features -but only in some cases -matches up well with findings from Pad\u00f3 (2016), who also found that n-gram (or n-gram and textual similarity) features suffice for reliable grade prediction for 18 out of the 30 CSSAG questions that were considered.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.6381133198738098}]}, {"text": "Pad\u00f3 (2016) suggested question-specific feature selection to optimize overall system performance and processing effort.", "labels": [], "entities": []}, {"text": "In our experiments on CSSAG, ROUGE also outperformed the n-gram features in 11 out of the 16 cases where it beat the NLP features.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9875119924545288}]}, {"text": "Taken together, these findings indicate that ROUGE should not be used as an addition to already established feature sets, but that it is a strong candidate for inclusion in a feature selection strategy that could further improve the overall classification result while at the same time simplifying the model.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9820499420166016}]}, {"text": "We expect the same to be true for SEB and Beetle.", "labels": [], "entities": [{"text": "SEB", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.7740811705589294}, {"text": "Beetle", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.7586382627487183}]}, {"text": "A second take-away from our results is the possibility of using ROUGE as a well-defined, reproducible baseline for SAG.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.9798775315284729}, {"text": "SAG", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.7435694932937622}]}, {"text": "ROUGE-S* captures much of information present in a bag-of-words baseline while clearly defining implementational detail like the use of stemming and stop words.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8545739650726318}]}, {"text": "This increases transparency and reproducibility of results for the community.", "labels": [], "entities": [{"text": "transparency", "start_pos": 15, "end_pos": 27, "type": "METRIC", "confidence": 0.964988112449646}]}], "tableCaptions": [{"text": " Table 1: Corpus sizes and characteristics (source,  number of questions and answers in development  (ASAP: training) and test sections)", "labels": [], "entities": []}, {"text": " Table 2: Optimal ROUGE parametrizations with  corresponding \u03c4 s and \u03c4 s for the final parametriza- tion. Final parameter values in bold face.", "labels": [], "entities": []}, {"text": " Table 3: Correlations between ROUGE predictions  and manual grades for seen (dev) and unseen (test)  corpus portions. All correlations significant at p <  0.001.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.8898595571517944}]}, {"text": " Table 4: Grade prediction F-Scores for the majority and unigram baselines, ROUGE, all NLP features,  and NLP+ROUGE. Random Forest (RF) and SVM classifiers. Best result per corpus in bold.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9867924451828003}]}]}