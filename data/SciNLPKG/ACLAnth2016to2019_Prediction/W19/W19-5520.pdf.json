{"title": [{"text": "HITS-SBD at the FinSBD Task: Machine Learning vs. Rule-based Sentence Boundary Detection", "labels": [], "entities": [{"text": "HITS-SBD", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7798905372619629}, {"text": "FinSBD Task", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.9014763534069061}, {"text": "Sentence Boundary Detection", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.607730915149053}]}], "abstractContent": [{"text": "This paper presents two different approaches towards Sentence Boundary Detection (SBD) that were submitted to the FinSBD-2019 shared task.", "labels": [], "entities": [{"text": "Sentence Boundary Detection (SBD)", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.9139405488967896}, {"text": "FinSBD-2019 shared task", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.9003836115201315}]}, {"text": "The first is a supervised machine learning approach which tackled the SBD task as a combination of binary classifications based on TF-IDF representations of context windows.", "labels": [], "entities": [{"text": "SBD task", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.908418744802475}]}, {"text": "The second approach is unsupervised and rule-based and applies manually created heuristics to automatically annotated input.", "labels": [], "entities": []}, {"text": "Since the latter approach yielded better results on the Dev set, we submitted it to evaluation for En-glish and reached an F score of 0.80 and 0.86 for detecting begin of sentences and end of sentences, respectively.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8158160746097565}, {"text": "F score", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.9855547845363617}]}], "introductionContent": [{"text": "Sentences are the fundamental units of text, consisting of words and punctuation, and constructing phrases and paragraphs.", "labels": [], "entities": []}, {"text": "Sentence Boundary Detection (SBD), or finding the start and end of sentences, is an essential prerequisite in the Natural Language Processing (NLP) pipeline for various applications, such as Discourse Parsing, Machine Translation, Document Summarization, Alignment of Parallel Text, Sentiment Analysis, and Information Retrieval.", "labels": [], "entities": [{"text": "Sentence Boundary Detection (SBD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8297002563873926}, {"text": "Discourse Parsing", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.7592900693416595}, {"text": "Machine Translation", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.826631635427475}, {"text": "Document Summarization", "start_pos": 231, "end_pos": 253, "type": "TASK", "confidence": 0.9239924252033234}, {"text": "Alignment of Parallel Text", "start_pos": 255, "end_pos": 281, "type": "TASK", "confidence": 0.8451704382896423}, {"text": "Sentiment Analysis", "start_pos": 283, "end_pos": 301, "type": "TASK", "confidence": 0.8819004893302917}, {"text": "Information Retrieval", "start_pos": 307, "end_pos": 328, "type": "TASK", "confidence": 0.816512405872345}]}, {"text": "SBD can have a strong impact on the performance of these applications due to error propagation in the NLP pipeline.", "labels": [], "entities": []}, {"text": "Probably the most important factors in SBD are 1) ambiguous expressions and 2) source of text.", "labels": [], "entities": [{"text": "SBD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9811581969261169}]}, {"text": "In disambiguating sentence boundaries, the most misleading expression is the period (.), which is not only used as end of sentence (ES) marker, but also in abbreviations or numerical expressions (e.g. ordinals and dates).", "labels": [], "entities": [{"text": "period", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9056488871574402}]}, {"text": "Some other examples of problematic expressions are question mark (?), exclamation mark (!), and colon (:).", "labels": [], "entities": []}, {"text": "The other key factor is the genre and / or quality of the text, which can impact the performance of an SBD system.", "labels": [], "entities": []}, {"text": "Most of the existing SBD systems are highly accurate on formal and high quality text, but their performance often degrades when the input text is noisy or informal.", "labels": [], "entities": []}, {"text": "The research work for SBD mostly focuses on disambiguating the ends of sentences informal, noise-free text.", "labels": [], "entities": [{"text": "SBD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9648138880729675}]}, {"text": "The FinSBD-2019 Shared Task, in contrast, tackles SBD in noisy text extracted from PDFs from the financial domain.", "labels": [], "entities": []}, {"text": "We applied two different approaches to solve the SBD problem.", "labels": [], "entities": [{"text": "SBD problem", "start_pos": 49, "end_pos": 60, "type": "TASK", "confidence": 0.9403240084648132}]}, {"text": "The first approach treats the problem as an supervised classification task on TF-IDF based representations of context windows, thus taking advantage of the annotated training data set.", "labels": [], "entities": []}, {"text": "The second approach is unsupervised, rulebased and applies manually created heuristics to automatically annotated input.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 2 covers some existing work on SBD.", "labels": [], "entities": [{"text": "SBD", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.9338952898979187}]}, {"text": "Section 3 explains the general configurations for experiments, including details about the data set and evaluation measures.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 explain the two approaches in more detail, and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section covers the data set and configuration used for our experiments.", "labels": [], "entities": []}, {"text": "The evaluation metrics include Precision, Recall and F score, which can be automatically computed by the supplied evaluation script for all three labels.", "labels": [], "entities": [{"text": "Precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9994505047798157}, {"text": "Recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9912488460540771}, {"text": "F score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9852264225482941}]}, {"text": "Due to the overwhelming number of tokens that are labeled as O, the official system performance was only computed as the average F score for BS and ES label prediction.", "labels": [], "entities": [{"text": "F score", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9754576086997986}, {"text": "BS and ES label prediction", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.49441084265708923}]}, {"text": "Also, the standard result format has only two decimal places, i.e. F score ranges from 0.00 to 1.00.", "labels": [], "entities": [{"text": "F score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9860938489437103}]}, {"text": "While this might be sufficient for overall ranking of shared task participants, we found it too coarse-grained, especially in the detailed analysis of the rule-based system (cf. Section 5), for which we changed the result precision to five decimal places.", "labels": [], "entities": [{"text": "precision", "start_pos": 222, "end_pos": 231, "type": "METRIC", "confidence": 0.7707618474960327}]}], "tableCaptions": [{"text": " Table 1: Statistics of the FinSBD-2019 Data Set", "labels": [], "entities": [{"text": "FinSBD-2019 Data Set", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.9853337208429972}]}, {"text": " Table 2: Results of ML based Approach with CW Size = 5", "labels": [], "entities": [{"text": "ML based", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.7643944323062897}, {"text": "Approach", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.6510116457939148}]}, {"text": " Table 3: Results of BS and ES Detection Rules with Cumulative Effect of Different Annotation Patterns", "labels": [], "entities": [{"text": "BS and ES Detection", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.5898674800992012}]}]}