{"title": [], "abstractContent": [{"text": "We present a deep generative model of bilingual sentence pairs for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7517488300800323}]}, {"text": "The model generates source and target sentences jointly from a shared latent representation and is parameterised by neural networks.", "labels": [], "entities": []}, {"text": "We perform efficient training using amortised variational inference and reparameterised gradients.", "labels": [], "entities": []}, {"text": "Additionally, we discuss the statistical implications of joint modelling and propose an efficient approximation to maximum a pos-teriori decoding for fast test-time predictions.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of our model in three machine translation scenarios: in-domain training, mixed-domain training, and learning from a mix of gold-standard and synthetic data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7160275280475616}]}, {"text": "Our experiments show consistently that our joint formulation outperforms conditional modelling (i.e. standard neural machine translation) in all such scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) systems) require vast amounts of labelled data, i.e. bilingual sentence pairs, to be trained effectively.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8134235839049021}]}, {"text": "Oftentimes, the data we use to train these systems area byproduct of mixing different sources of data.", "labels": [], "entities": []}, {"text": "For example, labelled data are sometimes obtained by putting together corpora from different domains.", "labels": [], "entities": []}, {"text": "Even fora single domain, parallel data often result from the combination of documents independently translated from different languages by different people or agencies, possibly following different guidelines.", "labels": [], "entities": []}, {"text": "When resources are scarce, it is not uncommon to mix in some synthetic data, e.g. bilingual data artificially obtained by having a model translate target monolingual data to the source language ().", "labels": [], "entities": []}, {"text": "Translation direction, original language, and quality of translation are some of the many factors that we typically choose not to control for (due to lack of information or simply for convenience).", "labels": [], "entities": [{"text": "Translation direction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9717674255371094}]}, {"text": "All those arguably contribute to making our labelled data a mixture of samples from various data distributions.", "labels": [], "entities": []}, {"text": "Regular NMT systems do not explicitly account for latent factors of variation, instead, given a source sentence, NMT models a single conditional distribution over target sentences as a fully supervised problem.", "labels": [], "entities": []}, {"text": "In this work, we introduce a deep generative model that generates source and target sentences jointly from a shared latent representation.", "labels": [], "entities": []}, {"text": "The model has the potential to use the latent representation to capture global aspects of the observations, such as some of the latent factors of variation just discussed.", "labels": [], "entities": []}, {"text": "The result is a model that accommodates members of a more complex class of marginal distributions.", "labels": [], "entities": []}, {"text": "Due to the presence of latent variables, this model requires posterior inference, in particular, we employ the framework of amortised variational inference).", "labels": [], "entities": []}, {"text": "Additionally, we propose an efficient approximation to maximum a posteriori (MAP) decoding for fast test-time predictions.", "labels": [], "entities": []}, {"text": "Contributions We introduce a deep generative model for NMT ( \u00a73) and discuss theoretical advantages of joint modelling over conditional modelling ( \u00a73.1).", "labels": [], "entities": []}, {"text": "We also derive an efficient approximation to MAP decoding that requires only a single forward pass through the network for prediction ( \u00a73.3).", "labels": [], "entities": [{"text": "MAP decoding", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.9269343912601471}]}, {"text": "Finally, we show in \u00a74 that our proposed model improves translation performance in at least three practical scenarios: i) in-domain training on little data, where test data are expected to follow the training data distribution closely; ii) mixed-domain training, where we train a single model but test independently on each domain; and iii) learning from large noisy synthetic data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate two translation tasks, namely, WMT's translation of news () and IWSLT's translation of transcripts of TED talks (, and concentrate on translations for German (DE) and English (EN) in either direction.", "labels": [], "entities": [{"text": "WMT's translation of news", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.6347401797771454}, {"text": "IWSLT", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.7687869668006897}, {"text": "translation of transcripts of TED talks", "start_pos": 87, "end_pos": 126, "type": "TASK", "confidence": 0.5064821938673655}]}, {"text": "In this section we aim to investigate scenarios where we expect observations to be representative of various data distributions.", "labels": [], "entities": []}, {"text": "As a sanity check, we start where training conditions can be considered in-domain with respect to test conditions.", "labels": [], "entities": []}, {"text": "Though note that this does not preclude the potential for appreciable variability in observations as various other latent factors still likely play a role (see \u00a71).", "labels": [], "entities": []}, {"text": "We then mix datasets from these two remarkably different translation tasks and investigate whether performance can be improved across tasks with a single model.", "labels": [], "entities": []}, {"text": "Finally, we investigate the case where we learn from synthetic data in addition to gold-standard data.", "labels": [], "entities": []}, {"text": "For this investigation we derive synthetic data from observations that are close to the domain of the test set in an attempt to avoid further confounders.", "labels": [], "entities": []}, {"text": "Pre-processing We tokenized and truecased all data using standard scripts from the Moses toolkit (, and removed sentences longer than 50 tokens.", "labels": [], "entities": []}, {"text": "For computational efficiency and to avoid problems with closed vocabularies, we segment the data using BPE) with 32, 000 merge operations independently for each language.", "labels": [], "entities": [{"text": "BPE", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.7719717621803284}]}, {"text": "For training the truecaser and the BPEs we used a concatenation of all the available bilingual and monolingual data for German and all bilingual data for English.", "labels": [], "entities": [{"text": "BPEs", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.6732901334762573}]}, {"text": "Systems We develop all of our models on top of Tensorflow NMT (.", "labels": [], "entities": []}, {"text": "Our baseline system is a standard implementation of conditional NMT (COND) (.", "labels": [], "entities": []}, {"text": "To illustrate the importance of latent variable modelling, we also include in the comparison a simpler attempt at JOINT modelling where we do not induce a shared latent space.", "labels": [], "entities": [{"text": "latent variable modelling", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6420348187287649}, {"text": "JOINT modelling", "start_pos": 114, "end_pos": 129, "type": "TASK", "confidence": 0.6930798292160034}]}, {"text": "Instead, the model is trained in a fully-supervised manner to maximise what is essentially a combination of two nearly independent objectives, namely, a language model and a conditional translation model.", "labels": [], "entities": []}, {"text": "Note that the two components of the model share very little, i.e. an embedding layer for the source language.", "labels": [], "entities": []}, {"text": "Finally, we aim at investigating the effectiveness of our auto-encoding variational NMT (AEVNMT).", "labels": [], "entities": [{"text": "auto-encoding variational NMT (AEVNMT", "start_pos": 58, "end_pos": 95, "type": "METRIC", "confidence": 0.5404962241649628}]}, {"text": "Hyperparameters Our recurrent cells are 256-dimensional GRU units ().", "labels": [], "entities": []}, {"text": "We train on batches of 64 sentence pairs with Adam (Kingma and Ba, 2015), learning rate 3 \u00d7 10 \u22124 , for at least T updates.", "labels": [], "entities": []}, {"text": "We then perform convergence checks every 500 batches and stop after 20 checks without any improvement measured by BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9990936517715454}]}, {"text": "For in-domain training we set T = 140, 000, and for mixeddomain training, as well as training with synthetic data, we set T = 280, 000.", "labels": [], "entities": [{"text": "T", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9909831285476685}, {"text": "T", "start_pos": 122, "end_pos": 123, "type": "METRIC", "confidence": 0.9760617017745972}]}, {"text": "For decoding we use abeam width of 10 and a length penalty of 1.0.", "labels": [], "entities": [{"text": "length", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9907993674278259}]}, {"text": "We investigate the use of dropout () for the conditional baseline with rates from 10% to 60% in increments of 10%.", "labels": [], "entities": []}, {"text": "Best validation performance on WMT required a rate of 40% for EN-DE and 50% for DE-EN, while on IWSLT it required 50% for either translation direction.", "labels": [], "entities": [{"text": "WMT", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.5144537091255188}, {"text": "IWSLT", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.9117732048034668}]}, {"text": "To spare resources, we also use these rates for training the simple JOINT model.", "labels": [], "entities": []}, {"text": "Avoiding collapsing to prior Many have noticed that VAEs whose observation models are parameterised by strong generators, such as recurrent neural networks, learn to ignore the latent representation ().", "labels": [], "entities": []}, {"text": "In such cases, the approximate posterior \"collapses\" to the prior, and where one has a fixed prior, such as our standard Gaussian, this means that the posterior becomes independent of the data, which is obviously not desirable.", "labels": [], "entities": []}, {"text": "proposed two techniques to counter this effect, namely, \"KL annealing\", and target word dropout.", "labels": [], "entities": []}, {"text": "KL annealing consists in incorporating the KL term of Equation into the objective gradually, thus allowing the posterior to move away from the prior more freely at early stages of training.", "labels": [], "entities": [{"text": "Equation", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.970328152179718}]}, {"text": "After and possibly a prediction network.", "labels": [], "entities": []}, {"text": "However, this does not add much sequential computation: the inference network can run in parallel with the source encoder, and the source language model runs in parallel with the target decoder.", "labels": [], "entities": []}, {"text": "14.8 ELBO x 14.9 a number of annealing steps, the KL term is incorporated in full and training continues with the actual ELBO.", "labels": [], "entities": []}, {"text": "In our search we considered annealing for 20, 000 to 80, 000 training steps.", "labels": [], "entities": []}, {"text": "Word dropout consists in randomly masking words in observed target prefixes at a given rate.", "labels": [], "entities": []}, {"text": "The idea is to harm the potential of the decoder to capitalise on correlations internal to the structure of the observation in the hope that it will rely more on the latent representation instead.", "labels": [], "entities": []}, {"text": "We considered rates from 20% to 40% in increments of 10%.", "labels": [], "entities": []}, {"text": "shows the configurations that achieve best validation results on EN-DE.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8562401533126831}]}, {"text": "To spare resources, we reuse these hyperparameters for DE-EN experiments.", "labels": [], "entities": []}, {"text": "With these settings, we attain a nonnegligible validation KL (see, last row of, which indicates that the approximate posterior is different from the prior at the end of training.", "labels": [], "entities": [{"text": "KL", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.5911656618118286}]}, {"text": "ELBO variants We investigate the effect of conditioning on target observations for posterior inference during training against a simpler variant that conditions on the source alone.", "labels": [], "entities": [{"text": "ELBO", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7464545369148254}]}, {"text": "suggests that conditioning on x is sufficient and thus we opt to continue with this simpler version.", "labels": [], "entities": []}, {"text": "Do note that when we use both observations for posterior inference, i.e. q \u03bb (z|x, y), and thus train an approximation r \u03c6 for prediction, we have additional parameters to estimate (e.g. due to the need to encode y for q \u03bb and x for r \u03c6 ), thus it maybe the case that for these variants to show their potential we need larger data and/or prolonged training.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Strategies to promote use of latent representa- tion along with the validation KL achieved.", "labels": [], "entities": [{"text": "KL", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.47958508133888245}]}, {"text": " Table 3: Test results for in-domain training on IWSLT (top) and NC (bottom): we report average (1std) across 5  independent runs for COND and AEVNMT, but a single run of JOINT.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8311296105384827}, {"text": "COND", "start_pos": 134, "end_pos": 138, "type": "DATASET", "confidence": 0.803808331489563}, {"text": "AEVNMT", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.8417048454284668}, {"text": "JOINT", "start_pos": 171, "end_pos": 176, "type": "DATASET", "confidence": 0.7876905202865601}]}, {"text": " Table 4: Test results for mixed-domain training: we report average (1std) across 5 independent runs for COND and  AEVNMT, but a single run of JOINT.", "labels": [], "entities": [{"text": "COND", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.7978746294975281}, {"text": "AEVNMT", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.8411931991577148}, {"text": "JOINT", "start_pos": 143, "end_pos": 148, "type": "DATASET", "confidence": 0.6654408574104309}]}, {"text": " Table 5: Test results for training on NC plus synthetic data (back-translated News Crawl): we report average (1std)  across 5 independent runs for COND and AEVNMT, but a single run of JOINT.", "labels": [], "entities": [{"text": "NC", "start_pos": 39, "end_pos": 41, "type": "DATASET", "confidence": 0.934689462184906}, {"text": "COND", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.8247921466827393}, {"text": "AEVNMT", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.8888306617736816}, {"text": "JOINT", "start_pos": 185, "end_pos": 190, "type": "DATASET", "confidence": 0.595228374004364}]}, {"text": " Table 8: Validation results reported in BLEU for mixed-domain training: we report average (1std) across 5 inde- pendent runs for COND and AEVNMT, but a single run of JOINT. The validation set used is a concatenation of  the development sets from WMT and IWSLT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9658312797546387}, {"text": "COND", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.7967977523803711}, {"text": "AEVNMT", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9348992705345154}, {"text": "JOINT", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.8394607901573181}, {"text": "WMT", "start_pos": 247, "end_pos": 250, "type": "DATASET", "confidence": 0.9160730242729187}, {"text": "IWSLT", "start_pos": 255, "end_pos": 260, "type": "DATASET", "confidence": 0.9029144644737244}]}, {"text": " Table 9: Validation results reported in BLEU for training on NC plus synthetic data: we report average (1std)  across 5 independent runs for COND and AEVNMT, but a single run of JOINT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9898583292961121}, {"text": "NC", "start_pos": 62, "end_pos": 64, "type": "DATASET", "confidence": 0.9423693418502808}, {"text": "COND", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.8114197254180908}, {"text": "AEVNMT", "start_pos": 151, "end_pos": 157, "type": "METRIC", "confidence": 0.9176450371742249}, {"text": "JOINT", "start_pos": 179, "end_pos": 184, "type": "DATASET", "confidence": 0.5264716744422913}]}, {"text": " Table 10: Test results for in-domain training on IWSLT: we report average (1std)", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.8649887442588806}]}]}