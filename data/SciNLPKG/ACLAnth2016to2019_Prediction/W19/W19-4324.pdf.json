{"title": [], "abstractContent": [{"text": "Since their inception, encoder-decoder models have successfully been applied to a wide array of problems in computational linguistics.", "labels": [], "entities": []}, {"text": "The most recent successes are predominantly due to the use of different variations of attention mechanisms, but their cognitive plausibility is questionable.", "labels": [], "entities": []}, {"text": "In particular, because past representations can be revisited at any point in time, attention-centric methods seem to lack an incentive to buildup incrementally more informative representations of incoming sentences.", "labels": [], "entities": []}, {"text": "This way of processing stands in stark contrast with the way in which humans are believed to process language: continuously and rapidly integrating new information as it is encountered.", "labels": [], "entities": []}, {"text": "In this work, we propose three novel metrics to assess the behavior of RNNs with and without an attention mechanism and identify key differences in the way the different model types process sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Incrementality -that is, building up representations \"as rapidly as possible as the input is encountered\") -is considered one of the key ingredients for humans to process language efficiently and effectively.", "labels": [], "entities": []}, {"text": "conjecture how this trait is realized inhuman cognition by identifying several components which either makeup or are implications of their hypothesized Nowor-Never bottleneck, a set of fundamental constraints on human language processing, which include a limited amount of available memory and time pressure.", "labels": [], "entities": []}, {"text": "First of all, one of the implications of the now-or-never bottleneck is anticipation, implemented by a mechanism called predictive processing.", "labels": [], "entities": [{"text": "anticipation", "start_pos": 72, "end_pos": 84, "type": "METRIC", "confidence": 0.8968839049339294}, {"text": "predictive processing", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8894566893577576}]}, {"text": "As humans have to process sequences of inputs fast, they already try to anticipate the next element before it is being uttered.", "labels": [], "entities": []}, {"text": "This is hypothesized to be the reason why people struggle with so-called garden path sentences like \"The horse race past the barn fell\", where the last word encountered, \"fell\", goes against the representation of the sentence built up until this point.", "labels": [], "entities": []}, {"text": "Secondly, another strategy being employed by humans in processing language seems to be eager processing: the cognitive system encodes new input into \"rich\" representations as fast as possible.", "labels": [], "entities": [{"text": "eager processing", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7222396731376648}]}, {"text": "These are buildup in chunks and then processed into more and more abstract representations, an operation call Chunkand-pass processing.", "labels": [], "entities": []}, {"text": "In this paper, we aim to gain a better insight into the inner workings of recurrent models with respect to incrementality while taking inspiration from and drawing parallels to this psycholinguistic perspective.", "labels": [], "entities": []}, {"text": "To ensure a successful processing of language, the human brain seems to be forced to employ an encoding scheme that seems highly reminiscent of the encoder in today's encoderdecoder architectures.", "labels": [], "entities": []}, {"text": "Here, we look at differences between a recurrent-based encoder-decoder model with and without attention.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}