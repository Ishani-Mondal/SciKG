{"title": [{"text": "Sentiment analysis is not solved! Assessing and probing sentiment classification", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9628666341304779}, {"text": "probing sentiment classification", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.729375958442688}]}], "abstractContent": [{"text": "Neural methods for sentiment analysis have led to quantitative improvements over previous approaches, but these advances are not always accompanied with a thorough analysis of the qualitative differences.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9705136120319366}]}, {"text": "Therefore, it is not clear what outstanding conceptual challenges for sentiment analysis remain.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9667798280715942}]}, {"text": "In this work, we attempt to discover what challenges still prove a problem for sentiment classifiers for English and to provide a challenging dataset.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8154481649398804}]}, {"text": "We collect the subset of sentences that an (oracle) ensemble of state-of-the-art sentiment classifiers misclassify and then annotate them for 18 linguistic and paralinguistic phenomena, such as negation, sarcasm, modality, etc.", "labels": [], "entities": []}, {"text": "1 Finally, we provide a case study that demonstrates the usefulness of the dataset to probe the performance of a given sentiment classifier with respect to linguistic phenomena.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last 15 years, approaches to sentiment analysis which concentrated on creating and curating sentiment lexicons) or used n-grams for classification () have been replaced by models that are able to exploit compositionality ( or implicitly learn relations between tokens (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.9429462254047394}]}, {"text": "These neural models push the state of the art to over 90% accuracy on binary sentence-level sentiment analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.999056875705719}, {"text": "binary sentence-level sentiment analysis", "start_pos": 70, "end_pos": 110, "type": "TASK", "confidence": 0.7001498267054558}]}, {"text": "Although these methods show a quantitative improvement over previous approaches, they are not often accompanied with a thorough analysis of the qualitative differences.", "labels": [], "entities": []}, {"text": "This has led to the current situation, where we are aware of quantitative, but not qualitative differences between state-of-the-art sentiment classifiers.", "labels": [], "entities": []}, {"text": "It also means that we are not aware of the outstanding conceptual challenges that we still face in sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.9410683810710907}]}, {"text": "In this work, we attempt to discover what conceptual challenges still prove a problem for all stateof-the-art sentiment methods for English.", "labels": [], "entities": []}, {"text": "To do so, we train and test three state-of-the-art machine learning classifiers (BERT, ELMo, and a BiLSTM) as well as a bag-of-words classifier on six sentencelevel sentiment datasets available for English.", "labels": [], "entities": [{"text": "BERT", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.99601149559021}, {"text": "ELMo", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.8825905919075012}]}, {"text": "We then collect the subset of sentences that all models misclassify and annotate them for 18 linguistic and paralinguistic phenomena, such as negation, sarcasm, modality or world knowledge.", "labels": [], "entities": []}, {"text": "We present this new data as a challenging dataset for future research in sentiment analysis, which enables probing the problems that sentiment classifiers still face in more depth.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.952055960893631}]}, {"text": "Specifically, the contributions of this work are: \u2022 the creation of a challenging sentiment dataset from previously available data, \u2022 the annotation of errors in this dataset for 18 linguistic and paralinguistic phenomena, \u2022 a thorough analysis of the dataset, \u2022 and finally presenting a practical use-case demonstrating how the dataset can be used to probe the particular types of errors made by anew model.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized into related work (Section 2), a description of the experimental setup (Section 3), a brief description of the dataset (Section 4), an in-depth analysis (Section 5), a case-study that demonstrates the usefulness of the dataset (Section 6), and finally a conclusion (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "In these experiments, we test three state-of-the-art models for sentence-level sentiment classification.", "labels": [], "entities": [{"text": "sentence-level sentiment classification", "start_pos": 64, "end_pos": 103, "type": "TASK", "confidence": 0.7898112833499908}]}, {"text": "We choose to focus on sentence-level classification for three reasons: 1) sentence-level classification is a popular and useful task, 2) there is a large amount of high-quality annotated data available, and 3) annotation of linguistic phenomena is easier at sentence-level than document-level.", "labels": [], "entities": [{"text": "sentence-level classification", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7095627039670944}, {"text": "sentence-level classification", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.7124209403991699}]}, {"text": "It is also likely that most phenomena that occur at sentencelevel, e. g., negation, comparative sentiment, or modality, will transfer to other sentiment tasks.", "labels": [], "entities": [{"text": "comparative sentiment", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7231701612472534}]}, {"text": "In order to discover a subset of sentences that all state-of-the-art models are unable to correctly predict, we collect six English-language datasets previously annotated for sentence-level sentiment from five domains (news wire, hotel reviews, movie reviews, twitter, and micro-blogs).", "labels": [], "entities": []}, {"text": "shows the statistics for each of the datasets.", "labels": [], "entities": []}, {"text": "MPQA The Multi-perspective Question Answer (MPQA) Opinion Corpus () provides contextual polarity annotations for English news documents from world press.", "labels": [], "entities": [{"text": "MPQA The Multi-perspective Question Answer (MPQA) Opinion Corpus", "start_pos": 0, "end_pos": 64, "type": "DATASET", "confidence": 0.6760501235723495}]}, {"text": "The annotations are private state frames, which include annotations for text anchor, source, target, and attitude type, among others.", "labels": [], "entities": []}, {"text": "We extract sentiment labeled sentences by taking only those sentences that have sentiment annotations.", "labels": [], "entities": []}, {"text": "Additionally, we remove sentences that contain both positive and negative sentiment.", "labels": [], "entities": []}, {"text": "This leaves a three-class (positive, neutral, negative) sentence-level dataset.", "labels": [], "entities": []}, {"text": "T\u00e4ckstr\u00f6m dataset The T\u00e4ckstr\u00f6m dataset (T\u00e4ckstr\u00f6m and McDonald, 2011) contains product reviews which have been annotated at both document-and sentence-level for three-class sentiment, although the sentence-level annotations also have a \"not relevant\" label.", "labels": [], "entities": [{"text": "T\u00e4ckstr\u00f6m dataset", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9033470451831818}, {"text": "T\u00e4ckstr\u00f6m dataset (T\u00e4ckstr\u00f6m and McDonald, 2011)", "start_pos": 22, "end_pos": 70, "type": "DATASET", "confidence": 0.8741576770941416}]}, {"text": "We keep the sentencelevel annotations, which gives 3,662 sentences annotated for three-class sentiment.", "labels": [], "entities": []}, {"text": "Thelwall dataset The Thelwall dataset derives from datasets provided with SentiStrength 2 (Thelwall et al., 2010).", "labels": [], "entities": [{"text": "Thelwall dataset", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.843513011932373}, {"text": "Thelwall dataset", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8026093542575836}]}, {"text": "It contains microblogs annotated for both positive and negative sentiment on a scale from 1 to 5.", "labels": [], "entities": []}, {"text": "We map these to single sentiment labels such that sentences which are clearly positive (pos >= 3 and neg < 3) are given the positive label, clearly negative sentences (pos < 3 and neg >= 3) the negative label, and clearly neutral sentences ( 3 < pos > 2 and 3 < neg > 2) the neutral.", "labels": [], "entities": []}, {"text": "We discard all other sentences, which finally leaves 6,334 annotated sentences.", "labels": [], "entities": []}, {"text": "The data are available at http:// sentistrength.wlv.ac.uk/  We create a challenging dataset by collecting the subset of test sentences that all of the sentiment systems predicted incorrectly (statistics are shown in).", "labels": [], "entities": []}, {"text": "After removing sentences with incorrect gold labels, there area total of 836 sentences in the dataset, with a similar number of positive, neutral, and negative labels and fewer strong labels.", "labels": [], "entities": []}, {"text": "This is expected, as only two datasets have strong labels.", "labels": [], "entities": []}, {"text": "Furthermore, the main sources of examples are the SemEval task (249), Stanford Sentiment Treebank (452) and Thelwall datasets (215), while the T\u00e4ckstr\u00f6m dataset (129), MPQA (39) and OpeNER (29) contribute much less.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 70, "end_pos": 97, "type": "DATASET", "confidence": 0.8745048840840658}, {"text": "Thelwall datasets", "start_pos": 108, "end_pos": 125, "type": "DATASET", "confidence": 0.9363783895969391}, {"text": "T\u00e4ckstr\u00f6m dataset", "start_pos": 143, "end_pos": 160, "type": "DATASET", "confidence": 0.7798626720905304}, {"text": "MPQA", "start_pos": 168, "end_pos": 172, "type": "DATASET", "confidence": 0.879036545753479}]}, {"text": "This is a result of both dataset size and difficulty.", "labels": [], "entities": []}, {"text": "In order to give a clearer view of the data found in the dataset, we annotate these instances using 19 linguistic and paralinguistic labels.", "labels": [], "entities": []}, {"text": "While most of these come from previous attempts to qualitatively analyze sentiment classifiers (, others (incorrect label, no sentiment, morphology) emerged during the error annotation process.", "labels": [], "entities": []}, {"text": "We further chose to manually annotate for the polarity of the sentence irrespective of the gold label in order to be able to locate possible annotation errors during our analysis.", "labels": [], "entities": []}, {"text": "The annotation scheme and (manually constructed) examples of each label are shown in.", "labels": [], "entities": []}, {"text": "Note that we did not limit the number of labels that the annotator could assign to each sentence and in principle they should assign all suitable labels during annotation.", "labels": [], "entities": []}, {"text": "An initial analysis of the errors shown in    Mixed Polarity The largest set of errors, with 185 sentences labeled, are what we refer to as \"mixed\" polarity sentences.", "labels": [], "entities": []}, {"text": "These are sentences where two differing polarities are expressed, either towards two separate entities, or towards the same entity.", "labels": [], "entities": []}, {"text": "While the first can be solved by a more fine-grained approach (aspect-level or targeted sentiment), the second is more difficult and is often considered a category of its own (  An analysis of the mixed category errors reveals that while most of the examples are in the \"neutral\" category (45%), the other 55% are annotated as having mostly positive or negative sentiment.", "labels": [], "entities": []}, {"text": "This is a confusing situation for both annotators and sentiment classifiers, and a direct product of performing sentence-level classification rather than aspect-level.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.8465369045734406}, {"text": "sentence-level classification", "start_pos": 112, "end_pos": 141, "type": "TASK", "confidence": 0.7111285328865051}]}, {"text": "Nearly a third of the errors contain \"but\" clauses, which could be correctly classified by splitting them.", "labels": [], "entities": []}, {"text": "A more problematic situation is found among nearly 20% of the examples, where the annotator found the original label to be completely incorrect.", "labels": [], "entities": []}, {"text": "Non-standard spelling Most errors in this category (180 total) are labeled either negative (49%) or positive (29%), with almost no strong positive or strong negative, which comes mainly from the fact that the noisier datasets do not contain the strong labels.", "labels": [], "entities": []}, {"text": "Around a third of the examples contain hashtags that clearly express the sentiment of the whole sentence, e. g., \"#imtiredof this SNOW and COLD weather!!!\".", "labels": [], "entities": [{"text": "COLD", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.8763248920440674}]}, {"text": "This indicates the need to properly deal with hashtags in order to correctly classify sentiment.", "labels": [], "entities": [{"text": "classify sentiment", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8354281187057495}]}, {"text": "Idioms   sentences labeled) are spread relatively uniformly across labels.", "labels": [], "entities": []}, {"text": "Learning these correctly from sentence-level annotations is unlikely, especially because they are seldom found repeatedly, even in a training corpus of decent size.", "labels": [], "entities": []}, {"text": "Therefore, incorporating idiomatic information from external data sources maybe necessary to improve the classification of sentences within this category.", "labels": [], "entities": []}, {"text": "Strong Labels This category (122 total) is particularly difficult for sentiment classifiers for several reasons.", "labels": [], "entities": [{"text": "sentiment classifiers", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8404425084590912}]}, {"text": "First, strong negative sentiment is often expressed in an understated or ironic manner.", "labels": [], "entities": []}, {"text": "For example, \"Better at putting you to sleep than a sound machine.\"", "labels": [], "entities": []}, {"text": "For strong positive examples in the dataset, there is often difficult vocabulary and morphologically creative uses of language, e. g., \"It is a kickass , dense sci-fi action thriller hybrid that delivers and then some.\", while strong negative examples often contain sarcasm or non-standard spelling, e. g., \"All prints of this film should be sent to and buried on Pluto.\".", "labels": [], "entities": []}, {"text": "Negation Negation, which accounts for 97 errors, directly affects the classification of polar sentence.", "labels": [], "entities": []}, {"text": "Therefore, we look at the differences between correctly and incorrectly classified sentences containing negation, by analyzing 100 correctly and incorrectly classified sentences containing negation.", "labels": [], "entities": []}, {"text": "From our analysis, there is no specific negator that is more difficult to resolve regarding its effect on sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 106, "end_pos": 130, "type": "TASK", "confidence": 0.9442235231399536}]}, {"text": "We also perform an analysis of negation scope under the assumption that when a negator occurs farther from its negated element, it is more difficult for the sentiment classifier to correctly resolve the negation.", "labels": [], "entities": [{"text": "negation scope", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9304604530334473}]}, {"text": "Let d be the distance between the negator n and the relevant sentiment element se, such that d = |ind(se) \u2212 ind(n)| where the function ind calculates the index of a token in a sentence.", "labels": [], "entities": []}, {"text": "We find that the incorrectly classified examples have an average d of 2.7, while the correctly classified examples had 2.5.", "labels": [], "entities": []}, {"text": "This seems to rule out a problem of negation scope as the underlying difference.", "labels": [], "entities": [{"text": "negation scope", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.9247838854789734}]}, {"text": "High-level or clausal negation occurs when the negator negates a full clause, rather than an adjective or noun phrase, e. g., \"I don't think it is a particularly interesting film\".", "labels": [], "entities": []}, {"text": "In the dataset this phenomenon is found more prevalently in the incorrectly classified examples (8%) versus the correctly classified examples (3%), but does not occur often in absolute terms.", "labels": [], "entities": []}, {"text": "The main source of difference regarding correctly classifying examples involving negation seems to be irrelevant negation.", "labels": [], "entities": []}, {"text": "Irrelevant negation refers to cases where a sentence contains a negation but where the sentiment-bearing expression is not within the scope of negation.", "labels": [], "entities": [{"text": "Irrelevant negation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8029504716396332}]}, {"text": "In our data, there is a strong difference in the distribution of irrelevant negation in correctly and incorrectly classified examples (80% vs. 25%, respectively), suggesting that sentiment classifiers learn to ignore most occurrences of negation.", "labels": [], "entities": []}, {"text": "World Knowledge Examples from the dataset where world knowledge is necessary to correctly classify a sentence (81 sentences) include comparisons with entities commonly associated with positive or negative polarity, e. g., \"Elicits more groans from the audience than Jar Jar Binks, Scrappy Doo and Scooby Dumb, all wrapped up into one.\", analogies, e. g., \"Adam Sandler is to Gary Cooper what a gnat is to a racehorse.\", or rating scales, e. g., \"10/10 overall\".", "labels": [], "entities": []}, {"text": "This category is also highly correlated with sarcasm and irony.", "labels": [], "entities": []}, {"text": "In fact, irony is often defined as \"violating expectations\" \"I love it when people yell at me first thing in the morning.\" emoji \":)\" no sentiment \"The president will hold a talk tomorrow.\" mixed \"The plot was nice, but a little slow.\" incorrect label Any clearly incorrect label.", "labels": [], "entities": []}, {"text": "which presupposes that we possess a world knowledge containing expectations of a situation.", "labels": [], "entities": []}, {"text": "Amplified Amplifiers occur mainly in negative and strong positive examples, such as \"It's an awfully derivative story.\"", "labels": [], "entities": [{"text": "Amplifiers", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.5984781384468079}]}, {"text": "Most of the amplified sentences found in the dataset (71/79) contain amplifiers other than \"very\", such as \"super\", \"incredibly\", or \"so\".", "labels": [], "entities": [{"text": "71/79)", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.8874859064817429}]}, {"text": "Comparative Comparative sentiment, with 68 errors, is known to be difficult (, as it is necessary to determine which entity is on which side of the inequality.", "labels": [], "entities": [{"text": "Comparative Comparative sentiment", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6900644898414612}]}, {"text": "Sentences like \"Will probably stay in the shadow of its two older, more accessible Qatsi siblings\" are difficult for sentiment classifiers that do not model this phenomenon explicitly.", "labels": [], "entities": []}, {"text": "Sarcasm/Irony Sarcasm and irony (58 errors), which are often treated separately from sentiment analysis), are present mainly in negative and strong negative examples in the dataset.", "labels": [], "entities": [{"text": "irony", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9876263737678528}]}, {"text": "Correctly capturing sarcasm and irony is necessary to classify some negative and strong negative examples, e. g., \"If Melville is creatively a great whale, this film is canned tuna.\"", "labels": [], "entities": []}, {"text": "Shifters Shifters (50 errors), such as \"abandon\", \"lessen\", or \"reject\" are less common within the dataset, but normally move positive polarity words towards a more negative sentiment.", "labels": [], "entities": [{"text": "Shifters Shifters", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7645742297172546}, {"text": "abandon", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9723897576332092}]}, {"text": "The most common shifter is the word \"miss\", used as in \"We miss the quirky amazement that used to come along for an integral part of the ride.\"", "labels": [], "entities": [{"text": "miss", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9886459112167358}]}, {"text": "Emoji While the models handle most occurrences of emojis well, they falter more on the negative examples (46 errors).", "labels": [], "entities": [{"text": "Emoji", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9312294721603394}]}, {"text": "More than half of the examples in the dataset present positive emoji with a negative gold label, such as \"Pricess Leia is going to be gutted!", "labels": [], "entities": [{"text": "Pricess", "start_pos": 106, "end_pos": 113, "type": "METRIC", "confidence": 0.7515198588371277}]}, {"text": "Modality None of the state-of-the-art sentiment systems deals explicitly with modality (38 total errors).", "labels": [], "entities": []}, {"text": "While in many of the examples modality does not express a different sentiment than the same sentence without modality, in the dataset there are examples that do, e. g., \"Still, I thought it could have been more.\"", "labels": [], "entities": []}, {"text": "Morphology While not the most prominent label (31 errors), the examples in the dataset that contain morphological features that effect sentiment are normally strong positive or strong negative.", "labels": [], "entities": []}, {"text": "This most often contains creative use of English morphology, e. g., \"It was fan-freakin-tastic!\" or \"It's hyper-cliched\".", "labels": [], "entities": []}, {"text": "Reducers Reducers (13 errors), such as \"kind of\", \"less\", or \"all that\" cooccur with both positive and negative polar words within the dataset, and  tend to lead to positive or neutral sentiment, e. g., \"It was a lot less hassle.\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics for the sentence-level annotations in  each dataset.", "labels": [], "entities": []}, {"text": " Table 6. Note that we did not limit the number  of labels that the annotator could assign to each  sentence and in principle they should assign all  suitable labels during annotation.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy of models on the sentiment datasets, where a different classifier is trained for each dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9661122560501099}, {"text": "sentiment datasets", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.7052729278802872}]}, {"text": " Table 3: Statistics of dataset, including the number of sentences from each dataset and for each label, the percentage  of the original dataset kept in the dataset, and average length (in tokens) of sentences.", "labels": [], "entities": []}, {"text": " Table 5: Number of labels for each category in anno- tation study. Bold numbers indicate the five most fre- quent sources of errors. The total number of labels does  not sum to the number of sentences in the dataset, as  each sentence can have multiple labels.", "labels": [], "entities": [{"text": "anno- tation", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.6761232614517212}]}, {"text": " Table 7: Per category accuracy and relative improve- ment (last column) of BERT model trained on SST sen- tences (8,544) and SST phrases (155,019).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9789886474609375}, {"text": "relative improve- ment", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.7615353614091873}, {"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9618383646011353}]}]}