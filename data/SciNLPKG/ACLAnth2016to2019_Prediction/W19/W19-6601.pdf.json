{"title": [{"text": "Online Sentence Segmentation for Simultaneous Interpretation using Multi-Shifted Recurrent Neural Network", "labels": [], "entities": [{"text": "Sentence Segmentation", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7724466323852539}, {"text": "Simultaneous Interpretation", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.9346613585948944}]}], "abstractContent": [{"text": "This paper is devoted to developing a recurrent neural network (RNN) solution for segmenting the unpunctuated transcripts generated by automatic speech recognition for simultaneous interpretation.", "labels": [], "entities": [{"text": "segmenting the unpunctuated transcripts generated by automatic speech recognition", "start_pos": 82, "end_pos": 163, "type": "TASK", "confidence": 0.664393405119578}]}, {"text": "RNNs are effective in capturing long-distance dependencies and straightforward for online decoding.", "labels": [], "entities": []}, {"text": "Thus, they are ideal for the task compared to the conventional n-gram language model (LM) based approaches and recent neural machine translation based approaches.", "labels": [], "entities": []}, {"text": "This paper proposes a multi-shifted RNN to address the trade-off between accuracy and latency, which is one of the key characteristics of the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9987989664077759}, {"text": "latency", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9427505731582642}]}, {"text": "Experiments show that our proposed method improves the segmentation accuracy measured in F 1 by 21.1% while maintains approximately the same latency, and reduces the BLEU loss to the oracle segmenta-tion by 28.6%, when compared to a strong baseline of the RNN LM-based method.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.9555791616439819}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9055403470993042}, {"text": "F 1", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.92641881108284}, {"text": "BLEU loss", "start_pos": 166, "end_pos": 175, "type": "METRIC", "confidence": 0.9884727299213409}]}, {"text": "Our online sentence segmentation toolkit is open-sourced 1 to promote the field.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.7475687563419342}]}], "introductionContent": [{"text": "Simultaneous interpretation (SI) is to translate one spoken language into another spoken language in real time.", "labels": [], "entities": [{"text": "Simultaneous interpretation (SI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9283842206001282}]}, {"text": "Automated SI typically requires integrating two fundamental natural language processing technologies -automatic speech recognition (ASR) and machine translation (MT).", "labels": [], "entities": [{"text": "Automated SI", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.5804728716611862}, {"text": "automatic speech recognition (ASR)", "start_pos": 102, "end_pos": 136, "type": "TASK", "confidence": 0.8055761357148489}, {"text": "machine translation (MT)", "start_pos": 141, "end_pos": 165, "type": "TASK", "confidence": 0.8224915385246276}]}, {"text": "Both technologies have become quite capable after half ac \ud97b\udf59 2019 The authors.", "labels": [], "entities": []}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND.", "labels": [], "entities": []}, {"text": "Online sentence segmentation smoothly bridges the gap between ASR and MT through segmenting the transcripts generated by ASR engines into sentences in real time.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7333575338125229}, {"text": "ASR", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9596610069274902}, {"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.7520263195037842}]}, {"text": "As a matter of fact, the task is non-trivial.", "labels": [], "entities": []}, {"text": "The example presented in is extracted from a TED talk 2 , which is used in the experiments of this paper.", "labels": [], "entities": [{"text": "TED talk 2", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.7429017027219137}]}, {"text": "Readers may find the raw sequence of words difficult to read.", "labels": [], "entities": []}, {"text": "However, the readability is greatly improved once it is segmented as follows, \u2022 even cats were watching this video \u2022 cats were watching other cats watch this video \u2022 but what 's important here is the creativity that it inspired amongst this techie geeky internet culture \u2022 there were remixes \u2022 someone made an old timey version \u2022 and then it went international \u2022 there were remixes someone made an old timey version Therefore, sentence segmentation is a meaningful natural language processing task.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 427, "end_pos": 448, "type": "TASK", "confidence": 0.8165910840034485}]}, {"text": "Correctly segmenting an ASR transcript requires a certain level of understanding the content.", "labels": [], "entities": [{"text": "ASR transcript", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7459846138954163}]}, {"text": "This paper proposes a multi-shifted RNN to approach the problem of online sentence segmentation, which shifts target signals by multiple durations of time as illustrated by.", "labels": [], "entities": [{"text": "online sentence segmentation", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.645114004611969}]}, {"text": "This design emphasizes two central elements of the task -accuracy and latency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9992037415504456}, {"text": "latency", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9422086477279663}]}, {"text": "Usually, predicting a sentence boundary immediately after a last input word is not wise.", "labels": [], "entities": [{"text": "predicting a sentence boundary immediately after a last input word", "start_pos": 9, "end_pos": 75, "type": "TASK", "confidence": 0.8562453925609589}]}, {"text": "Instead, waiting and checking a few words to make sure that anew sentence has started can raise the accuracy at the cost of latency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9994533658027649}, {"text": "latency", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9522340893745422}]}, {"text": "Shifting the target signals n time stamps right implements the idea of waiting and checking more words, but the optimal n varies on different textual contexts.", "labels": [], "entities": []}, {"text": "Therefore, the proposed network learns multiple shifted target signals during training, and maintains multiple pathway of trading latency with accuracy during test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9982559084892273}]}, {"text": "Experimental results demonstrate the effectiveness of our proposed method.", "labels": [], "entities": []}, {"text": "The contributions of this paper include, \u2022 proposing a multi-shifted RNN for online sentence segmentation; \u2022 achieving competitive performance on a realworld corpus; \u2022 releasing the source code for reproducibility.", "labels": [], "entities": [{"text": "online sentence segmentation", "start_pos": 77, "end_pos": 105, "type": "TASK", "confidence": 0.6482393046220144}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews a baseline n-gram LMbased method which serves as a foundation of our method.", "labels": [], "entities": []}, {"text": "Section 3 describes our method from the aspects of training, decoding and tuning.", "labels": [], "entities": []}, {"text": "Section 4 presents the experiments.", "labels": [], "entities": []}, {"text": "Section 5 compares our method with some related works.", "labels": [], "entities": []}, {"text": "Section 6 concludes this paper with a description on future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "The corpora from the shared task in the international workshop on spoken language translation Two operations are applied in order to simulate the transcripts generated by ASR following the setting in ( and (.", "labels": [], "entities": [{"text": "spoken language translation", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6836156447728475}]}, {"text": "First, because ASR engines normally do not produce punctuation, punctuation is removed from the text.", "labels": [], "entities": [{"text": "ASR", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9794700741767883}]}, {"text": "Second, because ASR engines split output based on long pauses, and each of the output contains multiple sentences; every 10 neighboring sentences in the development and test set are concatenated to form an input for sentence segmentation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9819851517677307}, {"text": "sentence segmentation", "start_pos": 216, "end_pos": 237, "type": "TASK", "confidence": 0.7320983707904816}]}, {"text": "Two baselines are used in the experiments.", "labels": [], "entities": []}, {"text": "The first baseline is the n-gram LM-based method proposed by.", "labels": [], "entities": []}, {"text": "The toolkit of SRILM is used to build n-gram LMs with Kneser-Ney Smoothing and an order of 6.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.6653209924697876}]}, {"text": "The second baseline is an extension of the first one by replacing the n-gram LM with an RNN LM.", "labels": [], "entities": []}, {"text": "The settings of RNN LM follow the large LSTM setting used by which consists of two layers of 1500 LSTM units, and a vocabulary size of 10K.", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 16, "end_pos": 22, "type": "TASK", "confidence": 0.642780601978302}]}, {"text": "A dropout of 0.65 is applied to the non-recurrent connections.", "labels": [], "entities": []}, {"text": "The proposed neural network adopts three layers of 512 LSTM units, and an input vocabulary size of 20K according to our pilot experiments.", "labels": [], "entities": []}, {"text": "The output dimension m is 6.", "labels": [], "entities": []}, {"text": "A dropout of 0.50 is applied to the non-recurrent connections.", "labels": [], "entities": []}, {"text": "Larger networks have been tried in our experiments, but no significant improvement has been observed.", "labels": [], "entities": []}, {"text": "Both the proposed network and RNN LM are trained using SGD with a start learning rate of 1.0.", "labels": [], "entities": []}, {"text": "The cross-entropy on the development set is measured after each epoch.", "labels": [], "entities": []}, {"text": "When the development cross-entropy stops decreasing, the learning rate starts to decay by 0.5 per epoch.", "labels": [], "entities": []}, {"text": "The training terminates when no improvement is made during 3 continuous attempts of decaying learning rates.", "labels": [], "entities": []}, {"text": "The numbers of future words for the two baseline methods are enumerated from 1 to 6, and the decoding thresholds are tuned by a grid search from -1.6 to 1.6 with a step of 0.2.", "labels": [], "entities": []}, {"text": "The decoding threshold vector for the proposed method is tuned by Algorithm 1 with \u03b8 0 = (0.9, 0.8, 0.7, 0.6, 0.5, 0.4), \u00b5 = 0.1, and \u03bd = 0.04 . The maximum sentence length is set to 40 for all the methods, which covers approximately 95% development and test sentences.", "labels": [], "entities": []}, {"text": "The software is implemented using C++ and NVIDIA's GPU-accelerated libraries.", "labels": [], "entities": []}, {"text": "The experiments are run on a workstation equipped with an Intel Xeon CPU E5-2630 and a GPU Quadro M4000.", "labels": [], "entities": []}, {"text": "The three methods -two baselines and the proposed method -first learn their models on the source side of the standard training set ().", "labels": [], "entities": []}, {"text": "The n-gram LM-based method learns a 6-ordered n-gram LM whose perplexity on the development set is 148.17.", "labels": [], "entities": []}, {"text": "The RNN LM-based method learns an RNN LM with a development perplexity of 62.93.", "labels": [], "entities": []}, {"text": "The proposed method learns a network model with a development cross entropy of 0.441.", "labels": [], "entities": []}, {"text": "After that, each method tunes its decoding parameters on the development set to maximize the score (the equation 6).", "labels": [], "entities": []}, {"text": "In the end, each method decodes the test set using its learned method and tuned parameters.", "labels": [], "entities": []}, {"text": "The evaluation of the results is presented in.", "labels": [], "entities": []}, {"text": "The proposed method outperforms the stronger baseline of the RNN LM-based method by 18.8% on the measurement of score, which is quite large.", "labels": [], "entities": [{"text": "measurement of score", "start_pos": 97, "end_pos": 117, "type": "METRIC", "confidence": 0.6807286540667216}]}, {"text": "The improvement is caused by the rise of the measurement of accuracy -F 1 -which is improved by 13.5%, and the stableness of the latency which is only enlarged by 3.4%.", "labels": [], "entities": [{"text": "measurement", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9715310335159302}, {"text": "accuracy -F 1 -", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.8946844696998596}, {"text": "latency", "start_pos": 129, "end_pos": 136, "type": "METRIC", "confidence": 0.9655736088752747}]}, {"text": "This result indicates that the architecture of the proposed network suits the task better than that of RNN LM.", "labels": [], "entities": []}, {"text": "In addition, the RNN LM-based method outperforms the n-gram LM-based method by 67.7%.", "labels": [], "entities": []}, {"text": "This confirms our expectation that RNN can model a sentence better than n-gram as it can capture long-distance dependencies.", "labels": [], "entities": []}, {"text": "The table also presents the latency of the oracle segmentation which assumes that every sentence is submitted to MT engines as soon as it ends.", "labels": [], "entities": [{"text": "oracle segmentation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7353385239839554}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.860769510269165}]}, {"text": "Suppose the i-th sentence has l i words, the average latency per word would be . On the experimental test set in, the latency of the oracle segmentation is 8.126, and the latency of the proposed method is 12.386.", "labels": [], "entities": [{"text": "latency", "start_pos": 171, "end_pos": 178, "type": "METRIC", "confidence": 0.977262556552887}]}, {"text": "This approximately means a delay of 4.2 words per sentence, which is acceptable in a real-world environment.", "labels": [], "entities": [{"text": "delay", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9502871036529541}]}, {"text": "Luong et al. and show that large-scale out-domain training data and model adaption can effective improve the quality of NMT models.", "labels": [], "entities": []}, {"text": "They first train models on the union set of in-domain and out-domain data, and then adapt the models by resuming training on in-domain data only.", "labels": [], "entities": []}, {"text": "Inspired by their work, we scale up the standard training set to pursuit better performance for sentence segmentation (see for details) . Through scaling up training set and model adaptation, the development perplexity of the RNN LM is reduced by 8.06% (from 62.93 to 57.86), and the development cross entropy of the model learned by the proposed method decreases by 0.082 (from 0.441 to 0.359).", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.7200856804847717}, {"text": "RNN LM", "start_pos": 226, "end_pos": 232, "type": "DATASET", "confidence": 0.8220473229885101}]}, {"text": "The n-gram LM is adapted by linear interpretation.", "labels": [], "entities": []}, {"text": "The mixture weight is tuned to minimize the development perplexity, whose value turns out to be 0.7.", "labels": [], "entities": [{"text": "development perplexity", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.9538035690784454}]}, {"text": "The development perplexity of the n-gram LM is reduced by 8.25% (from 148.16 to 135.93) Each method again tunes its decoding parameters, and then decodes the test set as described in Section 4.2.", "labels": [], "entities": []}, {"text": "summarizes the results, and compares them with the previous ones on the standard training set.", "labels": [], "entities": []}, {"text": "The performance of all three methods is found to be improved, while the proposed method achieves the largest improvement.", "labels": [], "entities": []}, {"text": "The detailed comparison between the two results (the last row in shows that all the individual performance measurements have been improved.", "labels": [], "entities": []}, {"text": "Moreover, the optimal thresholds generally get lower.", "labels": [], "entities": []}, {"text": "This clearly indicates that the quality of the trained model has been improved, which is quite impressive.", "labels": [], "entities": []}, {"text": "The same effects also     happen on the RNN LM-based method.", "labels": [], "entities": []}, {"text": "Therefore, adapting neural network models through resuming training is a very effective technique.", "labels": [], "entities": []}, {"text": "The best segmentations of each method, which are listed in in bold font, are post-processed to recover case and punctuation, and then piped into an English-to-Chinese NMT engine.", "labels": [], "entities": []}, {"text": "The postprocessing is conducted by a monotone phrasebased statistical MT system, which is trained to translate lower-cased unpunctuated sentences to cased punctuated sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.8328450918197632}]}, {"text": "Moses toolkit () is used.", "labels": [], "entities": [{"text": "Moses toolkit", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9188286066055298}]}, {"text": "The NMT engine is an implementation of attention-based encoder-decoder proposed by and , and the model is trained and tuned on an  The results show that the proposed method achieves the highest BLEU, which is lower than that of the oracle segmentation only by 0.25.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.9994747042655945}]}, {"text": "The improvement compared to the stronger baseline of the RNN LM-based method is 0.10 BLEU point, or 28.6% calculated by 0.10 / 0.35.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9994833469390869}]}], "tableCaptions": [{"text": " Table 3: Experimental Corpora.  \u2020 The subset consists of the first one million sentence pairs.", "labels": [], "entities": []}, {"text": " Table 4: Performance after Training on Standard Set.  \u2020 Improvement versus the stronger baseline of RNN LM.  \u2021 The optimal  threshold vector is (1.0, 0.8, 0.8, 0.5, 0.5, 0.3).", "labels": [], "entities": [{"text": "RNN LM", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.7706400156021118}]}, {"text": " Table 5: Segmentation Performance after Adapting the Models Trained on Scaled-up Set.  \u2020 Compared to the best score of each  method on the standard training set.  \u2021 The optimal threshold vector is (0.9, 0.8, 0.5, 0.5, 0.5, 0.4)", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9142991900444031}]}, {"text": " Table 6: Evaluation of End-to-end Translation Quality.  \u2020  Compared to the BLEU of the oracle sentence segmentation.   \u2021 Compared to the stronger baseline of RNN LM.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9994064569473267}, {"text": "oracle sentence segmentation", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.7265103856722513}, {"text": "RNN LM", "start_pos": 159, "end_pos": 165, "type": "DATASET", "confidence": 0.8001530468463898}]}]}