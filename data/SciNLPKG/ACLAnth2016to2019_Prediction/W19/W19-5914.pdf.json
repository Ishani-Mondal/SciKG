{"title": [{"text": "Spoken Conversational Search for General Knowledge", "labels": [], "entities": [{"text": "Spoken Conversational Search for General Knowledge", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.822858194510142}]}], "abstractContent": [{"text": "We present a spoken conversational question answering proof of concept that is able to answer questions about general knowledge from Wikidata 1.", "labels": [], "entities": []}, {"text": "The dialogue component does not only orchestrate various components but also solve coreferences and ellipsis.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conversational question answering is an open research problem.", "labels": [], "entities": [{"text": "Conversational question answering", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.859540601571401}]}, {"text": "It studies the integration of question answering (QA) systems in a dialogue system(DS).", "labels": [], "entities": [{"text": "integration of question answering (QA)", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.7585034455571856}]}, {"text": "Not long ago, each of these research subjects were studied separately; only very recently has studying the intersection between them gained increasing interest (.", "labels": [], "entities": []}, {"text": "We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7608603537082672}]}, {"text": "It solves coreference and ellipsis by modelling context.", "labels": [], "entities": [{"text": "coreference", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9622604250907898}]}, {"text": "Furthermore, it is extensible, thus other components such as neural approaches for question-answering can be easily integrated.", "labels": [], "entities": []}, {"text": "It is also possible to collect a dialogue corpus from its iterations.", "labels": [], "entities": []}, {"text": "In contrast to most conversational systems which support only speech, two input and output modalities are supported speech and text.", "labels": [], "entities": []}, {"text": "Thus it is possible to let the user check the answers by either asking relevant Wikipedia excerpts or by navigating through the retrieved name entities or by exploring the answer details of the QA components: the confidence score as well as the set of explored triplets.", "labels": [], "entities": []}, {"text": "Therefore, the user has the final word to consider the answer as corrector incorrect and to 1 https://www.wikidata.org provide a reward, which can be used in the future for training reinforcement learning algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation of the individual components of the proposed system was performed outside the scope of this work.", "labels": [], "entities": []}, {"text": "We evaluated out-ofcontext questions, as well as the coreference resolution module.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.9306789636611938}]}, {"text": "Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French) 4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively 5 . We also evaluated the coreference resolution model on the test-set of CALOR, obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively.", "labels": [], "entities": [{"text": "Bench'It, a dataset", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.9310489892959595}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.8781934380531311}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9992708563804626}, {"text": "F-1", "start_pos": 201, "end_pos": 204, "type": "METRIC", "confidence": 0.9964313507080078}, {"text": "coreference resolution", "start_pos": 273, "end_pos": 295, "type": "TASK", "confidence": 0.8818902373313904}, {"text": "CALOR", "start_pos": 321, "end_pos": 326, "type": "DATASET", "confidence": 0.7465678453445435}, {"text": "precision", "start_pos": 349, "end_pos": 358, "type": "METRIC", "confidence": 0.9981936812400818}, {"text": "recall", "start_pos": 360, "end_pos": 366, "type": "METRIC", "confidence": 0.9992930889129639}, {"text": "F-1", "start_pos": 371, "end_pos": 374, "type": "METRIC", "confidence": 0.9974192380905151}]}, {"text": "The same model reached a average F-1 of 68.8% for English (.", "labels": [], "entities": [{"text": "F-1", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9990915060043335}]}, {"text": "Comparable measurements are not available for French.", "labels": [], "entities": [{"text": "Comparable", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9409010410308838}]}, {"text": "F-1 scores for French are believed to be lower because of the lower amount of annotated data.", "labels": [], "entities": [{"text": "F-1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9732708930969238}]}], "tableCaptions": [{"text": " Table 1: Subset of the corpus CALOR used for train-", "labels": [], "entities": [{"text": "CALOR", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.7401833534240723}]}]}