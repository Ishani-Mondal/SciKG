{"title": [{"text": "Multi-label Hate Speech and Abusive Language Detection in Indonesian Twitter", "labels": [], "entities": [{"text": "Multi-label Hate Speech and Abusive Language Detection", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.6999301654951913}]}], "abstractContent": [{"text": "Hate speech and abusive language spreading on social media need to be detected automatically to avoid conflicts between citizens.", "labels": [], "entities": [{"text": "Hate speech and abusive language spreading on social media", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7409795456462436}]}, {"text": "Moreover, hate speech has a target, category, and level that also need to be detected to help the authority in prioritizing which hate speech must be addressed immediately.", "labels": [], "entities": []}, {"text": "This research discusses multi-label text classification for abusive language and hate speech detection including detecting the target, category, and level of hate speech in Indonesian Twitter using machine learning approaches with Support Vector Machine (SVM), Naive Bayes (NB), and Random Forest Decision Tree (RFDT) classifier and Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC) as the data transformation method.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.6821369528770447}, {"text": "hate speech detection", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.7159399588902792}]}, {"text": "We used several kinds of feature extractions which are term frequency, orthography, and lexicon features.", "labels": [], "entities": []}, {"text": "Our experiment results show that in general the RFDT classifier using LP as the transformation method gives the best accuracy with fast computational time.", "labels": [], "entities": [{"text": "RFDT classifier", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.49352769553661346}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9987140893936157}]}], "introductionContent": [{"text": "Hate speech is a director indirect speech toward a person or group containing hatred based on something inherent to that person or group . Factors that are often used as bases of hatred include ethnicity, religion, disability, gender, and sexual orientation.", "labels": [], "entities": []}, {"text": "Hate speech spreading is a very dangerous action which can have some negative effects such as discrimination, social conflict, and even human genocide.", "labels": [], "entities": [{"text": "Hate speech spreading", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8717124462127686}]}, {"text": "One of the most horrific genocides caused by the act of spreading hate speech Komisi Nasional Hak Asasi Manusia (Komnas HAM) is an independent institution that functions to carryout studies, research, counseling, monitoring, and mediation of human rights in Indonesia.", "labels": [], "entities": [{"text": "hate speech Komisi Nasional Hak Asasi Manusia (Komnas HAM)", "start_pos": 66, "end_pos": 124, "type": "TASK", "confidence": 0.6553244292736053}]}, {"text": "See https://www.komnasham.go.id/index.php/ about/1/tentang-komnas-ham.html was the Tutsi ethnic genocide in Rwanda in 1994.", "labels": [], "entities": []}, {"text": "The cause of the tragedy was hate speech propagated by some groups, claiming that the cause of increasing pressure in politics, economic and social was the Tutsi ethnic.", "labels": [], "entities": []}, {"text": "In everyday life, especially in social media, the hate speech spreading is often accompanied with abusive language (.", "labels": [], "entities": [{"text": "hate speech spreading", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6389379998048147}]}, {"text": "Abusive language is an utterance that contains abusive words/phrases that is conveyed to the interlocutor (individuals or groups), both verbally and in writing.", "labels": [], "entities": [{"text": "Abusive language is an utterance that contains abusive words/phrases that is conveyed to the interlocutor (individuals or groups), both verbally and in writing", "start_pos": 0, "end_pos": 159, "type": "Description", "confidence": 0.7483042712722506}]}, {"text": "Hate speech that contains abusive words/phrases often accelerates the occurrence of social conflict because of the use of the abusive words/phrases that triggers emotions.", "labels": [], "entities": []}, {"text": "In general, the use of abusive words aimed to curse someone (spreading hate speech) in Indonesia is divided into three types that are words, phrases, and clauses.", "labels": [], "entities": []}, {"text": "The spread of hate speech that is accompanied with abusive language often accelerates the occurrence of social conflict because of the use of the abusive words/phrases that triggers emotions.", "labels": [], "entities": []}, {"text": "Although abusive language are sometimes just being used as jokes (not to offend someone), the use of abusive language in social media still can lead to conflict because of misunderstand-ings among netizens (.", "labels": [], "entities": []}, {"text": "Moreover, children could be exposed to language inappropriate for their age from those abusive language scattered in their social media).", "labels": [], "entities": []}, {"text": "The hate speech and abusive language on social media must be detected to avoid conflicts between citizens and children learning the hate speech and inappropriate language from the social media they use.", "labels": [], "entities": []}, {"text": "In recent years, many researchers have done research in hate speech detection) and abusive language detection) in various social media genres and languages.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6517573595046997}, {"text": "abusive language detection", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.6605470776557922}]}, {"text": "According to, a hate speech has a certain target, category, and level.", "labels": [], "entities": []}, {"text": "Hate speeches can belong to a certain category such as ethnicity, religion, race, sexual orientation, etc. that are targeted to a particular individual or group with a certain level of hatred.", "labels": [], "entities": []}, {"text": "However, based on our literature study, there has been no research on abusive language and hate speech detection including the detection of hate speech target, category, and level conducted simultaneously.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 91, "end_pos": 112, "type": "TASK", "confidence": 0.6383702258268992}]}, {"text": "Many research in hate speech detection just identifying whether a text is hate speech or not.) performed research on hate speech level detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7540365060170492}, {"text": "hate speech level detection", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.7510150372982025}]}, {"text": "Their research was done to classifying Italian Facebook post and comment into three labels which are no hate speech, weak hate speech, and strong hate speech.", "labels": [], "entities": [{"text": "classifying Italian Facebook post and comment", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.6197241147359213}]}, {"text": "However, () did not classifying the target and category of hate speech.", "labels": [], "entities": []}, {"text": "Similar to research in hate speech detection, many studies in abusive language detection) also just identify whether a text is abusive language or not.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 23, "end_pos": 44, "type": "TASK", "confidence": 0.7865810990333557}, {"text": "abusive language detection", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.6902513901392618}]}, {"text": ") conducted research on hate speech and abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6580617229143778}]}, {"text": "Their research was done to classify Indonesian tweet into three labels that are no hate speech, abusive but no hate speech, and abusive and hate speech.", "labels": [], "entities": []}, {"text": "However, same as other studeis on hate speech and abusive language detection,  did not classify the target and category of hate speech.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.7238113880157471}]}, {"text": "Depending on , detection of the hate speech target, category, and level is important to help authorities prioritize cases of hate speech that must be handled immediately.", "labels": [], "entities": []}, {"text": "In this work, we do research on hate speech and abusive language detection in Indonesian Twitter.", "labels": [], "entities": [{"text": "hate speech and abusive language detection", "start_pos": 32, "end_pos": 74, "type": "TASK", "confidence": 0.6564394384622574}]}, {"text": "We chose Twitter as our dataset because Twitter is one of the social media platforms in Indonesia that is often used to spread the hate speech and abusive language (.", "labels": [], "entities": []}, {"text": "This problem is a multi-label text classification problem, where a tweet can be no hate speech, no hate speech but abusive, hate speech but no abusive, and hate speech and abusive.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.6142930388450623}]}, {"text": "Furthermore, hate speech also has a certain target, category, and level.", "labels": [], "entities": []}, {"text": "In doing multi-label hate speech and abusive language detection, we use machine learning approach with several classifiers.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.660475879907608}]}, {"text": "The classifiers that we use include Support Vector Machine (SVM), Nave Bayes (NB), and Random Forest Decision Tree (RFDT) using problem transformation methods including Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC).", "labels": [], "entities": []}, {"text": "Based on several previous works, these three classifiers are algorithms that can produce pretty good performance for hate speech and abusive language detection in Indonesian (.", "labels": [], "entities": [{"text": "hate speech and abusive language detection", "start_pos": 117, "end_pos": 159, "type": "TASK", "confidence": 0.6743232558170954}]}, {"text": "We used several kinds of text classification features including term frequency (word n-grams and character n-grams), orthography (exclamation mark, question mark, uppercase, and lowercase), and sentiment lexicon (negative, positive, and abusive).", "labels": [], "entities": []}, {"text": "We use accuracy for evaluating our proposed approach (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.999440610408783}]}, {"text": "To validate our experiment results, we use 10-fold cross validation technique.", "labels": [], "entities": []}, {"text": "In this paper, we built an Indonesian Twitter dataset for abusive language and hate speech detection including detecting the target, category, and level of hate speech.", "labels": [], "entities": [{"text": "Indonesian Twitter dataset", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.7968788146972656}, {"text": "hate speech detection", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.6847588618596395}]}, {"text": "In general, the contributions of this research are: \u2022 Analyzing the target, category, and level of 2 Staff of Direktorat Tindak Pidana Siber Bareskrim Polri 3 Direktorat Tindak Pidana Siber Badan Reserse Kriminal Kepolisian Negara Republik Indonesia (Bareskrim Polri) is a directorate of the Indonesian national police that charge of fostering and carrying out the function of investigating and investigating cyber crimes in Indonesia.", "labels": [], "entities": []}, {"text": "See https://humas.polri.go.id/category/ satker/cyber-crime-bareskrim-polri/ hate speech to make an annotator guide and gold standard annotation for building Indonesian hate speech and abusive language dataset.", "labels": [], "entities": []}, {"text": "Our annotator is arranged based on ( and the results of interviews and discussions with the staff of Direktorat Tindak Pidana Siber Bareskrim Polri () and a linguistic expert).", "labels": [], "entities": [{"text": "Direktorat Tindak Pidana Siber Bareskrim Polri", "start_pos": 101, "end_pos": 147, "type": "DATASET", "confidence": 0.5457090387741724}]}, {"text": "\u2022 Building a dataset for abusive language and hate speech detection including detecting the target, category, and level of hate speech in Indonesian Twitter.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6984089612960815}]}, {"text": "We provide this research dataset for public so that it can be used by other researchers who are interested in doing future work of this paper.", "labels": [], "entities": []}, {"text": "\u2022 Conducting preliminaries experiments on multi-label abusive language and hate speech detection (including hate speech target, category, and level detection) in Indonesian Twitter using machine learning approaches.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6619718273480734}, {"text": "hate speech target, category, and level detection", "start_pos": 108, "end_pos": 157, "type": "TASK", "confidence": 0.48667917648951214}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We discuss hate speech target, category, and level in Indonesia in Section 2.", "labels": [], "entities": []}, {"text": "Our data collection and annotation process is described in Section 3.", "labels": [], "entities": [{"text": "data collection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.735255777835846}]}, {"text": "Section 4 presenting our experiment results and discussion.", "labels": [], "entities": []}, {"text": "Finally, the conclusions and future work of our research are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct two scenarios for the experiment.", "labels": [], "entities": []}, {"text": "The first experiment scenario uses multi-label classification to identify abusive language and hate speech including the target, categories, and level that contained in a tweet.", "labels": [], "entities": []}, {"text": "Meanwhile, the second scenario uses multi-label classification to identify abusive language and hate speech that contained in a tweet without identifying the target, categories, and level of hate speech.", "labels": [], "entities": []}, {"text": "Both of these scenarios are performed to find out the best classifier, transformation method, and features for each scenario.", "labels": [], "entities": []}, {"text": "In general, both the first scenario and the second scenario have the same flow that can be seen in.", "labels": [], "entities": []}, {"text": "First, we do data preprocessing in order to make classification process more efficient and gives better results.", "labels": [], "entities": [{"text": "classification", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.9667250514030457}]}, {"text": "We do five processes in data preprocessing consists of case folding, data cleaning, text normalization, stemming, and stop words removal.", "labels": [], "entities": [{"text": "data preprocessing", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7808908522129059}, {"text": "case folding", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.8508788645267487}, {"text": "data cleaning", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8620486855506897}, {"text": "text normalization", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8274528980255127}, {"text": "stemming", "start_pos": 104, "end_pos": 112, "type": "TASK", "confidence": 0.9334065914154053}, {"text": "stop words removal", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.6673267483711243}]}, {"text": "Case folding was done to make all character in lowercase in order to standardize character case.", "labels": [], "entities": [{"text": "Case folding", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8379366099834442}]}, {"text": "Next, data cleaning was done to remove unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation.", "labels": [], "entities": []}, {"text": "Since we do not use emoticon for feature extraction, we also remove emoticon in data cleaning process.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7490585148334503}]}, {"text": "After that, we do text normalization, which is changing non-formal words into formal ones.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.795810878276825}]}, {"text": "In this research, we do text normalization simply using dictionary obtained from the combination dictionaries from several previous works) and the dictionary that we build based on our dataset.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7833596467971802}]}, {"text": "Next, we do stemming to lemmatize words in every tweet.", "labels": [], "entities": []}, {"text": "In this paper, stemming was done using Nazief-Adriani Algorithm) that implemented using Sastrawi Library . For stop words removal, we used stop word list given by.", "labels": [], "entities": [{"text": "stemming", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.9625648260116577}, {"text": "stop words removal", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7371410528818766}]}, {"text": "The next step after data preprocessing is feature extraction.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8293361961841583}]}, {"text": "In this research, we used several kinds of feature extractions which are term frequency, orthography and lexicon features.", "labels": [], "entities": []}, {"text": "Term frequency features that we used in our experiments consist of word n-grams (unigram, bigrams, trigrams, and the combination of word unigram, bigrams, and trigrams) and character n-grams (trigrams, quadgrams, and the combination of character trigrams and quadgrams).", "labels": [], "entities": []}, {"text": "For the orthography feature, we used the number of exclamation mark, question mark, uppercase and lowercase.", "labels": [], "entities": []}, {"text": "Meanwhile, for the lexicon features, we used sentiment lexicon (negative and positive sentiment) given by and abusive lexicon that we built ourselves compiled from abusive words that used as queries when crawling Twit-ter data.", "labels": [], "entities": [{"text": "Twit-ter data", "start_pos": 213, "end_pos": 226, "type": "DATASET", "confidence": 0.8429574072360992}]}, {"text": "After the feature extraction process was done, the dataset is ready for classification process.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6972263902425766}, {"text": "classification", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.967542290687561}]}, {"text": "For the classifier, we used three machine learning classification algorithms which are Naive Bayes (NB), Support Vector Machine (SVM), and Random Forest Decision Tree (RFDT).", "labels": [], "entities": [{"text": "machine learning classification", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.7115535736083984}]}, {"text": "Based on the previous works (, those three algorithms can give a pretty good performance in doing hate speech and abusive language detection in Bahasa Indonesia (Indonesian language).", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 114, "end_pos": 140, "type": "TASK", "confidence": 0.6771419048309326}]}, {"text": "Notice that these three classifiers are single label output classifiers.", "labels": [], "entities": []}, {"text": "It means, those three classifiers cannot solve multi-label text classification directly.", "labels": [], "entities": [{"text": "multi-label text classification", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6371154487133026}]}, {"text": "To overcome this problem, we applied data transformation method ( such that the classifiers that we use can solve multilabel text classification problem.", "labels": [], "entities": [{"text": "data transformation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7182033807039261}, {"text": "multilabel text classification", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.6558666030565897}]}, {"text": "We used three data transformation methods that are Binary Relevance (BR), Label Power-set (LP), and Classifier Chains (CC) (.", "labels": [], "entities": [{"text": "data transformation", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.7503228187561035}, {"text": "Binary Relevance (BR)", "start_pos": 51, "end_pos": 72, "type": "METRIC", "confidence": 0.6405032455921174}]}, {"text": "In doing classification, we do classification using each type of feature extraction first.", "labels": [], "entities": []}, {"text": "After that, we do classification using the combination of best feature from each type of feature extraction.", "labels": [], "entities": []}, {"text": "For the evaluation, we used 10-fold cross-validation technique) with accuracy as the metric evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9995583891868591}]}, {"text": "Accuracy in this research is calculated using formula as follow (: (1) where Dis total document in corpus (dataset), \u02c6 L (i) is the prediction result of i th document, and L (i) is the actual label of i th document.", "labels": [], "entities": [{"text": "Dis", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9649404883384705}]}, {"text": "In this research, the first experiment scenario was done to know the combination of features, classifier, and data transformation method that we used that can give the best accuracy in identifying abusive language and hate speech including the target, categories, and level that was contained in a tweet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9971330165863037}]}, {"text": "To obtain that, we do experiments using every type of feature extractions first.", "labels": [], "entities": []}, {"text": "The experiment results for the best type of feature based on average accuracy using all classifiers and data transformation methods for the first scenario is in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9981123208999634}]}, {"text": "Based on average accuracy when doing experiments using every type of feature extractions (Ta- ble 1), we observe that for the first experiment scenario, the combination of word unigram, bigrams, and trigrams is the best word n-grams feature, character quadgrams is the best character n-grams feature, question mark is the best orthography feature, and negative sentiment is the best lexicon feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9971989393234253}]}, {"text": "However, if viewed individually, RFDT classifier with LP data transformation method when using word unigram feature gives the best performance with 66.12% of accuracy.", "labels": [], "entities": [{"text": "RFDT classifier", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.4839867502450943}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.997908353805542}]}, {"text": "After obtaining the best features of each type of features, we do experiments using the combination of best features from each type of features.", "labels": [], "entities": []}, {"text": "Based on experiments using the combination of best features from each type of features, we obtain that the combination of best features in this first experiment scenario does not give a significant result on the classification accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.8543817400932312}]}, {"text": "The best performance in experiments using the combination of best features is obtained when using RFDT classifier with LP data transformation method using the combination of character quadgrams, question mark, and negative sentiment just gives 65.73% of accuracy, still cannot exceed the accuracy given by the RFDT classifier with LP data transformation method using word unigram feature that can give 66.12% of accuracy.", "labels": [], "entities": [{"text": "RFDT", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.8377689123153687}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9988335967063904}, {"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.9910599589347839}, {"text": "accuracy", "start_pos": 412, "end_pos": 420, "type": "METRIC", "confidence": 0.9949150085449219}]}, {"text": "Based on classification results and data analysis, we observe that the word unigram features gives the best accuracy maybe because they represent the characteristics of each label.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.998842179775238}]}, {"text": "In each classification label, there are words that characterize the label.", "labels": [], "entities": []}, {"text": "For example, in hate speech label, each tweet that labeled as hate speech contain hate words such as abusive words that demean an individual or group (e.g. jelek (ugly), murahan (gimrack), etc.), hate words related to politics in Indonesia (e.g. antek (henchman), komunis (communist), etc.), and threatening/provoking words (e.g. bakar (burn), bunuh (kill), etc.).", "labels": [], "entities": [{"text": "hate speech label", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.8373586932818095}]}, {"text": "Next, for classifiers analysis, the ensemble method on RFDT relatively can give better accuracy compared to NB and SVM.", "labels": [], "entities": [{"text": "classifiers analysis", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.924513190984726}, {"text": "RFDT", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8302648067474365}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9989364743232727}]}, {"text": "For data transformation methods, LP can give the best accuracy because each unique label formed from the power-set process will have a correlation between labels so that it can reduce classification error ().", "labels": [], "entities": [{"text": "data transformation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7198339700698853}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9985350370407104}]}, {"text": "The second experiment scenario in this research was done to know the combination of features, classifiers, and data transformation methods that can give the best accuracy in identifying abusive language and hate speech in a tweet without identifying the target, categories, and level of hate speech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9969344139099121}]}, {"text": "Same as the first experiment scenario, we do experiments using every type of feature extractions first.", "labels": [], "entities": []}, {"text": "The experiment results for best each type of feature based on average accuracy using all classifier and data transformation method for the second scenario can be seen in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9979881048202515}]}, {"text": "Based on average accuracy when doing experiment using every type of feature extractions (Table 2), we obtain that for the second experiment scenario, word unigram feature is the best word n-grams feature, character quadgrams is the best character quadgrams feature, exclamation mark is the best orthography feature, and the combination of positive sentiment and abusive lexicon is the best lexicon feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9941613078117371}]}, {"text": "If viewed individually, RFDT classifier with LP data transformation method when using word unigram feature gives the best performance with 76.16% of accuracy.", "labels": [], "entities": [{"text": "RFDT classifier", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.5476128309965134}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9982178807258606}]}, {"text": "After obtaining the best features of each type of features, we do experiment using the combination of best features from each type of feature.", "labels": [], "entities": []}, {"text": "Based on the experiment using the combination of best features from each type of features, we obtain that the combination of best features in this second experiment scenario can give slightly better performance compared to when we do not combine the best feature.", "labels": [], "entities": []}, {"text": "RFDT classifier with LP data transformation method when using the combination of word unigram, character quadgrams, positive sentiment, and abusive lexicon features can gives the best performance with 77.36% of accuracy.", "labels": [], "entities": [{"text": "RFDT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8025716543197632}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9981566071510315}]}], "tableCaptions": []}