{"title": [{"text": "Finding Syntactic Representations in Neural Stacks", "labels": [], "entities": []}], "abstractContent": [{"text": "Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities.", "labels": [], "entities": []}, {"text": "It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differ-entiable stack is not always interpretable.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsuper-vised learning of constituency structure.", "labels": [], "entities": []}, {"text": "Using a technique due to Shen et al.", "labels": [], "entities": []}, {"text": "(2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language model-ing and classification objectives.", "labels": [], "entities": []}, {"text": "We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequential models such as long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997) have been proven capable of exhibiting qualitative behavior that reflects a sensitivity to regularities that are structurally conditioned, such as subject-verb agreement (.", "labels": [], "entities": []}, {"text": "However, detailed analysis of such models has shown that this apparent sensitivity to structure does not always generalize to inputs with a high degree of syntactic complexity.", "labels": [], "entities": []}, {"text": "These observations suggest that sequential models may not in fact be representing sentences in the kind of hierarchically organized representations that we might expect.", "labels": [], "entities": []}, {"text": "Stack-structured recurrent memory units; Yo- * Work completed while the author was at Yale University.", "labels": [], "entities": []}, {"text": "gatama et al., 2018; and others) offer a possible method for explicitly biasing neural networks to construct hierarchical representations and make use of them in their computation.", "labels": [], "entities": []}, {"text": "Since syntactic structures can often be modeled in a context-free manner, the correspondence between pushdown automata and contextfree grammars makes stacks a natural data structure for the computation of hierarchical relations.", "labels": [], "entities": []}, {"text": "Recently, have shown that stack-augmented RNNs (henceforth stack RNNs) have the ability to learn classical stack-based algorithms for computing contextfree transductions such as string reversal.", "labels": [], "entities": [{"text": "string reversal", "start_pos": 178, "end_pos": 193, "type": "TASK", "confidence": 0.703590139746666}]}, {"text": "However, they also find that such algorithms can be difficult for stack RNNs to learn.", "labels": [], "entities": []}, {"text": "For many contextfree tasks such as parenthesis matching, the stack RNN models they consider instead learn heuristic \"push-only\" strategies that essentially reduce the stack to unstructured recurrent memory.", "labels": [], "entities": [{"text": "parenthesis matching", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7224690616130829}]}, {"text": "Thus, even if stacks allow hierarchical regularities to be expressed, the bias that stack RNNs introduce does not guarantee that the networks will detect them.", "labels": [], "entities": []}, {"text": "The current paper aims to move beyond the work of in two ways.", "labels": [], "entities": []}, {"text": "While that work was based on artificially generated formal languages, this paper considers the ability of stack RNNs to succeed on tasks over natural language data.", "labels": [], "entities": []}, {"text": "Specifically, we train such networks on two objectives: language modeling and the number prediction task, a classification task proposed by to determine whether or not a model can capture structure-sensitive grammatical dependencies.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.8021981120109558}, {"text": "number prediction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.6870005428791046}]}, {"text": "Further, in addition to using visualizations of the pushing and popping actions of the stack RNN to assess its hierarchical sensitivity, we use a technique proposed by to assess the presence of implicitly-represented hierarchically-organized structure through the task of unsupervised parsing.", "labels": [], "entities": []}, {"text": "We extract syntactic constituency trees from our models and find that they produce parses that broadly reflect phrasal groupings of words in the input sentences, suggesting that our models utilize the stack in away that reflects the syntactic structures of input sentences.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the architecture of our stack models, which extends the architecture of by allowing multiple items to be pushed to, popped from, or read from the stack at each computational step.", "labels": [], "entities": []}, {"text": "Section 3 then describes our training procedure and reports results on language modeling and agreement classification.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7936291396617889}, {"text": "agreement classification", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.7749612033367157}]}, {"text": "Section 4 investigates the behavior of the stack RNNs trained on these tasks by visualizing their pushing behavior.", "labels": [], "entities": []}, {"text": "Building on this, Section 5 describes how we adapt unsupervised parsing algorithm to stack RNNs and evaluates the degree to which the resulting parses reveal structural representations in stack RNNs.", "labels": [], "entities": []}, {"text": "Section 6 discusses our observations, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compute F1 scores for the parses obtained from our Wikipedia language models by comparing against parses from Section 23 of the Penn Treebank's Wall Street Journal corpus (WSJ23,.", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9991143345832825}, {"text": "Penn Treebank's Wall Street Journal corpus", "start_pos": 131, "end_pos": 173, "type": "DATASET", "confidence": 0.9134450129100254}, {"text": "WSJ23", "start_pos": 175, "end_pos": 180, "type": "DATASET", "confidence": 0.5991156101226807}]}, {"text": "Since Algorithm 1 produces unlabeled binary trees, our evaluation uses the gold standard of, which consists of unlabeled, binarized versions of the WSJ23 trees.", "labels": [], "entities": [{"text": "WSJ23 trees", "start_pos": 148, "end_pos": 159, "type": "DATASET", "confidence": 0.9681179523468018}]}, {"text": "We also decapitalize the first word of every sentence for compatibility with our training data.", "labels": [], "entities": []}, {"text": "As a baseline, we the F1 scores attained by our models to those computed for purely right-and left-branching trees.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9782766401767731}]}, {"text": "A right-branching parse is equivalent to the output of Algorithm 1 on a sequence of equal syntactic distances.", "labels": [], "entities": []}, {"text": "Thus, the difference between the right-branching F1 score and our models' scores is a measure of the amount of syntactic information encoded by the push and pop strength sequences.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9705265164375305}]}, {"text": "We also compare our replication study for the parsing-reading-predict network models (PRPN-LM and PRPN-UP), the two syntactic-distance-based unsupervised parsers originally proposed by.", "labels": [], "entities": [{"text": "parsing-reading-predict network", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.8886055052280426}]}], "tableCaptions": [{"text": " Table 1: Results for language models trained on the  Wikipedia dataset.", "labels": [], "entities": [{"text": "Wikipedia dataset", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.9696353673934937}]}, {"text": " Table 2: Number prediction accuracies attained by the  three stack RNN classifiers and the LSTM baseline.", "labels": [], "entities": [{"text": "Number prediction", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6756707280874252}]}, {"text": " Table 3: Unsupervised parsing performance evaluated  on the WSJ23 dataset, attained by our stack models  (top), the right-and left-branching baselines (middle),  and the PRPN models (bottom).", "labels": [], "entities": [{"text": "WSJ23 dataset", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9879384636878967}]}]}