{"title": [{"text": "Learning multilingual topics through aspect extraction from monolingual texts", "labels": [], "entities": []}], "abstractContent": [{"text": "Texts rating products and services of all kind are omnipresent on the internet.", "labels": [], "entities": []}, {"text": "They come in various languages and often in such a large amount that it is very time-consuming to get an overview of all reviews.", "labels": [], "entities": []}, {"text": "The goal of this work is to facilitate the summarization of opinions written in multiple languages, exemplified on a corpus of English and Finnish reviews.", "labels": [], "entities": [{"text": "summarization of opinions written in multiple languages", "start_pos": 43, "end_pos": 98, "type": "TASK", "confidence": 0.8804929852485657}]}, {"text": "To this purpose, we propose a framework that extracts aspect terms from reviews and groups them to multilingual topic clusters.", "labels": [], "entities": []}, {"text": "For aspect extraction we work on texts of each language separately.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8837889134883881}]}, {"text": "We evaluate three methods, all based on neural networks.", "labels": [], "entities": []}, {"text": "One of them is supervised, one unsupervised, based on an attention mechanism and one a rule-based hybrid method.", "labels": [], "entities": []}, {"text": "We then group the extracted aspect terms into multilingual clusters, whereby we evaluate three different clustering methods and juxtapose a method that creates clusters from multilingual word embeddings with a method that first creates monolingual clusters for each language separately and then merges them.", "labels": [], "entities": []}, {"text": "We report on our results from a variety of experiments, observing the best results when clustering aspect terms extracted by the supervised method, using the k-means algorithm on multilingual embeddings.", "labels": [], "entities": [{"text": "clustering aspect terms extracted", "start_pos": 88, "end_pos": 121, "type": "TASK", "confidence": 0.8442639112472534}]}], "introductionContent": [{"text": "Texts expressing opinions about products are becoming important fora constantly increasing number of people.", "labels": [], "entities": []}, {"text": "From 2011 to 2017, the percentage of customers in the United States that reads online reviews to determine if a business is good or bad at least occasionally has grown from 71% to 93%.", "labels": [], "entities": []}, {"text": "Summarizing these reviews objectively can help customers in their choice of a product.", "labels": [], "entities": []}, {"text": "As only about 40% of internet content is in English (, analyzing reviews also in other languages appears vital to give a full picture of opinions about an entity.", "labels": [], "entities": []}, {"text": "In this work, we propose a framework that derives aspect terms from reviews written in different languages and then summarizes them into multilingual topics.", "labels": [], "entities": []}, {"text": "Aspect term extraction is apart of aspect-level sentiment analysis (ALSA).", "labels": [], "entities": [{"text": "Aspect term extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5998446345329285}, {"text": "aspect-level sentiment analysis", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6482534805933634}]}, {"text": "ALSA is able to provide a detailed analysis of opinions conveyed in a text by extracting the sentiment expressed towards each mentioned aspect.", "labels": [], "entities": [{"text": "ALSA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7823966145515442}]}, {"text": "For example, given the sentence \"the waitress was friendly\", it should extract a positive sentiment towards the aspect \"waitress\".", "labels": [], "entities": []}, {"text": "As creating summaries or statistics on these aspects alone would result in a lot of clutter, it is beneficial to group semantically similar words into \"topics\"; for example, aspect terms \"waitress\", \"waiter\" and \"bartender\" could form a topic \"staff\".", "labels": [], "entities": []}, {"text": "A survey by provides an overview about ALSA, but reports nearly exclusively on research on English corpora.", "labels": [], "entities": [{"text": "ALSA", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.9277559518814087}]}, {"text": "Indeed, the vast majority of research on Sentiment Analysis and also natural language processing (NLP) in general has been done with English.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.9719231128692627}, {"text": "natural language processing (NLP)", "start_pos": 69, "end_pos": 102, "type": "TASK", "confidence": 0.7606762448946635}]}, {"text": "Crosslingual NLP tries to utilize resources from a source language (generally English) for application on another target language.", "labels": [], "entities": [{"text": "Crosslingual NLP", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.703959196805954}]}, {"text": "This is of advantage for languages where resources (in our work: opinions) are very rare.", "labels": [], "entities": []}, {"text": "Multilingual NLP rather combines resources from different languages to analyze content written in them).", "labels": [], "entities": []}, {"text": "In our work, we adhere to the second approach, in order to make full use of documents available in each of the languages under consideration.", "labels": [], "entities": []}, {"text": "We address the question \"How can we extract mono-lingual aspect terms from reviews in different languages and then combine them into multilingual topics that describe the multilingual corpus?\".", "labels": [], "entities": []}, {"text": "We evaluate different methods for both the aspect extraction and the clustering step, focusing on ways of reducing human involvement and automating the learning process with minimal human input.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8295643925666809}]}, {"text": "As proof of concept of our approach, we study a corpus containing English and Finnish reviews of restaurants.", "labels": [], "entities": []}, {"text": "These two languages belong to unrelated families (Indoeuropean vs Uralic), differ in the amount of available resources (Finnish resources are sparse), and are linguistically very different: English is a language with comparatively little morphology, while Finnish is an agglutinative language with very rich morphology).", "labels": [], "entities": []}, {"text": "Our results show that multilingual topics can be extracted for even so different languages, making full use of the resources available in each language.", "labels": [], "entities": []}, {"text": "This study is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we discuss relevant research advances.", "labels": [], "entities": []}, {"text": "In section 3 we describe our framework, its components and the mechanisms used to evaluate each component.", "labels": [], "entities": []}, {"text": "Our experiments and results are presented in section 4.", "labels": [], "entities": []}, {"text": "In section 5 we discuss our findings.", "labels": [], "entities": []}, {"text": "The last section concludes the paper with and outlook on future work and extensions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Clusterings are evaluated against a pre-defined clustering.", "labels": [], "entities": []}, {"text": "While this is in some ways slightly against the original purpose of dynamically creating topic clusters without pre-defining the set of topics, it appears to be the only way of providing an objective evaluation.", "labels": [], "entities": []}, {"text": "In order to maintain the sense of dynamic clustering, we are mainly interested in seeing if clusters contain only terms belonging to one topic and not so much if there are clusters that could maybe be merged.", "labels": [], "entities": []}, {"text": "To give an example, we would like to penalize if bartender and salmon steak are in the same cluster, since they very clearly do not belong to the same topic.", "labels": [], "entities": []}, {"text": "We do not care much though if salmon steak and beef tenderloin are in the same cluster or not, since this is just a matter of how fine-grained the topic clustering is.", "labels": [], "entities": []}, {"text": "In order to meet this evaluation goal, we only define very few, broad clusters to evaluate against and seethe homogeneity score ( as our primary evaluation metric.", "labels": [], "entities": []}, {"text": "Homogeneity is maximized when all clusters contain only elements from one ground-truth class, with 1 being the maximum and 0 the minimum value.", "labels": [], "entities": []}, {"text": "Homogeneity strongly prefers fine-grained clustering over coarse grained ones; in the most extreme case, if a clustering would contain one cluster for each datapoint, homogeneity would be maximised.", "labels": [], "entities": []}, {"text": "We therefore don't accept too finegrained clusterings and also report the complementing score, completeness, which is maximized when all ground-truth classes contain only elements of one cluster.", "labels": [], "entities": [{"text": "completeness", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9793399572372437}]}, {"text": "This evaluation method is based on the way He et al. are evaluating their results.", "labels": [], "entities": []}, {"text": "The main difference is that they manually assign clusters to ground-truth classes, which we avoid.", "labels": [], "entities": []}, {"text": "This subsection describes the datasets used for the experiments.", "labels": [], "entities": []}, {"text": "Which data was used for which experiment is explained in detail in the subsections for each method.", "labels": [], "entities": []}, {"text": "English For English, we used SemEval 2016 Task 5 ( as the annotated dataset.", "labels": [], "entities": [{"text": "SemEval 2016 Task 5", "start_pos": 29, "end_pos": 48, "type": "DATASET", "confidence": 0.7532577216625214}]}, {"text": "This dataset consists of 2674 sentences, of which 2000 are considered training and 674 testing data.", "labels": [], "entities": []}, {"text": "Some of these sentences are marked as \"out of scope\" in the dataset and not annotated, so these were removed here.", "labels": [], "entities": []}, {"text": "For the hybrid and unsupervised methods, an additional corpus of 75000 restaurant reviews, which consist of 368551 sentences, was used.", "labels": [], "entities": []}, {"text": "These reviews area random selection of reviews provided by TrustYou GmbH 2 , which is a company focusing on review management for hotels and restaurants . The reviews were collected from different public sources, including TripAdvisor, Google, OpenTable, Facebook and Zomato.", "labels": [], "entities": []}, {"text": "We use the pretrained word embeddings provided by the GloVe project (Pennington et al., 2014), as this embedding set was used also in the original experiments for the supervised method (.", "labels": [], "entities": []}, {"text": "It was trained on the CommonCrawl corpus, a general-purpose text corpus that includes text from several billion web pages; the GloVe embeddings were trained on 840 billion tokens.", "labels": [], "entities": [{"text": "CommonCrawl corpus", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.9641937911510468}]}, {"text": "The GloVe set includes embeddings for 2.2 million words, the embeddings have 300 dimensions.", "labels": [], "entities": [{"text": "GloVe set", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8837853372097015}]}, {"text": "As domainspecific embeddings for the supervised method, we use the embedding set provided by the authors, which is 100-dimensional and was trained with FastText () on a dataset provided by Yelp.", "labels": [], "entities": []}, {"text": "Finnish For Finnish, it was more difficult to obtain a sizable corpus of restaurant reviews.", "labels": [], "entities": []}, {"text": "We ended up crawling the page eat.fi, a website for reviews of restaurants in Finland.", "labels": [], "entities": []}, {"text": "After filtering out all reviews written in a language different than Finnish with langid.py (Lui and Baldwin, 2012), the obtained dataset consists of 71730 reviews, or 346144 sentences.", "labels": [], "entities": []}, {"text": "250 of these reviews, consisting of 1076 sentences, were labelled manually by the author.", "labels": [], "entities": []}, {"text": "A subset of 70 reviews was additionally labelled by a native speaker; no major discrepancies in annotation were discovered.", "labels": [], "entities": []}, {"text": "As general word embeddings, we use the Finnish word embeddings provided by FastText ( , which are also 300 dimensional and were trained on both CommonCrawl and Wikipedia data, together about 6 billion tokens.", "labels": [], "entities": [{"text": "CommonCrawl and Wikipedia data", "start_pos": 144, "end_pos": 174, "type": "DATASET", "confidence": 0.744600385427475}]}, {"text": "The provided dataset contains embeddings for exactly 2 million words, but also includes sub-word information that allows inferring embeddings for unknown words ().", "labels": [], "entities": []}, {"text": "We trained domain-specific embeddings ourselves with FastText on the full dataset of restaurant reviews.", "labels": [], "entities": [{"text": "FastText", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9449310898780823}]}, {"text": "We used the default parameters of FastText to train 100-dimensional vectors.", "labels": [], "entities": []}, {"text": "shows the baseline values for the aspect extraction task.", "labels": [], "entities": [{"text": "aspect extraction task", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8526180187861124}]}, {"text": "For both languages, these values were computed on the complete annotated datasets, consisting of 2579 sentences for English and 1076 sentences for Finnish.", "labels": [], "entities": []}, {"text": "The experiments show that the dropout rate has very little influence on the result.", "labels": [], "entities": []}, {"text": "The learning rate however has a significant influence, with performance generally increasing with bigger learning rates, despite the high number of training iterations.", "labels": [], "entities": []}, {"text": "In all experiments, recall was at least slightly higher than precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9998160004615784}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9991839528083801}]}, {"text": "The result fora dropout of 70 and a learning rate of 10 \u22125 sticks out as the system in this case learned to always predict the label O.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.947462409734726}]}, {"text": "An explanation for this result could be the choice of the loss function: The negative log-likelihood is calculated for every possible target label, including O.", "labels": [], "entities": []}, {"text": "With O being, naturally for this task, the by far most frequent label, a slight bias towards choosing it can be expected.", "labels": [], "entities": [{"text": "O", "start_pos": 5, "end_pos": 6, "type": "METRIC", "confidence": 0.9273164868354797}]}, {"text": "This is however contrary to our evaluation method, for which always predicting O is the worst possible result.", "labels": [], "entities": [{"text": "O", "start_pos": 79, "end_pos": 80, "type": "METRIC", "confidence": 0.8798293471336365}]}, {"text": "It is unclear why this happens only for this specific combination of parameters.", "labels": [], "entities": []}, {"text": "We use mainly the same datasets as for the aspect extraction task.", "labels": [], "entities": [{"text": "aspect extraction task", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.8655403057734171}]}, {"text": "The English labeled data from SemEval already contains category information, assigning each aspect term one of the classes ambiance, drinks, food, location, restaurant and service.", "labels": [], "entities": []}, {"text": "The restaurant category is used for terms describing the restaurant in general and such that don't match one of the other categories.", "labels": [], "entities": []}, {"text": "For the Finnish labeled data, we manually assigned each unique aspect term to one of these six classes.", "labels": [], "entities": [{"text": "Finnish labeled data", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.6209609508514404}]}, {"text": "The English dataset contains 874 unique aspect terms, the Finnish dataset 623.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.9173762202262878}, {"text": "Finnish dataset 623", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9341529607772827}]}, {"text": "Evaluation was done for all experiments with the full labeled datasets.", "labels": [], "entities": []}, {"text": "We did experiments both with training the clusters on only the labeled datasets and with training them on the 5000 most frequent single-word aspect terms extracted by the best performing aspect extraction method from the full datasets.", "labels": [], "entities": []}, {"text": "For both English and Finnish, we use as word embeddings the pretrained FastText embeddings, which were trained on CommonCrawl and Wikipedia data and include subword information.", "labels": [], "entities": [{"text": "CommonCrawl and Wikipedia data", "start_pos": 114, "end_pos": 144, "type": "DATASET", "confidence": 0.8020335882902145}]}, {"text": "For Finnish, this is the same embedding set used as for the aspect extraction task, for English, it is different.", "labels": [], "entities": [{"text": "aspect extraction task", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8511741558710734}]}, {"text": "The reason for this is that we wanted to have embeddings trained in the same way for both languages, since we assumed that this would improve performance for the creation of multilingual embeddings from them.", "labels": [], "entities": []}, {"text": "Both for creating multilingual word embeddings and for clustering we only worked with the embeddings of words actually required.", "labels": [], "entities": []}, {"text": "This includes \u2022 all unique aspect terms, \u2022 the full vocabulary of our datasets, preprocessed as for the attention-based aspect extraction and clustering method (which is lemmatized and reduced to only include words that appear at least two times in the corpus), \u2022 words from the evaluation datasets.", "labels": [], "entities": [{"text": "attention-based aspect extraction and clustering", "start_pos": 104, "end_pos": 152, "type": "TASK", "confidence": 0.6641139209270477}]}, {"text": "In total, this results in 25808 words for English and 28327 words for Finnish.", "labels": [], "entities": []}, {"text": "We did this mainly because the script to create multilingual embeddings is very memoryintensive and was not possible to run with the full embedding sets on our machines.", "labels": [], "entities": []}, {"text": "Also, this procedure allowed us to utilize the sub-word information of the FastText embeddings and create embeddings for all words in our vocabulary, also such that are not part of the pretrained set.", "labels": [], "entities": []}, {"text": "Another thing to notice is that using the full dataset for training increases the performance of the clustering for English and the merged clusters; it is to notice however that the number of clusters with the full data is slightly larger for these experiments.", "labels": [], "entities": []}, {"text": "Also it can be seen that merging monolingual clusters works worse than using multilingual embeddings when training on only the annotated dataset, but slightly better when using the full dataset.", "labels": [], "entities": []}, {"text": "Completeness scores are about constant for all experiments at values around 0.15.", "labels": [], "entities": [{"text": "Completeness", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9885401129722595}]}, {"text": "shows results for clustering with the attention model from He et al. for different numbers of clusters.", "labels": [], "entities": []}, {"text": "We ran tests for 14, 28 and 42 different clusters, 14 being the number chosen in the original paper.", "labels": [], "entities": []}, {"text": "All other hyper-parameters we set to the best results from the aspect extraction experiments, see section 4.2.5 for details.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7560032904148102}]}, {"text": "We created the topic clusters from the complete review dataset.", "labels": [], "entities": []}, {"text": "Different to the other methods, homogeneity scores don't generally increase with more clusters here; the results for 42 clusters are significantly worse than for 14 or 28 clusters across all setups.", "labels": [], "entities": []}, {"text": "28 clusters work best for English and merged clusters, for Finnish and when using multilingual embeddings 14 clusters work better.", "labels": [], "entities": []}, {"text": "Using multilingual embeddings results in significantly worse values than merging clusters, English performs better than Finnish.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Baseline values for English and Finnish", "labels": [], "entities": []}, {"text": " Table 2: Results for different dropout and learning rate values in Finnish", "labels": [], "entities": []}, {"text": " Table 4: Results for using different correlation value cut-offs for English", "labels": [], "entities": []}, {"text": " Table 5: Results for Finnish with and without lemmatization", "labels": [], "entities": []}, {"text": " Table 6: Results for experiments with different minimum aspect weights", "labels": [], "entities": []}, {"text": " Table 7: Summary of the best results for all methods", "labels": [], "entities": []}, {"text": " Table 8: Homogeneity values for clusters trained on either the full or only the the  annotated dataset", "labels": [], "entities": []}, {"text": " Table 9: Best results for clustering with affinity propagation", "labels": [], "entities": []}, {"text": " Table 10: Homogeneity values for clustering with the attention-based model, using  different numbers of clusters", "labels": [], "entities": []}]}