{"title": [{"text": "Utilizing Pre-Trained Word Embeddings to Learn Classification Lexicons with Little Supervision", "labels": [], "entities": []}], "abstractContent": [{"text": "A lot of the decision making in financial institutions , regarding particularly investments and risk management, is data-driven.", "labels": [], "entities": [{"text": "risk management", "start_pos": 96, "end_pos": 111, "type": "TASK", "confidence": 0.7978084981441498}]}, {"text": "An important task to effectively gain insights from unstruc-tured text documents is text classification and in particular sentiment analysis.", "labels": [], "entities": [{"text": "text classification", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7935812473297119}, {"text": "sentiment analysis", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.9437114894390106}]}, {"text": "Sentiment lexicons, i.e. lists of words with corresponding sentiment orientations, area very valuable resource to build strong baseline models for sentiment analysis that are easy to interpret and computationally efficient.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9298285245895386}]}, {"text": "We present a novel method to learn classification lexicons from a labeled text corpus that incorporates word similarities in the form of pre-trained word em-beddings.", "labels": [], "entities": []}, {"text": "We show on two sentiment analysis tasks that utilizing pre-trained word embed-dings improves the accuracy over the baseline method.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.9592483341693878}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9991911053657532}]}, {"text": "The accuracy improvement is particularly large when labeled data is scarce, which is often the casein the financial domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993888139724731}]}, {"text": "Moreover , the new method can be used to generate sensible sentiment scores for words outside the labeled training corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "A vast amount of information in business and especially in the finance area is only available in the form of unstructured text documents.", "labels": [], "entities": []}, {"text": "Automatic text analysis algorithms are increasingly being used to effectively and efficiently gain insights from this type of data.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6997688263654709}]}, {"text": "A particularly important text analytics task is document classification, i.e. the task to assign a document to a category within a set of pre-defined categories.", "labels": [], "entities": [{"text": "document classification", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.7594727575778961}]}, {"text": "For example, annual reports, news articles and social media services like twitter provide textual information that can be used in conjunction with structured data to quantify the creditworthiness of a debtor.", "labels": [], "entities": []}, {"text": "To give another example, intelligent process automation may require the categorization of documents to determine the process flow.", "labels": [], "entities": []}, {"text": "In both cases, sound text classification algorithms help saving costs and efforts.", "labels": [], "entities": [{"text": "sound text classification", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6717280944188436}]}, {"text": "To tackle the problem of document classification, classical methods combine hand-engineered features, e.g. word-count based features, n-grams, part-of-speech tags or negations features, with a non-linear classification algorithm such as Support Vector Machine.", "labels": [], "entities": [{"text": "document classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7414609789848328}]}, {"text": "A detailed survey of classical sentiment analysis models, a special case of text classification, has been compiled by and.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8255668580532074}, {"text": "text classification", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7480036318302155}]}, {"text": "Since the reign of deep learning, various neural network architectures such as convolutional neural networks (CNN), character level CNNs ( , recursive neural networks (, recurrent neural network (RNN) () and transformers () have been utilized in text classification models to yield state-of-the-art results.", "labels": [], "entities": [{"text": "text classification", "start_pos": 246, "end_pos": 265, "type": "TASK", "confidence": 0.8368901610374451}]}, {"text": "Recently, a steep performance increase has been achieved by very large pre-trained neural language models such as ELMo (), BERT), XLNet () and more).", "labels": [], "entities": [{"text": "BERT", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.995163083076477}]}, {"text": "These models generate powerful text representations that can be either used as context-aware word embeddings or the models can be directly fine tuned to specific tasks.", "labels": [], "entities": []}, {"text": "One disadvantage of these pre-trained language models, however, is the high demand of memory and computing power, e.g. a sufficiently large GPU to load the large models.", "labels": [], "entities": []}, {"text": "In finance, many documents that can be the subject of text classification applications (e.g. annual reports or legislative documents), are very large, so that the computational cost becomes very relevant.", "labels": [], "entities": [{"text": "text classification", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7338986098766327}]}, {"text": "Another disadvantage is that because of their complexity, many state-of-the-art deep learning models are hard to interpret and it is very difficult to retrace the model predictions.", "labels": [], "entities": []}, {"text": "Model interpretability, however, seems to be particularly important for many financial institutions and interpretable models with transparent features are often favored over more complex models even if the complex models are more accurate.", "labels": [], "entities": [{"text": "Model interpretability", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7443934381008148}]}, {"text": "A powerful resource for building interpretable text classification models are classification lexicons and in particular sentiment lexicons.", "labels": [], "entities": [{"text": "interpretable text classification", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6640373468399048}]}, {"text": "A sentiment lexicon is a list of words (or n-grams) where each word is assigned a sentiment orientation.", "labels": [], "entities": []}, {"text": "The sentiment orientation can be binary, i.e. each word in the lexicon is labeled as positive or negative, or continuous where a continuous sentiment score is assigned to the words (e.g. in the interval).", "labels": [], "entities": [{"text": "sentiment orientation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8570152223110199}]}, {"text": "More generally, a classification lexicon is a list of words where each word is assigned a vector with one score for each class.", "labels": [], "entities": []}, {"text": "Sentiment lexicons have been an integral part of many classical sentiment analysis classifiers.", "labels": [], "entities": [{"text": "sentiment analysis classifiers", "start_pos": 64, "end_pos": 94, "type": "TASK", "confidence": 0.8713012139002482}]}, {"text": "Approaches based on sentiment lexicons seem to be particularly popular in the finance domain).", "labels": [], "entities": []}, {"text": "In addition, it has been shown that even modern neural network models can profit from incorporating sentiment lexicon features (.", "labels": [], "entities": []}, {"text": "Using classification lexicon features can bethought of as away of inducing external information that has been learned from different data sets or compiled by experts.", "labels": [], "entities": []}, {"text": "Three approaches to sentiment lexicon generation are usually distinguished in the literature, namely the manual approach, the dictionary-based approach and the corpus-based approach, see for example (Liu, 2012, Chapter 6).", "labels": [], "entities": [{"text": "sentiment lexicon generation", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8963543375333151}]}, {"text": "A popular finance specific lexicon has been compiled by from 10-K fillings , but see also the General Inquirer () and the Subjectivity Lexicon ().", "labels": [], "entities": [{"text": "General Inquirer", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.8551770448684692}]}, {"text": "Fairly recently, models have been designed to generate sentiment lexicons from a labeled text corpus.", "labels": [], "entities": []}, {"text": "In many cases distant supervision approaches are employed to generate large amounts of labeled data.", "labels": [], "entities": []}, {"text": "For example, compiled a large twitter corpus where noisy labels are inferred from emoticons and hashtags.", "labels": [], "entities": []}, {"text": "Countbased methods such as pointwise mutual information (PMI) generate sentiment scores for words based on their frequency in positive and negative training sentences.", "labels": [], "entities": []}, {"text": "A more direct approach to learn sentiment lexicons from labeled corpora is to use supervised machine learning.", "labels": [], "entities": [{"text": "learn sentiment lexicons from labeled corpora", "start_pos": 26, "end_pos": 71, "type": "TASK", "confidence": 0.8159213562806448}]}, {"text": "The basic idea is to design a text classification model that contains a parametrized mapping from word token to sentiment score and an aggregation of word-level sentiment scores to document scores.", "labels": [], "entities": [{"text": "text classification", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7050087451934814}]}, {"text": "The parametrized mapping which yields the sentiment lexicon is learned during training.", "labels": [], "entities": []}, {"text": "proposed a linear SVM model and showed that the machine learning approach outperforms countbased approaches.", "labels": [], "entities": []}, {"text": "A simple linear neural network model has been proposed by . A similar model with a slightly more complex neural network architecture is used by.", "labels": [], "entities": []}, {"text": "They use data from StockTwits, asocial media platform designed for sharing ideas about stocks, which they also use to generate sentimentspecific word embeddings.", "labels": [], "entities": []}, {"text": "1 design a linear model and add L1 regularization to optimally control the size of the sentiment lexicons.", "labels": [], "entities": []}, {"text": "We see two main challenges for the generation of new domain specific classification lexicons via a pure supervised learning approach.", "labels": [], "entities": [{"text": "domain specific classification lexicons", "start_pos": 53, "end_pos": 92, "type": "TASK", "confidence": 0.6858828812837601}]}, {"text": "\u2022 The generation of robust classification lexicons requires large amounts of supervised training data.", "labels": [], "entities": []}, {"text": "Manual labeling of data is very expensive and a distant (or weak) labeling approach may not be possible for all applications.", "labels": [], "entities": []}, {"text": "\u2022 Using small or medium size supervised training data, one may encounter many words at prediction time that are not part of the training corpus.", "labels": [], "entities": []}, {"text": "To tackle these problems, we propose a novel supervised method to generate classification lexicons by utilizing unsupervised data in the form of pre-trained word embeddings.", "labels": [], "entities": []}, {"text": "This approach allows to build classification lexicons with very small amounts of supervised data.", "labels": [], "entities": []}, {"text": "In particular, it allows extending the classification lexicon to words outside the training corpus, namely to all words in the vocabulary of the pre-trained word embedding.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a short introduction to supervised learning of classification lexicons in general and then introduces the novel model extension to utilize pre-trained word embeddings.", "labels": [], "entities": [{"text": "supervised learning of classification lexicons", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.6388292193412781}]}, {"text": "We show empirically in Section 3 that the use of pre-trained word embeddings improves prediction accuracy and generates better classification lexicons.", "labels": [], "entities": [{"text": "prediction", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.9437580108642578}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8863003849983215}]}, {"text": "The accuracy improvement is particularly large for small training data sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994621872901917}]}, {"text": "In addition, we show that the model generates sensible word-level class scores for words that are not part of the training data.", "labels": [], "entities": []}, {"text": "For the experiments we use the popular SST-2 sentiment analysis dataset which is part of the GLUE benchmark and anew dataset of manually labeled financial newspaper headlines.", "labels": [], "entities": [{"text": "SST-2 sentiment analysis", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9109473625818888}, {"text": "GLUE benchmark", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.790961742401123}]}, {"text": "In Section 4 we describe how a modification of the proposed method can be applied to hierarchical (multi-level) document classification and supervised sentence highlighting in large documents.", "labels": [], "entities": [{"text": "document classification", "start_pos": 112, "end_pos": 135, "type": "TASK", "confidence": 0.7253049165010452}, {"text": "sentence highlighting in large documents", "start_pos": 151, "end_pos": 191, "type": "TASK", "confidence": 0.821188771724701}]}], "datasetContent": [{"text": "The purpose of the proposed classification model is to generate powerful application specific classification lexicons and we want to show that the new model generates better lexicons than the baseline model.", "labels": [], "entities": []}, {"text": "To this end, we train both models on two binary sentiment analysis datasets and compare the test set accuracy as a proxy for the classification lexicon quality.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9667820334434509}]}, {"text": "Since the new word-embedding based model and the baseline model contain the same aggregation function, any improvement in model predictions must result from the word-level classification scores, i.e. the learned classification lexicons.", "labels": [], "entities": []}, {"text": "The first dataset that we use for the evaluation is the SST-2 dataset) that contains binary labeled movie reviews.", "labels": [], "entities": [{"text": "SST-2 dataset", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.7524652183055878}]}, {"text": "This wellknown dataset is publicly available and part of the GLUE benchmark ().", "labels": [], "entities": [{"text": "GLUE benchmark", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.8461980223655701}]}, {"text": "The second dataset, which we call FNHL, consists of financial news headlines that have been manually labeled by experts.", "labels": [], "entities": [{"text": "FNHL", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.943909764289856}]}, {"text": "shows simple examples from both datasets and gives basic dataset statistics.", "labels": [], "entities": []}, {"text": "It should be emphasized that the proposed model is not restricted to binary classification problems and could also be applied to multi-class datasets.", "labels": [], "entities": []}, {"text": "Both the baseline and the new model are implemented as neural networks and optimized via the Adam optimizer.", "labels": [], "entities": []}, {"text": "For the baseline model dropout regularization is applied to the word level class scores and for the new model dropout is applied before the rectifiers.", "labels": [], "entities": []}, {"text": "The new model is implemented with pre-trained word2vec word embeddings.", "labels": [], "entities": []}, {"text": "For words that are not contained in word2vec the embedding is set to a vector of zeros.", "labels": [], "entities": []}, {"text": "Since the embedding model can always be refined based on an unlabeled domain-specific corpus, one can ensure that the embedding model contains the relevant vocabulary.", "labels": [], "entities": []}, {"text": "The SST-2 dataset is provided with a train/dev/test split which is used in our experiments whereas for the FNHL dataset nested cross-validation is used.", "labels": [], "entities": [{"text": "SST-2 dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7844381034374237}, {"text": "FNHL dataset", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9699585437774658}]}, {"text": "The dev set is used for early stopping and to evaluate model hyperparameters via grid-search.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8038027584552765}]}, {"text": "The optimal hyperparameters are provided in in the appendix.", "labels": [], "entities": []}, {"text": "shows that the new model outperforms the baseline model on both datasets which means that the new model generates better sentiment lexicons.", "labels": [], "entities": []}, {"text": "As an additional experiment we implement the new model with ELMo embeddings which further increases the accuracy on the SST-2 dataset by 3.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9995865225791931}, {"text": "SST-2 dataset", "start_pos": 120, "end_pos": 133, "type": "DATASET", "confidence": 0.8318144977092743}]}, {"text": "Since ELMo embeddings are contextdependent this model does not yield a fixed sentiment lexicon but instead yields a mapping from sentence-token pair to sentiment scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples from the SST-2 and FNHL datasets.", "labels": [], "entities": [{"text": "SST-2", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.7706981897354126}, {"text": "FNHL datasets", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9628360271453857}]}, {"text": " Table 2: Average sentence length (mean(|t|)), total  dataset size (N ), vocabulary size (|V |) and vocabulary  that is contained in word2vec (|V w2v |). Computed on  the pre-processed datasets.", "labels": [], "entities": []}]}