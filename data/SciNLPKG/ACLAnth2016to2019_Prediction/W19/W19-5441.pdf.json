{"title": [], "abstractContent": [{"text": "This paper describes the University of Helsinki Language Technology group's participation in the WMT 2019 parallel corpus filtering task.", "labels": [], "entities": [{"text": "WMT 2019 parallel corpus filtering task", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.8400870760281881}]}, {"text": "Our scores were produced using a two-step strategy.", "labels": [], "entities": []}, {"text": "First, we individually applied a series of filters to remove the 'bad' quality sentences.", "labels": [], "entities": []}, {"text": "Then, we produced scores for each sentence by weighting these features with a classification model.", "labels": [], "entities": []}, {"text": "This methodology allowed us to build a simple and reliable system that is easily adaptable to other language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven methodologies define the state of the art in a wide variety of language processing tasks.", "labels": [], "entities": []}, {"text": "The availability of well-formed, clean data varies from language to language, and finding such data in sufficient amounts can prove challenging for some of the lower-resourced languages.", "labels": [], "entities": []}, {"text": "In particular, the increasingly common neural machine translation systems are highly sensitive to the quality as well as the quantity of training data , which creates an impediment to achieving good-quality translations in a low-resource scenario.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.778169055779775}]}, {"text": "The web is a massive resource for text data in a wide array of languages.", "labels": [], "entities": []}, {"text": "However, it is costly to manually extract high-quality parallel samples from the web, and automatically-crawled datasets such as the ParaCrawl Corpus 1 are typically quite noisy.", "labels": [], "entities": [{"text": "ParaCrawl Corpus 1", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.917607049147288}]}, {"text": "Designing automatic methods to select high-quality aligned samples from noisy parallel corpora can therefore make crawling the web a more viable option for compiling useful training data.", "labels": [], "entities": []}, {"text": "To emphasize this untapped potential,  proposed the Shared Task on Parallel Corpus Filtering as part of WMT in 2018.", "labels": [], "entities": [{"text": "WMT", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.9231410026550293}]}, {"text": "We ParaCrawl can be downloaded from https: //paracrawl.eu/ participated in this year's task with three sets of quality scores.", "labels": [], "entities": []}, {"text": "Each score is a different aggregation of a shared set of features, with each feature representing a local quality estimate focusing on a different aspect.", "labels": [], "entities": []}, {"text": "Section 2 contains a brief discussion of this year's shared task.", "labels": [], "entities": []}, {"text": "We present our scoring system in Section 3, discussing the filters we used for feature extraction in Section 3.2, and the aggregate scorers in Section 3.3.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.6725519001483917}]}, {"text": "Finally, we report our contrastive results in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Selected threshold value for each feature.", "labels": [], "entities": []}, {"text": " Table 6: BLEU scores using SMT on 5 million sampled  training examples. The baseline refers to the Zipporah  model reported by the organizers of the shared task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9976598024368286}, {"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9829897880554199}]}, {"text": " Table 7: An overview of the relative performance (in  BLEU scores) of our (1) primary and (2) contrastive  SMT models trained on 1, 5, and 10 million samples.  The best and average rows represent the highest score  and the mean \u00b1 standard deviation among this year's  submissions, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9985395669937134}, {"text": "SMT", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9082399010658264}]}]}