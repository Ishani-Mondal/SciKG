{"title": [{"text": "A Repository of Conversational Datasets github.com/PolyAI-LDN/conversational-datasets", "labels": [], "entities": [{"text": "PolyAI-LDN", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.9215252995491028}]}], "abstractContent": [{"text": "Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing mod-eling approaches.", "labels": [], "entities": []}, {"text": "To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy.", "labels": [], "entities": [{"text": "conversational response selection", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.6830479701360067}, {"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.8607136011123657}]}, {"text": "The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their needs.", "labels": [], "entities": []}, {"text": "We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set.", "labels": [], "entities": [{"text": "conversational response selection", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.8230158487955729}]}], "introductionContent": [{"text": "Dialogue systems, sometimes referred to as conversational systems or conversational agents, are useful in a wide array of applications.", "labels": [], "entities": []}, {"text": "They are used to assist users in accomplishing well-defined tasks such as finding and/or booking flights and restaurants (, or to provide tourist information (.", "labels": [], "entities": []}, {"text": "They have found applications in entertainment, language learning (, and healthcare (.", "labels": [], "entities": []}, {"text": "Conversational systems can also be used to aid in customer service 1 or to provide the foundation for intelligent virtual assistants such as Amazon Alexa, Google Assistant, or Apple Siri.", "labels": [], "entities": [{"text": "customer service 1", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7038555145263672}]}, {"text": "Modern approaches to constructing dialogue systems are almost exclusively data-driven, supported corpus (, conversations from the three conversational datasets available in the repository are more natural and diverse.", "labels": [], "entities": []}, {"text": "What is more, the datasets are large: for instance, after preprocessing around 3.7B comments from Reddit available in 256M conversational threads, we obtain 727M valid contextresponse pairs.", "labels": [], "entities": []}, {"text": "Similarly, the number of valid pairs in the OpenSubtitles dataset is 316 million.", "labels": [], "entities": [{"text": "OpenSubtitles dataset", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.9725805819034576}]}, {"text": "To put these numbers into perspective, the frequently used Ubuntu corpus v2.0 comprises around 4M dialogue turns.", "labels": [], "entities": [{"text": "Ubuntu corpus v2.0", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.8939223488171896}]}, {"text": "Furthermore, our Reddit corpus includes 2 more years of data and so is substantially larger than the previous Reddit dataset of Al-, which spans around 2.1B comments and 133M conversational threads, and is not publicly available.", "labels": [], "entities": [{"text": "Reddit corpus", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.865740567445755}, {"text": "Reddit dataset of Al-", "start_pos": 110, "end_pos": 131, "type": "DATASET", "confidence": 0.9143862843513488}]}, {"text": "Besides the repository of large datasets, another key contribution of this work is the common evaluation framework.", "labels": [], "entities": []}, {"text": "We propose applying consistent data filtering and preprocessing to public datasets, and a simple evaluation metric for response selection, which will facilitate direct comparisons between models from different research groups.", "labels": [], "entities": [{"text": "response selection", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7611383497714996}]}, {"text": "These large conversational datasets may support modeling across a large spectrum of natural conversational domains.", "labels": [], "entities": []}, {"text": "Similar to the recent work on language model pretraining for diverse NLP applications, we believe that these datasets can be used in future work to pretrain large general-domain conversational models that are then fine-tuned towards specific tasks using much smaller amounts of task-specific conversational data.", "labels": [], "entities": [{"text": "language model pretraining", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6303064028422037}]}, {"text": "We hope that the presented repository, containing a set of strong baseline models and standardised modes of evaluation, will provide means and guidance to the development of nextgeneration conversational systems.", "labels": [], "entities": []}, {"text": "The repository is available at github.com/ PolyAI-LDN/conversational-datasets.", "labels": [], "entities": [{"text": "PolyAI-LDN", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.9601185917854309}]}], "datasetContent": [{"text": "Datasets are stored as Tensorflow record files containing serialized Tensorflow example protocol buffers (.", "labels": [], "entities": []}, {"text": "The training set is stored as one collection of Tensorflow record files, and the test set as another.", "labels": [], "entities": [{"text": "Tensorflow record files", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.750996599594752}]}, {"text": "Examples are shuffled randomly (and not necessarily reproducibly) within the Tensorflow record files.", "labels": [], "entities": [{"text": "Tensorflow record files", "start_pos": 77, "end_pos": 100, "type": "DATASET", "confidence": 0.8125558495521545}]}, {"text": "Each example is deterministically assigned to either the train or test set using a key feature, such as the conversation thread ID in Reddit, guaranteeing that the same split is created whenever the dataset is generated.", "labels": [], "entities": []}, {"text": "By default the train set consists of 90% of the total data, and the test set the remaining 10%.", "labels": [], "entities": []}, {"text": "context/1 Hello, how are you?", "labels": [], "entities": []}, {"text": "context/0 I am fine.", "labels": [], "entities": []}, {"text": "What do you think of the weather?", "labels": [], "entities": []}, {"text": "response It doesn't feel like February.", "labels": [], "entities": []}, {"text": "Each Tensorflow example contains a conversational context and a response that goes with that context, see e.g..", "labels": [], "entities": []}, {"text": "Explicitly, each example contains a number of string features: \u2022 A context feature, the most recent text in the conversational context.", "labels": [], "entities": []}, {"text": "\u2022 A response feature, text that is indirect response to the context.", "labels": [], "entities": []}, {"text": "\u2022 A number of extra context features, context/0, context/1 etc.", "labels": [], "entities": []}, {"text": "going back in time through the conversation.", "labels": [], "entities": []}, {"text": "They are named in reverse order so that context/i always refers to the i th most recent extra context, so that no padding needs to be done, and datasets with different numbers of extra contexts can be mixed.", "labels": [], "entities": []}, {"text": "\u2022 Depending on the dataset, there maybe some extra features also included in each example.", "labels": [], "entities": []}, {"text": "For instance, in Reddit the author of the context and response are identified using additional features.", "labels": [], "entities": []}, {"text": "Rather than providing the raw processed data, we provide scripts and instructions to the users to generate the data themselves.", "labels": [], "entities": []}, {"text": "This allows for viewing and potentially manipulating the pre-processing and filtering steps.", "labels": [], "entities": []}, {"text": "2015), which parallelizes the work across many machines.", "labels": [], "entities": []}, {"text": "Using the default quotas, the Reddit script starts 409 workers to generate the dataset in around 1 hour and 40 minutes.", "labels": [], "entities": []}, {"text": "This includes reading the comment data from the BigQuery source, grouping the comments into threads, producing examples from the threads, splitting the examples into train and test, shuffling the examples, and finally writing them to sharded Tensorflow record files.", "labels": [], "entities": [{"text": "BigQuery source", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8878062069416046}]}, {"text": "provides an overview of the Reddit, OpenSubtitles and AmazonQA datasets, and figure 3 in appendix A gives an illustrative example from each.", "labels": [], "entities": [{"text": "AmazonQA datasets", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.9670757353305817}]}, {"text": "All the methods discussed in section 4 are evaluated on the three standard datasets from section 3,", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the datasets included in the public repository. The Reddit data is taken from January 2015 to  December 2018, and the OpenSubtitles data from 2018.", "labels": [], "entities": [{"text": "Reddit data", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8458153009414673}, {"text": "OpenSubtitles data", "start_pos": 139, "end_pos": 157, "type": "DATASET", "confidence": 0.9425900280475616}]}, {"text": " Table 2: 1-of-100 accuracy results for keyword-based baselines, vector-based baselines, and the encoder model  for each of the three standard datasets. The latest evaluation results are maintained in the repository. Results are  computed on a random subset of 50,000 examples from the test set (500 batches of 100).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9892855286598206}]}]}