{"title": [{"text": "Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages", "labels": [], "entities": [{"text": "Unsupervised Morphological Segmentation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6377564966678619}]}], "abstractContent": [{"text": "Polysynthetic languages pose a challenge for morphological analysis due to the root-morpheme complexity and to the word class \"squish\".", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7591474950313568}]}, {"text": "In addition, many of these polysyn-thetic languages are low-resource.", "labels": [], "entities": []}, {"text": "We propose unsupervised approaches for morphological segmentation of low-resource polysyn-thetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016).", "labels": [], "entities": [{"text": "morphological segmentation of low-resource polysyn-thetic languages", "start_pos": 39, "end_pos": 106, "type": "TASK", "confidence": 0.8184531430403391}]}, {"text": "We experiment with four languages from the Uto-Aztecan family.", "labels": [], "entities": []}, {"text": "Our AG-based approaches outper-form other unsupervised approaches and show promise when compared to supervised methods , outperforming them on two of the four languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational morphology of polysynthetic languages is an emerging field of research.", "labels": [], "entities": [{"text": "Computational morphology of polysynthetic languages", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8981061577796936}]}, {"text": "Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7552089393138885}]}, {"text": "Previous approaches include rule-based methods based on finite state transducers), hybrid models (, and supervised machine learning, particularly deep learning approaches.", "labels": [], "entities": []}, {"text": "While each rule-based method is developed fora specific language (Inuktitut, or Arapaho (), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages.", "labels": [], "entities": []}, {"text": "We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor).", "labels": [], "entities": [{"text": "morphological segmentation of polysynthetic languages", "start_pos": 40, "end_pos": 93, "type": "TASK", "confidence": 0.7979835212230683}]}, {"text": "We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (.", "labels": [], "entities": []}, {"text": "Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 167, "end_pos": 193, "type": "TASK", "confidence": 0.7918885946273804}]}, {"text": "Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal.", "labels": [], "entities": []}, {"text": "We use the datasets introduced by in an unsupervised fashion (unsegmented words).", "labels": [], "entities": []}, {"text": "We design several AG learning setups: 1) use the best-on-average AG setup from; 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from; 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3).", "labels": [], "entities": [{"text": "NH", "start_pos": 340, "end_pos": 342, "type": "DATASET", "confidence": 0.9655252695083618}, {"text": "WX", "start_pos": 347, "end_pos": 349, "type": "DATASET", "confidence": 0.5747262835502625}]}, {"text": "We show that the AG-based approaches outperform other unsupervised methods -M orf essor and M orphoChain) -, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed as what is considered by native speakers to be just one word.", "labels": [], "entities": []}, {"text": "To illustrate, consider the following example from Inuktitut (, where the morpheme -tusaa-is the root and all the other morphemes are synthetically combined with it in one unit: tusaa-tsia-runna-nngit-tu-alu-u-jung hear-well-be.able-NEG-DOE-very-BE-PT.1S I can't hear very well.", "labels": [], "entities": []}, {"text": "Another example from WX, one of the languages in the dataset for this paper (from) shows this complexity: yu-huta-me ne-p+-we-iwa an-two-ns 1sg:s-asi-2pl:o-brother I have two brothers.", "labels": [], "entities": [{"text": "WX", "start_pos": 21, "end_pos": 23, "type": "DATASET", "confidence": 0.9695538282394409}]}, {"text": "In linguistic typology, the broader gradient is: isolating/analytic to synthetic to polysynthetic.", "labels": [], "entities": []}, {"text": "Agglutinating refers to the clarity of boundaries between morphemes.", "labels": [], "entities": [{"text": "Agglutinating", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9679135680198669}, {"text": "clarity", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9619370102882385}]}, {"text": "This more specific gradation is: agglutinating to mildly fusional to fusional.", "labels": [], "entities": []}, {"text": "Thus a language might be characterized overall as polysynthetic and agglutinating, i.e. generally a high number of morphemes per word, with clear boundaries between morphemes and thus easily segmentable.", "labels": [], "entities": []}, {"text": "Another language might be characterized as polysynthetic and fusional, so again, many morphemes per word, but many phonological and other processes so it is difficult to segment morphemes.", "labels": [], "entities": []}, {"text": "Thus, morphological analysis of polysynthetic languages is challenging due to the rootmorpheme complexity and to word class gradations.", "labels": [], "entities": [{"text": "morphological analysis of polysynthetic languages", "start_pos": 6, "end_pos": 55, "type": "TASK", "confidence": 0.8172983050346374}]}, {"text": "Linguists recognize a gradience in word classes, known as \"squishiness\", a term first discussed in who argued that, instead of a fixed, distinct inventory of syntactic categories, a quasi-continuum from verb, adjective and noun best reflects most lexical distinctions.", "labels": [], "entities": []}, {"text": "The rootmorpheme complexity and the word class \"squish\" makes developing segmented training data with reliability across annotators difficult to achieve.", "labels": [], "entities": []}, {"text": "have made a first step by releasing a small set of morphologically segmented datasets although even in these carefully curated datasets, the distinction between affix and clitic is not always indicated.", "labels": [], "entities": []}, {"text": "We use these datasets in an unsupervised fashion (i.e., we use the unsegmented words).", "labels": [], "entities": []}, {"text": "These datasets were taken from detailed descriptions in the Archive of Indigenous Languages collection for MX), NH (, WX (, and YN.", "labels": [], "entities": [{"text": "Archive of Indigenous Languages collection", "start_pos": 60, "end_pos": 102, "type": "DATASET", "confidence": 0.7543344378471375}, {"text": "MX", "start_pos": 107, "end_pos": 109, "type": "DATASET", "confidence": 0.7707469463348389}, {"text": "NH", "start_pos": 112, "end_pos": 114, "type": "DATASET", "confidence": 0.844571053981781}]}, {"text": "They were constructed so they include both segmentable as well as non- contains the count of words in the training, development and test.", "labels": [], "entities": []}, {"text": "Unlike, for training we do not use the segmented version of the data (our approach is unsupervised).", "labels": [], "entities": []}, {"text": "In addition to the datasets, for NH and WX we also have available the Bible (), which we consider for one of our experimental setups as additional training data.", "labels": [], "entities": [{"text": "NH", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.9654515981674194}, {"text": "WX", "start_pos": 40, "end_pos": 42, "type": "DATASET", "confidence": 0.5202219486236572}, {"text": "Bible", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9453454613685608}]}, {"text": "In the dataset from (, the maximum number of morphemes per word for MX is seven with an average of 2.13; for NH, six with an average of 2.2; for WX, maximum often with an average of 3.3; and for YN, the maximum is ten, with an average of 2.13.", "labels": [], "entities": []}, {"text": "We evaluate the different AG setups on the blind test set from and compare our AG approaches to state-of-the-art unsupervised systems as well as supervised models including the best supervised deep learning models from.", "labels": [], "entities": []}, {"text": "As the metric, we use the segmentation-boundary F1-score, which is standard for this task.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9320897459983826}]}, {"text": "shows the performance of our AG setups on the four languages.", "labels": [], "entities": []}, {"text": "The best AG setup learned for each of the four polysynthetic languages (AG BestL ) is the PrStSu+SM grammar using the Cascaded learning setup.", "labels": [], "entities": []}, {"text": "This is an interesting finding as the Cascaded PrSTSu+SM setup is in fact AG LIM S -the best-on-average AG setup obtained by    WX and YN, respectively.", "labels": [], "entities": [{"text": "AG LIM S", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.7408997615178426}]}, {"text": "Seeding affixes into the grammar trees (AG Scholar BestL ) improves the performance of the Cascaded P rStSu + SM setup only for MX and WX (additional absolute F1-scores of 0.023 and 0.019, respectively).", "labels": [], "entities": [{"text": "AG Scholar BestL", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8771040836970011}, {"text": "F1-scores", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9162374138832092}]}, {"text": "However, it does not help for NH, while it even decreases the performance on YN.", "labels": [], "entities": [{"text": "NH", "start_pos": 30, "end_pos": 32, "type": "DATASET", "confidence": 0.6431496143341064}, {"text": "YN", "start_pos": 77, "end_pos": 79, "type": "DATASET", "confidence": 0.7732691168785095}]}, {"text": "This occurs because AGs are able to recognize the main affixes in the Cascaded setup, while the seeded affixes were either abundant or conflicting with the automatically discovered ones.", "labels": [], "entities": []}, {"text": "The multilingual setup (AG M ulti ) does not improve the performance on any of the languages.", "labels": [], "entities": []}, {"text": "This could be because the datasets are too small to generalize common patterns across languages.", "labels": [], "entities": []}, {"text": "Overall, AG BestL is the best setup for YN, AG Scholar BestL is the best setup for MX and WX, while AG Aug is the best for NH.", "labels": [], "entities": [{"text": "NH", "start_pos": 123, "end_pos": 125, "type": "DATASET", "confidence": 0.9667568206787109}]}, {"text": "We consider M orf essor, a commonly-used toolkit for unsupervised morphological segmentation, and M orphoChain (, another unsupervised morphological system based on constructing morphological chains.", "labels": [], "entities": []}, {"text": "Our AG approaches significantly outperform both M orf essor and M orphoChain on all four languages, as shown in.", "labels": [], "entities": []}, {"text": "To obtain an upper bound, we compare the best AG setup to the best supervised neural methods presented in for each language.", "labels": [], "entities": []}, {"text": "We consider their best multi-task approach (BestMTT) and the best data-augmentation approach (BestDA), using F1 scores from their Table 4 for each language.", "labels": [], "entities": [{"text": "BestMTT", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8231844305992126}, {"text": "F1", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9969412684440613}]}, {"text": "In addition, we report the results on their other supervised baselines: a supervised seq-to-seq model (S2S) and a supervised CRF approach.", "labels": [], "entities": []}, {"text": "As can be seen in, our unsupervised AG-based approaches outperform the best supervised approaches for NH and YN with absolute F1-scores of 0.010 and 0.012, respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9867134094238281}]}, {"text": "An interesting observation is that for YN we only used the words in the training set of, without any data augmentation.", "labels": [], "entities": []}, {"text": "For MX and WX, the neural models from (BestMTT and BestDA), outperform our unsupervised AG-based approaches.", "labels": [], "entities": [{"text": "BestMTT", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9530627727508545}, {"text": "BestDA", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.875613808631897}]}, {"text": "For the purpose of error analysis, we train our unsupervised segmentation on the training sets and perform the analysis of results on the output of the development sets based on our best unsupervised models AG BestL . Since there is no distinction between stems and affixes in the labeled data, we only consider the morphemes that appear at least three times in order to eliminate open-class morphemes in our statistics.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.7027948945760727}, {"text": "AG BestL", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.8752225339412689}]}, {"text": "We first define the degree of ambiguity of a morpheme to be the percentage of times its sequence of characters does not form a segmentable morpheme when they appear in the training set.", "labels": [], "entities": []}, {"text": "We also define the degree of ambiguity of a language as the average degree of ambiguity of the morphemes in that language.: Examples of correct and incorrect segmentation ambiguity in each language.", "labels": [], "entities": []}, {"text": "Looking at the two languages where our models perform worse than the supervised models, we notice that MX has the least number of morphemes, and our unsupervised methods tend to oversegment; WX has the highest degree of ambiguity with a large number of one-letter morphemes, which makes the task more challenging for unsupervised segmentation as opposed to the case of a supervised setup.", "labels": [], "entities": []}, {"text": "Analyzing all the errors that our AG-based models made across all languages, we noticed one, or a combination, of the following factors: a high degree of morpheme ambiguity, short morpheme length and/or low frequency of a morpheme.", "labels": [], "entities": []}, {"text": "shows some examples of correctly and incorrectly segmented words by our models (blue indicates correct morphemes while red are wrong ones).", "labels": [], "entities": []}, {"text": "For MX, our models fail to recognize ka as a correct affix 100% of the time due to its high degree of ambiguity (71.79%), while we often wrongly detect ro as an affix, most likely since ro tends to appear at the end of a word; our approaches tend to oversegment in such cases.", "labels": [], "entities": [{"text": "MX", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.900066614151001}]}, {"text": "On the other hand, our method correctly identify ki as a correct affix 100% of the time since it appears frequently in the training data.", "labels": [], "entities": []}, {"text": "For NH, the morpheme tla has a high degree of ambiguity at 79.12%, which lead the model to fail in recognizing it as an affix (see an example in).", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9629881381988525}]}, {"text": "On the other hand, NH has a higher percentage of correctly recognized morphemes, due to their less ambiguous nature and higher frequency (such as ke, tl or mo).", "labels": [], "entities": [{"text": "frequency", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9616262316703796}]}, {"text": "For WX, a large portion of errors stem from one-letter morphemes that are highly ambiguous (e.g., u, a, e, m, n, p and r), in addition to having morphemes in the training set which are not frequent enough to learn from, such as ki,nua and wawi (see).", "labels": [], "entities": [{"text": "WX", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.8590112328529358}]}, {"text": "Examples of correct segmentation involve morphemes that are more frequent and less ambiguous (pe, p@ and ne).", "labels": [], "entities": []}, {"text": "For YN, ambiguity is the main source of segmentation errors (e.g., wa, wi and \u00dfa).slight", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of words in train, dev, test splits from  Kann et al. (2018) + additional Bible data", "labels": [], "entities": []}, {"text": " Table 3: AG systems compared to unsupervised baselines. Bold indicates best scores", "labels": [], "entities": [{"text": "AG", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8902943730354309}]}, {"text": " Table 4: Best AG results compared to supervised approaches from Kann et al. (2018). Bold indicates best scores.", "labels": [], "entities": [{"text": "AG", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.81113201379776}]}]}