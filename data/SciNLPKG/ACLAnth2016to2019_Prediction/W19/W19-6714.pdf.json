{"title": [{"text": "A Comparative Study of English-Chinese Translations of Court Texts by Machine and Human Translators and the Word2Vec Based Similarity Measure's Ability To Gauge Human Evaluation Biases", "labels": [], "entities": [{"text": "English-Chinese Translations of Court Texts by Machine and Human Translators", "start_pos": 23, "end_pos": 99, "type": "TASK", "confidence": 0.7408676087856293}, {"text": "Gauge Human Evaluation Biases", "start_pos": 155, "end_pos": 184, "type": "TASK", "confidence": 0.6502143815159798}]}], "abstractContent": [{"text": "In this comparative study, a jury instruction scenario was used to test the translating capabilities of multiple machine translation tools and a human translator with extensive court experience.", "labels": [], "entities": []}, {"text": "Three certified translators/interpreters subjectively evaluated the target texts generated using adequacy and fluency as the evaluation metrics.", "labels": [], "entities": []}, {"text": "This subjective evaluation found that the machine generated results had much poorer adequacy and fluency compared with results produced by their human counterpart.", "labels": [], "entities": []}, {"text": "Human translators can use strategic omission and explicitation strategies such as addition, paraphrasing, substitution, and repetition to remove ambiguity, and achieve a natural flow in the target language.", "labels": [], "entities": []}, {"text": "We also investigate instances where human evaluators have major disagreements and found that human experts could have very biased views.", "labels": [], "entities": []}, {"text": "On the other hand, a word2vec based algorithm, if given a good reference translation, can serve as a robust and reliable similarity reference to quantify human evalutors' biases beacuse it was trained on a large corpus using neural network models.", "labels": [], "entities": []}, {"text": "Even though the machine generated versions had better fluency performance compared to their adequacy performance, the human translator's fluency performance was still far superior.", "labels": [], "entities": []}, {"text": "The lack of understanding by machine translators led to inaccurate and improper word/phrase selections, which led to bad fluency.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In this section, we select instances of adequacy evaluation in which three evaluators had very different opinions.", "labels": [], "entities": [{"text": "adequacy evaluation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7135653495788574}]}, {"text": "We asked the evaluators to provide the reasons for their choices.", "labels": [], "entities": []}, {"text": "In addition, we compare the machine generated version with the human translation version which has the best adequacy.", "labels": [], "entities": []}, {"text": "The comparison is done using word2vec2.", "labels": [], "entities": []}, {"text": "Word2vec is a two-layer neural net that processes text (Artificial Intelligence Wiki).", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9306893944740295}]}, {"text": "Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus.", "labels": [], "entities": []}, {"text": "The objective of Word2vec is to group the vectors of similar words together in a vector space.", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.9281243681907654}]}, {"text": "Therefore it can be used to detect the similarity of two sentences.", "labels": [], "entities": []}, {"text": "The Chinese word2vec model used can be found on the website below: https://pan.baidu.com/s/1TZ8GII0CEX32ydjsfMc0zw, and the 64-dimension model was trained using the news, Baidu Encyclopedia, and Chinese novels.", "labels": [], "entities": [{"text": "Baidu Encyclopedia", "start_pos": 171, "end_pos": 189, "type": "DATASET", "confidence": 0.9572602212429047}]}, {"text": "The python code used to calculate the word2vec similarity between two sentences is listed in table based sentence similarity.", "labels": [], "entities": []}, {"text": "shows an example in which the three evaluators exhibit disagreement.", "labels": [], "entities": []}, {"text": "Evaluator 1 believed that the Google version is literal and accurate, while evaluator 2 and 3 observed some mistranslation and grammar/syntax errors.", "labels": [], "entities": []}, {"text": "We found that some evaluators can have very biased view.", "labels": [], "entities": []}, {"text": "For example, evaluator 2 believed that \"\u624d \u80fd\u9605\u8bfb\u5b8c\" (finish reading in 28 minutes and 14 seconds) is very different from \"\u9700\u898128\u520614\u79d2\u624d \u80fd\u9605\u8bfb\" (needs 28 minutes and 14 seconds to read).", "labels": [], "entities": [{"text": "finish", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9658210873603821}]}, {"text": "Realistically, those two expressions are not that different from each other.", "labels": [], "entities": []}, {"text": "Since word2vec measures cannot be performed on empty spaces and punctuation, we measured the clause similarities between the google result and the result generated by the human translator (as the reference).", "labels": [], "entities": []}, {"text": "Two clauses had similarity scores of around 0.88 and 0.78, while one clause \"\u8fd9\u4f1a\u6709\u6240\u4e0d\u540c\" (that vary a little bit can) had a very low similarity score (0.1058) compared to the human version \"\u65f6\u95f4\u6216\u957f\u6216\u77ed\" (the reading time could be longer or shorter).", "labels": [], "entities": [{"text": "similarity", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9592003226280212}, {"text": "similarity score", "start_pos": 129, "end_pos": 145, "type": "METRIC", "confidence": 0.9724336564540863}]}, {"text": "This is inline with the comment by evaluator 3.", "labels": [], "entities": []}, {"text": "The major difference is that the human version mentioned \"\u65f6 \u95f4\" (time), while the Google version did not specify what varies.", "labels": [], "entities": [{"text": "time", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9071797132492065}]}, {"text": "If we add the time (\"\u65f6\u95f4\") to the Google generated clause and change it to \"\u65f6\u95f4\u4f1a\u6709\u6240\u4e0d\u540c\", then the word2vec based similarity changes from 0.1058 to 0.6617.", "labels": [], "entities": []}, {"text": "That shows that the word2vec provides a very reliable similarity measure, and a good indicator of human bias.", "labels": [], "entities": []}, {"text": "In this case, only the evaluator 3's opinion was supported by the word2vec results.", "labels": [], "entities": []}, {"text": "Original English: It should take about 28 minutes and 14 seconds for me to read these instructions to you.", "labels": [], "entities": []}, {"text": "That's going to vary a little bit but it'll take just about a half an hour for me to read the instructions to you.", "labels": [], "entities": []}, {"text": "Google translator result: \u6211\u9700\u8981\u5927\u7ea628\u520614\u79d2 \u624d\u80fd\u9605\u8bfb\u8fd9\u4e9b\u8bf4\u660e\uff08word2vec similarity calculated against human generated translation = 0.8806\uff09\u3002", "labels": [], "entities": []}, {"text": "\u8fd9\u4f1a\u6709\u6240\u4e0d\u540c\uff08word2vec similarity calculated against human 0generated translation = 0.1058\uff09 \uff0c\u4f46\u6211\u9700\u8981\u5927\u7ea6\u534a\u5c0f\u65f6\u7684\u65f6\u95f4 \uff08word2vec similarity calculated against human generated translation = 0.7865\uff09\u624d\u80fd\u9605\u8bfb\u8bf4\u660e \u4e66\u3002", "labels": [], "entities": []}, {"text": "Evaluator 1 -> Agree: The translation is accurate and literal, but without taking into consideration the English conversation style and properly converting that to the target language, an accurate translation doesn't necessarily convey a message accurately.", "labels": [], "entities": [{"text": "Evaluator 1 ->", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8529117107391357}, {"text": "Agree", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.6685673594474792}]}, {"text": "Evaluator 2 -> Disagree: Here \"to read\" is translated to \"\u624d\u80fd\u9605\u8bfb\", which is a literal translation, but in Chinese, a more accurate translation should be \"\u624d\u80fd\u9605\u8bfb\u5b8c\", meaning \"\"to finish reading\".", "labels": [], "entities": []}, {"text": "Evaluator 3 ->Strongly Disagree: 1.", "labels": [], "entities": []}, {"text": "Missing \"to you\".", "labels": [], "entities": []}, {"text": "2. Literal translation of \"vary a little bit\" which can cause confusion.", "labels": [], "entities": [{"text": "vary", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9795239567756653}]}, {"text": "shows another example in which the three evaluators disagreed.", "labels": [], "entities": []}, {"text": "Evaluators 1 and 2 believed that the translation is not adequate because it should be a polite request instead of a conditional statement.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.9556857347488403}]}, {"text": "On the other hand, Evaluator 3 believed that the adequacy is acceptable.", "labels": [], "entities": [{"text": "Evaluator", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.562801718711853}]}, {"text": "Using the word2vec measure, we measured the clause similarity between the Tencent result and the result generated by the human translator \"\u8bf7 \u5927\u5bb6\u7ffb\u5230\u6307\u793a\u6587\u4ef6\u7684\u7b2c\u4e00\u9875\" (as the reference), and the similarity score is 0.5166.", "labels": [], "entities": []}, {"text": "If we change the Tencent result based on what Evaluators 1 and 2 suggested, the new similarity score is 0.6507.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 84, "end_pos": 100, "type": "METRIC", "confidence": 0.9841126501560211}]}, {"text": "This shows that Evaluators 1 and 2 had a valid point, but while the improvement is significant, it is limited.", "labels": [], "entities": [{"text": "Evaluators", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.6859058737754822}]}, {"text": "Again, the word2vec based similarity measure Original English: So if you would turn to page 1 which is always the best page.", "labels": [], "entities": [{"text": "word2vec based similarity measure", "start_pos": 11, "end_pos": 44, "type": "METRIC", "confidence": 0.674639604985714}]}, {"text": "Page 1 of the instructions.", "labels": [], "entities": []}, {"text": "Tencent Mr Translator: \u56e0\u6b64\uff0c\u5982\u679c\u4f60\u60f3\u7ffb\u5230\u7b2c \u4e00\u9875(0.5166)\uff0c\u8fd9\u59cb\u7ec8\u662f\u6700\u597d\u7684\u4e00\u9875\u3002\u8bf4\u660e\u7b2c 1\u9875\uff1a\u5ba1\u5224\u540e\u4ecb\u7ecd\u6027\u7cfb\u5217(0.6646)\u3002", "labels": [], "entities": []}, {"text": "Evaluator 1 -> Strongly Disagree: The translation is too literal to keep the intended meaning intact.", "labels": [], "entities": []}, {"text": "\"If you would turn to page 1\" in the sentence isn't a conditional statement, although grammatically incorrect, it's a polite way to give a command, to tell the jurors to do something, and such command shall be reflected in the translation, instead of a conditional statement.", "labels": [], "entities": []}, {"text": "Evaluator 2 -> Disagree \"so if you would turn to page 1\" is translated to \"\u5982\u679c\u4f60\u60f3\u7ffb\u5230\u7b2c\u4e00 \u9875\", which means \"if you want to\" But the original meaning is basically a polite way ofrequesting jury member to \"please turn to page 1\".", "labels": [], "entities": [{"text": "Evaluator 2 -> Disagree", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.8927283585071564}]}, {"text": "A more accurate translation is \"\u8bf7\u7ffb\u5230\" Evaluator 3 -> Agree: The overall quality is OK.", "labels": [], "entities": [{"text": "Evaluator 3 ->", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9426732659339905}, {"text": "Agree", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.8063389658927917}]}, {"text": "However, there area few places that can be improved.", "labels": [], "entities": []}, {"text": "Literal translation is an issue.", "labels": [], "entities": [{"text": "Literal translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7978948056697845}]}, {"text": "These examples showed that a word2vec based algorithm, if given a good reference translation, can serve as a robust and reliable similarity reference to quantify human evaluators' biases because it was trained on a large corpus using neural network models.", "labels": [], "entities": []}, {"text": "The evaluator's opinion is confirmed \"\u8fd9\u4f1a\u6709\u6240\u4e0d \u540c\" (that vary a little bit can) is not accurate given the context 0.1058 to 0.6617 after adding the word \"time\" by the Word2Vec result (the similarity score changed by more than 0.5): Word2vec based similarity measures serve as a robust and reliable similarity reference to quantify human evaluators' biases.", "labels": [], "entities": [{"text": "Word2Vec result", "start_pos": 163, "end_pos": 178, "type": "DATASET", "confidence": 0.9423976838588715}]}], "tableCaptions": []}