{"title": [{"text": "Cross-Corpus Data Augmentation for Acoustic Addressee Detection", "labels": [], "entities": [{"text": "Cross-Corpus Data Augmentation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6062164505322775}, {"text": "Acoustic Addressee Detection", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6621927718321482}]}], "abstractContent": [{"text": "Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in voice assistants.", "labels": [], "entities": [{"text": "Acoustic addressee detection (AD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.803747703631719}]}, {"text": "In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting.", "labels": [], "entities": [{"text": "recall", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.7131914496421814}]}, {"text": "We jointly solve both classification problems, using three models: a linear support vector machine dealing with acoustic functionals and two neural networks utilising raw waveforms alongside with acoustic low-level descriptors.", "labels": [], "entities": []}, {"text": "We investigate how different corpora influence each other, applying the mixup approach to data augmentation.", "labels": [], "entities": []}, {"text": "We also study the influence of various acoustic context lengths on AD.", "labels": [], "entities": [{"text": "AD", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.6993183493614197}]}, {"text": "Two-second speech fragments turnout to be sufficient for reliable AD.", "labels": [], "entities": [{"text": "AD", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.872440755367279}]}, {"text": "Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the past years, the phenomenon of multiparty spoken interaction has drawn many researchers' attention (.", "labels": [], "entities": []}, {"text": "How do we address other people in such conversations?", "labels": [], "entities": []}, {"text": "Normally, we do this either explicitly, directly specifying desirable addressees by their names, or implicitly, using contextual) and multimodal markers.", "labels": [], "entities": []}, {"text": "Particularly, we use acoustic markers to emphasise special addressees, such as hard-of-hearing people (, elderly people, children, and automatic spoken dialogue systems (SDSs) (.", "labels": [], "entities": [{"text": "automatic spoken dialogue systems (SDSs)", "start_pos": 135, "end_pos": 175, "type": "TASK", "confidence": 0.6496888526848384}]}, {"text": "We act in this way if we realise that our addressee may have some communicational difficulties, and therefore we modify our normal manner of speech, making it more rhythmical, louder, and generally more understandable as soon as we start talking to such conversational partners (.", "labels": [], "entities": []}, {"text": "In the present research, we deal with two binary acoustic addressee detection (AD) problems.", "labels": [], "entities": [{"text": "binary acoustic addressee detection (AD)", "start_pos": 42, "end_pos": 82, "type": "TASK", "confidence": 0.7982392055647713}]}, {"text": "The first problem of human-machine addressee detection (H-M AD) arises in conversations within a group of users solving a cooperative task by means of an SDS.", "labels": [], "entities": [{"text": "human-machine addressee detection (H-M AD)", "start_pos": 21, "end_pos": 63, "type": "TASK", "confidence": 0.7982917853764125}]}, {"text": "The users may talk to each other and also contact the system from time to time.", "labels": [], "entities": []}, {"text": "The system is supposed to distinguish between machineand human-directed utterances in order to maintain conversations in a realistic manner.", "labels": [], "entities": []}, {"text": "Humandirected utterances do not require a direct system response and should be processed with the system in an implicit way.", "labels": [], "entities": []}, {"text": "We use the following two corpora to model the H-M AD problem: the Smart Video Corpus (SVC) () and the Voice Assistant Conversation Corpus (VACC) ).", "labels": [], "entities": [{"text": "H-M AD problem", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7833644350369772}]}, {"text": "The first competitive VACC baseline is introduced in the present paper.", "labels": [], "entities": [{"text": "VACC baseline", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.7056585252285004}]}, {"text": "The second problem of adult-child addressee detection (A-C AD) appears in conversations between a group of adults and a child.", "labels": [], "entities": [{"text": "adult-child addressee detection (A-C AD)", "start_pos": 22, "end_pos": 62, "type": "TASK", "confidence": 0.6976728226457324}]}, {"text": "In this case, our system is supposed to distinguish between childand adult-directed utterances.", "labels": [], "entities": []}, {"text": "A possible application for such a system of adult-child conversation monitoring is the estimation of children's and adults' conversational behaviour that will allow us to measure Interaction Quality (IQ) (.", "labels": [], "entities": [{"text": "Interaction Quality (IQ)", "start_pos": 179, "end_pos": 203, "type": "METRIC", "confidence": 0.7492341995239258}]}, {"text": "According to this complex metric, we will be able to assess the children's progress in maintaining conversations.", "labels": [], "entities": []}, {"text": "We model the A-C AD problem, using the HomeBank Child-Adult Addressee Corpus (HB-CHAAC, mentioned as HB below for simplicity).", "labels": [], "entities": [{"text": "HomeBank Child-Adult Addressee Corpus", "start_pos": 39, "end_pos": 76, "type": "DATASET", "confidence": 0.7971317321062088}]}, {"text": "We consider both binary classification problems as one: the utterances belonging to the first category are directed to a special addressee that maybe an SDS or a child having alack of communicational skills.", "labels": [], "entities": []}, {"text": "The utterances belonging to the second category are directed to ordinary adults without any impairments that may cause miscommunication.", "labels": [], "entities": []}, {"text": "In this light, we conduct a series of cross-corpus experiments and merge several corpora with the mixup method.", "labels": [], "entities": []}, {"text": "This data augmentation technique has already been studied on image classification (, speech recognition (, and acoustic emotion recognition).", "labels": [], "entities": [{"text": "image classification", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.788229763507843}, {"text": "speech recognition", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7864602506160736}, {"text": "acoustic emotion recognition", "start_pos": 111, "end_pos": 139, "type": "TASK", "confidence": 0.6676656901836395}]}, {"text": "The present paper has the following contributions: the H-M and the A-C AD problem are jointly analysed by means of machine learning; mixup in combination with state-of-the-art classifiers is applied to cross-corpus acoustic AD for the first time; mixup capabilities are investigated over various speech signal representations (including raw data), acoustic context lengths, corpora, domains, languages, and classification problems.", "labels": [], "entities": [{"text": "cross-corpus acoustic AD", "start_pos": 202, "end_pos": 226, "type": "TASK", "confidence": 0.6498469611008962}]}], "datasetContent": [{"text": "Some metafeatures obtained from an automatic speech recogniser (ASR) are useful for H-M AD since people speak more clearly than usual when addressing an SDS.", "labels": [], "entities": [{"text": "speech recogniser (ASR)", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7483537077903748}, {"text": "H-M AD", "start_pos": 84, "end_pos": 90, "type": "TASK", "confidence": 0.9403614699840546}]}, {"text": "Machine-directed speech tends to match the ASR patterns better compared to human-directed speech, resulting in a higher ASR confidence.", "labels": [], "entities": [{"text": "ASR", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9660168290138245}, {"text": "ASR", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9025588035583496}]}, {"text": "It is interesting to check this approach on A-C AD.", "labels": [], "entities": []}, {"text": "Using the Google Cloud ASR for German (on VACC and SVC) and for English (on HB), we extract the following ASR metafeatures at the utterance level: confidence of the best hypothesis, number of hypotheses, number of words in the best hypothesis, and utterance duration in seconds.", "labels": [], "entities": [{"text": "VACC and SVC", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.7804939150810242}, {"text": "confidence", "start_pos": 147, "end_pos": 157, "type": "METRIC", "confidence": 0.9808171987533569}, {"text": "duration", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.7767615914344788}]}, {"text": "These features except the first one (it is already normalised) are brought to zero mean and unit variance and fed to an SVM with a radial kernel.", "labels": [], "entities": []}, {"text": "The UAR values obtained with this classifier on the test partitions from are equal to 0.778, 0.657, and 0.515 for VACC, SVC, and HB respectively.", "labels": [], "entities": [{"text": "UAR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9650670289993286}, {"text": "VACC", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.7060433626174927}]}, {"text": "The latter value is slightly above a random-choice UAR of 0.5, meaning that ASR confidence is nonrepresentative for A-C AD.", "labels": [], "entities": [{"text": "UAR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.8992440700531006}, {"text": "ASR confidence", "start_pos": 76, "end_pos": 90, "type": "METRIC", "confidence": 0.8992344737052917}]}], "tableCaptions": [{"text": " Table 1: General characteristics of the considered data sets and their utterance-level labelling. Number of speakers  is specified in parentheses. Utterance labels: H -human-, M -machine-, A -adult-, C -child-directed. It is assumed  that H = A and M = C.", "labels": [], "entities": []}, {"text": " Table 1. A kernel density  estimation (KDE) is depicted in", "labels": [], "entities": []}, {"text": " Table 2: Two-second UAR slices. Each marker corre- sponds to a curve of the same style in", "labels": [], "entities": [{"text": "UAR slices", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.6737989485263824}]}]}