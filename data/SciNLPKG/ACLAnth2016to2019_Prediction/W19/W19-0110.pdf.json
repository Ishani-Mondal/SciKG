{"title": [{"text": "Learning exceptionality and variation with lexically scaled MaxEnt \u21e4", "labels": [], "entities": []}], "abstractContent": [{"text": "A growing body of research in phonology addresses the representation and learning of variable processes and exceptional, lexically conditioned processes.", "labels": [], "entities": []}, {"text": "(2013) present a MaxEnt model with additive lexical scales to account for data exhibiting both variation and exceptionality.", "labels": [], "entities": []}, {"text": "In this paper, we implement a learning model for lexically scaled MaxEnt grammars which we show to be successful across a range of data containing patterns of variation and exceptionality.", "labels": [], "entities": []}, {"text": "We also explore how the model's parameters and the rate of exceptionality in the data influence its performance and predictions for novel forms.", "labels": [], "entities": []}], "introductionContent": [{"text": "While phonological research often focuses on categorical generalizations, a growing body of research addresses the representation and learning of variable processes and exceptional processes, where application is lexically conditioned (see and Pater (2010) for overviews).", "labels": [], "entities": []}, {"text": "A few recent studies have modeled processes that exhibit both variation and exceptionality.", "labels": [], "entities": []}, {"text": "model co-existing exceptionality and variation in Russian using a Maximum En- tropy (MaxEnt) grammar) with additive, lexically specified scales.", "labels": [], "entities": []}, {"text": "Russian contains a vowel alternation process that exhibits both variation and idiosyncratic lexical conditioning (exceptionality).", "labels": [], "entities": []}, {"text": "show that speakers apply this process variably and that its variation differs across lexical items.", "labels": [], "entities": []}, {"text": "In their lexical scaling framework, each lexical item is associated with a vector of scales that are added to the general weights of the grammar's constraints.", "labels": [], "entities": []}, {"text": "These summed weights are used to calculate the probability of the input's surface realization.", "labels": [], "entities": []}, {"text": "This allows the likelihood of a phonological process to differ across morphemes, since the scales can modulate how constraints are weighted for different lexemes.", "labels": [], "entities": []}, {"text": "While show that a lexically scaled MaxEnt grammar can successfully represent Russian speakers' knowledge of a pattern that is both variable and exceptional, they do not show how such a grammar would be learned.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a model for learning lexically scaled MaxEnt grammars from data exhibiting both variation and exceptionality.", "labels": [], "entities": []}, {"text": "The primary challenge for formalizing learning in this framework is generalizing appropriately beyond the learning data and limiting the learner's reliance on lexical scales.", "labels": [], "entities": []}, {"text": "Since every morpheme can potentially scale the weight of every constraint, there is potential for massively over-fitting the learning data and failing to generalize.", "labels": [], "entities": []}, {"text": "We approach this challenge as a problem of feature selection and seek a learner that utilizes scales (i.e., assigning them nonzero weights) only when needed to account for lexical conditioning.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7187183201313019}]}, {"text": "We propose an objective function relying on an L1 (linear) prior ( \u00a72), rather than the more commonly used L2 (quadratic) prior ( \u00a74.1), to formalize these criteria.", "labels": [], "entities": []}, {"text": "Our approach differs in a number of ways from previous models for learning exceptionality and variation.", "labels": [], "entities": []}, {"text": "We assume the learner must induce a weighting for general phonological constraints and make lexical conditioning choices without prior knowledge of which lexical items behave exceptionally (.", "labels": [], "entities": []}, {"text": "Rather than splitting the learning of general phonological patterns and the learning of exceptions/classes into distinct learning phases, or treating the learning of lexical conditioning as emergent from repeated exposure to the lexicon, we seek to formally characterize the criteria that favor the desired balance of lexical sensitivity and generalization in a model that optimizes general weights and lexical conditioning in parallel.", "labels": [], "entities": []}, {"text": "Our approach is most similar to; however, we argue for an L1 prior rather than an L2 prior ( \u00a74.1).", "labels": [], "entities": []}, {"text": "We demonstrate the capacity of our model to learn variation and exceptionality using a variety of toy languages based on the Russian process mentioned above ( \u00a73).", "labels": [], "entities": []}, {"text": "We also explore the model's predictions for novel data, examining how the learner decides which patterns to treat as exceptional and which to generalize ( \u00a74).", "labels": [], "entities": []}, {"text": "Previous behavioral investigations of speakers' productive knowledge of lexically conditioned (morpho-)phonological alternations have found that speakers extend statistical tendencies in the lexicon to novel forms.", "labels": [], "entities": []}, {"text": "In some cases, the absolute rates of application of a process in nonce forms closely follow rates observed in the lexicon, yielding so-called \"frequencymatching\" behavior (.", "labels": [], "entities": []}, {"text": "In other cases, however, rates of application of exceptional processes are systematically skewed lower as compared to the lexical rates.", "labels": [], "entities": []}, {"text": "Under a variety of learning assumptions, frequency-matching behavior is not automatic.", "labels": [], "entities": []}, {"text": "To better understand some of the factors that may play a role in these divergent findings, we examine the properties of the data distribution and parameters of the model that affect frequency-matching behavior on nonce forms.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Categorical weights and mean scales", "labels": [], "entities": []}, {"text": " Table 2: Variable weights and mean scales", "labels": [], "entities": []}, {"text": " Table 3: Lexical weights and mean scales", "labels": [], "entities": []}, {"text": " Table 4: Variable-Lexical weights and mean scales", "labels": [], "entities": []}, {"text": " Table 5: Lexical weights and mean scales, C = 0.5 and 30", "labels": [], "entities": []}, {"text": " Table 6: Variable weights and mean scales, L2 prior, \ud97b\udf59 2 = 1", "labels": [], "entities": [{"text": "Variable weights and mean scales", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.8401843428611755}]}]}