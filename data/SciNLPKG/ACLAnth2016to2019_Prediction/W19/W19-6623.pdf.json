{"title": [{"text": "Identifying Fluently Inadequate Output in Neural and Statistical Machine Translation", "labels": [], "entities": [{"text": "Identifying Fluently", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8420659303665161}, {"text": "Statistical Machine Translation", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.5889257887999216}]}], "abstractContent": [{"text": "With the impressive fluency of modern machine translation output, systems may produce output that is fluent but not adequate (fluently inadequate).", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7758398850758871}]}, {"text": "We seek to identify these errors and quantify their frequency in MT output of varying quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.957757294178009}]}, {"text": "To that end, we introduce a method for automatically predicting whether translated segments are fluently inadequate by predicting fluency using grammaticality scores and predicting adequacy by augmenting sentence BLEU with a novel Bag-of-Vectors Sentence Similarity (BVSS).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.962794303894043}, {"text": "Bag-of-Vectors Sentence Similarity (BVSS", "start_pos": 231, "end_pos": 271, "type": "TASK", "confidence": 0.6948529481887817}]}, {"text": "We then apply this technique to analyze the outputs of statistical and neural systems for six language pairs with different levels of translation quality.", "labels": [], "entities": []}, {"text": "We find that neural models are consistently more prone to this type of error than traditional statistical models.", "labels": [], "entities": []}, {"text": "However, improving the overall quality of the MT system such as through domain adaptation reduces these errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9874435067176819}, {"text": "domain adaptation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6829786151647568}]}], "introductionContent": [{"text": "Recent work has shown that well-trained, indomain neural machine translation (NMT) systems can produce translations that, at the sentence level, are rated on par with human reference translations).", "labels": [], "entities": [{"text": "indomain neural machine translation (NMT)", "start_pos": 41, "end_pos": 82, "type": "TASK", "confidence": 0.8121938279696873}]}, {"text": "Part of this success comes from the impressive improvements in fluency of NMT output compared to previous MT paradigms ( 2019 The authors.", "labels": [], "entities": [{"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.979022204875946}]}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND.).", "labels": [], "entities": []}, {"text": "However, NMT has also been shown to sometimes produce output that is low adequacy and even unrelated to the input-particularly when not trained on sufficient in-domain data.", "labels": [], "entities": []}, {"text": "Because of NMT's uncanny ability to produce fluent output, these translations may not just be inadequate but fluently inadequate.", "labels": [], "entities": []}, {"text": "The fluency of fluently inadequate translations may mislead users into trusting the content based on fluency alone-particularly in the context of other fluent and adequate translations.", "labels": [], "entities": []}, {"text": "Mitigating the effects of fluently inadequate translations first requires understanding the scale of the problem and what situations are likely to generate these errors.", "labels": [], "entities": []}, {"text": "The general success and high system level quality of NMT suggests that fluently inadequate translations are rare, but we cannot say how rare without a means of automatically identifying potentially fluently inadequate translations in large collections of MT output.", "labels": [], "entities": []}, {"text": "In this work, we propose a method to automatically detect fluently inadequate translations based on the underlying characteristics of fluency and adequacy.", "labels": [], "entities": []}, {"text": "We view fluently inadequate translations as translations that are fluent, well-formed sentences that could have been written by a human, and that do not preserve the meaning of the reference.", "labels": [], "entities": []}, {"text": "In practice, given a reference translation rand MT hypothesis h, we consider h to be fluently inadequate if f luency(h) > \u03c4 f and adequacy(h, r) < \u03c4 a , where \u03c4 a and \u03c4 fare minimum fluency and adequacy thresholds respectively.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9672343730926514}]}, {"text": "We define novel fluency and adequacy metrics for this purpose, building on prior work on grammaticality detection and comparisons of multisets applied to word embeddings (Section 2).", "labels": [], "entities": [{"text": "grammaticality detection", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7050510197877884}]}, {"text": "We conduct two sets of experiments.", "labels": [], "entities": []}, {"text": "First, we evaluate the fluency and adequacy metrics, establishing that they can be used for the task of detecting fluently inadequate translations, and set thresholds \u03c4 a and \u03c4 f empirically in Sections 3.1 and 3.2.", "labels": [], "entities": []}, {"text": "We then conduct an automatic analysis to assess how frequent these errors are in neural and statistical machine translation (SMT) systems fora variety of languages and varying levels of model quality and train/test domain match.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.8108121057351431}]}, {"text": "We find that fluently inadequate translations are more common in NMT overall, especially when there is less training data and when there is a mismatch between training and test data.", "labels": [], "entities": [{"text": "NMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9072216749191284}]}], "datasetContent": [{"text": "Since there is no existing dataset with manual annotation of fluently inadequate translations, we first evaluate our fluency and adequacy prediction approaches comparing against direct assessment scores from WMT16 () as 2016 was the only year in which human fluency judgments were collected.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 208, "end_pos": 213, "type": "DATASET", "confidence": 0.9319463968276978}]}, {"text": "We then use our automated fluency scores on reference translations and automated adequacy scores on synthetic low adequacy \"translations\" to determine thresholds for high fluency and dubious adequacy.", "labels": [], "entities": []}, {"text": "Task For WMT16, fluency judgments were collected for Czech-English (CS-EN), GermanEnglish (DE-EN), Finnish-English (FI-EN), Romanian-English (RO-EN), Russian-English (RU-EN), and Turkish-English (TR-EN) in the news shared task.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.7095401883125305}]}, {"text": "Annotations were collected with the goal of system-level reliability, so many segments only have one judgment.", "labels": [], "entities": []}, {"text": "To improve reliability, we use only segments where there are two or more judgments.", "labels": [], "entities": [{"text": "reliability", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9880577325820923}]}, {"text": "Model setup Fluency scores are based on a 5-gram language model.", "labels": [], "entities": []}, {"text": "We built a 5-gram KenLM) language model using the monolingual news training data from WMT16.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.7161592245101929}]}, {"text": "Results For each of the metrics described in section 2.1, we calculated the Pearson correlation with the direct assessment scores for each of the language pair data sets and for all the data combined.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 76, "end_pos": 95, "type": "METRIC", "confidence": 0.9649922847747803}]}, {"text": "Although these correlations are lower than we would like, we find that for all language pairs and for the combined data, Word LP mid yields the highest correlation, so we will use this formula for our fluency prediction metric.: Precision, recall, and F1 on fluent translations for Word LP mid on system outputs for each language pair and on all system outputs.", "labels": [], "entities": [{"text": "fluency prediction", "start_pos": 201, "end_pos": 219, "type": "TASK", "confidence": 0.6961056292057037}, {"text": "Precision", "start_pos": 229, "end_pos": 238, "type": "METRIC", "confidence": 0.9957951307296753}, {"text": "recall", "start_pos": 240, "end_pos": 246, "type": "METRIC", "confidence": 0.9987877011299133}, {"text": "F1", "start_pos": 252, "end_pos": 254, "type": "METRIC", "confidence": 0.9996508359909058}]}, {"text": "The percentage of outputs that were labeled fluent based on the human fluency judgments is also provided for reference.", "labels": [], "entities": []}, {"text": "Setting the fluency threshold Because our goal is to correctly label sentences as fluently inadequate rather than to provide an exact score, we must select a fluency threshold \u03c4 f to label a translation as \"fluent\".", "labels": [], "entities": []}, {"text": "To determine this threshold, we computed the Word LP mid scores for the reference translation sentences in the WMT16 news training data.", "labels": [], "entities": [{"text": "Word LP mid scores", "start_pos": 45, "end_pos": 63, "type": "METRIC", "confidence": 0.742602601647377}, {"text": "WMT16 news training data", "start_pos": 111, "end_pos": 135, "type": "DATASET", "confidence": 0.9605252593755722}]}, {"text": "To cover most examples while allowing for variance inhuman judgments, the threshold is set at the point where 90% of reference segments would be labeled as fluent.", "labels": [], "entities": []}, {"text": "Precision, recall, and F1 scores for Word LP mid , are shown in.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9946773052215576}, {"text": "recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9919753670692444}, {"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997754693031311}]}, {"text": "Across all data sets we see high recall but the precision is not as high.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9995957016944885}, {"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9997288584709167}]}, {"text": "Although this suggests that this metric might overestimate the fluency of translations, we are more concerned with comparing between systems than with the raw scores.", "labels": [], "entities": []}, {"text": "Task and Data We assess adequacy metrics using the direct assessment adequacy scores and system outputs for all language pairs from WMT16 (.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.9770880937576294}]}, {"text": "Adequacy judgments were collected for all submitted systems in all language pairs in the news shared task.", "labels": [], "entities": []}, {"text": "These annotations were used to determine the system rankings in the news task and as gold standard quality judgments for the metrics shared task.", "labels": [], "entities": []}, {"text": "For the metrics task, enough annotations were collected for each system-produced segment to establish segmentlevel reliability, while only enough judgments for system-level reliability were collected for the remainder of the segments for the news task.", "labels": [], "entities": []}, {"text": "Because we need segment-level reliability, we use only the metrics subset of the data as gold standard human judgments, and we use the reference translations from the news subset in generating synthetic inadequate examples.", "labels": [], "entities": []}, {"text": "We use the standardized human direct assessment adequacy scores from WMT16 () as gold standard in determining how well each adequacy metric correlates with human judgments.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8771529197692871}]}, {"text": "However, for binary questionable/acceptable adequacy judgments, we must be sure that the inadequate examples are clearly inadequate regardless of fluency and other MT quirks.", "labels": [], "entities": [{"text": "MT", "start_pos": 164, "end_pos": 166, "type": "TASK", "confidence": 0.9545205235481262}]}, {"text": "The high correlation between human judgments of fluency and adequacy in Callison-Burch et al (2007) and: Precision, recall, and F1 on BLEU, BVSS, BVSS-Reference, BVSS-System, and BLEU with BVSS and BVSS-System on the questionable adequacy test set with thresholds calculated based on predicted adequacy scores for the synthetic low adequacy dev data. and treating one as synthetic MT output and the other as reference.", "labels": [], "entities": [{"text": "Precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9964648485183716}, {"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9983432292938232}, {"text": "F1", "start_pos": 128, "end_pos": 130, "type": "METRIC", "confidence": 0.999687671661377}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9953886270523071}, {"text": "BVSS", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9590561389923096}, {"text": "BVSS-Reference", "start_pos": 146, "end_pos": 160, "type": "METRIC", "confidence": 0.9407529830932617}, {"text": "BVSS-System", "start_pos": 162, "end_pos": 173, "type": "METRIC", "confidence": 0.9576106667518616}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.995008111000061}, {"text": "BVSS", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.8922407031059265}, {"text": "BVSS-System", "start_pos": 198, "end_pos": 209, "type": "METRIC", "confidence": 0.8877592086791992}]}, {"text": "We split these synthetic examples into dev and test sets.", "labels": [], "entities": []}, {"text": "The dev synthetic examples are used in choosing the binary acceptable/questionable adequacy threshold \u03c4 a as described below.", "labels": [], "entities": []}, {"text": "The test synthetic examples are used as the questionable adequacy items in our adequacy precision/recall test set, with acceptable adequacy items chosen from actual WMT16 submissions.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9459657073020935}, {"text": "recall test set", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.7611640195051829}, {"text": "WMT16", "start_pos": 165, "end_pos": 170, "type": "DATASET", "confidence": 0.7723870277404785}]}, {"text": "Because we are looking for extreme inadequacy and the systems in WMT16 were of competitively high quality, we use segments with direct assessment scores in the top 90% as acceptable adequacy in the test set.", "labels": [], "entities": [{"text": "WMT16", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.8574029207229614}]}, {"text": "Results For each metric defined in Section 2.2, we calculated the Pearson correlation with the direct assessment scores for each of the WMT16 language pair data sets and for all the data sets combined.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 66, "end_pos": 85, "type": "METRIC", "confidence": 0.9645741283893585}, {"text": "WMT16 language pair data sets", "start_pos": 136, "end_pos": 165, "type": "DATASET", "confidence": 0.9369847655296326}]}, {"text": "The averaged sentence embeddings had the lowest correlation across all language pairs.", "labels": [], "entities": []}, {"text": "BVSS-System performed similarly well compared to BLEU, but BVSS and BVSSReference both outperformed BLEU.", "labels": [], "entities": [{"text": "BVSS-System", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.5965940952301025}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9776524305343628}, {"text": "BVSS", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9105367064476013}, {"text": "BVSSReference", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.8769914507865906}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.8551760315895081}]}, {"text": "Setting the adequacy threshold As with fluency, our goal for the adequacy metric is to correctly label a sentence as questionable adequacy rather than to provide an exact score.", "labels": [], "entities": []}, {"text": "We used each candidate adequacy metric described in section 2.2 to score the segments in the synthetic low adequacy dev set, and set adequacy threshold \u03c4 a for each metric such that 99% of dev set examples would be labeled inadequate.", "labels": [], "entities": []}, {"text": "The precision, recall, and F1 on the synthetic test set using this threshold for each metric is shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996367692947388}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9994950294494629}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9998563528060913}]}, {"text": "We see that as with correlation scores, the Averaged Embeddings have much lower precision than BLEU or any of the BVSS metrics, and the BVSS metric have higher precision than BLEU.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9959557056427002}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9945791959762573}, {"text": "BVSS", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.8161841034889221}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9929977655410767}, {"text": "BLEU", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9886713027954102}]}, {"text": "Because of the potentially complementary differences in BLEU and BVSS, we also tested combinations of BLEU and the highest-performing vector-based metric, BVSS.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9976812601089478}, {"text": "BVSS", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9902712106704712}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9970147609710693}, {"text": "BVSS", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.5871865749359131}]}, {"text": "We combine the metrics by marking a translation as questionable adequacy only if both metrics would label it as questionable.", "labels": [], "entities": []}, {"text": "We see a slight improvement in F1 with the combination, and we adopt this metric for labeling segments as questionable adequacy.", "labels": [], "entities": [{"text": "F1", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9993153810501099}]}], "tableCaptions": [{"text": " Table 1: Pearson correlation between each of the fluency prediction metrics and the human fluency  direct assessment scores for each language and across all languages.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9245477318763733}, {"text": "fluency prediction", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7036767601966858}]}, {"text": " Table 2: Precision, recall, and F1 on fluent translations for Word LP mid on system outputs for each  language pair and on all system outputs. The percentage of outputs that were labeled fluent based on the  human fluency judgments is also provided for reference.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988677501678467}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9990310668945312}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.99977046251297}]}, {"text": " Table 3: Pearson correlation between each of the adequacy prediction metrics and the human adequacy  direct assessment scores for each language and across all languages.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9246662855148315}, {"text": "adequacy prediction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7005190551280975}]}, {"text": " Table 4:  Precision, recall, and F1 on  BLEU, BVSS, BVSS-Reference, BVSS-System,  and BLEU with BVSS and BVSS-System on the  questionable adequacy test set with thresholds cal- culated based on predicted adequacy scores for the  synthetic low adequacy dev data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9987735152244568}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995940327644348}, {"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9997780919075012}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9982104301452637}, {"text": "BVSS", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9737154245376587}, {"text": "BVSS-Reference", "start_pos": 53, "end_pos": 67, "type": "METRIC", "confidence": 0.9689235687255859}, {"text": "BVSS-System", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.956618070602417}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9974603652954102}, {"text": "BVSS", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9790849685668945}, {"text": "BVSS-System", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9567961692810059}]}, {"text": " Table 5: Number of segments in General Domain and TED training and test data for all languages", "labels": [], "entities": []}]}