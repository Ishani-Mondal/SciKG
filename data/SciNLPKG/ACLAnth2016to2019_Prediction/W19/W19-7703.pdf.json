{"title": [{"text": "Syntactic dependencies correspond to word pairs with high mutual information", "labels": [], "entities": []}], "abstractContent": [{"text": "How is syntactic dependency structure reflected in the statistical distribution of words in corpora?", "labels": [], "entities": []}, {"text": "Here we give empirical evidence and theoretical arguments for what we call the Head-Dependent Mutual Information (HDMI) Hypothesis: that syntactic heads and their dependents correspond to word pairs with especially high mutual information, an information-theoretic measure of strength of association.", "labels": [], "entities": [{"text": "Head-Dependent Mutual Information (HDMI) Hypothesis", "start_pos": 79, "end_pos": 130, "type": "TASK", "confidence": 0.6000569803374154}]}, {"text": "In support of this idea, we estimate mutual information between word pairs in dependencies based on an automatically-parsed corpus of 320 million tokens of English web text, finding that the mutual information between words in dependencies is robustly higher than a controlled baseline consisting of non-dependent word pairs.", "labels": [], "entities": []}, {"text": "Next, we give a formal argument which derives the HDMI Hypothesis from a probabilistic interpretation of the postulates of dependency grammar.", "labels": [], "entities": []}, {"text": "Our study also provides some useful empirical results about mutual information in corpora: we find that maximum-likelihood estimates of mutual information between raw word-forms are biased even at our large sample size, and we find that there is a general decay of mutual information between part-of-speech tags with distance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of quantitative syntax requires away to link the discrete formal structures typically studied in syntax, such as dependency trees, with the probabilistic distributions over wordforms observable in corpora.", "labels": [], "entities": []}, {"text": "Formal syntactic structures are usually taken to define the categorical well-formedness of sentences, or the latent structures required to derive an interpretation.", "labels": [], "entities": []}, {"text": "It remains unclear what relationship should obtain between these structures and statistical co-occurrence patterns over linguistic units as one might observe in a corpus.", "labels": [], "entities": []}, {"text": "Early work in linguistics tried to use these co-occurrence patterns as the basis on which to define formal syntactic structures, formulating 'discovery procedures' which would enable co-occurrence statistics to be summarized mechanistically using formal syntactic structures, but modern generative theories of syntax have eschewed any connection between statistical and syntactic structure, and to date it remains unclear whether corpus statistics contain enough information to fully reconstruct syntactic structures as identified by linguists.", "labels": [], "entities": []}, {"text": "NLP researchers working on grammar induction and unsupervised parsing have achieved substantial gains in recovering dependency trees on the basis of corpus statistics, but overall accuracy remains modest (;.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7808708846569061}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9990721940994263}]}, {"text": "Here we propose a high-level linking hypothesis between dependency structures and co-ocurrence statistics: syntactic dependencies correspond to word pairs with high mutual information (MI), an information-theoretic measure of the strength of covariance between two random variables ().", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 165, "end_pos": 188, "type": "METRIC", "confidence": 0.7617852210998535}]}, {"text": "We call this claim the Head-Dependent Mutual Information (HDMI) Hypothesis.", "labels": [], "entities": [{"text": "Head-Dependent Mutual Information (HDMI) Hypothesis", "start_pos": 23, "end_pos": 74, "type": "TASK", "confidence": 0.5852071344852448}]}, {"text": "In doing so we formalize and justify an intuition that has underlain much of the work on grammar induction for over 20 years ().", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7556403577327728}]}, {"text": "The basic intuition is that MI is a generic measure of strength of covariance, and heads and dependents are those word pairs whose covariance is most strongly constrained by grammatical rules.", "labels": [], "entities": [{"text": "MI", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.5294223427772522}]}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Sections 2 discusses the methods and dataset we used to measure MI and evaluate the HDMI Hypothesis in corpora; we believe this to be the largest-scale attempt to date to estimate MI between wordforms in natural language text.", "labels": [], "entities": [{"text": "MI", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.8034669756889343}, {"text": "estimate MI between wordforms in natural language text", "start_pos": 171, "end_pos": 225, "type": "TASK", "confidence": 0.7233944162726402}]}, {"text": "In Section 3, we present the results of the study, showing that dependencies do identify word pairs with especially high MI as measured in various ways.", "labels": [], "entities": [{"text": "MI", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.995455265045166}]}, {"text": "We also find that mutual information between part-of-speech tags decreases with distance, but we do not observe a similar decay pattern for mutual information between words represented as distributional clusters.", "labels": [], "entities": []}, {"text": "Next, in Section 4, we elaborate on the theoretical justification for the HDMI Hypothesis, providing a formal derivation of the hypothesis from an information-theoretic interpretation of the basic postulates of dependency grammar.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Common Crawl corpus) of English web text.", "labels": [], "entities": [{"text": "Common Crawl corpus) of English web text", "start_pos": 11, "end_pos": 51, "type": "DATASET", "confidence": 0.8876191973686218}]}, {"text": "We filtered the corpus to contain mostly meaningful linguistic utterances and to remove irrelevant web boilerplate text.", "labels": [], "entities": []}, {"text": "We parsed and POS-tagged 10% of the filtered corpus using SyntaxNet ().", "labels": [], "entities": []}, {"text": "The final dataset used in this paper consists of a total of 320 million tokens of parsed text.", "labels": [], "entities": []}, {"text": "SyntaxNet produces function-word-headed dependencies, rather than content-head dependencies, so our results reflect syntactic dependencies rather than semantic dependencies.", "labels": [], "entities": []}], "tableCaptions": []}