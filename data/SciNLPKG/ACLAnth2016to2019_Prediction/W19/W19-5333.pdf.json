{"title": [], "abstractContent": [{"text": "This paper describes Facebook FAIR's submission to the WMT19 shared news translation task.", "labels": [], "entities": [{"text": "WMT19 shared news translation task", "start_pos": 55, "end_pos": 89, "type": "TASK", "confidence": 0.7380856275558472}]}, {"text": "We participate in four language directions, English \u2194 German and English \u2194 Russian in both directions.", "labels": [], "entities": []}, {"text": "Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence mo-deling toolkit.", "labels": [], "entities": [{"text": "FAIRSEQ sequence mo-deling toolkit", "start_pos": 118, "end_pos": 152, "type": "DATASET", "confidence": 0.738201454281807}]}, {"text": "This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data.", "labels": [], "entities": []}, {"text": "We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking.", "labels": [], "entities": []}, {"text": "Our system improves on our previous system's performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English\u2192Russian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9992994070053101}, {"text": "BLEU score", "start_pos": 113, "end_pos": 123, "type": "METRIC", "confidence": 0.9748338758945465}, {"text": "translation direction English\u2192Russian", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.8827182292938233}]}], "introductionContent": [{"text": "We participate in the WMT19 shared news translation task in two language pairs and four language directions, English\u2192German (En\u2192De), German\u2192English (De\u2192En), English\u2192Russian (En\u2192Ru), and Russian\u2192English (Ru\u2192En).", "labels": [], "entities": [{"text": "WMT19 shared news translation task", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.6834645986557006}]}, {"text": "Our methods are based on techniques and approaches used in our submission from last year ( , including the use of subword models,), large-scale backtranslation, and model ensembling.", "labels": [], "entities": [{"text": "model ensembling", "start_pos": 165, "end_pos": 181, "type": "TASK", "confidence": 0.706144168972969}]}, {"text": "We train all models using the FAIRSEQ sequence modeling toolkit.", "labels": [], "entities": [{"text": "FAIRSEQ sequence modeling", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7177621920903524}]}, {"text": "Although document level context for En\u2192De is now available, all our systems are pure sentence level systems.", "labels": [], "entities": []}, {"text": "In the future, we expect better results from leveraging this additional context information.", "labels": [], "entities": []}, {"text": "Compared to our WMT18 submission, we also decide to compete in the En\u2194Ru and De\u2192En translation directions.", "labels": [], "entities": [{"text": "WMT18 submission", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.817592591047287}]}, {"text": "Although all four directions are considered high resource settings where large amounts of bitext data is available, we demonstrate that leveraging high quality monolingual data through back-translation is still very important.", "labels": [], "entities": []}, {"text": "For all language directions, we back-translate the Newscrawl dataset using a reverse direction bitext system.", "labels": [], "entities": [{"text": "Newscrawl dataset", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9900867342948914}]}, {"text": "In addition to back-translating the relatively clean Newscrawl dataset, we also experiment with back-translating portions of the much larger and noisier Commoncrawl dataset.", "labels": [], "entities": [{"text": "Newscrawl dataset", "start_pos": 53, "end_pos": 70, "type": "DATASET", "confidence": 0.9891566634178162}, {"text": "Commoncrawl dataset", "start_pos": 153, "end_pos": 172, "type": "DATASET", "confidence": 0.9927162230014801}]}, {"text": "For our final models, we apply a domain-specific finetuning process and decode using noisy channel model reranking).", "labels": [], "entities": []}, {"text": "Compared to our WMT18 submission in the En\u2192De direction, we observe substantial improvements of 4.5 BLEU.", "labels": [], "entities": [{"text": "WMT18 submission", "start_pos": 16, "end_pos": 32, "type": "DATASET", "confidence": 0.8538836538791656}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9965409636497498}]}, {"text": "Some of these gains can be attributed to differences in dataset quality, but we believe most of the improvement comes from larger models, larger scale back-translation, and noisy channel model reranking with strong channel and language models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: SacreBLEU for English-Russian models trai- ned with data back-translated using a single model vs.  an ensemble of two models", "labels": [], "entities": []}, {"text": " Table 4: Perplexity scores for language models on bol- ded target languages in all translation directions", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.910992443561554}]}, {"text": " Table 5: SacreBLEU scores on English\u2192German.", "labels": [], "entities": []}, {"text": " Table 6: SacreBLEU scores on German\u2192English.", "labels": [], "entities": []}, {"text": " Table 7: SacreBLEU scores on English\u2192Russian", "labels": [], "entities": []}, {"text": " Table 8: SacreBLEU scores on Russian\u2192English", "labels": [], "entities": []}]}