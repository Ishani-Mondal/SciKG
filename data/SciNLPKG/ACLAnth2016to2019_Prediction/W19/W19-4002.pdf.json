{"title": [{"text": "WiRe57 : A Fine-Grained Benchmark for Open Information Extraction", "labels": [], "entities": [{"text": "WiRe57", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8377938866615295}, {"text": "Open Information Extraction", "start_pos": 38, "end_pos": 65, "type": "TASK", "confidence": 0.6324558059374491}]}], "abstractContent": [{"text": "We build a reference for the task of Open Information Extraction, on five documents.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.7499410311381022}]}, {"text": "We tentatively resolve a number of issues that arise, including coreference and granularity, and we take steps toward addressing inference , a significant problem.", "labels": [], "entities": []}, {"text": "We seek to better pinpoint the requirements for the task.", "labels": [], "entities": []}, {"text": "We produce our annotation guidelines specifying what is correct to extract and what is not.", "labels": [], "entities": []}, {"text": "In turn, we use this reference to score existing Open IE systems.", "labels": [], "entities": []}, {"text": "We address the non-trivial problem of evaluating the extractions produced by systems against the reference tu-ples, and share our evaluation script.", "labels": [], "entities": []}, {"text": "Among seven compared extractors, we find the MinIE system to perform best.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction systems, starting with TextRunner (, seek to extract all relational tuples expressed in text, without being bound to an anticipated list of predicates.", "labels": [], "entities": []}, {"text": "Such systems have been used recently for relation extraction, questionanswering, and for building domain-targeted knowledge bases (), among others.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9706002473831177}]}, {"text": "Subsequent extractors (ReVerb, Ollie, ClausIE, OpenIE 4, etc.) have sought to improve yield and precision, i.e. the number of facts extracted from a given corpus, and the proportion of those facts that is deemed correct.", "labels": [], "entities": [{"text": "Ollie", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8082749247550964}, {"text": "ClausIE", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.6500473618507385}, {"text": "OpenIE 4", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.7961167395114899}, {"text": "yield", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9928668737411499}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9970822930335999}]}, {"text": "Nonetheless, the task definition is underspecified, and, to the best of our knowledge, there is no gold standard.", "labels": [], "entities": []}, {"text": "Most evaluations require somewhat subjective and inconsistent judgment calls to be made about extracted tuples being acceptable or not.", "labels": [], "entities": []}, {"text": "The most recent automatic benchmark of  has some shortcomings that we propose to tackle here, regarding the theory underlining the task definition as well as the evaluation procedure.", "labels": [], "entities": []}, {"text": "We manually performed the task of Open Information Extraction on 5 short documents, elaborating tentative guidelines for the task, and resulting in aground truth reference of 347 tuples.", "labels": [], "entities": [{"text": "Open Information Extraction", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6625290413697561}]}, {"text": "We evaluate against our benchmark the available OIE engines up to MinIE, with a fine-grained token-level evaluation.", "labels": [], "entities": [{"text": "MinIE", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9131724238395691}]}, {"text": "We distribute our resource and annotation guidelines, along with the evaluation script.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1:  Frequencies of various phenomena in  WiRe57.", "labels": [], "entities": [{"text": "WiRe57", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.9147816300392151}]}, {"text": " Table 3: Performance of available OpenIE systems (in chronological order) on our reference. Precision and recall  are computed at the token level. Systems with lower precision of matches are penalized for producing overlong  tuples. High precision and recall of matches overall show that our matching function (one shared word in each of  the first three parts) works correctly. Inferred words are required for exact matches.", "labels": [], "entities": [{"text": "Precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9971215128898621}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9988083839416504}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9455660581588745}, {"text": "precision", "start_pos": 239, "end_pos": 248, "type": "METRIC", "confidence": 0.9674977660179138}, {"text": "recall", "start_pos": 253, "end_pos": 259, "type": "METRIC", "confidence": 0.9896803498268127}]}]}