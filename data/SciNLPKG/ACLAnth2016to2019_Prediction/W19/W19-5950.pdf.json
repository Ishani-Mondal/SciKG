{"title": [{"text": "Which aspects of discourse relations are hard to learn? Primitive decomposition for discourse relation classification", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.6702641646067301}]}], "abstractContent": [{"text": "Discourse relation classification has proven to be a hard task, with rather low performance on several corpora that notably differ on the relation set they use.", "labels": [], "entities": [{"text": "Discourse relation classification", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8924724260965983}]}, {"text": "We propose to decompose the task into smaller, mostly binary tasks corresponding to various primitive concepts encoded into the discourse relation definitions.", "labels": [], "entities": []}, {"text": "More precisely, we translate the discourse relations into a set of values for attributes based on distinctions used in the map-pings between discourse frameworks proposed by Sanders et al.", "labels": [], "entities": []}, {"text": "This arguably allows fora more robust representation of discourse relations, and enables us to address usually ignored aspects of discourse relation prediction, namely multiple labels and underspecified annotations.", "labels": [], "entities": [{"text": "discourse relation prediction", "start_pos": 130, "end_pos": 159, "type": "TASK", "confidence": 0.6530596415201823}]}, {"text": "We study experimentally which of the conceptual primitives are harder to learn from the Penn Discourse Treebank English corpus, and propose a correspondence to predict the original labels, with preliminary empirical comparisons with a direct model.", "labels": [], "entities": [{"text": "Penn Discourse Treebank English corpus", "start_pos": 88, "end_pos": 126, "type": "DATASET", "confidence": 0.9889198064804077}]}], "introductionContent": [{"text": "Discourse parsing is a crucial task for natural language understanding, as it accounts for the coherence of a text by identifying semantic and pragmatic links between sentences and clauses.", "labels": [], "entities": [{"text": "Discourse parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8718188107013702}, {"text": "natural language understanding", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.645636796951294}]}, {"text": "The links are sometimes marked by explicit lexical items, so-called discourse connectives, but very often they rely on several lexical cues, contextual interpretation or even world knowledge, in which case they are called \"implicit\" relations.", "labels": [], "entities": []}, {"text": "Automating discourse parsing consists in finding which sentences or clauses are directly related in a text, and with what type of semantico-pragmatic relation.", "labels": [], "entities": [{"text": "Automating discourse parsing", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6628009776274363}]}, {"text": "The examples below demonstrate each type of relation, with the explicit discourse connective marked in bold, and example labels inspired by the Penn Discourse Treebank 2.0 () relation set.", "labels": [], "entities": [{"text": "Penn Discourse Treebank 2.0 () relation set", "start_pos": 144, "end_pos": 187, "type": "DATASET", "confidence": 0.9359528592654637}]}, {"text": "(1) Climate change is caused by anthropic activities, but politics are not doing anything about it.", "labels": [], "entities": []}, {"text": "Comparison.Concession.Contra-expectation (2) Climate is changing.", "labels": [], "entities": [{"text": "Comparison.Concession.Contra-expectation", "start_pos": 0, "end_pos": 40, "type": "METRIC", "confidence": 0.946254312992096}]}, {"text": "Humans generate too much CO 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main goal is to assess which primitives are harder to identify, we thus build separate models for each of them, i.e. basic operation, polarity, source of coherence, implication order, temporality, conditional, alternative and specificity (see Section 3 for definitions).", "labels": [], "entities": []}, {"text": "In addition, we compute the performance of our systems on discourse relations using a reverse mapping from a set of predicted values for each primitive to a relation, or, more precisely, to a set of potential relations.", "labels": [], "entities": []}, {"text": "We describe the reverse mapping in Section 5.1.", "labels": [], "entities": []}, {"text": "We also train systems on the task of directly predicting discourse relations, in order to check the validity of our models and to compare to the predictions derived from the primitives.", "labels": [], "entities": []}, {"text": "Recall that we aim at keeping all the particularities of the PDTB annotations, meaning the multiple relations and the relations at different levels of granularity.", "labels": [], "entities": []}, {"text": "This calls for specific evaluation metrics, relying on hierarchical multi-label measurement, that we describe in Section 5.2.", "labels": [], "entities": []}, {"text": "Our experimental setup raises a number of questions with respect to the evaluation: mapping a set of primitive values back to a PDTB label implies there might be underspecifications and corresponding to a disjunction of relations, either a coarse-grain label in the hierarchy or a set of possible relations.", "labels": [], "entities": []}, {"text": "To account for the first case, we can apply measures for hierarchical classification; the second case can betaken care of by measures for multi-label classification, which are needed anyway to take PDTB annotations without restrictions.", "labels": [], "entities": [{"text": "hierarchical classification", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.6806959509849548}, {"text": "multi-label classification", "start_pos": 138, "end_pos": 164, "type": "TASK", "confidence": 0.7299001514911652}]}, {"text": "There has not been much work on hierarchical discourse relation classification except (Versley, 2011), and the evaluation was just done at each granularity level, with either exact matching or a Dice coefficient between sets of labels (a relative overlap measure).", "labels": [], "entities": [{"text": "hierarchical discourse relation classification", "start_pos": 32, "end_pos": 78, "type": "TASK", "confidence": 0.6446109935641289}]}, {"text": "For a more general measure, we use hierarchical precision and recall) on the set of all predicted relations.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9017152190208435}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9975810050964355}]}, {"text": "For instance a predicted X.Y evaluated against a gold X.Z.T would get 0.5 precision (one level correct, one incorrect), and 0.33 recall (2 out of 3 levels missing from the prediction).", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9975185394287109}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9969567060470581}]}, {"text": "For multilabels, all levels are put in the same set.", "labels": [], "entities": []}, {"text": "To have an idea of the upper bound we could obtain this way, we also evaluated by considering only the best predicted label, with respect to hierarchical F-score, and prefixed the corresponding measures with max-h.", "labels": [], "entities": [{"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9311555624008179}]}], "tableCaptions": [{"text": " Table 2: Scores of the systems for each primitive on  test set (section 23 of the PDTB). The baseline is a  majority classifier. We report Accuracy (\"Acc\"), and,  for non-binary tasks, macro averaged F 1 (\"m-F 1 \") and  weighted F 1 (\"w-F 1 \").", "labels": [], "entities": [{"text": "PDTB", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9347022771835327}, {"text": "Accuracy (\"Acc", "start_pos": 140, "end_pos": 154, "type": "METRIC", "confidence": 0.8774831295013428}, {"text": "macro averaged F 1", "start_pos": 186, "end_pos": 204, "type": "METRIC", "confidence": 0.7399716675281525}]}, {"text": " Table 3: Scores of the systems for relation prediction, using the full relation set of the PDTB. The predicted  relations are either inferred from the predicted primitives (\"Primitives\"), or directly predicted (\"Relations\").We  report hierarchical recall (h-R) and hierarchical precision (h-P), along with max-h-P max-h-R, and accuracy.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.8236619532108307}, {"text": "PDTB", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.932868242263794}, {"text": "recall", "start_pos": 249, "end_pos": 255, "type": "METRIC", "confidence": 0.8556426763534546}, {"text": "precision", "start_pos": 279, "end_pos": 288, "type": "METRIC", "confidence": 0.9468376040458679}, {"text": "accuracy", "start_pos": 328, "end_pos": 336, "type": "METRIC", "confidence": 0.9994760155677795}]}]}