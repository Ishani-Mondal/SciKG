{"title": [{"text": "Detecting Everyday Scenarios in Narrative Texts", "labels": [], "entities": [{"text": "Detecting Everyday Scenarios in Narrative Texts", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8669107854366302}]}], "abstractContent": [{"text": "Script knowledge consists of detailed information on everyday activities.", "labels": [], "entities": []}, {"text": "Such information is often taken for granted in text and needs to be inferred by readers.", "labels": [], "entities": []}, {"text": "Therefore, script knowledge is a central component to language comprehension.", "labels": [], "entities": []}, {"text": "Previous work on representing scripts is mostly based on extensive manual work or limited to scenarios that can be found with sufficient redundancy in large corpora.", "labels": [], "entities": []}, {"text": "We introduce the task of scenario detection , in which we identify references to scripts.", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9755807816982269}]}, {"text": "In this task, we address a wide range of different scripts (200 scenarios) and we attempt to identify all references to them in a collection of narrative texts.", "labels": [], "entities": []}, {"text": "We present a first benchmark data set and a baseline model that tackles scenario detection using techniques from topic segmentation and text classification.", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.9233516454696655}, {"text": "topic segmentation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7423942685127258}, {"text": "text classification", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7298589944839478}]}], "introductionContent": [{"text": "According to theory of pragmatics, people tend to omit basic information when participating in a conversation (or writing a story) under the assumption that left out details are already known or can be inferred from commonsense knowledge by the hearer (or reader).", "labels": [], "entities": []}, {"text": "Consider the following text fragment about eating in a restaurant from an online blog post: Example 1.1 (.", "labels": [], "entities": []}, {"text": ") we drove to Sham Shui Po and looked fora place to eat.", "labels": [], "entities": []}, {"text": ")ne of the restaurants was fully seated [so we] chose another.", "labels": [], "entities": []}, {"text": "We had 4 dishes-Cow tripe stir fried with shallots, ginger and chili.", "labels": [], "entities": []}, {"text": "1000-year-old-egg with watercress and omelet.", "labels": [], "entities": []}, {"text": "Then another kind of tripe and egg-all crispy on the top and soft on the inside.", "labels": [], "entities": []}, {"text": "Finally calamari stir fried with rock salt and chili.", "labels": [], "entities": []}, {"text": "Washed down with beers and tea at the end.", "labels": [], "entities": []}, {"text": ") The text in Example 1.1 obviously talks about a restaurant visit, but it omits many events that are involved while eating in a restaurant, such as finding a table, sitting down, ordering food etc., as well as participants such as the waiter, the menu,the bill.", "labels": [], "entities": [{"text": "Example 1.1", "start_pos": 14, "end_pos": 25, "type": "DATASET", "confidence": 0.8698575496673584}]}, {"text": "A human reader of the story will naturally assume that all these ingredients have their place in the reported event, based on their commonsense knowledge, although the text leaves them completely implicit.", "labels": [], "entities": []}, {"text": "For text understanding machines that lack appropriate common-sense knowledge, the implicitness however poses a non-trivial challenge.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8765476047992706}]}, {"text": "Writing and understanding of narrative texts makes particular use of a specific kind of commonsense knowledge, referred to as script knowledge.", "labels": [], "entities": [{"text": "Writing and understanding of narrative texts", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7044071008761724}]}, {"text": "Script knowledge is about prototypical everyday activity, called scenarios.", "labels": [], "entities": []}, {"text": "Given a specific scenario, the associated script knowledge enables us to infer omitted events that happen before and after an explicitly mentioned event, as well as its associated participants.", "labels": [], "entities": []}, {"text": "In other words, this knowledge can help us obtain more complete text representations, as required for many language comprehension tasks.", "labels": [], "entities": []}, {"text": "There has been some work on script parsing), i.e., associating texts with script structure given a specific scenario.", "labels": [], "entities": [{"text": "script parsing", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7495673596858978}]}, {"text": "Unfortunately, only limited previous work exists on determining which scenarios are referred to in a text or text segment (see Section 2).", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first dataset of narrative texts which have annotations at sentence level according to the scripts they instantiate.", "labels": [], "entities": []}, {"text": "In this paper, we describe first steps towards the automatic detection and labeling of scenariospecific text segments.", "labels": [], "entities": [{"text": "automatic detection and labeling of scenariospecific text segments", "start_pos": 51, "end_pos": 117, "type": "TASK", "confidence": 0.7653729617595673}]}, {"text": "Our contributions are as follows: \u2022 We define the task of scenario detection and introduce a benchmark dataset of annotated narrative texts, with segments labeled according to the scripts they in-stantiate (Section 3).", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.9671802818775177}]}, {"text": "To the best of our knowledge, this is the first dataset of its kind.", "labels": [], "entities": []}, {"text": "The corpus is publicly available for scientific research purposes at this http: //www.sfb1102.uni-saarland.de/ ?page_id=2582.", "labels": [], "entities": []}, {"text": "\u2022 As a benchmark model for scenario detection, we present a two-stage model that combines established methods from topic segmentation and text classification (Section 4).", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.9707548916339874}, {"text": "topic segmentation", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7656964957714081}, {"text": "text classification", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7350913733243942}]}, {"text": "\u2022 Finally, we show that the proposed model achieves promising results but also reveals some of the difficulties underlying the task of scenario detection (Section 5).", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.9565782248973846}]}], "datasetContent": [{"text": "The experiments and results presented in this section are based on our annotated dataset for scenario detection described in section 3.", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.9356008172035217}]}, {"text": "We represent each input to our model as a sequence of lemmatized content words, in particular nouns and verbs (including verb particles).", "labels": [], "entities": []}, {"text": "This is achieved by preprocessing each text using Stanford CoreNLP).", "labels": [], "entities": [{"text": "Stanford CoreNLP", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.9427498281002045}]}, {"text": "Since the segmentation model is unsupervised, we can use all data from both MCScript and the Spinn3r personal stories corpora to build the LDA model.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9689075946807861}, {"text": "Spinn3r personal stories corpora", "start_pos": 93, "end_pos": 125, "type": "DATASET", "confidence": 0.9551730304956436}]}, {"text": "As input to the TopicTiling segmentor, each sentence is represented by a vector in which each component represents the (weight of a) topic from the LDA model (i.e. the value of the i th component is the normalized weight of the words in the sentence whose most relevant topic is the i th topic).", "labels": [], "entities": [{"text": "TopicTiling segmentor", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7658674716949463}]}, {"text": "For the segmentation model, we tune the number of topics (200) and the window size (2) based on an artificial development dataset, created by merging segments from multiple documents from MCScript.", "labels": [], "entities": []}, {"text": "We train the scenario classification model on the scenario labels provided in MCScript (one per text).", "labels": [], "entities": [{"text": "scenario classification", "start_pos": 13, "end_pos": 36, "type": "TASK", "confidence": 0.8980320394039154}]}, {"text": "For training and hyperparameter selection, we split MCScript dataset (see Section 2) into a training and development set, as indicated in.", "labels": [], "entities": [{"text": "hyperparameter selection", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.77922323346138}, {"text": "MCScript dataset", "start_pos": 52, "end_pos": 68, "type": "DATASET", "confidence": 0.8245172798633575}]}, {"text": "We additionally make use of 18 documents from our scenario detection data (Section 3) to tune a classification threshold.", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9009222984313965}]}, {"text": "The remaining 486 documents are held out exclusively for testing (see).", "labels": [], "entities": []}, {"text": "Since we train separate classifiers for each scenario (one-vs-all classifiers), we get a probability distribution of how likely a sentence refers to a scenario.", "labels": [], "entities": []}, {"text": "We use entropy to measure the degree of scenario content in the sentences.", "labels": [], "entities": []}, {"text": "Sentences with entropy values higher than the threshold are considered as not referencing any scenario (None cases), while sentences with lower entropy values reference some scenario.", "labels": [], "entities": []}, {"text": "We experiment with three informed baselines: As a lower bound for the classification task, we compare our model against the baseline: Results for the scenario detection task sent maj, which assigns the majority label to all sentences.", "labels": [], "entities": [{"text": "scenario detection task", "start_pos": 150, "end_pos": 173, "type": "TASK", "confidence": 0.8740554054578146}]}, {"text": "To assess the utility of segmentation, we compare against two baselines that use our proposed classifier but not the segmentation component: the baseline sent tf.idf treats each sentence as a separate segment and random tf.idf splits each document into random segments.", "labels": [], "entities": []}, {"text": "We evaluate scenario detection performance at the sentence level using micro-average precision, recall and F 1 -score.", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.8922551274299622}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.8435288071632385}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9996570348739624}, {"text": "F 1 -score", "start_pos": 107, "end_pos": 117, "type": "METRIC", "confidence": 0.9903568774461746}]}, {"text": "We consider the top 1 predicted scenario for sentences with only one gold label (including the None label), and top n scenarios for sentences with n gold labels.", "labels": [], "entities": []}, {"text": "For sentences with multiple scenario labels, we take into account partial matches and count each label proportionally.", "labels": [], "entities": []}, {"text": "Assuming the gold labels are washing ones hair and taking a bath, and the classifier predicts taking a bath and getting ready for bed.", "labels": [], "entities": []}, {"text": "Taking a bath is correctly predicted and accounts for 0.5 true positive (TP) while washing ones hair is incorrectly missed, thus accounts for 0.5 false negative (FN).", "labels": [], "entities": [{"text": "true positive (TP)", "start_pos": 58, "end_pos": 76, "type": "METRIC", "confidence": 0.9054011464118957}, {"text": "false negative (FN)", "start_pos": 146, "end_pos": 165, "type": "METRIC", "confidence": 0.8271957278251648}]}, {"text": "Getting ready for bed is incorrectly predicted and accounts for 1 false positive (FP).", "labels": [], "entities": [{"text": "1 false positive (FP)", "start_pos": 64, "end_pos": 85, "type": "METRIC", "confidence": 0.8642675975958506}]}, {"text": "We additionally provide separate results of the segmentation component based on standard segmentation evaluation metrics.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.9756354093551636}]}], "tableCaptions": [{"text": " Table 1: Top part shows scenario collections and num- ber of associated event sequence descriptions (ESDs).  Bottom part lists story corpora together with the num- ber of stories and different scenarios covered. The last  two columns indicate whether the stories are classified  and segmented, respectively.", "labels": [], "entities": []}, {"text": " Table 2: Kappa (and raw) agreement between pairs of  annotators on sentence-level scenario labels", "labels": [], "entities": []}, {"text": " Table 3: Relative agreement on segment spans between  annotated segments that overlap by at least one token  and are assigned the same scenario label", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.6591522097587585}]}, {"text": " Table 4: Distribution of scenario labels over documents  (docs), sentences (sents) and segments (segs); the top  and bottom parts show the ten most and least frequent  labels, respectively. The middle part shows scenario  labels that appear at an average frequency.", "labels": [], "entities": []}, {"text": " Table 5: Datasets (number of documents) used in the  experiments", "labels": [], "entities": []}, {"text": " Table 6: Results for the scenario detection task", "labels": [], "entities": [{"text": "scenario detection", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.9624834656715393}]}, {"text": " Table 7: Top 10 misclassified scenario pairs (number  of misclassified sentences (# sents.)) by our approach  TT tf.idf in relation to the PMI scores for each pair.", "labels": [], "entities": []}, {"text": " Table 8: Top 10 scenario-wise Precision (P), Recall (R)  and F 1 -score (F 1 ) results using our approach TT tf.idf  and the number of gold sentences (# sents.) for each  scenario.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 31, "end_pos": 44, "type": "METRIC", "confidence": 0.9291212260723114}, {"text": "Recall (R)", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9448846876621246}, {"text": "F 1 -score (F 1 )", "start_pos": 62, "end_pos": 79, "type": "METRIC", "confidence": 0.9666042849421501}]}]}