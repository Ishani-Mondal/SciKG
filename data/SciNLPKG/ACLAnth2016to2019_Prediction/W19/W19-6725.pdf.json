{"title": [{"text": "Leveraging Rule-Based Machine Translation Knowledge for Under-Resourced Neural Machine Translation Models", "labels": [], "entities": [{"text": "Leveraging Rule-Based Machine Translation Knowledge", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.6239363431930542}, {"text": "Neural Machine Translation", "start_pos": 72, "end_pos": 98, "type": "TASK", "confidence": 0.7046481569608053}]}], "abstractContent": [{"text": "Rule-based machine translation is a machine translation paradigm where linguistic knowledge is encoded by an expert in the form of rules that translate from source to target language.", "labels": [], "entities": [{"text": "Rule-based machine translation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5983120501041412}, {"text": "machine translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7455053627490997}]}, {"text": "While this approach grants total control over the output of the system, the cost of formalising the needed linguistic knowledge is much higher than training a corpus-based system, where a machine learning approach is used to automatically learn to translate from examples.", "labels": [], "entities": []}, {"text": "In this paper, we describe different approaches to leverage the information contained in rule-based machine translation systems to improve a corpus-based one, namely, a neural machine translation model, with a focus on a low-resource scenario.", "labels": [], "entities": [{"text": "rule-based machine translation", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.7366121013959249}, {"text": "neural machine translation", "start_pos": 169, "end_pos": 195, "type": "TASK", "confidence": 0.7734334071477255}]}, {"text": "Our results suggest that adding morphological information to the source language is as effective as using subword units in this particular setting.", "labels": [], "entities": []}], "introductionContent": [{"text": "In rule-based machine translation (RBMT), a linguist formalises linguistic knowledge into lexicons and grammar rules.", "labels": [], "entities": [{"text": "rule-based machine translation (RBMT)", "start_pos": 3, "end_pos": 40, "type": "TASK", "confidence": 0.8244072099526724}]}, {"text": "This knowledge is used by the system to analyse sentences in the source language and translate them.", "labels": [], "entities": []}, {"text": "While this approach does not require any training corpora and grants control over the translations created by the system, the process of encoding linguistic knowledge requires great amounts of expert time.", "labels": [], "entities": []}, {"text": "Notable examples of RBMT systems are the original, rule-based Systran, Lucy LT ( and).", "labels": [], "entities": [{"text": "RBMT", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.8761664628982544}, {"text": "Lucy LT", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9526799619197845}]}, {"text": "Instead, corpus-based machine translation systems learn to translate from examples, usually in the form of sentence-level aligned corpora.", "labels": [], "entities": [{"text": "corpus-based machine translation", "start_pos": 9, "end_pos": 41, "type": "TASK", "confidence": 0.713708241780599}]}, {"text": "On the one hand, this approach is generally more computationally expensive and offers limited control over the generated translations.", "labels": [], "entities": []}, {"text": "Furthermore, it is not feasible for language pairs that have little to no available parallel resources.", "labels": [], "entities": []}, {"text": "On the other hand, it boasts a much higher coverage of the targeted language pair, depending on the availability of parallel corpora.", "labels": [], "entities": []}, {"text": "Examples of corpus-based machine translation paradigms are statistical phrase-based translation ( and neural machine translation (NMT) models (.", "labels": [], "entities": [{"text": "corpus-based machine translation", "start_pos": 12, "end_pos": 44, "type": "TASK", "confidence": 0.6933289766311646}, {"text": "statistical phrase-based translation", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.6021660069624583}, {"text": "neural machine translation (NMT)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.8202478289604187}]}, {"text": "In this work, we focused on leveraging RBMT knowledge for improving the performance of NMT systems in an under-resourced scenario.", "labels": [], "entities": [{"text": "RBMT", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.8971638083457947}]}, {"text": "Namely, we used information contained in Lucy LT, an RBMT system where the linguistic knowledge is formalised by human linguists as computational grammars, monolingual and bilingual lexicons.", "labels": [], "entities": [{"text": "Lucy LT", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9558877050876617}]}, {"text": "Monolingual lexicons are collections of lexical entries; each lexical entry is a set of feature-value pairs containing morphological, syntactic and semantic information.", "labels": [], "entities": []}, {"text": "Bilingual lexicon entries include source-target lexical correspondences and, optionally, contextual conditions and actions.", "labels": [], "entities": []}, {"text": "Grammars are collections of transformations to annotated trees.", "labels": [], "entities": []}, {"text": "The Lucy LT system divides the translation process into three sequential phases: analysis, transfer, and generation.", "labels": [], "entities": [{"text": "Lucy LT", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9404189884662628}]}, {"text": "During the analysis phase, the source sentence is tokenised and morphologically analysed by means of a lexicon that identifies each surface form and all its plausible morphological readings.", "labels": [], "entities": []}, {"text": "Next, the Lucy LT chart parser together with a analysis grammar consisting of augmented syntactic rules extracts the underlying syntax tree structure and annotates it.", "labels": [], "entities": [{"text": "Lucy LT chart", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9107019901275635}]}, {"text": "The transfer and generation grammars are then applied in succession on that tree, which undergoes multiple annotations and transformations that add information about the equivalences in the target language and adapt the original source language structures to the appropriate ones in the target language.", "labels": [], "entities": []}, {"text": "Finally, the terminal nodes of the generation tree are assembled into the translated sentence.", "labels": [], "entities": []}, {"text": "We focused on the analysis phase, with special interest for two of the features used: the morphological category (CAT) and the inflection class (CL) or classes of the lexical entries.", "labels": [], "entities": []}, {"text": "In order to test this approach, we focused on English-Spanish (both generic and medical domain), English-Basque, English-Irish and EnglishSimplified Chinese in an under-resourced scenario, using corpora with around one million parallel entries.", "labels": [], "entities": []}, {"text": "Results suggested that adding morphological information to the source language is as effective as using subword units in this particular setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the resources we used to train and evaluate the systems, along with the neural machine translation framework used.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.7130318284034729}]}, {"text": "In this work, we focused on NMT for underresourced scenarios.", "labels": [], "entities": [{"text": "NMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9325516819953918}]}, {"text": "On the one hand, we consider languages, such as Basque or Irish, which do not have a significant amount of parallel data necessary to train a neural model.", "labels": [], "entities": []}, {"text": "On the other hand, an under-resourced scenario can be a specific domain, e.g. medical, where a significant amount of data exists, but does not cover the targeted domain.", "labels": [], "entities": []}, {"text": "The shows the statistics on the used datasets.", "labels": [], "entities": []}, {"text": "For Basque and Irish, we used the available corpora stored on the OPUS webpage.", "labels": [], "entities": [{"text": "OPUS webpage", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9635539948940277}]}, {"text": "We used OpenSubtitles2018 (Lison and Tiedemann, 2016), Gnome and KDE4 datasets.", "labels": [], "entities": [{"text": "KDE4 datasets", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9182368516921997}]}, {"text": "Additionally, the English-Irish parallel corpus is augmented with second level education textbooks (Cuimhne na dT\u00e9acsleabhar) in the domain of economics and geography.", "labels": [], "entities": []}, {"text": "In addition to that, we also focused on well resourced languages (Spanish and Simplified Chinese), but limited the training datasets to around one million aligned sentences.", "labels": [], "entities": []}, {"text": "To ensure abroad lexical and domain coverage of our NMT system, we merged the existing English-Spanish parallel 2 opus.nlpl.eu 3 www.opensubtitles.org corpora from the OPUS web page into one parallel data set and randomly extracted the sentences.", "labels": [], "entities": [{"text": "OPUS web page", "start_pos": 168, "end_pos": 181, "type": "DATASET", "confidence": 0.9488228559494019}]}, {"text": "In addition to the previous corpora, we added Europarl (), DGT (), MultiUN corpus, EMEA and OpenOffice.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9815327525138855}, {"text": "OpenOffice", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9244852066040039}]}, {"text": "To evaluate the targeted under-resourced scenario within medical domain, we exclusively used the EMEA corpus.", "labels": [], "entities": [{"text": "EMEA corpus", "start_pos": 97, "end_pos": 108, "type": "DATASET", "confidence": 0.9411237239837646}]}, {"text": "For Simplified Chinese, we used a parallel corpus provided by the industry partner, which was collected from bilingual English-Simplified Chinese news portals.", "labels": [], "entities": [{"text": "Simplified Chinese", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.6461897194385529}]}, {"text": "The corpora were tokenised using the OpenNMT toolkit, with the exception of Simplified Chinese, that was tokenized using Jieba, and lowercased.", "labels": [], "entities": []}, {"text": "In order to evaluate the performance of the different systems, we used BLEU (), an automatic evaluation that boasts high correlation with human judgements, and translation error rate (TER) (), a metric that represents the cost of editing the output of the MT systems to match the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9987908005714417}, {"text": "translation error rate (TER)", "start_pos": 160, "end_pos": 188, "type": "METRIC", "confidence": 0.887435108423233}, {"text": "MT", "start_pos": 256, "end_pos": 258, "type": "TASK", "confidence": 0.9333388805389404}]}, {"text": "Additionally, we used bootstrap resampling) with a sample size of 1,000 and 1,000 iterations, and reported statistical significance with < 0.05.", "labels": [], "entities": []}, {"text": "We also presented a box-and-whisker plot with the first, second and third quartiles as a box, and the first (<0.025) and last (0.975) 40-quantiles as whiskers, corresponding to < 0.05.", "labels": [], "entities": []}, {"text": "In addition, we compared the performance of our NMT systems with the NMTbased Google Translate, and the translations performed using Lucy LT RBMT; for the latter, only English-Spanish and English-Basque models are available.", "labels": [], "entities": [{"text": "NMTbased Google Translate", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.9353294372558594}, {"text": "Lucy LT RBMT", "start_pos": 133, "end_pos": 145, "type": "DATASET", "confidence": 0.9131961862246195}]}], "tableCaptions": [{"text": " Table 1: Statistics on the used training, validation and evaluation datasets.", "labels": [], "entities": []}, {"text": " Table 2: Qualitative analysis of a sentence translated by all models for Spanish to English translation. Fragments in bold face  are translation mistakes, and fragments in italics are translation alternatives that, while being penalised by TER and BLEU, can  be considered correct.", "labels": [], "entities": [{"text": "Qualitative analysis of a sentence translated", "start_pos": 10, "end_pos": 55, "type": "TASK", "confidence": 0.8124431471029917}, {"text": "Spanish to English translation", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.7120563685894012}, {"text": "TER", "start_pos": 241, "end_pos": 244, "type": "METRIC", "confidence": 0.8935844302177429}, {"text": "BLEU", "start_pos": 249, "end_pos": 253, "type": "METRIC", "confidence": 0.9891663193702698}]}]}