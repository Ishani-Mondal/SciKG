{"title": [{"text": "Webinterpret Submission to the WMT2019 Shared Task on Parallel Corpus Filtering", "labels": [], "entities": [{"text": "WMT2019", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.6270180344581604}, {"text": "Parallel Corpus Filtering", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.58714892466863}]}], "abstractContent": [{"text": "This document describes the participation of Webinterpret in the shared task on parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019).", "labels": [], "entities": [{"text": "parallel corpus filtering at the Fourth Conference on Machine Translation (WMT 2019)", "start_pos": 80, "end_pos": 164, "type": "TASK", "confidence": 0.7661225625446865}]}, {"text": "Here, we describe the main characteristics of our approach and discuss the results obtained on the data sets published for the shared task.", "labels": [], "entities": []}, {"text": "1 Task Description Parallel corpus filtering task at WMT19 tackles the problem of cleaning noisy parallel corpora.", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.5923604269822439}, {"text": "WMT19", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9218454957008362}]}, {"text": "Given a noisy parallel corpus (crawled from the web), participants develop methods to filter it to a smaller size of high quality sentence pairs.", "labels": [], "entities": []}, {"text": "In comparison to the German-English task last year, the organizers now pose the problem under more challenging low-resource conditions including Nepali and Sinhala languages.", "labels": [], "entities": []}, {"text": "The organizers provide very noisy 40.6 million-word (English token count) Nepali-English and a 59.6 million-word Sinhala-English corpora.", "labels": [], "entities": []}, {"text": "Both raw corpora were crawled from the web as part of the Paracrawl project 1.", "labels": [], "entities": [{"text": "Paracrawl project 1", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.8516384561856588}]}, {"text": "Participants are asked to select a subset of sentence pairs that amount to (a) 5 million, and (b) 1 million English words.", "labels": [], "entities": []}, {"text": "The quality of the resulting subsets is determined by the quality of a statistical and a neural Machine Translation (MT) systems trained on the selected data.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7977615714073181}]}, {"text": "The quality of the translation systems is measured on a held-out test set of Wikipedia translations.", "labels": [], "entities": []}, {"text": "Despite the known origin of the test set, the organizers make explicit that the task addresses the challenge of data quality and not domain-relatedness of the data fora particular use case.", "labels": [], "entities": []}, {"text": "For our submission, we propose a variation of coverage augmentation ranking (Haffari et al., 2009; Gasc\u00f3 et al., 2012; Gonz\u00e1lez-Rubio, 2014).", "labels": [], "entities": [{"text": "coverage augmentation ranking", "start_pos": 46, "end_pos": 75, "type": "METRIC", "confidence": 0.7439524928728739}]}, {"text": "The main idea underlying our approach is to minimize the amount of unseen events for the model.", "labels": [], "entities": []}, {"text": "In MT, these unseen events are words or sequences thereof.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9806252717971802}]}, {"text": "These unseen events result in a loss 1 https://paracrawl.eu/ of model coverage and, ultimately, of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.9503076672554016}]}, {"text": "The main difference of our submission respect to previous approaches is that we do not rely on an in-domain corpus to identify underrep-resented events.", "labels": [], "entities": []}, {"text": "Instead, we look for the subset of sentences that provide the most coherent coverage among themselves.", "labels": [], "entities": []}, {"text": "One of the advantages of this approach is that it does not rely on pre-trained models requiring additional data to train.", "labels": [], "entities": []}, {"text": "This characteristic fits perfectly with the focus on low-resource languages of this year's task.", "labels": [], "entities": []}, {"text": "The rest of this document is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe the details of our approach.", "labels": [], "entities": []}, {"text": "Next, we present the results of our submission.", "labels": [], "entities": []}, {"text": "Finally, we close with the conclusions and some ideas for future developments.", "labels": [], "entities": []}, {"text": "2 Sentence Pairs Ranking Our goal is to rank the sentence pairs in the raw corpora such that the pairs in the top of the ranking are better candidates for training data.", "labels": [], "entities": []}, {"text": "As pre-processing, we only apply tokenization via the TokTok tokenizer in the NLTK python package.", "labels": [], "entities": [{"text": "NLTK python package", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.8666256467501322}]}, {"text": "First, we filtered out some of the pairs (x, y) in the raw corpus according to several heuristic rules (Section 2.1).", "labels": [], "entities": []}, {"text": "Then, for the remaining pairs, we computed a ranking value r(x, y) for each of them.", "labels": [], "entities": []}, {"text": "This ranking, was the result of the combination of several different ranking functions aiming at capturing the \"value\" of the sentence pair according to different criteria (Section 2.2 and Section 2.3).", "labels": [], "entities": []}, {"text": "Finally, we used the final ranking of each pair to compute its corresponding score as required for the shared task (Section 2.4).", "labels": [], "entities": []}, {"text": "2.1 Initial Rule-based Filtering We start by describing the set of filtering rules implemented to reduce the amount of candidates to be ranked by the more sophisticated methods Sections 2.2, and 2.3.", "labels": [], "entities": []}, {"text": "These rules have been previously proposed and successfully implemented in the literature, for instance (Junczys-Dowmunt, 2018; Rossenbach et al., 2018).", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Participants in the shared task were asked to submit a file with quality scores, one per line, corresponding to the sentence pairs on the NepaliEnglish and Sinhala-English corpora.", "labels": [], "entities": [{"text": "NepaliEnglish and Sinhala-English corpora", "start_pos": 138, "end_pos": 179, "type": "DATASET", "confidence": 0.8354582637548447}]}, {"text": "The performance of the submissions is evaluated by subsampling 1 million and 5 million word corpora based on these scores, training statistical () and neural MT systems with these corpora, and assessing translation quality on blind tests using BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 244, "end_pos": 248, "type": "METRIC", "confidence": 0.9919382929801941}]}, {"text": "shows the scores of our three submissions for each language pair and condition.", "labels": [], "entities": []}, {"text": "Of the three, the one based on coverage rankings (SECONDARYCOV) showed a lower performance consistently being outperformed, particularly in the 1 million condition, by both our PRIMARY and SECONDARYADE submissions.", "labels": [], "entities": [{"text": "coverage", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9800077080726624}, {"text": "SECONDARYCOV)", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.8716882765293121}, {"text": "PRIMARY", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.8222944736480713}, {"text": "SECONDARYADE submissions", "start_pos": 189, "end_pos": 213, "type": "DATASET", "confidence": 0.7968832850456238}]}, {"text": "We were surprised by the \"poor\" performance of coverage ranking.", "labels": [], "entities": []}, {"text": "Previous works) showed quite promised results.", "labels": [], "entities": []}, {"text": "However, in contrast to our case, all these assume the availability of a sample of the domain to be translated.", "labels": [], "entities": []}, {"text": "We hypothesize that the lack of this in-domain data in conjunction with the eclectic domains of the data to be filtered are the causes of the poor results of this approach.", "labels": [], "entities": []}, {"text": "Moreover, the greedy selection implemented may aggravate this issue by taking not-optimal initial decisions from which the algorithm cannot recover.", "labels": [], "entities": []}, {"text": "Another interesting observation is the unintu- itive results for NMT.", "labels": [], "entities": []}, {"text": "While SMT results tend to go up as more data is selected, results for NMT tend to show the opposite trend.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9925406575202942}]}, {"text": "A fact to consider is that actual BLEU figures are quite low so the actual relevance of these trends are not clear.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9982321858406067}]}, {"text": "Additionally, given that this observation is valid other submissions as we will see next, we think this is an issue worthy of further investigation.", "labels": [], "entities": []}, {"text": "After discussing the performance of our submissions, we will compare our best submission on each condition to the rest of participants.", "labels": [], "entities": []}, {"text": "summarizes the results of the shared task as reported by the organizers of the task (.", "labels": [], "entities": []}, {"text": "Each sub-figure displays the best submission of each individual participant institution fora particular task and condition.", "labels": [], "entities": []}, {"text": "Plots in the upper row show results for Nepalese-English while the bottom row does the same for Sinhala-English.", "labels": [], "entities": []}, {"text": "Plots in the left column are for the 1 million condition while results for the 5 million condition are shown in the right column.", "labels": [], "entities": []}, {"text": "Stacked bars displayed in the plots denote the BLEU scores for the statistical (blue) and neural (red) systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9983483552932739}]}, {"text": "We sort them in increasing order according to each system's sum of SMT and NMT scores.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.7922732830047607}]}, {"text": "The organizers do not provide confidence intervals for the reported scores so compare results is somehow difficult.", "labels": [], "entities": []}, {"text": "Still, as we mention previously, it is surprising the degradation in translation quality for NMT when comparing the 5 million condition to the 1 million condition.", "labels": [], "entities": []}, {"text": "Usually, a larger amount of data correlates with an increase in translation quality.", "labels": [], "entities": []}, {"text": "In this case, however, scores for SMT barely changed while NMT results went down.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9916531443595886}, {"text": "NMT", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.634105920791626}]}, {"text": "This seems to indicate that our methods were not sophisticated enough to find adequate data, or that the really adequate data in the noise corpora amount for less than 5 million words.", "labels": [], "entities": []}, {"text": "Our submission (WI) lays in the upper half among the best submission of the different participants.", "labels": [], "entities": [{"text": "submission (WI)", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.5763662531971931}]}, {"text": "Regarding Nepalese-English, it scored an aggregated of 7.1 and 6.7 BLEU points for the 1 million and 5 million conditions respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9991365075111389}]}, {"text": "This represent respectively about a 64% of the best result submitted for the 1 million condition, and about a 85% of the best result for the 5 million condition.", "labels": [], "entities": []}, {"text": "As for the Sinhala-English condition, we scored 6.8 and 5.8 BLEU points which represent a 64% of the best results respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9995468258857727}]}], "tableCaptions": [{"text": " Table 1: Amount of sentence pairs (in Millions) filtered out by each filtering method. \"Combined\" denotes the  final amount of sentence pairs filtered out after applying the three methods in sequence.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9538254737854004}]}]}