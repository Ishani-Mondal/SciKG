{"title": [{"text": "Seeing more than whitespace - Tokenisation and disambiguation in a North S\u00e1mi grammar checker", "labels": [], "entities": []}], "abstractContent": [{"text": "Communities of lesser resourced languages like North S\u00e1mi benefit from language tools such as spell checkers and grammar checkers to improve literacy.", "labels": [], "entities": []}, {"text": "Accurate error feedback is dependent on well-tokenised input, but traditional tokenisation as shallow preprocessing is inadequate to solve the challenges of real-world language usage.", "labels": [], "entities": []}, {"text": "We present an alternative where tokenisation remains ambiguous until we have linguistic context information available.", "labels": [], "entities": [{"text": "tokenisation", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.974780261516571}]}, {"text": "This lets us accurately detect sentence boundaries, multiwords and compound error detection.", "labels": [], "entities": [{"text": "compound error detection", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.547582765420278}]}, {"text": "We describe a North S\u00e1mi grammar checker with such a tokenisation system, and show the results of its evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bilingual users frequently face bigger challenges regarding literacy in the lesser used language than in the majority language due to reduced access to language arenas.", "labels": [], "entities": []}, {"text": "However, literacy and in particular writing is important in today's society, both in social contexts and when using a computer or a mobile phone.", "labels": [], "entities": []}, {"text": "Language tools such as spellcheckers and grammar checkers therefore play an important role in improving literacy and the quality of written text in a language community.", "labels": [], "entities": []}, {"text": "North S\u00e1mi is spoken in Norway, Sweden and Finland by approximately 25,700 speakers, and written in a number of institutions like the daily S\u00e1mi newspaper (\u00c1vvir 1 ), a few S\u00e1mi journals, websites and social media of the S\u00e1mi radio and TV (e.g. ). In addition, the S\u00e1mi parliaments, the national governments, and a S\u00e1mi university college produce North S\u00e1mi text.", "labels": [], "entities": []}, {"text": "An open-source spellchecker for North S\u00e1mi has been freely distributed since 2007 ().", "labels": [], "entities": [{"text": "North S\u00e1mi", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.8261045813560486}]}, {"text": "However, a spellchecker is limited to looking only atone word contexts.", "labels": [], "entities": []}, {"text": "It can only detect non-words, i.e. words that cannot be found in the lexicon.", "labels": [], "entities": []}, {"text": "A grammar checker, however, looks at contexts beyond single words, and can correct misspelled words that are in the lexicon, but are wrong in the given context.", "labels": [], "entities": []}, {"text": "In addition, a grammar checker can detect grammatical and punctuation errors.", "labels": [], "entities": []}, {"text": "A common error in North S\u00e1mi and other compounding languages is to spell compound words as separate words instead of one.", "labels": [], "entities": []}, {"text": "The norm typically requires them to be written as one word, with the non-final components being in nominative or genitive case if they are nouns.", "labels": [], "entities": []}, {"text": "This reflects a difference in meaning between two words written separately and the same two words written as a compound.", "labels": [], "entities": []}, {"text": "Being able to detect and correct such compounding errors is thus important for the language community.", "labels": [], "entities": []}, {"text": "This paper presents and evaluates a grammar checker framework that handles ambiguous tokenisation, and uses that to detect compound errors, as well as improve sentence boundary detection after abbreviations and numeral expressions.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 159, "end_pos": 186, "type": "TASK", "confidence": 0.6318674186865488}]}, {"text": "The framework is completely open source, and completely rule-based.", "labels": [], "entities": []}, {"text": "The evaluation is done manually, since gold standards for North S\u00e1mi tokenisation have not been developed prior to this work.", "labels": [], "entities": [{"text": "North S\u00e1mi tokenisation", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.4204188883304596}]}], "datasetContent": [{"text": "In this section we evaluate the previously described modules of the North S\u00e1mi grammar checker.", "labels": [], "entities": [{"text": "North S\u00e1mi grammar checker", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.7963559180498123}]}, {"text": "Firstly, we evaluate the disambiguation of compound errors in terms of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9996857643127441}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.998931348323822}]}, {"text": "Then we compare our system for sentence segmentation with an unsupervised system.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7160578519105911}]}, {"text": "Since a corpus with correctly annotated compound and sentence boundary tokenisation for North S\u00e1mi is not available, all evaluation and annotation is done from scratch.", "labels": [], "entities": []}, {"text": "We use the SIKOR corpus (SIKOR2016)), 12 a descriptive corpus which contains automatic annotations for linguistic research purposes, but no manually checked/verified tags.", "labels": [], "entities": [{"text": "SIKOR corpus (SIKOR2016))", "start_pos": 11, "end_pos": 36, "type": "DATASET", "confidence": 0.896050214767456}]}, {"text": "We selected a random corpus of administrative texts for two reasons.", "labels": [], "entities": []}, {"text": "We had a suspicion that it would have many abbreviations and cases of ambiguous tokenisation.", "labels": [], "entities": []}, {"text": "Secondly, administrative texts stand fora large percentage of the total North S\u00e1mi text body, and the genre is thus important fora substantial group of potential users of our programs.", "labels": [], "entities": [{"text": "North S\u00e1mi text body", "start_pos": 72, "end_pos": 92, "type": "DATASET", "confidence": 0.6849197223782539}]}, {"text": "For the quantitative evaluation of the disambiguation of potential compound errors we calculated both precision (correct fraction of all marked errors) and recall (correct fraction of all errors).", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9996480941772461}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.999049723148346}]}, {"text": "The corpus used contains 340,896 space separated strings, as reported by the Unix tool wc.", "labels": [], "entities": []}, {"text": "The exact number of tokens will vary depending on tokenisation techniques, as described below.", "labels": [], "entities": []}, {"text": "The evaluation is based on lexicalised compounds as potential targets of ambiguous tokenisation.", "labels": [], "entities": []}, {"text": "A previous approach allowed ambiguous tokenisation of dynamic compounds too, solely using syntactic rules to disambiguate.", "labels": [], "entities": []}, {"text": "However, this led to many false positives (which would require more rules to avoid).", "labels": [], "entities": []}, {"text": "Since our lexicon has over 110,000 lexicalised compounds (covering 90.5 % of the compounds in the North S\u00e1mi SIKOR corpus) coverage is acceptable without the riskier dynamic compound support.", "labels": [], "entities": [{"text": "North S\u00e1mi SIKOR corpus", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.7832130938768387}]}, {"text": "13 contains the quantitative results of the compound error evaluation.", "labels": [], "entities": []}, {"text": "Of the 340.895 running bigrams in the text, there were a total of 4.437 potential compound errors, i.e. 1.30 % of running bigrams are analysed as possible compounds by our lexicon.", "labels": [], "entities": []}, {"text": "On manually checking, we found 458 of these to be true compound errors (0.13 % of running bigrams, or 10.3 % of potential compound errors as marked by the lexicon).", "labels": [], "entities": []}, {"text": "So the indicates how well our Constraint Grammar disambiguates compound errors from words that are supposed to be written apart, and tells nothing of the work done by the lexicon in selecting possible compound errors (nor of possible compound errors missed by the lexicon).", "labels": [], "entities": [{"text": "Constraint Grammar disambiguates compound errors from words that are supposed to be written apart", "start_pos": 30, "end_pos": 127, "type": "TASK", "confidence": 0.7484234656606402}]}, {"text": "14 Precision for compound error detection is well above the 67% threshold for any error type in a commercial grammar checker mentioned by Arppe, and the F0.5 (weighting precision twice as much as recall) score is 77.0%, above e.g. Grundkiewicz and Junczys-Dowmunt (2018)'s 72.0%.", "labels": [], "entities": [{"text": "Precision", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9878873825073242}, {"text": "compound error detection", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.69655641913414}, {"text": "F0.5", "start_pos": 153, "end_pos": 157, "type": "METRIC", "confidence": 0.9989331364631653}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.5373309254646301}, {"text": "recall) score", "start_pos": 196, "end_pos": 209, "type": "METRIC", "confidence": 0.978593905766805}]}, {"text": "False positives occur for example in cases where there is an internal syntactic structure such as in the case of ex., where both b\u00e1lvalus 'service' and geavaheddjiide 'user (Ill.", "labels": [], "entities": []}, {"text": "Pl.)' are participants in the sentence's argument structure.", "labels": [], "entities": []}, {"text": "Since there is no finite verb, the syntactic relation could only be identified by defining the valency of b\u00e1lvalus 'service'.) are due to frequent expressions including l\u00e1gan (i.e. ie\u0161gu \u00af detlagan 'different', d\u00e1nl\u00e1gan 'this kind of', etc.), which need to be resolved by means of an idiosyncratic rule.", "labels": [], "entities": []}, {"text": "Dan and ie\u0161gu \u00af det are genitive or attributive pronoun forms and not typically part of a compound, so a syntactic rule only does not resolve the problem.", "labels": [], "entities": []}, {"text": "We have also not calculated the number of actual compounds in the evaluation corpus, so the ratio of compound errors to correct compounds is unknown.", "labels": [], "entities": []}, {"text": "We would like to compare performance on this task with a state-of-the-art machine learning method, but have seen no references for this particular task to use as an unbiased baseline.", "labels": [], "entities": []}, {"text": "However, the gold data set that was developed for evaluating our method is freely available to researchers who would like to experiment with improving on the results.", "labels": [], "entities": []}, {"text": "In ex., there is a compound error.", "labels": [], "entities": [{"text": "compound error", "start_pos": 19, "end_pos": 33, "type": "METRIC", "confidence": 0.9266980588436127}]}, {"text": "However, one of the central rules in the tokeniser disambiguation grammar removes the compound error reading if the first part of the potential compound is in the long genitive case form.", "labels": [], "entities": [{"text": "tokeniser disambiguation grammar", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.8456018368403116}, {"text": "compound error reading", "start_pos": 86, "end_pos": 108, "type": "METRIC", "confidence": 0.6902800798416138}]}, {"text": "However, in this case l\u00e1hka can be both the genitive form of l\u00e1hkka 'lacquer' and the nominative form of l\u00e1hka 'law'.", "labels": [], "entities": []}, {"text": "This unpredictable lexical ambiguity had not been taken into account by the disambiguation rule, which is why there is a false negative.", "labels": [], "entities": []}, {"text": "In the future it can be resolved by referring to the postposition birra 'about', which asks fora preceding genitive.", "labels": [], "entities": []}, {"text": "A common method for splitting sentences in a complete pipeline (used e.g. by LanguageTool) is to tokenise first, then do sentence splitting, followed by other stages of linguistic analysis.", "labels": [], "entities": [{"text": "tokenise", "start_pos": 97, "end_pos": 105, "type": "TASK", "confidence": 0.9624585509300232}, {"text": "sentence splitting", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7104697227478027}]}, {"text": "Here a standalone tokeniser would be used, e.g. PUNKT (), an unsupervised model that uses no linguistic analysis, or GATE 16 which uses regexbased rules.", "labels": [], "entities": []}, {"text": "The Python package SpaCy 17 on the other hand trains a supervised model that predicts sentence boundaries jointly with dependency structure.", "labels": [], "entities": []}, {"text": "Stanford CoreNLP 18 uses finite state automata to tokenise, then does sentence splitting.", "labels": [], "entities": [{"text": "Stanford CoreNLP 18", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.9005377491315206}, {"text": "tokenise", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.9721385836601257}, {"text": "sentence splitting", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.7598834931850433}]}, {"text": "In contrast, our method uses no statistical inference.", "labels": [], "entities": []}, {"text": "We tokenise as the first step, but the tokenisation remains ambiguous until part of the linguistic analysis is complete.", "labels": [], "entities": []}, {"text": "Below, we make a comparison with PUNKT 19 , which, although requiring no labelled training data, has been reported: Sentence segmentation errors per system on 2500 possible sentences.", "labels": [], "entities": [{"text": "PUNKT 19", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.7801309823989868}, {"text": "Sentence segmentation", "start_pos": 116, "end_pos": 137, "type": "TASK", "confidence": 0.8766757249832153}]}, {"text": "287.516 \"words\" (as counted by wc), and manually compared the differences between our system (named divvun below) and PUNKT.", "labels": [], "entities": [{"text": "PUNKT", "start_pos": 118, "end_pos": 123, "type": "DATASET", "confidence": 0.7800092101097107}]}, {"text": "We used a trivial sed script s/[.?:!]", "labels": [], "entities": []}, {"text": "* /&\\n/g to create a \"baseline\" count of possible sentences, and ran the evaluation on the first 2500 potential sentences given by this script (as one big paragraph), counting the places where the systems either split something that should have been one sentence, or treated two sentences as one; see table 2.", "labels": [], "entities": []}, {"text": "Of the differences, we note that PUNKT often treats abbreviations like nr or kap.", "labels": [], "entities": [{"text": "PUNKT", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.6888742446899414}]}, {"text": "as sentence boundaries, even if followed by lower-case words or numbers (st. meld. 15 as three sentences).", "labels": [], "entities": []}, {"text": "Our system sometimes makes this mistake too, but much more rarely.", "labels": [], "entities": []}, {"text": "Also, PUNKT never treats colon as sentence boundaries.", "labels": [], "entities": [{"text": "PUNKT", "start_pos": 6, "end_pos": 11, "type": "DATASET", "confidence": 0.6797910332679749}]}, {"text": "The colon in S\u00e1mi is used for case endings on names, e.g. J\u00f6nk\u00f6ping:s, but of course also as a clause or sentence boundary.", "labels": [], "entities": [{"text": "S\u00e1mi", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.9369258284568787}]}, {"text": "Thus many of the PUNKT errors are simply not marking a colon as a sentence boundary.", "labels": [], "entities": []}, {"text": "On the other hand, our system has some errors where an unknown word led to marking the colon (or period) as a boundary.", "labels": [], "entities": []}, {"text": "This could be fixed in our system with a simple CG rule.", "labels": [], "entities": []}, {"text": "There are also some odd cases of PUNKT not splitting on period even with following space and title cased word, e.g. geavahanguovlluid.", "labels": [], "entities": [{"text": "PUNKT", "start_pos": 33, "end_pos": 38, "type": "TASK", "confidence": 0.537366509437561}]}, {"text": "Where the baseline sed script creates the most sentence boundaries in our evaluation test set (2500), our system creates 2015 sentences, and PUNKT 1971.", "labels": [], "entities": [{"text": "PUNKT 1971", "start_pos": 141, "end_pos": 151, "type": "DATASET", "confidence": 0.858792781829834}]}, {"text": "Our system is able to distinguish sentence boundaries where the user forgot to include a space, e.g. buorrin.Vuoigatvuo \u00af dat is correctly treated as a sentence boundary.", "labels": [], "entities": []}, {"text": "This sort of situation is hard to distinguish in general without a large lexicon.", "labels": [], "entities": []}, {"text": "Our system does make some easily fixable errors, e.g. kap.1 was treated as a sentence boundary due to a wrongly-written CG rule (as such, this evaluation has been helpful in uncovering silly mistakes).", "labels": [], "entities": []}, {"text": "Being a rule-based system, it is easy to support new contexts when required.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Qualitative evaluation of CG compound error  detection", "labels": [], "entities": [{"text": "CG compound error  detection", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.61379574239254}]}, {"text": " Table 2: Sentence segmentation errors per system on  2500 possible sentences.", "labels": [], "entities": [{"text": "Sentence segmentation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9134505391120911}]}]}