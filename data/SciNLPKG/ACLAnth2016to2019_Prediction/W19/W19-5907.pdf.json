{"title": [{"text": "Time Masking: Leveraging Temporal Information in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Time Masking", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7694579660892487}]}], "abstractContent": [{"text": "Ina spoken dialogue system, dialogue state tracker (DST) components track the state of the conversation by updating a distribution of values associated with each of the slots being tracked for the current user turn, using the interactions until then.", "labels": [], "entities": [{"text": "dialogue state tracker (DST)", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.7311850885550181}]}, {"text": "Much of the previous work has relied on modeling the natural order of the conversation, using distance based offsets as an approximation of time.", "labels": [], "entities": []}, {"text": "In this work, we hypothesize that leveraging the wall-clock temporal difference between turns is crucial for finer-grained control of dialogue scenarios.", "labels": [], "entities": []}, {"text": "We develop a novel approach that applies a time mask, based on the wall-clock time difference , to the associated slot embeddings and empirically demonstrate that our proposed approach outperforms existing approaches that leverage distance offsets, on both an internal benchmark dataset as well as DSTC2.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 298, "end_pos": 303, "type": "DATASET", "confidence": 0.9674493074417114}]}], "introductionContent": [{"text": "Modern spoken dialogue systems -such as Intelligent Personal Digital Assistants (IPDAs) like Google Assistant, Siri, and Alexa -provide users a natural language interface to help complete tasks such as reserving restaurants, checking the weather, playing music etc.", "labels": [], "entities": []}, {"text": "Spoken language understanding (SLU) is a central component in such dialogue systems, and is responsible for parsing the natural language text to semantic frames.", "labels": [], "entities": [{"text": "Spoken language understanding (SLU)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8611690799395243}, {"text": "parsing the natural language text to semantic frames", "start_pos": 108, "end_pos": 160, "type": "TASK", "confidence": 0.8083047047257423}]}, {"text": "In task-oriented spoken dialogue systems, a key challenge is tracking entities the user introduced in previous dialogue turns.", "labels": [], "entities": []}, {"text": "For example, if a user request for what's the weather in arlington is followed by how about tomorrow, the dialogue system has to keep track of the entity arlington being referenced.", "labels": [], "entities": []}, {"text": "Typically, this is formulated as a dialogue state tracking (DST) task.", "labels": [], "entities": [{"text": "dialogue state tracking (DST) task", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.8078278686319079}]}, {"text": "Previous approaches to dialogue state tracking have mostly focused on dialogue representations, dealing with noisy input (, or tracking slots from multiple domains.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7890475591023763}]}, {"text": "In this paper, we focus on temporal information associated with each dialogue turn.", "labels": [], "entities": []}, {"text": "Although the dialogue representations -typically encoded using LSTMs -are able to implicitly capture the temporal order in the sequence of dialogue turns, we hypothesize that explicitly and accurately encoding temporal information is essential for resolving ambiguity in dialogue state tracking.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 271, "end_pos": 294, "type": "TASK", "confidence": 0.7184745272000631}]}, {"text": "Recently, ( presented work that models the slot distance offset from the current turn using a one-hot representation input to the DST module.", "labels": [], "entities": []}, {"text": "Alternatively, ( leverage the distance offset in an attention mechanism.", "labels": [], "entities": []}, {"text": "We posit that the notion of time based on distance offset relative to the current turn is too coarse-grained and often insufficient for resolving ambiguities associated with more complex multi-turn dialogues.", "labels": [], "entities": []}, {"text": "For example, in a dialogue \"how far is issaquah?\" followed by \"what is the weather like?\" we could have two possible interpretations -a follow-up utterance issued within 10 seconds would indicate that the user is referring to the city slot of \"Issaquah\" from the previous turn, whereas, if the follow-up utterance is more than 30 seconds apart there is a good chance that the user was just inquiring about the weather in their current location.", "labels": [], "entities": []}, {"text": "In this case, a dialogue system that only encodes the distance offset will be unable to correctly disambiguate the aforementioned situation.", "labels": [], "entities": []}, {"text": "Based on this intuition, we develop a novel approach for incorporating temporal information in dialogue state tracking by using a time mask over the slots.", "labels": [], "entities": [{"text": "dialogue state tracking", "start_pos": 95, "end_pos": 118, "type": "TASK", "confidence": 0.7435082395871481}]}, {"text": "To summarize, we introduce the notion of a time mask to incorporate temporal information into the embedding for slots.", "labels": [], "entities": []}, {"text": "In contrast to previous ap-proaches using distance offsets, we propose leveraging the wall-clock time difference between the current turn and the previous turns in the dialogue to explicitly model temporal information.", "labels": [], "entities": []}, {"text": "Furthermore, we demonstrate how domain and intent information can be mixed in with the temporal information in this framework to improve DST accuracy.", "labels": [], "entities": [{"text": "DST", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9857321381568909}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.8947274088859558}]}, {"text": "We demonstrate empirically that our proposed approach improves over the baseline that only encodes distance offsets as temporal information.", "labels": [], "entities": []}], "datasetContent": [{"text": "We present the results on 2 datasets.", "labels": [], "entities": []}, {"text": "The IPDA dataset, in shows the distribution of time between turns for both datasets.", "labels": [], "entities": [{"text": "IPDA dataset", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.972712904214859}]}, {"text": "If a slot candidate came from a context turn that was spoken 20 seconds before the current turn then d \u2206t = 20.", "labels": [], "entities": []}, {"text": "Based on human judged ground-truth, the slots that should be carried over to the current turn are shown in orange and the slots that should not are shown in blue.", "labels": [], "entities": []}, {"text": "One clear difference between the two distributions in the IPDA dataset is the long tail of the non-carryover distribution, indicating carryovers are more likely from a recent turn.", "labels": [], "entities": [{"text": "IPDA dataset", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9727300405502319}]}, {"text": "The domain specific distributions further indicate that leveraging dialogue time could be useful.", "labels": [], "entities": []}, {"text": "From, we can see that the TDA models offer a slight improvement over the baseline model.", "labels": [], "entities": []}, {"text": "Both models incorporate slot distance offset but the attention mechanism provides an additional boost.", "labels": [], "entities": [{"text": "slot distance offset", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.7859355608622233}]}, {"text": "The time mask models show additional gains demonstrating that leveraging dialogue time from each turn is important.", "labels": [], "entities": []}, {"text": "Moreover, the time information provides complementary information over the distance offset based measure, as shown by the improvements of the time masking models over the baseline model.", "labels": [], "entities": []}, {"text": "The DTM model performs the best overall in terms of F1, which suggests that adding domain information into the time mask provides additional disambiguation power.", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.9997356534004211}]}, {"text": "Interestingly, we see that the ITM model does not improve much over the STM model, possibly because the intent embeddings do not necessarily distinguish between temporal behavior, and are already being leveraged by the slot carryover model.", "labels": [], "entities": []}, {"text": "Since there is only one domain in DSTC2, we chose to only implement the STM model.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9084498882293701}]}, {"text": "From the last column in, we can see that the STM model produces the best result.", "labels": [], "entities": [{"text": "STM", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8561309576034546}]}, {"text": "The TDA model, contrary to previously reported results on DSTC4, does not perform as well.", "labels": [], "entities": [{"text": "DSTC4", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9306620359420776}]}, {"text": "Our hypothesis is that the temporal distribution across turns is not monotonically decaying, which is an assumption made in their approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: IPDA dataset statistics. Here 'positive car- ryover' slots is the number of candidate slots that are  relevant for the current turn.", "labels": [], "entities": [{"text": "IPDA dataset", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.93851438164711}]}, {"text": " Table 2: Overall F1 scores on the IPDA and DSTC2 dataset as well as F1 scores binned by d \u2206t for the IPDA  dataset, which is measured in seconds. Note: the DSTC2 dataset only contains a single domain", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9995507597923279}, {"text": "IPDA", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9694227576255798}, {"text": "DSTC2 dataset", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8893319368362427}, {"text": "F1 scores binned", "start_pos": 69, "end_pos": 85, "type": "METRIC", "confidence": 0.9671568473180135}, {"text": "IPDA  dataset", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9879463315010071}, {"text": "DSTC2 dataset", "start_pos": 157, "end_pos": 170, "type": "DATASET", "confidence": 0.9883923232555389}]}]}