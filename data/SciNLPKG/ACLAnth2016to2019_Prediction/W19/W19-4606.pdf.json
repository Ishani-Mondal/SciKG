{"title": [{"text": "Homograph Disambiguation Through Selective Diacritic Restoration", "labels": [], "entities": [{"text": "Homograph Disambiguation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8885407447814941}]}], "abstractContent": [{"text": "Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic.", "labels": [], "entities": [{"text": "Lexical ambiguity", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8336747288703918}]}, {"text": "Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling.", "labels": [], "entities": []}, {"text": "Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications.", "labels": [], "entities": [{"text": "Diacritic restoration", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6807787120342255}]}, {"text": "In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation.", "labels": [], "entities": [{"text": "diacritic restoration", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8177566230297089}]}, {"text": "Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical dis-ambiguation.", "labels": [], "entities": []}, {"text": "We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation , part-of-speech tagging, and semantic tex-tual similarity.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.6407504777113596}, {"text": "part-of-speech tagging", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.7231128811836243}]}, {"text": "Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical ambiguity, an inherent phenomenon in natural languages, refers to words or phrases that can have multiple meanings.", "labels": [], "entities": [{"text": "Lexical ambiguity", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8040073215961456}]}, {"text": "In written text, lexical ambiguity can be roughly characterized into two categories: polysemy and homonymy.", "labels": [], "entities": []}, {"text": "A polysemous word has multiple senses that express different but related meanings (e.g. 'head' as an anatomical body part, or as a person in charge), whereas homonyms are different words that happen to have the same spelling (e.g. 'bass' as an instrument vs. a fish).", "labels": [], "entities": []}, {"text": "Homographs are words that have the same spelling but may have different pronunciation and meaning.", "labels": [], "entities": []}, {"text": "A diacritic is a mark that is added above, below, or within letters to indicate pronunciation, vowels, or other functions.", "labels": [], "entities": []}, {"text": "For languages that use diacritical marks, such as Arabic or Hebrew, the orthography is typically under-specified for such marks, i.e. the diacritics are omitted.", "labels": [], "entities": []}, {"text": "This phenomenon exacerbates the lexical ambiguity problem since it increases the rate of homographs.", "labels": [], "entities": []}, {"text": "For example, without considering context, the undiacritized Arabic word ktb may refer to any of the following diacritized variants: 1 katab \"wrote\", kutub \"books\", or kutib \"was written\".", "labels": [], "entities": []}, {"text": "As an illustrative analogy in English, dropping vowels in a word such as pan yields the underspecified token pn which can be mapped to pin, pan, pun, pen.", "labels": [], "entities": []}, {"text": "It should be noted that even after fully specifying words with their relevant diacritics, homonyms such as \"bass\" are still ambiguous; likewise in Arabic, the fully-specified word bayot can either mean \"verse\" or \"house\".", "labels": [], "entities": []}, {"text": "In this paper, we devise strategies to automatically identify and disambiguate a subset of homographs that result from omitting diacritics.", "labels": [], "entities": []}, {"text": "While context is often sufficient for determining the meaning of ambiguous words, explicitly restoring missing diacritics should provide valuable additional information for homograph disambiguation.", "labels": [], "entities": [{"text": "homograph disambiguation", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.8393952548503876}]}, {"text": "This process, diacritization, would render the resulting text comparable to that of languages whose words are orthographically fully specified such as English.", "labels": [], "entities": []}, {"text": "Past studies have focused on developing models for automatic diacritic restoration that can be used as a pre-processing step for various applications such as text-to-speech () and reading comprehension (.", "labels": [], "entities": [{"text": "automatic diacritic restoration", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6679655810197195}]}, {"text": "In theory, restoring all diacritics should also help improve the performance of NLP applications such as machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8233340084552765}]}, {"text": "However, in practice, full diacritic restoration results in increased sparsity and out-of-vocabulary words, which leads to degradation in performance (.", "labels": [], "entities": []}, {"text": "The main objective of this work is to find a sweet spot between zero and full diacritization in order to reduce lexical ambiguity without increasing sparsity.", "labels": [], "entities": []}, {"text": "We propose selective diacritization, a process of restoring diacritics to a subset of the words in a sentence sufficient to disambiguate homographs without significantly increasing sparsity.", "labels": [], "entities": []}, {"text": "Selective diacritization can be viewed as a relaxed variant of word sense disambiguation since only homographs that arise from missing diacritics are disambiguated.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7124903897444407}]}, {"text": "Intrinsically evaluating the quality of a devised selective diacritization scheme against a gold set is challenging since it is difficult to obtain a dataset that exhibits consistent selective diacritization with reliable inter-annotator agreement, thereby necessitating an empirical automatic investigation.", "labels": [], "entities": []}, {"text": "Hence, in this work, we evaluate the proposed selective diacritization schemes extrinsically on various semantic and syntactic downstream NLP applications: Semantic Textual Similarity (STS), Neural Machine Translation (NMT), and Part-of-Speech (POS) tagging.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.7400234440962473}, {"text": "Neural Machine Translation (NMT)", "start_pos": 191, "end_pos": 223, "type": "TASK", "confidence": 0.8188208440939585}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 229, "end_pos": 257, "type": "TASK", "confidence": 0.6746441960334778}]}, {"text": "We compare our selective strategies against two baselines full diacritization and zero diacritics applied on all the words in the text.", "labels": [], "entities": []}, {"text": "We use Modern Standard Arabic (MSA) as a case-study.", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA)", "start_pos": 7, "end_pos": 35, "type": "DATASET", "confidence": 0.8667108317216238}]}, {"text": "Our approach is summarized as follows: we start with full diacritic restoration of a large corpus, then apply different unsupervised methods to identify the words that are ambiguous when undiacritized.", "labels": [], "entities": []}, {"text": "This results in a dictionary where each word is assigned an ambiguity label (ambiguous vs. unambiguous).", "labels": [], "entities": []}, {"text": "Selectively-diacritized datasets can then be constructed by restoring the full diacritics only to the words that are identified as ambiguous.", "labels": [], "entities": []}, {"text": "The contribution of this paper is threefold: 1.", "labels": [], "entities": []}, {"text": "We introduce automatic selective diacritization as a viable step in lexical disambiguation and provide an encouraging baseline for future developments towards optimal diacriti-zation.", "labels": [], "entities": [{"text": "lexical disambiguation", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7120667994022369}]}, {"text": "Section 2 describes existing work towards optimal diacritization and how they differ from our approach; 2.", "labels": [], "entities": []}, {"text": "We propose several unsupervised data-driven methods for the automatic identification of ambiguous words; 3.", "labels": [], "entities": [{"text": "automatic identification of ambiguous words", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.7539203524589538}]}, {"text": "We evaluate and analyze the impact of partial sense disambiguation (i.e. selective diacritic restoration of identified homographs) in downstream applications for MSA.", "labels": [], "entities": [{"text": "partial sense disambiguation", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.598604679107666}, {"text": "MSA", "start_pos": 162, "end_pos": 165, "type": "TASK", "confidence": 0.9570464491844177}]}], "datasetContent": [{"text": "Once we have generated the two variants of AmbigDict (AmbigDict-UNDIAC and AmbigDict-DIAC), we evaluate their efficacy extrinsically on downstream applications.", "labels": [], "entities": [{"text": "AmbigDict", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.854829728603363}]}, {"text": "For all downstream applications, training and test data are preprocessed using MADAMIRA () with the FULL-CM diacritization scheme where we only keep lexical diacritics.", "labels": [], "entities": [{"text": "MADAMIRA", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9044256210327148}, {"text": "FULL-CM", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9725993275642395}]}, {"text": "Then the data is filtered based on the AmbigDict of choice; namely, only word tokens in the text deemed ambiguous according to each AmbigDict maintain their full diacritics (as generated by MADAMIRA) while the unambiguous words are kept undiacritized.", "labels": [], "entities": []}, {"text": "Full diacritics are included except inflectional diacritics that reflect the syntactic positions of words within sentences but do not alter meaning.", "labels": [], "entities": []}, {"text": "For MULTI, SENSE, CR, we use a combination of four Modern Standard Arabic (MSA) datasets that vary in genre and domain and add up to \u223c50M tokens: Gigaword 5th edition, distributed by Linguistic Data Consortium (LDC), Wikipedia dump 2016, Corpus of Contemporary Arabic (CCA) (), and LDC Arabic Tree Bank (ATB).", "labels": [], "entities": [{"text": "Modern Standard Arabic (MSA) datasets", "start_pos": 51, "end_pos": 88, "type": "DATASET", "confidence": 0.5804974607058934}, {"text": "Wikipedia dump 2016", "start_pos": 217, "end_pos": 236, "type": "DATASET", "confidence": 0.9519380728403727}, {"text": "LDC Arabic Tree Bank (ATB)", "start_pos": 282, "end_pos": 308, "type": "DATASET", "confidence": 0.8640659877232143}]}, {"text": "For TR, we use an Arabic-English parallel dataset which includes \u223c60M tokens and is created from 53 LDC catalogs.", "labels": [], "entities": [{"text": "TR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9569870829582214}]}, {"text": "For data cleaning, we replace e-mails and URLs with a unified token and use SPLIT tool (Al-Badrashiny et al., 2016) to clean UTF8 characters (e.g. Latin and Chinese), remove diacritics in the original data, and separate punctuation, symbols, and numbers in the text, and replace them with separate unified tokens.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8566157817840576}, {"text": "SPLIT", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8849323987960815}]}, {"text": "We split long sentences (more than 150 words) by punctuation and then remove all sentences that are still longer than 150 words.", "labels": [], "entities": []}, {"text": "We use D3 style (i.e. all affixes are separated) () for Arabic tokenization without normalizing characters.", "labels": [], "entities": []}, {"text": "For English, we lowercase all characters and use TreeTagger (Schmid, 1999) for tokenization.", "labels": [], "entities": []}, {"text": "We used SkipGram word embeddings (, where applicable.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of the proposed approaches using three applications: Semantic Textual Similarity (STS), Neural Machine Translation (NMT), and Part-of-Speech (POS) tagging.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.7373296519120535}, {"text": "Neural Machine Translation (NMT)", "start_pos": 118, "end_pos": 150, "type": "TASK", "confidence": 0.799836794535319}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.6786972165107727}]}, {"text": "We used different significance testing methods appropriate for each application with p = 0.05.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Vocabulary size and percentage of ambiguous  entries in AmbigDict-DIAC and AmbigDict-UNDIAC.", "labels": [], "entities": [{"text": "AmbigDict-DIAC", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.9659432768821716}, {"text": "AmbigDict-UNDIAC", "start_pos": 85, "end_pos": 101, "type": "DATASET", "confidence": 0.945357084274292}]}, {"text": " Table 2:  Performance with selectively-diacritized  datasets in downstream applications. Bold numbers  indicate approaches with higher performance than the  best performing baseline. * refers to approaches with  statistically-significant performance gains against the  best performing baseline.", "labels": [], "entities": []}, {"text": " Table 3:  Performance of selectively-diacritized  datasets on homographs. Bold numbers indicate  approaches with higher performance than the best  performing baseline. * refers to approaches with  statistically-significant performance gains against the  best performing baseline.", "labels": [], "entities": []}, {"text": " Table 4: POS Tagging performance per most frequent  tag. Bold scores indicate the highest score in a column.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6006775498390198}]}]}