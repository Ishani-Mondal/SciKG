{"title": [{"text": "Linguistic Information in Neural Semantic Parsing with Multiple Encoders", "labels": [], "entities": [{"text": "Neural Semantic Parsing", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.6533017953236898}]}], "abstractContent": [{"text": "Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks.", "labels": [], "entities": [{"text": "semantic parsing tasks", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8599785765012106}]}, {"text": "However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further.", "labels": [], "entities": []}, {"text": "Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.688438763221105}]}, {"text": "We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing.", "labels": [], "entities": [{"text": "Discourse Representation Structure Parsing", "start_pos": 93, "end_pos": 135, "type": "TASK", "confidence": 0.7857498079538345}]}, {"text": "We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 59, "end_pos": 82, "type": "TASK", "confidence": 0.6879705190658569}]}], "introductionContent": [{"text": "Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.8639183044433594}]}, {"text": "This architecture is able to learn meaning representations fora range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax.", "labels": [], "entities": []}, {"text": "Though this is an impressive feat in itself, there is no reason to abandon these resources.", "labels": [], "entities": []}, {"text": "Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources) and a multi-source approach has proved particularly successful for adding syntax.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7728807628154755}]}, {"text": "The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in), or use multiple encoders to represent multiple languages rather than linguistic knowledge (.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7081483205159506}]}, {"text": "To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.7424553632736206}]}, {"text": "Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic parsing and (ii) whether it is better to include this linguistic information in the same encoder or in an additional one.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7467717826366425}]}, {"text": "We take as baseline the neural semantic parser for Discourse Representation Structures (DRS,; Van Noord, Abzianidze, Haagsma, and Bos, 2018) developed by Van Noord, Abzianidze, Toral, and . During encoding we add linguistic information in a multi-encoder setup, including various wide-spread automatic linguistic analyses for the input texts, ranging from lemmatisation, POS-tagging, syntactic analysis, to semantic tagging.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 384, "end_pos": 402, "type": "TASK", "confidence": 0.7435876429080963}, {"text": "semantic tagging", "start_pos": 407, "end_pos": 423, "type": "TASK", "confidence": 0.7276401370763779}]}, {"text": "We then empirically determine whether using a multi-encoder setup is preferable over merging all input features in a single encoder.", "labels": [], "entities": []}, {"text": "The insight gained from these experiments will provide suggestions to improve future neural semantic parsing for DRSs and other semantic formalisms.", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.715610682964325}]}], "datasetContent": [{"text": "Produced DRSs are compared with the gold standard representations by using COUNTER (Van Noord, Abzianidze, Haagsma, and . This is a tool that calculates micro precision, recall and F-score over matching clauses, similar to the SMATCH) evaluation tool for AMR parsing.", "labels": [], "entities": [{"text": "COUNTER", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9470481872558594}, {"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.7045987248420715}, {"text": "recall", "start_pos": 170, "end_pos": 176, "type": "METRIC", "confidence": 0.9983402490615845}, {"text": "F-score", "start_pos": 181, "end_pos": 188, "type": "METRIC", "confidence": 0.9945273995399475}, {"text": "AMR parsing", "start_pos": 255, "end_pos": 266, "type": "TASK", "confidence": 0.967102974653244}]}, {"text": "All clauses have the same weight in matching, except for REF clauses, which are ignored.", "labels": [], "entities": [{"text": "REF clauses", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.4945870041847229}]}, {"text": "An example of the matching procedure is shown in.", "labels": [], "entities": []}, {"text": "The produced DRSs go through a strict syntactic and semantic validation process, as described in Van Noord, Abzianidze, Toral, and . If a produced DRS is invalid, it is replaced by a dummy DRS, which gets an F-score of 0.0.", "labels": [], "entities": [{"text": "F-score", "start_pos": 208, "end_pos": 215, "type": "METRIC", "confidence": 0.9986226558685303}]}, {"text": "We check whether two systems differ significantly by performing approximate randomization, with \u03b1 = 0.05, R = 1000 and F (model 1 ) > F (model 2 ) as test statistic for each DRS pair.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Parameter settings for the Marian seq2seq model, found after a search on the development set.  Settings not mentioned are left at default.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9514484405517578}, {"text": "Marian seq2seq model", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.9599189956982931}]}, {"text": " Table 4: Table (a) and (b) show the results of adding a single type of linguistic information. Table (c) and  (d) show the results for stacking multiple types of linguistic information. Reported scores are F-scores  on the development set, averaged over 5 runs of the system, with confidence scores.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9631235599517822}]}, {"text": " Table 5: Results on the test set compared to a number of baseline parsers and the Seq2seq OpenNMT  model of Van Noord, Abzianidze, Toral, and Bos (2018). Our scores are averages of 5 runs, with confi- dence scores.", "labels": [], "entities": []}]}