{"title": [{"text": "On Evaluating Embedding Models for Knowledge Base Completion", "labels": [], "entities": [{"text": "Knowledge Base Completion", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.551126758257548}]}], "abstractContent": [{"text": "Knowledge graph embedding models have recently received significant attention in the literature.", "labels": [], "entities": []}, {"text": "These models learn latent semantic representations for the entities and relations in a given knowledge base; the representations can be used to infer missing knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we study the question of how well recent embedding models perform for the task of knowledge base completion, i.e., the task of inferring new facts from an incomplete knowledge base.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6216333011786143}]}, {"text": "We argue that the entity ranking protocol, which is currently used to evaluate knowledge graph embedding models, is not suitable to answer this question since only a subset of the model predictions are evaluated.", "labels": [], "entities": []}, {"text": "We propose an alternative entity-pair ranking protocol that considers all model predictions as a whole and is thus more suitable to the task.", "labels": [], "entities": []}, {"text": "We conducted an experimental study on standard datasets and found that the performance of popular embeddings models was unsatisfactory under the new protocol, even on datasets that are generally considered to be too easy.", "labels": [], "entities": []}, {"text": "Moreover, we found that a simple rule-based model often provided superior performance.", "labels": [], "entities": []}, {"text": "Our findings suggest that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 135, "end_pos": 160, "type": "TASK", "confidence": 0.6246038277943929}]}], "introductionContent": [{"text": "A knowledge base (KB) is a collection of relational facts, often represented as (subject, relation, object)-triples.", "labels": [], "entities": []}, {"text": "KBs provide rich information for NLP tasks such as question answering () or entity linking.", "labels": [], "entities": [{"text": "question answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8235011398792267}, {"text": "entity linking", "start_pos": 76, "end_pos": 90, "type": "TASK", "confidence": 0.7766218781471252}]}, {"text": "Since knowledge bases are inherently incomplete (), there has been considerable interest into methods that infer missing knowledge.", "labels": [], "entities": []}, {"text": "In particular, a large number of knowledge graph embedding (KGE) models have been proposed in the recent literature ().", "labels": [], "entities": []}, {"text": "These models embed the entities and relations of a given knowledge base into a low-dimensional latent space such that the structure of the knowledge base is captured.", "labels": [], "entities": []}, {"text": "The embeddings are subsequently used to assess whether unobserved triples constitute missing facts or are likely to be false.", "labels": [], "entities": []}, {"text": "To evaluate the performance of a KGE model, the most commonly adopted protocol is the entity ranking (ER) protocol.", "labels": [], "entities": []}, {"text": "The protocol takes as input a set of previously unobserved test triples, such as (Einstein, bornIn, Ulm), and uses the embedding model to rank all possible answers to the questions (?, bornIn, Ulm) and.", "labels": [], "entities": []}, {"text": "Model performance is then assessed based on the rank of the answer present in the test triple.", "labels": [], "entities": []}, {"text": "Since each question is constructed from a test triple, the protocol ensures that questions are meaningful and always have a correct answer.", "labels": [], "entities": []}, {"text": "Throughout this paper, we refer to the task of answering such questions as question answering (QA).", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.8183886885643006}]}, {"text": "The ER protocol is, in principle, well-suited to evaluate performance of KGE models for QA, although concerns about the benchmark datasets, the considered models () and the evaluation () have been raised.", "labels": [], "entities": []}, {"text": "In this paper, we aim to study the performance of popular embedding models for the task of knowledge base completion (KBC): given a relation of a knowledge base (bornIn), infer true missing facts ((Einstein, bornIn, Ulm)).", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8005959391593933}]}, {"text": "This task is different from QA (as defined above) since no information about potential missing triples is provided upfront.", "labels": [], "entities": []}, {"text": "We argue that the ER protocol is not well-suited to assess model performance for KBC.", "labels": [], "entities": []}, {"text": "To see this, observe that models that assign high confidence scores to incorrect triples such as are not penalized by the ER protocol because the corresponding questions (e.g.,) are never asked.", "labels": [], "entities": []}, {"text": "Thus a model that performs well on ER may still not perform well for KBC.", "labels": [], "entities": [{"text": "KBC", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.7399038672447205}]}, {"text": "In fact, we argue here that some commonly used KGE models are inherently not well-suited to KBC.", "labels": [], "entities": []}, {"text": "We propose a simple entity-pair ranking (PR) protocol (PR), which is more suitable to assess model performance for KBC.", "labels": [], "entities": [{"text": "entity-pair ranking (PR) protocol (PR)", "start_pos": 20, "end_pos": 58, "type": "TASK", "confidence": 0.6416432526376512}]}, {"text": "Given a relation such as bornIn, PR uses the KGE model to rank all possible answers-i.e., all entity pairs-to the question, and subsequently assesses model performance based on the rank of the test triples for relation bornIn in the answer.", "labels": [], "entities": []}, {"text": "The protocol ensures that a model's performance is negatively affected if the model assigns high scores to false triples such as.", "labels": [], "entities": []}, {"text": "We conducted an experimental study on commonly used benchmark datasets under the ER and the PR protocols.", "labels": [], "entities": []}, {"text": "We found that the performance of popular embeddings models was often good under the ER but unsatisfactory under the PR protocol, even on \"simple\" datasets that are generally considered to be too easy.", "labels": [], "entities": []}, {"text": "Moreover, we found that a simple rule-based model often provided superior performance for PR.", "labels": [], "entities": [{"text": "PR", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9811174273490906}]}, {"text": "Our findings suggests that there is a need for more research into embedding models as well as their training strategies for the task of knowledge base completion.", "labels": [], "entities": [{"text": "knowledge base completion", "start_pos": 136, "end_pos": 161, "type": "TASK", "confidence": 0.6237288018067678}]}], "datasetContent": [{"text": "We first review two widely used evaluation protocols for QA.", "labels": [], "entities": [{"text": "QA", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9236049652099609}]}, {"text": "We then argue that these protocols are not well-suited for assessing KBC performance, because they focus on a small subset of all possible facts fora given relation.", "labels": [], "entities": [{"text": "KBC", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8127374649047852}]}, {"text": "We then introduce the entity-pair ranking (PR) protocol and discuss its advantages and potential shortcomings.", "labels": [], "entities": [{"text": "entity-pair ranking (PR)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6727367281913758}]}, {"text": "The triple classification (TC) or the entity ranking (ER) protocols are commonly used to assess KGE model performance, where ER is arguably the most widely adopted protocol.", "labels": [], "entities": []}, {"text": "We assume throughout that only true but no false triples are available (as is commonly the case), and that the available true triples are divided into training, validation, and test triples.", "labels": [], "entities": []}, {"text": "Triple classification (TC) The goal of triple classification is to test the model's ability to discriminate between true and false triples).", "labels": [], "entities": [{"text": "Triple classification (TC)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8491509437561036}, {"text": "triple classification", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7360867559909821}]}, {"text": "Since only true triples are available in practice, pseudo-negative triples are generated by randomly replacing either the subject or the object of each test triple by a random entity (that appears as a subject or object in the considered relation).", "labels": [], "entities": []}, {"text": "All triples are then classified as positive or negative according to the KGE scores.", "labels": [], "entities": [{"text": "KGE", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.4732566773891449}]}, {"text": "In particular, triple (i, k, j) is classified as positive if its score s(i, k, j) exceeds a relation-specific decision threshold \u03c4 k (learned on validation data using the same procedure).", "labels": [], "entities": []}, {"text": "Model performance is assessed by classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.7433810830116272}]}, {"text": "Entity ranking (ER) ER assesses model performance by testing its ability to perform QA (as defined before).", "labels": [], "entities": [{"text": "QA", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9699562191963196}]}, {"text": "In particular, for each test triplet = (i, k, j), two questions q s = (?, k, j) and q o = (i, k, ?) are generated.", "labels": [], "entities": []}, {"text": "For question q s , all entities i \u2208 E are ranked based on the score s(i , k, j).", "labels": [], "entities": []}, {"text": "To avoid misleading results, entities i = i that correspond to observed triples in the dataset-i.e., (i , k, j) occurs in the training/validation/test triples-are discarded to obtain a filtered ranking.", "labels": [], "entities": []}, {"text": "The same process is applied for question q o . Model performance is evaluated based on the recorded positions of the test triples in the filtered ranking.", "labels": [], "entities": []}, {"text": "Models that tend to rank test triples (known to be true) higher than unknown triples (assumed to be false) are thus considered superior.", "labels": [], "entities": []}, {"text": "Usually, the micro-average of filtered Hits@K-i.e., the proportion of test triples ranking in the top-K-and filtered MRR-i.e., the mean reciprocal rank of the test triples-are used to assess model performance.", "labels": [], "entities": [{"text": "MRR-i.e.", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.6552866101264954}]}, {"text": "We conducted an experimental study to assess the performance of various bilinear embedding models for KBC.", "labels": [], "entities": []}, {"text": "All datasets, experimental results, and source code are publicly available.", "labels": [], "entities": []}, {"text": "For all models, we performed evaluation under both the ER and PR protocols in order to assess their performance for the QA and KBC tasks, respectively.", "labels": [], "entities": []}, {"text": "We found that many KGE models provided good ER but low PR performance.", "labels": [], "entities": [{"text": "ER", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9944637417793274}, {"text": "PR", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.7375473976135254}]}, {"text": "We also considered a simple rule-based system called, which provided good performance under the ER protocol, and found that RuleN performed better in both ER and PR.", "labels": [], "entities": []}, {"text": "Our results imply that more research into KGE models for KBC is needed.", "labels": [], "entities": []}, {"text": "We also investigated the extent to which PR underestimates model performance due to unobserved true triples.", "labels": [], "entities": [{"text": "PR", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9516241550445557}]}, {"text": "We found that underestimation is not the main reason for the low PR performance of many KGE models; in fact, many KGE models ranked high clearly wrong tuples (e.g., with incorrect types).", "labels": [], "entities": []}, {"text": "We used the four common KBC benchmark datasets: FB15K, WN18, FB-237, and WNRR.", "labels": [], "entities": [{"text": "KBC benchmark datasets", "start_pos": 24, "end_pos": 46, "type": "DATASET", "confidence": 0.8469436168670654}, {"text": "FB15K", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9270201325416565}, {"text": "WN18", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9270932674407959}, {"text": "FB-237", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.7702580690383911}, {"text": "WNRR", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9613881707191467}]}, {"text": "The latter two datasets were created since the former two datasets were considered too easy for embedding models (based on ER).", "labels": [], "entities": []}, {"text": "Key dataset statistics are summarized in.", "labels": [], "entities": []}, {"text": "We trained the embedding models using negative sampling to obtain pseudonegative triples.", "labels": [], "entities": []}, {"text": "We consider three sampling strategies in our experiments: Perturb 1: For each training triplet = (i, k, j), sample pseudo-negative triples by randomly replacing either i or j with a random entity (but such that the resulting triple is unobserved).", "labels": [], "entities": []}, {"text": "This strategy matches ER, which is based on questions (?, k, j) and (i, k, ?).", "labels": [], "entities": [{"text": "ER", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.987346887588501}]}, {"text": "Perturb 1-R: For each training triplet = (i, k, j), sample pseudo-negative triples by randomly replacing either i, k or j.", "labels": [], "entities": []}, {"text": "The generated negative samples are not compared with the training set (.", "labels": [], "entities": []}, {"text": "Perturb 2: For each training triplet = (i, k, j), obtain pseudo-negative triples by randomly sampling unobserved tuples for relation k.", "labels": [], "entities": []}, {"text": "This method appears more suited to PR.", "labels": [], "entities": [{"text": "PR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9871042370796204}]}, {"text": "in C++ using OpenMP.", "labels": [], "entities": []}, {"text": "For RuleN, we use the original implementation provided by the authors.", "labels": [], "entities": [{"text": "RuleN", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7951356768608093}]}, {"text": "The evaluation protocols were written in Python, with Bottleneck 5 used for efficiently obtaining the top-K entries for PR.", "labels": [], "entities": []}, {"text": "We found PR (which took \u224830-90 minutes) was about 3-4 times slower than ER . Hyperparameters.", "labels": [], "entities": [{"text": "PR", "start_pos": 9, "end_pos": 11, "type": "METRIC", "confidence": 0.9950799942016602}, {"text": "ER", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9721336960792542}]}, {"text": "The best hyperparameters were selected based on MRR (for ER) and MAP@100 (for PR) on the validation data.", "labels": [], "entities": [{"text": "MRR", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9888924956321716}, {"text": "MAP@100 (for PR)", "start_pos": 65, "end_pos": 81, "type": "METRIC", "confidence": 0.8242913058825901}]}, {"text": "For both protocols, we performed an exhaustive grid search over the following hyperparameter settings: d e \u2208 {100, 150, 200}, weight of l 2 -regularization \u03bb \u2208 {0.1, 0.01, 0.001}, learning rate \u03b7 \u2208 {0.01, 0.1}, negative sampling strategies Perturb 1, Perturb 2 and Perturb 1-R, and margin hyperparameter \u03b3 \u2208 {0.5, 1, 2, 3, 4} for TransE.", "labels": [], "entities": [{"text": "learning rate \u03b7", "start_pos": 180, "end_pos": 195, "type": "METRIC", "confidence": 0.9042786359786987}, {"text": "margin hyperparameter \u03b3", "start_pos": 282, "end_pos": 305, "type": "METRIC", "confidence": 0.9434419274330139}, {"text": "TransE", "start_pos": 330, "end_pos": 336, "type": "DATASET", "confidence": 0.9200949668884277}]}, {"text": "For each training triple, we sampled 6 pseudonegative triples.", "labels": [], "entities": []}, {"text": "To keep effort tractable, we only used the most frequent relations from each dataset for hyperparameter tuning (top-5, top-5, top-15, and top-30 most frequent relations for WN18, WNRR, FB-237 and FB-15k, respectively).", "labels": [], "entities": [{"text": "WN18", "start_pos": 173, "end_pos": 177, "type": "DATASET", "confidence": 0.9764589071273804}, {"text": "WNRR", "start_pos": 179, "end_pos": 183, "type": "DATASET", "confidence": 0.9078375697135925}, {"text": "FB-237", "start_pos": 185, "end_pos": 191, "type": "DATASET", "confidence": 0.8821136355400085}, {"text": "FB-15k", "start_pos": 196, "end_pos": 202, "type": "DATASET", "confidence": 0.9169855713844299}]}, {"text": "We trained each model for up to 500 epochs during grid search.", "labels": [], "entities": [{"text": "grid search", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.8077476620674133}]}, {"text": "In all cases, we evaluated model performance every 50 epochs and used the overall best-performing model.", "labels": [], "entities": []}, {"text": "For RuleN, we used the best settings reported by the authors for ER).", "labels": [], "entities": [{"text": "RuleN", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7880080938339233}]}, {"text": "For PR, we learned path rules of length 2 using a sampling size of 500 for FB15K and FB-237.", "labels": [], "entities": [{"text": "PR", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9746673703193665}, {"text": "FB15K", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.9172078967094421}, {"text": "FB-237", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.9658346176147461}]}, {"text": "For WN18 and WNRR, we learned path rules of length 3 and sampling size of 500.).", "labels": [], "entities": [{"text": "WN18", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9174675941467285}, {"text": "WNRR", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.913686990737915}]}, {"text": "In particular, although DistMult can only model symmetric relations, and although most relations in these datasets are asymmetric, DistMult has good ER performance.", "labels": [], "entities": []}, {"text": "Likewise, TransE achieved great performance in Hits@10 on all datasets, including WN18 which contains a large number of symmetric relations, which are not easily modeled by TransE.", "labels": [], "entities": [{"text": "WN18", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.9564624428749084}]}], "tableCaptions": [{"text": " Table 2: Results with the entity ranking protocol (ER), which assesses QA performance", "labels": [], "entities": []}, {"text": " Table 3: Results with the entity-pair ranking protocol (PR), which assesses KBC performance", "labels": [], "entities": []}]}