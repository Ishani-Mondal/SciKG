{"title": [], "abstractContent": [{"text": "We present Deep Generalized Canonical Correlation Analysis (DGCCA)-a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other.", "labels": [], "entities": [{"text": "Deep Generalized Canonical Correlation Analysis (DGCCA)-", "start_pos": 11, "end_pos": 67, "type": "TASK", "confidence": 0.7205326147377491}]}, {"text": "While methods for nonlinear two-view representation learning (Deep CCA, (An-drew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many sources, or views.", "labels": [], "entities": []}, {"text": "We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it.", "labels": [], "entities": []}, {"text": "We learn and evaluate DGCCA representations for three downstream tasks: pho-netic transcription from acoustic & articula-tory measurements, recommending hashtags, and recommending friends on a dataset of Twitter users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiview representation learning refers to settings where one has access to many \"views\" of data at train time.", "labels": [], "entities": [{"text": "Multiview representation learning", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8713719248771667}]}, {"text": "Views often correspond to different modalities about examples: a scene represented as a series of audio and image frames, asocial media user characterized by the messages they post and who they friend, or a speech utterance and the configuration of the speaker's tongue.", "labels": [], "entities": []}, {"text": "Multiview techniques learn a representation of data that captures the sources of variation common to all views.", "labels": [], "entities": []}, {"text": "Multiview representation techniques are attractive since a representation that is able to explain many views of the data is more likely to capture meaningful variation than a representation that * Work done while at Johns Hopkins University.", "labels": [], "entities": [{"text": "Multiview representation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8293291628360748}]}, {"text": "\u2020 Now at Google. is a good fit for only one of the views.", "labels": [], "entities": []}, {"text": "These methods are often based on canonical correlation analysis (CCA), a classical statistical technique proposed by.", "labels": [], "entities": [{"text": "canonical correlation analysis (CCA)", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7713610132535299}]}, {"text": "CCA-based techniques cannot currently model nonlinear relationships between arbitrarily many views.", "labels": [], "entities": []}, {"text": "Either they are able to model variation across many views, but can only learn linear mappings to the shared space, or they can learn nonlinear mappings, but they cannot be applied to data with more than two views using existing techniques based on Kernel CCA () and Deep CCA ().", "labels": [], "entities": []}, {"text": "We present Deep Generalized Canonical Correlation Analysis (DGCCA).", "labels": [], "entities": [{"text": "Deep Generalized Canonical Correlation Analysis (DGCCA)", "start_pos": 11, "end_pos": 66, "type": "TASK", "confidence": 0.6985033303499222}]}, {"text": "DGCCA learns a shared representation from data with arbitrarily many views and simultaneously learns nonlinear mappings from each view to this shared space.Our main methodological contribution is the derivation of the gradient update for the Generalized Canonical Correlation Analysis (GCCA) objective.", "labels": [], "entities": [{"text": "Generalized Canonical Correlation Analysis (GCCA) objective", "start_pos": 242, "end_pos": 301, "type": "TASK", "confidence": 0.7183067686855793}]}, {"text": "We evaluate DGCCA-learned representations on three downstream tasks: (1) phonetic transcription from aligned speech & articulatory data, (2) Twitter hashtag and (3) friend recommendation from six text and network feature views.", "labels": [], "entities": [{"text": "phonetic transcription from aligned speech & articulatory", "start_pos": 73, "end_pos": 130, "type": "TASK", "confidence": 0.7589736027376992}]}, {"text": "We find that features learned by DGCCA outperform linear multiview techniques on these tasks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: KNN phoneme classification performance.", "labels": [], "entities": [{"text": "KNN phoneme classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7818175951639811}]}, {"text": " Table 2: Dev/test performance at Twitter friend and  hashtag recommendation tasks.", "labels": [], "entities": []}]}