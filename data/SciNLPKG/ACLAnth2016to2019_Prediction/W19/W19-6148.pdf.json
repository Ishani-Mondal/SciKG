{"title": [{"text": "Docria: Processing and Storing Linguistic Data with Wikipedia", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.8802878260612488}]}], "abstractContent": [{"text": "The availability of user-generated content has increased significantly overtime.", "labels": [], "entities": []}, {"text": "Wikipedia is one example of a corpus , which spans a huge range of topics and is freely available.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9457844495773315}]}, {"text": "Storing and processing such corpora requires flexible document models as they may contain malicious or incorrect data.", "labels": [], "entities": []}, {"text": "Docria is a library which attempts to address this issue with a model using typed property hypergraphs.", "labels": [], "entities": []}, {"text": "Docria can be used with small to large corpora, from laptops using Python interactively in a Jupyter notebook to clusters running map-reduce frameworks with optimized compiled code.", "labels": [], "entities": []}, {"text": "Docria is available as open-source code at https://github.", "labels": [], "entities": [{"text": "Docria", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9014668464660645}]}, {"text": "com/marcusklang/docria.", "labels": [], "entities": []}], "introductionContent": [{"text": "The availability of user-generated content has increased significantly overtime.", "labels": [], "entities": []}, {"text": "Wikipedia is one example of a corpus, which spans a huge range of topics and is freely available.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9457844495773315}]}, {"text": "User-generated content tests the robustness of most tools as it may contain malicious or incorrect data.", "labels": [], "entities": []}, {"text": "In addition, data often comes with valuable metadata, which might be semi-structured and/or incomplete.", "labels": [], "entities": []}, {"text": "These kinds of resources require a flexible and robust data model capable of representing a diverse set of generic and domain-specific linguistic structures.", "labels": [], "entities": []}, {"text": "In this paper, we describe a document model which tries to fill the gap between fully structured and verifiable data models and domain-specific data structures.", "labels": [], "entities": []}, {"text": "This model, called Docria, aims at finding a tradeoff between the rigidity of the former and the specificity of the latter.", "labels": [], "entities": []}, {"text": "To show its merits, we contrast the application of fully structured data models to practical noisy datasets with the simplicity of Docria.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the spaCy library 2 to annotate all the English Wikipedia pages with parts of speech, entities, and dependency graphs, and we made the result available at http://fileadmin.cs. lth.se/papers/nodalida2019/.", "labels": [], "entities": []}, {"text": "On average, each page of the corpus, after annotation, contains 72.2 sentences, 901.8 tokens, 144.8 entities, and 4,383 characters.", "labels": [], "entities": []}, {"text": "We used this annotated corpus to evaluate the technical aspects of Docria and compare them to XML.", "labels": [], "entities": []}, {"text": "We chose XML as it is pervasive in the literature and capable of representing all the structures present in Wikipedia.", "labels": [], "entities": []}, {"text": "We selected FoLiA as the XML format.", "labels": [], "entities": []}, {"text": "FoLiA is well-defined, has good tooling, defines a diverse set of structural annotations which covers most, if not all, aspects of Wikipedia.", "labels": [], "entities": [{"text": "FoLiA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9517720937728882}]}, {"text": "FoLiA also has an official Python library, which we used to read documents.", "labels": [], "entities": [{"text": "FoLiA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9726755619049072}]}, {"text": "Millions of XML files can be stored uncompressed in a file system.", "labels": [], "entities": []}, {"text": "However, this often results in considerable overhead in terms of access times and reading and is therefore not practical for efficient processing.", "labels": [], "entities": []}, {"text": "In addition, XML is verbose and contains redundant information.", "labels": [], "entities": []}, {"text": "All this makes compression and streaming a necessity when storing and processing millions of documents.", "labels": [], "entities": []}, {"text": "To compare FoLiA XML with Docria, we chose to use a sequential tarball format with a bzip2 compression.", "labels": [], "entities": []}, {"text": "We chose this format as it provided the most similar way to store documents in sequence applicable to both FoLiA XML and Docria.", "labels": [], "entities": [{"text": "FoLiA XML", "start_pos": 107, "end_pos": 116, "type": "DATASET", "confidence": 0.8879803419113159}]}, {"text": "We created one XML file per article inmemory and saved them in a sequence using the tarfile API of Python.", "labels": [], "entities": []}, {"text": "The structures we included for the comparison were section, paragraph, entities, tokens with their part of speech and lemma, and dependency relations.", "labels": [], "entities": []}], "tableCaptions": []}