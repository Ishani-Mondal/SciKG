{"title": [{"text": "The University of Helsinki submissions to the WMT19 similar language translation task", "labels": [], "entities": [{"text": "WMT19 similar language translation", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.6415329203009605}]}], "abstractContent": [{"text": "This paper describes the University of Helsinki Language Technology group's participation in the WMT 2019 similar language translation task.", "labels": [], "entities": [{"text": "WMT 2019 similar language translation task", "start_pos": 97, "end_pos": 139, "type": "TASK", "confidence": 0.7676321665445963}]}, {"text": "We trained neural machine translation models for the language pairs Czech \u2194 Polish and Spanish \u2194 Portuguese.", "labels": [], "entities": []}, {"text": "Our experiments focused on different subword segmenta-tion methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmenta-tion and unsupervised segmentation methods for which the data from different languages were simply concatenated.", "labels": [], "entities": []}, {"text": "We did not observe major benefits from cognate-aware seg-mentation methods, but further research maybe needed to explore larger parts of the parameter space.", "labels": [], "entities": []}, {"text": "Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9840047359466553}]}], "introductionContent": [{"text": "Machine translation between closely related languages is, in principle, less challenging than translation between distantly related ones.", "labels": [], "entities": [{"text": "Machine translation between closely related languages", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8724683324495951}]}, {"text": "Sharing large parts of their grammars and vocabularies reduces the amount of effort needed fora machine translation system to be able to generalize (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7275488674640656}]}, {"text": "Nevertheless, and especially since the languages offered in this shared task are to some extent morphologically complex, we assume that proper subword segmentation will be beneficial for neural machine translation (NMT) performance.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.7486819326877594}, {"text": "neural machine translation (NMT)", "start_pos": 187, "end_pos": 219, "type": "TASK", "confidence": 0.8205401102701823}]}, {"text": "In particular, we aim at consistent segmentation across both related languages.", "labels": [], "entities": []}, {"text": "While generic subword segmentation methods such as BPE (, Morfessor (), or SentencePiece ( yield improved consistency by concatenating data from the two languages and training a single segmentation model, the Cognate Morfessor method () explicitly relies on cognate word pairs to enforce consistent segmentation.", "labels": [], "entities": [{"text": "BPE", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.5716928243637085}, {"text": "consistency", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9656774401664734}]}, {"text": "The University of Helsinki participated in the similar language translation task for the language pairs Czech \u2194 Polish and Spanish \u2194 Portuguese, obtaining the following rankings: -third (out of six) on Portuguese \u2192 Spanish, -fourth (out of five) on Spanish \u2192 Portuguese, -third (out of five) on Czech \u2192 Polish, -first (out of two) on Polish \u2192 Czech.", "labels": [], "entities": [{"text": "language translation task", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7757351994514465}]}, {"text": "Section 2 describes the different subword segmentation techniques we considered in our work.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7222610861063004}]}, {"text": "Section 3 details the training data and our preprocessing pipeline, whereas Section 4 presents the models we evaluated and the models we submitted, together with the results.", "labels": [], "entities": []}], "datasetContent": [{"text": "All our NMT models are trained with the same translation toolkit -OpenNMT-py () -, use the same model architecture -the Transformer () -, and the same hyperparameters . Training data are shuffled beforehand.", "labels": [], "entities": [{"text": "OpenNMT-py", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.915231466293335}]}, {"text": "We set a threshold in terms of epochs for each translation direction, after which we stop model training.", "labels": [], "entities": []}, {"text": "This allows us to compare models fairly, as they have all seen the same amount of training data, which is not guaranteed when relying on training time or number of batches.", "labels": [], "entities": []}, {"text": "Results on the development set are shown in and discussed in detail below.", "labels": [], "entities": []}, {"text": "We report two word-level metrics, BLEU () and TER (), as well as two character-level metrics, CharacTer () and chrF.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9982900023460388}, {"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.991323709487915}]}, {"text": "BLEU and chrF are computed with SacreBLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9874638319015503}, {"text": "SacreBLEU", "start_pos": 32, "end_pos": 41, "type": "DATASET", "confidence": 0.9194269776344299}]}, {"text": "In order to quantify the impact of preand post-processing, we compute BLEU scores with the unprocessed reference as well as with an additional reference that has been normalized, order not to impose any prior decision on preprocessing and segmentation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9971705079078674}]}, {"text": "3 tokenized, truecased and de-truecased and detokenized.", "labels": [], "entities": []}, {"text": "Surprisingly, the results with the two references may vary by up to 2 points.", "labels": [], "entities": []}, {"text": "Despite the large amounts of available training data, we chose hyperparameters resulting in rather small vocabulary sizes for all subword splitting schemes, ranging between 2800 and 8900 units per language pair.", "labels": [], "entities": [{"text": "subword splitting", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.8136365115642548}]}, {"text": "This choice was guided by three reasons: (1) the competitive performance of character-level models, (2) the desire to force the models to split words across languages, and to do so not only for rare words, and (3) the competitive performance of small vocabulary sizes in related problems such as historical text normalization (.", "labels": [], "entities": [{"text": "historical text normalization", "start_pos": 296, "end_pos": 325, "type": "TASK", "confidence": 0.6293249726295471}]}, {"text": "A general finding, shared by the other participants, is that the scores on the Slavic language pair are much lower than on the Romance language pair.", "labels": [], "entities": []}, {"text": "We assume that the Spanish-Portuguese development and test sets are built by translating directly from one language to the other, whereas the Czech-Polish development and test sets had been translated from English independently of each other, leading to much freer translations.", "labels": [], "entities": []}, {"text": "If this hypothesis is correct, the automatic evaluation scores for Czech-Polish may in fact underestimate the real translation quality.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Key figures and results of our experiments on the development set. All scores are percentage values. Proc  ref refers to a preprocessed and postprocessed version of the reference. Primary submissions are marked with  * ,  contrastive submissions with  \u2021.", "labels": [], "entities": [{"text": "Proc  ref", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.8923729956150055}]}, {"text": " Table 4: Official results of the submitted systems.  BLEU scores are based on mt-eval-v13b. The Cognate  Morfessor systems are primary submissions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9984412789344788}]}]}