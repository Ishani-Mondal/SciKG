{"title": [{"text": "To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks", "labels": [], "entities": [{"text": "Adapting Pretrained Representations to Diverse Tasks", "start_pos": 24, "end_pos": 76, "type": "TASK", "confidence": 0.7999256650606791}]}], "abstractContent": [{"text": "While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.9663877785205841}]}, {"text": "We focus on the two most common forms of adaptation, feature extraction (where the pre-trained weights are frozen), and directly fine-tuning the pretrained model.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7269267737865448}]}, {"text": "Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7172805517911911}]}, {"text": "We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequential inductive transfer learning consists of two stages: pretraining, in which the model learns a generalpurpose representation of inputs, and adaptation, in which the representation is transferred to anew task.", "labels": [], "entities": [{"text": "Sequential inductive transfer learning", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7673719227313995}]}, {"text": "Most previous work in NLP has focused on pretraining objectives for learning word or sentence representations (.", "labels": [], "entities": [{"text": "learning word or sentence representations", "start_pos": 68, "end_pos": 109, "type": "TASK", "confidence": 0.586773669719696}]}, {"text": "Few works, however, have focused on the adaptation phase.", "labels": [], "entities": [{"text": "adaptation phase", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.905626505613327}]}, {"text": "There are two main paradigms for adaptation: feature extraction and fine-tuning.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.783489853143692}]}, {"text": "In feature extraction ( ) the model's weights are 'frozen' and the pretrained representations are used in a downstream model similar to classic feature-based approaches (.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8147286176681519}]}, {"text": "Alternatively, a pretrained model's parameters can be unfrozen and fine-tuned ( ) on anew task  computationally cheaper as features only need to be computed once.", "labels": [], "entities": []}, {"text": "On the other hand, is convenient as it may allow us to adapt a general-purpose representation to many different tasks.", "labels": [], "entities": []}, {"text": "Gaining a better understanding of the adaptation phase is key in making the most use out of pretrained representations.", "labels": [], "entities": []}, {"text": "To this end, we compare two state-of-the-art pretrained models,) and BERT) using both and across seven diverse tasks including named entity recognition, natural language inference (NLI), and paraphrase detection.", "labels": [], "entities": [{"text": "BERT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.999213695526123}, {"text": "named entity recognition", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6258793373902639}, {"text": "paraphrase detection", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.8987327218055725}]}, {"text": "We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task.", "labels": [], "entities": []}, {"text": "We find that and have comparable performance inmost cases, except when the source and target tasks are either highly similar or highly dissimilar.", "labels": [], "entities": []}, {"text": "We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the NLP practitioner, as summarized in.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare ELMo and BERT as representatives of the two best-performing pretraining settings.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.7149046063423157}, {"text": "BERT", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9971665740013123}]}, {"text": "This section provides an overview of our methods; seethe supplement for full details.", "labels": [], "entities": []}, {"text": "We evaluate on a diverse set of target tasks: named entity recognition (NER), sentiment analysis (SA), and three sentence pair tasks, natural language inference (NLI), paraphrase detection (PD), and semantic textual similarity (STS).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.7867240607738495}, {"text": "sentiment analysis (SA)", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.8469203591346741}, {"text": "paraphrase detection (PD)", "start_pos": 168, "end_pos": 193, "type": "TASK", "confidence": 0.8236681878566742}]}, {"text": "NER We use the, which provides token level annotations of newswire across four different entity types (PER, LOC, ORG, MISC).", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9520690441131592}, {"text": "PER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9324730634689331}, {"text": "ORG", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.7822521328926086}, {"text": "MISC", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.8281351327896118}]}, {"text": "SA We use the binary version of the Stanford Sentiment Treebank (SST-2;, providing sentiment labels (negative or positive) for sentences of movie reviews.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.8733298579851786}]}, {"text": "NLI We use both the broad-domain MultiNLI dataset ( and Sentences Involving Compositional Knowledge (SICK-E;).", "labels": [], "entities": [{"text": "MultiNLI dataset", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8399200439453125}]}, {"text": "PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC;).", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.9026665985584259}, {"text": "Microsoft Research Paraphrase Corpus (MRPC", "start_pos": 105, "end_pos": 147, "type": "DATASET", "confidence": 0.8669922550519308}]}, {"text": "STS We employ the Semantic Textual Similarity Benchmark (STS-B; and SICK-R ().", "labels": [], "entities": []}, {"text": "Both datasets provide a similarity value from 1 to 5 for each sentence pair.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9765574336051941}]}, {"text": "For fair comparison, all experiments include extensive hyper-parameter tuning.", "labels": [], "entities": []}, {"text": "We tuned the learning rate, dropout ratio, weight decay and number of training epochs.", "labels": [], "entities": [{"text": "weight decay", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9299791157245636}]}, {"text": "In addition, the finetuning experiments also examined the impact of triangular learning rate schedules, gradual unfreezing, and discriminative learning rates.", "labels": [], "entities": []}, {"text": "Hyperparameters were tuned on the development sets and the best setting evaluated on the test sets.", "labels": [], "entities": []}, {"text": "All models were optimized with the Adam optimizer () with weight decay fix (.", "labels": [], "entities": [{"text": "weight decay fix", "start_pos": 58, "end_pos": 74, "type": "METRIC", "confidence": 0.9098585446675619}]}, {"text": "We used the publicly available pretrained ELMo 8 and BERT 9 models in all experiments.", "labels": [], "entities": [{"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9911885261535645}]}, {"text": "For ELMo, we used the original two layer bidirectional LM.", "labels": [], "entities": []}, {"text": "In the case of BERT, we used the BERT-base model, a 12 layer bidirectional transformer.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.47830599546432495}, {"text": "BERT-base", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.8897510766983032}]}, {"text": "We used the English uncased model for all tasks except for NER which used the English cased model.", "labels": [], "entities": [{"text": "NER", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.7778942584991455}]}], "tableCaptions": [{"text": " Table 2: Test set performance of feature extraction ( ) and fine-tuning ( ) approaches for ELMo and BERT-base  compared to one sentence embedding method. Settings that are good for  are colored in red (\u2206= -> 1.0);  settings good for  are colored in blue (\u2206= -< -1.0). Numbers for baseline methods are from respective  papers, except for SST-2, MNLI, and STS-B results, which are from", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7225581109523773}, {"text": "BERT-base", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9867933392524719}, {"text": "SST-2", "start_pos": 338, "end_pos": 343, "type": "TASK", "confidence": 0.770548939704895}, {"text": "MNLI", "start_pos": 345, "end_pos": 349, "type": "DATASET", "confidence": 0.9304341077804565}]}, {"text": " Table 3: Comparison of ELMO-cross-sentence em- bedding methods on dev. sets of sentence pair tasks.", "labels": [], "entities": [{"text": "ELMO-cross-sentence em- bedding", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.5697711333632469}]}, {"text": " Table 4: Comparison of BERT-cross-sentence em- bedding methods on dev. sets of sentence pair tasks.", "labels": [], "entities": [{"text": "BERT-cross-sentence em- bedding", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.79161736369133}]}, {"text": " Table 4. The latter reduces performance,  which shows that BERT representations encode  cross-sentence relationships and are therefore par- ticularly well-suited for sentence pair tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.8366520404815674}]}, {"text": " Table 5. We  find that additional parameters are key for , but  hurt performance with .", "labels": [], "entities": []}, {"text": " Table 5: Comparison of CoNLL 2003 NER develop- ment set performance (F 1 ) for ELMo for both feature  extraction and fine-tuning. All results averaged over  five random seeds.", "labels": [], "entities": [{"text": "CoNLL 2003 NER develop- ment set performance (F 1 )", "start_pos": 24, "end_pos": 75, "type": "METRIC", "confidence": 0.789906178911527}, {"text": "feature  extraction", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7057534605264664}]}, {"text": " Table 6: Accuracy of feature extraction ( ) and dif- ference compared to fine-tuning ( ) with BERT-base  trained on training data of different MNLI domains and  evaluated on corresponding dev sets. TE: telephone. FI:  fiction. TR: travel. GO: government. SL: slate.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9939444661140442}, {"text": "feature extraction", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.6792808324098587}, {"text": "dif- ference", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.7259580294291178}, {"text": "BERT-base", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9986640214920044}, {"text": "MNLI", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.9041168093681335}]}]}