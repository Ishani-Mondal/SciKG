{"title": [], "abstractContent": [{"text": "This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source.", "labels": [], "entities": [{"text": "open domain factual Arabic question answering (QA)", "start_pos": 34, "end_pos": 84, "type": "TASK", "confidence": 0.7543753352430131}]}, {"text": "This constrains the answer of any question to be a span of text in Wikipedia.", "labels": [], "entities": []}, {"text": "Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7122434079647064}]}, {"text": "To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD).", "labels": [], "entities": [{"text": "Arabic Reading Comprehension Dataset (ARCD)", "start_pos": 59, "end_pos": 102, "type": "DATASET", "confidence": 0.6990617173058646}, {"text": "Stanford Question Answering Dataset (Arabic-SQuAD)", "start_pos": 209, "end_pos": 259, "type": "DATASET", "confidence": 0.8399091022355216}]}, {"text": "Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT.", "labels": [], "entities": [{"text": "open domain question answering in Arabic (SOQAL)", "start_pos": 15, "end_pos": 63, "type": "TASK", "confidence": 0.764908469385571}, {"text": "document retriever", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7093654721975327}, {"text": "BERT", "start_pos": 248, "end_pos": 252, "type": "METRIC", "confidence": 0.9743673205375671}]}, {"text": "Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.", "labels": [], "entities": [{"text": "ARCD", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.6916525959968567}, {"text": "BERT-based", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9701598286628723}, {"text": "F1 score", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9790581464767456}, {"text": "F1 score", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9779215753078461}]}], "introductionContent": [{"text": "One of the goals in artificial intelligence (AI) is to build automated systems that can perform opendomain question answering (QA) through understanding natural language and gathering knowledge ().", "labels": [], "entities": [{"text": "opendomain question answering (QA)", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.792758584022522}]}, {"text": "The driver behind progress in English QA has been the release of massive datasets including the Stanford Question Answering Dataset (SQuAD), WikiQA).", "labels": [], "entities": [{"text": "English QA", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.5119052231311798}, {"text": "Stanford Question Answering Dataset (SQuAD)", "start_pos": 96, "end_pos": 139, "type": "DATASET", "confidence": 0.8532232471874782}, {"text": "WikiQA", "start_pos": 141, "end_pos": 147, "type": "DATASET", "confidence": 0.8463449478149414}]}, {"text": "The task in these datasets is to find the span of text in a document that answers a given question.", "labels": [], "entities": []}, {"text": "On the other hand, progress in Arabic QA systems has lagged behind their English counterparts.", "labels": [], "entities": []}, {"text": "While there has been a good body of work on methods for question ,: Example data point from ARCD containing a paragraph with two accompanying questions answering, they mostly have a common limitation of being tested on small amounts of data and relying on classical methods.", "labels": [], "entities": [{"text": "ARCD", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.8307443261146545}]}, {"text": "In this work, we tackle the problem of answering Arabic open-domain factual questions using Arabic Wikipedia as our knowledge source.", "labels": [], "entities": [{"text": "answering Arabic open-domain factual questions", "start_pos": 39, "end_pos": 85, "type": "TASK", "confidence": 0.8788221836090088}]}, {"text": "The open-domain setting poses many challenges, from efficient large scale information retrieval, to highly accurate answer extraction modules, and this requires a sizable amount of data for training and testing.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7336971461772919}, {"text": "answer extraction", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.8574922382831573}]}, {"text": "First, to deal with the need of large Arabic reading comprehension datasets, we develop the following: (1) The Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 crowdsourced questions with accompanying text seg-ments on Arabic Wikipedia as seen in, and (2) Arabic-SQuAD consisting of 48k paragraphquestion-answer machine translated tuples from the SQuAD dataset.", "labels": [], "entities": [{"text": "Arabic Reading Comprehension Dataset (ARCD)", "start_pos": 111, "end_pos": 154, "type": "DATASET", "confidence": 0.7050060331821442}, {"text": "SQuAD dataset", "start_pos": 360, "end_pos": 373, "type": "DATASET", "confidence": 0.7258477956056595}]}, {"text": "Second, modern open-domain QA systems are generally composed of two parts: a retriever that obtains relevant segments of text, and a machine reading comprehension (MRC) model that extracts the answer from the text (.", "labels": [], "entities": []}, {"text": "For our retriever, we propose the use of a hierarchical TF-IDF retriever that is efficiently able to trade off between n-gram features and the number of documents retrieved.", "labels": [], "entities": [{"text": "retriever", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9590250253677368}, {"text": "TF-IDF retriever", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.5975832790136337}]}, {"text": "We chose raw Wikipedia text as our information source instead of knowledge bases () which are commonly used for open-ended QA as it enables our approach to tackle other domains and settings with little adaptation.", "labels": [], "entities": []}, {"text": "Now there has been remarkable progress in designing neural MRC models that read and extract answers from short paragraphs; we selected two of the best performing models on the SQuAD dataset ( as our document readers.", "labels": [], "entities": [{"text": "SQuAD dataset", "start_pos": 176, "end_pos": 189, "type": "DATASET", "confidence": 0.9137813448905945}]}, {"text": "The first is QANet (, an efficient convolution and selfattention-based neural network, and the second is BERT (), a transformer-based pre-trained model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9949182868003845}]}, {"text": "From the document retriever and reader we build an open domain QA system named SOQAL by combining confidence scores from each.", "labels": [], "entities": [{"text": "document retriever", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.661010205745697}]}, {"text": "We evaluated our system components on the crowdsoured ARCD dataset: Our hierarchical TF-IDF retriever is competitive with Google Search, and our BERT reader is the current state-of-the-art for reading comprehension.", "labels": [], "entities": [{"text": "ARCD dataset", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9031879305839539}, {"text": "TF-IDF retriever", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.6320053189992905}, {"text": "BERT", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9866932034492493}]}, {"text": "Finally, our open domain system SOQAL achieves a respectable 27.6 F1 on ARCD.", "labels": [], "entities": [{"text": "F1", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9834051132202148}, {"text": "ARCD", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.8875052332878113}]}, {"text": "To summarize, the contributions of the paper are: \u2022  All the data and system implementation is available at https://github.com/ husseinmozannar/SOQAL.", "labels": [], "entities": []}], "datasetContent": [{"text": "To properly evaluate our system, we must have questions written by proficient Arabic speakers, and thus we resort to crowdsourcing to develop our dataset.", "labels": [], "entities": []}, {"text": "Each task presented to the crowdworkers consists of five articles taken from Arabic Wikipedia, from which we extracted the first three paragraphs with a length greater than 250 characters.", "labels": [], "entities": []}, {"text": "The worker has to write three question-answer pairs for each paragraph in clear Modern Standard Arabic, where the answer to each question should bean exact span of text from the paragraph.", "labels": [], "entities": []}, {"text": "The interface, shown in figure 2, consists of a paragraph along with two text boxes for each of the 3 question-answer pairs.", "labels": [], "entities": []}, {"text": "Pasting is disabled in the question fields in order to encourage workers to use their own words, but it is enforced in the answer fields to guarantee that the answer is taken as-is from the paragraph.", "labels": [], "entities": []}, {"text": "Before workers begin the task, they have to answer a reading comprehension question from a test set we created to make sure of their language proficiency.", "labels": [], "entities": []}, {"text": "Only workers who succeeded in the test were accepted.", "labels": [], "entities": []}, {"text": "The articles presented in the tasks were 155 articles randomly sampled from the 1000 most viewed articles on Wikipedia in 2018.", "labels": [], "entities": []}, {"text": "We used MediaWiki's API 1 to retrieve the most viewed articles per month in 2018 for Arabic Wikipedia and aggregated the results.", "labels": [], "entities": []}, {"text": "The articles covered a diverse set of topics including religious and historical figures, sports celebrities, countries, and companies.", "labels": [], "entities": []}, {"text": "We additionally manually filtered out adult content.", "labels": [], "entities": []}, {"text": "We resorted to Amazon Mechanical Turk for crowdsourcing.", "labels": [], "entities": []}, {"text": "Crowdworkers were required to have a minimum HIT acceptance of 97%, and at least 100 HITs submitted.", "labels": [], "entities": [{"text": "HIT", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8736487030982971}, {"text": "acceptance", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.5795505046844482}]}, {"text": "Moreover, our task description highlighted the need for good Arabic skills.", "labels": [], "entities": []}, {"text": "Workers were advised to spend 3 to 4 minutes per paragraph and were paid close to 10 USD per hour.", "labels": [], "entities": []}, {"text": "They were encouraged to ask difficult questions framed in such away that they can be answered outside the scope of the paragraph.", "labels": [], "entities": []}, {"text": "In total, we collected 1,395 questions based on 465 paragraphs from 155 articles based on the Amazon Turk HITs.", "labels": [], "entities": [{"text": "Amazon Turk HITs", "start_pos": 94, "end_pos": 110, "type": "DATASET", "confidence": 0.935825248559316}]}, {"text": "We now showcase experiments for every component in our system and the end-to-end open domain system.", "labels": [], "entities": []}, {"text": "Arabic-SQuAD is split 80-10-10% into three parts for training, development and testing: Arabic-SQuad-Test is composed of 2,966 questions on 24 articles; note that articles are distinct between the parts.", "labels": [], "entities": []}, {"text": "Similarly, ARCD is split 50-50 into training and testing with ARCD-Test having 702 questions on 78 articles.", "labels": [], "entities": [{"text": "ARCD", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.7661979794502258}, {"text": "ARCD-Test", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.6229318380355835}]}, {"text": "We examine the performance of our different retriever modules on the full ARCD dataset.", "labels": [], "entities": [{"text": "ARCD dataset", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9623396992683411}]}, {"text": "To compare the approaches we assign to each the ratio of questions for which the answer appears in any of the retrieved document over the total number of questions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Comparison of the different retrievers on  ARCD. k: number of documents retrieved", "labels": [], "entities": [{"text": "ARCD", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9143984317779541}]}, {"text": " Table 5: Comparison of the different document reader modules on Arabic-SQuAD test set and all of ARCD.  QANet and BERT were trained only on the training set of Arabic-SQuAD.", "labels": [], "entities": [{"text": "Arabic-SQuAD test set", "start_pos": 65, "end_pos": 86, "type": "DATASET", "confidence": 0.870111882686615}, {"text": "ARCD", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.652084469795227}, {"text": "BERT", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.996368408203125}]}]}