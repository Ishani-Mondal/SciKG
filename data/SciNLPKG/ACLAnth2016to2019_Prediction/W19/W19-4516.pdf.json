{"title": [], "abstractContent": [{"text": "We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., \"Python has better NLP libraries than MATLAB\" \u2192 Python, better, MATLAB).", "labels": [], "entities": []}, {"text": "To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27% of the sentences contain an oriented comparison in the sense of \"better\" or \"worse\").", "labels": [], "entities": []}, {"text": "A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85% in our experimental evaluation.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9858457148075104}]}, {"text": "The model can be used to extract comparative sentences for pro/con argumentation in comparative / argument search engines or debating technologies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Everyone faces choice problems on a daily basis: from choosing between products (e.g., which camera to buy), to more generic preferences for all kinds of things: cities to visit, universities to study at, or even programming languages to use.", "labels": [], "entities": []}, {"text": "Informed choices need to be based on a comparison and objective argumentation to favor one of the candidates.", "labels": [], "entities": []}, {"text": "Often, people seek support from other people-for instance, a lot of questions like \"How does X compare to Y?\" are asked on question answering platforms.", "labels": [], "entities": [{"text": "question answering", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7358464002609253}]}, {"text": "The Web also contains pages about comparing various objects: Specialized web resources systematize human experts results for domainspecific comparisons (for insurances, cameras, restaurants, hotels, etc.) while systems like WolframAlpha aim at providing comparative functionality across domains.", "labels": [], "entities": [{"text": "WolframAlpha", "start_pos": 224, "end_pos": 236, "type": "DATASET", "confidence": 0.9466444253921509}]}, {"text": "Still, such pages and systems usually suffer from coverage issues relying on structured databases as the only source of information ignoring the rich textual content available on the web.", "labels": [], "entities": []}, {"text": "No system is currently able to satisfy opendomain comparative information needs with sufficient coverage and explanations of the compared items' relative qualities.", "labels": [], "entities": []}, {"text": "Indeed, information retrieval systems and web search engines are able to directly answer many factoid questions (oneboxes, direct answers, etc.) but do not yet treat comparative information needs any different than standard queries.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.7147634327411652}]}, {"text": "Search engines show the default \"ten blue links\" for many comparative information needs even though a direct answer enriched by pro/cons for the different options might be the much more helpful result.", "labels": [], "entities": []}, {"text": "One reason might be that despite the wealth of comparisons on the web with argumentative explanations, there is still no widespread technology for its extraction.", "labels": [], "entities": []}, {"text": "In this work, we propose the first steps towards closing this gap by proposing classifiers to identify and to categorize comparative sentences.", "labels": [], "entities": []}, {"text": "The task of identifying and categorizing comparative sentences is to decide fora given sentence whether it compares at least two items and, if so, which item \"wins\" the comparison.", "labels": [], "entities": []}, {"text": "For instance, given the sentence Python is better suited for data analysis than MATLAB due to the many available deep learning libraries, the system should categorize it as comparative and that it favors Python (Python \"wins\" over MATLAB).", "labels": [], "entities": [{"text": "data analysis", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.727671891450882}]}, {"text": "Identifying and categorizing comparative sentences can be viewed as a sub-task of argumentation mining) in the sense that detected comparative sentences (and probably also their context sentences) can support pro/con analyses for two or more items.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.7977788746356964}]}, {"text": "Such comparative pro/cons might be used to trigger reactions in debates (one advantage of some item can be countered by some advantage of the other item, etc.) or they can form the basis for answering comparative information needs submitted to argument search engines.", "labels": [], "entities": []}, {"text": "Our main contributions are two-fold: 1.", "labels": [], "entities": []}, {"text": "We release CompSent-19, anew corpus consisting of 7,199 sentences containing item pairs (27% of the sentences are tagged as comparative and annotated with a preference); 2.", "labels": [], "entities": []}, {"text": "We present an experimental study of supervised classifiers and a strong rule-based baseline from prior work.", "labels": [], "entities": []}, {"text": "The new CompSent-19 corpus, 1 pre-trained sentence categorization models, and our source codes 2 are publicly available online.", "labels": [], "entities": [{"text": "CompSent-19 corpus", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.8942465484142303}]}], "datasetContent": [{"text": "As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or \"wins\") / WORSE (the first item is worse or \"looses\") or NONE (the sentence does not contain a comparison of the target items).", "labels": [], "entities": [{"text": "comparative argument mining", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.86884073416392}, {"text": "BETTER", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9983123540878296}, {"text": "WORSE", "start_pos": 198, "end_pos": 203, "type": "METRIC", "confidence": 0.9591770768165588}, {"text": "NONE", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.9818914532661438}]}, {"text": "The BETTERsentences represent a pro argument in favor of the first compared item (or a con argument for the sec-ond item) while the roles are exchanged for the WORSE-sentences.", "labels": [], "entities": [{"text": "BETTERsentences", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9977536797523499}]}, {"text": "In our dataset, we aim to minimize domainspecific biases to rather capture the nature of comparison and not the nature of particular domains.", "labels": [], "entities": []}, {"text": "We thus decided to control the specificity of domains via the selection of the comparison targets.", "labels": [], "entities": []}, {"text": "We hypothesized and could confirm in preliminary experiments that comparison targets usually have a common hypernym (i.e., they are instances of the same class), which we utilize for the selection of the compared item pairs.", "labels": [], "entities": []}, {"text": "The most specific domain we choose is Computer Science with comparison targets like programming languages, database products and technology standards such as Bluetooth or Ethernet.", "labels": [], "entities": []}, {"text": "Many computer science concepts can be compared objectively (e.g., via transmission speed or suitability for certain applications).", "labels": [], "entities": []}, {"text": "The comparison targets were manually extracted from Wikipedia \"List of\"-articles that cover computer science.", "labels": [], "entities": [{"text": "Wikipedia \"List of\"-articles", "start_pos": 52, "end_pos": 80, "type": "DATASET", "confidence": 0.9190156360467275}]}, {"text": "In the annotation process, annotators were asked to label sentences from this domain only if they had some basic knowledge in computer science.", "labels": [], "entities": []}, {"text": "The second, broader domain is Brands.", "labels": [], "entities": []}, {"text": "It contains items of various types (e.g., cars, electronics, or food).", "labels": [], "entities": []}, {"text": "As brands are present in everyday life, we assume basically anyone to be able to label sentences containing well-known brands such as Coca-Cola or Mercedes.", "labels": [], "entities": []}, {"text": "Again, target items for this domain were manually extracted from Wikipedia \"List of\"-articles.", "labels": [], "entities": [{"text": "Wikipedia \"List of\"-articles", "start_pos": 65, "end_pos": 93, "type": "DATASET", "confidence": 0.900280108054479}]}, {"text": "The third Random domain is not restricted to any topic.", "labels": [], "entities": []}, {"text": "For each of 24 randomly selected seed words, 3 10 similar words were collected based on the distributional similarity JoBimText API.", "labels": [], "entities": []}, {"text": "Especially for brands and computer science, the resulting item lists were large (4,493 in brands and 1,339 in computer science).", "labels": [], "entities": []}, {"text": "Ina manual inspection, low-frequency and ambiguous items were removed (e.g., the computer science concepts \"RAID\" (a hardware concept) and \"Unity\" (a game engine) are also regularly used nouns).", "labels": [], "entities": []}, {"text": "The remaining items were combined into pairs.", "labels": [], "entities": []}, {"text": "For each item type (seed Wikipedia list or seed word), all possible item combinations were created.", "labels": [], "entities": []}, {"text": "These pairs were then used to mine sentences containing both items from a web-scale corpus.", "labels": [], "entities": []}, {"text": "Our sentence source is the publicly available index of the DepCC (), an index of more then 14 billion dependency-parsed English sentences from the Common Crawl filtered for duplicates.", "labels": [], "entities": [{"text": "DepCC", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.9273567795753479}]}, {"text": "This index was queried for sentences containing both items in each target pair.", "labels": [], "entities": []}, {"text": "For 90% of the pairs, we also added frequent comparative cue words 4 to the query in order to bias the results towards actual comparative sentences but at the same time also allow for comparisons that do not contain any of the anticipated cues.", "labels": [], "entities": []}, {"text": "This focused querying was necessary as a random sampling would have resulted in only a very tiny fraction of comparative sentences.", "labels": [], "entities": []}, {"text": "Note that even sentences containing a cue word do not necessarily express a comparison between the desired targets (e.g., dog vs. cat: He's the best pet that you can get, better than a dog or cat).", "labels": [], "entities": []}, {"text": "It is thus especially crucial to enable a classifier to learn not to rely on the presence of the cue words only (which is very likely in a random sample of sentences with very few comparisons).", "labels": [], "entities": []}, {"text": "For our dataset, we keep target pairs with at least 100 retrieved sentences.", "labels": [], "entities": []}, {"text": "From all sentences for the target pairs, we randomly sampled 2,500 instances in each category as potential candidates fora crowd-sourced annotation that we conducted on the Figure Eight platform in several small batches.", "labels": [], "entities": []}, {"text": "Each sentence was annotated by at least five trusted workers.", "labels": [], "entities": []}, {"text": "Of all annotated sentences, 71% received unanimous votes, and at least 4 out of 5 workers agreed for over 85%, at least 4 out of 5 workers agreed.", "labels": [], "entities": []}, {"text": "Our final Comparative Sentences Corpus 2019 is formed by the 7,199 sentences for 271 distinct item pairs that remained after removing the 301 sentences with an annotation confidence below 50%, a Figure-Eight-internal measure combining annotator trust and voting.", "labels": [], "entities": [{"text": "Comparative Sentences Corpus 2019", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.7276744097471237}]}, {"text": "Table 1 shows example sentences with their annotation while outlines the corpus characteristics.", "labels": [], "entities": []}, {"text": "Only a 27%-minority of the sentences are annotated as comparative (despite the selection bias with comparative cue words); in 70% of these, the favored item is named first.: Examples sentences for the three domains with their annotated comparative label (the first item is BETTER/WORSE/NONE than the second item (note that the item order matters).", "labels": [], "entities": [{"text": "BETTER/WORSE/NONE", "start_pos": 273, "end_pos": 290, "type": "METRIC", "confidence": 0.7703567862510681}]}, {"text": "We conduct classification experiments using several machine learning approaches and representations and analyse the results.", "labels": [], "entities": []}, {"text": "We use common performance metrics: precision, recall and F1 per each class and micro-averaged when reporting overall results.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9996531009674072}, {"text": "recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9989821314811707}, {"text": "F1", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.9991175532341003}]}, {"text": "Similarly, we applied the rule-based baseline to three domains independently and obtained F1 of 0.80 for CompSci, 0.81 for Brands and 0.84 for Random domains.", "labels": [], "entities": [{"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9996687173843384}]}], "tableCaptions": [{"text": " Table 2: Characteristics of our CompSent-19 dataset.", "labels": [], "entities": [{"text": "CompSent-19 dataset", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.8889169692993164}]}, {"text": " Table 3: Performance (F1) of the best classifier-based  model compared to the rule-based baseline.", "labels": [], "entities": [{"text": "F1)", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9824645817279816}]}, {"text": " Table 4: Cross-domain evaluation in terms of total F1  for all classes (best results per row in bold).", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.994522213935852}]}, {"text": " Table 5: Examples of XGBoost errors with the InferSent features. Confidence shows the confidence of the anno- tators and is calculated as (judgments for majority class) / (total judgments).", "labels": [], "entities": [{"text": "Confidence", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9894464612007141}]}]}