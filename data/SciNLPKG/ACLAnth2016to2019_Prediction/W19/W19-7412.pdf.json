{"title": [], "abstractContent": [], "introductionContent": [{"text": "Authorship Verification (AV) is a type of authorship analysis task where a document of questionable attribution is judged as to whether it is written by a certain author, given a number of documents known to be written by that author.", "labels": [], "entities": [{"text": "Authorship Verification (AV)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7843329787254334}, {"text": "authorship analysis task", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7641907334327698}]}, {"text": "AV tasks are often compared to Authorship Attribution (AA) tasks, where a document of unknown attribution is attributed to one of a number of candidate authors.", "labels": [], "entities": [{"text": "Authorship Attribution (AA) tasks", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.7299332370360693}]}, {"text": "AV has a number of applications in forensic linguistics and literary studies in areas where an AA task cannot answer the problem at hand.", "labels": [], "entities": [{"text": "AV", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.4588363766670227}, {"text": "forensic linguistics", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7943057417869568}]}, {"text": "For example, while an AA task is appropriate in some cases of plagiarism detection, an AV task can better suite a situation where the text is not written by any of the candidate authors, or when there is only one candidate author.", "labels": [], "entities": [{"text": "AA", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.8964147567749023}, {"text": "plagiarism detection", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.8139902353286743}]}, {"text": "This paper examines the effect of small sample size on the accuracy of AV tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9988960027694702}, {"text": "AV tasks", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.8425733149051666}]}, {"text": "Specifically, it addresses the following question: is it possible to use small testing and training datasets without significant accuracy sacrifices in an Arabic AV task?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9920065999031067}]}, {"text": "Recent developments in AV (and AA) have achieved high rates of accuracy using various Machine Learning (ML) techniques and feature configurations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9964364767074585}]}, {"text": "Current research (c.f. section 2) achieves accurate AV results using relatively large training and testing corpora.", "labels": [], "entities": [{"text": "accurate", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9416851997375488}, {"text": "AV", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.3596806824207306}]}, {"text": "A smaller training and/or testing set is, of course, advantageous.", "labels": [], "entities": []}, {"text": "For one thing, a smaller data size allows for more efficient processing.", "labels": [], "entities": []}, {"text": "For another, in real-life situations, there may not be plenty of large texts available for the AV task.", "labels": [], "entities": [{"text": "AV task", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.8602795302867889}]}, {"text": "Either the question document or the authentic corpus could be of small size.", "labels": [], "entities": []}, {"text": "Ina situation specific to Arabic literary studies, a great deal of documents is only available in non-machine readable format, and in typeface that does not allow for efficient OCR.", "labels": [], "entities": []}, {"text": "Digitizing large texts for the purpose of automatic AV is, then, an unduly expensive procedure.", "labels": [], "entities": [{"text": "Digitizing large texts", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8855564991633097}, {"text": "AV", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.7462292909622192}]}, {"text": "In this paper I examine the effects of using a small corpus for training or testing documents on the accuracy of predicting AV in different domains in Modern Standard Arabic (MSA) through a number of AV experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9953635931015015}, {"text": "predicting AV", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.8153404891490936}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 151, "end_pos": 179, "type": "DATASET", "confidence": 0.755496064821879}]}, {"text": "This paper is organized as follows: section 2 outlines a brief review of literature on AV, Arabic AV and AA, and how sample size is handled in the relevant literature.", "labels": [], "entities": [{"text": "AA", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.8233582973480225}]}, {"text": "Section 3 describes the corpus and features used in the experiments.", "labels": [], "entities": []}, {"text": "Section 4 describes the verification method.", "labels": [], "entities": []}, {"text": "Section 5 describes the procedure of the two experiments conducted.", "labels": [], "entities": []}, {"text": "Section 6 outlines the results and I discuss their implications in 7.", "labels": [], "entities": []}, {"text": "Section 8 is the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each domain, input to the training phase is a set of strings representing the feature in question known to be attributed to a given author.", "labels": [], "entities": []}, {"text": "N-grams of appropriate value for n are generated using NLTK (, and relative (normalized) frequencies of the features described in the section  Feature Selection are calculated, also using NLTK.", "labels": [], "entities": [{"text": "NLTK", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9210907220840454}, {"text": "NLTK", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.9444599151611328}]}, {"text": "Output of the training phase is a similarity value threshold for an authentic document.", "labels": [], "entities": [{"text": "similarity value threshold", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.8865020076433817}]}, {"text": "Similarity is calculated using Manhattan Distance between a document X and a corpus of known documents Y: Similarity Threshold \u00ed \u00b5\u00ed\u00bc\u0083 is calculated by determining Sim for each document in the training set in relation to the reset of the training documents, creating a confidence interval for all the training documents.", "labels": [], "entities": [{"text": "Manhattan Distance", "start_pos": 31, "end_pos": 49, "type": "METRIC", "confidence": 0.8509284853935242}, {"text": "Threshold \u00ed \u00b5\u00ed\u00bc\u0083", "start_pos": 117, "end_pos": 133, "type": "METRIC", "confidence": 0.914718747138977}, {"text": "Sim", "start_pos": 163, "end_pos": 166, "type": "METRIC", "confidence": 0.9966128468513489}]}, {"text": "\u00ed \u00b5\u00ed\u00bc\u0083 is then calculated as the upper bound of the interval at p<0.005.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00bc\u0083", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8737590710322062}]}, {"text": "Testing and evaluation are done by calculating Sim for each test document.", "labels": [], "entities": [{"text": "Sim", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.96646648645401}]}, {"text": "Accuracy is calculated as the number of correct answers divided by the total number of documents tested.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9959009289741516}]}, {"text": "Although the aim of this paper is to evaluate the effectiveness of using different sample sizes, a task that essentially does not require a baseline, an accuracy of 87.1% will be used as a guiding baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9980993866920471}]}, {"text": "This accuracy is the best accuracy achieved in the relevant literature, albeit coming from a different register (MSA).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.999423623085022}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.999030590057373}]}, {"text": "In order to be able to plot AV accuracies against document size, it is necessary to identify best performing feature ensemble (feature + ngram).", "labels": [], "entities": []}, {"text": "Although previous literature suggests that stem bigrams are the most successful feature combinations, it should not betaken for granted that the feature combination that has been successful for Classical Arabic is also the best performer across domains in MSA.", "labels": [], "entities": []}, {"text": "To select the best performing feature-n-gram ensemble for each domain, the AV task described in the previous section is implemented on the full size of the corpus.", "labels": [], "entities": [{"text": "AV", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.8709469437599182}]}, {"text": "For each domain, the accuracy of each feature ensemble is evaluated using the leave-one-out method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9992437362670898}]}, {"text": "shows the best performing feature combination for each domain.", "labels": [], "entities": []}, {"text": "The results of experiment 1 show that with a test document size averaging 850 -1000 tokens, best performing features vary by MSA domain.", "labels": [], "entities": []}, {"text": "None of the domains achieved an accuracy close to the baseline, although the two domains that score lowest accuracy (economics and columnists) have the lowest document average size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9991517066955566}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9974817633628845}]}, {"text": "There are three factors in play for determining size effects in AV: the size of the question document, the number of training documents, and the size of the training set overall.", "labels": [], "entities": [{"text": "AV", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.939075231552124}]}, {"text": "Experiment 2 examines all three variables.", "labels": [], "entities": []}, {"text": "For Experiment 2, the training and testing procedure for Experiment 1 is replicated 6 times, using only the highest performing features as indicated in Experiment 1, and with varying sizes of the training set S \u2208 {5, 6, 7, 8, 9, 10} documents.", "labels": [], "entities": []}, {"text": "The result of the experiment is an ordered set (Q, T, R), where Q is the size of the question document, T is the size of the combined training set, and R (1, 0) is the result of the verification process.", "labels": [], "entities": []}, {"text": "R = 1 if the correct prediction is made, and R = 0 if an incorrect prediction is made.", "labels": [], "entities": [{"text": "R", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9765722155570984}, {"text": "R", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.989629328250885}]}, {"text": "Accuracy is calculated for values of Q in intervals demarked by Q \u2208 {0, 500, 600, 700, 800, 900, 1000, 1100, 1200}, and for T \u2208 {0, 5000, 6000, 7000, 7500, 8000, 8500, 9000, 11000}.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9909816384315491}]}, {"text": "Two datapoints are excluded (fiction T = (8000, 8500) and nonfiction T = (7000, 7500)) as outliers.", "labels": [], "entities": []}, {"text": "Each of the two datapoints consist of one document and have R = 0%.", "labels": [], "entities": [{"text": "R", "start_pos": 60, "end_pos": 61, "type": "METRIC", "confidence": 0.9942165613174438}]}, {"text": "Linear regression analysis between accuracy and relevant size variable is then conducted using SPSS.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9990597367286682}, {"text": "SPSS", "start_pos": 95, "end_pos": 99, "type": "DATASET", "confidence": 0.6547710299491882}]}, {"text": "shows the results of testing the verification method using the leave-one-out method on a training corpus of 5 -10 documents, and using the best-performance features identified in Experiment 1.", "labels": [], "entities": []}, {"text": "In all five domains, the verification method performs best at S = 5 training documents.", "labels": [], "entities": []}, {"text": "Regression Analysis shows a strong correlation coefficient of -0.931, with p<0.005 (c.f..", "labels": [], "entities": []}, {"text": "Regression analysis to identify correlation between the accuracy of the verification method and the total size of the training set in tokens is conducted.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9990910291671753}]}, {"text": "As shows, there is a moderate positive correlation of 0.492, with p<0.05 (p = 0.003).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: average document size per domain.", "labels": [], "entities": [{"text": "average document size", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8803282777468363}]}, {"text": " Table 3: Best performing feature ensemble per  domain.", "labels": [], "entities": []}, {"text": " Table 4: Best performing feature ensemble per  domain.", "labels": [], "entities": []}]}