{"title": [{"text": "Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Low-Resource Neural Machine Translation", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.6394327580928802}]}], "abstractContent": [{"text": "This paper proposes a novel multilingual multistage fine-tuning approach for low-resource neural machine translation (NMT), taking a challenging Japanese-Russian pair for benchmarking.", "labels": [], "entities": [{"text": "low-resource neural machine translation (NMT)", "start_pos": 77, "end_pos": 122, "type": "TASK", "confidence": 0.7818018623760769}]}, {"text": "Although there are many solutions for low-resource scenarios, such as multilingual NMT and back-translation, we have empirically confirmed their limited success when restricted to in-domain data.", "labels": [], "entities": []}, {"text": "We therefore propose to exploit out-of-domain data through transfer learning, by using it to first train a multilingual NMT model followed by multistage fine-tuning on in-domain parallel and back-translated pseudo-parallel data.", "labels": [], "entities": []}, {"text": "Our approach, which combines domain adaptation, multilingualism , and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7139876037836075}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9987175464630127}]}], "introductionContent": [{"text": "Neural machine translation (NMT) ( has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PB-SMT) (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7656562228997549}, {"text": "word alignments", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7408780455589294}, {"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 226, "end_pos": 279, "type": "TASK", "confidence": 0.6844834131853921}]}, {"text": "Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios.", "labels": [], "entities": []}, {"text": "Only by exploiting cross-lingual transfer learning techniques, can the NMT performance approach PBSMT performance in lowresource scenarios.", "labels": [], "entities": []}, {"text": "However, such methods usually require an NMT model trained on a resource-rich language pair like French\u2194English (parent), which is to be fine-tuned fora low-resource language pair like Uzbek\u2194English (child).", "labels": [], "entities": []}, {"text": "On the other hand, multilingual approaches propose to train a single model to translate multiple language pairs.", "labels": [], "entities": []}, {"text": "However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En).", "labels": [], "entities": []}, {"text": "Furthermore, the parents and children models should be trained on similar domains; otherwise, one has to take into account an additional problem of domain adaptation (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.7628589570522308}]}, {"text": "In this paper, we work on a linguistically distant and thus challenging language pair Japanese\u2194Russian (Ja\u2194Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor.", "labels": [], "entities": []}, {"text": "Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja\u2194En and Ru\u2194En, are also small.", "labels": [], "entities": []}, {"text": "As we demonstrate in Section 4, this severely limits the performance of prominent low-resource techniques, such as multilingual modeling, back-translation, and pivotbased PBSMT.", "labels": [], "entities": []}, {"text": "To remedy this, we propose a novel multistage fine-tuning method for NMT that combines multilingual modeling and domain adaptation (.", "labels": [], "entities": []}, {"text": "We have addressed two important research questions (RQs) in the context of extremely lowresource machine translation (MT) and our explorations have derived rational contributions (CTs) as follows: RQ1.", "labels": [], "entities": [{"text": "lowresource machine translation (MT)", "start_pos": 85, "end_pos": 121, "type": "TASK", "confidence": 0.8034398059050242}]}, {"text": "What kind of translation quality can we obtain in an extremely low-resource scenario?", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9750276803970337}]}, {"text": "We have made extensive comparisons with multiple architectures and MT paradigms to show how difficult the problem is.", "labels": [], "entities": [{"text": "MT paradigms", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.9112108647823334}]}, {"text": "We have also explored the utility of back-translation and show that it is ineffective given the poor performance of base MT systems used to generate pseudo-parallel data.", "labels": [], "entities": []}, {"text": "Our systematic exploration shows that multilingualism is extremely useful for in-domain translation with very limited corpora (see Section 4).", "labels": [], "entities": []}, {"text": "This type of exhaustive exploration has been missing from most existing works.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics on our in-domain parallel data.", "labels": [], "entities": []}, {"text": " Table 3: Number of lines in our monolingual data. Whereas  the first four are from the news corpora (in-domain), the last  two, i.e., \"IWSLT\" and \"Tatoeba,\" are from other domains.", "labels": [], "entities": [{"text": "Tatoeba", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.7283305525779724}]}, {"text": " Table 5: BLEU scores of baseline systems. Bold indicates the best BLEU score for each translation direction.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9967708587646484}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9983066320419312}]}, {"text": " Table 7: Over-sampling criteria for pseudo-parallel data generated by back-translation.", "labels": [], "entities": []}, {"text": " Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel  data generated by back-translation using (b3). Six \"X * \u2192Y\" columns show whether the pseudo-parallel data for each translation  direction is involved. Bold indicates the scores higher than (b3) and \" \u2022 \" indicates statistical significance of the improvement.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989410042762756}]}, {"text": " Table 11: BLEU scores obtained through multistage fine-tuning. \"Initialized\" column indicates the model used for initializing  parameters that are fine-tuned on the data indicated by . Bold indicates the best BLEU score for each translation direction.  \"", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9981215596199036}, {"text": "BLEU", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.998197615146637}]}, {"text": " Table 12: BLEU scores achieved through fine-tuning on the mixture of the original parallel data and six-way pseudo-parallel  data. \"Initialized\" column indicates the model used for initializing parameters and so does \"BT\" column the model used to  generate pseudo-parallel data. \" \u2022 \" indicates statistical significance of the improvement over #10.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9982045888900757}]}]}