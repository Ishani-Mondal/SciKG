{"title": [{"text": "User Evaluation of a Multi-dimensional Statistical Dialogue System", "labels": [], "entities": [{"text": "Statistical Dialogue", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7542282044887543}]}], "abstractContent": [{"text": "We present the first complete spoken dialogue system driven by a multi-dimensional statistical dialogue manager.", "labels": [], "entities": []}, {"text": "This framework has been shown to substantially reduce data needs by leveraging domain-independent dimensions , such as social obligations or feedback , which (as we show) can be transferred between domains.", "labels": [], "entities": []}, {"text": "In this paper, we conduct a user study and show that the performance of a multi-dimensional system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-driven approaches to spoken dialogue systems (SDS) are limited by their reliance on substantial amounts of annotated data in the target domain.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.8244563440481821}]}, {"text": "This can be addressed by considering transfer learning techniques, e.g. (, in which data from a source domain is leveraged to improve learning in a target domain.", "labels": [], "entities": []}, {"text": "In particular, domain adaptation has been used in the context of dialogue systems, focusing on identifying and exploiting similarities between domain ontologies in slot-filling tasks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.746283233165741}]}, {"text": "In contrast to this previous work, we take a multidimensional approach, which combines machine learning with linguistic theory.", "labels": [], "entities": []}, {"text": "Following Bunt (2011), we exploit the linguistic phenomenon that utterances serve more than one function in a conversation, i.e. they have more than one dimension (see Section 2).", "labels": [], "entities": []}, {"text": "1 For example, the utterance \"On what date would you like to fly to London?\" both asks a task-oriented question, and provides feedback about understanding the requested destination.", "labels": [], "entities": []}, {"text": "We take advantage of this phenomenon by training See also https://dit.uvt.nl/.", "labels": [], "entities": []}, {"text": "separate, fully-statistical dialogue models for each dimension and generating system responses along multiple dimensions simultaneously.", "labels": [], "entities": []}, {"text": "Such an SDS thus has the potential to adapt more efficiently to new domains by exploiting previously trained policies of the domain-independent dimensions, such as feedback and social conventions.", "labels": [], "entities": []}, {"text": "Previous implementations of multi-dimensional SDSs were mostly handcrafted (. were the first to present a statistical multidimensional dialogue manager (DM).", "labels": [], "entities": []}, {"text": "Their results suggest an up to 80% reduction in data: a task success rate of over 90% can be achieved after only 2,000 dialogues when using pre-trained policies, whereas at least 10,000 dialogues are required without pre-training.", "labels": [], "entities": []}, {"text": "In comparison, Ga\u0161i\u00b4c achieve similar success rates for in-domain systems trained on 5,000 dialogues.", "labels": [], "entities": [{"text": "Ga\u0161i\u00b4c", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.7354971766471863}]}, {"text": "However, Keizer and Rieser's findings are only tested in simulation.", "labels": [], "entities": []}, {"text": "In this paper, we present the first complete statistical SDS with multi-dimensional DM, and the first crowdsourced human user evaluation of this type of system, comparing a one-dimensional baseline and three multi-dimensional variants, using a novel web-based setup.", "labels": [], "entities": [{"text": "statistical SDS", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.5146083384752274}]}, {"text": "A novel aspect of our statistical analysis is testing for equivalence.", "labels": [], "entities": []}, {"text": "The four system variants were designed in such away that we would expect their performance levels to be indistinguishable when using fully trained policies.", "labels": [], "entities": []}, {"text": "Should the data provide statistical evidence for this, the multidimensional variants can be preferred due to their inherent potential for domain transfer.", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 138, "end_pos": 153, "type": "TASK", "confidence": 0.746392548084259}]}], "datasetContent": [{"text": "To get a better picture of what we might expect during the human evaluation, we first ran evaluations with simulated data.", "labels": [], "entities": []}, {"text": "The results obtained with the same settings as those during training are shown in.", "labels": [], "entities": []}, {"text": "As we hypothesised, the scores are very similar, the one-dim system only slightly outperforming the multi-dimensional systems.", "labels": [], "entities": []}, {"text": "We then extended the setup with different semantic error rates (; the results are shown in.", "labels": [], "entities": []}, {"text": "The performance levels of the  We use crowdsourcing to evaluate our system, following Jur\u010d\u00ed\u010dek et al.", "labels": [], "entities": []}, {"text": "In both of these works a phone-based system was deployed, using a bespoke ASR and Voice over IP (VoIP) to connect speech input/output with the dialogue system.", "labels": [], "entities": []}, {"text": "Here, we follow a similar evaluation methodology, but with a novel, simpler web-based interface using Google Chrome's builtin web speech API, embedded into the crowdsourcing task webpages.", "labels": [], "entities": []}, {"text": "A detailed description of the technical setup can be found in Appendix A.  The subjective evaluation metrics are derived from the following questionnaire, with one yes/no question (Q1) and four 6-point Likert Scale ratings.", "labels": [], "entities": []}, {"text": "Q1: Did you find all the information you were looking for?", "labels": [], "entities": []}, {"text": "Please state your attitude towards the following statements: The system was easy to understand (the voice was intelligible).", "labels": [], "entities": []}, {"text": "Q3: In this conversation, the system understood what you said.", "labels": [], "entities": []}, {"text": "Q4 [AsExpect]: The system worked the way you expected it to during the conversation.", "labels": [], "entities": [{"text": "Q4", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8386898040771484}]}, {"text": "Q5: From your experience with the system, you think you would use it in the future to find a place to eat.", "labels": [], "entities": []}, {"text": "Dialogue Act Agent one-dim multi-dim trans-fixed trans-adapt      The following objective success metrics are derived from the logs: EntProv: the system recommended an entity matching the task constraints, ConstrConf: the system confirmed all task constraints in its recommendation, InfoProv: the system provided all information requested by the user.", "labels": [], "entities": []}, {"text": "In total, 982 dialogues were collected (see), i.e. 246 dialogues per system variant on average.", "labels": [], "entities": []}, {"text": "We carried out a number of statistical tests to analyse the observed effect sizes in comparing the systems, including chi-squared (for success rates) and Mann-Whitney tests (for the Likert scale ratings), but also the 'two one-sided test', or TOST (, for equivalence, as argued in Section 2.1.", "labels": [], "entities": [{"text": "TOST", "start_pos": 243, "end_pos": 247, "type": "METRIC", "confidence": 0.7325755953788757}]}, {"text": "Ina TOST scenario, the null hypothesis is that the difference in performance between two systems, \u2206, is greater than a given threshold (a hyperparameter).", "labels": [], "entities": [{"text": "TOST", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.8795074820518494}]}, {"text": "This translates into two onesided null hypotheses: If both H lo and H hi are rejected, we can conclude that \u2212 < \u2206 < +, i.e. the difference lies below the threshold.", "labels": [], "entities": []}, {"text": "This testis much more conservative than failing to reject the null hypothesis in a conventional statistical test of significant difference.", "labels": [], "entities": []}, {"text": "The underlying one-sided tests can differ according to the nature of data at hand.", "labels": [], "entities": []}, {"text": "The default proposed by is t-tests.", "labels": [], "entities": []}, {"text": "However, our data fails the normal distribution assumption of a t-test.", "labels": [], "entities": []}, {"text": "Therefore, we use the robust t-test of for testing equivalence on Likert scale data, which does not assume normality, and a pooled z-test with continuity correction (, p. 53ff.) for success rates.", "labels": [], "entities": [{"text": "Likert scale data", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9003794391949972}, {"text": "continuity correction", "start_pos": 143, "end_pos": 164, "type": "METRIC", "confidence": 0.9879041016101837}]}, {"text": "We used a   threshold of = 10% for the equivalence tests.", "labels": [], "entities": []}, {"text": "shows the results for both objective and subjective metrics.", "labels": [], "entities": []}, {"text": "When considering the metrics for task success (SubjSucc, EntProv, ConstrConf, InfoProv), the one-dim system is the highest scoring, although the trans-adapt system is often a close second and in some cases the top scorer.", "labels": [], "entities": []}, {"text": "However, no statistically significant differences were detected, and the one-dim system was moreover found to be equivalent to the multi-dim (p = 0.024) and trans-adapt (p = 0.002) systems in perceived success (SubjSucc), and all three multi-dimensional systems were found to be equivalent to each other (p = 0.006, 0.009, and 0.031).", "labels": [], "entities": []}, {"text": "Similarly, several equivalences were detected for the three objective success metrics, as illustrated in Appendix B. All systems are equivalent on the other subjective ratings Q2-Q5.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8455654382705688}]}, {"text": "To get a sense of the noise levels encountered by the different system variants, we collected crowdsourced transcriptions of 2,931 utterances from 496 dialogues (45.6% of the total number of turns in the evaluation corpus and 50.5% of collected dialogues), spread approximately evenly across all system variants.", "labels": [], "entities": []}, {"text": "We then computed word error rate (WER).", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 17, "end_pos": 38, "type": "METRIC", "confidence": 0.8613274494806925}]}, {"text": "8 Results in show comparable noise Following Armstrong (2014), we do not apply a correction for multiple comparisons since we only performed a limited number of pre-planned comparisons and did not require testing against the universal null hypothesis \"nothing is significant\".", "labels": [], "entities": []}, {"text": "The reference transcriptions were obtained by majority voting over the three transcriptions collected for each utterance, with manual fixes in case of a tie (20% of the utterances).", "labels": [], "entities": []}, {"text": "levels for all system variants.", "labels": [], "entities": []}, {"text": "No significant differences were found and equivalence tests confirmed WER to be equivalent for all the systems.", "labels": [], "entities": [{"text": "WER", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9955313801765442}]}, {"text": "This confirms that none of the systems was disadvantaged and the results in are indeed comparable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of task domains.", "labels": [], "entities": []}, {"text": " Table 3: Test results on simulated data (same error rates  as in training): task success rate (SuccRate), average  dialogue length (AvgLen), average reward (AvgRew).", "labels": [], "entities": [{"text": "task success rate (SuccRate)", "start_pos": 77, "end_pos": 105, "type": "METRIC", "confidence": 0.7472235709428787}, {"text": "dialogue length (AvgLen)", "start_pos": 116, "end_pos": 140, "type": "METRIC", "confidence": 0.8280792951583862}, {"text": "average reward (AvgRew)", "start_pos": 142, "end_pos": 165, "type": "METRIC", "confidence": 0.6979277610778809}]}, {"text": " Table 4: Corpus statistics: the number of dialogues col- lected (NumDials) and the average number of turns per  dialogue (NumTurns) with standard deviation (StDv).", "labels": [], "entities": [{"text": "standard deviation (StDv)", "start_pos": 138, "end_pos": 163, "type": "METRIC", "confidence": 0.9235671281814575}]}, {"text": " Table 5: Overview of subjective and objective evaluation results (cf. Section 3.2 for metrics).", "labels": [], "entities": []}, {"text": " Table 6: WER analysis results (NumDials indicates the  number of dialogues transcribed for each system).", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.553966760635376}, {"text": "NumDials", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8664274215698242}]}]}