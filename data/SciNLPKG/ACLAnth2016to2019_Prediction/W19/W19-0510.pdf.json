{"title": [{"text": "Investigating the Stability of Concrete Nouns in Word Embeddings", "labels": [], "entities": [{"text": "Investigating the Stability of Concrete Nouns in Word Embeddings", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.7953030930625068}]}], "abstractContent": [{"text": "We know that word embeddings trained using neural-based methods (such as word2vec SGNS) are sensitive to stability problems and that across two models trained using the exact same set of parameters, the nearest neighbors of a word are likely to change.", "labels": [], "entities": []}, {"text": "All words are not equally impacted by this internal instability and recent studies have investigated features influencing the stability of word embeddings.", "labels": [], "entities": []}, {"text": "This stability can be seen as a clue for the reliability of the semantic representation of a word.", "labels": [], "entities": []}, {"text": "In this work, we investigate the influence of the degree of concreteness of nouns on the stability of their semantic representation.", "labels": [], "entities": []}, {"text": "We show that for English generic corpora, abstract words are more affected by stability problems than concrete words.", "labels": [], "entities": []}, {"text": "We also found that to a certain extent, the difference between the degree of concreteness of a noun and its nearest neighbors can partly explain the stability or instability of its neighbors.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are more and more used in corpus linguistics studies to draw conclusions on the usage of a word.", "labels": [], "entities": []}, {"text": "Looking at a word's nearest neighbors in embeddings models is a common way to do that.) quickly became one of the most popular tools to train word embeddings since it is easy and convenient to use and yields state of the arts results.", "labels": [], "entities": []}, {"text": "Word2vec, like any other neural-based method, implies several random processes when preprocessing the data used for training (subsampling of the corpus) and training word embeddings (initialization of neural networks weights, dynamic window, negative sampling).", "labels": [], "entities": [{"text": "Word2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8971295356750488}]}, {"text": "As a consequence training several times using the same controllable hyperparameters (number of dimensions, window size etc.) will not yield identical models.", "labels": [], "entities": []}, {"text": "Although this instability is not critical when using embeddings in deep learning models for NLP applications, concerns have been raised regarding the use of word embeddings in digital humanities.", "labels": [], "entities": []}, {"text": "Observations made by looking at nearest neighbors of a specific word might not be accurate since the nearest neighbors of a word might change from one model to the other.", "labels": [], "entities": []}, {"text": "Recently, several studies have investigated the stability of word embeddings and the possible ways to overcome the instability triggered byword embeddings to be able to use them in digital humanities.", "labels": [], "entities": []}, {"text": "studied the influence of training methods and hyperparameters on this instability.", "labels": [], "entities": []}, {"text": "They showed that an accurate selection of the number of training epochs would help prevent the unreliability of word embeddings while preventing overfitting on the training data.", "labels": [], "entities": []}, {"text": "examined the influence of the corpus used when training word embeddings and showed that embeddings are not a \"a single objective view of a corpus\".", "labels": [], "entities": []}, {"text": "They also emphasized the importance of taking variability into account when observing words' nearest neighbors and showed that the size of the corpus used for training will also influence this variability, in the sense that smaller corpora trigger more variability.", "labels": [], "entities": []}, {"text": "A good yet expensive way to overcome this variability would be to draw conclusions from several word embeddings sets trained using the same controllable parameters to confirm the observations made.", "labels": [], "entities": []}, {"text": "Other studies investigated the influence of several features on the instability of word embeddings.", "labels": [], "entities": []}, {"text": "showed that several factors contributed to the instability of word embeddings, POS being one of the most important one, and that unlike what could be expected frequency did not play a major role in the stability of word embeddings.", "labels": [], "entities": [{"text": "POS", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9324142336845398}]}, {"text": "investigated the influence of several features that were intrinsic to a word, corpus or model on the stability of word embeddings.", "labels": [], "entities": []}, {"text": "They proposed a technique to predict the variation of a word given simple features (POS, degree of polysemy of a word, frequency, entropy of a word with its contexts, norm of the vectors and score of the nearest neighbor of a word).", "labels": [], "entities": [{"text": "POS", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.995366096496582}]}, {"text": "They showed that the cosine similarity score of the nearest neighbor and the POS play a major role in the prediction of the variation.", "labels": [], "entities": [{"text": "cosine similarity score", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.7759501934051514}, {"text": "POS", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9451845288276672}]}, {"text": "They also showed that words with very low or very high frequency are more affected by variation.", "labels": [], "entities": []}, {"text": "Pierrejean and Tanguy (2018a) also identified some semantic clusters that would remain stable from one model to the other when training word embeddings using the same controllable hyperparameters.", "labels": [], "entities": []}, {"text": "Most of these clusters seem to consist of concrete words (e.g. family members, objects and rooms of the house).", "labels": [], "entities": []}, {"text": "Some recent works studied the semantic representations of concrete and abstract words in countbased distributional models and showed that concrete words have concrete nearest neighbors while abstract words tend to have abstract nearest neighbors (.", "labels": [], "entities": []}, {"text": "also showed that abstract words have higher contextual variability and are thus more difficult to predict than concrete words.", "labels": [], "entities": []}, {"text": "We wonder if those findings would also apply to models trained using neural-based methods and if different behaviors regarding variability could be observed for concrete and abstract In this work we analyze the relationship between the variation of nearest neighbors of a noun and its degree of concreteness.", "labels": [], "entities": []}, {"text": "In order to get a good understanding of this relationship, we decided to perform our analysis on 4 corpora of different sizes and types.", "labels": [], "entities": []}, {"text": "First, we investigate the relationship existing between the variation of nearest neighbors and frequency.", "labels": [], "entities": []}, {"text": "Then we investigate the impact of the degree of concreteness of a noun on the stability of its semantic representation.", "labels": [], "entities": []}, {"text": "Finally, we analyze the stability of the nearest neighbors of nouns through their degree of concreteness.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained word embeddings using word2vec (Mikolov et al., 2013) on 4 corpora of different sizes and types.", "labels": [], "entities": []}, {"text": "We used 2 generic corpora, the BNC made of about 100 million words 1 , and UMBC, a webbased corpus made of about 3 billion words ().", "labels": [], "entities": [{"text": "UMBC", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.868391752243042}]}, {"text": "We also used 2 specialized corpora, ACL (NLP scientific papers from the ACL Anthology () made of about 100 million words, and PLOS also consisting of about 100 million words (biology scientific papers gathered from the PLOS archive collections 2 ).", "labels": [], "entities": [{"text": "ACL (NLP scientific papers from the ACL Anthology", "start_pos": 36, "end_pos": 85, "type": "DATASET", "confidence": 0.8386289179325104}, {"text": "PLOS archive collections", "start_pos": 219, "end_pos": 243, "type": "DATASET", "confidence": 0.9233753085136414}]}, {"text": "Corpora were lemmatized and POS-tagged using the Talismane toolkit.", "labels": [], "entities": []}, {"text": "For each corpus, we trained 5 models using the same following default hyperparameters: architecture Skip-Gram with negative sampling rate of 5, window size set to 5, vectors dimensions set to 100, subsampling rate set to 10 \u22123 and number of iterations set to 5.", "labels": [], "entities": []}, {"text": "We only considered words that appear more than 100 times.", "labels": [], "entities": []}, {"text": "To check the overall performance of our models, we ran an intrinsic evaluation using a standard evaluation test set, MEN (.", "labels": [], "entities": [{"text": "MEN", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.7283137440681458}]}, {"text": "MEN consists of 3000 pairs of words.", "labels": [], "entities": [{"text": "MEN", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.87413090467453}]}, {"text": "Some words used in MEN pairs are not present in the different models vocabulary.", "labels": [], "entities": []}, {"text": "Thus the evaluation was run on 1 176 pairs for ACL, 2 687 pairs for the BNC, 1 516 pairs for PLOS and 2 996 pairs for UMBC.", "labels": [], "entities": [{"text": "ACL", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7791439890861511}, {"text": "BNC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.946193516254425}, {"text": "UMBC", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.956940233707428}]}, {"text": "We reported the average score (Spearman correlation) for the 5 models for each corpus in.", "labels": [], "entities": [{"text": "Spearman correlation)", "start_pos": 31, "end_pos": 52, "type": "METRIC", "confidence": 0.7858307858308157}]}, {"text": "We can see that results are different for all corpora.", "labels": [], "entities": []}, {"text": "Generic corpora have higher scores (0.73 for the BNC and 0.70 for UMBC) compared to specialized corpora (0.51 for ACL and 0.57 for PLOS).", "labels": [], "entities": [{"text": "BNC", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.727130651473999}]}, {"text": "This is not surprising because the type of evaluation test set we used is not really tailored for small specialized corpora such as ACL and PLOS.", "labels": [], "entities": []}, {"text": "We observed that the results were quite stable across models.", "labels": [], "entities": []}, {"text": "As aside note, it is important to mention that most of the nouns in the MEN test set are concrete nouns with an average concreteness score of 4.6.", "labels": [], "entities": [{"text": "MEN test set", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.901567538579305}]}, {"text": "This raises questions regarding the bias of intrinsic evaluation test sets.", "labels": [], "entities": []}, {"text": "When evaluating distributional semantics models, what does it mean to focus mainly on evaluating the semantic representation of concrete words?", "labels": [], "entities": []}, {"text": "If we consider word embeddings more particularly and the fact that they are prone to stability problems, how does the stability relate to the concreteness of a word?", "labels": [], "entities": []}, {"text": "Are concrete words more stable than abstract words?", "labels": [], "entities": []}, {"text": "We propose to investigate those effects in the following experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MEN score, vocabulary size, mean variation score and standard deviations for each corpus (5  models trained per corpus).", "labels": [], "entities": [{"text": "MEN score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8155644536018372}, {"text": "mean variation score", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.9086739222208658}]}, {"text": " Table 2: Spearman correlation scores between frequency and variation, frequency and degree of con- creteness and variation and degree of concreteness. All correlations scores are significant at the 0.05  level except for the one where ns is indicated.", "labels": [], "entities": []}]}