{"title": [{"text": "Erroneous data generation for Grammatical Error Correction", "labels": [], "entities": [{"text": "Erroneous data generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6651059786478678}, {"text": "Grammatical Error Correction", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7172335584958395}]}], "abstractContent": [{"text": "It has been demonstrated that the utilization of a monolingual corpus in neural Grammatical Error Correction (GEC) systems can significantly improve the system performance.", "labels": [], "entities": []}, {"text": "The previous state-of-the-art neural GEC system is an ensemble of four Transformer models pretrained on a large amount of Wikipedia Edits.", "labels": [], "entities": []}, {"text": "The Singsound GEC system follows a similar approach but is equipped with a sophisticated erroneous data generating component.", "labels": [], "entities": [{"text": "Singsound GEC system", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.8687235911687216}]}, {"text": "Our system achieved an F 0.5 of 66.61 in the BEA 2019 Shared Task: Grammatical Error Correction.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9908903241157532}, {"text": "BEA 2019 Shared Task", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.8365784883499146}]}, {"text": "With our novel erroneous data generating component, the Singsound neural GEC system yielded an M 2 of 63.2 on the CoNLL-2014 benchmark (8.4% relative improvement over the previous state-of-the-art system).", "labels": [], "entities": [{"text": "Singsound neural GEC", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6751217643419901}, {"text": "M 2", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9905911982059479}, {"text": "CoNLL-2014 benchmark", "start_pos": 114, "end_pos": 134, "type": "DATASET", "confidence": 0.9294151365756989}]}], "introductionContent": [{"text": "The most effective approaches to Grammatical Error Correction (GEC) task are machine translation based methods.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC) task", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.8733716862542289}, {"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7098238915205002}]}, {"text": "Both Statistical Machine Translation (SMT) approaches and Neural Machine Translation (NMT) methods have achieved promising results in the GEC task.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 5, "end_pos": 42, "type": "TASK", "confidence": 0.8525318503379822}, {"text": "Neural Machine Translation (NMT)", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.8167284528414408}, {"text": "GEC task", "start_pos": 138, "end_pos": 146, "type": "TASK", "confidence": 0.9178532958030701}]}, {"text": "Pretraining a decoder as a language model is an effective method to improve the performance of neural GEC systems.", "labels": [], "entities": []}, {"text": "As an extension of this work, showed pretraining on 4 billion tokens of Wikipedia edits to be beneficial for the GEC task.", "labels": [], "entities": [{"text": "GEC task", "start_pos": 113, "end_pos": 121, "type": "TASK", "confidence": 0.8816111981868744}]}, {"text": "In this work, we investigate a similar approach by systematically generating parallel data for pretraining.", "labels": [], "entities": []}, {"text": "As shown in, in addition to spelling errors (price \u2192 puice), transposition errors (independent voters \u2192 voters independent) and concatenation errors (the man \u2192 theman), our", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we generated a corpus of 3 billion tokens, of which about 24% were errors.", "labels": [], "entities": []}, {"text": "Following, we also use Transformer as our encoder-decoder model, using Tensor2Tensor open source implementation . The models are trained on words, and rare words are segmented into sub-words with the byte pair encoding (BPE) ().", "labels": [], "entities": [{"text": "byte pair encoding (BPE)", "start_pos": 200, "end_pos": 224, "type": "METRIC", "confidence": 0.7216936002175013}]}, {"text": "We use 6 layers for both encoder and decoder, and 4 attention heads.", "labels": [], "entities": []}, {"text": "The embedding size and hidden size are 1024, and the filter size for all positionwise feed forward network is 4096.", "labels": [], "entities": []}, {"text": "We set dropout rate to 0.3, and source word dropout is set to 0.2 as a noising technique.", "labels": [], "entities": []}, {"text": "Following Junczys-, source, target and output embeddings are tied in our models.", "labels": [], "entities": []}, {"text": "Following, we first trained our model on an artificially generated parallel corpus with a batch size of approximately 3072 tokens.", "labels": [], "entities": []}, {"text": "Then we set the batch size to 2048 tokens and fine-tuned on human annotated data for 20 epochs, and we averaged the 5 best checkpoints.", "labels": [], "entities": []}, {"text": "Finally, the averaged model was fine-tuned on the ABCN and FCE training data for 1000 steps as domain adaptation.", "labels": [], "entities": [{"text": "ABCN", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9549441933631897}, {"text": "FCE training data", "start_pos": 59, "end_pos": 76, "type": "DATASET", "confidence": 0.8767180840174357}, {"text": "domain adaptation", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7194823324680328}]}, {"text": "There are about 50% sentence pairs without any correction in the Lang-8 dataset, and we noticed that training with too many error-free sentence pairs had a negative effect.", "labels": [], "entities": [{"text": "Lang-8 dataset", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.9482573568820953}]}, {"text": "Therefore, we filtered out these error-free sentence pairs in the Lang-8 dataset.", "labels": [], "entities": [{"text": "Lang-8 dataset", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.8723647594451904}]}, {"text": "Since the NUCLE, FCE and ABCN datasets are much smaller than the Lang-8 set, we did not filter out the error-free sentence pairs in these datasets.", "labels": [], "entities": [{"text": "NUCLE", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.9335874319076538}, {"text": "FCE", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.6498093008995056}, {"text": "ABCN datasets", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8338631391525269}]}, {"text": "We used beam search for decoding with abeam size of 4 at evaluation time.", "labels": [], "entities": []}, {"text": "For the ensemble, we averaged logits from 4 Transformer models with identical hyper-parameters at each decoding step.", "labels": [], "entities": []}, {"text": "Following (Grundkiewicz and JunczysDowmunt, 2018; Junczys-Dowmunt et al., 2018; Lichtarge et al., 2018), we preprocessed the JF-LEG dataset with spell-checking.", "labels": [], "entities": [{"text": "JF-LEG dataset", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.9354310631752014}]}, {"text": "We did not apply spell-checking to the ABCN and CoNLL-2014 datasets.", "labels": [], "entities": [{"text": "ABCN and CoNLL-2014 datasets", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.8065458983182907}]}], "tableCaptions": [{"text": " Table 2: Statistics for training data sets.", "labels": [], "entities": []}, {"text": " Table 4: Probability distribution of sentence errors.", "labels": [], "entities": []}, {"text": " Table 6: Number of misspells in a token.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9558131694793701}]}, {"text": " Table 8: Results of BEA 2019 GEC competition.", "labels": [], "entities": [{"text": "BEA 2019 GEC competition", "start_pos": 21, "end_pos": 45, "type": "DATASET", "confidence": 0.8316973596811295}]}, {"text": " Table 9: Results of ABCN set. \"w/o pretrain\" refers to  models without pretraining.", "labels": [], "entities": [{"text": "ABCN set", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8500273823738098}]}]}