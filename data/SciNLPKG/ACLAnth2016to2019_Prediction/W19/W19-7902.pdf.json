{"title": [{"text": "Information-theoretic locality properties of natural language", "labels": [], "entities": []}], "abstractContent": [{"text": "I present theoretical arguments and new empirical evidence for an information-theoretic principle of word order: information locality, the idea that words that strongly predict each other should be close to each other in linear order.", "labels": [], "entities": [{"text": "information locality", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7097800970077515}]}, {"text": "I show that information locality can be derived under the assumption that natural language is a code that enables efficient communication while minimizing information-processing costs involved in online language comprehension, using recent psycholinguistic theories to characterize those processing costs information-theoretically.", "labels": [], "entities": [{"text": "information locality", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7516178786754608}]}, {"text": "I argue that information locality subsumes and extends the previously-proposed principle of dependency length minimization (DLM), which has shown great explanatory power for predicting word order in many languages.", "labels": [], "entities": [{"text": "information locality", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7436528503894806}, {"text": "dependency length minimization (DLM)", "start_pos": 92, "end_pos": 128, "type": "TASK", "confidence": 0.7281780193249384}, {"text": "predicting word order", "start_pos": 174, "end_pos": 195, "type": "TASK", "confidence": 0.8058962424596151}]}, {"text": "Finally, I show corpus evidence that information locality has improved explanatory power over DLM in two domains: in predicting which dependencies will have shorter and longer lengths across 50 languages, and in predicting the preferred order of adjectives in English.", "labels": [], "entities": []}], "introductionContent": [{"text": "The field of functional linguistics has long argued that the distinctive properties of natural language are best explained in terms of what makes for an efficient communication system under the cognitive constraints particular to human beings.", "labels": [], "entities": [{"text": "functional linguistics", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7890405058860779}]}, {"text": "The idea is that the properties of language are determined by the pressure to enable efficient communication while minimizing the information-processing effort required for language production and comprehension by humans.", "labels": [], "entities": []}, {"text": "Within that field, a particularly promising concept is the principle of dependency length minimization (DLM): the idea that words linked in syntactic dependencies are under a pressure to be close in linear order.", "labels": [], "entities": [{"text": "dependency length minimization (DLM)", "start_pos": 72, "end_pos": 108, "type": "TASK", "confidence": 0.7791216919819514}]}, {"text": "DLM provides a single unified explanation for many of the word order properties of natural language: Greenberg's harmonic word order universals) and exceptions to them; the rarity of crossing dependencies, which correspond to deviations from context-free grammar; ordering preferences based on constituent length such as Heavy NP Shift; and the statistical distribution of orders in treebank corpora.", "labels": [], "entities": []}, {"text": "See and for recent reviews.", "labels": [], "entities": []}, {"text": "The theoretical motivation for DLM is based on efficiency of language processing: the idea is that long dependencies tax the working memory capacities of speakers and listeners; inline with this view, there is observable processing cost in terms of reading time for long dependencies (.", "labels": [], "entities": []}, {"text": "At the same time, there have been attempts to derive the properties of human language formally from information-theoretic models of efficiency.", "labels": [], "entities": []}, {"text": "But it is not yet clear how a principle such as DLM, which appears to be necessary for explaining the syntactic properties of natural language, would fit into these theories, or more generally into the information-theoretic view of language as an efficient code.", "labels": [], "entities": []}, {"text": "The motivation for DLM is based on heuristic arguments about memory usage and on empirical results from studies of online processing, and it is not clear how to translate this motivation into the mathematical language of information theory.", "labels": [], "entities": []}, {"text": "Here I bridge this gap by providing theoretical arguments and empirical evidence fora new, information-theoretic principle of word order, grounded in empirical findings from the psycholinguistic literature and in the theory of communication in a noisy channel.", "labels": [], "entities": [{"text": "word order", "start_pos": 126, "end_pos": 136, "type": "TASK", "confidence": 0.7020886391401291}]}, {"text": "I assume that linguistic speakers and listeners are processing language incrementally using lossy memory representations of linguistic context.", "labels": [], "entities": []}, {"text": "Under these circumstances, we can derive a principle of information locality, which states that an efficient language will minimize the linear distance between elements with high mutual information, an information-theoretic measure of how strongly two words predict each other.", "labels": [], "entities": [{"text": "information locality", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6881878674030304}]}, {"text": "Furthermore, assuming a particular probabilistic interpretation of dependency grammar), I show that DLM falls out as an approximation to information locality.", "labels": [], "entities": []}, {"text": "Finally, I present two new pieces of empirical evidence that information locality provides improved explanatory power over DLM in predicting word orders in corpora.", "labels": [], "entities": [{"text": "predicting word orders in corpora", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.8353624701499939}]}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews relevant psycholinguistic results and information-theoretic models of online processing difficulty, concluding that they are inadequate for predicting word order patterns.", "labels": [], "entities": [{"text": "predicting word order patterns", "start_pos": 158, "end_pos": 188, "type": "TASK", "confidence": 0.8015675693750381}]}, {"text": "Section 3 shows how to derive the principle of information locality from a modified model of online processing difficulty, and how DLM can be seen as a special case of information locality.", "labels": [], "entities": []}, {"text": "In Section 4, I give corpus evidence that information locality makes correct predictions in two cases where DLM makes no predictions: in predicting the distance between words in dependencies in general across 50 languages, and in predicting the relative order of adjectives in English.", "labels": [], "entities": [{"text": "predicting the distance between words in dependencies", "start_pos": 137, "end_pos": 190, "type": "TASK", "confidence": 0.7675169706344604}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Regression coefficients predicting dependency length as a function of pmi between head and  dependent. A negative sign indicates that words with higher pmi are closer to each other. Languages  where the effect is not significant at p < .001 are in italics.", "labels": [], "entities": [{"text": "Regression", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9716785550117493}]}]}