{"title": [{"text": "From Explainability to Explanation: Using a Dialogue Setting to Elicit Annotations with Justifications", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite recent attempts in the field of explain-able AI to go beyond black box prediction models, typically already the training data for supervised machine learning is collected in a manner that treats the annotator as a \"black box\", the internal workings of which remains unobserved.", "labels": [], "entities": []}, {"text": "We present an annotation method where a task is given to a pair of annotators who collaborate on finding the best response.", "labels": [], "entities": []}, {"text": "With this we want to shed light on the questions if the collaboration increases the quality of the responses and if this \"thinking to-gether\" provides useful information in itself, as it at least partially reveals their reasoning steps.", "labels": [], "entities": []}, {"text": "Furthermore, we expect that this setting puts the focus on explanation as a linguistic act, vs. explainability as a property of models.", "labels": [], "entities": []}, {"text": "Ina crowd-sourcing experiment, we investigated three different annotation tasks, each in a collaborative dialogical (two annotators) and monological (one annotator) setting.", "labels": [], "entities": []}, {"text": "Our results indicate that our experiment elicits collaboration and that this collaboration increases the response accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9824120402336121}]}, {"text": "We see large differences in the annotators' behavior depending on the task.", "labels": [], "entities": []}, {"text": "Similarly, we also observe that the dialog patterns emerging from the collaboration vary significantly with the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Imagine asking a friend whether you can borrow their car for the afternoon, and the only reply you get is \"no\".", "labels": [], "entities": []}, {"text": "You would presumably perceive this as somewhat brusque, and Conversation Analysis would back you up there: This kind of dispreferred reply typically needs more work, often being initiated with a filled pause, and being augmented with a reason for the refusal.", "labels": [], "entities": []}, {"text": "Now imagine you are asking a car rental place, via their website, whether you can rent a car for the afternoon, and again all you get as a reply is a \"no\".", "labels": [], "entities": []}, {"text": "You would still not be pleased, but the difference here would be that while your friend may have been unwilling to tell you their reasons, the car rental company, having used a complex statistical model that judged you untrustworthy, based on various kinds of information it has about you, would be unable to state reasons (other than a vacuous one like \"your score is too low\").", "labels": [], "entities": []}, {"text": "The field of explainable AI has set itself as a goal to open up the blackbox of current prediction models in order to make their decisions more transparent and also identifying problems concerning the core issues in AI safety.", "labels": [], "entities": []}, {"text": "(See () for recent overviews.)", "labels": [], "entities": []}, {"text": "The focus there typically is on providing explanations of decisions in terms of examples or secondary models (e.g. (), where the resulting explanations are understandable at best to experts.", "labels": [], "entities": []}, {"text": "In contrast, our interest is in learning to provide verbal explanations, accessible also to novice users.", "labels": [], "entities": []}, {"text": "As a first step, we are interested in methods for eliciting data that can be used for this.", "labels": [], "entities": []}, {"text": "In this paper, we present an annotation scheme where a pair of annotators works in collaboration to find the best response to a question.", "labels": [], "entities": []}, {"text": "Our hypothesis is that a) this leads to better quality responses compared to non-collaborative annotation, as the annotators can actively acknowledge/correct/help their partners, b) the resulting discussions give access to the collaborative thinking directions that lead to the final response, and c) puts the focus on explanation as a linguistic act, vs. explainability as a property of models.", "labels": [], "entities": []}, {"text": "We present results from three different annotation tasks.", "labels": [], "entities": []}, {"text": "For each task we compare the accuracies of the responses we obtain in a dialog (two annotators) and a monologue setting, analyze to what extent the task triggered discussions in the dialog setting and quantify dialog patterns emerging in the interaction of the annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the hypotheses set out above, we created a number of tasks (pairs of questions Q and information I), which we put to individual annotators and also to pairs of annotators in a dialogical setting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistical Overview of Data", "labels": [], "entities": [{"text": "Statistical Overview of Data", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.6813392639160156}]}, {"text": " Table 2: Dialogue vs. Monologue: Correct Answers", "labels": [], "entities": [{"text": "Answers", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.4382449686527252}]}, {"text": " Table 3: Proportions of active dialogues in each task  with different patterns.", "labels": [], "entities": []}]}