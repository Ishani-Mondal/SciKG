{"title": [{"text": "Language Modeling with Syntactic and Semantic Representation for Sentence Acceptability Predictions", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6570733785629272}, {"text": "Sentence Acceptability Predictions", "start_pos": 65, "end_pos": 99, "type": "TASK", "confidence": 0.8530260721842448}]}], "abstractContent": [{"text": "In this paper, we investigate the effect of enhancing lexical embeddings in LSTM language models (LM) with syntactic and semantic representations.", "labels": [], "entities": []}, {"text": "We evaluate the language models using perplexity, and we evaluate the performance of the models on the task of predicting human sentence acceptability judgments.", "labels": [], "entities": [{"text": "predicting human sentence acceptability judgments", "start_pos": 111, "end_pos": 160, "type": "TASK", "confidence": 0.8648309469223022}]}, {"text": "We train LSTM language models on sentences automatically annotated with universal syntactic dependency roles (Nivre et al., 2016), dependency tree depth features, and universal semantic tags (Abzianidze et al., 2017) to predict sentence acceptability judgments.", "labels": [], "entities": []}, {"text": "Our experiments indicate that syntactic depth and tags lower the perplex-ity compared to a plain LSTM language model, while semantic tags increase the perplexity.", "labels": [], "entities": []}, {"text": "Our experiments also show that neither syntactic nor semantic tags improve the performance of LSTM language models on the task of predicting sentence acceptability judgments.", "labels": [], "entities": [{"text": "predicting sentence acceptability judgments", "start_pos": 130, "end_pos": 173, "type": "TASK", "confidence": 0.8794582188129425}]}], "introductionContent": [{"text": "show that human acceptability judgments are graded rather than binary.", "labels": [], "entities": []}, {"text": "It is not entirely obvious what determines sentence acceptability for speakers and listeners.", "labels": [], "entities": []}, {"text": "However, syntactic structure and semantic content are clearly central to acceptability judgments.", "labels": [], "entities": []}, {"text": "In fact, as show, it is possible to use a language model, augmented with a scoring function, to predict acceptability.", "labels": [], "entities": []}, {"text": "Standard RNN language models perform fairly well on the sentence acceptability prediction task.", "labels": [], "entities": [{"text": "sentence acceptability prediction", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.8042586843172709}]}, {"text": "By experimenting with different sorts of enrichments of the training data, one can explore their effect on both the perplexity and the predictive accuracy of the LM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.979249894618988}]}, {"text": "For example, report that including contextual information in training and testing improves the performance of an LSTM LM on the acceptability task, when contextual information is contributed by preceding and following sentences in a document.", "labels": [], "entities": []}, {"text": "Here we report several experiments on the possible contribution of symbolic representations of semantic and syntactic features to the accuracy of LSTM LMs in predicting human sentence acceptability judgments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9978765249252319}, {"text": "predicting human sentence acceptability judgments", "start_pos": 158, "end_pos": 207, "type": "TASK", "confidence": 0.8512205839157104}]}, {"text": "For semantic tags, we use the Universal Semantic Tagging scheme, which provides language independent and fine-grained semantic categories for individual words (.", "labels": [], "entities": [{"text": "Universal Semantic Tagging", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.6327645579973856}]}, {"text": "We take our syntactic roles from the Universal Dependency Grammar scheme (.", "labels": [], "entities": []}, {"text": "This allows us to assign to each word in a sentence a semantic and a syntactic role, respectively.", "labels": [], "entities": []}, {"text": "Our working hypothesis is that fora language model the syntactic and semantic annotations will highlight semantic and syntactic patterns observed in the data.", "labels": [], "entities": []}, {"text": "Therefore sentences that exhibit these patterns should be more acceptable than sentences which diverge from them.", "labels": [], "entities": []}, {"text": "One would expect that if we get lower perplexity for one of the tagging scheme LMs, then its performance would improve on the acceptability prediction task.", "labels": [], "entities": [{"text": "acceptability prediction", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.8976665437221527}]}, {"text": "Clearly, better performance on this task indicates that tagging supplies useful information for predicting acceptability.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, we train a set of language models, some of them on tag annotated corpora, and some on plain text.", "labels": [], "entities": []}, {"text": "While we are interested in the effect of the tags on model perplexity, our main concern is to measure the influence of the tags on an LSTM LM's predictive power in the sentence acceptability task.", "labels": [], "entities": [{"text": "sentence acceptability task", "start_pos": 168, "end_pos": 195, "type": "TASK", "confidence": 0.7277555465698242}]}, {"text": "We implement four variants of LSTM language models.", "labels": [], "entities": []}, {"text": "The first model is a plain LSTM that predicts the next word based on the previous sequence of words.", "labels": [], "entities": []}, {"text": "The second, third and fourth models predict the next word w i conditioned on the previous sequence of words and tags, for which we write PM (w i ).", "labels": [], "entities": []}, {"text": "For a model M that uses syntactic or semantic information: (1) We stress that the current tag (t i ) is not given when the model predicts the current word (w i ).", "labels": [], "entities": []}, {"text": "Using the main hyperparameters from a previous similar experiment (, all language models use a unidirectional LSTM of size 600.", "labels": [], "entities": []}, {"text": "We apply a drop-out of 0.4 after the LSTM layer.", "labels": [], "entities": []}, {"text": "The models are trained on a vocabulary of 100,000 words.", "labels": [], "entities": []}, {"text": "We randomly initialise word embeddings of size 300 dimensions, and tag embeddings of size 30 dimensions.", "labels": [], "entities": []}, {"text": "Each model is trained for 10 epochs.", "labels": [], "entities": []}, {"text": "Following the literature on acceptability (, we predict a judgment by applying a variant of the scoring function SLOR () to a model's predictions.", "labels": [], "entities": [{"text": "SLOR", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.532859742641449}]}, {"text": "We evaluate the model by calculating the Weighted Pearson correlation coefficient between the SLOR score assigned by the model and the judgments assigned by the annotators.", "labels": [], "entities": [{"text": "Weighted Pearson correlation coefficient", "start_pos": 41, "end_pos": 81, "type": "METRIC", "confidence": 0.8258317410945892}, {"text": "SLOR score", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.9726411402225494}]}, {"text": "Even though we show only the mean judgment in, each data point comes also with a variance (there is heteroscedasticity).", "labels": [], "entities": []}, {"text": "Thus we have chosen to weight the data points with the inverse of the variance when computing the Pearson correlation, as is standard when computing least square regression on heteroscedastic data.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.929664820432663}]}, {"text": "We report the weighted correlation point wise between all models, and between each model and the human judgments.", "labels": [], "entities": [{"text": "weighted correlation point wise", "start_pos": 14, "end_pos": 45, "type": "METRIC", "confidence": 0.7526559084653854}]}, {"text": "Additionally, we perform three experiments where we shuffle the syntactic and semantic representations in the test sentences.", "labels": [], "entities": []}, {"text": "This is done to determine if the tags provide useful information for the task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mean judgments and standard deviation  for the test set.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.983860969543457}, {"text": "standard", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9689679145812988}]}, {"text": " Table 2: Weighted Pearson correlation between prediction from different models on the SMOG1 dataset.  * indicates that the tags have been shuffled.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.9134132564067841}, {"text": "SMOG1 dataset", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.8482044637203217}]}, {"text": " Table 3: Training loss and accuracy for the lan- guage modeling task.", "labels": [], "entities": [{"text": "Training loss", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9345937371253967}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9990026354789734}]}, {"text": " Table 4: Comparison of the average relative score  assigned by the models and humans for the differ- ent sentences in the test set.", "labels": [], "entities": []}, {"text": " Table 5: Shared erroneous sentences between the  models.", "labels": [], "entities": []}, {"text": " Table 6: Human judgments and model scores for  sentence (1).", "labels": [], "entities": []}, {"text": " Table 7: Human judgments and model scores for  sentence (2).", "labels": [], "entities": []}]}