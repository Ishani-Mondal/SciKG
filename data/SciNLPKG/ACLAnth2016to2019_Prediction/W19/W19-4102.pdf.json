{"title": [{"text": "A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension", "labels": [], "entities": [{"text": "BERT", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9973999261856079}]}], "abstractContent": [{"text": "Conversational machine comprehension (CMC) requires understanding the context of multi-turn dialogue.", "labels": [], "entities": [{"text": "Conversational machine comprehension (CMC)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7608450750509897}]}, {"text": "Using BERT, a pre-training language model, has been successful for single-turn machine comprehension, while modeling multiple turns of question answering with BERT has not been established because BERT has a limit on the number and the length of input sequences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9842532277107239}, {"text": "question answering", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7382217943668365}, {"text": "BERT", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9838395118713379}, {"text": "BERT", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9622461795806885}]}, {"text": "In this paper, we propose a simple but effective method with BERT for CMC.", "labels": [], "entities": [{"text": "BERT", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9980655312538147}]}, {"text": "Our method uses BERT to encode a paragraph independently conditioned with each question and each answer in a multi-turn context.", "labels": [], "entities": [{"text": "BERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9957772493362427}]}, {"text": "Then, the method predicts an answer on the basis of the paragraph representations encoded with BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9870154857635498}]}, {"text": "The experiments with representative CMC datasets, QuAC and CoQA, show that our method outperformed recently published methods (+0.8 F1 on QuAC and +2.1 F1 on CoQA).", "labels": [], "entities": [{"text": "CMC datasets", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.8606859743595123}, {"text": "CoQA", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8431346416473389}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9887780547142029}, {"text": "F1", "start_pos": 152, "end_pos": 154, "type": "METRIC", "confidence": 0.9868859052658081}]}, {"text": "In addition, we conducted a detailed analysis of the effects of the number and types of dialogue history on the accuracy of CMC, and we found that the gold answer history, which may not be given in an actual conversation, contributed to the model performance most on both datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.999420166015625}]}], "introductionContent": [{"text": "Single-turn machine comprehension (MC) has been studied as a question answering method (.", "labels": [], "entities": [{"text": "Single-turn machine comprehension (MC)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.771662970383962}, {"text": "question answering", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.841131180524826}]}, {"text": "Conversational artificial intelligence (AI) such as Siri and Google Assistant requires answering not only a single-turn question but also multi-turn questions in a dialogue.", "labels": [], "entities": []}, {"text": "Recently, two datasets, QuAC () and CoQA (, were released to answer sequential questions in a dialogue by comprehending a paragraph.", "labels": [], "entities": []}, {"text": "This task is called conversational machine comprehension (CMC) (, which requires understanding the context of multi-turn dialogue that consists of the question and answer history.", "labels": [], "entities": [{"text": "conversational machine comprehension (CMC)", "start_pos": 20, "end_pos": 62, "type": "TASK", "confidence": 0.5849931587775549}]}, {"text": "Learning machine comprehension models requires a lot of question answering data.", "labels": [], "entities": [{"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7301394194364548}]}, {"text": "Therefore, transfer learning from pre-training language models based on a large-scale unlabeled corpus is useful for improving the model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9955089092254639}]}, {"text": "In particular, BERT) achieved stateof-the-art results when performing various tasks including the single-turn machine comprehension dataset SQuAD (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9875665903091431}]}, {"text": "BERT takes a concatenation of two sequences as input during pre-training and can capture the relationship between the two sequences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9160302877426147}]}, {"text": "When adapting BERT for MC, we use a question and a passage as input and fine-tune the pre-trained BERT model to extract an answer from the paragraph.", "labels": [], "entities": [{"text": "BERT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9719422459602356}]}, {"text": "However, BERT can accept only two sequences of 512 tokens and thus cannot handle CMC naively.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.914972722530365}]}, {"text": "proposed a method for CMC that is based on an architecture for single-turn MC and uses BERT as a feature-based approach.", "labels": [], "entities": [{"text": "BERT", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9955102205276489}]}, {"text": "To convert CMC into a single-turn MC task, the method uses a reformulated question, which is the concatenation of the question and answer sequences in a multi-turn context with a special token.", "labels": [], "entities": []}, {"text": "It then uses BERT to obtain contextualized embeddings for the reformulated question and paragraph, respectively.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9979881048202515}]}, {"text": "However, it cannot use BERT to capture the interaction between each sequence in the multi-turn context and the paragraph.", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9982767105102539}]}, {"text": "In this paper, we propose a simple but effective method for CMC based on a fine-tuning approach with BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.992577850818634}]}, {"text": "Our method consists of two main steps.", "labels": [], "entities": []}, {"text": "The first step is contextual encoding where BERT is used for independently obtaining paragraph representations conditioned with the current question, each of the previous questions, and each of the previous answers.", "labels": [], "entities": [{"text": "contextual encoding", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.6920784264802933}, {"text": "BERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9979105591773987}]}, {"text": "The second step is answer span extraction, where the start and end position of the current answer are predicted based on the concatenation of the paragraph representations encoded in the previous step.", "labels": [], "entities": [{"text": "answer span extraction", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.8145884474118551}]}, {"text": "The contributions of this paper are as follows: \u2022 We propose a novel method for CMC based on fine-tuning BERT by regarding the sequences of the questions and the answers as independent inputs.", "labels": [], "entities": [{"text": "BERT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9878998398780823}]}, {"text": "\u2022 The experimental results show that our method outperformed published methods on both QuAC and CoQA.", "labels": [], "entities": [{"text": "CoQA", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.8346464037895203}]}, {"text": "\u2022 We found that the gold answer history contributed to the model performance most by analyzing the effects of dialogue history.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our method on two conversational machine comprehension datasets, QuAC (Choi et al., 2018) and CoQA ().", "labels": [], "entities": [{"text": "QuAC", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.7166619300842285}]}, {"text": "Although CoQA is released as an abstractive CMC dataset, shows that the extractive approach is also effective for CoQA.", "labels": [], "entities": [{"text": "CMC dataset", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.7500055730342865}]}, {"text": "Thus, we also use our extractive approach on CoQA.", "labels": [], "entities": []}, {"text": "To handle answer types in CoQA, we predict the probability distribution of the answer type (SPAN, YES, NO, and UNANSWERABLE) and replace the predicted span with \"yes\", \"no\", or \"unknown\" tokens except for the \"SPAN\" answer type.", "labels": [], "entities": [{"text": "SPAN", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8144901394844055}, {"text": "YES", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9411595463752747}, {"text": "NO", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.7502697110176086}, {"text": "UNANSWERABLE", "start_pos": 111, "end_pos": 123, "type": "METRIC", "confidence": 0.991097092628479}]}, {"text": "In QuAC, the unanswerable questions are handled as an answer span (P contains a special token), and the type prediction for yes/no questions is not evaluated on the leaderboard.", "labels": [], "entities": []}, {"text": "Therefore, we skip the answer type prediction step.", "labels": [], "entities": [{"text": "answer type prediction", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.8546823263168335}]}], "tableCaptions": [{"text": " Table 1: The results on the CoQA test set of single models (F 1 score). Our BERT w/ 2-ctx model ranked 13th  among all unpublished and published models (including ensemble) on the leaderboard at the submission time  (April 13, 2019). The ConvBERT and the Google SQuAD 2.0 + MMFT are the current state-of-the-art models,  but they are unpublished.", "labels": [], "entities": [{"text": "CoQA test set", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.9200705687204996}, {"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9170347849527994}, {"text": "BERT", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9963364601135254}, {"text": "ConvBERT", "start_pos": 239, "end_pos": 247, "type": "DATASET", "confidence": 0.9229763150215149}]}, {"text": " Table 2: The results on the QuAC test set of single  models. Our BERT w/ 2-ctx model ranked 1st among  all unpublished and published models on the leader- board at the submission time (March 7, 2019). The  ConvBERT and Bert-FlowDelta are the current state- of-the-art models, but they are unpublished.", "labels": [], "entities": [{"text": "QuAC test set", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.8724329471588135}, {"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9944784641265869}, {"text": "ConvBERT", "start_pos": 207, "end_pos": 215, "type": "DATASET", "confidence": 0.8884061574935913}]}, {"text": " Table 3: The results with the number of previous con- texts on the development set of QuAC and CoQA (F 1  score)", "labels": [], "entities": [{"text": "F 1  score", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9658595323562622}]}, {"text": " Table 4: Ablation study on the development set of  QuAC and CoQA (F 1 score)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9824997186660767}, {"text": "F 1 score", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9657147725423177}]}]}