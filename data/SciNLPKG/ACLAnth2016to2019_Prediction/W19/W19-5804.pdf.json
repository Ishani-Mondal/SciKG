{"title": [{"text": "An ELMo-inspired approach to SemDeep-5's Word-in-Context task", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a submission to the Word-in-Context competition for the IJ-CAI 2019 SemDeep-5 workshop.", "labels": [], "entities": [{"text": "IJ-CAI 2019 SemDeep-5 workshop", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.6107126772403717}]}, {"text": "The task is to determine whether a given focus word is used in the same or different senses in two contexts.", "labels": [], "entities": []}, {"text": "We took an ELMo-inspired approach similar to the baseline model in the task description paper, where contex-tualized representations are obtained for the focus words and a classification is made according to the degree of similarity between these representations.", "labels": [], "entities": []}, {"text": "Our model had a few simple differences, notably joint training of the forward and backward LSTMs, a different choice of states for the contextualized representations and anew similarity measure for them.", "labels": [], "entities": []}, {"text": "These changes yielded a 3.5% improvement on the ELMo baseline.", "labels": [], "entities": [{"text": "ELMo baseline", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.845629870891571}]}], "introductionContent": [{"text": "Traditional word embedding systems such as word2vec () assign each word a single fixed embedding.", "labels": [], "entities": []}, {"text": "One weakness of these systems is that they are not well suited for representing words which have multiple meanings, as their embeddings are forced to occupy a point in the vector space which corresponds to some combination of these meanings.", "labels": [], "entities": []}, {"text": "Much attention has been given to developing systems which can assign a word an embedding specific to the sense in which it is used in a given context, among others).", "labels": [], "entities": []}, {"text": "Such a system could bethought of as yielding \"sense embeddings\" rather than word embeddings.", "labels": [], "entities": []}, {"text": "Some sense embedding systems have shown advantages over traditional word embeddings, performing better on contextual word similarity tasks) and relational similarity tasks (.", "labels": [], "entities": []}, {"text": "One of the greatest successes has been the ELMo system) whose contextual embeddings were used to obtain state-of-the-art results on six NLP tasks.", "labels": [], "entities": []}, {"text": "The Word-in-Context (WiC) dataset provides an opportunity to evaluate sense embedding systems by testing their ability to discriminate between finely-grained meanings of a word.", "labels": [], "entities": [{"text": "Word-in-Context (WiC) dataset", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.5290243923664093}]}, {"text": "Each instance in the dataset consists of two sentences which both contain a certain \"focus\" word.", "labels": [], "entities": []}, {"text": "The instances must be classified according to whether the focus word is used in the same sense in the two sentences or not.", "labels": [], "entities": []}, {"text": "There are two main approaches to producing sense-specific embeddings.", "labels": [], "entities": []}, {"text": "The first is to learn a number of embeddings for each word which correspond to its discrete senses, known as multi-prototype embeddings.", "labels": [], "entities": []}, {"text": "The second is to dynamically create a unique embedding fora word for every context it appears in, which attempts to capture the particular shade of meaning the word has in that context.", "labels": [], "entities": []}, {"text": "Notable examples of these \"contextualized word embedding\" systems include context2vec () and ELMo (.", "labels": [], "entities": []}, {"text": "The ELMo 1 baseline system in the task description paper (Pilehvar and CamachoCollados, 2019) and our system can both bethought of as having three components: the LSTM-based) language model; the \"contextualization\" component, in which contextualized embeddings for the focus words are obtained using the language model; and the \"classification\" component, where some similarity measure between the two contextualized embeddings is calculated and a positive classification is made if it is above a threshold learned on the training set.", "labels": [], "entities": []}, {"text": "In ELMo 1 , the language model is as described in (, the contextualized embeddings are the hidden states of the first LSTM layer at the focus word's position, and the similarity measure is cosine similarity.", "labels": [], "entities": []}, {"text": "In sections 2.1, 2.2 and 2.3, we will describe how these three components operate in our system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results with different system configurations. All results listed were obtained with the same  training run of the language model trained on Wikipedia 2018. \"Predictor\" refers to the use of the x  vectors used for predicting missing words, while \"hidden\" refers to the outputs of the first LSTM layer  for both directions concatenated.", "labels": [], "entities": [{"text": "Predictor", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9737932085990906}, {"text": "predicting missing words", "start_pos": 223, "end_pos": 247, "type": "TASK", "confidence": 0.8730363647143046}]}]}