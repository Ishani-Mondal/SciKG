{"title": [{"text": "IIT-KGP at MEDIQA 2019: Recognizing Question Entailment using Sci-BERT stacked with a Gradient Boosting Classifier", "labels": [], "entities": [{"text": "IIT-KGP at MEDIQA 2019", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.8200623542070389}, {"text": "Recognizing Question Entailment", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.7173011898994446}]}], "abstractContent": [{"text": "The number of people turning to the Internet to search fora diverse range of health-related subjects continues to grow and with this multitude of information available, duplicate questions become more frequent and finding the most appropriate answers becomes problematic.", "labels": [], "entities": []}, {"text": "This issue is important for question-answering platforms as it complicates the retrieval of all information relevant to the same topic, particularly when questions similar in essence are expressed differently, and answering a given medical question by retrieving similar questions that are already answered by human experts seems to be a promising solution.", "labels": [], "entities": []}, {"text": "In this paper we present our novel approach to detect question entailment by determining the type of question asked rather than focusing on the type of the ailment given.", "labels": [], "entities": [{"text": "detect question entailment", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.804281214872996}]}, {"text": "This unique methodology makes the approach robust towards examples which have different ailment names but are synonyms of each other.", "labels": [], "entities": []}, {"text": "Also it enables us to check entailment at a much more fine-grained level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Seeking health-related information is one of the top activities of todays online users via both personal computers and mobile devices.", "labels": [], "entities": [{"text": "Seeking health-related information", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8890803853670756}]}, {"text": "In all, 80 percent of Internet users, or about 93 million Americans, have searched fora health-related topic online, according to a study released on 16th July 2018 by the Pew Internet & American Life Project .Thats up from 62 percent of Internet users who said they went online to research health topics in 2001, the Washington research firm found.", "labels": [], "entities": [{"text": "Pew Internet & American Life Project", "start_pos": 172, "end_pos": 208, "type": "DATASET", "confidence": 0.7831587493419647}, {"text": "Washington research firm", "start_pos": 318, "end_pos": 342, "type": "DATASET", "confidence": 0.9416122635205587}]}, {"text": "China () also has 194.76 million Internet health users in 2016, increased 28.0% compared with that in 2015.", "labels": [], "entities": []}, {"text": "Despite the widespread need, the search engines often fail in returning relevant and trustworthy health information ().", "labels": [], "entities": []}, {"text": "In this paper we try to bridge this gap by predicting entailment between questions.", "labels": [], "entities": []}, {"text": "We particularly tackle this problem by checking entailment of a given consumer health question (CHQ) with most similar Frequently Asked Question (FAQ).", "labels": [], "entities": []}, {"text": "Given two general English sentences this Question Entailment system can conclude whether answer of one question implies the other question's answer.", "labels": [], "entities": [{"text": "Question Entailment", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6786804050207138}]}, {"text": "Q1: \"Can you mail me patient information about Glaucoma, I was recently diagnosed and want to learn all I can about the disease.\"", "labels": [], "entities": []}, {"text": "Q2: \"What is glaucoma?\"", "labels": [], "entities": []}, {"text": "In the above two questions the answer of Q1 implies the answer of Q2.", "labels": [], "entities": [{"text": "Q1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.8105541467666626}]}, {"text": "(Entailment) Detecting Question Entailment is a challenging task as it involves an amalgamation of tasks like Question Answering and Textual Entailment ( ).", "labels": [], "entities": [{"text": "Detecting Question Entailment", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.8595360318819681}, {"text": "Question Answering", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8095324337482452}]}, {"text": "Question answering is used to generate answers for both the questions and then checking textual entailment between the answers to give predictions possibly integrating Named Entity Recognition(NER) to our advantage.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.81557297706604}, {"text": "Named Entity Recognition(NER)", "start_pos": 168, "end_pos": 197, "type": "TASK", "confidence": 0.7645447999238968}]}, {"text": "In this paper, we experiment on the MEDIQA 2019 task  by presenting an all-together different approach QSpider which overcomes these challenges by detecting question types instead of treating it like a pure Textual entailment or Question answering task.", "labels": [], "entities": [{"text": "MEDIQA 2019 task", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.6218928893407186}, {"text": "Question answering task", "start_pos": 229, "end_pos": 252, "type": "TASK", "confidence": 0.8564738631248474}]}, {"text": "Attempts have been made to tackle this problem, the most notable one being) which is the baseline for this task.", "labels": [], "entities": []}, {"text": "The Baseline method uses supervised methods like SVM, Logistic Regression, Naive Bayes and used manual feature engineering but it fails to explore over the semantic space of the sentence.", "labels": [], "entities": []}, {"text": "In this paper, we propose our model QSpider to tackle this problem.", "labels": [], "entities": []}, {"text": "QSpider is a staged system consisting of state-of-the-art model Sci-BERT used as a multi-class classifier aimed at capturing both question types and semantic relations stacked with a Gradient Boosting Classifier which checks for entailment.", "labels": [], "entities": [{"text": "QSpider", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8360974788665771}]}, {"text": "QSpider achieves an accuracy score of 68.4% which outperforms the baseline model (54.1%) by an accuracy score of 14.3%.", "labels": [], "entities": [{"text": "QSpider", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8336355090141296}, {"text": "accuracy score", "start_pos": 20, "end_pos": 34, "type": "METRIC", "confidence": 0.9810165464878082}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9995636343955994}]}], "datasetContent": [{"text": "The objective of this task is to identify entailment between two questions in the context of Question Answering.", "labels": [], "entities": [{"text": "identify entailment between two questions", "start_pos": 33, "end_pos": 74, "type": "TASK", "confidence": 0.8548401236534119}, {"text": "Question Answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7517854869365692}]}, {"text": "We use the following definition of question entailment: Question A entails a Question B if every answer to B is also a complete or partial answer to A.", "labels": [], "entities": [{"text": "question entailment", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7060382515192032}]}, {"text": "So, basically we need to predict, given two questions, if they entail each other or not.", "labels": [], "entities": []}, {"text": "The question pairs in validation and hidden test set had its first question a Consumer asked Health Question (CHQ) and second question a Frequently Asked Question (FAQ).", "labels": [], "entities": []}, {"text": "Upon doing an elementary analysis of the task dataset, we observe there are examples in validation and test set where medical entities are not in same form (either synonyms or abbreviation) in both questions but they still entail each other and vice versa.", "labels": [], "entities": []}, {"text": "We additionally used an annotated corpus of consumer health questions () to build our question type prediction classifier.", "labels": [], "entities": [{"text": "question type prediction classifier", "start_pos": 86, "end_pos": 121, "type": "TASK", "confidence": 0.6934521123766899}]}, {"text": "The corpus consists of 1,467 consumer-generated requests for disease information, containing a total of 2,937 questions.", "labels": [], "entities": []}, {"text": "The dataset has these requests classified into 13 question types or classes namely: Anatomy, Cause, Complication, Diagnosis, Information, Management, Manifestation, Other effects, PersonOrg, Prognosis, Susceptibility, Other, Not Disease.", "labels": [], "entities": [{"text": "Anatomy", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9551960229873657}, {"text": "Prognosis", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.9466765522956848}]}], "tableCaptions": [{"text": " Table 1: Dataset Statistics : Positive means Entailment  & Negative means Not Entailment.", "labels": [], "entities": []}, {"text": " Table 2: Accuracy results for various models.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990190267562866}]}, {"text": " Table 3: Number of Correct and Wrong predictions  made by Sci-BERT on the task dataset. Positive means  Entailment & Negative means Not Entailment.", "labels": [], "entities": []}]}