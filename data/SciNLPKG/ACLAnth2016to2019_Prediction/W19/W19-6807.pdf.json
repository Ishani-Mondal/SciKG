{"title": [{"text": "Sentence-Level Adaptation for Low-Resource Neural Machine Translation", "labels": [], "entities": [{"text": "Sentence-Level Adaptation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8011343479156494}, {"text": "Low-Resource Neural Machine Translation", "start_pos": 30, "end_pos": 69, "type": "TASK", "confidence": 0.6414669528603554}]}], "abstractContent": [{"text": "Current neural machine translation (NMT) approaches achieve state-of-the-art accuracy in high-resource contexts.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 8, "end_pos": 40, "type": "TASK", "confidence": 0.8419562379519144}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9902147054672241}]}, {"text": "However, NMT requires a great deal of parallel data to deliver acceptable results; thus, it is currently unsuited for translating in low-resource contexts (especially when compared to phrase-based approaches).", "labels": [], "entities": [{"text": "translating", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.9749308228492737}]}, {"text": "We propose a method that better leverages the limited data available in such low-resource settings by adapting the model for each sentence at inference time.", "labels": [], "entities": []}, {"text": "A general NMT model is trained on the limited training data; then, for each test sentence, its parameters are fine-tuned over a subset of similar sentences extracted from the training set.", "labels": [], "entities": []}, {"text": "We experiment with various similarity metrics to extract these similar sentences.", "labels": [], "entities": []}, {"text": "It is observed that the sentence-adapted models achieve slightly increased BLEU scores compared to standard neural approaches on a Xhosa-English dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9991706609725952}]}], "introductionContent": [{"text": "Neural machine translation (NMT) () has become the primary paradigm in machine translation literature.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7991659343242645}, {"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7614771723747253}]}, {"text": "NMT aims to learn an end-to-end neural model to optimize translation performance by generalizing machine translation as a sequence-to-sequence machine learning problem.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7918508052825928}, {"text": "generalizing machine translation", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.6953491469224294}]}, {"text": "The first NMT systems were built with recurrent neural networks based on encoderdecoder architectures. and proposed the use of attention mechanisms to translate better by considering the context in which particular target words occur with respect to the source contexts.", "labels": [], "entities": []}, {"text": "Recently, transformers () have been shown to achieve state-of-the-art performance across various high-resource language pairs.", "labels": [], "entities": []}, {"text": "The strength of this approach lies in processing large amounts of parallel data and quickly learning from aligned translations without pre-defined linguistic rules.", "labels": [], "entities": []}, {"text": "NMT directly models the probability of a target-language sentence given aligned sourceand target-language sentences and does not need to train separate language models and alignment models like statistical machine translation (SMT) ().", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8000839352607727}, {"text": "statistical machine translation (SMT)", "start_pos": 194, "end_pos": 231, "type": "TASK", "confidence": 0.7693034559488297}]}, {"text": "The unavailability of large parallel corpora for most language pairs, however, is a ubiquitous problem.", "labels": [], "entities": []}, {"text": "These are only available fora handful of resource-rich languages, and in limited domains such as news reports or parliamentary/congressional proceedings.", "labels": [], "entities": []}, {"text": "Neural approaches to MT in general are datahungry and therefore tend to perform inadequately in low-resource contexts.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9954953193664551}]}, {"text": "Thus, improving NMT for low-resource languages has been a topic of recent interest.", "labels": [], "entities": [{"text": "NMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9489657878875732}]}, {"text": "While unsupervised NMT ( has been suggested to reduce NMT's need for aligned translations, it does not perform effectively for low-resource languages.", "labels": [], "entities": []}, {"text": "Present practices in the domain leverage the strength of preliminary word-level translation models, which do notwork well.", "labels": [], "entities": [{"text": "word-level translation", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.6790163815021515}]}, {"text": "However, transfer learning from high-resource parallel datasets (, as well as data augmentation through pivot corpora (, trans-lating monolingual data (, and/or copying data from source to target side () have proven effective in such cases.", "labels": [], "entities": []}, {"text": "Our method attempts to better leverage limited data by adapting parameters for each sentence at inference time.", "labels": [], "entities": []}, {"text": "This is carried out by finetuning () the parameters of an NMT model over a subset of training sentences which are similar to a given test sentence.", "labels": [], "entities": []}, {"text": "By contrast, existing NMT systems tend to employ parameters which are unchanged for any given test sentence after training or continued training.", "labels": [], "entities": []}, {"text": "There exists evidence that customising an NMT model for each test sentence gives it a better chance of producing correct translations).", "labels": [], "entities": []}, {"text": "In our model, for every test sentence, a unique subset of similar training sentences is retrieved.", "labels": [], "entities": []}, {"text": "This training-sentence subset is used to fine-tune the base model at inference time.", "labels": [], "entities": []}, {"text": "We experiment with string-based similarity and representation-based similarity to retrieve similar sentences; precision, recall, and Levenshtein distance are used for the former, and cosine similarity on word embeddings is used for the latter.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9994575381278992}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9981125593185425}, {"text": "Levenshtein distance", "start_pos": 133, "end_pos": 153, "type": "METRIC", "confidence": 0.8947235345840454}]}, {"text": "A combination of these is used to create the final subset of similar sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "The learning rate for adaptation \u03b1 A essentially dictates how much fine-tuning the NMT system receives during adaptation.", "labels": [], "entities": []}, {"text": "Each language has a different ideal adaptation rate, so we perform a sweep and report our findings in.", "labels": [], "entities": []}, {"text": "It is clear that trying to learn very aggressively from the adaptation subset results in a decrease in performance.", "labels": [], "entities": []}, {"text": "Trying to adjust the weights of the network too much with respect to the loss function might result in disregarding some local minima from consideration, resulting in an adverse effect.", "labels": [], "entities": []}, {"text": "It is also found that setting \u03b1 A too low also results in a slight score decrease, so finding the optimal \u03b1 A is crucial.", "labels": [], "entities": [{"text": "score decrease", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.9467678964138031}]}, {"text": "It is observed that, in this case, an \u03b1 A of 0.0004 best suits our objective.", "labels": [], "entities": [{"text": "\u03b1 A", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.7218977808952332}]}, {"text": "Note that this is similar to the training learning rate \u03b1 T of 0.0005, and that the other best-performing \u03b1 A values are similar to \u03b1 T as well.", "labels": [], "entities": [{"text": "training learning rate \u03b1 T", "start_pos": 33, "end_pos": 59, "type": "METRIC", "confidence": 0.7397001802921295}]}, {"text": "To investigate what types of sentences are retrieved by our similarity metrics from Section 3.2, we run a script which retrieves the most similar training sentences (per-metric) fora randomly chosen test sentence in English.", "labels": [], "entities": []}, {"text": "The most similar sentences per-metric, as well as their similarity/distance scores, are shown in.", "labels": [], "entities": [{"text": "similarity/distance scores", "start_pos": 56, "end_pos": 82, "type": "METRIC", "confidence": 0.9065505266189575}]}, {"text": "Note that this sentence similarity process is run for only the source language, Xhosa, and that this set of similar sentences in English is retrieved solely to demonstrate what types of sentences these similarity metrics choose in general.", "labels": [], "entities": []}, {"text": "Notably, precision and recall sometimes result in different similar sentences for the same n-gram orders.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9991766810417175}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9989135265350342}]}, {"text": "Unigram precision and unigram recall retrieve largely distinct sentences with very different scores, though there is often overlap: unigram recall, bigram precision, and bigram recall return the same sentence as most similar.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9276167750358582}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.7212121486663818}]}, {"text": "Trigram precision and recall return similar sentences that are distinct from the previous n-gram orders; the precision and recall sentences are the same in this case, but not always.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9546368718147278}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.996966540813446}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9989665746688843}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9890393018722534}]}, {"text": "Thus, using different n-gram orders-and precision as well as recall within each n-gram order-can feasibly return different similar sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9933933615684509}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9955832362174988}]}, {"text": "We thus keep all of these similarity metrics in our similar-sentence subset.", "labels": [], "entities": []}, {"text": "Cosine similarity retrieves a sentence which has a similar general tone to the test sentence, as well as a similar topic (the story of creation), but otherwise the n-grams are quite different.", "labels": [], "entities": []}, {"text": "This seems to be beneficial, for it demonstrates that we retrieve sentences which do not necessarily have the same words as the sentence on which we perform inference, but which have commonalities with respect to some supralinguistic or semantic feature(s).", "labels": [], "entities": []}, {"text": "This trend also holds for other sentences in the test set for which we retrieved similar sentences, so it does generally seem to return related sentences.", "labels": [], "entities": []}, {"text": "Levenshtein distance, in contrast, does not seem to return a useful similar sentence in this example.", "labels": [], "entities": []}, {"text": "There are few n-gram or morphemic matches in common between the test and similar sentences, and the meaning of the retrieved sentence bears little resemblance to that of the test sentence.", "labels": [], "entities": []}, {"text": "In general, the Levenshtein distance seems useful in retrieving similar sentences with different inflections of the same words primarily when there exists another sentence with similar unigrams in the same order as the test sentence (i.e., it works primarily when two sentences exist that are already very lexically similar).", "labels": [], "entities": []}, {"text": "In the future, it would perhaps it would be more beneficial to run Levenshtein distance on subwords after performing a BPE operation, rather than on characters.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 67, "end_pos": 87, "type": "METRIC", "confidence": 0.6953282654285431}]}, {"text": "As this metric only comprises a small fraction of the similar-sentence subset on which we adapt, it should be inconsequential if some sentences are not particularly relevant from this metric.", "labels": [], "entities": []}, {"text": "If they are relevant, however, it will be quite beneficial, so we keep these sentences in our similar-sentence adaptation set regardless.", "labels": [], "entities": []}, {"text": "We observe that sometimes, a sentence with zero or negligible score is also returned by one of the metrics.", "labels": [], "entities": []}, {"text": "As an extension, thresholding the", "labels": [], "entities": [{"text": "thresholding", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.9655504822731018}]}], "tableCaptions": [{"text": " Table 1: English-Xhosa Bible dataset at a glance.", "labels": [], "entities": [{"text": "English-Xhosa Bible dataset", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.7896842757860819}]}, {"text": " Table 2: Learning rate during adaptation (\u03b1A) vs. BLEU  scores in the Xhosa\u2192English translation task. Note: \u03b1T =  0.0005.", "labels": [], "entities": [{"text": "Learning rate during adaptation (\u03b1A)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.8971865773200989}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.998744010925293}, {"text": "Xhosa\u2192English translation task", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.6497142076492309}, {"text": "\u03b1T", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9580415487289429}]}, {"text": " Table 3: Evaluation of Xhosa\u2192English translation systems.", "labels": [], "entities": []}, {"text": " Table 4: Sample translations comparing unadapted and adapted output. Notably poor translations are highlighted in red bold,  whereas notably good translations are highlighted in blue italics.", "labels": [], "entities": []}, {"text": " Table 5: Investigation of the constituent features of our  BLEU scores for Xhosa\u2192English translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9957002401351929}]}]}