{"title": [{"text": "Evaluating the Supervised and Zero-shot Performance of Multi-lingual Translation Models", "labels": [], "entities": [{"text": "Multi-lingual Translation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6880342364311218}]}], "abstractContent": [{"text": "We study several methods for full or partial sharing of the decoder parameters of multilingual NMT models.", "labels": [], "entities": []}, {"text": "Using only the WMT 2019 shared task parallel datasets for training, we evaluate both fully supervised and zero-shot translation performance in 110 unique translation directions.", "labels": [], "entities": [{"text": "WMT 2019 shared task parallel datasets", "start_pos": 15, "end_pos": 53, "type": "DATASET", "confidence": 0.9062955379486084}]}, {"text": "We use additional test sets and re-purpose evaluation methods recently used for unsupervised MT in order to evaluate zero-shot translation performance for language pairs where no gold-standard parallel data is available.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9703081250190735}]}, {"text": "To our knowledge, this is the largest evaluation of multilingual translation yet conducted in terms of the total size of the training data we use, and in terms of the number of zero-shot translation pairs we evaluate.", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6874568164348602}]}, {"text": "We conduct an in-depth evaluation of the translation performance of different models , highlighting the trade-offs between methods of sharing decoder parameters.", "labels": [], "entities": []}, {"text": "We find that models which have task-specific decoder parameters outperform models where decoder parameters are fully shared across all tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-lingual translation models, which can map from multiple source languages into multiple target languages, have recently received significant attention because of the potential for positive transfer between high-and low-resource language pairs, and because of the potential efficiency gains enabled by translation models which share parameters across many languages (.", "labels": [], "entities": []}, {"text": "Multi-lingual models which share parameters across tasks can also perform zero-shot translation, translating between language pairs for which no parallel training data is available ().", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 74, "end_pos": 95, "type": "TASK", "confidence": 0.7124912738800049}]}, {"text": "Although multi-task models have recently been shown to achieve positive transfer for some combinations of NLP tasks, in the context of MT, multi-lingual models do not universally outperform models trained to translate in a single direction when sufficient training data is available.", "labels": [], "entities": [{"text": "MT", "start_pos": 135, "end_pos": 137, "type": "TASK", "confidence": 0.988919198513031}]}, {"text": "However, the ability to do zero-shot translation maybe of practical importance in many cases, as parallel training data is not available for most language pairs (.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6850647032260895}]}, {"text": "Therefore, small decreases in the performance of supervised pairs maybe admissible if the corresponding gain in zero-shot performance is large.", "labels": [], "entities": []}, {"text": "In addition, zeroshot translation can be used to generate synthetic training data for low-or zero-resource language pairs, making it a practical alternative to the bootstrapping by back-translation approach that has recently been used to build completely unsupervised MT systems (.", "labels": [], "entities": [{"text": "zeroshot translation", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7376961410045624}]}, {"text": "Therefore, understanding the trade-offs between different methods of constructing multi-lingual MT systems is still an important line of research.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9417969584465027}]}, {"text": "Deep sequence to sequence models have become the established state-of-the-art for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7939825654029846}]}, {"text": "The dominant paradigm continues to be models divided into roughly three high-level components: embeddings, which map discrete tokens into real-valued vectors, encoders, which map sequences of vectors into an intermediate representation, and decoders, which use the representation from an encoder, combined with a dynamic representation of the current state, and output a sequence of tokens in the target language conditioned upon the encoder's representation of the input.", "labels": [], "entities": []}, {"text": "For multi-lingual systems, any combination of encoder and/or decoder parameters can potentially be shared by groups of tasks, or duplicated and kept private for each task..", "labels": [], "entities": []}, {"text": "We can share all parameters across all target tasks, or we can create a unique set of decoder parameters for each task (outer dashed line).", "labels": [], "entities": []}, {"text": "Alternatively, we can create unique attention parameters for each task, while sharing the final feed-forward layers (inner dotted lines).", "labels": [], "entities": []}, {"text": "The possiblility of including an embedding for the target task is visualized at the bottom of the diagram.", "labels": [], "entities": []}, {"text": "Illustration modeled after Sachan and Neubig (2018).", "labels": [], "entities": []}, {"text": "Our work builds upon recent research on manyto-one, one-to-many, and many-to-many translation models.", "labels": [], "entities": []}, {"text": "We are interested in evaluating manyto-many models under realistic conditions, including: 1.", "labels": [], "entities": []}, {"text": "A highly imbalanced amount of training data available for different language pairs.", "labels": [], "entities": []}, {"text": "2. Avery diverse set of source and target languages.", "labels": [], "entities": []}, {"text": "3. Training and evaluation data from many domains.", "labels": [], "entities": []}, {"text": "We focus on multi-layer transformer models (, which achieve state-ofthe-art performance on large scale MT and NLP tasks.", "labels": [], "entities": [{"text": "MT and NLP tasks", "start_pos": 103, "end_pos": 119, "type": "TASK", "confidence": 0.8212767392396927}]}, {"text": "We study four ways of building multi-lingual translation models.", "labels": [], "entities": []}, {"text": "Importantly, all of the models we study can do zero-shot translation: translating between language pairs for which no parallel data was seen at training time.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 47, "end_pos": 68, "type": "TASK", "confidence": 0.6939840912818909}]}, {"text": "The models use training data from 11 distinct languages 1 , with supervised data available from the WMT19 news-translation task for 22 of the 110 unique translation directions 2 . This leaves 88 translation directions for which no parallel data is available.", "labels": [], "entities": [{"text": "WMT19 news-translation task", "start_pos": 100, "end_pos": 127, "type": "DATASET", "confidence": 0.9263990720113119}]}, {"text": "We try to evaluate zero-shot translation performance on all of these additional directions.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.4842137396335602}]}, {"text": "Target Language Specification Although the embedding and encoder parameters of a multilingual system maybe shared across all languages without any special modification to the model, decoding from a multi-lingual model requires a means of specifying the desired output language.", "labels": [], "entities": [{"text": "Target Language Specification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6057215829690298}]}, {"text": "Previous work has accomplished this in different ways: \u2022 pre-pending a special target-language token to the input ( \u2022 using an additional embedding vector for the target language (Lample and Conneau, 2019) \u2022 using unique decoders for each target language ( \u2022 partially sharing some of the decoder parameters while keeping others unique to each target language However, to the best of our knowledge, no sideby-side comparison of these approaches has been conducted.", "labels": [], "entities": []}, {"text": "We therefore train models which are identical except for the way that decoding into different target languages is handled, and conduct a large-scale evaluation.", "labels": [], "entities": []}, {"text": "We use only the language pairs and official parallel data released by the WMT task organisers, meaning that all of our systems correspond to the constrained setting of the WMT shared task, and our experimental settings should thus be straightforward to replicate.", "labels": [], "entities": [{"text": "WMT task organisers", "start_pos": 74, "end_pos": 93, "type": "DATASET", "confidence": 0.7781112591425577}, {"text": "WMT shared task", "start_pos": 172, "end_pos": 187, "type": "TASK", "confidence": 0.6695324182510376}]}], "datasetContent": [{"text": "All experiments are conducted using the transformer-base configuration of with the relevant modifications for each system discussed in the previous section.", "labels": [], "entities": []}, {"text": "We use a shared sentencepiece 3 segmentation model with 32000 pieces.", "labels": [], "entities": []}, {"text": "We use all available parallel data from the WMT19 news-translation task for training, with the exception of commoncrawl, which we found to be very noisy after manually checking a sample of the data, and paracrawl, which we use only for EN-FI and EN-LT . We train each model on two P100 GPUs with an individual batch size of up to 2048 tokens.", "labels": [], "entities": [{"text": "WMT19 news-translation task", "start_pos": 44, "end_pos": 71, "type": "DATASET", "confidence": 0.8613047003746033}, {"text": "paracrawl", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9019150137901306}]}, {"text": "Gradients are accumulated over 8 mini-batches and parameters are updated synchronously, meaning that our effective batch size is 2 * 2048 * 4 = 16384 tokens per iteration.", "labels": [], "entities": []}, {"text": "Because the task pair for: Training dataset statistics for our multilingual NMT experiments.", "labels": [], "entities": []}, {"text": "# seen is the total number of segments seen during training.", "labels": [], "entities": []}, {"text": "# available is the number of unique segments available in the parallel training datasets.", "labels": [], "entities": []}, {"text": "# epochs is the number of passes made over the available training data -when this is < 1, the available training data was only partially seen.", "labels": [], "entities": [{"text": "epochs", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9643115401268005}]}, {"text": "% budget is the percentage of the training budget allocated to this pair of tasks.", "labels": [], "entities": []}, {"text": "each mini-batch is sampled according to our policy weights and (fixed) random seed, and each iteration consists of 8 unique mini-batches, a single parameter update can potentially contain information from up to 8 unique task pairs.", "labels": [], "entities": []}, {"text": "We train each model for 100,000 iterations without early stopping, which takes about 40 hours per model.", "labels": [], "entities": []}, {"text": "When evaluating we always use the final model checkpoint (i.e. the model parameters saved after 100,000 iterations).", "labels": [], "entities": []}, {"text": "We use our in-house research NMT system, which is heavily based upon OpenNMT-py ().", "labels": [], "entities": []}, {"text": "The sampling policy weights were specified manually by looking at the amount of available data for each pair, and estimating the difficulty of each translation direction.", "labels": [], "entities": []}, {"text": "The result of the sampling policy is that lower resource language pairs are upsampled significantly.", "labels": [], "entities": []}, {"text": "summarizes the statistics for each language pair.", "labels": [], "entities": []}, {"text": "Note that the data in each row represents a pair of tasks, i.e. the total number of segments seen for EN-CS is split evenly between EN\u2192CS, and CS\u2192EN.", "labels": [], "entities": []}, {"text": "Because we train for only 100,000 iterations, we do not see all of the available training data for some high-resource language pairs.", "labels": [], "entities": []}, {"text": "With the exception of the system which prepends a target task token to each input, the input to each model is identical.", "labels": [], "entities": []}, {"text": "Each experimental setting is mutually exclusive, i.e. in the EMB setting we do not prepend task tokens, and in the ATTN setting we do not use task embeddings.", "labels": [], "entities": [{"text": "EMB setting", "start_pos": 61, "end_pos": 72, "type": "DATASET", "confidence": 0.9153865873813629}]}, {"text": "plots the validation performance during training on one of our validation datasets.", "labels": [], "entities": [{"text": "validation", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.9691308736801147}]}, {"text": "The language embeddings from the EMB system are visualized in figure 3.", "labels": [], "entities": [{"text": "EMB system", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.9553825557231903}]}, {"text": "We evaluate the performance of our models in four ways.", "labels": [], "entities": []}, {"text": "First, we check performance on the supervised pairs using dev and test sets from the WMT shared task.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.7243032058080038}]}, {"text": "We then try to evaluate zeroshot translation performance in several ways.", "labels": [], "entities": [{"text": "zeroshot translation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7022797167301178}]}, {"text": "We use the TED talks multi-parallel dataset () to create gold sets for all zero-shot pairs that occur in the TED talks corpus, and evaluate on those pairs.", "labels": [], "entities": [{"text": "TED talks multi-parallel dataset", "start_pos": 11, "end_pos": 43, "type": "DATASET", "confidence": 0.8359202742576599}, {"text": "TED talks corpus", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.84374072154363}]}, {"text": "We also try two ways of evaluating zero-shot translation without gold data.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6418092846870422}]}, {"text": "In the first, we do round-trip translation SRC \u2192 PIVOT \u2192 SRC , and measure performance on the (SRC , SRC) pair -this method is labeled   ZERO-SHOT PIVOT.", "labels": [], "entities": [{"text": "translation SRC \u2192 PIVOT \u2192 SRC", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.59844904144605}]}, {"text": "In the second, we use parallel evaluation datasets from the WMT shared tasks (consisting of (SRC, REF) pairs), and translate SRC \u2192 PIVOT \u2192 TRG , then measure performance on the resulting (TRG , REF) pairs (see below for more details), where the pivot and target language pair is a zero-shot translation task -this method is labeled ZERO-SHOT PARALLEL PIVOT.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.5939756433169047}]}, {"text": "lists the WMT evaluation dataset that we use for each language pair.", "labels": [], "entities": [{"text": "WMT evaluation dataset", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.6799948811531067}]}, {"text": "In the ZERO-SHOT PIVOT setting, the reference side of the dataset is used as input.", "labels": [], "entities": []}, {"text": "shows global results for all parallel tasks and all zero-shot tasks, by system.", "labels": [], "entities": []}, {"text": "Global scores are obtained by concatenating the segmented outputs for each translation direction, and computing the BLEU score against the corresponding concatenated, segmented reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9990953207015991}]}, {"text": "The results in table 3 are thus tokenized BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9813073873519897}]}, {"text": "We conduct an additional evaluation on some of the language pairs from the TED Talks multiparallel corpus, which has recently been used for the training and evaluation of multi-lingual models.", "labels": [], "entities": [{"text": "TED Talks multiparallel corpus", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.8540636152029037}]}, {"text": "We filter the dev and test sets of this corpus to find segments which have translations for all of EN, FR, RU, TR, DE, CS, LT, FI, and are at least 20 characters long, resulting in 606 segments.", "labels": [], "entities": [{"text": "FI", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.847964346408844}]}, {"text": "Because this corpus is preprocessed, we first de-tokenize and de-escape punctuation using sacremoses 8 . We then evaluate zero-shot translation for all possible pairs which do not occur in our parallel training data, aggregate results are shown in the third row of table 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training dataset statistics for our multilingual  NMT experiments. # seen is the total number of seg- ments seen during training. # available is the num- ber of unique segments available in the parallel training  datasets. # epochs is the number of passes made over  the available training data -when this is < 1, the avail- able training data was only partially seen. % budget is  the percentage of the training budget allocated to this  pair of tasks.", "labels": [], "entities": []}, {"text": " Table 2: The WMT evaluation dataset used for each  language pair.", "labels": [], "entities": [{"text": "WMT evaluation dataset", "start_pos": 14, "end_pos": 36, "type": "DATASET", "confidence": 0.7889084815979004}]}, {"text": " Table 3: Overall results for supervised and zero-shot  tasks. Tokenized BLEU scores are computed by con- catenating all of the hypotheses for all translation di- rections, and computing BLEU with respect to the  concatenated references. We use the sentencepiece- segmented hypotheses and references to avoid issues  with tokenization of multi-lingual hypotheses and ref- erences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9724942445755005}, {"text": "BLEU", "start_pos": 187, "end_pos": 191, "type": "METRIC", "confidence": 0.9981181621551514}]}, {"text": " Table 4: Results for all task pairs in the WMT 2019  news-translation shared task where parallel training  data is available.", "labels": [], "entities": [{"text": "WMT 2019  news-translation shared task", "start_pos": 44, "end_pos": 82, "type": "DATASET", "confidence": 0.6814241766929626}]}, {"text": " Table 5: Zero-shot translation results for RU\u2192*\u2192RU  Note that BLEU scores are computed by translating", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9988960027694702}]}, {"text": " Table 6: Out of 110 pivot translation tasks, how many  failed the language identification check?", "labels": [], "entities": [{"text": "language identification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.7460449039936066}]}]}