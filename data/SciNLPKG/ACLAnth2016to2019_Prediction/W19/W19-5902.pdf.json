{"title": [{"text": "Improving Interaction Quality Estimation with BiLSTMs and the Impact on Dialogue Policy Learning", "labels": [], "entities": [{"text": "Improving Interaction Quality Estimation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8320010304450989}]}], "abstractContent": [{"text": "Learning suitable and well-performing dialogue behaviour in statistical spoken dialogue systems has been in the focus of research for many years.", "labels": [], "entities": []}, {"text": "While most work which is based on reinforcement learning employs an objective measure like task success for modelling the reward signal, we use a reward based on user satisfaction estimation.", "labels": [], "entities": []}, {"text": "We propose a novel estimator and show that it outperforms all previous estimators while learning temporal dependencies implicitly.", "labels": [], "entities": []}, {"text": "Furthermore, we apply this novel user satisfaction estimation model live in simulated experiments where the satisfaction estimation model is trained on one domain and applied in many other domains which cover a similar task.", "labels": [], "entities": [{"text": "user satisfaction estimation", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6597420076529185}]}, {"text": "We show that applying this model results in higher estimated satisfaction, similar task success rates and a higher robustness to noise.", "labels": [], "entities": []}], "introductionContent": [{"text": "One prominent way of modelling the decisionmaking component of a spoken dialogue system (SDS) is to use (partially observable) Markov decision processes ((PO)MDPs) (.", "labels": [], "entities": [{"text": "spoken dialogue system (SDS)", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.688325305779775}]}, {"text": "There, reinforcement learning (RL)) is applied to find the optimal system behaviour represented by the policy \u03c0.", "labels": [], "entities": [{"text": "reinforcement learning (RL))", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.5772679388523102}]}, {"text": "Task-oriented dialogue systems model the reward r, used to guide the learning process, traditionally with task success as the principal reward component;).", "labels": [], "entities": []}, {"text": "An alternative approach proposes user satisfaction as the main reward component ().", "labels": [], "entities": []}, {"text": "However, the applied statistical user satisfaction estimator heavily relies on handcrafted temporal features.", "labels": [], "entities": [{"text": "user satisfaction estimator", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.5334041515986124}]}, {"text": "Furthermore, the impact of the estimation performance on the resulting dialogue policy remains unclear.", "labels": [], "entities": []}, {"text": "In this work, we propose a novel LSTM-based user satisfaction reward estimator that is able to learn the temporal dependencies implicitly and compare the performance of the resulting dialogue policy with the initially used estimator.", "labels": [], "entities": [{"text": "LSTM-based user satisfaction reward estimator", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.610739815235138}]}, {"text": "Optimising the dialogue behaviour to increase user satisfaction instead of task success has multiple advantages: 1.", "labels": [], "entities": []}, {"text": "The user satisfaction is more domainindependent as it can be linked to interaction phenomena independent of the underlying task ().", "labels": [], "entities": []}, {"text": "2. User satisfaction is favourable over task success as it represents more accurately the user's view and thus whether the user is likely to use the system again in the future.", "labels": [], "entities": []}, {"text": "Task success has only been used as it has shown to correlate well with user satisfaction ().", "labels": [], "entities": []}, {"text": "Based on previous work by, the interaction quality (IQ)-a less subjective version of user satisfaction 1 -will be used for estimating the reward.", "labels": [], "entities": [{"text": "interaction quality (IQ)-", "start_pos": 31, "end_pos": 56, "type": "METRIC", "confidence": 0.8579167127609253}]}, {"text": "The estimation model is thus based on domain-independent, interaction-related features which do not have any information available about the goal of the dialogue.", "labels": [], "entities": []}, {"text": "This allows the reward estimator to be applicable for learning in unseen domains.", "labels": [], "entities": []}, {"text": "The originally applied IQ estimator heavily relies on handcrafted temporal features.", "labels": [], "entities": [{"text": "IQ estimator", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9577039480209351}]}, {"text": "In this work, we will present a deep learning-based IQ estimator that utilises the capabilities of recurrent neural networks to get rid of all handcrafted fea-tures that encode temporal effects.", "labels": [], "entities": [{"text": "IQ estimator", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.8271905779838562}]}, {"text": "By that, these temporal dependencies maybe learned instead.", "labels": [], "entities": []}, {"text": "The applied RL framework is shown in Figure 1.", "labels": [], "entities": [{"text": "RL", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9340550303459167}]}, {"text": "Within this setup, both IQ estimators are used for learning dialogue policies in several domains to analyse their impact on general dialogue performance metrics.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: in Section 2, related work is presented focusing on dialogue learning and the type of reward that is applied.", "labels": [], "entities": [{"text": "dialogue learning", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.8062831163406372}]}, {"text": "In Section 3, the interaction quality is presented and how it is used in the reward model.", "labels": [], "entities": []}, {"text": "The deep learning-based interaction quality estimator proposed in this work is then described in detail in Section 4 followed by the experiments and results both of the estimator itself and the resulting dialogue policies in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The proposed BiLSTM IQ estimator is both trained and evaluated on the LEGO corpus and applied within the IQ reward estimation framework ( on several domains within a simulated environment.", "labels": [], "entities": [{"text": "BiLSTM IQ estimator", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.5485371152559916}, {"text": "LEGO corpus", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.9374223351478577}, {"text": "IQ reward estimation", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8245604435602824}]}], "tableCaptions": [{"text": " Table 2: Performance of the proposed LSTM-based  variants with the traditional cross-validation setup. Due  to overlapping sub-dialogues in the train and test sets,  the performance of the LSTM-based models achieve  unrealistically high performance.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the proposed LSTM-based  variants with the dialogue-wise cross-validation setup.  The models by", "labels": [], "entities": []}, {"text": " Table 2. Due to  the way the task is framed (one label for each sub- dialogue), memorising effects may be observed  with the traditional cross-validation setup that has  been used in previous work. Hence, the results  in", "labels": [], "entities": []}, {"text": " Table 4: Statistics of the domains the IQ reward esti- mator is trained on (LetsGo) and applied to (rest).", "labels": [], "entities": []}]}