{"title": [{"text": "Lifelong and Interactive Learning of Factual Knowledge in Dialogues", "labels": [], "entities": [{"text": "Lifelong and Interactive Learning of Factual Knowledge in Dialogues", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.5951271189583672}]}], "abstractContent": [{"text": "Dialogue systems are increasingly using knowledge bases (KBs) storing real-world facts to help generate quality responses.", "labels": [], "entities": []}, {"text": "However , as the KBs are inherently incomplete and remain fixed during conversation, it limits dialogue systems' ability to answer questions and to handle questions involving entities or relations that are not in the KB.", "labels": [], "entities": []}, {"text": "In this paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations.", "labels": [], "entities": []}, {"text": "With more knowledge accumulated overtime, they will be able to learn better and answer more questions.", "labels": [], "entities": []}, {"text": "Our empirical evaluation shows that CILK is promising.", "labels": [], "entities": [{"text": "CILK", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.4493308663368225}]}], "introductionContent": [{"text": "Dialogue systems, including question-answering (QA) systems are now commonly used in practice.", "labels": [], "entities": []}, {"text": "Early such systems were built mainly based on rules and information retrieval techniques (.", "labels": [], "entities": []}, {"text": "Recent deep learning models) learn from large corpora.", "labels": [], "entities": []}, {"text": "However, since they do not use explicit knowledge bases (KBs), they often suffer from generic and dull responses (.", "labels": [], "entities": []}, {"text": "KBs have been used to deal with the problem ().", "labels": [], "entities": []}, {"text": "Many task-oriented dialogue systems) also use KBs to support information-seeking conversations.", "labels": [], "entities": []}, {"text": "One major shortcoming of existing systems that use KBs is that the KBs are fixed once the dialogue systems are deployed.", "labels": [], "entities": []}, {"text": "However, it is almost impossible for the initial KBs to contain all possible knowledge that the user may ask, not to mention that new knowledge appears constantly.", "labels": [], "entities": []}, {"text": "It is thus highly desirable for dialogue systems to learn by themselves while in use, i.e., learning on the job in lifelong learning . Clearly, the system can (1) extract more knowledge from the Web or other sources, and (2) learn directly from users during conversations.", "labels": [], "entities": []}, {"text": "This paper focuses on the latter and makes an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) to give the dialogue system the ability to acquire/learn new knowledge from the user during conversation.", "labels": [], "entities": []}, {"text": "Specifically, it focuses on learning new knowledge interactively from the user when the system is unable to answer a user's WHquestion.", "labels": [], "entities": [{"text": "WHquestion", "start_pos": 124, "end_pos": 134, "type": "DATASET", "confidence": 0.7396716475486755}]}, {"text": "The acquired new knowledge makes the system better able to answer future user questions, and no longer be limited by the fixed knowledge provided by the human developers.", "labels": [], "entities": []}, {"text": "The type of knowledge that the CILK engine focuses on is the facts that can be expressed as triples, (h, r, t), which means that the head entity hand the tail entity t can be linked by the relation r.", "labels": [], "entities": []}, {"text": "An example of a fact is (Boston, LocatedInCountry, USA), meaning that Boston is located in USA.", "labels": [], "entities": [{"text": "Boston, LocatedInCountry, USA)", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.891064465045929}]}, {"text": "This paper only develops the core engine.", "labels": [], "entities": []}, {"text": "It does not study other dialogue functions like response generation, semantic parsing, fact extraction from user utterances, entity linking, etc., which have been studied extensively before and are assumed to be available for use.", "labels": [], "entities": [{"text": "response generation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7376343160867691}, {"text": "semantic parsing", "start_pos": 69, "end_pos": 85, "type": "TASK", "confidence": 0.7304231971502304}, {"text": "fact extraction from user utterances", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.8382127463817597}, {"text": "entity linking", "start_pos": 125, "end_pos": 139, "type": "TASK", "confidence": 0.7314007729291916}]}, {"text": "Thus, this paper works only with structured queries (h, r, ?), e.g., meaning \"In what Country is Boston located ?,\" or (?, r, t), e.g., (?, PresidentOf, USA) meaning \"Who is the President of USA?\"", "labels": [], "entities": []}, {"text": "It assumes that a semantic parser is available that can convert natural language queries from users into query triples.", "labels": [], "entities": []}, {"text": "Similarly, it assumes an information extraction tool like OpenIE () is employed to extract facts as triples (h, r, t) from user's utterances during conversation.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7355894446372986}]}, {"text": "Building a full-fledged dialogue system that can also learn during conversation is a huge undertaking and is out of the scope of this paper.", "labels": [], "entities": []}, {"text": "We thus only investigate the core knowledge learning engine here.", "labels": [], "entities": []}, {"text": "We also assume that the user has good intentions (i.e., user answers questions with 100% conformity about the veracity of his/her facts) ; but is not omniscient (opposed to the teacher-student learning setup).", "labels": [], "entities": []}, {"text": "Problem Definition: Given a user query / question (h, r, ?) [or (?, r, t)], where rand h (or t) may not be in the KB (i.e., unknown), our goal is two-fold: (i) answering the user query or rejecting the query to remain unanswered in the case when the correct answer is believed to not exist in the KB and (ii) learning / acquiring some knowledge (facts) from the user to help the answering task.", "labels": [], "entities": []}, {"text": "We only focus on the setting where the query cannot be answered directly with the current KB and need inference over existing facts, as considering structured query, it's trivial to retrieve the answer if the answer triple is already in KB.", "labels": [], "entities": []}, {"text": "We further distinguish two types of queries: (1) closed-world queries, where h (or t) and rare known to the KB, and (2) open-world queries, where either one or both h (or t) and rare unknown to the KB.", "labels": [], "entities": []}, {"text": "It is easy to see that the problem is essentially a lifelong learning problem , where each query to be processed is a task and the knowledge gained is retained in the KB.", "labels": [], "entities": []}, {"text": "To process anew query/task, the knowledge learned and accumulated from the past queries can be leveraged.", "labels": [], "entities": []}, {"text": "For each new open-world query, the proposed approach works in two steps: Step 1 -Interact with the user: It converts open-world queries (2) to closed-world queries (1) by asking the user questions related to h (or t) and r to make them known to the KB (added to KB).", "labels": [], "entities": []}, {"text": "The reason for the conversion will be clear below.", "labels": [], "entities": [{"text": "conversion", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.9642624855041504}]}, {"text": "The user answers, called supporting facts (SFs), are the new knowledge to be added to KB.", "labels": [], "entities": []}, {"text": "This step is also called interactive knowledge learning.", "labels": [], "entities": [{"text": "interactive knowledge learning", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.6143839756647745}]}, {"text": "Note, closed-world queries (1) do not need this step.", "labels": [], "entities": []}, {"text": "Step 2 -Infer the query answer: It solves closed-world queries (1) by inferring the query answer.", "labels": [], "entities": []}, {"text": "The main idea is to use each entity e in the KB to form a candidate triple (h, r, e) (or (e, r, t)), Figure 1: An example of interactive learning and inference.", "labels": [], "entities": []}, {"text": "Note that CILK only works with triples.", "labels": [], "entities": []}, {"text": "Each triple above is assumed to be extracted from the sentence after it.", "labels": [], "entities": []}, {"text": "Ask for Clue and Ask for Entity Fact are interaction query types, discussed in Sec.", "labels": [], "entities": [{"text": "Ask for Entity Fact", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.48848388344049454}]}, {"text": "3. SF denotes supporting fact.", "labels": [], "entities": [{"text": "SF", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.9927181005477905}]}, {"text": "The entity e with the highest score is predicted as the answer of the query.", "labels": [], "entities": []}, {"text": "Scoring each candidate is modeled as a knowledge base completion (KBC) problem.", "labels": [], "entities": []}, {"text": "KBC aims to infer new facts (knowledge) from existing facts in a KB and is defined as a link prediction problem: Given a query triple, (e, r, ?) [or (?, r, e)], it predicts a tail entity t true [head entity h true ] which makes the query triple true and thus should be added to the KB.", "labels": [], "entities": [{"text": "link prediction", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7299024313688278}]}, {"text": "KBC makes the closed-world assumption that h, rand tare all known to exist in the KB ().", "labels": [], "entities": [{"text": "KBC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9656256437301636}]}, {"text": "This is not suitable for knowledge learning in conversations because in a conversation, the user can ask or say anything, which may contain entities and relations that are not in the KB.", "labels": [], "entities": [{"text": "knowledge learning", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7790647149085999}]}, {"text": "CILK removes the closed-world assumption and allows all h (or t) and/or r to be unknown (not in the KB).", "labels": [], "entities": []}, {"text": "Step 1 above basically asks the user questions to make h (or t) and/or r known to the KB.", "labels": [], "entities": []}, {"text": "Then, an existing KBC model as a query inference model can be applied to retrieve an answer entity from KB.", "labels": [], "entities": []}, {"text": "CILK acquires supporting facts SF1 and SF2 to accomplish the goal of knowledge learning and utilizes these pieces of knowledge along with existing KB facts to answer the user query (i.e., to infer over the query relation \"LocatedInCountry\").", "labels": [], "entities": []}, {"text": "CILK aims to achieve these two sub-goals.", "labels": [], "entities": [{"text": "CILK", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9381402134895325}]}, {"text": "The new knowledge (SFs) is added to the KB for future use 2 . We evaluate CILK using two real-world KBs: Nell and WordNet and obtain promising results.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 114, "end_pos": 121, "type": "DATASET", "confidence": 0.9183188676834106}]}], "datasetContent": [{"text": "As indicated earlier, the proposed CILK system is best used in a multi-user environment, so it naturally observes many more query triples (hence, accumulates more facts) from different users overtime.", "labels": [], "entities": []}, {"text": "Presently CILK fulfills its knowledge learning requirement by only adding the supporting facts into the KB.", "labels": [], "entities": []}, {"text": "The predicted query triples are not added as they are unverified knowledge.", "labels": [], "entities": []}, {"text": "However, in practice, CILK can store these predicted triples in the KB as well after checking their correctness through cross-verification while conversing with other users in some future related conversations by smartly asking them.", "labels": [], "entities": []}, {"text": "Note that CILK may not verify its prediction with the same user who asked the question/query q because he/she may not know the answer(s) for q.", "labels": [], "entities": []}, {"text": "However, there is no problem that it acquires the correct answer(s) of q when it asks q to some other user u in a future related conversation and u answers q.", "labels": [], "entities": []}, {"text": "At this point, CILK can incorporate q into its KB and also, train itself using triple q.", "labels": [], "entities": [{"text": "CILK", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.959097146987915}]}, {"text": "We do not address the issue here.", "labels": [], "entities": []}, {"text": "Evaluation of CILK with real users in a crowdsource based setup would be very difficult to conduct and prohibitively time-consuming (and expensive) as it needs a large number of real-time and continuous user interaction.", "labels": [], "entities": []}, {"text": "Thus, we design a simulated interactive environment for the evaluation.", "labels": [], "entities": []}, {"text": "We create a simulated user (a program) to interact with CILK, where the simulated user issues a query to CILK and CILK answers the query.", "labels": [], "entities": []}, {"text": "The (simulated) user has (1) a knowledge base (K u ) for answering questions from CILK, and (2) an query dataset (D q ) from which the user issues queries to CILK.", "labels": [], "entities": []}, {"text": "Here, D q consists of a set of structured query triples q of the form (e, r, ?) and (?, r, e) readable by CILK.", "labels": [], "entities": []}, {"text": "In practice, the user only issues queries to CILK, but cannot evaluate the performance of the system unless the user knows the answer.", "labels": [], "entities": []}, {"text": "To evaluate the performance of CILK on D q in the simulated setting, we also collect the answer set for each query q \u2208 D q (discussed shortly).", "labels": [], "entities": []}, {"text": "As CILK is supposed to perform continuous online knowledge acquisition and learning, we evaluate its performance on the streaming query dataset.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.7313562631607056}]}, {"text": "We assume that, CILK has been deployed with an initial knowledge base ) and the inference model M has been trained overall triples in Kb fora given number of epochs N init . We call Kb the base KB of CILK which serves as its knowledge base at the time point (t eval ) when our evaluation starts.", "labels": [], "entities": []}, {"text": "And the training process of M using triples in Kb is referred to as the initial training phase of CILK onwards.", "labels": [], "entities": [{"text": "CILK", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.7422704100608826}]}, {"text": "In the initial training phase, we randomly split Kb triples into a set of training triples D tr and a set of validation triples D vd with 9:1 ratio (we use \u03b1 = 0.9) and train M with D tr . D vd is used to tune model hyper-parameters and populate initial performance and threshold buffers P and T respectively.", "labels": [], "entities": []}, {"text": "D tr , D vd , P, and T get updated continuously after t eval in the online training and evaluation phase (with new acquired triples) during interaction with the simulated user.", "labels": [], "entities": [{"text": "T", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.9543164968490601}]}, {"text": "The relations and entities in Kb are regarded as known relations and known entities to CILK till t eval . Thus, the initial inference model M is trained and validated with triples involving only known relations and known entities (in Kb ).", "labels": [], "entities": [{"text": "CILK", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7485339641571045}]}, {"text": "During the online training and evaluation phase, CILK faces queries (from D q ) involving both known and unknown relations and entities.", "labels": [], "entities": []}, {"text": "More specifically, if a relation (entity) appearing in a query q \u2208 D q exists in Kb , we consider that query relation (entity) as known query relation (entity).", "labels": [], "entities": []}, {"text": "Otherwise, it is referred to as unknown query relation (entity).", "labels": [], "entities": []}, {"text": "We create simulated user's KB K u , base KB (K b ) and query dataset D q from two standard KB datasets: (1) WordNet ( and Nell ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9532263278961182}]}, {"text": "From each KB dataset, (Threshold) variants denoted ase\"X-BTr\" and last three (dataset sampling strategy) variants denoted as \"MaxTh-X\" and marked the highest H@1 and H@10 values (among each of the groups of four and three) in bold.", "labels": [], "entities": [{"text": "KB dataset", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8311180174350739}, {"text": "Threshold", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9452382922172546}]}, {"text": "Thus, some columns have at max.", "labels": [], "entities": []}, {"text": "two values marked bold (due to the two comparison groups).", "labels": [], "entities": []}, {"text": "MaxTh-BTr in the table is the version of CILK proposed in Sec.", "labels": [], "entities": [{"text": "MaxTh-BTr", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8624216318130493}]}, {"text": "3.  we first build a fairly large triple store and use it as the original KB (K org ) and then, create K u of user, base KB (K b ) of CILK and D q from K org , as discussed below shows the results).", "labels": [], "entities": []}, {"text": "Simulated User, Base KB Creation and Query Dataset Generation.", "labels": [], "entities": [{"text": "Base KB Creation", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.5451590816179911}, {"text": "Query Dataset Generation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.5957916478315989}]}, {"text": "In Nell, we found 150 relations with \u2265 300 triples, and we randomly selected 25 relations for D q . We shuffle the list of 25 relations, select 34% of them as unknown relations and consider the rest (66%) as known relations.", "labels": [], "entities": []}, {"text": "For each known relation r, we randomly shuffle the list of distinct triples for r, choose (maximum) 250 triples and randomly select 20% as test and add a randomly chosen subset of the rest of the triples along with the leftovers (not in the list of 250), into Kb and the other subset are added to K u (to provide supporting facts involving poorly learned known relations and/or entities, if asked [see).", "labels": [], "entities": []}, {"text": "For each unknown relation r, we remove all triples of r from K org , randomly choose 20% triples among them and reserve them as query triples for unknown r.", "labels": [], "entities": []}, {"text": "Rest 80% triples of unknown rare added to K u (for providing clues).", "labels": [], "entities": []}, {"text": "In this process, we also make sure that the query instances involving unknown rare excluded from K u . Thus, the user cannot provide the query triple itself as a clue to CILK (during inference) and also, to simulate the case that the user does not know the answer of its issued query.", "labels": [], "entities": [{"text": "CILK", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9439562559127808}]}, {"text": "Note, if the user cannot provide a clue for an unknown query relation or a fact for an unknown query entity (not likely), CILK will not be able to correctly answer the query.", "labels": [], "entities": []}, {"text": "At this point, D q consists of query triples involving both known and unknown relations, but all known entities.", "labels": [], "entities": []}, {"text": "To create queries in D q having unknown entities, we randomly choose 20% of the  entities in D q triples, remove all triples involving those entities from K org and add them to K u . Now, K org gets reduced to Kb (base KB).", "labels": [], "entities": []}, {"text": "Next, for each query triple (h, r, t) \u2208 D q , we convert the triple into ahead query q =(?, r, t) [or a tail query q =(h, r, ?)] by randomly deleting the head or tail entity.", "labels": [], "entities": []}, {"text": "We also collect the answer set for each q \u2208 D q based on observed triples in K org for CILK evaluation.", "labels": [], "entities": []}, {"text": "Note, the generated query triples (with answer entity) in D q are not directly in Kb or K u . The WordNet dataset being small, we use all its 18 relations for creating D q , K u , Kb following Nell.", "labels": [], "entities": [{"text": "WordNet dataset", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.9863510131835938}]}, {"text": "As mentioned earlier, the triples in Kb are randomly split into 90% training and 10% validation datasets for simulating initial training phase of CILK.", "labels": [], "entities": []}, {"text": "L2-regularization parameter set as 0.001.", "labels": [], "entities": []}, {"text": "Adam optimizer is used for optimization.", "labels": [], "entities": []}, {"text": "\u2022 CILK variants based on prediction threshold types, namely EntTh-BTr, RelTh-BTr, MinThBTr and MaxTh-BTr (see \u2022 CILK variants based on dataset sampling strategies: MaxTh-BTr (as explained above), MaxTh-EntTr and MaxTh-RelTr (see).", "labels": [], "entities": []}, {"text": "Given the query entity e and query relation r, MaxTh-EntTr only samples triples involving e and MaxTh-RelTr samples only triples involving r to build D tr and D vd . Note, if the sampled dataset D tr (D vd ) is \u2205, CILK skips online training (validation) steps for that session.", "labels": [], "entities": []}, {"text": "We use two common KBE evaluation metrics: mean reciprocal rank (MRR) and Hits@k (H@k).", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 42, "end_pos": 68, "type": "METRIC", "confidence": 0.955180029074351}]}, {"text": "MRR is the average inverse rank of the top ranked true answer entity for all queries (.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8112591505050659}, {"text": "inverse rank", "start_pos": 19, "end_pos": 31, "type": "METRIC", "confidence": 0.9417277276515961}]}, {"text": "Hits@k is the proportion of test queries for which the true answer entity has appeared in top-k (ranked) predictions.", "labels": [], "entities": []}, {"text": "Higher MRR and Hits@k indicate better performance.", "labels": [], "entities": [{"text": "MRR", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9978035092353821}, {"text": "Hits@k", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9465508659680685}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics [kwn = known, unk = unknown]", "labels": [], "entities": []}, {"text": " Table 3: Performance of CILK Threshold variants on Rejec-", "labels": [], "entities": [{"text": "Rejec", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.9142709374427795}]}, {"text": " Table 4: Overall Performance of MaxTh-BTr (CILK), vary-", "labels": [], "entities": []}, {"text": " Table 5: Performance of MaxTh-BTr (CILK) on test queries", "labels": [], "entities": []}]}