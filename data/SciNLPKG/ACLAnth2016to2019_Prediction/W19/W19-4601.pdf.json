{"title": [{"text": "Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings", "labels": [], "entities": [{"text": "Incremental Domain Adaptation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7794551253318787}, {"text": "Neural Machine Translation", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.6958684921264648}]}], "abstractContent": [{"text": "We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training.", "labels": [], "entities": [{"text": "generic neural machine translation", "start_pos": 59, "end_pos": 93, "type": "TASK", "confidence": 0.6677175983786583}, {"text": "human translations", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.6737643182277679}]}, {"text": "In this paper, we propose a novel query strategy for selecting \"unlabeled\" samples from anew domain based on sentence em-beddings for Arabic.", "labels": [], "entities": []}, {"text": "We accelerate the fine-tuning process of the generic model to the target domain.", "labels": [], "entities": []}, {"text": "Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain.", "labels": [], "entities": []}, {"text": "We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling base-line with our new approach, similar to active learning, using two small update sets for simulating the work of human translators.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.6823101490736008}, {"text": "Ar-to-En", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9937921166419983}]}, {"text": "For the prescribed setting we can save more than 50% of the annotation costs without loss in quality , demonstrating the effectiveness of our approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is the task of translating text from one language (source) to another (target) using, most commonly, Recurrent Neural Networks (RNN), specifically the Encoder-Decoder or Sequence-to-Sequence models ().", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7903571675221125}]}, {"text": "Recently, NMT has become a quite popular and effective alternative to traditional Phrase-Based Statistical Machine Translation (PBSMT) ().", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation (PBSMT)", "start_pos": 82, "end_pos": 134, "type": "TASK", "confidence": 0.6823889655726296}]}, {"text": "Major problems that arise include very high cost of training NMT models for new domains and that abundant parallel corpora are required for this task: the standard encoder-decoder models with attention have been shown to perform poorly in low-resource settings.", "labels": [], "entities": []}, {"text": "Sufficient data might not be available for all languages due to resource restrictions, particularly for resource-poor languages.", "labels": [], "entities": []}, {"text": "Hence, we are in need of cost-effective adaptation techniques that transfer existing knowledge to new domains as much as possible.", "labels": [], "entities": []}, {"text": "A recently proposed approach for domain adaptation filters generic corpora based on sentence embeddings of a potentially low amount of indomain samples to train domain-specific models from scratch (.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.8006028532981873}]}, {"text": "However, the problem of time-and resource-consuming training still remains which is unsuitable for incremental model updates.", "labels": [], "entities": []}, {"text": "Fine-tuning can accelerate the training process because it transfers knowledge from a pre-trained generic model to anew domain and, hence, requires less parallel training samples.", "labels": [], "entities": []}, {"text": "However, respective differences in contents and writing style can reduce machine translation quality, if they are not properly addressed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7757924795150757}]}, {"text": "Recent approaches include fine-tuning with mixed batches containing in-and out-of-domain samples ( and with different regularization methods for differing amounts of new samples for English \u2192 German and English \u2192 Russian (.", "labels": [], "entities": []}, {"text": "The findings of suggest that there is an \"approximately logarithmic relation between the size of in-domain training set and improvement in BLEU score\".", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 139, "end_pos": 149, "type": "METRIC", "confidence": 0.9824760556221008}]}, {"text": "We want to find out whether incremental model training can be accelerated using an advanced query strategy for sample selection.", "labels": [], "entities": []}, {"text": "Previous works on incremental machine translation include cache-based computer aided translation tools (), active learning techniques for interactive statistical machine translation (), interactive visualizations for understanding and manipulating attention weights and beam search parameters in NMT (, and domain adap-tation through user interactions.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.742728590965271}, {"text": "cache-based computer aided translation", "start_pos": 58, "end_pos": 96, "type": "TASK", "confidence": 0.7433292716741562}, {"text": "statistical machine translation", "start_pos": 150, "end_pos": 181, "type": "TASK", "confidence": 0.6417702436447144}]}, {"text": "In this work, we implement anew query strategy for selecting \"unlabeled\" instances from a target domain and investigate its effect on fine-tuning a generic NMT model.", "labels": [], "entities": []}, {"text": "We borrow techniques and terms from the active learning domain: a query strategy is a method for selecting instances from a pool of unlabeled data that lead to a high information gain when used for training the machine learning model under consideration.", "labels": [], "entities": []}, {"text": "Selected instances are labeled by an oracle which can be a human.", "labels": [], "entities": []}, {"text": "Iteratively including the most informative instances, labeled on demand, has been shown to increase the model performance while using the same amount of training data.", "labels": [], "entities": []}, {"text": "Our proposed methods for domain adaptation in NMT include query strategies that consider untranslated sentences as unlabeled instances.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7190573066473007}]}, {"text": "We simulate a human oracle by using parallel corpora in the evaluation, but we do not consider incremental updates for the query strategy.", "labels": [], "entities": []}, {"text": "This is of interest for crowdbased domain-adaptation with limited resources as described in (), in particular, because our method only requires monolingual data for filtering (see.", "labels": [], "entities": []}, {"text": "We compare random sampling as a na\u00a8\u0131vena\u00a8\u0131ve baseline strategy with our novel method based on distances between sentence embeddings.", "labels": [], "entities": []}, {"text": "We estimate the informativeness of instances from the target domain by comparing the distances of their sentence embeddings to the embeddings of the generic domain.", "labels": [], "entities": []}, {"text": "For computing the sentence embeddings, we present AraSIF: we adapted the methodology presented by, which is known to capture the semantics of sentences well, to work with Arabic.", "labels": [], "entities": []}, {"text": "In our experiments, we use existing parallel corpora for simulating human workers: The MEDAR 1 and GlobalVoices dataset are considered as new target domains which mainly concern the domain of climate change and politics, respectively.", "labels": [], "entities": [{"text": "MEDAR 1", "start_pos": 87, "end_pos": 94, "type": "DATASET", "confidence": 0.7565733790397644}, {"text": "GlobalVoices dataset", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.9267448782920837}]}, {"text": "The LDC Newswire parallel corpus is used as the dataset for training generic domain model.", "labels": [], "entities": [{"text": "LDC Newswire parallel corpus", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9613478183746338}]}, {"text": "We fine-tune this generic NMT model using different amounts of samples from anew domain and varying training epoch settings while observing the BLEU score () on a held-out in-domain test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.9791178107261658}]}, {"text": "Our hypothesis is that the proposed novel query strategy can effec-1 http://medar.info tively reduce the number of fine-tuning samples required without hampering the translation quality when compared to the baseline.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 provides an overview on related works, section 3 describes the NMT system and considered query strategies.", "labels": [], "entities": []}, {"text": "In section 4, we describe our experiment, and we report the results in section 5.", "labels": [], "entities": []}, {"text": "The results are discussed in section 6 and we conclude our work in section 7.", "labels": [], "entities": []}, {"text": "This result motivates us to study domain adaptation of NMT models rather than PBSMT models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, we use the LDC Newswire corpus (Munteanu and) and two publicly available datasets, MEDAR and GlobalVoices.", "labels": [], "entities": [{"text": "LDC Newswire corpus", "start_pos": 30, "end_pos": 49, "type": "DATASET", "confidence": 0.9705135027567545}, {"text": "Munteanu", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.6941717267036438}, {"text": "MEDAR", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.7805476188659668}, {"text": "GlobalVoices", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9496334791183472}]}, {"text": "The corpus statistics are summarized in: Kernel density estimates for the distributions of distances (cosine similarity) between sentence embeddings of each considered dataset and e ref , the mean of sentence embeddings of the dataset used for training of the generic translation model.", "labels": [], "entities": [{"text": "generic translation", "start_pos": 260, "end_pos": 279, "type": "TASK", "confidence": 0.7616463005542755}]}, {"text": "The gray dotted lines represent the 25% and the 75% percentile of the distance distribution for the generic model.", "labels": [], "entities": []}, {"text": "different genres to investigate whether our findings generalize irrespective of the domain of the fine-tuning set.", "labels": [], "entities": []}, {"text": "To reduce noise in the data, we clean the datasets by discarding instances with mixed tokens (i.e. English sentences containing Arabic words or Arabic sentences containing English words).", "labels": [], "entities": []}, {"text": "This step removes around 0.01%, 1.2%, and 10.14% of sentence pairs from LDC Newswire, MEDAR, and GlobalVoices datasets respectively.", "labels": [], "entities": [{"text": "LDC Newswire", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.9651540815830231}, {"text": "MEDAR", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.7623190879821777}, {"text": "GlobalVoices datasets", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.9788247346878052}]}, {"text": "Further preprocessing steps of our system pipeline include normalization and tokenization 4 of the sentences and generation of byte pair encodings (BPE) 5 (Sennrich et al., 2016) for the tokenized sentences and the vocabulary.", "labels": [], "entities": [{"text": "byte pair encodings (BPE) 5", "start_pos": 127, "end_pos": 154, "type": "METRIC", "confidence": 0.6270557429109301}]}, {"text": "We perform the simulation experiment with two new domain datasets and observe the impact of different parameters on domain adaptation of the generic NMT model using small amounts of new samples.", "labels": [], "entities": []}, {"text": "These can be considered to stem from human workers, e.g., professional translators or crowdworkers.", "labels": [], "entities": []}, {"text": "Considered parameters include the number of training samples in the update set n t and the number of training epochs e.", "labels": [], "entities": []}, {"text": "The number of epochs defines the number of training iterations: The number of Stochastic Gradient Descent (SGD) updates, denoted by T sgd , is computed by: where |S| is the mini-batch size which we set to 50 throughout our experiments, n t is the number of sentence pairs in the update set, and e is the number of epochs.", "labels": [], "entities": []}, {"text": "provides an overview of all considered configurations.", "labels": [], "entities": []}, {"text": "The update sets used for model adaptation are generated from either MEDAR or GlobalVoices dataset, after excluding a static test set of 100 sentences for each.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 25, "end_pos": 41, "type": "TASK", "confidence": 0.7670767307281494}, {"text": "MEDAR", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9161474704742432}, {"text": "GlobalVoices dataset", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9528937041759491}]}, {"text": "Both update sets are constrained to a maximum sample size of 400 to allow a fair comparison (this is the maximum size for MEDAR, see table 1).", "labels": [], "entities": [{"text": "MEDAR", "start_pos": 122, "end_pos": 127, "type": "DATASET", "confidence": 0.7036606669425964}]}, {"text": "Further, we assume that the amount of data from the new domain might be small due to resource constraints or scarcity.", "labels": [], "entities": []}, {"text": "For the random sampling case, we select all 400 samples from each of the datasets in a random order and use it to adapt our generic model for all parameter configurations in.", "labels": [], "entities": []}, {"text": "Samples 1 to 50 of the update set are used for training then t = 50 model for all epochs.", "labels": [], "entities": []}, {"text": "The model finetuning is continued with samples 51 to 100 for then t = 100 model for all epochs, and soon.", "labels": [], "entities": []}, {"text": "Considering the stochastic nature of SGD, we repeat the experiment 5 times and report the average scores on the respective test sets, instead of providing a point estimate.", "labels": [], "entities": [{"text": "SGD", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9805404543876648}]}, {"text": "We observe the training times on the update set and the BLEU scores) on the test set as a dependent variable.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9994864463806152}]}, {"text": "For our advanced sampling strategy, we select a subset of all training samples for both datasets using the filter mechanism described above.", "labels": [], "entities": []}, {"text": "We include a sentence s, if its cosine distance d toe ref is smaller than the 25% percentile (\u22120.208) or larger than the 75% percentile (0.317) of the generic distance distribution (see).", "labels": [], "entities": []}, {"text": "This leaves us with 135 fine-tuning samples for MEDAR and 169 for GlobalVoices from the original 400 samples.", "labels": [], "entities": [{"text": "MEDAR", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8637290000915527}, {"text": "GlobalVoices", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.967496395111084}]}, {"text": "We consider the same set of parameters than before with the difference that the size of the update set n t is limited to the reduced number of samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Considered combinations of update set sizes  (n t ) and SGD updates or iterations (T sgd ) used for fine- tuning.", "labels": [], "entities": [{"text": "fine- tuning", "start_pos": 110, "end_pos": 122, "type": "TASK", "confidence": 0.859809935092926}]}]}