{"title": [{"text": "ANU-CSIRO at MEDIQA 2019: Question Answering Using Deep Contextual Knowledge", "labels": [], "entities": [{"text": "ANU-CSIRO at MEDIQA 2019", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.7825034260749817}, {"text": "Question Answering", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.8604117035865784}]}], "abstractContent": [{"text": "We report on our system for textual inference and question entailment in the medical domain for the ACL BioNLP 2019 Shared Task, MEDIQA.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7116530686616898}, {"text": "question entailment", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.6663064956665039}, {"text": "MEDIQA", "start_pos": 129, "end_pos": 135, "type": "DATASET", "confidence": 0.5888961553573608}]}, {"text": "Textual inference is the task of finding the semantic relationships between pairs of text.", "labels": [], "entities": [{"text": "Textual inference", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7850073873996735}]}, {"text": "Question entailment involves identifying pairs of questions which have similar semantic content.", "labels": [], "entities": [{"text": "Question entailment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7742697894573212}]}, {"text": "To improve upon medical natural language inference and question en-tailment approaches to further medical question answering, we propose a system that incorporates open-domain and biomedical domain approaches to improve semantic understanding and ambiguity resolution.", "labels": [], "entities": [{"text": "medical question answering", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6747656861941019}, {"text": "semantic understanding", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.7270570695400238}, {"text": "ambiguity resolution", "start_pos": 247, "end_pos": 267, "type": "TASK", "confidence": 0.7990157604217529}]}, {"text": "Our models achieve 80% accuracy on medical natural language inference (6.5% absolute improvement over the original baseline), 48.9% accuracy on recognising medical question entail-ment, 0.248 Spearman's rho for question answering ranking and 68.6% accuracy for question answering classification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9993822574615479}, {"text": "medical natural language inference", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.5872126221656799}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9992204904556274}, {"text": "question answering ranking", "start_pos": 211, "end_pos": 237, "type": "TASK", "confidence": 0.8340831200281779}, {"text": "accuracy", "start_pos": 248, "end_pos": 256, "type": "METRIC", "confidence": 0.9991042017936707}, {"text": "question answering classification", "start_pos": 261, "end_pos": 294, "type": "TASK", "confidence": 0.9033078948656718}]}], "introductionContent": [{"text": "Medical health search is the second most searched thematic query, representing 5% of all queries on Google ().", "labels": [], "entities": []}, {"text": "However, many queries are semantically identical and are potentially already answered by experts ().", "labels": [], "entities": []}, {"text": "However, these questions may not be directly retrievable due to semantic ambiguity involving abbreviations (, patient colloquialism ( or esoteric terminology ().", "labels": [], "entities": []}, {"text": "Furthermore, in regards to disease, temporality is a key factor in determining the relevance of retrieved answers ().", "labels": [], "entities": []}, {"text": "For example, it is more appropriate to retrieve answers relating to the summer cold in the summer.", "labels": [], "entities": []}, {"text": "As a means to retrieve these questions that are already answered by experts, question entailment has been proposed to discern relationships between pairs of questions.", "labels": [], "entities": []}, {"text": "Recognising Question Entailment (RQE) is the task of determining the relationship between a question pair, RQE(Q 1 , Q 2 ), as either entailment or not entailment, where define question entailment as the situation where \"a question, Q 1 , entails another question, Q 2 , if every answer to Q 2 is also a complete or partial answer to Q 1 .\" Natural Language Inference (NLI) is determining the relationship between pairs of sentences, not just questions.", "labels": [], "entities": [{"text": "Recognising Question Entailment (RQE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7769990265369415}, {"text": "Natural Language Inference (NLI)", "start_pos": 341, "end_pos": 373, "type": "TASK", "confidence": 0.7929965058962504}]}, {"text": "NLI is the task of determining whether a hypothesis, H, is inferred (entailment), not inferred (contradiction) or neither (neutral), given a premise.", "labels": [], "entities": []}, {"text": "In the context of question answering (QA), it can be used to validate if the answer can be inferred from the question.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.8829372882843017}]}, {"text": "Though RQE and NLI have thrived in the opendomain setting, there are unique challenges in applying these tasks directly to the biomedical question answering field.", "labels": [], "entities": [{"text": "biomedical question answering field", "start_pos": 127, "end_pos": 162, "type": "TASK", "confidence": 0.7056429162621498}]}, {"text": "Previous models in the medical domain that used NLI and RQE relied on models which were shallowly bidirectional or rule-based approaches with shallow keyword matching techniques) which would not generalise well.", "labels": [], "entities": []}, {"text": "The MEDIQA) challenge, as part of the ACL BioNLP workshop, aims to further research efforts in NLI and RQE by introducing their applications to Biomedical QA.", "labels": [], "entities": [{"text": "ACL BioNLP workshop", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.8445740739504496}, {"text": "Biomedical QA", "start_pos": 144, "end_pos": 157, "type": "TASK", "confidence": 0.7387296855449677}]}, {"text": "In this paper, we detail our approach in MEDIQA which addresses some of the problems with biomedical text such as utilising deep contextual relationships between words within a sentence for semantic understanding and ambiguity associated with esoteric terminology, abbreviations, and patient colloquialism.", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.7412179112434387}]}, {"text": "We combine biomedical and open-domain approaches as a means to improve generalisation and bridge the gap between patient colloquialism and biomedical terminology.", "labels": [], "entities": [{"text": "generalisation", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.960004985332489}]}], "datasetContent": [{"text": "MEDIQA 2019 provides datasets to be used for three different tasks.", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.931780606508255}]}], "tableCaptions": [{"text": " Table 1: Hyperparamters used for each run for Tasks 1 & 2.", "labels": [], "entities": []}, {"text": " Table 2: Tokenisation statistics for all Tasks.", "labels": [], "entities": []}, {"text": " Table 3: Results for all 3 tasks in the MEDIQA shared task, additional post challenge runs are included. Note:  With the exception of Task 1, all post challenge runs were evaluated using the official evaluation script.", "labels": [], "entities": [{"text": "MEDIQA shared task", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.6028380592664083}]}, {"text": " Table 4: Common mistakes made by the baseline system in Task 1.", "labels": [], "entities": []}]}