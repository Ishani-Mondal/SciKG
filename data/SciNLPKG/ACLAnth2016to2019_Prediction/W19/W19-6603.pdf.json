{"title": [{"text": "Enhancing Transformer for End-to-end Speech-to-Text Translation", "labels": [], "entities": [{"text": "End-to-end Speech-to-Text Translation", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.5690155327320099}]}], "abstractContent": [{"text": "Neural end-to-end architectures have been recently proposed for spoken language translation (SLT), following the state-of-the-art results obtained in machine translation (MT) and speech recognition (ASR).", "labels": [], "entities": [{"text": "spoken language translation (SLT)", "start_pos": 64, "end_pos": 97, "type": "TASK", "confidence": 0.8308631082375845}, {"text": "machine translation (MT)", "start_pos": 150, "end_pos": 174, "type": "TASK", "confidence": 0.8499995112419129}, {"text": "speech recognition (ASR)", "start_pos": 179, "end_pos": 203, "type": "TASK", "confidence": 0.8396903693675994}]}, {"text": "Motivated by this contiguity, we propose an SLT adaptation of Transformer (the state-of-the-art architecture in MT), which exploits the integration of ASR solutions to cope with long input sequences featuring low information density.", "labels": [], "entities": [{"text": "SLT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.98699551820755}]}, {"text": "Long audio representations hinder the training of large models due to Transformer's quadratic memory complexity.", "labels": [], "entities": []}, {"text": "Moreover, for the sake of translation quality, handling such sequences requires capturing both short-and long-range dependencies between bi-dimensional features.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.970328152179718}]}, {"text": "Focusing on Trans-former's encoder, our adaptation is based on: i) downsampling the input with con-volutional neural networks, which enables model training on non cutting-edge GPUs, ii) modeling the bidimensional nature of the audio spectrogram with 2D components , and iii) adding a distance penalty to the attention, which is able to bias it towards short-range dependencies.", "labels": [], "entities": []}, {"text": "Our experiments show that our SLT-adapted Transformer outperforms the RNN-based baseline both in translation quality and training time, setting the state-of-the-art performance on six language directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural encoder-decoder models) with attention () is a general architecture that, by enabling to tackle sequence-to-sequence problems with a single endto-end model, achieved state-of-the-art results on machine translation (MT) ( and obtained increasingly good performance in automatic speech recognition.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 201, "end_pos": 225, "type": "TASK", "confidence": 0.8568612933158875}, {"text": "automatic speech recognition", "start_pos": 274, "end_pos": 302, "type": "TASK", "confidence": 0.6452455818653107}]}, {"text": "The advantages of end-to-end techniques, besides their conceptual simplicity, reside on the prevention of error propagation, and a reduced inference latency.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 106, "end_pos": 123, "type": "TASK", "confidence": 0.7085025608539581}]}, {"text": "Error propagation is particularly problematic for the SLT task (, in which MT would be significantly penalized by errors resulting from the previous ASR processing step.", "labels": [], "entities": [{"text": "Error propagation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.904176265001297}, {"text": "SLT task", "start_pos": 54, "end_pos": 62, "type": "TASK", "confidence": 0.9260885417461395}, {"text": "ASR processing", "start_pos": 149, "end_pos": 163, "type": "TASK", "confidence": 0.8789332807064056}]}, {"text": "For this reason, end-to-end solutions have been recently proposed) but, in terms of performance, they are still far behind the pipeline approach.", "labels": [], "entities": []}, {"text": "The reason of the worse performance for this task can be found in its intrinsic difficulty, as it inherits and combines the challenges of the two pipelined tasks.", "labels": [], "entities": []}, {"text": "Indeed, SLT models map audio features into words, like in ASR, but the input is mapped into text in a different target language, like in MT.", "labels": [], "entities": [{"text": "ASR", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9032144546508789}, {"text": "MT", "start_pos": 137, "end_pos": 139, "type": "TASK", "confidence": 0.7057453989982605}]}, {"text": "Thus, the problems of word reordering and ambiguous meaning typical of translation are combined with the ambiguity of speech signal and speaker variety.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.7388086020946503}]}, {"text": "One possible approach to deal with this task is to start from an MT solution and adapt it to speech input.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9692485928535461}]}, {"text": "Transformer () is an encoder-decoder architecture based on self-attention networks) that, because of its strong results, is the most popular architecture in MT, and is now used as abase for many NLP tasks.", "labels": [], "entities": [{"text": "MT", "start_pos": 157, "end_pos": 159, "type": "TASK", "confidence": 0.9584206938743591}]}, {"text": "While LSTMs are known to require long trainig time (, Transformer reduces the training time by performing parallel computation along all the time steps, similarly to convolutional neural networks (CNNs).", "labels": [], "entities": []}, {"text": "Despite the appealing advantages, the research on end-to-end SLT has focused so far on recurrent architectures, and only big industrial players have been able to train networks with many layers, many parameters, and additional synthetic data (.", "labels": [], "entities": [{"text": "SLT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9320846199989319}]}, {"text": "In fact, for computational and modeling reasons, the application of SANs to speech input has to face additional challenges compared to handling textual data.", "labels": [], "entities": []}, {"text": "In particular, these include: 1.", "labels": [], "entities": []}, {"text": "SANs have a memory complexity that is quadratic in the sequence length.", "labels": [], "entities": []}, {"text": "From a computational perspective, this becomes a problem when the input is an audio signal, which is typically represented as a very long sequence of log-filter-banks.", "labels": [], "entities": []}, {"text": "For the same utterance, this type of input is considerably longer than the corresponding textual representation fed to MT encoders.", "labels": [], "entities": [{"text": "MT encoders", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7344641983509064}]}, {"text": "2. The bidimensional dependencies along the time and frequency dimensions in the spectrogram (.", "labels": [], "entities": []}, {"text": "This 2-dimensional representation is more difficult to handle compared to the 1-dimensional input representation (i.e. along the time dimension only) processed by MT encoders.", "labels": [], "entities": []}, {"text": "3. The absence of an explicit bias towards the local context.", "labels": [], "entities": []}, {"text": "Differently from MT, modeling long-range dependencies between words is logically preceded, as the input is unsegmented, by modeling short-range dependencies between time-frames belonging to the same linguistic constituents ().", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9412484169006348}]}, {"text": "Focusing on these problems, in this paper we explore different adaptations of Transformer to the end-to-end SLT task.", "labels": [], "entities": [{"text": "SLT task", "start_pos": 108, "end_pos": 116, "type": "TASK", "confidence": 0.9266121983528137}]}, {"text": "Initially, we show that asis and with a comparable number of parameters, Transformer is not competitive with LSTM models.", "labels": [], "entities": []}, {"text": "In order to investigate the reasons of its lower performance, we posit that the problem lies in the inability of the Transformer encoder to properly model long audio input.", "labels": [], "entities": []}, {"text": "This hypothesis is checked by switching the encoders and decoders of the Transformer and LSTM architecture, which results in better performance when the Transformer decoder is preceded by the LSTM encoder.", "labels": [], "entities": []}, {"text": "These results inform and motivate our enhancements to the Transformer architecture.", "labels": [], "entities": []}, {"text": "To this aim, we proceed incrementally showing, through comparative experiments, that: 1.", "labels": [], "entities": []}, {"text": "Sequence compression with CNNs and downsampling enables effective audio encoding while allowing to train the system even on single GPUs; 2.", "labels": [], "entities": [{"text": "Sequence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7666192054748535}, {"text": "audio encoding", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7655428349971771}]}, {"text": "Modeling 2D dependencies produces more stable and better results; 3.", "labels": [], "entities": []}, {"text": "Biasing the encoder self-attention with a distance penalty improves translation quality.", "labels": [], "entities": []}, {"text": "Our experiments are run on different datasets covering different languages.", "labels": [], "entities": []}, {"text": "First, we evaluate our architecture on two relatively small corpora: Augmented Librispeech ( ) for English\u2192French and IWSLT 2018 for English\u2192German.", "labels": [], "entities": [{"text": "IWSLT 2018", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.7084380388259888}]}, {"text": "Then, we broaden the language coverage through experiments with MuST-C (Di), 1 a large multilingual SLT dataset recently released.", "labels": [], "entities": [{"text": "MuST-C (Di), 1 a large multilingual SLT dataset", "start_pos": 64, "end_pos": 111, "type": "DATASET", "confidence": 0.7044128423387354}]}, {"text": "This allows to validate our findings on six language directions (EnDe/Es/Fr/Pt/Ro/Ru).", "labels": [], "entities": []}, {"text": "Overall, our evaluation indicates that the proposed SLT-oriented adaptation of Transformer results in a model that significantly outperforms a strong end-to-end system both in translation quality and training speed.", "labels": [], "entities": [{"text": "SLT-oriented", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.9716200232505798}, {"text": "translation", "start_pos": 176, "end_pos": 187, "type": "TASK", "confidence": 0.9554799199104309}]}, {"text": "For the sake of results' replicability the code developed for the experiments described in this paper can be downloaded at http://github.com/mattiadg/ FBK-Fairseq-ST.", "labels": [], "entities": [{"text": "FBK-Fairseq-ST", "start_pos": 151, "end_pos": 165, "type": "DATASET", "confidence": 0.7721784114837646}]}], "datasetContent": [{"text": "We run our experiments on three SLT datasets, of which two comprise a single language direction and one comprises, then we aligned the resulting English sentences with the corresponding audio using Gentle, 5 a forced-aligner based on the Kaldi toolkit).", "labels": [], "entities": [{"text": "Gentle, 5", "start_pos": 198, "end_pos": 207, "type": "DATASET", "confidence": 0.8498470981915792}]}, {"text": "In order to improve the alignment quality we performed two successive steps of filtering.", "labels": [], "entities": []}, {"text": "In the first step, we removed all the talks whereat least 15% of the words have not been recognized by Gentle.", "labels": [], "entities": [{"text": "Gentle", "start_pos": 103, "end_pos": 109, "type": "DATASET", "confidence": 0.9592326879501343}]}, {"text": "In the second step, we removed from the remaining talks all the sentences with no recognized words.", "labels": [], "entities": []}, {"text": "For replicability of results, the corpus is released with a predefined train, validation and test split.", "labels": [], "entities": [{"text": "replicability", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9613941311836243}]}, {"text": "The corpora statistics are listed in and show that each language direction of MuST-C is considerably larger than the other 2 corpora.", "labels": [], "entities": []}, {"text": "For a fair comparison of the different architectures, we first set the parameters of the recurrent baseline (CNN+LSTM, \u00a73.1) similar to what reported in.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 109, "end_pos": 117, "type": "DATASET", "confidence": 0.7507676482200623}]}, {"text": "Then, we adjust the Transformer to have a number of parameters similar to the recurrent one (\u223c9.5M).", "labels": [], "entities": []}, {"text": "The CNNs have a 3\u00d73 kernel and 16 output filters.", "labels": [], "entities": []}, {"text": "The LSTMs in the baseline have a hidden size of 512, with 3 layers in the encoder and 2 in the decoder.", "labels": [], "entities": []}, {"text": "The initial encoder states are learnable parameters, while the initial decoder state is computed as the mean of the encoder states.", "labels": [], "entities": []}, {"text": "We found the learnable encoder states to be critical to reach convergence.", "labels": [], "entities": []}, {"text": "The Transformer models have 6 layers in both encoder and decoder, with layer size of 256, hidden size of 768 and 4 heads in multi-head attention.", "labels": [], "entities": []}, {"text": "To further asses the performance of our models, we also experiment with a BIG version with more parameters, featuring layer size 512, hidden size 1024, and 8 heads.", "labels": [], "entities": [{"text": "BIG", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9894736409187317}]}, {"text": "set dropout to 0.2 for CNN+LSTM and 0.1 for Transformer.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8186513384183248}]}, {"text": "No dropout is applied in the recurrent connections.", "labels": [], "entities": []}, {"text": "Training is performed using the Adam optimizer () with learning rate 0.001 for LSTM and 0.0002 for Transformer.", "labels": [], "entities": []}, {"text": "The learning rate is kept fixed for Transformer for the sake of a fair comparison with the baseline.", "labels": [], "entities": []}, {"text": "B-Transformer serves as a baseline to evaluate the impact of the proposed adaptations.", "labels": [], "entities": [{"text": "B-Transformer", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9359008073806763}]}, {"text": "We train our R-and S-Transformer models with and without distance penalty, either Gaussian or logarithmic.", "labels": [], "entities": []}, {"text": "We test all these configurations on the IWSLT and Librispeech corpora.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 40, "end_pos": 45, "type": "DATASET", "confidence": 0.9037156701087952}, {"text": "Librispeech corpora", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.8702076375484467}]}, {"text": "Then, due to the higher number of directions in the multilingual corpus, we only run experiments on it with the best-performing system.", "labels": [], "entities": []}, {"text": "Following (, we first train a model with the ASR part of each corpus and then we use it to initialize the weights of the SLT encoder.", "labels": [], "entities": []}, {"text": "All the experiments are run on a single GPU Nvidia 1080 Ti with 12G of RAM, and the code used for all the experiments is based on Pytorch (Paszke et al., ).", "labels": [], "entities": [{"text": "RAM", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.957402229309082}]}, {"text": "40-dimensional MFCC filter-banks were extracted from the audio signals of each dataset using window size of 25 ms and step size 10 ms.", "labels": [], "entities": []}, {"text": "The frame energy feature was additionally extracted from the LibriSpeech audio, similarly to).", "labels": [], "entities": [{"text": "LibriSpeech audio", "start_pos": 61, "end_pos": 78, "type": "DATASET", "confidence": 0.895648181438446}]}, {"text": "All texts were tokenized and split into characters.", "labels": [], "entities": []}, {"text": "Performance is evaluated with BLEU () at token level after aggregating the output characters into words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9990076422691345}]}, {"text": "6 Results and Discussion 6.1 Baseline.", "labels": [], "entities": []}, {"text": "As a first step, we want to evaluate our baseline B-Transformer against CNN+LSTM to understand the effectiveness 2D convolutional compression with Transformer.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.842419703801473}]}, {"text": "We ran the experiments with no pre-training, by pre-training only the encoder or both encoder and decoder.", "labels": [], "entities": []}, {"text": "As can be seen in, the best results with CNN+LSTM are obtained by pre-training only the encoder, while for B-Transformer the training is more unstable and this is reflected also in the results.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.7684712012608846}]}, {"text": "Considering the results of CNN+LSTM and the relatively good results of B-Transformer when pre-training only the encoders, we decided to follow this practice in all the following experiments.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.6723467310269674}]}, {"text": "When considering only the results with the pre-trained encoder, CNN+LSTM outperforms BTransformer by 4 BLEU points on Librispeech and 2.1 BLEU points on IWSLT.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.7952962517738342}, {"text": "BTransformer", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.7577065825462341}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9984323382377625}, {"text": "BLEU", "start_pos": 138, "end_pos": 142, "type": "METRIC", "confidence": 0.9988125562667847}, {"text": "IWSLT", "start_pos": 153, "end_pos": 158, "type": "DATASET", "confidence": 0.9764834642410278}]}, {"text": "To better understand the source of degradation for the B-Transformer, we performed an experiment switching encoder and decoder between the two architectures with pre-trained encoder (table 3) and evaluated them on Librispeech.", "labels": [], "entities": [{"text": "Librispeech", "start_pos": 214, "end_pos": 225, "type": "DATASET", "confidence": 0.9782312512397766}]}, {"text": "When using CNN+LSTM encoder, the Transformer decoder causes a degradation of 1.3 BLEU points, while having Transformer encoder and LSTM decoder causes a degradation of 5 points over CNN+LSTM.", "labels": [], "entities": [{"text": "CNN+LSTM encoder", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.8666523098945618}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9987711310386658}, {"text": "CNN+LSTM", "start_pos": 182, "end_pos": 190, "type": "DATASET", "confidence": 0.8706879615783691}]}, {"text": "Given these  results, the following experiments all focus on enhancing the B-Transformer encoder.", "labels": [], "entities": []}, {"text": "Despite the poor translation quality, exploring the Transformer is still interesting because of its reduced training time), which is reduced by a factor of 2 on IWSLT (67K vs 112K seconds) and even more on Librispeech (101K vs 248K seconds).", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.9562583565711975}, {"text": "IWSLT", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.6621330380439758}, {"text": "Librispeech", "start_pos": 206, "end_pos": 217, "type": "DATASET", "confidence": 0.92060387134552}]}, {"text": "These results show that input compression makes the training of Transformer feasible for SLT, but it does not result in immediate improvements over LSTMs.", "labels": [], "entities": [{"text": "input compression", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7281079143285751}, {"text": "SLT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9902054667472839}]}, {"text": "The previous experiments have shown that STransformer performs better than the other variants, and as such we report experiments on the larger MuST-C corpus only with S-Transformer and the two distance penalties.", "labels": [], "entities": [{"text": "MuST-C corpus", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.9106084406375885}]}, {"text": "S-Transformer outperforms CNN+LSTM on all the 6 language directions with gains from +0.5 to +1.6 BLEU points with log penalty and from +0.7 to +2.6 with Gaussian penalty.", "labels": [], "entities": [{"text": "CNN+LSTM", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.8557353019714355}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9978542923927307}]}, {"text": "Gaussian penalty generally achieves results only slightly better than the logarithmic one, except for the top improvements of +0.9 and +1.1 respectively on En\u2192Fr and En\u2192Pt.", "labels": [], "entities": []}, {"text": "To explain this difference, it is useful to recall that the parameters of the encoders of SLT models (including their Gaussian variances) are initialized from a model pre-trained on English ASR.", "labels": [], "entities": []}, {"text": "In particular, for the multilingual corpus we use the same model trained on the larger dataset.", "labels": [], "entities": []}, {"text": "The inherited variance from this model may affect differently the different target languages.", "labels": [], "entities": []}, {"text": "Experiments with a larger model (STransformer BIG) show further improvements from a minimum of 1.5 points for En\u2192Pt to a maximum of 3.8 points for En\u2192Fr with log penalty, while the poor results with Gaussian penalty confirm that it is less stable than the logarithmic one.", "labels": [], "entities": [{"text": "STransformer", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.6223486065864563}, {"text": "BIG", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.5635510683059692}]}, {"text": "The number of training iterations is also reduced to less than half of the previous experiments.", "labels": [], "entities": []}, {"text": "The improvements obtained in this experiment, up to 4.6 BLEU point in En\u2192Fr over the baseline, represent a step forward towards a translation quality that allows real-world applications for end-to-end SLT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9988767504692078}, {"text": "SLT", "start_pos": 201, "end_pos": 204, "type": "TASK", "confidence": 0.9437204003334045}]}, {"text": "To conclude, our experiments show that: i) our task-specific adaptations make the Transformer trainable for the SLT task, also on a single GPU; ii) when both short-range and 2D dependencies are explicitly addressed in the model, they allow it to outperform a strong baseline based on LSTMs; iii) the logarithmic distance penalty can be preferable over the Gaussian one because it does not require additional hyperparameter tuning and results in competitive performance.", "labels": [], "entities": [{"text": "SLT task", "start_pos": 112, "end_pos": 120, "type": "TASK", "confidence": 0.9368693828582764}]}], "tableCaptions": [{"text": " Table 1: Data statistics for IWSLT, Librispeech and our mul- tilingual corpus. Train, Valid and Test are numbers of sen- tence pairs.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.8892629146575928}, {"text": "Train", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9424111247062683}, {"text": "Valid", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.8899503350257874}, {"text": "Test", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.8900001049041748}]}, {"text": " Table 2: Speech translation results for the Librispeech and  IWSLT corpora wuth our two baseline models. A checkmark  on Enc (Dec) means that the encoder (decoder) has been pre- trainined.", "labels": [], "entities": [{"text": "Speech translation", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7433268427848816}, {"text": "IWSLT corpora", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8689672350883484}]}, {"text": " Table 3: Mixed-architecture experiments on Librispeech.", "labels": [], "entities": [{"text": "Librispeech", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9326897859573364}]}, {"text": " Table 4: Results on the Librispeech and IWSLT 2014 test set.  Differences wrt the baseline (CNN+LSTM) are statistically  significant (randomization test, p=0.05).", "labels": [], "entities": [{"text": "IWSLT 2014 test set", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8928506523370743}, {"text": "CNN+LSTM)", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.6524863019585609}]}, {"text": " Table 6: Results on six language pairs covered by the mul- tilingual corpus. LSTM is the CNN+LSTM model. Results  in columns 3-6 are computed with S-Transformer with loga- rithmic (log) or Gaussian (Gauss) distance penalty. Improve- ments over CNN+LSTM are statistically significant.", "labels": [], "entities": [{"text": "Improve- ments", "start_pos": 225, "end_pos": 239, "type": "METRIC", "confidence": 0.9622609416643778}]}]}