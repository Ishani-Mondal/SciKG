{"title": [{"text": "Improving Word Embeddings Using Kernel PCA", "labels": [], "entities": [{"text": "Improving Word Embeddings", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8685594399770101}]}], "abstractContent": [{"text": "Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections; however, they fail to do so with small datasets.", "labels": [], "entities": []}, {"text": "Extensions such as fastText reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew approach to warm-start embedding models with morphological information, in order to reduce training time and enhance their performance.", "labels": [], "entities": []}, {"text": "We use word embeddings generated using both word2vec and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices.", "labels": [], "entities": []}, {"text": "This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities.", "labels": [], "entities": []}, {"text": "Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time.", "labels": [], "entities": [{"text": "word similarity and analogy tasks", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.8166682124137878}, {"text": "accuracies", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.9784294366836548}]}, {"text": "Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies.", "labels": [], "entities": []}, {"text": "Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results .", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.7940033276875814}]}], "introductionContent": [{"text": "Continuous vector representations of words learned from unstructured text corpora are an effective way of capturing semantic relationships among words.", "labels": [], "entities": []}, {"text": "Approaches to computing word embeddings are typically based on the context of words, their morphemes, or corpus-wide cooccurrence statistics.", "labels": [], "entities": []}, {"text": "As of this writing, arguably the most popular approaches are the Word2Vec skip-gram model () and the fastText model ().", "labels": [], "entities": []}, {"text": "The skip-gram model generates embeddings based on windowed word contexts.", "labels": [], "entities": []}, {"text": "While it incorporates semantic information, it ignores word morphology.", "labels": [], "entities": []}, {"text": "Yet, the latter might be beneficial especially for morphologically rich languages such as German and Turkish.", "labels": [], "entities": []}, {"text": "therefore introduced fastText which builds on the Word2Vec approach but also incorporates morphology by considering sub-word units and representing a word by a sum of its character n-grams as well as the word itself.", "labels": [], "entities": []}, {"text": "To learn high-quality embeddings, Word2Vec requires huge text corpora with billions of words and still fails to generate high-quality vector representations for less frequent or unknown words.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9181036353111267}]}, {"text": "Although fastText improves the results by incorporating subword information, it still fails in many cases.", "labels": [], "entities": []}, {"text": "This is particularly evident in the news domain where frequently new words such as names occur overtime which, in turn, impacts the performance of downstream applications.", "labels": [], "entities": []}, {"text": "In this paper, we therefore propose an alternative approach which not only makes use of morphological information but also performs well when trained on smaller datasets or domains with rapidly changing vocabulary.", "labels": [], "entities": []}, {"text": "Research questions we answer in this paper are: 1.", "labels": [], "entities": []}, {"text": "Can high-quality word embeddings be trained on small datasets?", "labels": [], "entities": []}, {"text": "2. Can high-quality embeddings be generated for infrequent words?", "labels": [], "entities": []}, {"text": "3. Can the use of morphological information increase the efficiency of learning semantic and syntactic similarities?", "labels": [], "entities": []}, {"text": "proposed log-bilinear models to learn vector representations of words from the context in which they appear in large corpora.", "labels": [], "entities": []}, {"text": "These are the Continuous Bag-of-Words Model (CBOW) and the Continuous Skip-gram Model (skip-gram) which predict target words from source context words and source context words from target words, respectively.", "labels": [], "entities": []}, {"text": "An extension proposed by involves training lightweight log-bilinear language models with noise-contrastive estimation and achieves results comparable to the best previous models with one quarter of the training data and in less computing time.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our models' performance when trained on datasets of different sizes, we consider English and German datasets such as Text8 3 , 20 News, we list all the datasets that we have used to train the models.", "labels": [], "entities": [{"text": "Text8 3", "start_pos": 129, "end_pos": 136, "type": "DATASET", "confidence": 0.8727199137210846}]}, {"text": "We evaluate our models using intrinsic evaluation tasks which assess how well the vectors capture meanings of and relationships between words.", "labels": [], "entities": []}, {"text": "In particular, we evaluate all the models with respect to work for subsequent processing steps such as a sentence classification task).", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 105, "end_pos": 133, "type": "TASK", "confidence": 0.7961085538069407}]}, {"text": "Word similarity tasks evaluate word embeddings in terms of their k-nearest neighbors.", "labels": [], "entities": [{"text": "Word similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6944256573915482}]}, {"text": "For selected words, we show nearest neighbors according to cosine similarity for vectors trained using the proposed models as well as the baseline models.", "labels": [], "entities": []}, {"text": "Here we illustrate how the models performed for frequent as well as for infrequent words.", "labels": [], "entities": []}, {"text": "presents examples for all the models obtained after being trained for an epoch on the small Text8 dataset.", "labels": [], "entities": [{"text": "Text8 dataset", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9763326942920685}]}, {"text": "The table illustrates that for the frequent words, all the models learn a good representation and are able to produce relevant nearest neighbors.", "labels": [], "entities": []}, {"text": "For the infrequent words, the skip-gram did not learn a very good representation as there are not enough examples for the word to learn from.", "labels": [], "entities": []}, {"text": "In these cases the nearest neighbor of the skip-gram model are not very meaningful, e.g. it places firecracker close to \"prochnow\", while the KPCA fastText model places it closer to \"cracker\" and \"fire\" related words.", "labels": [], "entities": [{"text": "KPCA fastText", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.7883605659008026}]}, {"text": "Since the fastText model uses sub-word information, it achieves better performance at this task compared to the skip-gram model.", "labels": [], "entities": []}, {"text": "It finds meaningful neighbors for \"placental\", however it fails for words such as \"cruel\".", "labels": [], "entities": []}, {"text": "KPCA skip-gram gets a warm start with the morphological information learned from KPCA, which helps in learning a better representation for scarce words, thus producing better knearest neighbors for words such as \"cruel\".", "labels": [], "entities": [{"text": "KPCA skip-gram", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8728803396224976}]}, {"text": "When we compare KPCA skip-gram with KPCA fastText, we observe that KPCA fastText generally generates better neighbors.", "labels": [], "entities": []}, {"text": "We assume this is because of the fact that it benefits from the fastText approach of jointly refining subword and word representations.", "labels": [], "entities": []}, {"text": "Comparing fastText with the better initialized KPCA fastText model, KPCA produces decidedly better neighbors, especially for \"scrubbing\", \"firecracker\", \"linguistically\" and  Pre-trained word vectors are available fora dataset of 100 billion words from Google News.", "labels": [], "entities": []}, {"text": "observed that, when word vectors are trained on such a large dataset, they are able to answer very subtle relationships between words.", "labels": [], "entities": []}, {"text": "Yet, for the news data or fora small dataset such results cannot be achieved.", "labels": [], "entities": [{"text": "news data", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.8426851630210876}]}, {"text": "Warm-starting the models with our KPCA embeddings, however, yields good performance in such settings, too.", "labels": [], "entities": []}, {"text": "To assess accuracies in the word analogy task, we use a comprehensive test set provided by.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7743589083353678}]}, {"text": "This test consists of semantic and syntactic similarity questions which include relationships like adjective-to-adverb, currency, plural-verbs, city-in-state, comparative, superlative relationships, and others.", "labels": [], "entities": []}, {"text": "A question is assumed to be correctly answered only if the closest word to the vector is exactly the same as the correct word in the question; synonyms are considered as mistakes.", "labels": [], "entities": []}, {"text": "In order to use this evaluation to compare our models' results to those of the skip-gram and the fastText models, we train the models on the different datasets shown in A comparison of 300-dimensional and 128-dimensional embeddings on the analogy tasks on the text8 and 20-Newsgroups datasets showed that all models (including baselines) perform best when we picked 128-dimensional embeddings.", "labels": [], "entities": []}, {"text": "For the sake of simplicity we used 128-dimensions in all tasks.", "labels": [], "entities": []}, {"text": "From, it is evident that our KPCA fastText model outperforms the skip-gram as well as the fastText model when trained on a small dataset.", "labels": [], "entities": []}, {"text": "KPCA skip-gram as well as KPCA fastText models have better accuracies for both semantic and syntactic questions in the initial epochs compared to their cold-start counterparts.", "labels": [], "entities": [{"text": "KPCA skip-gram", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8538607358932495}, {"text": "accuracies", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9622994661331177}]}, {"text": "One question arising in this context is whether the skip-gram or the fastText models can also learn from smaller datasets.", "labels": [], "entities": []}, {"text": "The answer for the 20 Newsgroup as well as for the text8 datasets is \"yes\", but only if they are trained for several epochs.", "labels": [], "entities": [{"text": "20 Newsgroup", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9316791594028473}, {"text": "text8 datasets", "start_pos": 51, "end_pos": 65, "type": "DATASET", "confidence": 0.9104995131492615}]}, {"text": "The results for the 20 Newsgroups dataset in also show that the skip-gram models completely fail to learn analogies on this dataset.", "labels": [], "entities": [{"text": "20 Newsgroups dataset", "start_pos": 20, "end_pos": 41, "type": "DATASET", "confidence": 0.7665171225865682}]}, {"text": "The KPCA fastText embeddings benefit from their warm-start and show a quicker convergence rate.", "labels": [], "entities": [{"text": "KPCA fastText embeddings", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.853036363919576}]}, {"text": "We make the following observations from the accuracies obtained after each epoch of training.", "labels": [], "entities": []}, {"text": "During the 1 st epoch, the skip-gram and the fastText model do not perform well.", "labels": [], "entities": []}, {"text": "However, after the 2 nd epoch, the fastText model starts performing better on the syntactic questions.", "labels": [], "entities": []}, {"text": "Meanwhile KPCA skip-gram and KPCA fastText models still achieve higher accuracies than the respective skipgram and fastText models.", "labels": [], "entities": [{"text": "KPCA skip-gram", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.7814050912857056}, {"text": "accuracies", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.989797830581665}]}, {"text": "Hence, considering accuracies from the initial epochs, we can conclude that training of our model converges faster than the training of the fastText model and the skip-gram model.", "labels": [], "entities": []}, {"text": "From, we observe that the skip-gram model always seems to perform better on the semantic questions but when we compare these accuracies with the nearest neighbors results from, it can be observed that although KPCA models seem to work badly on semantic tasks, they generate better k-nearest neighbors than the respective skip-gram and fastText models.", "labels": [], "entities": []}, {"text": "We also compare the accuracies achieved by the models when trained for one epoch on a large data set, namely English Wikipedia dataset.", "labels": [], "entities": [{"text": "English Wikipedia dataset", "start_pos": 109, "end_pos": 134, "type": "DATASET", "confidence": 0.9461740056673685}]}, {"text": "The results in, illustrate the performance of the different models when training them on a large training dataset size.", "labels": [], "entities": []}, {"text": "When compared to fastText, KPCA skip-gram performs better on semantic questions, but worse on syntactic questions.", "labels": [], "entities": []}, {"text": "Noticeably KPCA fastText performs better on semantic questions than all the other models.", "labels": [], "entities": [{"text": "KPCA fastText", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.6083283722400665}]}, {"text": "However plain fastText outperforms it on the syntactic questions.", "labels": [], "entities": []}, {"text": "The overall accuracy of fastText is also slightly higher than for KPCA fastText model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997522234916687}]}, {"text": "We also report accuracies for the analogy task when the models are trained on the German news dataset for morphologically rich German language.", "labels": [], "entities": [{"text": "German news dataset", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.8227657079696655}]}, {"text": "We use the German version of the semantic/syntactic analogy dataset, introduced by) for evaluation.", "labels": [], "entities": []}, {"text": "shows how different models perform on the analogies tasks.", "labels": [], "entities": []}, {"text": "We note that morphological information  Finally, we investigate how well the embeddings obtained from the different models and different datasets perform on a downstream task in a neural network architecture.", "labels": [], "entities": []}, {"text": "We choose the convo-lutional neural network proposed in and evaluate it with the embeddings in a sentence classification task.", "labels": [], "entities": [{"text": "sentence classification task", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.7819369435310364}]}, {"text": "We initialize the CNN with the embeddings obtained from the different embedding models and keep the embeddings static during training.", "labels": [], "entities": []}, {"text": "Initializing the word vectors with the pre-trained word embeddings instead of random embeddings improves the performance as noted by).", "labels": [], "entities": []}, {"text": "In our experiment, the classification task is a sentiment classification task, i.e. detecting whether reviews are positive or negative.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.8697847922643026}]}, {"text": "The dataset used for it, consists of movie reviews 7 with one sentence per review.", "labels": [], "entities": []}, {"text": "We use embeddings generated by training the embedding models on the 20 Newsgroups and English Wikipedia datasets.", "labels": [], "entities": [{"text": "20 Newsgroups and English Wikipedia datasets", "start_pos": 68, "end_pos": 112, "type": "DATASET", "confidence": 0.6798005302747091}]}, {"text": "The CNN network is trained for 10 epochs.", "labels": [], "entities": [{"text": "CNN network", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9361831843852997}]}, {"text": "In, we list the accuracies for the model trained with different embeddings obtained from the two datasets.", "labels": [], "entities": []}, {"text": "In both cases, the model initialized with embeddings generated using KPCA skip-gram model and the KPCA fastText outperform the models initialized with the respective cold-start embeddings, albeit with a small margin.", "labels": [], "entities": [{"text": "KPCA fastText", "start_pos": 98, "end_pos": 111, "type": "DATASET", "confidence": 0.7717464566230774}]}, {"text": "The models initialized with the KPCA skip-gram model achieve the best results in both cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Analogy accuracies of embeddings in R 128 trained for different epochs on Text8 dataset.", "labels": [], "entities": [{"text": "Text8 dataset", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.9837575554847717}]}, {"text": " Table 4: Analogy accuracies of embeddings in R 128  trained for different epochs on 20 Newsgroups dataset.", "labels": [], "entities": [{"text": "R 128", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8662658929824829}, {"text": "Newsgroups dataset", "start_pos": 88, "end_pos": 106, "type": "DATASET", "confidence": 0.8949157893657684}]}, {"text": " Table 5: Analogy accuracies of embeddings in R 128  trained for one epoch on English Wiki 2016 dataset.", "labels": [], "entities": [{"text": "R 128", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8495341539382935}, {"text": "English Wiki 2016 dataset", "start_pos": 78, "end_pos": 103, "type": "DATASET", "confidence": 0.9643302857875824}]}, {"text": " Table 6: Analogy accuracies of embeddings in R 128  trained for one epoch on German news 2012 dataset.", "labels": [], "entities": [{"text": "R 128", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8888989090919495}, {"text": "German news 2012 dataset", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.9606363922357559}]}]}