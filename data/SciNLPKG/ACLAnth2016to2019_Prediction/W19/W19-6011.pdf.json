{"title": [{"text": "Improving Low-Resource Morphological Learning with Intermediate Forms from Finite State Transducers", "labels": [], "entities": [{"text": "Improving Low-Resource Morphological Learning", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8825987577438354}]}], "abstractContent": [{"text": "Neural encoder-decoder models are usually applied to morphology learning as an end-to-end process without considering the underlying phonological representations that linguists posit as abstract forms before morphophono-logical rules are applied.", "labels": [], "entities": []}, {"text": "Finite State Transducers for morphology, on the other hand, are developed to contain these underlying forms as an intermediate representation.", "labels": [], "entities": []}, {"text": "This paper shows that training a bidirectional two-step encoder-decoder model of Arapaho verbs to learn two separate mappings between tags and abstract morphemes and morphemes and surface allomorphs improves results when training data is limited to 10,000 to 30,000 examples of inflected word forms.", "labels": [], "entities": []}], "introductionContent": [{"text": "A morphological analyzer is a prerequisite for many NLP tasks.", "labels": [], "entities": [{"text": "morphological analyzer", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.6858336478471756}]}, {"text": "A successful morphological analyzer supports applications such as speech recognition and machine translation that could provide speakers of low-resource languages access to online dictionaries or tools similar to Siri or Google Translate and might support and accelerate language revitalization efforts.", "labels": [], "entities": [{"text": "morphological analyzer", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7158320546150208}, {"text": "speech recognition", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.7565002739429474}, {"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7606178224086761}]}, {"text": "This is even more crucial for morphologically complex languages such as Arapaho, an Algonquian language indigenous to the western USA.", "labels": [], "entities": []}, {"text": "In polysynthetic languages such as Arapaho, inflected verbal forms are often semantically equivalent to whole sentences in morphologically simpler languages.", "labels": [], "entities": []}, {"text": "A standard linguistic model of morphophonology holds that multiple morphemes are concatenated together and then phonological rules are applied to produce the inflected forms.", "labels": [], "entities": []}, {"text": "The operation of phonological rules can reshape the string of fixed morphemes considerably, making it difficult for learners, whether human or machines, to recreate correct forms (generation) from the morpheme sequence or to analyze the reshaped inflected forms into their individual morphemes (parsing).", "labels": [], "entities": []}, {"text": "In this paper we describe an experiment in training a neural encoder-decoder model to replicate the bidirectional behavior of an existing finite state morphological analyzer for the Arapaho verb (.", "labels": [], "entities": []}, {"text": "When a language is low-resource, natural language processing needs strategies that achieve usable results with less data.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6970433990160624}]}, {"text": "We attempt to replicate a low-resource context by using a limited number of training examples.", "labels": [], "entities": []}, {"text": "We evaluate the feasibility of learning abstract intermediate forms to achieve better results on various training set sizes.", "labels": [], "entities": []}, {"text": "While common wisdom regarding neural models has it that, given enough data (, end-to-end training is usually preferable to pipelined models, an argument can be made that morphology is an exception to this: learning two regular mappings separately maybe easier than learning a single complex one.", "labels": [], "entities": []}, {"text": "In, adressing a related task, noticeably better results were reached for German, Finnish, and Russian when a neural system was first tasked to learn morphosyntactic tags than when it was tasked to produce an inflected form directly from uninflected forms and context.", "labels": [], "entities": []}, {"text": "These three languages are morphologically complex or unpredictable, but marginally better results were achieved for the less complex languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare two strategies to train a neural model to generate inflected verbs from morphosyntactic tags with verb stem or to parse inflected verb forms.", "labels": [], "entities": []}, {"text": "First, we train the neural model to learn correct output forms directly from the parsed or inflected input.", "labels": [], "entities": []}, {"text": "Second, we added an intermediate step where the model first learns the mapping to intermediate forms and, from there, the mapping to the correct parsed or inflected form.", "labels": [], "entities": []}, {"text": "We measure the final accuracy score and the average Levenshtein distance and compare the performance of the two strategies in generation and in parsing.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.9737420380115509}, {"text": "Levenshtein distance", "start_pos": 52, "end_pos": 72, "type": "METRIC", "confidence": 0.5895427912473679}, {"text": "parsing", "start_pos": 144, "end_pos": 151, "type": "TASK", "confidence": 0.9652072191238403}]}, {"text": "Accuracy is measured as the fraction of correct generated/parsed forms in the output compared to complete gold inflected or parsed forms.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9960247278213501}]}, {"text": "We trained a bidirectional LSTM encoderdecoder with attention () to generate Arapaho verbs using five training sets with approximately 14.5, 18, 27, 31.5, and 36 thousand examples.", "labels": [], "entities": []}, {"text": "The direct strategy trains on the morphosyntactic tags and verb stem.", "labels": [], "entities": []}, {"text": "Each tag occurs in the same order as its corresponding morpheme appears in the intermediate form.", "labels": [], "entities": []}, {"text": "Only \"direction-of-action\" tags/morphemes come after the stem.", "labels": [], "entities": []}], "tableCaptions": []}