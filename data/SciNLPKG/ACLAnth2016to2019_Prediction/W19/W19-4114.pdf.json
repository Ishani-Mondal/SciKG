{"title": [{"text": "Multi-turn Dialogue Response Generation in an Adversarial Learning Framework", "labels": [], "entities": [{"text": "Multi-turn Dialogue Response Generation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7531291842460632}]}], "abstractContent": [{"text": "We propose an adversarial learning approach for generating multi-turn dialogue responses.", "labels": [], "entities": []}, {"text": "Our proposed framework, hredGAN, is based on conditional generative adversarial networks (GANs).", "labels": [], "entities": []}, {"text": "The GAN's generator is a modified hierarchical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bidirectional RNN that shares context and word embeddings with the generator.", "labels": [], "entities": []}, {"text": "During inference, noise samples conditioned on the dialogue history are used to perturb the generator's latent space to generate several possible responses.", "labels": [], "entities": []}, {"text": "The final response is the one ranked best by the discriminator.", "labels": [], "entities": []}, {"text": "The hredGAN shows improved performance over existing methods: (1) it generalizes better than networks trained using only the log-likelihood criterion, and (2) it generates longer, more informative and more diverse responses with high utterance and topic relevance even with limited training data.", "labels": [], "entities": []}, {"text": "This improvement is demonstrated on the Movie triples and Ubuntu dialogue datasets using both automatic and human evaluations.", "labels": [], "entities": [{"text": "Movie triples", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8980455994606018}, {"text": "Ubuntu dialogue datasets", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.9032168587048849}]}], "introductionContent": [{"text": "Recent advances in deep neural network architectures have enabled tremendous success on a number of difficult machine learning problems.", "labels": [], "entities": []}, {"text": "While these results are impressive, producing a deployable neural network-based model that can engage in open domain conversation still remains elusive.", "labels": [], "entities": []}, {"text": "A dialogue system needs to be able to generate meaningful and diverse responses that are simultaneously coherent with the input utterance and the overall dialogue topic.", "labels": [], "entities": []}, {"text": "Unfortunately, earlier conversation models trained with naturalistic dialogue data suffered greatly from limited contextual information) and lack of diversity ().", "labels": [], "entities": []}, {"text": "These problems often lead to generic and safe responses to a variety of input utterances.  and proposed the Hierarchical Recurrent EncoderDecoder (HRED) network to capture long temporal dependencies in multi-turn conversations to address the limited contextual information but the diversity problem remained.", "labels": [], "entities": []}, {"text": "In contrast, some HRED variants such as variational () and multi-resolution () HREDs attempt to alleviate the diversity problem by injecting noise at the utterance level and by extracting additional context to condition the generator on.", "labels": [], "entities": []}, {"text": "While these approaches achieve a certain measure of success over the basic HRED, the generated responses are still mostly generic since they do not control the generator's output.", "labels": [], "entities": []}, {"text": "This is because the output conditional distribution is not calibrated., on the other hand, consider a diversity promoting training objective but their model is for single turn conversations and cannot be trained end-to-end.", "labels": [], "entities": []}, {"text": "The generative adversarial network (GAN) () seems to bean appropriate solution to the diversity problem.", "labels": [], "entities": [{"text": "generative adversarial network (GAN)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.8174766500790914}]}, {"text": "GAN matches data from two different distributions by introducing an adversarial game between a generator and a discriminator.", "labels": [], "entities": []}, {"text": "We explore hredGAN: conditional GANs for multi-turn dialogue models with an HRED generator and discriminator.", "labels": [], "entities": []}, {"text": "hredGAN combines ideas from both generative and retrieval-based multi-turn dialogue systems to improve their individual performances.", "labels": [], "entities": []}, {"text": "This is achieved by sharing the context and word embeddings between the generator and the discriminator allowing for joint end-to-end training using back-propagation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, no existing work has applied conditional GANs to multi-turn dialogue models and especially not with HRED generators and discriminators.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of hredGAN over the VHRED for dialogue modeling with evaluations on the Movie triples and Ubuntu technical support datasets.", "labels": [], "entities": [{"text": "VHRED", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9787185192108154}, {"text": "dialogue modeling", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9026457965373993}, {"text": "Ubuntu technical support datasets", "start_pos": 123, "end_pos": 156, "type": "DATASET", "confidence": 0.8188995271921158}]}], "datasetContent": [{"text": "We consider the task of generating dialogue responses conditioned on the dialogue history and the current input utterance.", "labels": [], "entities": []}, {"text": "We compare the proposed hredGAN model against some alternatives on publicly available datasets.", "labels": [], "entities": []}, {"text": "Movie Triples Corpus (MTC) dataset . This dataset was derived from the Movie-DiC dataset by.", "labels": [], "entities": [{"text": "Movie Triples Corpus (MTC) dataset", "start_pos": 0, "end_pos": 34, "type": "DATASET", "confidence": 0.8749430520193917}, {"text": "Movie-DiC dataset", "start_pos": 71, "end_pos": 88, "type": "DATASET", "confidence": 0.9539223611354828}]}, {"text": "Although this dataset spans a wide range of topics with few spelling mistakes, its small size of only about 240,000 dialogue triples makes it difficult to train a dialogue model, as pointed out by . We thought that this scenario would really benefit from the proposed adversarial generation.", "labels": [], "entities": []}, {"text": "Ubuntu Dialogue Corpus (UDC) dataset).", "labels": [], "entities": [{"text": "Ubuntu Dialogue Corpus (UDC) dataset", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.9680089695113046}]}, {"text": "This dataset was extracted from the Ubuntu Relay Chat Channel.", "labels": [], "entities": [{"text": "Ubuntu Relay Chat Channel", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.9414028376340866}]}, {"text": "Although the topics in the dataset are not as diverse as in the MTC, the dataset is very large, containing about 1.85 million conversations with an average of 5 utterances per conversation.", "labels": [], "entities": [{"text": "MTC", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.7178565263748169}]}, {"text": "We split both MTC and UDC into training, validation, and test sets, using 90%, 5%, and 5% proportions, respectively.", "labels": [], "entities": [{"text": "MTC", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.7688320875167847}]}, {"text": "We performed minimal preprocessing of the datasets by replacing all words except the top 50,000 most frequent words by an UNK symbol.", "labels": [], "entities": [{"text": "UNK symbol", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.8725956976413727}]}, {"text": "Accurate evaluation of dialogue models is still an open challenge.", "labels": [], "entities": []}, {"text": "In this paper, we employ both automatic and human evaluations.", "labels": [], "entities": []}, {"text": "We employed some of the automatic evaluation metrics that are used in probabilistic language and dialogue models, and statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.6813582181930542}]}, {"text": "Although these metrics may not correlate well with human judgment of dialogue responses (, they provide a good baseline for comparing dialogue model performance.", "labels": [], "entities": []}, {"text": "Perplexity -For a model with parameter \u03b8, we define perplexity as: where K is the number of conversations in the dataset, N k is the number of utterances in conversation k, and NW is the total number of word tokens in the entire dataset.", "labels": [], "entities": []}, {"text": "The lower the perplexity, the better.", "labels": [], "entities": [{"text": "perplexity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9363802075386047}]}, {"text": "The perplexity measures the likelihood of generating the ground truth given the model parameters.", "labels": [], "entities": []}, {"text": "While a generative model can generate a diversity of responses, it should still assign a high probability to the ground truth utterance.", "labels": [], "entities": []}, {"text": "BLEU -The BLEU score () provides a measure of overlap between the generated response (candidate) and the ground truth (reference) using a modified n-gram precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9781588315963745}, {"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9861688017845154}]}, {"text": "According to, BLEU-2 score is fairly correlated with human judgment for non-technical dialogue (such as MTC).", "labels": [], "entities": [{"text": "BLEU-2 score", "start_pos": 14, "end_pos": 26, "type": "METRIC", "confidence": 0.9769249558448792}]}, {"text": "ROUGE -The ROUGE score) is similar to BLEU but it is recall-oriented instead.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9333425164222717}, {"text": "ROUGE score", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.9020883440971375}, {"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9966520667076111}, {"text": "recall-oriented", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9955629110336304}]}, {"text": "It is used for automatic evaluation of text summarization and machine translation.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.691629096865654}, {"text": "machine translation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7814210951328278}]}, {"text": "To compliment the BLEU score, we use ROUGE-N with N = 2 for our evaluation.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9670228362083435}, {"text": "ROUGE-N", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9984630346298218}]}, {"text": "Distinct n-gram -This is the fraction of unique n-grams in the generated responses and it provides a measure of diversity.", "labels": [], "entities": []}, {"text": "Models with higher a number of distinct n-grams tend to produce more diverse responses ().", "labels": [], "entities": []}, {"text": "For our evaluation, we use 1-and 2-grams.", "labels": [], "entities": []}, {"text": "Normalized Average Sequence Length (NASL) -This measures the average number of words in model-generated responses normalized by the average number of words in the ground truth.", "labels": [], "entities": [{"text": "Normalized Average Sequence Length (NASL)", "start_pos": 0, "end_pos": 41, "type": "METRIC", "confidence": 0.7480360780443464}]}, {"text": "For human evaluation, we follow a similar setup as, employing crowd-sourced judges to evaluate a random selection of 200 samples.", "labels": [], "entities": []}, {"text": "We presented both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance and informativeness.", "labels": [], "entities": []}, {"text": "For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1.", "labels": [], "entities": []}, {"text": "The scores are normalized between 0 and 1 and averaged over the total number of samples and judges.", "labels": [], "entities": []}, {"text": "For each model, we also estimated the per sample score variance between judges and then averaged over the number of samples, i.e., sum of variances divided by the square of number of samples (assuming sample independence).", "labels": [], "entities": []}, {"text": "The square root of result is reported as the standard error of the human judgment for the model.", "labels": [], "entities": []}, {"text": "Before proposing the above adversarial learning framework for multi-turn dialogue, we carried out some experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Generator Performance Evaluation", "labels": [], "entities": [{"text": "Generator Performance Evaluation", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6585651238759359}]}, {"text": " Table 4: Generator Performance: HRED, HRED+Attn and hredGAN without noise", "labels": [], "entities": [{"text": "HRED", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9647321105003357}, {"text": "HRED+Attn", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.8656375606854757}]}]}