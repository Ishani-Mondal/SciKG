{"title": [{"text": "Augmenting Neural Response Generation with Context-Aware Topical Attention", "labels": [], "entities": [{"text": "Augmenting Neural Response Generation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9016047716140747}]}], "abstractContent": [{"text": "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges.", "labels": [], "entities": []}, {"text": "Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic.", "labels": [], "entities": []}, {"text": "In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses.", "labels": [], "entities": [{"text": "Topical Hierarchical Recurrent Encoder Decoder (THRED)", "start_pos": 29, "end_pos": 83, "type": "TASK", "confidence": 0.6458783932030201}]}, {"text": "Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation.", "labels": [], "entities": []}, {"text": "To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments.", "labels": [], "entities": []}, {"text": "We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation.", "labels": [], "entities": [{"text": "THRED", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.6588563323020935}, {"text": "Response Echo Index", "start_pos": 81, "end_pos": 100, "type": "METRIC", "confidence": 0.8721928199132284}]}, {"text": "Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "With the recent success of deep neural networks in natural language processing tasks such as machine translation) and language modeling (, there has been growing research interest in building datadriven dialogue systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7016743868589401}, {"text": "language modeling", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.755719780921936}]}, {"text": "Fortunately, innovation in deep learning architectures and the availability of large public datasets have produced fertile ground for the data-driven approaches to become feasible and quite promising.", "labels": [], "entities": []}, {"text": "In particular, the Sequence-to-Sequence (Seq2Seq) neural network model) has witnessed substantial breakthroughs in improving the performance of conversational agents.", "labels": [], "entities": []}, {"text": "Such a model succeeds in learning the backbone of the conversation but lacks any aptitude for producing contextsensitive and diverse conversations.", "labels": [], "entities": []}, {"text": "Instead, generated responses are dull, short and carry little information ().", "labels": [], "entities": []}, {"text": "Instinctively, humans tend to adapt conversations to their interlocutor not only by looking at the last utterance but also by considering information and concepts covered in the conversation history (Danescu-NiculescuMizil and Lee, 2011).", "labels": [], "entities": []}, {"text": "Such adaptation increase the smoothness and engagement of the generated responses.", "labels": [], "entities": []}, {"text": "We speculate that incorporating conversation history and topic information with our novel model and method will improve generated conversational responses.", "labels": [], "entities": []}, {"text": "In this work, we introduce a novel, fully data-driven, multi-turn response generation system intended to produce context-aware and diverse responses.", "labels": [], "entities": [{"text": "multi-turn response generation", "start_pos": 55, "end_pos": 85, "type": "TASK", "confidence": 0.601888914903005}]}, {"text": "Our model builds upon the basic Seq2Seq model by combining conversational data and external knowledge information trained through a hierarchical joint attention neural model.", "labels": [], "entities": []}, {"text": "We find that our method leads to both diverse and contextual responses compared to the literature strong baselines.", "labels": [], "entities": []}, {"text": "We also introduce two novel quantitative metrics for dialogue model development, dubbed Semantic Similarity and Response Echo Index.", "labels": [], "entities": [{"text": "dialogue model development", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.8771620392799377}, {"text": "Response Echo Index", "start_pos": 112, "end_pos": 131, "type": "METRIC", "confidence": 0.7417542735735575}]}, {"text": "While the former measures the capability of the model to be consistent with the context and to maintain the topic of the conversation, the latter assesses how much our approach is able to generate unique and plausible responses which are measurably distant from the input dataset.", "labels": [], "entities": []}, {"text": "Used together, they provide a means to reduce burden of human evaluation and allow rapid testing of dialogue models.", "labels": [], "entities": []}, {"text": "We show that such metrics correlate well with human judgment, making a step towards a good automatic evaluation procedure.", "labels": [], "entities": []}, {"text": "The key contributions of this work are: \u2022 We devise a fully data-driven neural conversational model that leverages conversation history and topic information in the response generation process through a hierarchical joint attention mechanism; making the dialogue more diverse and engaging.", "labels": [], "entities": [{"text": "response generation process", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.7926374276479086}]}, {"text": "\u2022 We introduce two novel automated metrics: Semantic Similarity and Response Echo Index and we show that they correlate well with human judgment.", "labels": [], "entities": [{"text": "Response Echo Index", "start_pos": 68, "end_pos": 87, "type": "METRIC", "confidence": 0.8081346154212952}]}, {"text": "\u2022 We collect, parse and clean a conversational dataset from Reddit comments 1 .", "labels": [], "entities": [{"text": "conversational dataset from Reddit comments", "start_pos": 32, "end_pos": 75, "type": "DATASET", "confidence": 0.7229110479354859}]}], "datasetContent": [{"text": "One of the main weaknesses of dialogue systems is caused by the paucity of high-quality conversational dataset.", "labels": [], "entities": []}, {"text": "The well-known OpenSubtitles dataset (Tiedemann, 2012) lacks speaker annotations, thus making it more difficult to train conversation systems which demand high quality speaker and conversation level tags.", "labels": [], "entities": [{"text": "OpenSubtitles dataset (Tiedemann, 2012)", "start_pos": 15, "end_pos": 54, "type": "DATASET", "confidence": 0.9164682030677795}]}, {"text": "Therefore, the assumption of treating consecutive utterances as turn exchanges uttered by two persons (Vinyals and Le, 2015) could not be viable.", "labels": [], "entities": []}, {"text": "To enable the study of high-quality and large-scale dataset for dialogue modeling, we have collected a corpus of 35M conversations drawn from the Reddit data 2 , where each dialogue is composed of three turn exchanges.", "labels": [], "entities": [{"text": "dialogue modeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.8691530823707581}, {"text": "Reddit data 2", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9276593724886576}]}, {"text": "The Reddit dataset is composed of posts and comments, where each comment is annotated with rich metadata (i.e., author, number of replies, user's comment karma, etc.)", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.895195335149765}]}, {"text": "To harvest the dataset, we curated 95 English subreddits out of roughly 1.1M public subreddits 4 . Our choice was based on the top-ranked subreddits that discuss topics such as news, education, business, politics and sports.", "labels": [], "entities": []}, {"text": "We processed Reddit fora 12 month-period ranging from December 2016 until December 2017.", "labels": [], "entities": []}, {"text": "For each post, we retrieved all comments and we recursively followed the chain of replies of each comment to recover the entire conversation.", "labels": [], "entities": []}, {"text": "Reddit dataset is often semantically well-structured and is not filled with spelling errors thanks to moderator's efforts.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9147748351097107}]}, {"text": "Therefore, we do not perform any spelling correction procedure.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.8486384153366089}]}, {"text": "Due to resource limitations, we randomly sampled 6M dialogues as training data, 700K dialogues as development data, and 40K dialogues as test data.", "labels": [], "entities": []}, {"text": "For OpenSubtitles, we trained the models on the same size of data as for Reddit.", "labels": [], "entities": []}, {"text": "In this section, we focus on the task of evaluating the next utterance given the conversation history.", "labels": [], "entities": []}, {"text": "We compare THRED against three opensource baselines, namely Standard Seq2Seq with attention mechanism (), HRED (, and Topic-Aware (TA).", "labels": [], "entities": [{"text": "THRED", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.7373144030570984}, {"text": "HRED", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8350570201873779}]}, {"text": "As done in (), for Standard Seq2Seq and TASeq2Seq, we concatenate the dialogue history to account for context in a multi-turn conversation.", "labels": [], "entities": [{"text": "TASeq2Seq", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.4873899519443512}]}, {"text": "All experiments are conducted on two datasets (i.e., Reddit and OpenSubtitles).", "labels": [], "entities": []}, {"text": "We report results on OpenSubtitles in the supplementary material.", "labels": [], "entities": [{"text": "OpenSubtitles", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.9169546961784363}]}, {"text": "In the following subsections, we introduce two metrics that can impartially evaluate THRED and compare against the different baselines.", "labels": [], "entities": [{"text": "THRED", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9972500205039978}]}, {"text": "These metrics were tested on 5000 dialogues randomly sampled from the test dataset.", "labels": [], "entities": []}, {"text": "It is worth mentioning that we present word perplexity (PPL) on the test data in (along with diversity metric).", "labels": [], "entities": [{"text": "word perplexity (PPL)", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.8430009365081788}]}, {"text": "However, we do not believe that it represents a good measure for assessing the quality of responses (.", "labels": [], "entities": []}, {"text": "This is because perplexity captures how likely the responses are under a generation probability distribution, and does not measure the degree of diversity and engagingness in the responses.", "labels": [], "entities": []}, {"text": "Besides the quantitative measures, 4-scale and side-by-side human evaluation were carried out.", "labels": [], "entities": []}, {"text": "Five human raters were recruited for the purpose of evaluating the quality of the responses.", "labels": [], "entities": []}, {"text": "They were fluent, native English speakers and wellinstructed for the judgment task to ensure quality rating.", "labels": [], "entities": [{"text": "quality rating", "start_pos": 93, "end_pos": 107, "type": "METRIC", "confidence": 0.8709883987903595}]}, {"text": "We showed every judge 300 conversations (150 dialogues from Reddit and 150 dialogues from OpenSubtitles) and two generated responses for each dialogue: one generated by THRED model and the other one generated by one of our baselines.", "labels": [], "entities": []}, {"text": "The source models were unknown to the evaluators.", "labels": [], "entities": []}, {"text": "The responses were ordered in a random way to avoid biasing the judges.", "labels": [], "entities": []}, {"text": "Additionally, Fleiss' Kappa score is used to gauge the reliability of the agreement between human evaluators ().", "labels": [], "entities": [{"text": "Fleiss' Kappa score", "start_pos": 14, "end_pos": 33, "type": "METRIC", "confidence": 0.8384498556454977}]}, {"text": "An example of generated responses from the Reddit dataset are provided in For the 4-scale human evaluation, judges were asked to judge the responses from Bad (0) to Excellent (3).", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.9433538615703583}, {"text": "Bad (0)", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9445236027240753}, {"text": "Excellent", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9397299289703369}]}, {"text": "Additional details are provided in the supplementary material.", "labels": [], "entities": []}, {"text": "The results of this experiment, conducted on Reddit, are detailed in: Performance results of diversity and perplexity metrics of all the models on the Reddit test dataset.", "labels": [], "entities": [{"text": "Reddit test dataset", "start_pos": 151, "end_pos": 170, "type": "DATASET", "confidence": 0.9359705050786337}]}, {"text": "THRED surpasses all the baselines with again of 5% in distinct-1 and 37% in distinct-2 over TA-Seq2Seq (second best).", "labels": [], "entities": [{"text": "THRED", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9700129628181458}]}, {"text": "consensus degree rated 32.9% and 36.9% of the THRED responses in OpenSubtitles and Reddit respectively as Excellent, which is greatly larger than all baselines (up to 11.6% and 22.7% respectively).", "labels": [], "entities": [{"text": "consensus degree", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8942815363407135}, {"text": "THRED", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9039516448974609}]}, {"text": "Apart from the 4-scale rating, we conducted the evaluations side-by-side to measure the gain in THRED over the strong baselines.", "labels": [], "entities": [{"text": "THRED", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9987988471984863}]}, {"text": "Specific comparison instructions are included in the supplementary material.", "labels": [], "entities": []}, {"text": "The results, illustrated in, suggest that THRED is substantially superior to all baselines in producing informative and plausible responses from human's perspective.", "labels": [], "entities": [{"text": "THRED", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.984062135219574}]}, {"text": "The high Kappa scores imply that a major agreement prevails among the lablers.", "labels": [], "entities": [{"text": "Kappa", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.9749573469161987}]}, {"text": "In particular, THRED beats the strong baselines in 52% of the test data in Reddit (the percentage is achieved by averaging the win ratio).", "labels": [], "entities": [{"text": "THRED", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9533823132514954}]}, {"text": "However, for the rest of the cases, THRED is equally good with the baselines in 25% in Reddit (calculated similarly based on).", "labels": [], "entities": [{"text": "THRED", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9985577464103699}]}, {"text": "Hence, the ratio of cases where THRED is better than or equal with the baselines in terms of quality is 77% in Reddit.", "labels": [], "entities": [{"text": "THRED", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.9959344863891602}]}, {"text": "We also carried out an analysis on the correlation between the human evaluator ratings and our.", "labels": [], "entities": []}, {"text": "In addition, we assessed ADEM on our test datasets using the pre-trained weights 5 , provided by the authors.", "labels": [], "entities": [{"text": "ADEM", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8190756440162659}]}, {"text": "ADEM achieves low correlation with human judgment (\u03c1 = 0.014 on Reddit and \u03c1 = 0.034 on OpenSubtitles) presumably since the quality of its predicted scores highly depends on the corpus on which the model is trained.", "labels": [], "entities": []}, {"text": "Finally, we investigate the impact of training datasets on the quality of the responses generated by THRED and all baselines. has results which support that our cleaner, well-parsed Reddit dataset generates significantly improved responses over our metrics of interest.", "labels": [], "entities": [{"text": "THRED", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.931455671787262}, {"text": "Reddit dataset", "start_pos": 182, "end_pos": 196, "type": "DATASET", "confidence": 0.9111294746398926}]}, {"text": "In particular, we contrast the two datasets in terms of human judgment and the automated metrics among all the models.", "labels": [], "entities": []}, {"text": "Regarding human assessment, we took the mean evaluation rating (MER) per response in the test data to draw the comparison between the datasets.", "labels": [], "entities": [{"text": "mean evaluation rating (MER)", "start_pos": 40, "end_pos": 68, "type": "METRIC", "confidence": 0.931153933207194}]}, {"text": "As demonstrated in (see more details in in the Appendix), the human evaluators scored generated responses from the Reddit dataset higher than utterances generated from the OpenSubtitles dataset, which is true not only  in THRED, but in all models.", "labels": [], "entities": [{"text": "Reddit dataset", "start_pos": 115, "end_pos": 129, "type": "DATASET", "confidence": 0.8928231000900269}, {"text": "OpenSubtitles dataset", "start_pos": 172, "end_pos": 193, "type": "DATASET", "confidence": 0.8708707392215729}]}, {"text": "Consequently, the training data plays a crucial role in generating high-quality responses.", "labels": [], "entities": []}, {"text": "Morever, in OpenSubtitles, the assumption of spotting a conversation, as stated in Section 4, tends to include extraneous utterances in the dialogue, impeding the response generation process.", "labels": [], "entities": [{"text": "spotting a conversation", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7948274413744608}, {"text": "response generation", "start_pos": 163, "end_pos": 182, "type": "TASK", "confidence": 0.7327700853347778}]}, {"text": "While such presumption may seem valid in dealing with two-turn dialogues, it can aggravate the quality of conversations in multi-turn dialogues.", "labels": [], "entities": []}, {"text": "The model parameters are learned by optimizing the log-likelihood of the utterances via Adam optimizer with a learning rate of 0.0002; we followed () for decaying the learning rate.", "labels": [], "entities": []}, {"text": "The dropout rate is set to 0.2 for both the encoder and the decoder to avoid overfitting.", "labels": [], "entities": []}, {"text": "For all the baselines, we experimented hidden state units with the size of 1024.", "labels": [], "entities": []}, {"text": "For our model, we tested with encoder and decoder hidden state units of size 800, the same for the context encoder.", "labels": [], "entities": []}, {"text": "During inference, we experimented with the standard beam search with the beam width 5 and the length normalization \u03b1 = 1 (Wu et al., 2016).", "labels": [], "entities": [{"text": "length normalization \u03b1", "start_pos": 94, "end_pos": 116, "type": "METRIC", "confidence": 0.8659030397733053}]}, {"text": "We noticed that applying the length normalization resulted in a more diverse and longer sentences but at the expense of the semantic coherence of the response in some cases.", "labels": [], "entities": []}, {"text": "Training LDA model: We trained two LDA models 6 : one trained on OpenSubtitles and the other one trained on Reddit.", "labels": [], "entities": []}, {"text": "Both of them were trained on 1M dialogues.", "labels": [], "entities": [{"text": "1M dialogues", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.6946758925914764}]}, {"text": "We set the number of topics to 150, \u03b1 to 1 150 and \u03b3 to 0.01.", "labels": [], "entities": []}, {"text": "We filtered out stop words and universal words.", "labels": [], "entities": []}, {"text": "We also discarded the 1000 words with the highest frequency from the topic words.", "labels": [], "entities": []}, {"text": "For the 4-scale human evaluation, judges were asked to judge the responses from Bad (0) to Excellent (3).", "labels": [], "entities": [{"text": "Bad (0)", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9521227478981018}, {"text": "Excellent", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9663164019584656}]}, {"text": "Excellent (score 3): The response is very appropriate, on topic, fluent, interesting and shows understanding of the context.", "labels": [], "entities": []}, {"text": "Good (score 2): The response is coherent with the context but it is not diverse and informative.", "labels": [], "entities": []}, {"text": "It may imply the answer.", "labels": [], "entities": []}, {"text": "Poor (score 1): The response is interpretable and grammatically correct but completely off-topic.", "labels": [], "entities": []}, {"text": "Bad (score 0): The response is grammatically broken and it does not provide an answer.", "labels": [], "entities": [{"text": "Bad (score 0)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.7796343564987183}]}, {"text": "Regarding the side-by-side evaluation, humans were asked to favor response 1 over response 2 if: (1) response 1 is relevant, logically consistent to the context, fluent and on topic; or (2) Both responses 1 and 2 are relevant, consistent and fluent but response 1 is more informative than response 2.", "labels": [], "entities": []}, {"text": "If judges cannot tell which one is better, they can rate the responses as Equally good or Equally Bad.", "labels": [], "entities": [{"text": "Equally", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9779350161552429}]}, {"text": "Similarity metric with respect to Utt.1 and Utt.2 (complementary to).", "labels": [], "entities": []}, {"text": "From left to right, the labels in horizontal axis are THRED, HRED, Seq2Seq and TA-Seq2Seq.", "labels": [], "entities": [{"text": "THRED", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9768310785293579}, {"text": "HRED", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9383668899536133}]}, {"text": "THRED surpasses all baselines in similarity with Utt.2, and works mildly better in similarity with Utt.1.", "labels": [], "entities": [{"text": "THRED", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9122804999351501}]}, {"text": "The median for every box plot is also reported between brackets.", "labels": [], "entities": []}, {"text": "The order of the values is the same as the order of the methods.", "labels": [], "entities": []}, {"text": "Human judgments are provided in the brackets.", "labels": [], "entities": []}, {"text": "The blue arrow specifies a dialogue exchange and the highlighted words in red represent the topic words acquired from the pre-trained LDA model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: One cherry-picked dialogue out of 150 conversations along with the generated responses from all models.  Human judgments are provided in the brackets. The blue arrow specifies a dialogue exchange and the highlighted  words in red represent the topic words acquired from the pre-trained LDA model.", "labels": [], "entities": []}, {"text": " Table 2: Mean \u00b5 and standard deviation \u03c3 of SS scores for", "labels": [], "entities": [{"text": "Mean \u00b5", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9722382724285126}, {"text": "standard deviation \u03c3 of SS scores", "start_pos": 21, "end_pos": 54, "type": "METRIC", "confidence": 0.8072422196467718}]}, {"text": " Table 4: Performance results of diversity and perplexity met-", "labels": [], "entities": []}, {"text": " Table 5: Side-by-side human evaluation along with 4-scale human evaluation of dialogue utterance prediction on  Reddit dataset (mean preferences \u00b190% confidence intervals).", "labels": [], "entities": [{"text": "dialogue utterance prediction", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.8001965880393982}, {"text": "Reddit dataset", "start_pos": 113, "end_pos": 127, "type": "DATASET", "confidence": 0.9638452529907227}]}, {"text": " Table 8: Complete performance results of diversity and perplexity on Reddit test data and OpenSubtitles test data  (complementary to", "labels": [], "entities": [{"text": "Reddit test data", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.9125678737958273}, {"text": "OpenSubtitles test data", "start_pos": 91, "end_pos": 114, "type": "DATASET", "confidence": 0.9220494627952576}]}, {"text": " Table 9: Side-by-side human evaluation along with 4-scale human evaluation of dialogue utterance prediction on  OpenSubtitles dataset (mean preferences \u00b190% confidence intervals). Results on Reddit dataset are reported in", "labels": [], "entities": [{"text": "dialogue utterance prediction", "start_pos": 79, "end_pos": 108, "type": "TASK", "confidence": 0.7964818874994913}, {"text": "OpenSubtitles dataset", "start_pos": 113, "end_pos": 134, "type": "DATASET", "confidence": 0.9360096156597137}, {"text": "Reddit dataset", "start_pos": 192, "end_pos": 206, "type": "DATASET", "confidence": 0.96274134516716}]}]}