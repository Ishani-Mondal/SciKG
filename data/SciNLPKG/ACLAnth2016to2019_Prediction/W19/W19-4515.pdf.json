{"title": [{"text": "Evaluation of Scientific Elements for Text Similarity in Biomedical Publications", "labels": [], "entities": [{"text": "Text Similarity", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7717374265193939}]}], "abstractContent": [{"text": "Rhetorical elements from scientific publications provide a more structured view of the document and allow algorithms to focus on particular parts of the text.", "labels": [], "entities": []}, {"text": "We surveyed the literature for previously proposed schemes for rhetorical elements and present an overview of its current state of the art.", "labels": [], "entities": [{"text": "rhetorical elements", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.9366521537303925}]}, {"text": "We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity.", "labels": [], "entities": []}, {"text": "Comparison of the tools with two strong baselines shows that the predictions provided by the ArguminSci tool can support our use case of mining alternative methods for animal experiments.", "labels": [], "entities": [{"text": "ArguminSci", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.8568956851959229}]}], "introductionContent": [{"text": "We aim to mine alternative methods to animal experiments from the biomedical literature.", "labels": [], "entities": []}, {"text": "These are methods that address any of the so-called 3R principles of replacement (no animals at all or use of invertebrates over vertebrates), reduce (use of less animals), or refinement (cause less harm to animals) (.", "labels": [], "entities": []}, {"text": "For such complex natural language processing (NLP) applications, it is necessary to rely on appropriate tools to precisely understand the text and better find the potential relevant documents.", "labels": [], "entities": []}, {"text": "The rhetorical elements, such as zones or particular entities, can support NLP algorithms by focusing on the relevant elements of the text.", "labels": [], "entities": []}, {"text": "Given a certain document that describes an animal experiment fora certain research goal, hereafter called input document, we would like to find potential publications, hereafter called candidate documents, that describe an alternative method for the same research goal.", "labels": [], "entities": []}, {"text": "Thus, some of the scientific elements should be similar between input and candidate documents, e.g. research goals and outcomes, while some others should be different, e.g. methods.", "labels": [], "entities": []}, {"text": "Finding an alternative method to animal experiment requires two tasks: (a) performing a text similarity task with respect to some aspects of the publication, and (b) precisely understanding the proposed method with respect to the 3R principles.", "labels": [], "entities": []}, {"text": "Therefore, the extraction of rhetorical elements has the potential to boost performance for these tasks.", "labels": [], "entities": []}, {"text": "Previous works have proposed many schemes for rhetorical elements in scientific publication, as reviewed in.", "labels": [], "entities": []}, {"text": "Ina more recent survey, present a good overview on both metadata and schemes for scientific articles.", "labels": [], "entities": []}, {"text": "On the one hand, many of these schemes are not supported by an annotated corpus for training suitable information extraction tools.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7737322151660919}]}, {"text": "On the other hand, some tools based on these schemes are readily available for use.", "labels": [], "entities": []}, {"text": "We surveyed published schemes for rhetorical elements, whether focused on the biomedical domain or not, and we present a short overview on these.", "labels": [], "entities": [{"text": "rhetorical elements", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.9334480166435242}]}, {"text": "For those schemes for which we could find available tools, the latter was used to process a collection of 562 biomedical abstracts.", "labels": [], "entities": []}, {"text": "We performed a comparison of the output (rhetorical elements) from the tools in the scope of a text similarity task on a manually annotated dataset.", "labels": [], "entities": []}, {"text": "In this work, we limited our evaluation for text similarity but did not address whether the proposed methods comply with the 3R principles.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.7244303822517395}]}, {"text": "In summary, the contributions of this work are the following: (a) a short survey on existing schemes and corpora for rhetorical elements in scientific publications; (b) the identification of the schemes for which available tools are readily available for use; and (c) the evaluation of the available tools on a biomedical use case for text similarity.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 335, "end_pos": 350, "type": "TASK", "confidence": 0.7357748746871948}]}, {"text": "The next section presents a survey on the available schemes, followed by the methodology that we propose to compare the tools in the scope of text similarity.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.6880847215652466}]}, {"text": "We present the results in Section 4 and our discussion in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the tools for the task of text similarity.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7627407610416412}]}, {"text": "Therefore, we calculated the similarity between the input and candidate documents, either based on the whole text or on selected rhetorical elements as provided by the tools.", "labels": [], "entities": [{"text": "similarity", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9563755393028259}]}, {"text": "When utilizing the output from the various tools, we built a pseudo-document based either on the sentences or entities that we obtained.", "labels": [], "entities": []}, {"text": "For the zoning tools, we concatenated the sentences to form a single text, while we printed the entities (one per line) for the entity-based predictions.", "labels": [], "entities": []}, {"text": "Similarly, when evaluating combination of various labels, we concatenated the text from various labels into a single file.", "labels": [], "entities": []}, {"text": "We performed text similarity using the TextFlow tool and utilized these similarity scores to rank the candidate documents.", "labels": [], "entities": [{"text": "text similarity", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.6686162352561951}]}, {"text": "Subsequently, we evaluated the ranked list with regard the metrics of precision, recall and f-score at rank 10, i.e. P@10, R@10 and F@10.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9995201826095581}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9994038343429565}, {"text": "f-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9670361280441284}, {"text": "R", "start_pos": 123, "end_pos": 124, "type": "METRIC", "confidence": 0.9173309206962585}, {"text": "F", "start_pos": 132, "end_pos": 133, "type": "METRIC", "confidence": 0.9730764627456665}]}, {"text": "Given the few of these instances in our datasets, we decided to make no distinction between both categories.", "labels": [], "entities": []}, {"text": "As a result, the number of positive examples for the input documents in and 6, respectively.", "labels": [], "entities": []}, {"text": "We evaluated at rank 10 due to the reason that only two datasets have more than 20 positive instances, while only two of them over 10 positive instances.", "labels": [], "entities": []}, {"text": "For datasets which contain more than 10 positive examples, we considered the number of positive instances to be equal to 10 in the equation of R@10.", "labels": [], "entities": []}, {"text": "For the final comparison between the various tools and baselines, we per- formed an average of the metrics over the seven datasets.", "labels": [], "entities": []}, {"text": "We defined two baselines for comparison: (i) the original order of the candidate documents as returned by PubMed's \"similar articles\" functionality; and (ii) string similarity based on the whole text (title and abstract) without any pre-processing on the text.", "labels": [], "entities": []}, {"text": "For the first baseline, we searched in PubMed for each of the seven PMIDs and downloaded the list of the top 100 similar articles (stand of March 13th, 2019).", "labels": [], "entities": [{"text": "PubMed", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9606737494468689}]}, {"text": "Given that the current list of similar articles might include citations not present at the time when our corpus was annotated, we dismissed any document not included in our dataset when calculating the above metrics, i.e., we did not consider them as false positives.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Summary of the results from the two base- lines (two first rows) and when using the selected tools.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8422900438308716}]}, {"text": " Table 3: Performance of the single labels in the re- ranking task.", "labels": [], "entities": [{"text": "re- ranking task", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.8100342452526093}]}]}