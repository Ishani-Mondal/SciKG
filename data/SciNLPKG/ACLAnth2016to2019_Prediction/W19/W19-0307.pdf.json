{"title": [{"text": "An OCR system for the Unified Northern Alphabet", "labels": [], "entities": [{"text": "Unified Northern Alphabet", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.8264634807904562}]}], "abstractContent": [{"text": "This paper presents experiments done in order to build a functional OCR model for the Unified Northern Alphabet.", "labels": [], "entities": [{"text": "Unified Northern Alphabet", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.880483349164327}]}, {"text": "This writing system was used between 1931 and 1937 for 16 (Uralic and non-Uralic) minority languages spoken in the Soviet Union.", "labels": [], "entities": []}, {"text": "The character accuracy of the developed model reaches more than 98% and clearly shows cross-linguistic applicability.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9758796691894531}]}, {"text": "The tests described here therefore also include general guidelines for the amount of training data needed to boot-strap an OCR system under similar conditions.", "labels": [], "entities": []}, {"text": "Tiivistelm\u00e4 Tutkimus esittelee Yhteiselle pohjoiselle aakkostolle kehitett\u00e4v\u00e4\u00e4n tekstin-tunnistusmalliin t\u00e4ht\u00e4\u00e4vi\u00e4 kokeita.", "labels": [], "entities": []}, {"text": "Kyseist\u00e4 aakkostoa k\u00e4ytettiin 16 Neuvostolii-ton pohjoiselle kielelle noin vuosina 1931-1937.", "labels": [], "entities": []}, {"text": "Kehitetty malli saavuttaa merkki-tasolla parhaimmillaan yli 98% tunnistustarkkuuden, ja se kykenee tunnistamaan samalla kirjoitusj\u00e4rjestelm\u00e4ll\u00e4 kirjoitettuja eri kieli\u00e4.", "labels": [], "entities": []}, {"text": "Tehtyjen kokeiden perus-teella tehd\u00e4\u00e4n arvioita siit\u00e4, kuinka suuria aineistom\u00e4\u00e4ri\u00e4 tarvitaan uuden teks-tintunnistusj\u00e4rjestelm\u00e4n toteuttamiseen.", "labels": [], "entities": []}], "introductionContent": [{"text": "This article describes the tests conducted recently as part of the Kone Foundationfunded IKDP-2 project on developing an OCR system for the Unified Northern Alphabet, a writing system used during a period of time for several languages spoken in Northern areas of the Soviet Union.", "labels": [], "entities": [{"text": "Kone Foundationfunded IKDP-2", "start_pos": 67, "end_pos": 95, "type": "DATASET", "confidence": 0.8744110465049744}]}, {"text": "Part of the work has been conducted in the Institute for the Languages of Finland in relation to the OCR and HTR experiments recently carried out at the institute.", "labels": [], "entities": []}, {"text": "The study uses openly available materials so that the resources created and evaluated here can be used further in downstream NLP tasks.", "labels": [], "entities": []}, {"text": "The trained models and the scripts used to create them, alongside the evaluation scripts, are all published alongside the paper as an independent data package", "labels": [], "entities": []}], "datasetContent": [{"text": "The model training is done with Ocropy , as it offers a very convenient set of tools for various OCR-related tasks.", "labels": [], "entities": []}, {"text": "Other options would have been Tesseract and Transkribus, and repeating the tests with various systems should be carried out in further research.", "labels": [], "entities": [{"text": "Tesseract", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8913179636001587}, {"text": "Transkribus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9255799651145935}]}, {"text": "Ocropy, as with other modern OCR systems, is given training data as pairs of line images and corresponding text.", "labels": [], "entities": []}, {"text": "The text recognition is distinct from Layout Analysis, which refers to element detection and line segmentation, with the goal of finding the lines in their correct order.", "labels": [], "entities": [{"text": "text recognition", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7724670767784119}, {"text": "element detection", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7498185634613037}, {"text": "line segmentation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7050082981586456}]}, {"text": "It is important to note that when we speak of OCR accuracy we mean the accuracy for already correctly segmented lines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.7967566251754761}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9992228746414185}]}, {"text": "The model is given line-based material, which Ocropy keeps learning iteratively, saving the model at regular intervals.", "labels": [], "entities": []}, {"text": "The number of iterations controls the time the model is given to train.", "labels": [], "entities": []}, {"text": "The model learns the correspondence of line images and texts, and it does not need any specific font or character style information.", "labels": [], "entities": []}, {"text": "If a character does not exist in Unicode, as is the case with several letters used in UNA, a mapping has been done in Ground Truth to visually similar but factually incorrect letters.", "labels": [], "entities": []}, {"text": "This is done simply to aid visual inspection of the results, as mapping could have been done for any unique characters.", "labels": [], "entities": []}, {"text": "The primary languages involved in the study are Kildin Saami, Northern Selkup, Tundra Nenets and Northern Mansi.", "labels": [], "entities": []}, {"text": "These were used in the Ground Truth package, and the large amount of Kildin Saami material made it possible to design our study so that Kildin Saami could be compared to a setting in which all four languages are mixed together in the same OCR system.", "labels": [], "entities": []}, {"text": "The third Evenki experiment is also explained below.", "labels": [], "entities": []}, {"text": "The Ground Truth package was sampled and processed for our experiments with a script that prepares the working environment for the experiments.", "labels": [], "entities": []}, {"text": "It is provided with other documentation in an additional data packagePartanen and Rie\u00dfler (2018a) stored in a GitHub repository associated with the paper.", "labels": [], "entities": []}, {"text": "The repository also contains detailed examples of how to reproduce all plots and figures presented in this study.", "labels": [], "entities": []}, {"text": "In the first experiment, the idea was to test the amount resources needed to bootstrap an OCR system in this kind of situation.", "labels": [], "entities": []}, {"text": "We tested the training of a model on different amounts of lines, divided equally into subsets that are equivalent to pages (an addition of 20 lines counted as an increase of one page).", "labels": [], "entities": []}, {"text": "Twenty experiments were carried out, for an incrementally growing amount of training material.", "labels": [], "entities": []}, {"text": "The Ocropy system was trained for 10,000 iterations per model.", "labels": [], "entities": []}, {"text": "In the next experiment, two different OCR models were trained using a larger, apparently sufficiently sized, body of training material.", "labels": [], "entities": []}, {"text": "One model was trained on all four languages in equal proportions, and the other with only data from Kildin Saami.", "labels": [], "entities": []}, {"text": "In this experiment, the model was trained for 50,000 iterations and the number of training lines was also larger, 200 lines per language, fora total of 800 lines.", "labels": [], "entities": []}, {"text": "Similarly, the Kildin Saami monolingual model was trained for an equal number of iterations and with 800 lines.", "labels": [], "entities": []}, {"text": "The test sets common for both experiments contained 100 lines per language, or altogether 400 lines.", "labels": [], "entities": []}, {"text": "A test set that is half the size of the training set may seem too large, but this seemed reasonable since otherwise the number of lines in individual languages would have been so small that it would have been uncertain whether the different characters were at all equally present.", "labels": [], "entities": []}, {"text": "Similarly, one of our primary topics of investigation was whether a practical OCR system could be built with these resources and training scenarios, which makes extensive testing reasonable.", "labels": [], "entities": [{"text": "OCR", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9374153017997742}]}, {"text": "Since we aim to provide an OCR system for the Unified Northern Alphabet, it would be important to test the system on a language that is not at all included in the current models.", "labels": [], "entities": [{"text": "Unified Northern Alphabet", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.8307434916496277}]}, {"text": "This would truly reveal whether the OCR system actually generalizes toward the whole writing system.", "labels": [], "entities": []}, {"text": "With this in mind, the Evenki dataset described in section 3 was used as an additional test experiment.", "labels": [], "entities": [{"text": "Evenki dataset", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.9557464122772217}]}, {"text": "The scores on the Evenki dataset were reported and analysed in context, but this data was not used in training in any of the models.", "labels": [], "entities": [{"text": "Evenki dataset", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.9703105688095093}]}, {"text": "Section 6 contains an error analysis.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 22, "end_pos": 36, "type": "METRIC", "confidence": 0.9398056268692017}]}, {"text": "In this section, the error output of Ocropy is evaluated in order to identify the language-specific bottlenecks that keep the error rate high in some test scenarios.", "labels": [], "entities": []}, {"text": "shows the gradual improvement in the accuracy of the Kildin Saami model as the number of training pages is increased.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9996150732040405}, {"text": "Kildin Saami model", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.7272477547327677}]}, {"text": "The figure shows that the model improves very quickly when more pages are added for training.", "labels": [], "entities": []}, {"text": "With 8 pages, the model reaches an error rate approaching 2%, and falls below that if the number of pages is increased to 11.", "labels": [], "entities": [{"text": "error rate", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9936751127243042}]}, {"text": "The remaining mistakes are analysed further in section 6.", "labels": [], "entities": []}, {"text": "By increasing the training time per model and adjusting other parameters, this accuracy could However, the test does offer some very valuable insight.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9995980858802795}]}, {"text": "After 5 pages, the error rate had already fallen into 2.91%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9928082227706909}]}, {"text": "This is perhaps not yet a state-of-the-art level, but a character accuracy of 97% is already rather effortless and quick to proofread.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9902672171592712}]}, {"text": "Individual percentages can be squeezed out by increasing the number of pages, but in order to OCR an entirely new book, five pages, or approximately 100 lines, seems to be enough to bootstrap a useful OCR system that, although not necessarily ready for production, can at least be used to produce the needed increase in the number of pages more quickly and easily.", "labels": [], "entities": [{"text": "OCR", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.8397958278656006}]}, {"text": "Some of the characters recognized poorly belong to a group of characters that generally resemble one another quite a lot; especially pairs such as I : l, e : \u0454, s : \ua7a9, z : \u01b6 are confused occasionally even with the best-performing models.", "labels": [], "entities": []}, {"text": "A more common type of remaining mistake comes from uppercase letters.", "labels": [], "entities": []}, {"text": "However, since the training has been done in a low-resource scenario with a smaller amount of training data than would be common, the prevalence of capital letters in the errors seems easily explainable.", "labels": [], "entities": []}, {"text": "Uppercase letters are used rarely inmost of the texts, making up only slightly more than 5% of all letter characters in the training data.", "labels": [], "entities": []}, {"text": "From this point of view, it seems obvious that the accuracy of uppercase letters will trail behind the rest until the entire training set has reached a relatively large size.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9993653893470764}]}, {"text": "The Kildin Saami model performed relatively poorly on Selkup and Tundra Nenets.", "labels": [], "entities": []}, {"text": "The previous error analysis in this section showed that this was related to the lack of recognition accuracy in those letters that are present in those languages but not in Kildin Saami.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8950198888778687}]}, {"text": "The fact that the Kildin Saami model performed rather well on Northern Mansi must be related to the somewhat small character inventory used in Mansi and to the fact that it largely overlaps with the inventory of Kildin Saami.", "labels": [], "entities": []}, {"text": "The only character present in Northern Mansi but missing in Kildin Saami, at least in this dataset, is \ua727.", "labels": [], "entities": [{"text": "Kildin Saami", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.8145817220211029}]}, {"text": "The additional Evenki test further and more profoundly illustrates the problems seen in section 5.2 when testing different language combinations.", "labels": [], "entities": []}, {"text": "For example, the Evenki letters that were not recognized by the mixed model were \u0292 and \u0259\u0304 , both of which are rare or non-existent in the current training data.", "labels": [], "entities": []}, {"text": "Kildin Saami has four instances of \u0292 in entire Ground Truth package in word internal positions, whereas in Evenki this is a highly common character.", "labels": [], "entities": []}, {"text": "The Kildin model has a more narrow character set in use than the mixed model, which is illustrated by the very common error that occurs when using Kildin Saami model for Evenki: w : vv.", "labels": [], "entities": []}, {"text": "Kildin Saami does not use win UNA, whereas Evenki does not use v.", "labels": [], "entities": [{"text": "win UNA", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.6555100977420807}, {"text": "Evenki", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.9446220993995667}]}, {"text": "These differences, when added up, provide a good ex-planation for the accuracy rates seen in the experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9994962215423584}]}, {"text": "They also illustrate how a cross-linguistic writing system such as UNA benefits specifically from mixed language training, as the model has the opportunity to see characters across the languages.", "labels": [], "entities": [{"text": "UNA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8848446607589722}]}, {"text": "A further type of error comes from numerals, which are very rare in the Ground Truth package.", "labels": [], "entities": []}, {"text": "They occur a few times in running text, but at the moment the models simply do not recognize them at all.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Mixed and monolingual OCR models compared", "labels": [], "entities": []}]}