{"title": [{"text": "The Impact of Word Representations on Sequential Neural MWE Identification", "labels": [], "entities": [{"text": "Sequential Neural MWE Identification", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.7941263616085052}]}], "abstractContent": [{"text": "Recent initiatives such as the PARSEME shared task have allowed the rapid development of MWE identification systems.", "labels": [], "entities": [{"text": "PARSEME shared task", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.5139951407909393}, {"text": "MWE identification", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.977706253528595}]}, {"text": "Many of those are based on recent NLP advances, using neural sequence models that take continuous word representations as input.", "labels": [], "entities": []}, {"text": "We study two related questions in neural verbal MWE identification: (a) the use of lemmas and/or surface forms as input features, and (b) the use of word-based or character-based em-beddings to represent them.", "labels": [], "entities": [{"text": "neural verbal MWE identification", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7103702127933502}]}, {"text": "Our experiments on Basque, French, and Polish show that character-based representations yield systematically better results than word-based ones.", "labels": [], "entities": []}, {"text": "In some cases, character-based representations of surface forms can be used as a proxy for lem-mas, depending on the morphological complexity of the language.", "labels": [], "entities": []}], "introductionContent": [{"text": "MWE identification consists in finding multiword expressions (MWEs) in running text).", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9396263062953949}]}, {"text": "For many years, MWE identification was considered unrealistic, with most MWE research focusing on out-of-context MWE discovery (.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9850073158740997}, {"text": "MWE discovery", "start_pos": 113, "end_pos": 126, "type": "TASK", "confidence": 0.9460620582103729}]}, {"text": "Indeed, the availability of MWE-annotated corpora was limited to some treebanks with partial annotations, often a by-product of syntax trees (.", "labels": [], "entities": []}, {"text": "This prevented the widespread development and evaluation of MWE identification systems, as compared to other tasks such as POS tagging and named entity recognition.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.9801911115646362}, {"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.8337784111499786}, {"text": "named entity recognition", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.6654484073321024}]}, {"text": "This landscape has drastically changed in the last few years, thanks to shared tasks such as DiMSUM () and PARSEME 1.0 and 1.1 ( and to the release of open corpora annotated for MWEs in \u223c20 languages.", "labels": [], "entities": []}, {"text": "These initiatives provide a unified framework for MWE identification, including training/test corpus splits, evaluation metrics, benchmark results, and analysis tools.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9897638857364655}]}, {"text": "As a consequence, it is now possible to study some classical text processing problems and their impact on MWE identification systems.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.9755614101886749}]}, {"text": "One of these problems is the relation between a language's morphology, lemmatisation, input feature representations, out-of-vocabulary (OOV) words, and the performance of the system.", "labels": [], "entities": []}, {"text": "For instance, an MWE identification system based on (inflected) surface forms will likely encounter more OOV words than a system based on lemmas, especially for morphologically-rich languages in which a single lemma may correspond to dozens of surface forms).", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9601232707500458}]}, {"text": "This problem is particularly relevant for verbal MWEs, which present high morphological and syntactic variability . Our goal is to study the impact of word representations on verbal MWE (VMWE) identification, comparing lemmas, surface forms, traditional word embeddings and subword representations.", "labels": [], "entities": [{"text": "verbal MWE (VMWE) identification", "start_pos": 175, "end_pos": 207, "type": "TASK", "confidence": 0.6802841275930405}]}, {"text": "We compare the performance of an off-the-shelf MWE identification system based on neural sequence tagging () using lemmas and surface forms as input features, encoded in the form of classical pre-initialised word2vec embeddings ( or, alternatively, using new-generation FastText embeddings built from character n-grams (.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.949098140001297}, {"text": "neural sequence tagging", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.6949547727902731}]}, {"text": "Our main hypothesis is that the latter can model morphological variability, representing an alternative for lemmatisation.", "labels": [], "entities": []}, {"text": "We carryout experiments in 3 languages with varying morphological complexity: French, Polish and Basque.", "labels": [], "entities": []}, {"text": "popular models for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9825797975063324}]}, {"text": "Parsing-based methods take the (recursive) structure of language into account, trying to identify MWEs as a by-product of parsing (, or jointly.", "labels": [], "entities": []}, {"text": "Sequence tagging models, on the other hand, consider only linear context, using models such as) and averaged perceptron () combined with some variant of begin-inside-outside (BIO) encoding.", "labels": [], "entities": [{"text": "Sequence tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8485290706157684}]}, {"text": "Recurrent neural networks can be used for sequence tagging, being able to handle continuous word representations and unlimited context.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7204690426588058}]}, {"text": "The first neural identification system was MU-MULS, submitted to the PARSEME shared task 1.0 (.", "labels": [], "entities": [{"text": "neural identification", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7269443571567535}, {"text": "PARSEME shared task 1.0", "start_pos": 69, "end_pos": 92, "type": "DATASET", "confidence": 0.6951190680265427}]}, {"text": "Although it did not obtain the best results, MUMULS influenced the development of more advanced models () which ultimately led to the popularisation of the approach.", "labels": [], "entities": [{"text": "MUMULS", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.40380528569221497}]}, {"text": "As a consequence, and inspired by the success of neural models in NLP, nine out of the 17 systems submitted to the PARSEME shared task 1.1 used neural networks ( . Recently, improvements have been proposed, e.g. to deal with discontinuous MWEs ().", "labels": [], "entities": [{"text": "PARSEME shared task 1.1", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.5811470001935959}]}, {"text": "Previous work studied the impact of external lexicons ( and of several feature sets ( on CRFs for MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9779281616210938}]}, {"text": "Character-based embeddings have been shown useful to predict MWE compositionality out of context.", "labels": [], "entities": [{"text": "MWE compositionality", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.9571461379528046}]}, {"text": "In other tasks such as named entity recognition, character convolution layers have been successfully applied.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6364241242408752}]}, {"text": "The use of pre-trained vs. randomly initialised embeddings has been analysed in some PARSEME shared task papers (.", "labels": [], "entities": [{"text": "PARSEME shared task papers", "start_pos": 85, "end_pos": 111, "type": "DATASET", "confidence": 0.6362324208021164}]}, {"text": "The closest works to ours are the Veyn ( and SHOMA (Taslimipoor and Rohanian, 2018) systems, submitted to the PARSEME shared task 1.1.", "labels": [], "entities": [{"text": "Veyn", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.7241268754005432}, {"text": "SHOMA", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.6275637149810791}, {"text": "PARSEME shared task 1.1", "start_pos": 110, "end_pos": 133, "type": "DATASET", "confidence": 0.6827515810728073}]}, {"text": "Veyn is used as our off-the-shelf base system, so most of its architecture is identical to ours.", "labels": [], "entities": [{"text": "Veyn", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9462990760803223}]}, {"text": "Similarly to us, SHOMA employs FastText embeddings, a recurrent layer and a CRF output layer.", "labels": [], "entities": []}, {"text": "To our knowledge, however, this is the first study to compare input representations for neural MWE identification.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8862783014774323}]}], "datasetContent": [{"text": "Corpora The PARSEME shared task 1.1 released freely available VMWE-annotated corpora in 20 languages.", "labels": [], "entities": []}, {"text": "1 Each language's corpus is split into training, development and test parts.", "labels": [], "entities": []}, {"text": "To choose our target languages, we analysed the PARSEME corpora, choosing 3 languages with varying morphological richness: Basque (EU), French (FR) and Polish (PL), shown in.", "labels": [], "entities": [{"text": "PARSEME corpora", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8130123019218445}]}, {"text": "The FR training corpus has more than 420K tokens, whereas the PL and EU training corpora have around 220K and 117K tokens.", "labels": [], "entities": [{"text": "FR training corpus", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8571049372355143}, {"text": "PL and EU training corpora", "start_pos": 62, "end_pos": 88, "type": "DATASET", "confidence": 0.6979385852813721}]}, {"text": "EU contains less annotated VMWE occurrences than both FR and PL.", "labels": [], "entities": [{"text": "FR", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7967303991317749}]}, {"text": "The average length of annotated VMWE occurrences is similar in the three languages (2.02/2.29/2.13 in EU/FR/PL).", "labels": [], "entities": [{"text": "EU/FR/PL", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.7518525958061218}]}, {"text": "The proportion of discontinuous VMWEs in highest in FR (42.12%), whereas in Polish (29.76%) and in Basque (19.28%) they are less frequent.", "labels": [], "entities": [{"text": "FR", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.5140040516853333}]}, {"text": "These languages do have not the same morphological richness, as measured by the average number of surface forms per lemma in the vocabulary ('Morph' column).", "labels": [], "entities": []}, {"text": "For instance, the EU training corpus (2.32) has a higher morphological richness than PL (2.21) and FR (1.33).", "labels": [], "entities": [{"text": "EU training corpus", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.8135277231534322}, {"text": "FR", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.6957629919052124}]}, {"text": "The rate of OOVs, that is, of words that appear in the dev or test corpus vocabularies, but not in the training corpus, is higher for surface forms than for lemmas, with a potential negative impact on VMWE identification systems based on surface forms only.", "labels": [], "entities": [{"text": "rate of OOVs", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.7988405823707581}, {"text": "VMWE identification", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.8647426664829254}]}, {"text": "As expected, the OOV rate for surface forms is lowest in FR (20-26%), which also has the lowest morphological richness, and highest for EU (43%).", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9878743886947632}, {"text": "FR", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.6968376040458679}]}, {"text": "These differences are less visible for lemmas, which abstract away from morphology.", "labels": [], "entities": []}, {"text": "An interesting figure is the OOV rate focusing on verbs only.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.960846334695816}]}, {"text": "Here, PL presents more OOV verb forms (42-44%) than EU (32%), but again this difference disappears for lemmas.", "labels": [], "entities": []}, {"text": "This is relevant because our experimen-1 http://hdl.handle.net/11372/LRT-2842 2 Other languages have similar characteristics but were not selected due to the size of the corpora or to incomplete information (e.g. Turkish has missing surface forms for some verbs, preventing us from training a system based on surface forms only).", "labels": [], "entities": []}, {"text": "The official PARSEME French test corpus presents 11,632 missing lemmas.", "labels": [], "entities": [{"text": "PARSEME French test corpus", "start_pos": 13, "end_pos": 39, "type": "DATASET", "confidence": 0.8258967399597168}]}, {"text": "We have lemmatised it using UDPipe (http://ufal.mff.cuni.cz/udpipe) with default parameters, trained on the PARSEME shared task training corpus, to remain in the \"closed track\" conditions.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.8321104049682617}, {"text": "PARSEME shared task training corpus", "start_pos": 108, "end_pos": 143, "type": "DATASET", "confidence": 0.7857834100723267}]}, {"text": "For EU, we consider the POS tags VERB, ADI and ADT according to the conversion tal setup implies that it is difficult fora system to predict a VMWE without a reliable representation fora verb, learned from the training data.", "labels": [], "entities": [{"text": "VERB", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7200224995613098}]}], "tableCaptions": [{"text": " Table 2: MWE-based F-measure (F-MWE) and token-based F-measures (F-TOK) of the models on the test corpus,  using word2vec and FastText word representations for different feature sets: lemmas, surface forms, and both.", "labels": [], "entities": []}]}