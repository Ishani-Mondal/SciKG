{"title": [{"text": "Unsupervised Learning of Cross-Lingual Symbol Embeddings Without Parallel Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We present anew method for unsupervised learning of multilingual symbol (e.g. character) embeddings, without any parallel data or prior knowledge about correspondences between languages.", "labels": [], "entities": []}, {"text": "It is able to exploit similarities across languages between the distributions over symbols' contexts of use within their language , even in the absence of any symbols in common to the two languages.", "labels": [], "entities": []}, {"text": "In experiments with an artificially corrupted text corpus, we show that the method can retrieve character correspondences obscured by noise.", "labels": [], "entities": []}, {"text": "We then present encouraging results of applying the method to real linguistic data, including for low-resourced languages.", "labels": [], "entities": []}, {"text": "The learned representations open the possibility of fully unsu-pervised comparative studies of text or speech corpora in low-resourced languages with no prior knowledge regarding their symbol sets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Linguistic typology aims to map connections and similarities between different languages or dialects along multiple dimensions of comparison.", "labels": [], "entities": []}, {"text": "A large proportion of languages spoken today have few speakers and little data annotated with linguistic analyses such as syntactic parses or partof-speech tags.", "labels": [], "entities": []}, {"text": "This makes mapping their typology difficult, but doing so could help in developing just such resources, for example by language transfer.", "labels": [], "entities": [{"text": "language transfer", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7748869955539703}]}, {"text": "There may exist digital text in these languages (e.g. forum posts or newspapers), or field recordings of speech.", "labels": [], "entities": []}, {"text": "We attempt to learn about a language's typology purely from its surface form.", "labels": [], "entities": []}, {"text": "We focus on languages known to be fairly closely related (e.g. in the same language family), but where knowing more about the precise nature of the typology (e.g. regular sound correspondences in cognate words or differences in morphology) could help with resource development.", "labels": [], "entities": []}, {"text": "One example is the Uralic family, which contains many low-resourced languages and dialects.", "labels": [], "entities": []}, {"text": "To compare languages' surface forms, we must first address how to compare their basic units, characters in the case of text.", "labels": [], "entities": []}, {"text": "Even closely related languages may use different writing systems, conventions, or transcription practices, as well as having systematic linguistic differences.", "labels": [], "entities": []}, {"text": "These considerations mean that, without prior knowledge of a correspondence between two languages, it may not make sense to assume that, say, the letter a in one is directly comparable to a in the other.", "labels": [], "entities": []}, {"text": "For example, Swedish \u00e5 typically corresponds Finnish o, and loanwords from Swedish to Finnish replace the former with the latter.", "labels": [], "entities": [{"text": "\u00e5", "start_pos": 21, "end_pos": 22, "type": "METRIC", "confidence": 0.8875709772109985}]}, {"text": "Whilst such direct and well known correspondences can easily be written down by someone familiar with the language pair, capturing less clear-cut or systematic correspondences, and doing so fora large number of low-resourced language pairs, is labour intensive.", "labels": [], "entities": []}, {"text": "In an extreme case, two corpora may use completely distinct symbol sets, e.g. different scripts.", "labels": [], "entities": []}, {"text": "There maybe systematic linguistic differences that create a close correspondence between different symbols across languages), such as the phonological correspondence between Frisian f and Danish v ().", "labels": [], "entities": []}, {"text": "It may also be desirable to find correspondences between sequences of symbols, e.g. Spanish \u00f1 and Portuguese nh.", "labels": [], "entities": []}, {"text": "We tackle this problem using unsupervised learning of vector representations (embeddings) of symbols, learning purely from unannotated, unaligned linguistic corpora.", "labels": [], "entities": []}, {"text": "Here, we apply our method to text, learning representations of characters, but it is equally applicable to other sequences, such as phonetic sequences from speech.", "labels": [], "entities": []}, {"text": "To be applicable to extreme cases of very little overlap between symbol vocabularies (e.g. different scripts, or types of phonological transcription), it does not assume a correspondence even between common symbols.", "labels": [], "entities": []}, {"text": "E.g., if both use a, it treats a in the two languages as distinct symbols (1:a and 2:a).", "labels": [], "entities": []}, {"text": "This means that, where such correspondences are found, we know that they are motivated by statistical regularities in their usages, rather than any initial bias.", "labels": [], "entities": []}, {"text": "It may learn that 1:a corresponds to 2:a, or to 2:\u00e4, or that it has a weak correspondence to multiple characters.", "labels": [], "entities": []}, {"text": "This makes fora challenging learning task, since it becomes impossible to exploit the idea behind typical distributional methods -that similar symbols can be recognized by similarities between their contexts of occurrences -since the contexts across languages consist of symbols from distinct sets.", "labels": [], "entities": []}, {"text": "We present a method that is able to discover similarities between inter-lingual symbol pairs by exploiting similarities between their respective intra-lingual distributions over contexts of occurrence.", "labels": [], "entities": []}, {"text": "It must recognize that 1:a plays a role in relation to other symbols in language 1 that is similar to, say, 2:\u00e4's role in relation to other symbols in language 2.", "labels": [], "entities": []}, {"text": "It does not rely on parallel or comparable corpora, so is robust to use on whatever corpora are available for the languages of interest.", "labels": [], "entities": []}, {"text": "In this paper, we describe our learning method, XSYM ( \u00a73).", "labels": [], "entities": []}, {"text": "Then we present two sets of experiments.", "labels": [], "entities": []}, {"text": "In the first ( \u00a74), we use artificially corrupted linguistic data, allowing us to observe how well the technique recovers known mappings between character pairs obscured by the corruption.", "labels": [], "entities": []}, {"text": "In the second ( \u00a75), we demonstrate encouraging initial results of applying the method to real linguistic data, including several low-resourced pairs, which show that it is able to build a coherent space of characters, for example placing the majority of identical characters in two related languages close to each other.", "labels": [], "entities": []}, {"text": "This demonstrates its potential to recover correspondences between symbol pairs on the basis of distributional statistics without any other connection between the observed corpora.", "labels": [], "entities": []}, {"text": "Code for data preprocessing and model training, as well as trained embeddings, are available online 1 .", "labels": [], "entities": [{"text": "data preprocessing", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7350881099700928}]}], "datasetContent": [{"text": "We now apply XSYM to real linguistic data.", "labels": [], "entities": []}, {"text": "To ensure that the method is not exploiting similarities between two corpora due to a shared domain (e.g., prevalence of particular cognate words peculiar to that domain), we apply it to corpora from unrelated domains, as well as in-domain pairs.", "labels": [], "entities": []}, {"text": "We first compare Finnish and Estonian.", "labels": [], "entities": []}, {"text": "Whilst not low-resourced languages, it is easier to interpret results from these well-studied, closely related languages, and they area good starting point for studying low-resourced Uralic languages.", "labels": [], "entities": []}, {"text": "For Finnish, we use the YLILAUTA corpus again.", "labels": [], "entities": [{"text": "YLILAUTA corpus", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9013375341892242}]}, {"text": "For Estonian, we use the newspaper portion of the Estonian Reference Corpus, balanced subcorpus (, henceforth EST-REF-NEWS).", "labels": [], "entities": [{"text": "Estonian Reference Corpus", "start_pos": 50, "end_pos": 75, "type": "DATASET", "confidence": 0.9082189400990804}]}, {"text": "We use only the first 190k documents in Ylilauta, to match the size of EST-REF-NEWS (\u21e05.8M tokens).", "labels": [], "entities": [{"text": "Ylilauta", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8322141766548157}]}, {"text": "We lower-case the text to simplify analysis and treat very rare characters (< 500 occurrences) as a single out-of-vocabulary token.", "labels": [], "entities": []}, {"text": "We also run on a single-domain corpus pair, to see how the outcome is affected by comparable versus noncomparable corpora.", "labels": [], "entities": []}, {"text": "We train on YLILAUTA together with the forum portion of the Estonian Reference Corpus (\u21e06.4M tokens, henceforth EST-REF-FORUM).", "labels": [], "entities": [{"text": "YLILAUTA", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.917198121547699}, {"text": "Estonian Reference Corpus", "start_pos": 60, "end_pos": 85, "type": "DATASET", "confidence": 0.9612777829170227}]}, {"text": "Training parameters are identical to the previous section and nn-sim is used for early stopping and model selection.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.6887553036212921}, {"text": "model selection", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7659269571304321}]}, {"text": "We also apply the method to several combinations of low-resourced Uralic (North Finnic) languages: two dialects of Karelian (Olonets and North Karelian) and the severely endangered Ingrian language (\u21e0130 speakers).", "labels": [], "entities": []}, {"text": "All corpora are Bible translations from the University of Helsinki Corpus Server 3 , with \u21e0150k, 200k and 30k tokens respectively.", "labels": [], "entities": [{"text": "Helsinki Corpus Server 3", "start_pos": 58, "end_pos": 82, "type": "DATASET", "confidence": 0.7931741327047348}]}, {"text": "We report metrics for some pairs within low-resourced languages and also for Ingrian-Finnish, since many applications will involve comparing a low-resourced language to a better-resourced one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson correlation coefficient and regression  slope between the level of each type of corruption and  the pair-rank evaluation metric.", "labels": [], "entities": [{"text": "Pearson correlation coefficient", "start_pos": 10, "end_pos": 41, "type": "METRIC", "confidence": 0.9105079571406046}]}, {"text": " Table 2: Correspondence between common characters for cross-domain and in-domain models, as a sanity check.  Chars 1 and 2 are the number of characters in each language's vocabulary after the frequency filter. Mean pair  rank (MPR): mean rank of a character by cosine similarity to its identical character in the other language. R@1  is the proportion that are nearest neighbours, R@3 the proportion that are within the three closest.", "labels": [], "entities": [{"text": "Chars", "start_pos": 110, "end_pos": 115, "type": "METRIC", "confidence": 0.993097722530365}, {"text": "Mean pair  rank (MPR)", "start_pos": 211, "end_pos": 232, "type": "METRIC", "confidence": 0.908765991528829}]}]}