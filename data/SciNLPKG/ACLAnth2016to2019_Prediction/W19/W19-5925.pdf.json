{"title": [{"text": "Speaker-adapted neural-network-based fusion for multimodal reference resolution", "labels": [], "entities": [{"text": "multimodal reference resolution", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7116811871528625}]}], "abstractContent": [{"text": "Humans use a variety of approaches to reference objects in the external world, including verbal descriptions, hand and head gestures , eye gaze or any combination of them.", "labels": [], "entities": []}, {"text": "The amount of useful information from each modality, however, may vary depending on the specific person and on several other factors.", "labels": [], "entities": []}, {"text": "For this reason, it is important to learn the correct combination of inputs for inferring the best-fitting reference.", "labels": [], "entities": []}, {"text": "In this paper, we investigate speaker-dependent and independent fusion strategies in a multimodal reference resolution task.", "labels": [], "entities": [{"text": "multimodal reference resolution task", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.69497100263834}]}, {"text": "We show that without any change in the modality models, only through an optimized fusion technique, it is possible to reduce the error rate of the system on a reference resolution task by more than 50%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9728572368621826}, {"text": "reference resolution task", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.780118594566981}]}], "introductionContent": [{"text": "Reference resolution is of vital importance when human-machine interaction is expected to become natural and be integrated into everyday life.", "labels": [], "entities": [{"text": "Reference resolution", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9614859819412231}]}, {"text": "Humans have at their disposal abroad range of modalities to refer to objects in their environment, including verbal and material signals.", "labels": [], "entities": []}, {"text": "Equipping machines with the capability to correctly interpret such reference resolutions raises the question of how to fuse the information derived from the different modalities.", "labels": [], "entities": []}, {"text": "Popular fusion methods in this domain can be categorized along two dimensions.", "labels": [], "entities": []}, {"text": "The first is at which level of processing the fusion happens and the second how the fusion is performed (see; fora comprehensive overview).", "labels": [], "entities": []}, {"text": "In socalled early fusion or feature level fusion the features derived from the different modalities are combined, whereas in late fusion or decision level fusion classification results, e.g. in the form of probabilities, are combined.", "labels": [], "entities": [{"text": "decision level fusion classification", "start_pos": 140, "end_pos": 176, "type": "TASK", "confidence": 0.6074383780360222}]}, {"text": "Regarding the second dimension, the methods are mainly grouped into classification-based and estimation-based methods.", "labels": [], "entities": []}, {"text": "As for the classification-based techniques, the modalities are usually combined at the feature level, i.e. early fusion, and the decision is obtained using a classifier.", "labels": [], "entities": []}, {"text": "approached a reference resolution task in which two humans collaboratively solve a Tangram puzzle.", "labels": [], "entities": [{"text": "reference resolution task", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.8813842137654623}]}, {"text": "Their method computed linguistic, gaze and taskspecific features for each object of the board game and the objects were ranked using an SVM classifier.", "labels": [], "entities": []}, {"text": "Ina similar puzzle task, proposed a model that could resolve verbal descriptions as well as gestures utilizing a Bayesian network.", "labels": [], "entities": [{"text": "resolve verbal descriptions", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.8070968190828959}]}, {"text": "The Bayesian network design was later employed by for interpreting referring expressions with speech and pointing gestures in a real-world cooking task.", "labels": [], "entities": []}, {"text": "Regarding the rule-based fusion, linear weighted fusion is one of the simplest and most widely used rule-based methods.", "labels": [], "entities": []}, {"text": "This method combines the information from the different modalities linearly and it is assumed that the share of each modality in decision making does not change.", "labels": [], "entities": []}, {"text": "It has been successfully utilized in multiple studies on reference resolution.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.9331298172473907}]}, {"text": "A constraint-based rule system was used by) where the constraints considered the time correlation of events and their semantic content for the fusion.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on one rule-based method used in.", "labels": [], "entities": []}, {"text": "For this purpose, first, we explain the task and dataset in Sec.", "labels": [], "entities": []}, {"text": "2. Then, we discuss different approaches for the fusion of data in Sec.", "labels": [], "entities": []}, {"text": "3, including linear weighted fusion (Sec. 3.1) and our proposed neural-network-based fusion (Sec.", "labels": [], "entities": [{"text": "linear weighted fusion", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6382091740767161}]}, {"text": "3.2), which also provides the possibility of learning speakerdependent weights (Sec. 3.3).", "labels": [], "entities": []}, {"text": "We summarize the results in Sec.", "labels": [], "entities": []}, {"text": "4 and give a short conclusion and outlook on future work in Sec.", "labels": [], "entities": []}, {"text": "5. 2 Previous work 2.1 The TAKE Dataset: Example PENTO board on the TAKE dataset ( The TAKE dataset was first introduced in.", "labels": [], "entities": [{"text": "TAKE Dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.8539643883705139}, {"text": "PENTO board", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9455765783786774}, {"text": "TAKE dataset", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9160814881324768}, {"text": "TAKE dataset", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.794549435377121}]}, {"text": "It is a Wizard-of-Oz study, in which the participants were placed in front of a screen showing 15 pieces of a PENTO board game in random colors and shapes.", "labels": [], "entities": []}, {"text": "The pieces were grouped into the four corners of the screen.", "labels": [], "entities": []}, {"text": "For every episode, the shown objects and their positions on the screen was set randomly.", "labels": [], "entities": []}, {"text": "The participants were asked to instruct the system to select one specific PENTO piece on the board per episode.", "labels": [], "entities": []}, {"text": "There was no instruction telling the participants how to refer to the item.", "labels": [], "entities": []}, {"text": "According to the setup, it was possible to specify the object using spoken words, pointing gestures or eye gaze.", "labels": [], "entities": []}, {"text": "Next, one piece was marked and the participant confirmed whether this selection was correct.", "labels": [], "entities": []}, {"text": "The example episode below, corresponding to, shows the English translation of the speech input and the true referent identifier: \u2022 then we take now the se-so the second t that is on the top right ...", "labels": [], "entities": []}, {"text": "out of this group there I would like to have the yellow t ...", "labels": [], "entities": []}, {"text": "yes For this work, the confirmation utterance, e.g. the word \"yes\" in the above example, was removed, since it is not available at the time the decision is made.", "labels": [], "entities": []}, {"text": "After this cleanup, the dataset includes 1034 episodes distributed over 7 users as shown in.", "labels": [], "entities": []}, {"text": "The participants were native speakers, except for one, who spoke proficient but not native German.", "labels": [], "entities": []}, {"text": "The speech, an average of 6.8 words per utterance, was transcribed using Google Web Speech as an automatic speech recognition (ASR), with a vocabulary size of 1049.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.7071094512939453}]}, {"text": "Additionally, the speech was transcribed by hand, which can provide a reasonable upper bound for the results.", "labels": [], "entities": []}, {"text": "A Microsoft Kinect above the screen captured the arm movements and an eye tracker (Seeingmachines FaceLab) was used to determine the eye gaze.", "labels": [], "entities": []}, {"text": "Since the scenes in this dataset are virtual, we can directly annotate the objects with the properties and then query the scene representation.", "labels": [], "entities": []}, {"text": "For this simplified task, the properties are the color, the shape and the spatial relations of the pieces.", "labels": [], "entities": []}, {"text": "Using image processing techniques described in, several features for each object are extracted, including the number of edges, RGB (red, green, blue) values, HSV (hue, saturation, value), its centroid, horizontal and vertical skewness, and the orientation value denoting the direction of the principal axis.", "labels": [], "entities": [{"text": "HSV", "start_pos": 158, "end_pos": 161, "type": "METRIC", "confidence": 0.9831005334854126}]}, {"text": "These features are used for the natural language grounding described in the next section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate all fusion methods on the same data asunder the same four conditions: speech only, speech with gaze, speech with deixis, and speech with gaze and deixis.", "labels": [], "entities": []}, {"text": "For this purpose, we compare the error rate E = 100 \u00b7 M \u2212C M under all conditions, with C as the number of correctly estimated referents, among M estimates made for the test set.", "labels": [], "entities": [{"text": "error rate E", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.9723803202311198}]}, {"text": "However, for the linear fusion with fixed weights (fw) presented in Sec.", "labels": [], "entities": []}, {"text": "3.1, we did not use the weights suggested in.", "labels": [], "entities": []}, {"text": "Instead, a grid search was run on the training data to determine optimal weights for the dataset (ow).", "labels": [], "entities": []}, {"text": "This yielded an average improvement of 5.9% absolute for hand-annotated data and also improved all individual cases for ASR-annotated data except for the fusion of all modalities.", "labels": [], "entities": [{"text": "ASR-annotated", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.9637255072593689}]}, {"text": "Here, the results slightly deteriorated from 60.3% to 60.0%.", "labels": [], "entities": []}, {"text": "We used 10-fold cross validation to obtain an estimate of the error rate together with its standard deviation.", "labels": [], "entities": [{"text": "error rate", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9504285752773285}]}, {"text": "These results are depicted in Speech +Gaze +Deixis All performance between the results using the handannotated speech data vs. the ASR system, indicating a likely high number of transcription errors for the informative keywords.", "labels": [], "entities": []}, {"text": "It can also be seen that adding more modalities consistently improves the performance.", "labels": [], "entities": []}, {"text": "hand ASR The neural network-based fusion (Sec. 3.2) increased performance compared to the linear fusion (fw) notably and for all conditions.", "labels": [], "entities": []}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "We obtain the best results with an error rate of 8.9% for the fusion of all modalities using the hand-annotated data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.9756724238395691}]}, {"text": "In comparison to the fixed-weight baseline, with an error rate of 30% (see  : Results of the user-dependent (black) and the user-independent (gray) model in terms of error rate (%) and standard deviation \u03c3. compares the results of the speakerdependent and -independent models for each user.", "labels": [], "entities": [{"text": "error rate", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.953172504901886}, {"text": "error rate", "start_pos": 166, "end_pos": 176, "type": "METRIC", "confidence": 0.9471849501132965}]}, {"text": "Here, we only report the results for the fusion of all modalities.", "labels": [], "entities": []}, {"text": "When using the hand annotation, the speaker-adapted fusion reduces the error rate further, from 8.9% to 7.0%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9910513758659363}]}, {"text": "But it can also be seen that the results vary largely from user to user.", "labels": [], "entities": []}, {"text": "In particular, for user 1 (ASR data), the speaker-adapted version outperforms the other version easily, but for user 7, the original, speakerindependent version is more accurate.", "labels": [], "entities": []}, {"text": "For handannotated data, the difference between the two versions is smaller, but the users for which the speaker-adapted version outperforms the other remain the same.", "labels": [], "entities": []}, {"text": "Interestingly the speaker-adapted version performs least well for the two users with the most episodes that mostly contain gaze and pointing information, as can be seen in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of episodes, per user and cumula- tively, in the TAKE dataset.", "labels": [], "entities": [{"text": "TAKE dataset", "start_pos": 66, "end_pos": 78, "type": "DATASET", "confidence": 0.9380387961864471}]}]}