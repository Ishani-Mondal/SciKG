{"title": [{"text": "USAAR-DFKI -The Transference Architecture for English-German Automatic Post-Editing", "labels": [], "entities": [{"text": "USAAR-DFKI", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.919200599193573}]}], "abstractContent": [{"text": "In this paper we present an English-German Automatic Post-Editing (APE) system called transference, submitted to the APE Task organized at WMT 2019.", "labels": [], "entities": [{"text": "transference", "start_pos": 86, "end_pos": 98, "type": "TASK", "confidence": 0.9619900584220886}, {"text": "APE Task organized at WMT 2019", "start_pos": 117, "end_pos": 147, "type": "DATASET", "confidence": 0.6779932578404745}]}, {"text": "Our transference model is based on a multi-encoder transformer architecture.", "labels": [], "entities": []}, {"text": "Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a transformer decoder block, but without masking, for self-attention on mt, which effectively acts as second encoder combining src \u2192 mt, and (iii) feeds this representation into a final decoder block generating pe.", "labels": [], "entities": []}, {"text": "This model improves over the raw black-box neural machine translation system by 0.9 and 1.0 absolute BLEU points on the WMT 2019 APE development and test set.", "labels": [], "entities": [{"text": "black-box neural machine translation", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7546407282352448}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9901207685470581}, {"text": "WMT 2019 APE development and test set", "start_pos": 120, "end_pos": 157, "type": "DATASET", "confidence": 0.8942136423928397}]}, {"text": "Our submission ranked 3rd, however compared to the two top systems, performance differences are not statistically significant.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We compare our approach against the raw MT output provided by the 1 st -stage MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.9729623794555664}, {"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.942748486995697}]}, {"text": "We evaluate the systems using BLEU () and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9989546537399292}, {"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9983420372009277}]}, {"text": "We split the released data (13.4K) into two sets; we use the first 12K for training and the remaining 1.4K as validation data.", "labels": [], "entities": []}, {"text": "The development set (Dev) released by WMT2019 2 is used as test data for our experiment.", "labels": [], "entities": [{"text": "WMT2019", "start_pos": 38, "end_pos": 45, "type": "DATASET", "confidence": 0.8171904683113098}]}, {"text": "We build two models transference4M and transferenceALL using slightly different training procedures.", "labels": [], "entities": []}, {"text": "For transference4M, we first train on a training set called eScape4M combined with the first 12k of the provided NMT training data.", "labels": [], "entities": [{"text": "transference4M", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9699622392654419}, {"text": "NMT training data", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.7928226391474406}]}, {"text": "This eScape4M data is prepared using in-domain (for our case the 12K training data) bilingual cross-entropy difference for data selection as described in.", "labels": [], "entities": []}, {"text": "The difference in cross-entropy is computed based on two language models (LM): a domain-specific LM is estimated from the indomain (12K) PE corpus (lm i ) and the out-domain LM (lm o ) is estimated from the eScape corpus.", "labels": [], "entities": [{"text": "eScape corpus", "start_pos": 207, "end_pos": 220, "type": "DATASET", "confidence": 0.9252136051654816}]}, {"text": "We rank the eScape corpus by assigning a score to each of the individual sentences which is the sum of the three cross-entropy (H) differences.", "labels": [], "entities": [{"text": "eScape corpus", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.8530879020690918}]}, {"text": "For a j th sentence pair src j -mt j -pe j , the score is calculated based on Equation 1.", "labels": [], "entities": [{"text": "Equation", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9816583395004272}]}, {"text": "For transferenceALL, we initially train on the complete eScape dataset (eScapeAll) combined with the first 12k of the training data.", "labels": [], "entities": [{"text": "transferenceALL", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9785251021385193}, {"text": "eScape dataset (eScapeAll)", "start_pos": 56, "end_pos": 82, "type": "DATASET", "confidence": 0.9208669304847718}]}, {"text": "The eScapeAll data is sorted based on their in-domain similarities as described in Equation 1.", "labels": [], "entities": [{"text": "eScapeAll data", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8624126613140106}]}, {"text": "Both models are then fine-tuned towards the real data, by training again solely on the first 12k segments of the provided data.", "labels": [], "entities": []}, {"text": "For both models, we perform checkpoint averaging using the 8 best checkpoints.", "labels": [], "entities": [{"text": "checkpoint averaging", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7013386934995651}]}, {"text": "We report the results on the development set provided by WMT2019, which we use as a test set.", "labels": [], "entities": [{"text": "WMT2019", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8615227937698364}]}, {"text": "To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units () by using byte-pair encoding (BPE).", "labels": [], "entities": []}, {"text": "In the preprocessing step, instead of learning an explicit mapping between BPEs in the src, mt and pe, we define BPE tokens by jointly processing all triplets.", "labels": [], "entities": []}, {"text": "Thus, src, mt and pe derive a single BPE vocabulary.", "labels": [], "entities": []}, {"text": "Since mt and pe belong to the same language (DE) and src is a close language (EN), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size.", "labels": [], "entities": [{"text": "BPE", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.7250150442123413}]}], "tableCaptions": [{"text": " Table 2: Evaluation results on the WMT APE 2019 development set for the EN-DE NMT task.", "labels": [], "entities": [{"text": "WMT APE 2019 development set", "start_pos": 36, "end_pos": 64, "type": "DATASET", "confidence": 0.6970831394195557}, {"text": "EN-DE NMT task", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.5840723812580109}]}, {"text": " Table 3: Evaluation results on the WMT APE 2019 test set for the EN-DE NMT task.", "labels": [], "entities": [{"text": "WMT APE 2019 test set", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.7760915517807007}, {"text": "EN-DE NMT task", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.5282793045043945}]}, {"text": " Table 4: Comparison with wmt2018 Best on the WMT APE 2018 Dev/Test set for the EN-DE NMT task.", "labels": [], "entities": [{"text": "WMT APE 2018 Dev/Test set", "start_pos": 46, "end_pos": 71, "type": "DATASET", "confidence": 0.929302283695766}, {"text": "EN-DE NMT task", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.5100955863793691}]}]}