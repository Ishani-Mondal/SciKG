{"title": [{"text": "ChiMed: A Chinese Medical Corpus for Question Answering", "labels": [], "entities": [{"text": "ChiMed: A Chinese Medical Corpus", "start_pos": 0, "end_pos": 32, "type": "DATASET", "confidence": 0.6524034639199575}, {"text": "Question Answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7415362298488617}]}], "abstractContent": [{"text": "Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9459740042686462}, {"text": "natural language processing (NLP)", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7822639346122742}]}, {"text": "While models trained in the general domain can be adapted to anew target domain, their performance often degrades significantly due to domain mismatch.", "labels": [], "entities": []}, {"text": "Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain.", "labels": [], "entities": []}, {"text": "In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the rele-vancy prediction task and the adoption prediction task.", "labels": [], "entities": [{"text": "Chinese medical QA corpus called ChiMed", "start_pos": 46, "end_pos": 85, "type": "DATASET", "confidence": 0.7274325142304102}, {"text": "rele-vancy prediction task", "start_pos": 242, "end_pos": 268, "type": "TASK", "confidence": 0.807288408279419}, {"text": "adoption prediction task", "start_pos": 277, "end_pos": 301, "type": "TASK", "confidence": 0.9441125591595968}]}, {"text": "Several benchmark models are applied to the datasets, producing good results for both tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the big data era, it is often challenging to locate the most helpful information in many realworld applications, such as search engine, customer service, personal assistant, etc.", "labels": [], "entities": []}, {"text": "A series of NLP tasks, such as text representation, text classification, summarization, keyphrase extraction, and answer ranking, are able to help QA systems in finding relevant information.", "labels": [], "entities": [{"text": "text representation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7580590546131134}, {"text": "text classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7746955752372742}, {"text": "summarization", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.9813215136528015}, {"text": "keyphrase extraction", "start_pos": 88, "end_pos": 108, "type": "TASK", "confidence": 0.6933561861515045}, {"text": "answer ranking", "start_pos": 114, "end_pos": 128, "type": "TASK", "confidence": 0.8157273530960083}]}, {"text": "Currently, most QA corpora are built for the general domain focusing on extracting/generating answers from articles, such as CNN/Daily Mail (,,), SearchQA (), CoQA (, etc., with few others from community QA forums, such as TrecQA (), WikiQA (, and).", "labels": [], "entities": [{"text": "extracting/generating answers from articles", "start_pos": 72, "end_pos": 115, "type": "TASK", "confidence": 0.7603828012943268}, {"text": "CNN/Daily Mail", "start_pos": 125, "end_pos": 139, "type": "DATASET", "confidence": 0.9146694540977478}]}, {"text": "In the medical domain, most medial QA corpora consist of scientific articles, such as BioASQ (), emrQA (, and CliCR.", "labels": [], "entities": [{"text": "BioASQ", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.8957492709159851}]}, {"text": "Although some studies were done for conversational datasets (, corpora designed for community QA are extremely rare.", "labels": [], "entities": []}, {"text": "Meanwhile, given that many online medical service forums have emerged (e.g. MedHelp ), there are increasing demands from users to search for answers for their medical concerns.", "labels": [], "entities": [{"text": "MedHelp", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.923166811466217}]}, {"text": "One might be tempted to build QA corpora from such forums.", "labels": [], "entities": []}, {"text": "However, in doing so, one must address a series of challenges such as how to ensure the quality of the derived corpus despite the noise in the original forum data.", "labels": [], "entities": []}, {"text": "In this paper, we introduce our work on building a Chinese medical QA corpus named ChiMed by crawling data from a big Chinese medical forum 2 . In the forum, the questions are asked by web users and all the answers are provided by accredited physicians.", "labels": [], "entities": [{"text": "Chinese medical QA corpus named ChiMed", "start_pos": 51, "end_pos": 89, "type": "DATASET", "confidence": 0.6856617778539658}]}, {"text": "In addition to (Q, A) pairs, the corpus contains rich information such as the title of the question, key phrases, age and gender of the user, the name and affiliation of the accredited physicians who answer the question, and soon.", "labels": [], "entities": []}, {"text": "As a result, the corpus can be used for many NLP tasks.", "labels": [], "entities": []}, {"text": "In this study, we focus on two tasks: relevancy prediction (whether an answer is relevant to a question) and adoption prediction (whether an answer will be adopted).", "labels": [], "entities": [{"text": "relevancy prediction", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8222008943557739}, {"text": "adoption prediction", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.9515297710895538}]}], "datasetContent": [{"text": "As shown in, the majority of answers in ChiMed are relevant to the questions in the same QA records.", "labels": [], "entities": [{"text": "QA records", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.8637519180774689}]}, {"text": "To create a dataset for the relevancy task, we start with the 25,594 QA records which have exactly one adopted and one unadopted answer (see), Next, we filter out QA records whose questions or answers are too long or too short, because very short questions or answers We will remove a QA record if it contains a question/answer that is ranked either top 1% or bottom 1% of all questions/answers according to their character-based length.", "labels": [], "entities": [{"text": "relevancy task", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.9008034765720367}]}, {"text": "tend to be lack of crucial information, whereas very long ones tend to include much redundant or irrelevant information.", "labels": [], "entities": []}, {"text": "The remaining dataset contains 24,940 QA records.", "labels": [], "entities": []}, {"text": "We divide it into training/development/testing set with portions of 80%/10%/10% and call the dataset ChiMed-QA1.", "labels": [], "entities": []}, {"text": "Since each QA record has one adopted and one unadopted answer, we will use the dataset to train an adoption predictor.", "labels": [], "entities": []}, {"text": "For the relevancy task, we need both positive and negative examples.", "labels": [], "entities": []}, {"text": "We start with ChiMed-QA1, and for each QA record, we keep the adopted answer as a positive instance, and replace the unadopted answer with an adopted answer from another QA record randomly selected from the same training/dev/testing subsets to distinguish relevant vs. irrelevant answers.", "labels": [], "entities": []}, {"text": "We call this synthesized dataset ChiMed-QA2.", "labels": [], "entities": []}, {"text": "We will use those two datasets for the adoption prediction task and the relevancy task (see the next section).", "labels": [], "entities": [{"text": "adoption prediction task", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.9529457092285156}]}, {"text": "We are notable to use the corpus for the answer ranking task as we cannot infer the ranking label from the Adopted flag.", "labels": [], "entities": [{"text": "answer ranking task", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8927539388338724}, {"text": "Adopted flag", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9266920685768127}]}, {"text": "shows the statistics of the two datasets.", "labels": [], "entities": []}, {"text": "The first three rows are the same for the two datasets; the average length of As in ChiMed-QA2 is slightly longer than that in ChiMed-QA1 because adopted answers tend to be longer than unadopted ones.", "labels": [], "entities": [{"text": "As", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.8525933623313904}]}, {"text": "In this section, we use ChiMed-QA1 and ChiMed-QA2 (See) to build NLP systems for the adoption prediction task and the relevancy prediction task, respectively.", "labels": [], "entities": [{"text": "adoption prediction task", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.938246468702952}, {"text": "relevancy prediction task", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.847907543182373}]}, {"text": "Both tasks are binary classification tasks with the same type of input; the only difference is the meaning of class labels (relevancy vs. adopted flag).", "labels": [], "entities": []}, {"text": "Therefore, we build a set of NLP systems and apply them to both tasks.", "labels": [], "entities": []}, {"text": "Second, when both Q and A are present (System 5-9), the accuracy of relevancy prediction is higher than that of adoption prediction, because the former is an easier task (at least for humans).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9995793700218201}, {"text": "relevancy prediction", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.8465062975883484}, {"text": "adoption prediction", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.976538360118866}]}, {"text": "The only exception is ARC-I (System 7), whose results on relevancy is close to random guess (50.34% and 50.60%) while the result on adoption is comparable with other systems.", "labels": [], "entities": [{"text": "ARC-I", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8532668352127075}, {"text": "adoption", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.6457552909851074}]}, {"text": "This is due to the way that ARC-I matches questions and answers.", "labels": [], "entities": []}, {"text": "Because embeddings of a question and an answer are directly concatenated in ARC-I, Q-A similarity are not fully captured, leading to low performance on relevancy.", "labels": [], "entities": []}, {"text": "On the contrary, the adoption prediction does not rely much on the Q-A similarity (as explained above).", "labels": [], "entities": [{"text": "adoption", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9822168350219727}]}, {"text": "Third, for the relevancy task, systems that capture more features of Q-A similarity tend to have a better result.", "labels": [], "entities": []}, {"text": "For example, under the Q-A setting, DUET (System 8) outperforms CNN, LSTM and ARC-I (System 5-7) because DUET has an additional model of exact phrase matching between questions and answers.", "labels": [], "entities": [{"text": "CNN", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.8935644626617432}, {"text": "phrase matching between questions and answers", "start_pos": 143, "end_pos": 188, "type": "TASK", "confidence": 0.81696085135142}]}, {"text": "DRMM (System 9) performs better than DUET (System 8) because DRMM uses word embedding instead of exact phrase when matching pairs of phrases between a question and an answer.", "labels": [], "entities": [{"text": "DRMM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9620988368988037}]}, {"text": "In contrast, the performances of the five systems on the adoption task are very similar.", "labels": [], "entities": [{"text": "adoption task", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.9364591538906097}]}, {"text": "In addition, except for the relevancy task evaluated with CR, the contrast between System 10-14 vs. System 5-9 indicates comparing two As always helps predictors in both tasks because intuitively knowing both answers would help us to decide which one is relevant/adopted.", "labels": [], "entities": [{"text": "CR", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9926704168319702}]}, {"text": "On the contrary, the comparison between the same two groups of systems with CR in the relevancy task indicates comparing two As may hurt the relevancy predictors (System 5, 7, 8) because the relevancy is really between Q and A, which might be affected by the existence of other As.", "labels": [], "entities": []}, {"text": "Finally, all the systems under A-Only and Q-A settings (Systems 1-2 and 5-9) benefit from CR.", "labels": [], "entities": [{"text": "CR", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9906762838363647}]}, {"text": "It is also worth noting that running the models under Q-A setting and to evaluate them without CR in previous studies) is much more common.", "labels": [], "entities": [{"text": "CR", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9780585765838623}]}, {"text": "Under this setting, the highest performance achieved is 93.60% (System 9).", "labels": [], "entities": []}, {"text": "The score is not as high as our expectation and there still exist room for improvement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of ChiMed with respect to the num- ber of answers (As) per question (Q).", "labels": [], "entities": [{"text": "num- ber of answers (As)", "start_pos": 51, "end_pos": 75, "type": "METRIC", "confidence": 0.8288415223360062}]}, {"text": " Table 2: An example of QA record in ChiMed. The English translation is not part of the corpus.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of ChiMed.", "labels": [], "entities": [{"text": "Statistics of ChiMed.", "start_pos": 10, "end_pos": 31, "type": "DATASET", "confidence": 0.6666190251708031}]}, {"text": " Table 6: Inter-annotator agreement for relevancy and  ranking labeling on the Full-100 set in terms of percent- age (%) and Cohen's Kappa (\u03ba). I and II refer to the an- notations by the two annotators before any discussion,  and Agreed is the annotation after the annotators have  resolved their disagreement.", "labels": [], "entities": [{"text": "Full-100 set", "start_pos": 79, "end_pos": 91, "type": "DATASET", "confidence": 0.9776651561260223}, {"text": "Agreed", "start_pos": 230, "end_pos": 236, "type": "METRIC", "confidence": 0.994983434677124}]}, {"text": " Table 7: The confusion matrices of two annotators on  relevancy labels and ranking labels on the Full-100 set.", "labels": [], "entities": [{"text": "Full-100 set", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.985329657793045}]}, {"text": " Table 8: An example where one annotator thinks the two answers are equally good because they both answer the  question informatively. The other annotator thinks A1 is better because it tells the patient how to take care of  his/her hair in daily life, although A1 provides less analysis of the causes of the symptom. After discussion, the  two annotators reach an agreement that advice on daily care is very important and thus A1 is better than A2.", "labels": [], "entities": [{"text": "A1", "start_pos": 428, "end_pos": 430, "type": "METRIC", "confidence": 0.9869984984397888}, {"text": "A2", "start_pos": 446, "end_pos": 448, "type": "METRIC", "confidence": 0.9519157409667969}]}, {"text": " Table 9: The Adopted flag vs. relevancy label on the  Full-100 set. Here, answers with relevancy label \"1\" or  \"2\" are regarded as relevant answers.", "labels": [], "entities": [{"text": "Full-100 set", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.9869470000267029}]}, {"text": " Table 10: The adopted flags vs. the ranking labels from  annotators on the Full-100 set.", "labels": [], "entities": [{"text": "Full-100 set", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9877407252788544}]}, {"text": " Table 11: The answer does not directly answer the  question, but it has an adopted flag.", "labels": [], "entities": []}, {"text": " Table 12: Statistics of the two ChiMed-QA Datasets.  Average lengths of Qs and As are in characters.", "labels": [], "entities": [{"text": "ChiMed-QA Datasets", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.9128082096576691}, {"text": "Average lengths", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.8898083865642548}]}, {"text": " Table 13: Results of all systems under different set- tings with respect to (Q, A) pair prediction accuracy  with (+CR) and without (-CR) conflict resolution. We  do not present results of +CR in A-A and Q-As settings  because they are equivalent to the results of -CR.", "labels": [], "entities": [{"text": "pair prediction", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.5817695111036301}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.7256413698196411}, {"text": "conflict resolution", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.7001719921827316}]}]}