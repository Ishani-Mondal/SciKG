{"title": [{"text": "Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References", "labels": [], "entities": []}], "abstractContent": [{"text": "The aim of this paper is to mitigate the shortcomings of automatic evaluation of open-domain dialog systems through multi-reference evaluation.", "labels": [], "entities": []}, {"text": "Existing metrics have been shown to correlate poorly with human judgement, particularly in open-domain dialog.", "labels": [], "entities": []}, {"text": "One alternative is to collect human annotations for evaluation, which can be expensive and time consuming.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of multi-reference evaluation, we augment the test set of DailyDialog with multiple references.", "labels": [], "entities": []}, {"text": "A series of experiments show that the use of multiple references results in improved correlation between several automatic metrics and human judgement for both the quality and the diversity of system output.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialog agents trained end-to-end to hold open-domain conversations have recently progressed rapidly, generating substantial interest (.", "labels": [], "entities": []}, {"text": "Development of these systems is driven by available data and benchmarks based on only a single ground truth reference response fora given context.", "labels": [], "entities": []}, {"text": "However, such single-reference evaluation does not account for all the plausible responses for any given conversational context).", "labels": [], "entities": []}, {"text": "This is known as the one-to-many response problem ().", "labels": [], "entities": []}, {"text": "Computing word-overlap metrics against a single-reference response may penalize perfectly valid responses () (e.g., \"Was anything stolen?\", \"Is anyone hurt\") that deviate from the particular target response (\"When was the break-in?\").", "labels": [], "entities": []}, {"text": "Unlike human evaluation, automatic evaluation with a single-reference may also disproportionately benefit models that produce generic responses with more probable words (e.g., \"I don't know\") Dialog Context: Person A: 911 emergency.", "labels": [], "entities": []}, {"text": "Person B: I would like to report a break-in. single-reference Response: When was this break-in?", "labels": [], "entities": []}, {"text": "Other Valid Responses: Was anything stolen?", "labels": [], "entities": [{"text": "Valid Responses", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.7357574105262756}]}, {"text": "Is anyone hurt or injured?", "labels": [], "entities": []}, {"text": "Is the perpetrator still inside the house?", "labels": [], "entities": []}, {"text": "I will send someone right away.", "labels": [], "entities": []}, {"text": "which is known as the dull-response problem (.", "labels": [], "entities": []}, {"text": "As a result, single-reference evaluations correlate weakly with human judgments of quality (.", "labels": [], "entities": []}, {"text": "To address these problems, this paper proposes to carryout automatic evaluation using multiple reference responses instead of a single-reference.", "labels": [], "entities": []}, {"text": "Multiple reference evaluation is attractive for several reasons.", "labels": [], "entities": [{"text": "Multiple reference evaluation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7484036286671957}]}, {"text": "First, the additional information in the multiple reference response can be used to provide more robust quality evaluation under the one-to-many condition.", "labels": [], "entities": []}, {"text": "Second, we can use the multiple references to better measure the diversity of the model, which is a widely studied topic in open-domain response generation (.", "labels": [], "entities": [{"text": "open-domain response generation", "start_pos": 124, "end_pos": 155, "type": "TASK", "confidence": 0.6037563880284628}]}, {"text": "Prior explorations in this area either rely on synthetically created or small scale reference sets (, or perform experiments only on a small set of metrics focused on only response quality (.", "labels": [], "entities": []}, {"text": "Our investigations for using multiple references for automatic evaluation covers the following aspects -1) We propose methodology for evaluating both the quality and the diversity of generated responses using multiple references.", "labels": [], "entities": []}, {"text": "2) The proposed evaluation framework is metricagnostic and the experiments cover a large spectrum of existing metrics, and 3) We augmented the exiting test set of DailyDialog dataset () with multiple references and perform human judgment correlation studies with humangenerated references.", "labels": [], "entities": [{"text": "DailyDialog dataset", "start_pos": 163, "end_pos": 182, "type": "DATASET", "confidence": 0.8872179687023163}]}, {"text": "Our extensive experimental results show that using multiple test references leads to significantly better correlation of automated metrics with human judgment in terms of both response quality and diversity.", "labels": [], "entities": []}, {"text": "This suggests that the use of multiple references serves to make automatic metrics more reliable mechanisms for evaluating open-domain dialog systems.", "labels": [], "entities": []}, {"text": "Moreover, followup studies are conducted to better understand the nature of the multi-reference evaluation, such as the number of reference responses needed to achieve high correlation.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes the experiments we conducted to explore the effectiveness of multireference evaluation.", "labels": [], "entities": [{"text": "multireference evaluation", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.7672357559204102}]}, {"text": "We use our multi-reference evaluation methodology to compare the models and the human generated responses on the whole test dataset.", "labels": [], "entities": []}, {"text": "For the human model, we use one reference from the multi-reference set as the hypothesis.", "labels": [], "entities": []}, {"text": "Human responses are generally more interesting and diverse than model responses, which are known to suffer from the dull response problem ().", "labels": [], "entities": []}, {"text": "Because of this reason, we would expect the human generated responses to get higher scores than the dialog models.", "labels": [], "entities": []}, {"text": "However, the results presented in show that single-reference automatic evaluation ranks few models higher than the hu-   mans model.", "labels": [], "entities": []}, {"text": "With multi-reference evaluation, human performance is significantly higher than model performance.", "labels": [], "entities": []}, {"text": "We further present scores for diversity metrics on multiple hypothesis generated for 100 contexts in the last two rows of the table.", "labels": [], "entities": []}, {"text": "The use of multi-reference evaluation covers a wider array of valid responses, which strongly rewards the diverse human responses compared to single-reference evaluation.", "labels": [], "entities": []}, {"text": "The interface designed for multi-reference data collection is shown in.", "labels": [], "entities": [{"text": "multi-reference data collection", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6570371786753336}]}, {"text": "The final design of the interface incorporates improvements based on multiple rounds of experiments and interviews on a small set of users.", "labels": [], "entities": []}, {"text": "The workers were shown a modal box with instructions and several good and bad examples before they start the task.", "labels": [], "entities": []}, {"text": "Then they are shown 5 contexts fora HIT, one by one.", "labels": [], "entities": [{"text": "HIT", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.6031305193901062}]}, {"text": "For each context, they are asked to write 4 diverse responses in the Textbox provided.", "labels": [], "entities": [{"text": "Textbox", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9521095752716064}]}, {"text": "Workers can enter multi-line responses and submit a response by pressing enter or clicking on a button.", "labels": [], "entities": []}, {"text": "They are shown the number of remaining responses they need to enter for the conversation.", "labels": [], "entities": []}, {"text": "We also record the timestamps for click and enter presses in the interface.", "labels": [], "entities": []}, {"text": "We prevent workers from entering replies shorter than 2 characters, the exact same reply more than 1 time and show them a warning prompt if enter their response too quickly consistently.", "labels": [], "entities": []}, {"text": "Data Collection modes -For the collection of 4 responses per context, we have the following options -A) 4R1W-Collect 4 responses from a single worker B) 2R2W-Collect 2 responses each from 2 separate workers, and C) 1R4W -Collect 1 response each from 4 separate workers.", "labels": [], "entities": [{"text": "Data Collection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5525916218757629}]}, {"text": "In order to decide between these collection modes, we designed an experiment where, for 100 random contexts, we collected 4 responses using all three styles A), B) and C).", "labels": [], "entities": []}, {"text": "In order to decide the best option, we measured lexical diversity across the 4 responses using self-BLEU () metrics, and the collected responses' relevance through the average BLEU score of the multi-reference responses with the ground truth (Gt-BLEU) in the dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9982821941375732}, {"text": "ground truth (Gt-BLEU)", "start_pos": 229, "end_pos": 251, "type": "METRIC", "confidence": 0.6898553013801575}]}, {"text": "The results are reported in.", "labels": [], "entities": []}, {"text": "To calculate Self-BLEU, we calculate the BLEU score for every response by treating the response as a hypothesis and the others as the references, and we define the average BLEU scores calculated this way to be the Self-BLEU of the response set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9788148701190948}, {"text": "BLEU", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9953452944755554}]}, {"text": "A higher Self-BLEU score implies less diversity in the set.", "labels": [], "entities": []}, {"text": "We observe that 4R1W and 2R2W achieve higher lexical diversity than 1R4W.", "labels": [], "entities": []}, {"text": "This is because when a worker is asked to write multiple responses, they can make their responses more diverse conditioned on their previous responses.", "labels": [], "entities": []}, {"text": "Relevance metrics Gt-BLEU-1,2,3,4 indicate that 1R4W achieve higher lexical similarity with the ground truth response in the dataset, followed by 4R1W.", "labels": [], "entities": [{"text": "Gt-BLEU-1,2,3,4", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.9568685293197632}]}, {"text": "We chose the 4R1W mode, that is, a collection of 4 responses from 1 worker, to balance the diversity and relevance metrics.", "labels": [], "entities": []}, {"text": "Instructions for annotation collection for Diversity Study We provided following instructions to the workers for collecting diversity ratings-\"Please read the following conversation between two persons.", "labels": [], "entities": [{"text": "annotation collection", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.6973869204521179}]}, {"text": "Then read some possible follow-up responses for the conversation.", "labels": [], "entities": []}, {"text": "You will be shown 5 sets of responses, with 5 responses in each set.", "labels": [], "entities": []}, {"text": "For each response set, first select the responses you think are appropriate responses for the conversation.", "labels": [], "entities": []}, {"text": "Then use the sliders to rate the diversity of the response set, that is, how many of the appropriate responses in the response set had different meanings or were different replies.", "labels": [], "entities": []}, {"text": "Please provide the diversity score only for the appropriate responses you have marked.", "labels": [], "entities": [{"text": "diversity score", "start_pos": 19, "end_pos": 34, "type": "METRIC", "confidence": 0.9740203320980072}]}, {"text": "The diversity score should not be more than the number of appropriate responses in that set.\"", "labels": [], "entities": []}, {"text": "These instructions were followed by an example to make the task clear.", "labels": [], "entities": []}, {"text": "There are only a few open-domain multi-reference datasets and they have been collected artificially either by retrieval ( or are very small in scale (.", "labels": [], "entities": []}, {"text": "Therefore we augmented the original test set of the DailyDialog dataset (, which has a sufficiently large test set.", "labels": [], "entities": [{"text": "DailyDialog dataset", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.9721279740333557}]}, {"text": "Conversa-  We present the average number of unique 1, 2 and 3 grams in the original ground truth and the set of collected multi-reference ground truth in.", "labels": [], "entities": []}, {"text": "The higher number of unique ngrams in the multi-reference ground truth indicates that the new ground truth captures more variation in the set of possible responses.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results from dataset quality experiment", "labels": [], "entities": []}, {"text": " Table 4: Correlation of various metrics when evaluated using single-reference and multi-reference test sets. Eval- uation using Multiple References leads to better correlation across all metrics.", "labels": [], "entities": [{"text": "correlation", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.9681034684181213}]}, {"text": " Table 5: Correlation scores for diversity metrics", "labels": [], "entities": []}, {"text": " Table 6: Model evaluation with automatic metrics on Single and Multiple references. Multiple reference evaluation  is able to correctly rank human responses higher than model responses.", "labels": [], "entities": [{"text": "Model evaluation", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8372235894203186}]}, {"text": " Table 7: Example of difference in metric scoring for  single versus multiple reference evaluation.", "labels": [], "entities": []}, {"text": " Table 8: Diversity and relevance for different modes of  data collection.", "labels": [], "entities": []}, {"text": " Table 9: Comparison of number of unique n-grams in  original versus multiple references.", "labels": [], "entities": []}]}