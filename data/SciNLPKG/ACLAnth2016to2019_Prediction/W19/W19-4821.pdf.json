{"title": [{"text": "An LSTM adaptation study of (un)grammaticality", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel approach to the study of how artificial neural network perceive the distinction between grammatical and ungram-matical sentences, a crucial task in the growing field of synthetic linguistics.", "labels": [], "entities": []}, {"text": "The method is based on performance measures of language models trained on corpora and fine-tuned with either grammatical or ungram-matical sentences, then applied to (different types of) grammatical or ungrammatical sentences.", "labels": [], "entities": []}, {"text": "The results show that both in the difficult and highly symmetrical task of detecting subject islands and in the more open CoLA dataset, grammatical sentences give rise to better scores than ungrammatical ones, possibly because they can be better integrated within the body of linguistic structural knowledge that the language model has accumulated.", "labels": [], "entities": [{"text": "CoLA dataset", "start_pos": 122, "end_pos": 134, "type": "DATASET", "confidence": 0.7596344649791718}]}], "introductionContent": [{"text": "As the language modeling abilities of Artificial Neural Network (ANN) expand, a growing number of studies have started to address a network's ability to distinguish sentences contain various types of syntactic errors from minimally different correct sentences, thus providing the equivalent of human grammaticality judgments, one of the cornerstones of theoretical linguistics since.", "labels": [], "entities": []}, {"text": "These studies are important for at least two reasons: they can shed light on the type and amount of information which can be learned from pure linguistic data without any specialized language-learning device (thus contributing to the debate on human Universal Grammar,, and they can be used as probes on the ANNs themselves, investigating whether models which are apparently proficient at language modeling are actually sensitive to the same syntactic and semantic cues humans use.", "labels": [], "entities": [{"text": "human Universal Grammar", "start_pos": 244, "end_pos": 267, "type": "TASK", "confidence": 0.6364585856596628}]}, {"text": "The ANNs used in this area of research (often LSTMs, Hochreiter and Schmidhuber 1997, but recently also transformer-based ANN,, all trained on large datasets of normal text) are tested on a mix of grammatical or ungrammatical sentences.", "labels": [], "entities": []}, {"text": "The latter are obtained either by altering naturally occurring sentences (semi-randomly, as in, or systematically,, by collecting examples from the published linguistic literature ( or by creating minimal pairs by hand (individually,, or with sentence-schemata, as in Chowdhury and Zamparelli 2018).", "labels": [], "entities": []}, {"text": "Once test data have been acquired, the literature has threaded between two very different approaches: treating grammaticality as a classification problem (i.e. feeding grammatical/-ungrammatical sentences to a classifier and asking it to discriminate, cf. the first experiment in, or feeding the test sentences to a Language Model (LM) pretrained on normal language and measuring the perplexity accumulated by the LM as it traverses the sentence.", "labels": [], "entities": []}, {"text": "The classification approach works somewhat better, and can tell us if the possibility to spot un-1 Most studies except take the simplifying assumption that judgments can be treated as binary (e.g. acceptable/non-acceptable).", "labels": [], "entities": []}, {"text": "This position is not entirely satisfactory, theoretically, but we believe that it won't do much harm at this early stage of research.", "labels": [], "entities": []}, {"text": "Intermediate methods are possible: and train a classifier on sentence vectors produced by various types of language models.", "labels": [], "entities": []}, {"text": "grammaticality can in principle be learned from the data, but is not directly comparable with the human ability to detect ungrammaticality, since explicit syntactic judgments play a negligible role in language acquisition.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 201, "end_pos": 221, "type": "TASK", "confidence": 0.7150702178478241}]}, {"text": "The approach which reads (un)grammaticality from the performance of a LM starts from a more naturalistic task-predicting what's coming)-and can thus be more directly compared to human performances, but the probability assigned by a LM to the words reflects many factors (sentence complexity, level of embedding, semantic coherence, etc.), making it difficult to tease apart 'grammaticality' from a more general notion of 'acceptability' or 'processing load'.", "labels": [], "entities": []}, {"text": "In this paper we propose a third approach to measuring grammaticality, derived from the LM method.", "labels": [], "entities": []}, {"text": "In this approach, we utilized our inhouse pre-trained LSTM LM and adapt the model via fine-tuning) on variations of the test sentences.", "labels": [], "entities": []}, {"text": "Grammaticality is then treated as a comparative measure of coherence: to what extent the new (un)grammatical input can be integrated with what the ANN has learned so far, and to what extent it can improve similar grammatical or ungrammatical constructions.", "labels": [], "entities": []}, {"text": "We test this method with a large number of artificially generated examples, focusing on a particularly difficult contrast, the case of subject vs. object subextraction . We then apply the method to a more general scenario, the CoLA dataset, tuning a LM with either grammatical or ungrammatical CoLA sentences and measuring its performance in various testing scenarios.", "labels": [], "entities": [{"text": "CoLA dataset", "start_pos": 227, "end_pos": 239, "type": "DATASET", "confidence": 0.7885226309299469}]}, {"text": "In the following sections, we first present a detailed task description, in Section 2, followed by a brief overview of the methodology and datasets used for the study (Section 3).", "labels": [], "entities": []}, {"text": "In Section 4, we formalize our hypothesis of how the model should behave and report the results and observation of the network behavior in Section 5; we then discuss our observation and conclude the study with future directions in Section 6.", "labels": [], "entities": []}, {"text": "The expanded test sets for each task can be found in https://github.com/LiCo-TREiL/ Computational-Ungrammaticality/tree/ master/blackboxnlp2019.", "labels": [], "entities": []}, {"text": "Every sentence in the corpus, which can be found at https://nyu-mll.github.io/CoLA/, is marked as grammatical or ungrammatical.", "labels": [], "entities": []}, {"text": "The values are drawn from the published literature, see2) for details.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Detailed information of the training/testing set for the adaptation experiments. inst. represent instances", "labels": [], "entities": []}, {"text": " Table 3: Effect of fine-tuning. AX represent the models tuned with affirmatives followed by X. Y-Aff(Y) represent  the test scores (SLOR) using the particular model minus the SLOR of the model adapted on affirmatives (Aff) for  the Y test set. X, Y \u2208 {RelObj, RelSubj, W hObj, W hSubj}.", "labels": [], "entities": [{"text": "test scores (SLOR)", "start_pos": 120, "end_pos": 138, "type": "METRIC", "confidence": 0.6585259675979614}, {"text": "SLOR", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9734495282173157}]}]}