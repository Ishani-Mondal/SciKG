{"title": [{"text": "The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations", "labels": [], "entities": [{"text": "Bias Identification of Contextualized Word Representations", "start_pos": 42, "end_pos": 100, "type": "TASK", "confidence": 0.9197878142197927}]}], "abstractContent": [{"text": "Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied.", "labels": [], "entities": [{"text": "BERT", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.9692115187644958}]}, {"text": "Standard de-biasing methods require large, heterogeneous lists of target words to identify the \"bias sub-space\".", "labels": [], "entities": []}, {"text": "We show that using new contextu-alized word embeddings in conceptor debias-ing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining (\"Or'ing\") the debiasing conceptors of the different subsets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Contextualized word representations are replacing word vectors in many natural language processing (NLP) tasks such as sentiment analysis, coreference resolution, question answering, textual entailment, and named entity recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.9626021981239319}, {"text": "coreference resolution", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.9388306736946106}, {"text": "question answering", "start_pos": 163, "end_pos": 181, "type": "TASK", "confidence": 0.8752706348896027}, {"text": "textual entailment", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.7172349393367767}, {"text": "named entity recognition", "start_pos": 207, "end_pos": 231, "type": "TASK", "confidence": 0.6611979703108469}]}, {"text": "However, ELMo and BERT have bias similar ( to the well documented bias in traditional word embedding methods (, and this could cause bias in NLP pipelines used for high stakes downstream tasks such as resume selection or bail setting algorithms).", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9959261417388916}, {"text": "resume selection or bail setting algorithms", "start_pos": 201, "end_pos": 244, "type": "TASK", "confidence": 0.7460382630427679}]}, {"text": "Traditional word embeddings, such as word2vec (), GloVe (), and Fasttext () require large sets of target words, since debiasing is generally done in the space of the PCA of the word embeddings.", "labels": [], "entities": []}, {"text": "(If one only uses a two words, like \"man\" and \"woman\", the PCA space is just a single vector pointing in the difference between those two vectors.)", "labels": [], "entities": []}, {"text": "Context-sensitive embedding such as ELMo and BERT give an embedding for every token (based on its context), giving large numbers of embedding for each word (such as \"man\"), so that principal components can be calculated even for word lists of size two as shown in.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9917798638343811}]}, {"text": "Use of contextualized word embedding allows better debiasing by allowing one (as will be described below) to breakup target word lists into smaller homogeneous subsets; it also gives better insight into where the bias maybe coming from.", "labels": [], "entities": []}, {"text": "Word embeddings capture distributional similarities; just as humans come to associate certain professions (homemaker or computer programmer) with certain genders (woman or man), word embeddings capture very similar associations (.", "labels": [], "entities": []}, {"text": "Such embedding biases tend to track statistical regularities such as percentage of people with a given occupation (Nikhil Garg and Zou, 2018) but sometimes deviate from them.", "labels": [], "entities": []}, {"text": "A number of debiasing methods have been proposed.", "labels": [], "entities": []}, {"text": "Most of them use hard debiasing -zeroing out one or more directions in the embedding space, generally selected using principal components (.", "labels": [], "entities": []}, {"text": "In this paper, we use a soft debiasing method, conceptor debiasing, which also works in the principal component space, but does a softer shrinkage of the bias and close-by directions (.", "labels": [], "entities": []}, {"text": "Many debiasing algorithms rely entirely on so called \"target lists\" of protected classes in order to identify and mitigate the \"bias subspace\"; however, to our knowledge no work examines the role of these target lists in defining this space.", "labels": [], "entities": []}, {"text": "This in part due to the fact that in standard word embeddings there is only one embedding fora token.", "labels": [], "entities": []}, {"text": "In contrast, new contextualized word representations such as BERT and ELMo have a different embedding for each word token in a context.", "labels": [], "entities": [{"text": "BERT", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9633916020393372}]}, {"text": "This allows us an opportunity to more closely examine what information target word lists are capturing.", "labels": [], "entities": []}, {"text": "This paper: \u2022 Examines bias in ELMo and BERT, taking advantage of their context-sensitivity to give better visualizations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9762307405471802}]}, {"text": "\u2022 Shows how heterogeneity in content and size of the \"target list\" of gendered or racially marked terms interferes with debiasing, and how conceptors on contextual embeddings can be used to address such target list heterogeneity.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}