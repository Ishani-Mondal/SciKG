{"title": [], "abstractContent": [{"text": "In this paper, we examined several algorithms to detect sentence boundaries in legal text.", "labels": [], "entities": [{"text": "detect sentence boundaries in legal text", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.6999099751313528}]}, {"text": "Legal text presents challenges for sentence tokenizers because of the variety of punctuations, linguistic structure, and syntax of legal text.", "labels": [], "entities": []}, {"text": "Out-of-the-box algorithms perform poorly on legal text affecting further analysis of the text.", "labels": [], "entities": []}, {"text": "A novel and domain-specific approach is needed to detect sentence boundaries to further analyze legal text.", "labels": [], "entities": []}, {"text": "We present the results of our investigation in this paper.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence Boundary Detection (SBD) is an important fundamental task in any Natural Language Processing (NLP) application because errors tend to propagate to high-level tasks and because the obviousness of SBD errors can lead users to question the correctness and value of an entire product.", "labels": [], "entities": [{"text": "Sentence Boundary Detection (SBD)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8836548725763956}]}, {"text": "While SBD is regarded as a solved problem in many domains, legal text presents unique challenges.", "labels": [], "entities": [{"text": "SBD", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9842319488525391}]}, {"text": "The remainder of this paper describes those challenges and evaluates three approaches to the task, including a modification to a commonly-used semisupervised and rule-based library as well as two supervised sequence labeling approaches.", "labels": [], "entities": []}, {"text": "We find that a fully-supervised approach is superior to the semi-supervised rule library.", "labels": [], "entities": []}], "datasetContent": [{"text": "We reviewed the following approach: \u2022 Punkt Model with Custom Abbreviations \u2022 Conditional Random Field  The Punkt model is an unsupervised algorithm with an assumption that SBD can be improved if abbreviations are correctly detected and then eliminated).", "labels": [], "entities": []}, {"text": "In our investigation of Punkt, we used the PunktSentenceTokenizer without further training and a trained instance with modified abbreviations.", "labels": [], "entities": [{"text": "Punkt", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8109294772148132}]}, {"text": "gives us a summary of the results for Precision, Recall, F1-score, and Support of the experiment.", "labels": [], "entities": [{"text": "Precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.999188244342804}, {"text": "Recall", "start_pos": 49, "end_pos": 55, "type": "METRIC", "confidence": 0.9983171224594116}, {"text": "F1-score", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9991574287414551}, {"text": "Support", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9985768795013428}]}, {"text": "The support column is the number of elements in each class label.", "labels": [], "entities": []}, {"text": "The majority of labeled tokens are \"I\" (Inside), but the end of sentence labels \"L\" (Last), which we need to target, are most important because it is the token label that represents the end of sentences.", "labels": [], "entities": []}, {"text": "The unmodified and untrained PunktSentenceTokenizer gave us a weighted average F1-Score of 0.959 including the predictions for tokens \"I\" (Inside) the sentences.", "labels": [], "entities": [{"text": "PunktSentenceTokenizer", "start_pos": 29, "end_pos": 51, "type": "DATASET", "confidence": 0.951395571231842}, {"text": "F1-Score", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9648914337158203}]}, {"text": "Excluding \"I\" (Inside), we get a weighted average F1-Score of only 0.741.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9803496599197388}]}, {"text": "A precision of around 65.4% (precision for \"L\" (Last) on) detecting end of sentences might not be adequate when processing large volumes of legal text.", "labels": [], "entities": [{"text": "precision", "start_pos": 2, "end_pos": 11, "type": "METRIC", "confidence": 0.9990034699440002}, {"text": "precision for \"L\" (Last) on)", "start_pos": 29, "end_pos": 57, "type": "METRIC", "confidence": 0.866169673204422}, {"text": "detecting end of sentences", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.619886040687561}]}, {"text": "In comparison,) reports an error rate of 1.02% (98.98% Precision) 2 on the Brown corpus and 1.65% (98.35% Precision) on the WSJ.", "labels": [], "entities": [{"text": "error rate", "start_pos": 27, "end_pos": 37, "type": "METRIC", "confidence": 0.9833309054374695}, {"text": "Brown corpus", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9857770502567291}, {"text": "WSJ", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.9858668446540833}]}, {"text": "To improve the score of the Punkt model, we trained it and gave it an updated abbreviation list based on the legal text domain.", "labels": [], "entities": []}, {"text": "Please see (Appendix A 1) fora sample list of abbreviations that we included in the model.", "labels": [], "entities": []}, {"text": "Additionally, we replace '\\n'(newline) and '\\t' (tab) with space and \"(double quotes) with \" (two single quotes) for each sentence used for training.", "labels": [], "entities": []}, {"text": "The model was trained to learn parameters unsupervised using the cleaned sentences of training set files.", "labels": [], "entities": []}, {"text": "To test the Punkt model, we used the test set file.", "labels": [], "entities": []}, {"text": "The test labels  were generated using the B, I, and L labels as described in (Section 4).", "labels": [], "entities": [{"text": "B", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.9482797384262085}]}, {"text": "Each document in the test set was sentence tokenized using the model then assigned the appropriate B, I, and L labels to generate the predicted labels for the test file.", "labels": [], "entities": []}, {"text": "After Punkt, we evaluate the use of Conditional Random Field (CRF) for SBD.", "labels": [], "entities": [{"text": "Conditional Random Field (CRF)", "start_pos": 36, "end_pos": 66, "type": "METRIC", "confidence": 0.7830708026885986}, {"text": "SBD", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9262127876281738}]}, {"text": "A CRF is a random field conditioned on an observation sequence (.", "labels": [], "entities": []}, {"text": "A sentence is an excellent example of an observation sequence.", "labels": [], "entities": []}, {"text": "CRF's are being used successfully fora variety of text processing tasks ().", "labels": [], "entities": [{"text": "text processing tasks", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.8178502321243286}]}, {"text": "We build on Savelka's (2017) work on using CRF for SDB for legal text.", "labels": [], "entities": []}, {"text": "Features were extracted for each token using a window of 3 tokens before and after the token that is in focus.", "labels": [], "entities": []}, {"text": "For those 3 tokens before and after, we extracted a total of 8 features based on the characters in the token.", "labels": [], "entities": []}, {"text": "The combination of those features represents a token in our feature space before being used to train the model.", "labels": [], "entities": []}, {"text": "The features used in this experiment are based on the simple features mentioned in.", "labels": [], "entities": []}, {"text": "Some sample features are IsLower, IsUpper, IsSpace (see Appendix A 2 for feature sample).", "labels": [], "entities": []}, {"text": "Using sklearn_crfsuite CRF, the model was trained using a gradient descent L-BFGS method with a maximum of 100 iterations with L1 (0.1) and L2 (0.1) regularization.", "labels": [], "entities": []}, {"text": "The CRF model gave us a weighted average F1-Score of 0.985) including the predictions for tokens \"I\" (Inside) the sentences.", "labels": [], "entities": [{"text": "F1-Score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9829135537147522}]}, {"text": "Excluding this, we get 0.893.", "labels": [], "entities": []}, {"text": "The precision on \"L\" (Last) labels for the CRF model is acceptable at 92.8%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995467066764832}, {"text": "L\" (Last) labels", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.888875275850296}]}, {"text": "After CRF, token context gives a significant performance gain for detecting sentence boundaries.", "labels": [], "entities": [{"text": "detecting sentence boundaries", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.8112886150677999}]}, {"text": "The imbalance of the class labels is an inherent characteristic of the SBD task because sentence endings would occur at a rate, we see in the distribution frequency in written legal text for however many labeled training examples.", "labels": [], "entities": [{"text": "SBD task", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.9316816926002502}]}, {"text": "We experimented with a deep learning neural network representing sentence tokens as a fixed dimensional vector that encoded the context of the text using word embeddings.", "labels": [], "entities": []}, {"text": "Gensim word2vec (Mikolov 2013) was trained on all the Adjudicatory decision data pre-processed as tokens as described in Section 4 in 200 epochs.", "labels": [], "entities": [{"text": "Gensim word2vec (Mikolov 2013)", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.8568335473537445}]}, {"text": "We used an embedding size of 300 using the skip-gram model with negative sampling.", "labels": [], "entities": []}, {"text": "Please see Appendix A 3 for the Gensim Word2vec parameters that were used.", "labels": [], "entities": [{"text": "Gensim Word2vec parameters", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.7556028962135315}]}], "tableCaptions": []}