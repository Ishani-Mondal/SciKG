{"title": [{"text": "Exploring Deep Multimodal Fusion of Text and Photo for Hate Speech Classification", "labels": [], "entities": [{"text": "Hate Speech Classification", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.9333431720733643}]}], "abstractContent": [{"text": "Interactions among users on social network platforms are usually positive, constructive and insightful.", "labels": [], "entities": []}, {"text": "However, sometimes people also get exposed to objectionable content such as hate speech, bullying, and verbal abuse etc.", "labels": [], "entities": []}, {"text": "Most social platforms have explicit policy against hate speech because it creates an environment of intimidation and exclusion, and in some cases may promote real-world violence.", "labels": [], "entities": []}, {"text": "As users' interactions on today's social networks involve multiple modalities, such as texts, images and videos, in this paper we explore the challenge of automatically identifying hate speech with deep multimodal technologies , extending previous research which mostly focuses on the text signal alone.", "labels": [], "entities": []}, {"text": "We present a number of fusion approaches to integrate text and photo signals.", "labels": [], "entities": []}, {"text": "We show that augmenting text with image embedding information immediately leads to a boost in performance , while applying additional attention fusion methods brings further improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "While social network platforms give people the voice to speak, they also have a need to moderate abusive and objectionable content that is harmful for their communities.", "labels": [], "entities": []}, {"text": "Most social platforms have explicit policy against hate speech (e.g. https://www.facebook.com/ communitystandards/hate_speech) because such content creates an environment of intimidation, exclusion, and in some cases promote real-world violence.", "labels": [], "entities": []}, {"text": "The automatic identification of hate speech has been mostly formulated as a natural language processing problem (e.g.).", "labels": [], "entities": [{"text": "automatic identification of hate speech", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.82614386677742}]}, {"text": "The signal from text, however, sometimes is not sufficient for determining whether apiece of content (such as a post) on the social network platforms constitutes hate speech.", "labels": [], "entities": []}, {"text": "There is a need to take into account signals from multiple modalities in order to have a full comprehension of the content for hate speech classification.", "labels": [], "entities": [{"text": "hate speech classification", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.809405783812205}]}, {"text": "For example, \"these are disgusting parasites\", the sentence itself can be either benign or hateful, depending on what \"these\" refer to; and when it is combined with a photo of people or symbols in a post, it is very likely to be hate speech.", "labels": [], "entities": []}, {"text": "We have seen many cases where the text itself is benign, but the whole post is hateful if we consider the context of the image.", "labels": [], "entities": []}, {"text": "There has been a number of research on multimodal fusion in the deep learning era.", "labels": [], "entities": [{"text": "multimodal fusion", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.702187642455101}]}, {"text": "For example, apply an outer product fusion method to combine text and photo information for the task of detecting human trafficking.", "labels": [], "entities": [{"text": "detecting human trafficking", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.901488721370697}]}, {"text": "For the task of user profiling, formulated as a multi-tasking classification problem, propose a hierarchical attention model propose the UDMF framework, a hybrid integration model that combines both early feature fusion and later decision fusion using both stacking and power-set combination.", "labels": [], "entities": []}, {"text": "also studied the combination of image and captions for the task of detecting cyberbullying.", "labels": [], "entities": [{"text": "detecting cyberbullying", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8844287693500519}]}, {"text": "For the task of name tagging, formulated as a sequence labeling problem, apply a visual attention model to put the focus on the sub-areas of a photo that are more relevant to the text encoded by a bi-LSTM model.", "labels": [], "entities": [{"text": "name tagging", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.8507353067398071}, {"text": "sequence labeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7174423635005951}]}, {"text": "For the task of image-text matching, compare an embedding network that projects texts and photos into a joint space where semantically-similar texts and photos are close to each other, with a similarity network that fuses text embeddings and photo embeddings via element multiplication.", "labels": [], "entities": [{"text": "image-text matching", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7269684374332428}]}, {"text": "For the task of sentiment analysis, ;; propose several models, namely contextual intermodal attention, dynamic fusion graph, and lowrank multimodal fusion, for integrating visual, audio, and text signals on the CMU-MOSEI data set.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9606674909591675}, {"text": "CMU-MOSEI data set", "start_pos": 211, "end_pos": 229, "type": "DATASET", "confidence": 0.9765253663063049}]}, {"text": "There is also research initiative in multimodal summarization () and multimodal translation.", "labels": [], "entities": [{"text": "multimodal summarization", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6674261689186096}, {"text": "multimodal translation", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.8006569445133209}]}, {"text": "These works have demonstrated the effectiveness of multimodal fusion methods in problems where non-text signals play an important role in disambiguating the text.", "labels": [], "entities": []}, {"text": "In this research, we explore deep multimodal fusion of text and photo for the task of hate speech classification on social networks, where hate speech posts frequently appear with images.", "labels": [], "entities": [{"text": "hate speech classification", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.7497049570083618}]}, {"text": "We experiment with many fusion techniques, including simple concatenation, bilinear transformation, gated summation, and attention mechanism.", "labels": [], "entities": []}, {"text": "We find that concatenation with photo information in the convolution text classifier immediately gives us a nice gain, while fusion with attention offers further improvement.", "labels": [], "entities": []}, {"text": "Specifically attention with deep cloning, sparsemax, and symmetric gate provides the best performance.", "labels": [], "entities": []}, {"text": "These results shall shed light on better identifying hate speech to provide a safer community of online social networks.", "labels": [], "entities": [{"text": "identifying hate speech", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.8431798418362936}]}], "datasetContent": [], "tableCaptions": []}