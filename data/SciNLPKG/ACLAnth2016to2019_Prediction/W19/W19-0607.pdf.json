{"title": [{"text": "R-grams: Unsupervised Learning of Semantic Units", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates data-driven segmentation using RePair or Byte Pair Encoding-techniques.", "labels": [], "entities": []}, {"text": "In contrast to previous work which has primarily been focused on subword units for machine translation , we are interested in the general properties of such segments above the word level.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7339745759963989}]}, {"text": "We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution.", "labels": [], "entities": []}, {"text": "The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings.", "labels": [], "entities": []}, {"text": "We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural Language Processing (NLP) requires data to be segmented into units.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6971949984629949}]}, {"text": "These units are normally called words, which in itself is a somewhat vague and controversial concept) that is often operationalized as meaning something like \"white-space (and punctuation) delimited string of characters\".", "labels": [], "entities": []}, {"text": "Of course, some languages do not use white-space delimiters, such as Chinese and Thai, which have context-dependent notions of what constitute words without special symbols dedicated to segmentation.", "labels": [], "entities": []}, {"text": "As an example, the sequence \u6211\u559c\u6b22\u65b0\u897f\u5170\u82b1 can be segmented (correctly) in two different ways: Even for white-space segmenting languages, it is seldom as simple as merely using white-space delimited strings of characters as atomic units.", "labels": [], "entities": []}, {"text": "As one example, morphologically sparse languages such as English rely to a large extent on word order to encode grammar, which means that such languages often form lexical multi-word units, which by all accounts function as atomic units on the same level as white-space delimited words.", "labels": [], "entities": []}, {"text": "As an example, \"white house\" and \"rock and roll\" are both distinct semantic concepts that it would be beneficial to include as atomic units in an NLP application.", "labels": [], "entities": []}, {"text": "Of course, atomic units of language can also exist below the level of white-space delimited strings of characters.", "labels": [], "entities": []}, {"text": "In linguistics, morphemes are defined as the atomic units of language.", "labels": [], "entities": []}, {"text": "For synthetic languages such as Turkish, Finnish, or Greenlandic, where grammatical relations are encoded by morphology rather than word order, there can be a possibly large number of morphemes within one single white-space delimited string of characters.", "labels": [], "entities": []}, {"text": "The canonical example in this case tends to be Western Greenlandic, which is a polysynthetic language that produces notoriously long white-space delimited string of characters.", "labels": [], "entities": []}, {"text": "As an example, the string \"tusaanngitsuusaartuaannarsinnaanngivipputit\" consists of 9 different morphemes (\"hear\"|neg.|intrans.participle|\"pretend\"|\"all the time\"|\"can\"|neg.|\"really\"|2nd.sng.indicat.) and \u2020 These authors contributed equally to the means \"you simply cannot pretend not to be hearing all the time\".", "labels": [], "entities": []}, {"text": "One white-space delimited string of characters in Western Greenlandic, eleven in English.", "labels": [], "entities": []}, {"text": "Similarly, compounding languages such as Swedish can form productive compounds, where a potentially large number of words (and morphemes) are compounded into one single white-space delimited string of characters.", "labels": [], "entities": []}, {"text": "As an example, the string \"forskningsinformationsf\u00f6rs\u00f6rjningssystemet\" is a compound of the words for research information supply system.", "labels": [], "entities": []}, {"text": "The arbitrariness of segmenting units based on white space becomes especially clear when considering translations between languages.", "labels": [], "entities": []}, {"text": "As one example, the concept of a \"knife sharpener\" is realized as two white-space delimited strings of characters in English, one in Swedish (\"knivslip\"), and three in Spanish (\"afilador de cuchillo\").", "labels": [], "entities": [{"text": "knife sharpener\"", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7896578709284464}]}, {"text": "Segmentation is thus as non-trivial as it is foundational for NLP.", "labels": [], "entities": []}, {"text": "Consequently, there exists a large body of work on segmentation algorithms (often driven by the need for segmenting languages other than English).", "labels": [], "entities": [{"text": "segmentation algorithms", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.9529819488525391}]}, {"text": "Examples include; Chen and Liu (1992);;;;.", "labels": [], "entities": []}, {"text": "Related areas (from the perspective of segmentation) such as multiword expressions and morphological normalization also have a rich literature of prior art.", "labels": [], "entities": [{"text": "morphological normalization", "start_pos": 87, "end_pos": 114, "type": "TASK", "confidence": 0.7118558138608932}]}, {"text": "For multiword expressions, see e.g.;;, and for morphological normalization see e.g.;;.", "labels": [], "entities": []}, {"text": "In recent years, interest have begun to shift towards the use of character-level techniques, which bypass the problem of segmentation by simply operating on the raw character sequence.", "labels": [], "entities": []}, {"text": "Much of this work is driven by research on deep learning, and techniques inspired by neural language models.", "labels": [], "entities": []}, {"text": "In theory, such models can learn task-specific segmentations of the input that are optimal for solving whatever task the network is trained to perform.", "labels": [], "entities": []}, {"text": "The approach presented in this paper is inspired by character-level modeling, but in contrast to such techniques we seek a task-independent and objective segmentation of text.", "labels": [], "entities": []}, {"text": "Our work is motivated by the idea that if there exists an optimal and language-invariant segmentation of text, it should be based on statistical properties of language rather than heuristics.", "labels": [], "entities": []}, {"text": "We argue that such a segmentation exists, and introduce a novel type of data-driven segmented unit: the recursion-gram or r-gram in short.", "labels": [], "entities": []}, {"text": "The name is inspired by the n-gram introduced by, who used it to explore language modeling in the context of information entropy, which was also introduced in the same paper.", "labels": [], "entities": []}, {"text": "Our approach is inspired by information theoretic concerns.", "labels": [], "entities": []}, {"text": "In the applications where r-grams can be used, it replaces segmentation but not necessarily normalization.", "labels": [], "entities": []}, {"text": "R-grams capture a range of semantic units from morphemes (or more generally, parts of words) to words to compounds to multi-word units, all based on simple frequency statistics.", "labels": [], "entities": []}, {"text": "In this paper, we demonstrate an algorithm for computing one type of r-grams, and discuss novel observations and characteristics of the statistical distribution of natural language.", "labels": [], "entities": []}, {"text": "We then demonstrate how r-grams can be used as basic building blocks in embeddings, and evaluate the resulting embeddings using both monolingual and multilingual test sets.", "labels": [], "entities": []}, {"text": "We conclude the paper with some directions for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Comparison of word embeddings benchmarks using r-grams and words.", "labels": [], "entities": []}, {"text": " Table 4: Examples of the 5 nearest neighbors to four different targets in the r-gram embedding.", "labels": [], "entities": []}, {"text": " Table 5: R-gram and baseline performance and coverage on the word analogy tasks. The baseline is  taken from Grave et al.", "labels": [], "entities": [{"text": "R-gram", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9204589128494263}, {"text": "coverage", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9847519993782043}, {"text": "word analogy tasks", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8041955232620239}]}]}