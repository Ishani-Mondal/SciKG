{"title": [{"text": "Silent HMMs: Generalized Representation of Hidden Semi-Markov Models and Hierarchical HMMs", "labels": [], "entities": []}], "abstractContent": [{"text": "Modeling sequence data using probabilistic finite state machines (PFSMs) is a technique that analyzes the underlying dynamics in sequences of symbols.", "labels": [], "entities": []}, {"text": "Hidden semi-Markov models (HSMMs) and hierarchical hidden Markov models (HHMMs) are PFSMs that have been successfully applied to a wide variety of applications by extending HMMs to make the extracted patterns easier to interpret.", "labels": [], "entities": []}, {"text": "However, these models are independently developed with their own training algorithm, so that we cannot combine multiple kinds of structures to build a PFSM fora specific application.", "labels": [], "entities": []}, {"text": "In this paper, we prove that silent hidden Markov models (silent HMMs) are flexible models that have more expressive power than HSMMs and HHMMs.", "labels": [], "entities": []}, {"text": "Silent HMMs are HMMs that contain silent states, which do not emit any observations.", "labels": [], "entities": []}, {"text": "We show that we can obtain silent HMM equivalent to given HSMMs and HHMMs.", "labels": [], "entities": []}, {"text": "We believe that these results form a firm foundation to use silent HMMs as a unified representation for PFSM modeling.", "labels": [], "entities": [{"text": "PFSM modeling", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.9405693113803864}]}], "introductionContent": [{"text": "Probabilistic finite state machines (PFSMs) are widely used for modeling non-deterministic behaviors in languages (.", "labels": [], "entities": []}, {"text": "One of the powerful applications of PFSMs is automatic (unsupervised) induction of language patterns ().", "labels": [], "entities": []}, {"text": "The automatic induction of finite state models can potentially impact the direction that research takes on finite state machines, which have been applied to natural language processing such as morphological modeling, word transduction between different languages, dialog action, etc.", "labels": [], "entities": [{"text": "word transduction between different languages", "start_pos": 217, "end_pos": 262, "type": "TASK", "confidence": 0.8314991235733032}]}, {"text": "Hidden Markov models (HMMs) are the simplest and most well-known probabilistic finite state machines.", "labels": [], "entities": []}, {"text": "However, the unsupervised training of HMMs usually does not produce good finite state machines like the ones crafted by human experts because of the complexity of reconstructing language patterns from a finite number of observations.", "labels": [], "entities": []}, {"text": "Human experts can build finite state machines that are comprehensible because they have intuition about the latent structure of languages.", "labels": [], "entities": []}, {"text": "This discussion suggests that we need to incorporate prior knowledge into the model structure of HMMs, which is a basic idea that pervades the recent methods of automatic induction of language patterns.", "labels": [], "entities": [{"text": "automatic induction of language patterns", "start_pos": 161, "end_pos": 201, "type": "TASK", "confidence": 0.694282329082489}]}, {"text": "Several kinds of PFSMs, such as hidden semi-Markov models (HSMMs) ( and hierarchical hidden Markov models (, reflect several different additional structural assumptions.", "labels": [], "entities": []}, {"text": "Each model comes with a specialized training algorithm that has to be implemented separately.", "labels": [], "entities": []}, {"text": "This requirement prevents us from trying several models; more importantly, we cannot easily combine multiple assumptions that are implemented in different PFSMs.", "labels": [], "entities": []}, {"text": "To move the research of automatic finite state machine induction forward, we need to develop a more flexible way to incorporate our prior knowledge into the PFSMs.", "labels": [], "entities": [{"text": "finite state machine induction", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.5929722636938095}]}, {"text": "In this paper, we propose silent hidden Markov models (silent HMMs) as a generalized representation of other PFSMs that at least can express the structure that is assumed in HSMMs and HHMMs.", "labels": [], "entities": []}, {"text": "A silent HMM is an HMM that contains silent states, which do not emit any observations.", "labels": [], "entities": []}, {"text": "We prove that the expressive power of silent HMMs is better than HSMMs and HHMMs, and we propose a method that obtains a silent HMM that is equivalent to an HSMM and an HHMM.", "labels": [], "entities": []}, {"text": "This result indicates that we can combine and/or customize the structural assumptions of HSMMs and HHMMs in the unified framework of silent HMMs, potentially leading us to more precise and flexible automatic induction of finite state machines.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we dfine silent HMMs.", "labels": [], "entities": []}, {"text": "In Sections 3 and 4, we detail the HSMMs and HHMMs respectively and prove the expressivity of silent HMMs is better than these models.", "labels": [], "entities": [{"text": "HHMMs", "start_pos": 45, "end_pos": 50, "type": "DATASET", "confidence": 0.7576578855514526}]}, {"text": "In Section 5, we discuss an inference algorithm of silent HMMs.", "labels": [], "entities": []}, {"text": "In Section 6, we conclude the discussion and mention future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}