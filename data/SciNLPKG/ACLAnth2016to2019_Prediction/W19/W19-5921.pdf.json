{"title": [], "abstractContent": [{"text": "Neural dialog models have exhibited strong performance, however their end-to-end nature lacks a representation of the explicit structure of dialog.", "labels": [], "entities": []}, {"text": "This results in a loss of generaliz-ability, controllability and a data-hungry nature.", "labels": [], "entities": []}, {"text": "Conversely, more traditional dialog systems do have strong models of explicit structure.", "labels": [], "entities": []}, {"text": "This paper introduces several approaches for explicitly incorporating structure into neu-ral models of dialog.", "labels": [], "entities": []}, {"text": "Structured Fusion Networks first learn neural dialog modules corresponding to the structured components of traditional dialog systems and then incorporate these modules in a higher-level genera-tive model.", "labels": [], "entities": []}, {"text": "Structured Fusion Networks obtain strong results on the MultiWOZ dataset, both with and without reinforcement learning.", "labels": [], "entities": [{"text": "MultiWOZ dataset", "start_pos": 56, "end_pos": 72, "type": "DATASET", "confidence": 0.8746605217456818}]}, {"text": "Structured Fusion Networks are shown to have several valuable properties, including better domain generalizability, improved performance in reduced data scenarios and ro-bustness to divergence during reinforcement learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "End-to-end neural dialog systems have shown strong performance (.", "labels": [], "entities": []}, {"text": "However such models suffer from a variety of shortcomings, including: a data-hungry nature (, a tendency to produce generic responses (), an inability to generalize), alack of controllability (, and divergent behavior when tuned with reinforcement learning (.", "labels": [], "entities": []}, {"text": "Traditional dialog systems, which are generally free of these problems, consist of three distinct components: the natural language understanding (NLU), which produces a structured representation of an * * Equal contribution.", "labels": [], "entities": []}, {"text": "input (e.g., a belief state); the natural language generation (NLG), which produces output in natural language conditioned on an internal state (e.g. dialog acts); and the dialog manager (DM), which describes a policy that combines an input representation (e.g., a belief state) and information from some database to determine the desired continuation of the dialog (e.g., dialog acts).", "labels": [], "entities": []}, {"text": "A traditional dialog system, consisting of an NLU, DM and NLG, is pictured in.", "labels": [], "entities": []}, {"text": "The structured components of traditional dialog systems facilitate effective generalizability, interpretability, and controllability.", "labels": [], "entities": []}, {"text": "The structured output of each component allows for straightforward modification, understanding and tuning of the system.", "labels": [], "entities": []}, {"text": "On the other hand, end-to-end neural models of dialog lack an explicit structure and are treated as a black box.", "labels": [], "entities": []}, {"text": "To this end, we explore several methods of incorporating the structure of traditional dialog systems into neural dialog models.", "labels": [], "entities": []}, {"text": "First, several neural dialog modules are constructed to serve the role of the NLU, the DM and the NLG.", "labels": [], "entities": []}, {"text": "Next, a number of methods are proposed for incorporating these dialog modules into end-to-end dialog systems, including Na\u00a8\u0131veNa\u00a8\u0131ve Fusion, Multitask Fusion and Structured Fusion Networks (SFNs).", "labels": [], "entities": []}, {"text": "This paper will show that SFNs obtain strong results on the MultiWOZ dataset () both with and without the use of reinforcement learning.", "labels": [], "entities": [{"text": "SFNs", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9790294170379639}, {"text": "MultiWOZ dataset", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.8442048132419586}]}, {"text": "Due to the explicit structure of the model, SFNs are shown to exhibit several valuable properties including improved performance in reduced data scenarios, better domain generalizability and robustness to divergence during reinforcement learning ().", "labels": [], "entities": [{"text": "SFNs", "start_pos": 44, "end_pos": 48, "type": "TASK", "confidence": 0.9347131848335266}]}], "datasetContent": [{"text": "The dialog systems are evaluated on the Multi-WOZ dataset (, which consists often thousand human-human conversations covering several domains.", "labels": [], "entities": [{"text": "Multi-WOZ dataset", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.8140088617801666}]}, {"text": "The MultiWOZ dataset contains conversations between a tourist and a clerk at an information center which fall into one of seven domains -attraction, hospital, police, hotel, restaurant, taxi, train.", "labels": [], "entities": [{"text": "MultiWOZ dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7494000494480133}]}, {"text": "Individual conversations span one to five of the domains.", "labels": [], "entities": []}, {"text": "Dialogs were collected using the Wizard-of-Oz framework, where one participant plays the role of an automated system.", "labels": [], "entities": []}, {"text": "Each dialog consists of a goal and multiple user and system utterances.", "labels": [], "entities": []}, {"text": "Each turn is annotated with two binary vectors: a belief state vector and a dialog act vector.", "labels": [], "entities": []}, {"text": "A single turn may have multiple positive values in both the belief state and dialog act vectors.", "labels": [], "entities": []}, {"text": "The belief state and dialog act vectors are of dimensions 94 and 593, respectively.", "labels": [], "entities": []}, {"text": "Several metrics are used to evaluate the models.", "labels": [], "entities": []}, {"text": "BLEU () is used to com- pute the word overlap between the generated output and the reference response.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9863507151603699}]}, {"text": "Two task-specific metrics, defined by, Inform rate and Success rate, are also used.", "labels": [], "entities": [{"text": "Inform rate", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9859659373760223}, {"text": "Success rate", "start_pos": 55, "end_pos": 67, "type": "METRIC", "confidence": 0.9873780906200409}]}, {"text": "Inform rate measures how often the system has provided the appropriate entities to the user.", "labels": [], "entities": [{"text": "Inform rate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9608948230743408}]}, {"text": "Success rate measures how often the system answers all the requested attributes.", "labels": [], "entities": [{"text": "Success rate", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9849224984645844}]}, {"text": "Similarly to, the best model is selected during validation using the combined score which is defined as BLEU + 0.5 \u00d7 (Inf orm + Success).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9991845488548279}, {"text": "Inf orm + Success)", "start_pos": 118, "end_pos": 136, "type": "METRIC", "confidence": 0.9365639328956604}]}, {"text": "This combined score is also reported as an evaluation metric.", "labels": [], "entities": []}, {"text": "The hyperparameters match those used by: embedding dimension of 50, hidden dimension of 150, and a single-layer LSTM.", "labels": [], "entities": []}, {"text": "All models are trained for 20 epochs using the Adam optimizer (, with a learning rate of 0.005 and batch size of 64.", "labels": [], "entities": []}, {"text": "The norm of the gradients are clipped to 5 (Pascanu et al., 2012).", "labels": [], "entities": []}, {"text": "Greedy decoding is used during inference.", "labels": [], "entities": []}, {"text": "All previous work uses the ground-truth belief state vector during training and evaluation.", "labels": [], "entities": []}, {"text": "Therefore the experiments with the SFNs have the NLU module replaced by an \"oracle NLU\" which always outputs the ground-truth belief state.", "labels": [], "entities": []}, {"text": "in the Appendix shows experimental results which demonstrate that using only the ground-truth belief state results in the best performance.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8226487040519714}]}, {"text": "To supplement the results in, human evaluation was used to compare seq2seq, SFN, SFN fine-tuned with reinforcement learning, and the ground-truth human response.", "labels": [], "entities": []}, {"text": "Workers on Amazon Mechanical Turk (AMT) were asked to read the context, and score the appropriateness of each response on a Likert scale (1-5).", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.8413753608862559}, {"text": "Likert scale", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.8859565556049347}]}, {"text": "One hundred context-response pairs were labeled by three workers each.", "labels": [], "entities": []}, {"text": "The results shown in demonstrate that SFNs with RL outperform the other methods in terms of human judgment.", "labels": [], "entities": [{"text": "SFNs", "start_pos": 38, "end_pos": 42, "type": "TASK", "confidence": 0.9635822772979736}]}, {"text": "These results indicate that in addition to improving on automated metrics, SFNs result in user-favored responses.", "labels": [], "entities": [{"text": "SFNs", "start_pos": 75, "end_pos": 79, "type": "TASK", "confidence": 0.9541127681732178}]}], "tableCaptions": [{"text": " Table 1: Experimental results for the various models. This table compares two classes of methods: those trained  with supervised learning and those trained with reinforcement learning. All bold-face results are statistically  significant (p < 0.01).", "labels": [], "entities": []}, {"text": " Table 2: Results of human evaluation experiments. The  \u2265 4 and \u2265 5 columns indicate the percentage of system  outputs which obtained a greater than 4 and 5 rating,  respectively.", "labels": [], "entities": []}, {"text": " Table 3: Results of the domain transfer experiment  comparing sequence-to-sequence and Structured Fu- sion Networks. All bold-face results are statistically  significant (p < 0.01).", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8048061430454254}]}, {"text": " Table 4: Results of the domain transfer experiment  comparing sequence-to-sequence and Structured Fu- sion Networks. All bold-face results are statistically  significant (p < 0.01).", "labels": [], "entities": [{"text": "domain transfer", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.8053319454193115}]}]}