{"title": [{"text": "Application of an Automatic Plagiarism Detection System in a Large-scale Assessment of English Speaking Proficiency", "labels": [], "entities": [{"text": "Assessment of English Speaking Proficiency", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.5544501662254333}]}], "abstractContent": [{"text": "This study aims to build an automatic system for the detection of plagiarized spoken responses in the context of an assessment of English speaking proficiency for non-native speakers.", "labels": [], "entities": []}, {"text": "Classification models were trained to distinguish between plagiarized and non-plagiarized responses with two different types of features: text-to-text content similarity measures, which are commonly used in the task of plagiarism detection for written documents , and speaking proficiency measures, which were specifically designed for spontaneous speech and extracted using an automated speech scoring system.", "labels": [], "entities": [{"text": "task of plagiarism detection for written documents", "start_pos": 211, "end_pos": 261, "type": "TASK", "confidence": 0.8373863867350987}]}, {"text": "The experiments were first conducted on a large data set drawn from an operational English proficiency assessment across multiple years, and the best classifier on this heavily imbalanced data set resulted in an F1-score of 0.761 on the plagiarized class.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.9992818236351013}]}, {"text": "This system was then validated on operational responses collected from a single administration of the assessment and achieved a recall of 0.897.", "labels": [], "entities": [{"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9930235147476196}]}, {"text": "The results indicate that the proposed system can potentially be used to improve the validity of both human and automated assessment of non-native spoken En-glish.", "labels": [], "entities": [{"text": "validity", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.970109760761261}]}], "introductionContent": [{"text": "Plagiarism of spoken responses has become a vexing problem in the domain of spoken language assessment, in particular, the evaluation of nonnative speaking proficiency, since there exists avast amount of easily accessible online resources covering a wide variety of topics that test takers can use to prepare responses prior to the test.", "labels": [], "entities": []}, {"text": "In the context of large-scale, standardized assessments of spoken English for academic purposes, such as the TOEFL iBT test, the Pearson Test of English Academic, and the IELTS Academic assessment), some test takers may utilize content from online resources or other prepared sources in their spoken responses to test questions that are intended to elicit spontaneous speech.", "labels": [], "entities": [{"text": "TOEFL iBT test", "start_pos": 109, "end_pos": 123, "type": "DATASET", "confidence": 0.8086305061976115}, {"text": "Pearson Test of English Academic", "start_pos": 129, "end_pos": 161, "type": "DATASET", "confidence": 0.9239070534706115}, {"text": "IELTS Academic assessment", "start_pos": 171, "end_pos": 196, "type": "DATASET", "confidence": 0.7109889586766561}]}, {"text": "These responses that are based on canned material pose a problem for both human raters and automated scoring systems, and can reduce the validity of scores that are provided to the test takers; therefore, research into the automated detection of plagiarized spoken responses is necessary.", "labels": [], "entities": [{"text": "validity", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9886243343353271}]}, {"text": "In this paper, we investigate a variety of features for automatically detecting plagiarized spoken responses in the context of a standardized assessment of English speaking proficiency.", "labels": [], "entities": []}, {"text": "In addition to examining several commonly used text-to-text content similarity features, we also use features that compare various aspects of speaking proficiency across multiple responses provided by a test taker, based on the hypothesis that certain aspects of speaking proficiency, such as fluency, maybe artificially inflated in a test taker's canned responses in comparison to non-canned responses.", "labels": [], "entities": []}, {"text": "These features are designed to be independent of the availability of the reference source materials.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the effectiveness of this system on a data set with a large number of control (non-plagiarized) responses in an attempt to simulate the imbalanced distribution from an operational setting in which only a small number of the test takers' responses are plagiarized.", "labels": [], "entities": []}, {"text": "In addition, we further validate this system on operational data and show how it can practically assist both human and automated scoring in a large scale assessment of English speaking proficiency", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to ASR failures, a small number of responses were excluded from the experiments; finally, a total of 1,551 canned and 66,257 control responses were included in the simulated data.", "labels": [], "entities": [{"text": "ASR", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9791231155395508}]}, {"text": "Since this work was conducted on a very imbalanced data set and only 2.3% of the responses in the simulated data are authentic plagiarized ones confirmed by human raters, 10-fold cross-validation was performed first on the simulated data.", "labels": [], "entities": []}, {"text": "Afterward, the classification model built on the simulated data set was further evaluated on a corpus with real operational data.", "labels": [], "entities": []}, {"text": "We employed the machine learning tool of scikit-learn 3 (Pedregosa et al., 2011), for training the classifier.", "labels": [], "entities": []}, {"text": "It provides various classification methods, such as decision tree, random forest, AdaBoost, etc.", "labels": [], "entities": []}, {"text": "This study involves a variety of features from two different categories, and preliminary experiments demonstrated that the random forest model can achieve the overall better perfor-mance.", "labels": [], "entities": []}, {"text": "Therefore, the random forest method is used to build classification models in the following experiments, and the precision, recall, as well as F1-score on the positive class (plagiarized responses) are used as the evaluation metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9994702935218811}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.995015561580658}, {"text": "F1-score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9992204904556274}]}, {"text": "First, in order to verify the effectiveness of the newly developed n-gram overlap features, a preliminary experiment was conducted to compare this set of features with BLEU-based features, since they had been shown to be effective in previous research (.", "labels": [], "entities": [{"text": "BLEU-based", "start_pos": 168, "end_pos": 178, "type": "METRIC", "confidence": 0.9956633448600769}]}, {"text": "The results as shown in indicate that the F1-Measure of the n-gram features outperforms the BLEU features (0.761 vs. 0.748), and the recall of the ngram features is higher than the BLEU features (0.716 vs. 0.683); inversely, the BLEU features result in higher precision (0.83 vs. 0.814).", "labels": [], "entities": [{"text": "F1-Measure", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9986400008201599}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9968602657318115}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9994747042655945}, {"text": "BLEU", "start_pos": 181, "end_pos": 185, "type": "METRIC", "confidence": 0.9937479496002197}, {"text": "BLEU", "start_pos": 229, "end_pos": 233, "type": "METRIC", "confidence": 0.9905754327774048}, {"text": "precision", "start_pos": 260, "end_pos": 269, "type": "METRIC", "confidence": 0.9978815913200378}]}, {"text": "Accordingly, the n-gram features are used to replace the BLEU ones, since it is more important to reduce the number of false negatives, i.e., improve the recall, for our task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9978981018066406}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.999413013458252}]}, {"text": "Furthermore, each individual type of feature and their combinations were examined in the classification experiments described above.", "labels": [], "entities": []}, {"text": "As shown in, each feature set alone was used to build classification models.", "labels": [], "entities": []}, {"text": "The n-gram overlap features result in the best performance with an F1-score of 0.761, and the WMD features capturing the topical relevance between word pairs result in a much lower F1-score of 0.649.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993748068809509}, {"text": "F1-score", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9973607659339905}]}, {"text": "Furthermore, the combination of both types of content similarity features, i.e., n-gram and WMD, slightly reduces the F1-score to 0.76.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9995940327644348}]}, {"text": "These results indicate that for this particular task, the exact match of certain expressions appearing in both the test response and a source material plays a critical role.", "labels": [], "entities": []}, {"text": "As to the speaking proficiency related features, they can lead to a promising precision of 0.8 but with a very low recall of 0.009.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.999043881893158}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9989909529685974}]}, {"text": "After reexamining the assumption that human experts maybe able to differentiate prepared speech from fully spontaneous speech based on the way how the speech is delivered, it turns out that it is quite challenging for human experts to make a reliable judgment of plagiarism just based on the speech delivery without any reference to the source materials, in particular, within the context of high-stakes language assessment.", "labels": [], "entities": []}, {"text": "Accordingly, the features capturing the difference in speaking proficiency of prepared and spontaneous speech can be used as contributory information to improve the accuracy of an automatic detection system, but they are unable to achieve promising performance alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.998530387878418}]}, {"text": "Also as shown in, By adding the speaking proficiency features, the precision can be improved to 0.821, but the recall is reduced; finally, the F1-score is reduced to 0.752.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9997813105583191}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.999649167060852}, {"text": "F1-score", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9995166063308716}]}], "tableCaptions": [{"text": " Table 2: Word error rate (WER %) reduction with  an unsupervised language model adaptation method,  where the WERs on Independent items (IND), Inte- grated items (INT), as well as all items (ALL), are  reported. The WER with the supervised adaptation  method based on human transcriptions is also listed for  comparison.", "labels": [], "entities": [{"text": "Word error rate (WER %)", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8342698514461517}, {"text": "Inte- grated items (INT)", "start_pos": 144, "end_pos": 168, "type": "METRIC", "confidence": 0.8632890837533134}]}, {"text": " Table 3: Comparison of n-gram and BLEU features.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9973502159118652}]}, {"text": " Table 4: Classification performance using each individ- ual feature set and their combinations.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9557892680168152}]}]}