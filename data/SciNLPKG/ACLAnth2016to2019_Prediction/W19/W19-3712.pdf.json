{"title": [{"text": "Tuning Multilingual Transformers for Named Entity Recognition on Slavic Languages", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.7392295996348063}]}], "abstractContent": [{"text": "Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish.", "labels": [], "entities": [{"text": "multilingual named entity recognition", "start_pos": 35, "end_pos": 72, "type": "TASK", "confidence": 0.6129952371120453}]}, {"text": "We solve this task using the BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9933801889419556}]}, {"text": "We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages.", "labels": [], "entities": []}, {"text": "Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7718111276626587}]}, {"text": "Additional improvement is achieved by extending BERT with a word-level CRF layer.", "labels": [], "entities": [{"text": "BERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.825598955154419}]}, {"text": "Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and took the 1st place in 3 competition metrics out of 4 we participated in.", "labels": [], "entities": [{"text": "BSNLP 2019 Shared Task on Multilingual Named Entity Recognition", "start_pos": 28, "end_pos": 91, "type": "TASK", "confidence": 0.626166777478324}]}, {"text": "We open-sourced NER models and BERT model pre-trained on the four Slavic languages.", "labels": [], "entities": [{"text": "BERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9918505549430847}]}], "introductionContent": [{"text": "Named Entity Recognition (further, NER) is a task of recognizing named entities in running text, as well as detecting their type.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6377256910006205}, {"text": "NER)", "start_pos": 35, "end_pos": 39, "type": "TASK", "confidence": 0.7428070306777954}]}, {"text": "For example, in the sentence Asia Bibi is from Pakistan, the following NER classes can be detected: [Asia Bibi] PER is from LOC . The commonly used BIOannotation for this sentence is shown in.", "labels": [], "entities": [{"text": "PER", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9578948616981506}, {"text": "BIOannotation", "start_pos": 148, "end_pos": 161, "type": "METRIC", "confidence": 0.9049671292304993}]}, {"text": "The recognizer of named entities can be trained on a single target task dataset as any other sequence tagging model.", "labels": [], "entities": []}, {"text": "However, it often benefits from additional data from a different source, either labeled or unlabeled, which is known as transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 120, "end_pos": 137, "type": "TASK", "confidence": 0.9305358529090881}]}, {"text": "To enrich the model one can either train it on several tasks simultaneously), which makes its word representations more flexible and robust, or pretrain on large amounts of unlabeled data to utilize unlimited sources available in the Web and then fine-tune them on a specific task.", "labels": [], "entities": []}, {"text": "One of the most powerful unsupervised models is BERT (, which is a multi-layer Transformer trained on the objective of masked words recovery and on the task of next sentence prediction (known also as Natural Language Inference (NLI) task).", "labels": [], "entities": [{"text": "BERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9966620206832886}, {"text": "masked words recovery", "start_pos": 119, "end_pos": 140, "type": "TASK", "confidence": 0.6439400215943655}, {"text": "next sentence prediction", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.5920619269212087}]}, {"text": "The original model was trained on vast amounts of data for more than 104 languages which makes its representations useful for almost any task.", "labels": [], "entities": []}, {"text": "Our contribution is three-fold: first, multilingual BERT embeddings with a dense layer on the top clearly beat BiLSTM-CRF over FastText embeddings trained on the four target languages.", "labels": [], "entities": []}, {"text": "Second, languagespecific BERT, trained only on the target languages from Wikipedia and news dump, significantly outperforms the multilingual BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9651625752449036}, {"text": "BERT", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.869171679019928}]}, {"text": "Third, we adapt a CRF layer as a atop module over the outputs of the BERT-based model and demonstrate that it improves performance even further.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9443748593330383}]}], "datasetContent": [{"text": "The 2019 edition of the Balto-Slavic Natural Language Processing (BSNLP) ( shared task aims at recognizing mentions of named entities in web documents in Slavic languages.", "labels": [], "entities": [{"text": "Balto-Slavic Natural Language Processing (BSNLP) ( shared task", "start_pos": 24, "end_pos": 86, "type": "TASK", "confidence": 0.6668707430362701}, {"text": "recognizing mentions of named entities in web documents in Slavic languages", "start_pos": 95, "end_pos": 170, "type": "TASK", "confidence": 0.8153351653705944}]}, {"text": "The input text collection consists of sets of news articles from online media, each collection revolving around a certain entity or an event.", "labels": [], "entities": []}, {"text": "The corpus was obtained by crawling the web and parsing the HTML of relevant documents.", "labels": [], "entities": []}, {"text": "The 2019 edition of the shared task covers 4 languages (Bulgarian, Czech, Polish, Russian) and focuses on recognition of five types of named entities including persons (PER), locations (LOC), organizations (ORG), events (EVT) and products (PRO).", "labels": [], "entities": []}, {"text": "The dataset consists of pairs of files: news text and a file with mentions of entities with corresponding tags.", "labels": [], "entities": []}, {"text": "There are two groups of documents in the train part of the dataset.", "labels": [], "entities": []}, {"text": "Namely, news about Asia Bibi and Brexit.", "labels": [], "entities": [{"text": "Asia Bibi", "start_pos": 19, "end_pos": 28, "type": "DATASET", "confidence": 0.7943524420261383}, {"text": "Brexit", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.6257570385932922}]}, {"text": "Brexit part is substantially bigger, therefore, we used it for training and Asia Bibi for validation.", "labels": [], "entities": [{"text": "Brexit", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9184350371360779}, {"text": "Asia Bibi", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.5547350943088531}]}], "tableCaptions": [{"text": " Table 1: Metrics for BSNLP on validation set (Asia Bibi documents). Metrics on the test set are in the brackets.", "labels": [], "entities": [{"text": "BSNLP", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9104218482971191}, {"text": "Asia Bibi documents", "start_pos": 47, "end_pos": 66, "type": "DATASET", "confidence": 0.9279892444610596}]}]}