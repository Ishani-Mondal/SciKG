{"title": [{"text": "Relevant and Informative Response Generation using Pointwise Mutual Information", "labels": [], "entities": [{"text": "Relevant and Informative Response Generation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7600975871086121}]}], "abstractContent": [{"text": "A sequence-to-sequence model tends to generate generic responses with little information for input utterances.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a neural model that generates relevant and informative responses.", "labels": [], "entities": []}, {"text": "Our model has simple architecture to enable easy application to existing neural dialogue models.", "labels": [], "entities": []}, {"text": "Specifically , using positive pointwise mutual information , it first identifies keywords that frequently co-occur in responses given an utterance.", "labels": [], "entities": []}, {"text": "Then, the model encourages the de-coder to use the keywords for response generation.", "labels": [], "entities": [{"text": "response generation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.816118448972702}]}, {"text": "Experiment results demonstrate that our model successfully diversifies responses relative to previous models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural networks are common approaches to building chat-bots.", "labels": [], "entities": []}, {"text": "have proposed a neural dialogue model using sequenceto-sequence (Seq2Seq) networks) and achieved fluent response generation.", "labels": [], "entities": [{"text": "fluent response generation", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.6115262309710184}]}, {"text": "Because a Seq2Seq model uses a word-by-word loss function at the time of training, any words outside the reference are penalized equally.", "labels": [], "entities": []}, {"text": "Consequently, the Seq2Seq model tends to generate generic responses that consist of frequent words, such as \"Yes\" and \"I don't know.\"", "labels": [], "entities": []}, {"text": "This is a central concern in neural dialogue generation.", "labels": [], "entities": [{"text": "neural dialogue generation", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.7306749224662781}]}, {"text": "To tackle this problem, proposed a model for considering mutual dependency between an utterance and response modeled by maximum mutual information (MMI).", "labels": [], "entities": [{"text": "maximum mutual information (MMI)", "start_pos": 120, "end_pos": 152, "type": "METRIC", "confidence": 0.6825787425041199}]}, {"text": "However, their model disregarded the aspect of informativeness of responses, which is also important for user experience of chat-bots.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a response generation model that outputs diverse words while preserving relevance in response to the input utterance.", "labels": [], "entities": [{"text": "response generation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7048400938510895}]}, {"text": "In our model, Positive Pointwise Mutual Information (PPMI) identifies keywords from a large-scale conversational corpus that are likely to appear in the response to an input utterance.", "labels": [], "entities": []}, {"text": "Then, the model modifies the loss function in a Seq2Seq model to reward responses using the identified keywords.", "labels": [], "entities": []}, {"text": "In order to calculate the loss function using the words output by the decoder, we need to sample words from the probability distribution of the output layer.", "labels": [], "entities": []}, {"text": "Hence, we apply the GumbelSoftmax trick) as a differentiable pseudo-sampling method.", "labels": [], "entities": []}, {"text": "Experiments using a Japanese dialogue corpus crawled from Twitter and OpenSubtitles revealed that the proposed model outperformed () for all automatic evaluation metrics for correspondence to references and diversity in outputs.", "labels": [], "entities": [{"text": "Japanese dialogue corpus crawled from Twitter", "start_pos": 20, "end_pos": 65, "type": "DATASET", "confidence": 0.7953025499979655}]}], "datasetContent": [{"text": "We empirically evaluate how our model avoids generic responses to generate relevant and informative responses.", "labels": [], "entities": []}, {"text": "We used two datasets, OpenSubtitles (English) and Twitter (Japanese).", "labels": [], "entities": []}, {"text": "The details of each dataset are as follows.", "labels": [], "entities": []}, {"text": ", we assumed that each line of the subtitles represents an independent utterance, and constructed a single-turn dialogue corpus by regarding two consecutive utterances as an utteranceresponse pair.", "labels": [], "entities": []}, {"text": "We randomly sampled 2 million utterance-response pairs.", "labels": [], "entities": []}, {"text": "All sentences were tokenized using the Punkt Sentence Tokenizer of nltk 1 . Twitter We crawled conversations in Japanese Twitter using \"@\" mention as a clue.", "labels": [], "entities": []}, {"text": "A single-turn dialogue corpus was constructed by regarding a tweet and its reply as an utterance-response pair.", "labels": [], "entities": []}, {"text": "The dataset consists of about 1.3 million utterance-response pairs.", "labels": [], "entities": []}, {"text": "All sentences were tokenized by MeCab 2 . In both datasets, 10k utterance-response pairs were separated as validation data, another 10k were separated as test data, and the rest were used as training data.", "labels": [], "entities": []}, {"text": "We employed several automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "BLEU and NIST measure the validity of generated sentences in comparison with references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9146225452423096}, {"text": "NIST", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.861076831817627}, {"text": "validity", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9452998638153076}]}, {"text": "BLEU () measures the correspondence between n-grams in generated responses and those in reference sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9812244176864624}]}, {"text": "Following, we used the average of BLEU scores from 1-gram to 4-gram in the experiment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9994925260543823}]}, {"text": "NIST) also measures the correspondence between generated responses and reference sentences.", "labels": [], "entities": [{"text": "NIST)", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9238607883453369}]}, {"text": "Unlike BLEU, NIST places lower weights on frequent n-grams, i.e., NIST regards content words as more important than function words.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9690048098564148}, {"text": "NIST", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8562792539596558}]}, {"text": "In the experiment, we used the average of NIST from 1-gram to 5-gram.", "labels": [], "entities": [{"text": "NIST", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.592658281326294}]}, {"text": "In addition, dist and ent measure the diversity of generated responses.", "labels": [], "entities": []}, {"text": "Dist () is defined as the number of distinct n-grams in generated responses divided by the total number of generated tokens.", "labels": [], "entities": [{"text": "Dist", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7804980874061584}]}, {"text": "On the other hand, ent) considers the frequency of n-grams in generated responses: where X is a set of n -grams output by the system, and F (w) computes the frequency of each n-gram.", "labels": [], "entities": []}, {"text": "In this paper, we focus on automatic evaluation.", "labels": [], "entities": []}, {"text": "Human evaluation is our future work.", "labels": [], "entities": [{"text": "Human evaluation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7657522559165955}]}], "tableCaptions": [{"text": " Table 1: Results on the OpenSubtitle corpus (English)", "labels": [], "entities": [{"text": "OpenSubtitle corpus", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.9257747530937195}]}, {"text": " Table 2: Results on the Twitter corpus (Japanese)", "labels": [], "entities": [{"text": "Twitter corpus", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.7298504710197449}]}]}