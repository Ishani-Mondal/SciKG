{"title": [{"text": "Improving Robustness of Neural Machine Translation with Multi-task Learning", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6087746719519297}]}], "abstractContent": [{"text": "While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise.", "labels": [], "entities": [{"text": "neural machine translation (NMT", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.7873569130897522}]}, {"text": "In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9118829965591431}]}, {"text": "We describe our submission to the WMT 2019 Robustness shared task (Li et al., 2019) based on this method.", "labels": [], "entities": [{"text": "WMT 2019 Robustness shared task", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.61463702917099}]}, {"text": "Our model achieves a BLEU score of 32.8 on the shared task French to En-glish dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text 1 .", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9841175973415375}, {"text": "French to En-glish dataset", "start_pos": 59, "end_pos": 85, "type": "DATASET", "confidence": 0.6503602266311646}, {"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9991282820701599}]}], "introductionContent": [{"text": "Real world data, especially in the realm of social media, often contains noise such as mis-spellings, grammar errors, or lexical variations.", "labels": [], "entities": []}, {"text": "Even though humans do not have much difficulty in recognizing and translating noisy or ungrammatical sentences, neural machine translation (NMT;;) systems are known to degrade drastically when confronted with noisy data.", "labels": [], "entities": [{"text": "recognizing and translating noisy or ungrammatical sentences", "start_pos": 50, "end_pos": 110, "type": "TASK", "confidence": 0.7134467022759574}, {"text": "neural machine translation", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.7266911069552103}]}, {"text": "Thus, there is increasing need to build robust NMT systems that are resilient to naturally occurring noise.", "labels": [], "entities": []}, {"text": "In this work, we attempt to enhance the robustness of the NMT system through multi-task learning.", "labels": [], "entities": []}, {"text": "Our model is a transformer-based model () augmented with two decoders, with each decoder bound to different learning objectives.", "labels": [], "entities": []}, {"text": "It has a cascade architecture ( where the first decoder reads in the output of the encoder and the second decoder reads in the output of both encoder and the first decoder.", "labels": [], "entities": []}, {"text": "The objective of the first decoder, namely the denoising decoder, is to recover from the noisy sentence and generate the corresponding clean sentence.", "labels": [], "entities": []}, {"text": "Given both the noisy and clean sentence, the objective of the second decoder, namely the translation decoder, is to correctly translate the sentence to the target language.", "labels": [], "entities": []}, {"text": "This framework should be beneficial in two ways: 1) Since the model is trained with noisy text, it should inherently better generalize to noisy text.", "labels": [], "entities": []}, {"text": "2) The translation decoder could potentially take advantage of the recovered clean sentence while maintaining specific varieties of noise (e.g. emoji) by referring to the original noisy sentence.", "labels": [], "entities": []}, {"text": "This framework requires triplets of clean and noisy source sentences, along with target translations, so we also follow and design a back-translation strategy that synthesizes noisy data.", "labels": [], "entities": []}, {"text": "Our proposed model outperforms the baseline vanilla transformer trained with clean text by 4.6 BLEU points on the WMT 2019 Robustness shared task () French to English dataset.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9988831877708435}, {"text": "WMT 2019 Robustness shared task () French to English dataset", "start_pos": 114, "end_pos": 174, "type": "DATASET", "confidence": 0.7431132107973099}]}, {"text": "The fine-tuning process brings an additional 2.5 points improvement.", "labels": [], "entities": []}, {"text": "According to our analysis, however, the improvements can mainly be attributed to introducing noisy data during training rather than the multi-task learning objective.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we first describe in detail our data pre-processing scheme, as well as the choice of hyperparameters.", "labels": [], "entities": []}, {"text": "Then we compare our system with the baseline model (a vanilla transformer trained on clean French and clean English parallel data).", "labels": [], "entities": []}, {"text": "Finally, we carryout a case study by comparing the output of our model with the baseline model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU score of different models. The second  column shows the score in MTNT test dataset intro- duced in Michel and Neubig (2018) and the third col- umn shows the score in the MTNT test dataset provided  by WMT Robustness share task (Li et al., 2019).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988942742347717}, {"text": "MTNT test dataset intro- duced", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.8657243847846985}, {"text": "MTNT test dataset", "start_pos": 185, "end_pos": 202, "type": "DATASET", "confidence": 0.8381050030390421}, {"text": "WMT Robustness share task", "start_pos": 216, "end_pos": 241, "type": "DATASET", "confidence": 0.773494765162468}]}]}