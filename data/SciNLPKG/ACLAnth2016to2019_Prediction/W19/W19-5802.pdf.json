{"title": [{"text": "LIMSI-MULTISEM at the IJCAI SemDeep-5 WiC Challenge: Context Representations for Word Usage Similarity Estimation", "labels": [], "entities": [{"text": "LIMSI-MULTISEM", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8567113280296326}, {"text": "IJCAI SemDeep-5 WiC Challenge", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.8559346497058868}, {"text": "Word Usage Similarity Estimation", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.8334726393222809}]}], "abstractContent": [{"text": "We present the LIMSI-MULTISEM system submitted to the IJCAI-19 SemDeep-5 WiC challenge.", "labels": [], "entities": [{"text": "IJCAI-19 SemDeep-5 WiC challenge", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8713404685258865}]}, {"text": "The system measures word usage similarity in sentence pairs.", "labels": [], "entities": []}, {"text": "We experiment with cosine similarities of word and sentence embeddings of different types, and with features based on in-context substitute annotations automatically assigned to WiC sentence pairs.", "labels": [], "entities": []}, {"text": "The model with the highest performance on the WiC development set uses a combination of cosine similarities from different embedding types.", "labels": [], "entities": [{"text": "WiC development set", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.6738642156124115}]}, {"text": "It obtains an accuracy of 66.7 on the shared task test set and is ranked third among the participating systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996458292007446}]}], "introductionContent": [{"text": "The SemDeep-5 WiC shared task proposes to identify the intended meaning of words in context.", "labels": [], "entities": []}, {"text": "It is framed as a binary classification task that addresses whether two instances of a target word have the same meaning).", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 18, "end_pos": 44, "type": "TASK", "confidence": 0.798141876856486}]}, {"text": "The WiC dataset contains 7,466 sentence pairs and is proposed as anew evaluation benchmark for context-sensitive word representations.", "labels": [], "entities": [{"text": "WiC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7436255663633347}, {"text": "context-sensitive word representations", "start_pos": 95, "end_pos": 133, "type": "TASK", "confidence": 0.6424884796142578}]}, {"text": "We apply to this task the method from Gar\u00ed which addresses the usage similarity of contextualized instances of words.", "labels": [], "entities": []}, {"text": "The method integrates cosine similarities from different types of context-sensitive embeddings and in-context automatic substitutes.", "labels": [], "entities": []}, {"text": "Our best system combines cosine similarities from three embedding types.", "labels": [], "entities": []}, {"text": "It obtains an accuracy of 66.7 on the WiC test set, and is ranked third among all systems that participated in the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999646782875061}, {"text": "WiC test set", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8916446765263876}]}], "datasetContent": [{"text": "The WiC dataset contains 7,466 sentence pairs of target words automatically labelled as having the same (T) or different (F) meaning.", "labels": [], "entities": [{"text": "WiC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8898869454860687}]}, {"text": "It was automatically compiled by extracting usage examples and sense information from lexical resources) and Wiktionary 1 ).", "labels": [], "entities": []}, {"text": "To exclude instance pairs describing fine-grained sense distinctions, the resource was automatically pruned based on synset proximity in the WordNet network.", "labels": [], "entities": [{"text": "WordNet network", "start_pos": 141, "end_pos": 156, "type": "DATASET", "confidence": 0.9682912230491638}]}, {"text": "Human accuracy upper bound on the dataset was defined as 80%, which corresponds to the average human accuracy on a sample of sentence pairs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9746155738830566}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9593831300735474}]}, {"text": "Inter-annotator agreement was at the same level.", "labels": [], "entities": []}, {"text": "The WiC dataset provides a benchmark for evaluating context-sensitive word representations, and their capacity to capture the dynamic aspects of word meaning and usage.", "labels": [], "entities": [{"text": "WiC dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8677330017089844}]}], "tableCaptions": [{"text": " Table 2: Accuracy of the models with embedding- based and substitute features on the WiC development  set. We report results of the models trained only on  WiC, and on the extended (WiC+CnC) dataset. The  best configurations (marked in boldface) were applied  to the WiC test set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9751530885696411}, {"text": "WiC development  set", "start_pos": 86, "end_pos": 106, "type": "DATASET", "confidence": 0.7765165964762369}, {"text": "WiC test set", "start_pos": 268, "end_pos": 280, "type": "DATASET", "confidence": 0.8391862114270529}]}, {"text": " Table 3: Accuracy of our two best models on the WiC  test set, compared to the best result from previous work.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9986868500709534}, {"text": "WiC  test set", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9021495381991068}]}]}