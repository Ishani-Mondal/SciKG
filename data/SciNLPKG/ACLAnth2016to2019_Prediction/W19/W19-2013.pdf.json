{"title": [{"text": "Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set Expansion", "labels": [], "entities": [{"text": "Multi-Context Term Embeddings", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.758954256772995}]}], "abstractContent": [{"text": "In this paper, we present a novel algorithm that combines multi-context term embeddings using a neural classifier and we test this approach on the use case of corpus-based term set expansion.", "labels": [], "entities": [{"text": "corpus-based term set expansion", "start_pos": 159, "end_pos": 190, "type": "TASK", "confidence": 0.6595106050372124}]}, {"text": "In addition, we present a novel and unique dataset for intrinsic evaluation of corpus-based term set expansion algorithms.", "labels": [], "entities": [{"text": "corpus-based term set expansion", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.6176050826907158}]}, {"text": "We show that, over this dataset, our algorithm provides up to 5 mean average precision points over the best baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.7187039852142334}]}], "introductionContent": [{"text": "Term set expansion is the task of expanding a given seed set of terms into a more complete set of terms that belong to the same semantic class.", "labels": [], "entities": [{"text": "Term set expansion", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8667675654093424}]}, {"text": "For example, given a seed of personal assistant application terms like 'Siri' and 'Cortana', the expanded set is expected to include additional terms such as 'Amazon Echo' and 'Google Now'.", "labels": [], "entities": []}, {"text": "Most prior work on corpus-based term set expansion is based on distributional similarity, where early work is primarily based on using sparse vectors while recent work is based on word embeddings.", "labels": [], "entities": [{"text": "corpus-based term set expansion", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6746101975440979}]}, {"text": "The prototypical term set expansion methods utilize corpus-based semantic similarity between seed terms and candidate expansion terms.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, each of the prior methods used a single context type for embedding generation, and there are no reported comparisons of the effectiveness of embedding different context types.", "labels": [], "entities": [{"text": "embedding generation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.7548814117908478}]}, {"text": "Moreover, the lack of a publicly available dataset hinders the replicability of previous work and method comparison.", "labels": [], "entities": [{"text": "replicability", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9600647687911987}]}, {"text": "In this paper, we investigate the research question of whether embeddings of different context types can complement each other and enhance the performance of computational semantics tasks like term set expansion.", "labels": [], "entities": [{"text": "term set expansion", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.629564623037974}]}, {"text": "To address this question, we propose an approach that combines term embeddings over multiple contexts for capturing different aspects of semantic similarity.", "labels": [], "entities": []}, {"text": "The algorithm uses 5 different context types, 3 of which were previously proposed for term set expansion and additional two context types that were borrowed from the general distributional similarity literature.", "labels": [], "entities": [{"text": "term set expansion", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.6655123233795166}]}, {"text": "We show that combining the different context types yields improved results on term set expansion.", "labels": [], "entities": [{"text": "term set expansion", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.6619429985682169}]}, {"text": "In addition to the algorithm, we developed a dataset for intrinsic evaluation of corpus-based set expansion algorithms, which we propose as a basis for future comparisons.", "labels": [], "entities": []}, {"text": "Code, demonstration system, dataset and term embeddings pre-trained models are distributed as part of NLP Architect by Intel AI Lab.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the lack of suitable standard dataset for training and testing term set expansion models, we used Wikipedia to develop a standard dataset.", "labels": [], "entities": []}, {"text": "Our motivation for using Wikipedia is two-fold.", "labels": [], "entities": []}, {"text": "First, Wikipedia contains human-generated lists of terms ('List of' pages) that cover many domains; these lists can be used for supervised training (MLP training in our approach) and for evaluating set expansion algorithms.", "labels": [], "entities": [{"text": "evaluating set expansion algorithms", "start_pos": 187, "end_pos": 222, "type": "TASK", "confidence": 0.7521766871213913}]}, {"text": "Second, it contains textual data that can be used for unsupervised training of corpus-based approaches (multi-context term embedding training in our approach).", "labels": [], "entities": []}, {"text": "We thus extracted from an English Wikipedia dump a set of term lists and a textual corpus for term embedding training.", "labels": [], "entities": []}, {"text": "Following previous work (, we report the Mean Average Precision at several top n values (MAP@n) to evaluate ranked candidate lists returned by the algorithm.", "labels": [], "entities": [{"text": "Mean Average Precision", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.8875845670700073}, {"text": "MAP", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9738108515739441}]}, {"text": "When computing MAP, a candidate term is considered as matching a gold term if they both appear in the same term variations group.", "labels": [], "entities": [{"text": "MAP", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9110212326049805}]}, {"text": "We first compare the different context types; then, we report results on their combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. These context types are baselines  and we compare them to the linear context that is  more standard. Note that the dependency context  type is affected by the performance of the depen- dency parser.", "labels": [], "entities": []}, {"text": " Table 2: Comparison of the different context types. For  each context type, we report the scoring method with  higher MAP@10 on dev set, MAP@10 with 5 seed  terms, its standard deviation among the different test  term lists, the percentage of the test term lists where  the context type achieves best performance.", "labels": [], "entities": [{"text": "MAP", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9883472323417664}]}, {"text": " Table 3: MAP@n performance evaluation of the linear  context, concatenation, MLP binary classification and  oracle, with 5 seed terms.", "labels": [], "entities": [{"text": "MLP binary classification", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.5781344473361969}]}]}