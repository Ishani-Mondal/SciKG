{"title": [{"text": "SWOW-8500: Word Association Task for Intrinsic Evaluation of Word Embeddings", "labels": [], "entities": [{"text": "SWOW-8500", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8057117462158203}]}], "abstractContent": [{"text": "Downstream evaluation of pretrained word embeddings is expensive, more so for tasks where current state of the art models are very large architectures.", "labels": [], "entities": []}, {"text": "Intrinsic evaluation using word similarity or analogy datasets, on the other hand, suffers from several disadvantages.", "labels": [], "entities": []}, {"text": "We propose a novel intrinsic evaluation task employing large word association datasets (particularly the Small World of Words dataset).", "labels": [], "entities": [{"text": "Small World of Words dataset", "start_pos": 105, "end_pos": 133, "type": "DATASET", "confidence": 0.7123659968376159}]}, {"text": "We observe correlations not just between performances on SWOW-8500 and previously proposed intrinsic tasks of word similarity prediction, but also with downstream tasks (eg. Text Classification and Natural Language Inference).", "labels": [], "entities": [{"text": "SWOW-8500", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.8755142688751221}, {"text": "word similarity prediction", "start_pos": 110, "end_pos": 136, "type": "TASK", "confidence": 0.7983758846918741}, {"text": "Text Classification", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.6943153738975525}]}, {"text": "Most importantly , we report better confidence intervals for scores on our word association task, with no fall in correlation with downstream performance .", "labels": [], "entities": [{"text": "word association task", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.8242741227149963}]}], "introductionContent": [{"text": "With the recent rise in popularity of distributional semantics, word embeddings have become the basic building block of several state-of-the-art models spanning multiple problems across Natural Language Processing and Information Retrieval.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 218, "end_pos": 239, "type": "TASK", "confidence": 0.7451144158840179}]}, {"text": "Word embeddings are essentially non-sparse representations of words in the form of one (relatively) small dimensional vector of real numbers for every word, and all of these vectors lie in the same continuous space.", "labels": [], "entities": []}, {"text": "Despite the clear benefits of these distributed representations, it is not obvious how to come up with apt word embeddings fora given NLP task.", "labels": [], "entities": []}, {"text": "Approaches such as word2vec (), GloVe (), etc.", "labels": [], "entities": []}, {"text": "have been shown to perform well on downstream tasks such as text classification, sequence labelling, question answering, text summarization, and machine translation.", "labels": [], "entities": [{"text": "text classification", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7938851118087769}, {"text": "sequence labelling", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6651616990566254}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8789839744567871}, {"text": "text summarization", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.7828355133533478}, {"text": "machine translation", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.8006466031074524}]}, {"text": "Typically, word vectors are used in NLP models in two ways: fixed pretrained embeddings, and finetuning.", "labels": [], "entities": []}, {"text": "In the first way, word vectors have already been trained on some large dataset (e.g. Wikipedia, Twitter, Blog corpus, etc.) using one of the aforementioned techniques.", "labels": [], "entities": [{"text": "Blog corpus", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.8002035319805145}]}, {"text": "These vectors are taken as fixed weights and the model merely uses them as they are rather than learning them during the training phase.", "labels": [], "entities": []}, {"text": "On the other hand, finetuning allows for these vectors to be modified too, using backpropagation.", "labels": [], "entities": []}, {"text": "Here the word embeddings are taken only as initialized weights for the model's first layer.", "labels": [], "entities": []}, {"text": "It is of natural interest to the NLP community to identify evaluation metrics for word embeddings.", "labels": [], "entities": []}, {"text": "Besides direct performance measurement on downstream tasks, there have also been proposed several intrinsic evaluation measures such as MEN, WordSim, SimLex, etc.", "labels": [], "entities": [{"text": "WordSim", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.929201602935791}]}, {"text": "These are small proxy tasks which word vectors are expected to perform well on, given the assumption that they capture semantics of words.", "labels": [], "entities": []}, {"text": "While Extrinsic evaluations use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task, Intrinsic evaluations directly test for syntactic or semantic relationships between words (.", "labels": [], "entities": []}, {"text": "For example, the word similarity task asks word embeddings to predict how similar are the meanings of two prompt words.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.7851710518201193}]}, {"text": "The closer this estimate is to human judgements, higher is the score allotted to the (pretrained) word embedding.", "labels": [], "entities": []}, {"text": "Through this paper, we propose the Word Association task for evaluating non-contextualized pretrained word embeddings, with the help of word asssociation datasets originally collected for psychological research.", "labels": [], "entities": []}, {"text": "The datasets were formed by asking participants to respond to certain cue words.", "labels": [], "entities": []}, {"text": "For example, given the cue tiger, one could respond with the words lion, panther, wild, etc.", "labels": [], "entities": []}, {"text": "Large datasets of this sort are now available online, and it can be argued that they capture a notion of which words are in close association with others (as perceived by human participants).", "labels": [], "entities": []}, {"text": "According to cognitive theories of the mind, people form associations between concepts based on similarity, contiguity, or contrast.", "labels": [], "entities": []}, {"text": "Our task proposal stems from the following argument: Any model that claims to understand the semantics of words should be able to mimic human beings in recognizing the associations between pairs of words.", "labels": [], "entities": []}, {"text": "For example, a distributed representation of words, i.e., word embeddings, should be able to tell that the word tiger is in someway associated with lion but not with, say, kettle, assuming such a statistic is observed in the word association dataset too.", "labels": [], "entities": []}, {"text": "Given the scale of these datasets, they seem like a lucrative way to evaluate pretrained word embeddings.", "labels": [], "entities": []}, {"text": "We see them as a manually annotated corpus of word associations, though not originally meant for word embedding evaluation.", "labels": [], "entities": [{"text": "word embedding evaluation", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.6495703260103861}]}, {"text": "Therefore, we must devise a convenient way to compare the semantics captured in a given set of pretrained word vectors with that captured in such word association datasets.", "labels": [], "entities": []}, {"text": "We make our scripts, along with several other resources, available at https://github.", "labels": [], "entities": []}, {"text": "com/avi-jit/SWOW-eval 2 Related Work", "labels": [], "entities": []}], "datasetContent": [{"text": "There exist several intrinsic evaluation tasks for word embeddings.", "labels": [], "entities": []}, {"text": "One way to tell apart intrinsic from extrinsic evaluations is the lack of any trainable parameters in the former.", "labels": [], "entities": []}, {"text": "discuss word relatedness, analogy, selective preference, and categorization as types of intrinsic tasks.", "labels": [], "entities": []}, {"text": "Our proposed task is most similar to the word relatedness/similarity tasks, several of which have already been proposed in literature: WS-3533 (), WS-SIM and WS-REL (), RG-65,), Rare Words (, etc.", "labels": [], "entities": [{"text": "word relatedness/similarity tasks", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.8233748495578765}, {"text": "WS-3533", "start_pos": 135, "end_pos": 142, "type": "DATASET", "confidence": 0.9027271866798401}]}, {"text": "We list the ones above specifically since those are the ones we compare our proposed task to, using the online resource wordvectors.org, whose code remains available on GitHub . Association of Computational Linguistics 2 and Vecto AI 3 also maintain benchmark pages for word similarity.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 270, "end_pos": 285, "type": "TASK", "confidence": 0.7072706669569016}]}, {"text": "Likewise, VecEval (Nayak et al., 2016) and Multilingual-embeddings-eval-portal ( are GitHub repositories for Extrinsic Evaluation of word embeddings.", "labels": [], "entities": [{"text": "VecEval", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.8468493223190308}]}, {"text": "Another direction of work has been towards critiquing intrinsic evaluation, in a bid to understand its shortcomings and potential workarounds (.", "labels": [], "entities": []}, {"text": "One of the key shortcomings is the Absence of Statistical Significance, which we aim to tackle through this proposal.", "labels": [], "entities": [{"text": "Absence", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9948447942733765}, {"text": "Statistical Significance", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6117149740457535}]}, {"text": "We believe that a massive dataset of word associations can be used to circumvent issues related to confidence intervals of scores reported.", "labels": [], "entities": []}, {"text": "We put this belief to test in later sections of this paper.", "labels": [], "entities": []}, {"text": "Here onwards, we restrict ourselves to only the Small World of Words dataset (SWOW), apart of which can be seen in.", "labels": [], "entities": [{"text": "Small World of Words dataset (SWOW)", "start_pos": 48, "end_pos": 83, "type": "DATASET", "confidence": 0.7863085195422173}]}, {"text": "For each cueresponse pair C-R, the value R123 is the number of participants who responded with R when given the cue C.", "labels": [], "entities": [{"text": "R123", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9630834460258484}]}, {"text": "Note that out of at most three responses collected per cue per respondent, it does not matter to the R123 score whether R occurred in the first response or the third.", "labels": [], "entities": [{"text": "R123", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.8785567879676819}, {"text": "R", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.789711594581604}]}, {"text": "N is the number of total responses given the cue C in the processed version of released SWOW dataset.", "labels": [], "entities": [{"text": "SWOW dataset", "start_pos": 88, "end_pos": 100, "type": "DATASET", "confidence": 0.8613492548465729}]}, {"text": "The value R123.Strength is simply equal to R123 N . There are 978, 908 cue-response pairs in the latest release of SWOW dataset.", "labels": [], "entities": [{"text": "R123.Strength", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9015054106712341}, {"text": "R123 N", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9020309150218964}, {"text": "SWOW dataset", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.9016033709049225}]}, {"text": "The statistics for the number of responses per cue is shown in.", "labels": [], "entities": []}, {"text": "For our own SWOW evaluations, we got rid of anything that was not a single word, e.g. New York or get-together.", "labels": [], "entities": [{"text": "SWOW evaluations", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7738467454910278}]}, {"text": "We further selected only the most frequently co-occurring word associations.", "labels": [], "entities": []}, {"text": "In particular, we kept only those cue-response pairs that have R123.strength (i.e., number of people who cited this response for this cue within any of the three responses they gave, divided by the total number of responses for this cue word) is greater than 0.2 which corresponds to saying that at least one fifth of all respondents believe this response is one of the three top associated words for the given cue.", "labels": [], "entities": [{"text": "R123.strength", "start_pos": 63, "end_pos": 76, "type": "METRIC", "confidence": 0.9759564399719238}]}, {"text": "We were now left with 8500 cues and a few of their corresponding top responses each.", "labels": [], "entities": []}, {"text": "While we restrict ourselves to experimenting only on the SWOW-8500 dataset, we make available the code and resources to create even larger datasets (with fewer restrictions on, say, minimum strength of association, needed).", "labels": [], "entities": [{"text": "SWOW-8500 dataset", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9372081458568573}]}, {"text": "Note that word association datasets are asymmetric in that they treat the pairs C-R and R-C separately, i.e., for the cue coffee, the response tea might be the most frequent one but for the cue tea, the most frequent response could be black.", "labels": [], "entities": []}, {"text": "We need to bear this in mind when using this dataset to evaluate word embeddings intrinsically, since usually intrinsic datasets give out a single value fora word pair.", "labels": [], "entities": []}, {"text": "This also does not fit well with the traditional measure of similarity/relatedness between two words, i.e., cosine distance, which is asymmetric metric.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A few example cue-response tuples from  the SWOW dataset, along with their associated  R123.Strength scores", "labels": [], "entities": [{"text": "SWOW dataset", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.9522122740745544}, {"text": "R123.Strength", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9049679040908813}]}, {"text": " Table  2. For our own SWOW evaluations, we got rid of  anything that was not a single word, e.g. New York  or get-together. We further selected only the most", "labels": [], "entities": []}, {"text": " Table 4: Intrinsic tasks performance. CN: ConceptNet Numberbatch; FT: FastText; Count: Baroni and Lenci.  Pairs: Number of word pairs in the dataset; OOV: Number of word pairs of which at least one word was missing  (for upper half of table) or Number of cues missing (for lower half of table) in the common vocabulary shared by  the six pre-trained embeddings. All Confidence Intervals (CI) reported at 99% confidence level.", "labels": [], "entities": [{"text": "FT", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9814519286155701}, {"text": "Count", "start_pos": 81, "end_pos": 86, "type": "METRIC", "confidence": 0.9845321774482727}, {"text": "OOV", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9947792291641235}]}, {"text": " Table 5: Downstream tasks performance. CN: ConceptNet Numberbatch; FT: FastText; Count: Baroni and Lenci", "labels": [], "entities": [{"text": "FT", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9838535785675049}, {"text": "Count", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.9808185696601868}]}]}