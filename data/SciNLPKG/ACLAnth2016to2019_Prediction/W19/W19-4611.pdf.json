{"title": [{"text": "En-Ar Bilingual word Embeddings without Word Alignment: Factors Effects", "labels": [], "entities": [{"text": "Bilingual word Embeddings without Word Alignment", "start_pos": 6, "end_pos": 54, "type": "TASK", "confidence": 0.5663249393304189}]}], "abstractContent": [{"text": "This paper introduces the first attempt to investigate morphological segmentation on En-Ar bilingual word embeddings using bilingual word embeddings model without word alignment (BilBOWA).", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7208542227745056}, {"text": "BilBOWA", "start_pos": 179, "end_pos": 186, "type": "METRIC", "confidence": 0.9680616855621338}]}, {"text": "We investigate the effect of sentence length and embedding size on the learning process.", "labels": [], "entities": []}, {"text": "Our experiment shows that using the D3 segmentation scheme improves the accuracy of learning bilingual word em-beddings upto 10 percentage points comparing to the ATB and D0 schemes in all different training settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9992839694023132}]}], "introductionContent": [{"text": "In the last decade, neural networks (NN) have attracted many researchers attention and showed very promising results in many natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "Many models have been introduced including: semantics and question answering (, Machine Translation (MT)), parsing () and many works in word embeddings have been reported.", "labels": [], "entities": [{"text": "question answering", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7827710509300232}, {"text": "Machine Translation (MT))", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.8447269201278687}]}, {"text": "Word embedding is one of the most important NLP tasks due to its ability to capture the semantic similarities between words.", "labels": [], "entities": []}, {"text": "The main idea behind learning word embeddings is to transform words from discrete space into a continuous vector space of features that capture their syntactic and semantic information.", "labels": [], "entities": []}, {"text": "In other words, words having similar meaning should have similar vectors.", "labels": [], "entities": []}, {"text": "This similarity can be measured using different distance methods such as cosine similarity and Euclidean distance.", "labels": [], "entities": []}, {"text": "Now a days, many word embedding models have been introduced and show a significant improvement in different NLP tasks; language modelling (), MT (), named entity recognition (, document classification and sentiment analysis etc.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7402999699115753}, {"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9560129046440125}, {"text": "named entity recognition", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.6174744466940562}, {"text": "document classification and sentiment analysis", "start_pos": 177, "end_pos": 223, "type": "TASK", "confidence": 0.7333885431289673}]}, {"text": "Word embeddings can be classified, based on the objective function that needs to be learnt, into two main categories.", "labels": [], "entities": []}, {"text": "Firstly, Monolingual word embedding, which is the process of learning similar word representations for similar word meaning in the same language.", "labels": [], "entities": [{"text": "Monolingual word embedding", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.770535409450531}]}, {"text": "Secondly, Bilingual/cross-lingual approaches, which is the process of learning similar words among languages.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the effect of different Arabic segmentation schemes, sentence length and embedding sizes on learning Arabic-English (Ar-En) Bilingual word embeddings.", "labels": [], "entities": [{"text": "learning Arabic-English (Ar-En) Bilingual word embeddings", "start_pos": 122, "end_pos": 179, "type": "TASK", "confidence": 0.5384515300393105}]}, {"text": "The experiments show a noticeable accuracy change using different training settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9992349147796631}]}, {"text": "Firstly, we give an overview of some related recent works on bilingual word embeddings in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 gives a brief introduction to the Arabic language, and it describes the details of Arabic language morphological complex and preprocessing techniques.", "labels": [], "entities": []}, {"text": "Next is the experiment section that contains a description of the model architecture, training dataset, preprocessing settings and training hyper-parameters.", "labels": [], "entities": []}, {"text": "The evaluation section presents the evaluation methods used as well as discussing the trained models' evaluation results.", "labels": [], "entities": []}, {"text": "Finally, we conclude this work outcomes in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of this set of experiments is to evaluate the effect of sentence length on the process of learning bilingual embeddings using different segmentation schemes.", "labels": [], "entities": []}, {"text": "As with word-level bilingual word embeddings (BWEs), similarly to (, the trained BWEs has been evaluated on a word translation task using Edit Distance, used by).", "labels": [], "entities": [{"text": "word-level bilingual word embeddings (BWEs)", "start_pos": 8, "end_pos": 51, "type": "TASK", "confidence": 0.6823994815349579}, {"text": "word translation task", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7854531904061636}]}, {"text": "First, we extracted the most frequent 3K words from the Ar-En dataset D0/UT No tokenization.", "labels": [], "entities": [{"text": "Ar-En dataset D0/UT No tokenization", "start_pos": 56, "end_pos": 91, "type": "DATASET", "confidence": 0.8675101143973214}]}, {"text": "D1 Separates the conjunction proclitics.", "labels": [], "entities": []}, {"text": "D2 D1 + Separates prepositional clitics and particles.", "labels": [], "entities": [{"text": "Separates prepositional clitics", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.8480134606361389}]}, {"text": "D3/S1 Separates all clitics including the definite article and the pronominal enclitics.", "labels": [], "entities": []}, {"text": "S0 Splitting off the conjunction proclitic w+.", "labels": [], "entities": []}, {"text": "S2 Same as S1 but all proclitics are put together in a single proclitics cluster.", "labels": [], "entities": []}, {"text": "ATB The Arabic Treebank is splitting the word into affixes.", "labels": [], "entities": [{"text": "ATB The Arabic Treebank", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.7620081007480621}]}, {"text": "S3 Splits off all clitics from the (CONJ+) class and all suffixes form the (+PRON)class.", "labels": [], "entities": []}, {"text": "In addition to splitting of all clitics of (PART+) class except s+ prefix.", "labels": [], "entities": []}, {"text": "S0PR S0 + splitting off all sufixes from (+PRON) class.", "labels": [], "entities": []}, {"text": "S4 S3 + splitting off the s+ clitics.", "labels": [], "entities": []}, {"text": "D0 wtAvrt Tfwlty bAlryf ldrjp qd AEjz En $rHhA kmA tmyzt bAlfkr bmA yfwq twqEAtkm . D3 wtAvrt Tfwlp +y b+ Al+ ryf l+ drjp qd AEjz En $rH +hA k+ mA tmyzt b+ Al+ fkr b+ mA yfwq twqEAt +km . ATB wtAvrt Tfwlp +y b+ Alryf l+ drjp qd AEjz En $rH +hA k+ mA tmyzt b+ Alfkr b+ mA yfwq twqEAt +km . and preprocessed them similarly to the training dataset.", "labels": [], "entities": [{"text": "AEjz", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9748179912567139}, {"text": "ATB", "start_pos": 188, "end_pos": 191, "type": "METRIC", "confidence": 0.9698734283447266}]}, {"text": "Then, we translate the extracted words using Google translator to create a dictionary.", "labels": [], "entities": []}, {"text": "After that, for Arabic as source and English as a target, we compute the distances between vectors in order to extract the embeddings of the k nearest neighbours fora given source word embedding in the target word embeddings.", "labels": [], "entities": []}, {"text": "After computing the similarity, the top k nearest neighbours (for k=1, 3, 5) have been selected to compute the accuracy among the test dataset, which consists of 3000 words and their translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9992844462394714}]}, {"text": "Then we computed the accuracy of 10 runs randomly selecting 500 source words and their k nearest neighbours as: Where ct is the number of correct translations and T is the number of all test samples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9994915723800659}]}, {"text": "The accuracy is computed for all experiments with all different settings: sentence-length, embeddings size and segmentation schemes and the results are discussed below.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995638728141785}]}, {"text": "We also took into account the observed variance in considering significance of the observed differences in performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of tokens in training Datasets with  different segmentations schemes. Note that prepro- cessing changes sentence length, and different methods  therefore produce different datasets", "labels": [], "entities": []}, {"text": " Table 4: 100D Models' Results", "labels": [], "entities": []}, {"text": " Table 5: 200D Models' Results", "labels": [], "entities": [{"text": "200D Models'", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.6531301587820053}]}, {"text": " Table 6: 300D Models' Results", "labels": [], "entities": [{"text": "300D Models'", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.612407460808754}]}]}