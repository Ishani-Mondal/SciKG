{"title": [{"text": "From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions", "labels": [], "entities": []}], "abstractContent": [{"text": "We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation.", "labels": [], "entities": []}, {"text": "In many of the attention heads, we frequently find sequences of consecutive states attending to the same position , which resemble syntactic phrases.", "labels": [], "entities": []}, {"text": "We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences.", "labels": [], "entities": []}, {"text": "We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9996623992919922}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9975858926773071}]}], "introductionContent": [{"text": "The classical approach to Natural Language Processing used to be complex pipelines, e.g. (Popel and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2010;), consisting of multiple steps of linguistically motivated analyses, such as partof-speech tagging or syntactic parsing, using explicit intermediate representations (e.g. dependency trees) to abstract over the underlying texts.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 226, "end_pos": 247, "type": "TASK", "confidence": 0.7987408638000488}, {"text": "syntactic parsing", "start_pos": 251, "end_pos": 268, "type": "TASK", "confidence": 0.6974155604839325}]}, {"text": "In recent years, this has changed with the introduction of deep neural end-to-end models, which take raw text as input and produce the desired output directly.", "labels": [], "entities": []}, {"text": "Any intermediate representations of the text may emerge during the training of the neural network, and are hidden to us.", "labels": [], "entities": []}, {"text": "We focus on the encoder part of the Transformer architecture (, applied to neural machine translation (NMT), as visualizations presented by the authors suggest that its attention heads capture various phenomena such as syntax, semantic roles or anaphora links.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.8664065897464752}]}, {"text": "In this work, we analyze the syntactic properties of the self-attention heads both qualitatively and quantitatively.", "labels": [], "entities": []}, {"text": "For the quantitative evaluation, we devise anew technique that quantifies the amount of syntactic information by explicitly building constituency trees from the attentions and comparing them with the standard syntactic trees.", "labels": [], "entities": []}, {"text": "Section 3 briefly describes the Transformer encoder architecture and the way we visualize the self-attention matrices using heatmaps.", "labels": [], "entities": []}, {"text": "In Section 4, we present our findings from an extensive manual inspection of the heatmaps, identifying several common patterns, including the balusterlike structures which seem to resemble syntactic phrases.", "labels": [], "entities": []}, {"text": "To avoid confirmation bias, we proceed by devising a linguistically uninformed tree extraction algorithm (Section 5), which builds a constituency tree based solely on the assumption that the balusters correspond to syntactic phrases.", "labels": [], "entities": [{"text": "confirmation bias", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9317737519741058}, {"text": "linguistically uninformed tree extraction", "start_pos": 53, "end_pos": 94, "type": "TASK", "confidence": 0.6966991275548935}]}, {"text": "We analyze the resulting parse trees and compare them with standard syntactic trees, both manually and via automatic evaluation.", "labels": [], "entities": []}, {"text": "In Section 6, we follow the hypothesis that only some of the attention heads are \"syntactic\", and try to identify them.", "labels": [], "entities": []}], "datasetContent": [{"text": "We analyze the Transformer NMT encoders for the following three languages: English (en), en-de 33.5 en-fr 45.2 fr-de 24.3 de-en 39.8 fr-en 42.1 de-fr 32.9: BLEU scores measured on the test data.", "labels": [], "entities": [{"text": "Transformer NMT encoders", "start_pos": 15, "end_pos": 39, "type": "DATASET", "confidence": 0.7537399331728617}, {"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.998908519744873}]}, {"text": "French (fr), and German (de).", "labels": [], "entities": []}, {"text": "We selected those particular languages because they are available in the Europarl corpus 1 () comprising large high-quality multiparallel data, and because constituency syntax parse trees can be obtained for them by the Stanford parser () out-of-the-box.", "labels": [], "entities": [{"text": "Europarl corpus 1", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9848630825678507}]}, {"text": "As we want to explore a state-of-the-art setup, we use the Transformer model () as reimplemented by in the Neural Monkey framework 3 in standard setting: 6 encoder and decoder layers, 16 attention heads, embedding size of 512, hidden-layers' size of 4096, dropout 0.9, and batch size 30.", "labels": [], "entities": []}, {"text": "We train the translator for all 6 source-target language pairs (en-fr, en-de, fr-en, fr-de, de-en, de-fr).", "labels": [], "entities": []}, {"text": "From the Europarl corpus, we take first 1,000 sentences as development data, last 1,000 sentences as evaluation data, and the remaining 486,272 sentences for training.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.9950563311576843}]}, {"text": "lists the BLEU scores of the systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979423880577087}]}, {"text": "All inspections and evaluations, both manual and automatic, have been performed on the evaluation data.", "labels": [], "entities": []}, {"text": "The data are tokenized by the Stanford Tokenizer to make the tokens consistent with the constituency trees with which we will compare our results.", "labels": [], "entities": [{"text": "Stanford Tokenizer", "start_pos": 30, "end_pos": 48, "type": "DATASET", "confidence": 0.902132511138916}]}, {"text": "We then build a shared dictionary of 100,000 BPE subword units () on the concatenated training data of all three languages, append an EOS symbol to each sentence, and train the translation model.", "labels": [], "entities": []}, {"text": "To evaluate the syntacticity of the Transformer self-attentive encoder, we extract the constituency trees using our tree extraction algorithm for the 1,000 sentences of our evaluation set; we will refer to these as extracted trees.", "labels": [], "entities": []}, {"text": "We then induce syntactic trees for these sentences with the Stanford Parser.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9196829795837402}]}, {"text": "We use the factored lexicalized parsing models distributed together with the parser, which had been trained on standard constituency treebanks of the languagesEnglish Penn Treebank (, German Negra Corpus (), and French Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.9704227447509766}, {"text": "German Negra Corpus", "start_pos": 184, "end_pos": 203, "type": "DATASET", "confidence": 0.6909518539905548}, {"text": "French Treebank", "start_pos": 212, "end_pos": 227, "type": "DATASET", "confidence": 0.9761383831501007}]}, {"text": "We post-process the trees in the following way: 1.", "labels": [], "entities": []}, {"text": "remove phrase labels We compare the extracted trees with the parse trees, assuming that the more similar they are, the more syntactic the Transformer encoder is.", "labels": [], "entities": []}, {"text": "We calculate the precision of the extracted tree as the proportion of its phrases that are \"correct\" in the sense that they are consistent with the parse tree, not crossing any of its phrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9991961121559143}]}, {"text": "(For the sake of this analysis, we only consider one possible way of capturing syntax, as defined in the respective treebanks; we discuss that in Section 5.3.)", "labels": [], "entities": []}, {"text": "Let P be the parse tree, an extracted phrase e is correct if and only if: Recall is computed inversely, as the proportion of phrases in the parse tree that are consistent with the extracted tree.", "labels": [], "entities": [{"text": "Recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.999234676361084}]}, {"text": "We compute the total precision and recall as an average overall extracted phrases in all the trees, and also report their harmonic mean (F1).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9895042181015015}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9994906187057495}, {"text": "F1", "start_pos": 137, "end_pos": 139, "type": "METRIC", "confidence": 0.7546883821487427}]}, {"text": "The results of the evaluations for all three source languages are shown in.", "labels": [], "entities": []}, {"text": "To put them into perspective, we also report scores for several uninformed parsing baselines: 1.", "labels": [], "entities": []}, {"text": "rbal: balanced binary tree aligned right 2.", "labels": [], "entities": []}, {"text": "lbal: balanced binary tree aligned left 3.", "labels": [], "entities": []}, {"text": "rand.init: our proposed algorithm using randomly initialized Transformer weights Examples of the lbal and rbal baselines are shown in.: Scores of baseline trees and our extracted trees using all attention heads, evaluated against standard syntactic parse trees.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Scores of baseline trees and our extracted trees  using all attention heads, evaluated against standard  syntactic parse trees.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of syntactic heads subselection.  Score gains over the base tree extraction as reported  in Table 2, in percentage points.", "labels": [], "entities": [{"text": "Score", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9808206558227539}]}, {"text": " Table 4: Average proportion of attention head layers in  the best subselection setups for all language pairs. L  is the number of the layer, P is the proportion of the  selected heads that come from the given layer.", "labels": [], "entities": []}]}