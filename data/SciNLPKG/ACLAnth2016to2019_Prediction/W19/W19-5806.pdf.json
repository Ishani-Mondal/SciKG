{"title": [{"text": "Extending Neural Question Answering with Linguistic Input Features", "labels": [], "entities": [{"text": "Extending Neural Question Answering", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7595274895429611}]}], "abstractContent": [{"text": "Considerable progress in neural question answering has been made on competitive general domain datasets.", "labels": [], "entities": [{"text": "question answering", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.704496368765831}]}, {"text": "In order to explore methods to aid the generalization potential of question answering models, we reimplement a state-of-the-art architecture, perform a parameter search on an open-domain dataset and evaluate a first approach for integrating linguistic input features such as part-of-speech tags, syntactic dependency relations and semantic roles.", "labels": [], "entities": [{"text": "question answering", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7746796607971191}]}, {"text": "The results show that adding these input features has a greater impact on performance than any of the architectural parameters we explore.", "labels": [], "entities": []}, {"text": "Our findings suggest that these layers of linguistic knowledge have the potential to substantially increase the generalization capacities of neural QA models, thus facilitating cross-domain model transfer or the development of domain-agnostic QA models.", "labels": [], "entities": [{"text": "cross-domain model transfer", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.6531697710355123}]}], "introductionContent": [{"text": "Recently, deep neural network approaches for question answering (QA) have gained traction.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.9212191581726075}]}, {"text": "The strong interest in this task maybe explained by two promises that resonate in neural QA approaches: For one thing, QA is claimed to bear the potential to subsume a lot of other NLP challenges.", "labels": [], "entities": []}, {"text": "From this perspective, almost every task can be framed as a natural language question ().", "labels": [], "entities": []}, {"text": "Thus, a QA model with the capacity to learn mappings from natural language terminology to formal linguistic concepts could be used as a surrogate model, reducing annotation and training effort and providing fast solutions to potentially complex NLP problems.", "labels": [], "entities": []}, {"text": "For another, QA systems have always been considered as intuitive natural language interfaces for information access in various domains of (technical) knowledge.", "labels": [], "entities": []}, {"text": "As any other practical NLP solution targeting specialized domains, QA systems face the inherent challenges of cross-domain generalization or domain adaptation, respectively.", "labels": [], "entities": [{"text": "cross-domain generalization", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.677537202835083}, {"text": "domain adaptation", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.7165616154670715}]}, {"text": "However, QA approaches can be considered particularly suitable for this kind of problem, as the semantic underpinnings of question/answer pairs capture a universal layer of meaning that is domain-agnostic to some extent (but might require fine-tuning wrt. particular domain concepts or terminology).", "labels": [], "entities": []}, {"text": "We hypothesize that a promising approach towards rapid information access in specialized domains would be (i) to learn the aforementioned universal meaning layer from large collections of open-domain question/answer pairs, and (ii) adapt the resulting meaning representations to more specific domains subsequently.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the first problem.", "labels": [], "entities": []}, {"text": "Our work is based on the assumption that rich representations of linguistic knowledge at high levels of syntactic and semantic abstraction facilitates neural NLP models to capture \"universal\", domain-agnostic meaning, which in turn fosters performance in open-domain QA.", "labels": [], "entities": []}, {"text": "Against this backdrop, we evaluate the impact of explicitly encoded linguistic information in terms of part-ofspeech tags, syntactic dependencies and semantic roles on open-domain performance of a stateof-the-art neural QA model.", "labels": [], "entities": []}, {"text": "We find that our re-implementation of the deep neural QANet architecture () benefits considerably from these linguistically enriched representations, which we consider a promising first step towards generalizable, rapidly adaptable QA models.", "labels": [], "entities": []}], "datasetContent": [{"text": "To obtain a baseline, we reimplemented QANet and performed a parameter search.", "labels": [], "entities": [{"text": "QANet", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.7834669351577759}]}, {"text": "After that, we evaluated the integration of linguistic input features against that baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Impact of evaluated individual parameters and  the combination of their best settings on F1 and exact  match (EM) scores.", "labels": [], "entities": [{"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9993649125099182}, {"text": "exact  match (EM) scores", "start_pos": 106, "end_pos": 130, "type": "METRIC", "confidence": 0.943971167008082}]}, {"text": " Table 2: Results of enriching the inputs with linguistic  input embeddings of different sizes, in terms of F1 and  exact match (EM) scores. DL refers to dependency la- bels, SRL to sematic role labels. No linguistic features  were used in the baseline.", "labels": [], "entities": [{"text": "F1", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.999028205871582}, {"text": "exact match (EM) scores", "start_pos": 116, "end_pos": 139, "type": "METRIC", "confidence": 0.9610379238923391}]}]}