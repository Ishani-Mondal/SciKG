{"title": [{"text": "Is artificial data useful for biomedical Natural Language Processing algorithms?", "labels": [], "entities": [{"text": "biomedical Natural Language Processing algorithms", "start_pos": 30, "end_pos": 79, "type": "TASK", "confidence": 0.6753725647926331}]}], "abstractContent": [{"text": "A major obstacle to the development of Natural Language Processing (NLP) methods in the biomedical domain is data accessibility.", "labels": [], "entities": []}, {"text": "This problem can be addressed by generating medical data artificially.", "labels": [], "entities": []}, {"text": "Most previous studies have focused on the generation of short clinical text, and evaluation of the data utility has been limited.", "labels": [], "entities": []}, {"text": "We propose a generic methodology to guide the generation of clinical text with key phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data availability is a major obstacle in the development of more powerful Natural Language Processing (NLP) methods in the biomedical domain.", "labels": [], "entities": []}, {"text": "In particular, current state-of-the-art (SOTA) neural techniques used for NLP rely on substantial amounts of training data.", "labels": [], "entities": [{"text": "NLP", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9607512950897217}]}, {"text": "In the NLP community, this low-resource problem is typically addressed by generating complementary data artificially (.", "labels": [], "entities": []}, {"text": "This approach is also gaining attention in biomedical NLP.", "labels": [], "entities": [{"text": "biomedical NLP", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.6646565943956375}]}, {"text": "Most of these studies present work on the generation of short text (typically under 20 tokens), given structural information to guide this generation (e.g., chief complaints using basic patient and diagnosis information).", "labels": [], "entities": []}, {"text": "Evaluation scenarios for the utility of the artificial text usually involve a single downstream NLP task (typically, text classification).", "labels": [], "entities": [{"text": "text classification", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7222563177347183}]}, {"text": "SOTA approaches tackle other language generation tasks by applying neural models: variations of the encoder-decoder architecture (ED) model), a.k.a sequence to sequence (seq2seq), e.g., the Transformer model (.", "labels": [], "entities": [{"text": "language generation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7467156052589417}]}, {"text": "In this work, we follow these approaches and guide the generation process with key phrases in the Transformer model.", "labels": [], "entities": []}, {"text": "Our main contribution is thus twofold: (1) a single methodology to generate medical text fora series of downstream NLP tasks; (2) an assessment of the utility of the generated data as complementary training data in two important biomedical NLP tasks: text classification (phenotype classification) and temporal relation evaluation.", "labels": [], "entities": [{"text": "text classification (phenotype classification", "start_pos": 251, "end_pos": 296, "type": "TASK", "confidence": 0.8133066773414612}, {"text": "temporal relation evaluation", "start_pos": 302, "end_pos": 330, "type": "TASK", "confidence": 0.6777995824813843}]}, {"text": "Additionally, we thoroughly study the usefulness of the generated data in a set of scenarios where it fully replaces real training data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In what follows, we describe the data used in experiments (Subsection 4.1), details of generation models (Subsection 4.2) and classification models (Subsection 4.3) we use.", "labels": [], "entities": []}, {"text": "In this section we present results of our experiments, first of the intrinsic evaluation of the quality of generated text (Section 5.1) and then of the extrinsic evaluation of its utility for NLP (text classification and temporal relation extraction tasks, Section 5.2).", "labels": [], "entities": [{"text": "text classification and temporal relation extraction tasks", "start_pos": 197, "end_pos": 255, "type": "TASK", "confidence": 0.7154058260577065}]}, {"text": "As expected, automatic evaluation scores show that for both test sets our model generates context preserving pieces of the real text from the input (e.g., ROUGE-L = 67.74 for test-gen-pheno, ROUGE-L = 48.47 for test-gen-temp).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 155, "end_pos": 162, "type": "METRIC", "confidence": 0.997157096862793}, {"text": "ROUGE-L", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9912204146385193}]}, {"text": "The proximity of average lengths of sentences for the generated text and the real text supports this statement.", "labels": [], "entities": []}, {"text": "As automatic metrics perform only a shallow comparison, we also manually reviewed a sample of texts.", "labels": [], "entities": []}, {"text": "In general, most of the generated text preserves the main meaning of the original text adding or dropping some details.", "labels": [], "entities": []}, {"text": "Incomprehensible generated sentences are rare.", "labels": [], "entities": []}, {"text": "shows examples of the generated text for both datasets.", "labels": [], "entities": []}, {"text": "In examples 1 and 3, Transformer generates text with a meaning very close to the original one (e.g., no evidence of \u2248 did not reveal, for test-gen-pheno).", "labels": [], "entities": []}, {"text": "Examples 2 and 4 are \"bad\" modifications.", "labels": [], "entities": []}, {"text": "In general, such examples are infrequent.", "labels": [], "entities": []}, {"text": "For instance in Example 2, the real phrase unable to walk without losing blood is incorrectly modified into a walk of losing blood.", "labels": [], "entities": []}, {"text": "However, the main sense of losing blood is preserved.", "labels": [], "entities": []}, {"text": "Overall, our observations indicate that the generation methodology successfully adapts to changes in generation conditions.", "labels": [], "entities": []}, {"text": "shows results of our text classification experiments.", "labels": [], "entities": [{"text": "text classification", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8141672909259796}]}, {"text": "They indicate that the artificial training data used as complimentary to the real training data is in general beneficial for the CNN model (e.g., av. F-score=0.50 for real + gen > 0.47 for real).", "labels": [], "entities": [{"text": "F-score", "start_pos": 150, "end_pos": 157, "type": "METRIC", "confidence": 0.7320347428321838}]}, {"text": "real +gen setup also outperforms the model trained using larger volume data, where the training data was repeated two times (2 \u00d7 real).", "labels": [], "entities": []}, {"text": "Overall, real +gen outperforms real for 9 phenotypes out of 13 with an average \u2206F-score=0.06, while 2 \u00d7 real for 6 phenotypes with an average \u2206F-score=0.04 only.", "labels": [], "entities": [{"text": "F-score", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.9920146465301514}, {"text": "F-score", "start_pos": 143, "end_pos": 150, "type": "METRIC", "confidence": 0.9785271286964417}]}, {"text": "To get further insights into the actual informativeness of the generated data, we study the performance of both CNN and NB in a series of setups where the artificial training data fully replace the real training data.", "labels": [], "entities": [{"text": "NB", "start_pos": 120, "end_pos": 122, "type": "DATASET", "confidence": 0.8368926048278809}]}, {"text": "To be more precise, we study: (a) gen setup, where the full generated data with traces of input key phrases are used as the training data; and (b) gen-key setup, where the generated text without traces of input data is used as the training data (see.", "labels": [], "entities": []}, {"text": "The results of these experiments are in, lower part.", "labels": [], "entities": []}, {"text": "They show that average performances of gen and real tend to be comparable for each algorithm (e.g., \u2206 avg. F-score=0.03 for both CNN and NB).", "labels": [], "entities": [{"text": "\u2206 avg. F-score", "start_pos": 100, "end_pos": 114, "type": "METRIC", "confidence": 0.7010769446690878}, {"text": "CNN", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9242996573448181}, {"text": "NB", "start_pos": 137, "end_pos": 139, "type": "DATASET", "confidence": 0.6686625480651855}]}, {"text": "However, the gen-key text still potentially bears some relevant information that allows both CNN and NB have comparable performance for this setup.", "labels": [], "entities": [{"text": "CNN", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8905882835388184}, {"text": "NB", "start_pos": 101, "end_pos": 103, "type": "DATASET", "confidence": 0.9152538180351257}]}, {"text": "Taking advantage of the easy interpretability of the NB model, we analyse the words that contribute the most to classification decisions (highest likelihoods given the positive class) for the Adv.", "labels": [], "entities": []}, {"text": "Lung Disease as a an example of a phenotype with an average frequency for the dataset.", "labels": [], "entities": []}, {"text": "Table 6 displays those words in order of importance for real, gen and gen-key.", "labels": [], "entities": []}, {"text": "As expected, for real and gen with higher F-score values, there are more relevant medical terms: e.g., pulmonary and chest.", "labels": [], "entities": [{"text": "F-score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9982151985168457}]}, {"text": "For gen-key, there are words more distantly related to the phenotype: e.g., ct and breath.", "labels": [], "entities": []}, {"text": "For the i2b2 dataset, we focus only on the evaluation for the OVERLAP temporal relation between events as the most well-represented group.", "labels": [], "entities": [{"text": "i2b2 dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.700414389371872}, {"text": "OVERLAP", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.7597318291664124}]}, {"text": "Inspired by the SOTA solutions for the temporal relations extraction task (, we provide only the text spans that link the two events as inputs to our models.", "labels": [], "entities": [{"text": "temporal relations extraction task", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.7008007690310478}]}, {"text": "This setup is particularly beneficial to assess the utility of the generated text (see).", "labels": [], "entities": []}, {"text": "As mentioned earlier, for this dataset we guide the text generation with event text spans.", "labels": [], "entities": [{"text": "text generation", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7459178864955902}]}, {"text": "Thus, for this setup, we take only the text between those real text spans essentially copied from the input.", "labels": [], "entities": []}, {"text": "This allows us to better assess the utility only of what was generated.", "labels": [], "entities": []}, {"text": "8 reports results for our experiments with the i2b2 dataset.", "labels": [], "entities": [{"text": "i2b2 dataset", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7137882262468338}]}, {"text": "They are similar to the ones performed for the phenotyping dataset.", "labels": [], "entities": []}, {"text": "Note that we reduce the initial training set provided by the task due to particularities of our generation procedure.", "labels": [], "entities": []}, {"text": "In our data augmentation experiments we add this reduced generated data to all the provided real training data (real all).", "labels": [], "entities": []}, {"text": "The results show that real all + gen (F-score=0.62) outperforms the real setup (Fscore=0.59), as well as the upsampled setup (2 \u00d7 real all, F-score=0.58).", "labels": [], "entities": [{"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9894381761550903}, {"text": "Fscore", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9958124756813049}, {"text": "F-score", "start_pos": 140, "end_pos": 147, "type": "METRIC", "confidence": 0.9580204486846924}]}, {"text": "This confirms the utility of our data augmentation procedure for the BiLSTM model.", "labels": [], "entities": []}, {"text": "Results for gen and real reduced are again comparable for BiL-  STM.", "labels": [], "entities": []}, {"text": "For NB, we even observe an improvement of \u2206F-score=0.08 for gen as compared to real reduced for NB.", "labels": [], "entities": [{"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9978546500205994}]}, {"text": "This maybe explained by a stronger semantic signal in the generated data.", "labels": [], "entities": []}, {"text": "Overall, our results demonstrate the potential of developing a model that would generate artificial medical data fora series of NLP tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics over train-gen, and val-gen.  # denotes number.", "labels": [], "entities": [{"text": "val-gen", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9584435820579529}, {"text": "number", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9299650192260742}]}, {"text": " Table 2: Statistics over test-gen-temp. The all  dataset corresponds to the one provided by the organ- isers. The reduced dataset is the one for which the  annotations are preserved by the generation model. #  denotes number.", "labels": [], "entities": []}, {"text": " Table 3: Examples of real and generated text. The underlined text highlights \"good\" (examples 1 and 3) or \"bad\"  (examples 2 and 4) modifications. All sentences have been paraphrased.", "labels": [], "entities": []}, {"text": " Table 5: Phenotyping results for CNN and Naive Bayes (NB), test-pheno. Best performing models for CNN  data augmentation experiments are highlighted in bold. We report results for the models trained with: real data  augmented with generated gen data, real data only, 2 \u00d7 real data upsampled twice, gen data only, gen-key  data without traces of the input real data.", "labels": [], "entities": [{"text": "CNN  data augmentation", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.6162460148334503}]}, {"text": " Table 7: Temporal relations extraction for OVERLAP  for CNN and Naive Bayes (NB), test-temp. Only  the real/generated text between events serves as input.  Best performing models for data augmentation experi- ments are highlighted in bold. We report results for the  models trained using: real all training data from  the i2b2 task augmented with the generated gen data,  real all data only, 2 \u00d7 real all data upsampled  twice, real reduced data only, gen data only.", "labels": [], "entities": [{"text": "Temporal relations extraction", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8540889819463094}]}]}