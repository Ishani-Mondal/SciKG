{"title": [{"text": "Improving Neural Machine Translation Using Noisy Parallel Data through Distillation", "labels": [], "entities": [{"text": "Improving Neural Machine Translation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8894091993570328}]}], "abstractContent": [{"text": "Due to the scarcity of parallel training data for many language pairs, quasi-parallel or comparable training data provides an important alternative resource for training machine translation systems for such language pairs.", "labels": [], "entities": [{"text": "training machine translation", "start_pos": 161, "end_pos": 189, "type": "TASK", "confidence": 0.6498591105143229}]}, {"text": "Since comparable corpora are not of as high quality as manually annotated parallel data, using them for training can have a negative effect on the translation performance of an NMT model.", "labels": [], "entities": []}, {"text": "We propose distillation as a remedy to effectively leverage comparable data where the training of a student model on combined clean and comparable data is guided by a teacher model trained on the high-quality, clean data only.", "labels": [], "entities": []}, {"text": "Our experiments for Arabic-English, Chinese-English, and German-English translation demonstrate that distillation yields significant improvements compared to off-the-shelf use of comparable data and performs comparable to state-of-the-art methods for noise filtering.", "labels": [], "entities": [{"text": "noise filtering", "start_pos": 251, "end_pos": 266, "type": "TASK", "confidence": 0.7600793242454529}]}], "introductionContent": [{"text": "Traditional machine translation systems are trained on parallel corpora consisting of sentences in the source language aligned to their translations in the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7155425250530243}]}, {"text": "However, for many language pairs substantial amounts of high-quality parallel corpora are not available.", "labels": [], "entities": []}, {"text": "On the other hand, for many languages, another useful resource known as comparable corpora can be obtained relatively easily in substantially larger amounts.", "labels": [], "entities": []}, {"text": "Such comparable corpora can be created by crawling large monolingual data in the source and target languages from multilingual news portals such as Agence FrancePresse (AFP), BBC news, Euronews etc.", "labels": [], "entities": [{"text": "Euronews", "start_pos": 185, "end_pos": 193, "type": "DATASET", "confidence": 0.6211445331573486}]}, {"text": "Source and target sentences in these monolingual corpora are then aligned by automatic document and sentence alignment techniques ().", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7051940262317657}]}, {"text": "Such a bitext extracted from comparable data is usually not of the same quality as annotated parallel corpora.", "labels": [], "entities": []}, {"text": "Recent research has shown that building models from low-quality data can have a degrading effect on the performance of recurrent NMT models (.", "labels": [], "entities": []}, {"text": "Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts.", "labels": [], "entities": []}, {"text": "Recently, the \"Parallel corpus filtering\" ( shared task was held at WMT-2018.", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.7554738322893778}, {"text": "WMT-2018", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.9710733294487}]}, {"text": "This task aims at extracting high-quality sentence pairs from Paracrawl 1 , which is a large noisy parallel corpus.", "labels": [], "entities": []}, {"text": "Most of the participants in this task, used rule-based pre-filtering followed by a classifier-based scoring of sentence pairs (Barbu and Barbu.", "labels": [], "entities": []}, {"text": "A subset sampled with a fixed number of target tokens is then used to train recurrent NMT systems in order to evaluate the relative quality of the filtered bitexts.", "labels": [], "entities": []}, {"text": "Some of the submissions show good translation performance for the German-English translation task by training on the filtered bitext only.", "labels": [], "entities": [{"text": "German-English translation task", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.7358899315198263}]}, {"text": "In this paper, we propose a strategy to leverage additional low-quality bitexts without any filtering when used in conjunction with a high-quality parallel corpus.", "labels": [], "entities": []}, {"text": "Motivated by the \"knowledge distillation\" frame-Arabic-English (ISI bitext) Src: Trg: The Dutch justice ministry decided to expel the Iraqi Kurd despite Amman's demand that he be handed over to Jordanian authorities.", "labels": [], "entities": []}, {"text": "Human: The Dutch Justice Ministry decided to deport him, despite Jordan's request to hand him over as part of a drug smuggling case.", "labels": [], "entities": []}, {"text": "Chinese-English (ISI bitext) Src: \u7f8e\u56fd\u63d0\u51fa\u7684\u62a5\u590d\u6e05\u5355\u662f\u4e2d\u56fd\u653f\u5e9c\u7edd\u5bf9\u4e0d\u80fd\u63a5\u53d7\u7684\u3002", "labels": [], "entities": [{"text": "ISI bitext) Src", "start_pos": 17, "end_pos": 32, "type": "DATASET", "confidence": 0.7854879945516586}]}, {"text": "Trg: And the Chinese side would certainly not accept the unreasonable demands put forward by the Americans concerning the protection of intellectual property rights.", "labels": [], "entities": []}, {"text": "Human: The revenge list proposed by America will definitely not be accepted by Chinese government.", "labels": [], "entities": []}, {"text": "German-English (Paracrawl) Src: Der Elektroden Schalter KARI EL22 dient zur F\u00fcllstandserfassung und -regelung von elektrisch leitf\u00e4higen Fl\u00fcssigkeiten . Tgt: The KARI EL22 electrode switch is designed for the control of conductive liquids . Human: The electrode switch KARI EL22 is used for level detection and control of electrically conductive liquids.", "labels": [], "entities": [{"text": "level detection", "start_pos": 291, "end_pos": 306, "type": "TASK", "confidence": 0.7485553622245789}]}, {"text": "work of, we propose \"distillation\" as a strategy to exploit comparable training data for training an NMT system.", "labels": [], "entities": []}, {"text": "In our distillation strategy, we first train a teacher model on the clean parallel data, which then guides the training of a final student model trained on the combination of clean and noisy data.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that for Arabic-English and Chinese-English translation, distillation not only helps to successfully utilize noisy comparable corpora without any performance degradation, but it also outperforms one of the best performing filtering techniques reported in.", "labels": [], "entities": []}, {"text": "In addition, we conduct similar experiments for German-English translation and observe that while simply adding noisy data to the training data pool degrades performance, our distillation approach still yields slight improvements over the baseline.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.6010874509811401}]}, {"text": "In Section 2, we discuss the relevant literature in NMT as well as in other deep learning based tasks which aim to utilize low quality training corpus.", "labels": [], "entities": [{"text": "NMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8608477711677551}]}, {"text": "In Section 3, we provide a brief discussion of the type of noise in the comparable data, the architecture of the NMT model used in our experiments, and the knowledge distillation framework proposed by.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.6822962015867233}]}, {"text": "In Section 4, we describe our strategy to use knowledge distillation for training with noisy data.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.7696926295757294}]}, {"text": "We discuss our experimental settings including datasets and parameters in Section 5 and results in Section 6.", "labels": [], "entities": []}, {"text": "reported that NMT models can suffer substantial degradation from adding noisy bitexts when compared to a baseline model trained on high-quality parallel text only.", "labels": [], "entities": []}, {"text": "The \"Parallel corpus filtering\" ( task evaluated submissions based on NMT systems trained only on the bitext filtered from Paracrawl.", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.7033268014589945}, {"text": "Paracrawl", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.8906422257423401}]}, {"text": "However, given that many language pairs have at least some small amount of high-quality parallel corpora (which is also used by many of the participants to train a classifier for scoring the noisy data), it is important to investigate whether a bitext filtered using these proposed techniques results in any additional improvements in conjunction with the original high-quality data.", "labels": [], "entities": []}, {"text": "Filtering techniques involve discarding a sentence pair with low confidence score.", "labels": [], "entities": []}, {"text": "However, a sentence pair with a low score may still have fragments in the source and target sentences which can provide useful contexts.", "labels": [], "entities": []}, {"text": "Our results show that fora recurrent NMT model, filtering the noisy bitext below a specific threshold using one of the best techniques submitted to the filtering task (known as \"Dual conditional cross entropy filtering\" (Junczys-Dowmunt, 2018)) yields only small improvements.", "labels": [], "entities": [{"text": "Dual conditional cross entropy filtering\" (Junczys-Dowmunt, 2018))", "start_pos": 178, "end_pos": 244, "type": "TASK", "confidence": 0.7243993092666973}]}], "datasetContent": [{"text": "We conduct experiments for Arabic to English, Chinese to English, and German-English NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.4888441562652588}]}, {"text": "As a commonly used representative of comparable data, we consider all AFP sources from the ISI Arabic-English bitext (LDC2007T08) with a size of 1.1M sentence pairs and Xinhua news sources for the Chinese-English bitext (LDC2007T09) with a size of 550K sentence pairs.", "labels": [], "entities": [{"text": "ISI Arabic-English bitext (LDC2007T08", "start_pos": 91, "end_pos": 128, "type": "DATASET", "confidence": 0.8170190930366517}]}, {"text": "Both corpora are created by automatically aligning (Munteanu and Marcu, 2005) sentences from monolingual corpora.", "labels": [], "entities": []}, {"text": "For Arabic-English, we compose the parallel data consisting of 325k sentence pairs from various LDC catalogues 2 For Chinese-English, a parallel text of 550k parallel sentence pairs from LDC catalogues 3 is used.", "labels": [], "entities": []}, {"text": "Note that for Arabic-English, the size of the comparable corpus is approximately 4 times that of the parallel data while for Chinese-English, the comparable corpora size is the same as that of the parallel corpus . A byte pair encoding of size 20k is trained on the parallel data for the respective languages.", "labels": [], "entities": []}, {"text": "NIST MT05 is used as dev set for both language pairs and MT08, MT09 as test set for Arabic-English and MT-06, MT-08 as test set for Chinese-English.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7973864376544952}, {"text": "MT08", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.906367301940918}, {"text": "MT09", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8348644971847534}, {"text": "MT-06", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.8489639163017273}, {"text": "MT-08", "start_pos": 110, "end_pos": 115, "type": "DATASET", "confidence": 0.8499212265014648}]}, {"text": "Translation quality is measured in terms of case-sensitive 4-gram BLEU).", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9469087719917297}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9765154123306274}]}, {"text": "Approximate randomization) is used to detect statistically significant differences.", "labels": [], "entities": [{"text": "Approximate", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.970752477645874}]}, {"text": "For German-English, we use high-quality data from the training corpus provided for WMT-17 (  Paracrawl corpus (\"very noisy\" 1 billion English tokens) similar to.", "labels": [], "entities": [{"text": "WMT-17", "start_pos": 83, "end_pos": 89, "type": "DATASET", "confidence": 0.9443649649620056}, {"text": "Paracrawl corpus", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.7659074366092682}]}, {"text": "To be able to compare with the best filtering method, we also use a bitext of 100M target tokens submitted by Junczys-Dowmunt (2018) (available from the shared task website using a score file) which is filtered using their proposed \"Dual cross entropy\" score.", "labels": [], "entities": []}, {"text": "A BPE of 32k is trained on the WMT-17 training data, newstest15 is used as dev set and newstest16 and newstest17 are used as test set.", "labels": [], "entities": [{"text": "BPE", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9984830021858215}, {"text": "WMT-17 training data", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.9594143231709799}]}, {"text": "summarizes clean and noisy training data for all language pairs.", "labels": [], "entities": []}, {"text": "We train an LSTM-based encoder-decoder model as described in using the Open-NMT-python toolkit (, with both embeddings and hidden layers of size 1000.", "labels": [], "entities": []}, {"text": "The maximum sentence length is restricted to 80 tokens.", "labels": [], "entities": []}, {"text": "Parameters are optimized using Adam with an initial learning rate of 0.001, a decay rate of 0.5 (after every 10k steps), a dropout probability of 0.2 and label smoothing of 0.1.", "labels": [], "entities": []}, {"text": "A fixed batch size of 64 is used.", "labels": [], "entities": []}, {"text": "Model weights are initialized uniformly within [-0.02, 0.02].", "labels": [], "entities": []}, {"text": "We train fora maximum of 200k steps and select the model with best BLEU score on the development set for the final evaluation and decode with abeam size of 5.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9806190133094788}]}], "tableCaptions": [{"text": " Table 2: Datasets and statistics. Pararn = Randomly sampled  subset of Paracrawl. Filt toks=100M = 100 million target token  subsample submitted by", "labels": [], "entities": [{"text": "Paracrawl", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.9326807260513306}]}, {"text": " Table 3: Performance of various training strategies for Arabic/Chinese-English. Comparable bck = Back-translated comparable  corpora. KD =Knowledge distillation. Boldfaced = Significant differences at p < 0.01.", "labels": [], "entities": [{"text": "Knowledge distillation", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.674855038523674}]}]}