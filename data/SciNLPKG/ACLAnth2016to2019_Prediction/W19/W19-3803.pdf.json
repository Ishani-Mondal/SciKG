{"title": [{"text": "Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis", "labels": [], "entities": [{"text": "Relating Word Embedding Gender Biases to Gender Gaps", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.8897279351949692}]}], "abstractContent": [{"text": "Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text.", "labels": [], "entities": []}, {"text": "These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text.", "labels": [], "entities": []}, {"text": "These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data.", "labels": [], "entities": []}, {"text": "This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health.", "labels": [], "entities": []}, {"text": "We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries.", "labels": [], "entities": []}, {"text": "We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine-learned models are the de facto method for NLP tasks.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.9122733771800995}]}, {"text": "Recently, machine-learned models that utilize word embeddings (i.e., vector-based representations of word semantics) have come under scrutiny for biases and stereotypes, e.g., in race and gender, arising primarily from biases in their training data).", "labels": [], "entities": []}, {"text": "These biases produce systematic mistakes, so recent work has developed debiasing language models to improve NLP models' accuracy and and remove stereotypes (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9941196441650391}]}, {"text": "Concurrently, other research has begun to characterize how biases in language models correspond to disparities in the cultures that produced the training text, e.g., by mapping embeddings to survey data (, casting analogies in the vector space to compute that \"man is to woman as doctor is to nurse\" (, or varying the training text over decades and mapping each decade's model bias against its statistical disparities to capture periods of societal shifts (.", "labels": [], "entities": []}, {"text": "Building on previous work, this paper presents initial work characterizing word embedding biases with statistical gender gaps (i.e., discrepancies in opportunities and status across genders).", "labels": [], "entities": [{"text": "word embedding biases", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7803507645924886}]}, {"text": "This is an important step in approximating cultural attitudes and relating them to cultural behaviors.", "labels": [], "entities": []}, {"text": "We analyze 51 U.S. states and 99 countries, by (1) training separate word embeddings for each of these cultures from Twitter and (2) correlating the biases in these word embeddings with 5 U.S.-based and 18 international gender gap statistics.", "labels": [], "entities": []}, {"text": "Our claims are as follows: (1) some cultural gender biases in language are associated with gender gaps; (2) we can characterize biases based on strength and direction of correlation with gender gaps; and (3) themed word sets, representative of values and social constructs, capture different dimensions of gender bias and gender gaps.", "labels": [], "entities": []}, {"text": "We continue with a brief overview of gender gaps (Sec. 2) and then a description of our training data (Sec. 3) and four experiments (Sec. 4).", "labels": [], "entities": []}, {"text": "We close with a discussion of the above claims and future work (Sec. 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our international and U.S.-based analyses have an identical experimental setup, varying only in the gender gap statistics and the word embeddings.", "labels": [], "entities": []}, {"text": "Our materials included word-sets based in part on survey data) and recent work on word embeddings (.", "labels": [], "entities": []}, {"text": "These word-sets included (1) female words including female pronouns and nouns, (2) male words, including male pronouns and nouns, and (3) neutral words that were grouped thematically.", "labels": [], "entities": []}, {"text": "We use the same male and female word sets for international and U.S. state analyses, and we compute per-gender vectors \u2212 \u2212\u2212\u2212 \u2192 female and \u2212\u2212\u2192 male by averaging the vectors of each constituent word, following ().", "labels": [], "entities": []}, {"text": "For any country or state's word embedding, we compute the average axis projection of a neutral word set W onto the male-female axis as: This average axis projection is our primary measure of gender bias in word embeddings.", "labels": [], "entities": []}, {"text": "For any neutral word list (e.g., government terms), we compute the average axis projection for all countries (or states) and compute its correlation to international (or U.S.) gender gaps.", "labels": [], "entities": [{"text": "average axis projection", "start_pos": 67, "end_pos": 90, "type": "METRIC", "confidence": 0.7100779811541239}]}, {"text": "govt intellect workplace excellent childcare illness communal victim \"pretty\" r-1 r-2 r-3 r-  plots each country's government/political word bias against the World Economic Forum's Political Empowerment Gender Gap sub-index (from 0 to 1, where greater score indicates less gap).", "labels": [], "entities": []}, {"text": "The value 0.0 on the x-axis indicates no gender bias, and female bias increases along the x-axis.", "labels": [], "entities": []}, {"text": "Consequently, is consistent with the hypothesis that-globally, over our set of 99 countries-women's political influence and power increase (relative to men) as political language shows a more female bias.", "labels": [], "entities": []}, {"text": "We present results of each thematic word set regressed against all available international statistics.", "labels": [], "entities": []}, {"text": "For each pair of themed word set and gender gap statistic, the algorithm (1) performs feature selection on 20% of the countries to optionally down-select from the set of words in the themed word set, (2) uses the down-selected word set to compute the R 2 determination against the full set of countries, and then (3) repeats a total of five times and averages the answers.", "labels": [], "entities": []}, {"text": "Feature selection monotonically increases the R 2 , and using 20% of countries helps prevent over-fitting.", "labels": [], "entities": [{"text": "R 2", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9806855916976929}]}, {"text": "includes our results over this analysis, grouping gender gap sub-indices (bold) with their related statistics.", "labels": [], "entities": []}, {"text": "This illustrates that different word sets vary in their correlation direction and strength across different statistic groups: the political set is positively correlated with the political empowerment subgroup and marginal on some economic statistics, but weak over health and education; intellectual and workplace terms positively correlate with economic statistics but are weak predictors otherwise; illness terms indirectly correlated with health and survival statistics, but are weak correlates elsewhere; and so-forth.", "labels": [], "entities": []}, {"text": "The word \"pretty,\" shown in, was the single word with the strongest determination against the overall gender gap and other sub-indices.", "labels": [], "entities": []}, {"text": "also includes four randomly-generated word sets, which do not exceed R 2 = 0.09 for any gender gap.", "labels": [], "entities": [{"text": "R 2", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9607182145118713}]}, {"text": "The selective correlation of these thematic word sets with related gender gap statistics supports our claim that gender biases in word embeddings can help characterize and predict statistical gender gaps across cultures.", "labels": [], "entities": []}, {"text": "Since we trained our embeddings on tweets alone-with as few as 98K tweets  for some countries-this also supports our claim that social media is a plausible source to compute a culture's gender bias in language.", "labels": [], "entities": []}, {"text": "None of our themed word sets strongly correlated with: (1) sex ratio at birth, which was 1.0 for the vast majority of countries; (2) percentage of last 50 years with female head of state; and (3) survey-based wage equality.", "labels": [], "entities": [{"text": "sex ratio", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.8626595139503479}]}, {"text": "The latter two gender gaps may correlate with other themed word sets, or they may have a more complex or nonlinear relationship to a culture's gender bias in language.", "labels": [], "entities": []}], "tableCaptions": []}