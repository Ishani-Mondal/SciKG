{"title": [{"text": "Contextualized Representations for Low-resource Utterance Tagging", "labels": [], "entities": [{"text": "Contextualized Representations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8306437432765961}, {"text": "Low-resource Utterance Tagging", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6732431848843893}]}], "abstractContent": [{"text": "Utterance-level analysis of the speaker's intentions and emotions is a core task in conversational understanding.", "labels": [], "entities": [{"text": "Utterance-level analysis of the speaker's intentions and emotions", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.8797504835658603}, {"text": "conversational understanding", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.7275551557540894}]}, {"text": "Depending on the end objective of the conversational understanding task, different categorical dialog-act or affect labels are expertly designed to cover specific aspects of the speakers' intentions or emotions respectively.", "labels": [], "entities": [{"text": "conversational understanding task", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.7670396963755289}]}, {"text": "Accurately annotating with these labels requires a high level of human expertise , and thus applying this process to a large conversation corpus or new domains is prohibitively expensive.", "labels": [], "entities": []}, {"text": "The resulting paucity of data limits the use of sophisticated neural models.", "labels": [], "entities": []}, {"text": "In this paper, we tackle these limitations by performing unsupervised training of utterance representations from a large corpus of spontaneous dialogue data.", "labels": [], "entities": []}, {"text": "Models initialized with these representations achieve competitive performance on utterance-level dialogue-act recognition and emotion classification, especially in low-resource settings encountered when analyzing conversations in new domains.", "labels": [], "entities": [{"text": "utterance-level dialogue-act recognition", "start_pos": 81, "end_pos": 121, "type": "TASK", "confidence": 0.5992263456185659}, {"text": "emotion classification", "start_pos": 126, "end_pos": 148, "type": "TASK", "confidence": 0.7522461414337158}]}], "introductionContent": [{"text": "Spontaneous human conversations have been collected in different domains to support research in data-driven dialogue systems (, affective computing (), clinical psychology () and tutoring systems (.", "labels": [], "entities": []}, {"text": "These conversations are analyzed by segmenting transcriptions into each speaker's utterances (, which are often labeled with different types of information.", "labels": [], "entities": []}, {"text": "The exact type of label to be used depends on the downstream task or research questions to be answered, and thus the tagging paradigms are varied and numerous.", "labels": [], "entities": []}, {"text": "For example, the speaker's intention can be specified using a dialogue acts (DAs) or speech acts, which capture the pragmatic or semantic function of the utterance.", "labels": [], "entities": []}, {"text": "Utterances may also be tagged with traits such as sentiment, emotion and valence labels (, speaker persuasiveness (), speaker dominance( and other characteristics at the utterance and conversational level.", "labels": [], "entities": []}, {"text": "While these labels vary greatly, one constant is that they are often ambiguous and contextdependent, making it challenging for humans to annotate efficiently and accurately.", "labels": [], "entities": []}, {"text": "Thus, curating large corpora is labor-intensive, and we are always faced with a paucity of data in new domains and labeling paradigms of interest.", "labels": [], "entities": []}, {"text": "Moreover, the label assigned to an utterance depends on the current state of the dialogue) and prediction of an utterance's label benefits from referring to other utterances in context and their labels (.", "labels": [], "entities": []}, {"text": "Deep learning models like RNNs and CNNs have proven effective tools to encode neighbouring utterances.", "labels": [], "entities": []}, {"text": "However such models rely on large annotated corpora that are prohibitively expensive to procure, especially for niche domains.", "labels": [], "entities": []}, {"text": "One recently popular method to overcome the dearth of supervised data in NLP is unsupervised pretraining overlarge unlabeled corpora.", "labels": [], "entities": []}, {"text": "For ex-ample,;; use language modeling as an unsupervised task to learn word embeddings in context, and demonstrate remarkable improvements on a number of downstream NLP tasks.", "labels": [], "entities": []}, {"text": "However, these methods learn representations for individual words, whereas for dialog analysis tasks, we need representations for utterances in the context of the entire dialog.", "labels": [], "entities": [{"text": "dialog analysis", "start_pos": 79, "end_pos": 94, "type": "TASK", "confidence": 0.8769673109054565}]}, {"text": "In this paper, we adapt the technique of learning contextualized representations using unsupervised pretraining to learn representations for utterances in the context of the dialogue.", "labels": [], "entities": []}, {"text": "We first introduce a general model architecture consisting of a token, utterance, and conversation encoder.", "labels": [], "entities": []}, {"text": "We then present a method to efficiently train this model by predicting the bag-of-word vectors of previous and next utterances over a large heterogeneous corpus of spoken dialogue transcripts.", "labels": [], "entities": []}, {"text": "We quantify the effectiveness of learnt contextual utterance representations on two downstream utterance-labeling tasks: DA tagging and emotion recognition.", "labels": [], "entities": [{"text": "DA tagging", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.8692143559455872}, {"text": "emotion recognition", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7291604280471802}]}, {"text": "We obtain competitive performance on two popular DA tagging tasks (SwitchBoard and ICSI Meeting Recorder) and an emotion labeling task (IEMO-CAP).", "labels": [], "entities": [{"text": "DA tagging", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.942999005317688}, {"text": "ICSI Meeting Recorder", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.7860035101572672}, {"text": "emotion labeling task", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7454855144023895}]}, {"text": "Particularly, we observe significant improvements over training complex utterance tagging models from scratch for simulated low-resource settings for these tasks as well as for considerably smaller DA datasets such as LEGO and Map Task.", "labels": [], "entities": [{"text": "utterance tagging", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7396044135093689}]}], "datasetContent": [{"text": "We train contextualized utterance representations on transcriptions of spontaneous human-human conversation corpora ().", "labels": [], "entities": []}, {"text": "We choose the corpora presented in for this work.", "labels": [], "entities": []}, {"text": "A majority of the conversations are dialogues, and utterances across all corpora are 10 words long on average.", "labels": [], "entities": []}, {"text": "However, the chosen corpora have conversations of widely varying lengths (no. of utterances/conversation).", "labels": [], "entities": []}, {"text": "For computation/memory efficiency, and also because more distant utterances likely have diminishing influence on discourse modeling, we divide each conversation into conversational snippets of length 64 1 by moving a 64-length window over the conversation with stride 1 and train the bag-of-word loss on each snippet thus obtained.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.693159356713295}]}, {"text": "For the conversational encoder, we use 2 layers of the transformer with 8 attention heads of 64 dimensions each.", "labels": [], "entities": []}, {"text": "All feedforward networks use 2 layers with hidden size of 512.", "labels": [], "entities": []}, {"text": "For training and fine-tuning, we use the Adam () with learning rate 0.0001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.9624226689338684}]}, {"text": "Tasks We evaluate performance of our model on these utterance-level tagging tasks: SwDA, the Switchboard Dialogue Act Corpus, annotates 1,155 telephonic conversations (224K utterances) with one of the 42 DAs in the DAMSL (Jurafsky, 1997) taxonomy.", "labels": [], "entities": [{"text": "DAMSL (Jurafsky, 1997) taxonomy", "start_pos": 215, "end_pos": 246, "type": "DATASET", "confidence": 0.6835389392716544}]}, {"text": "MRDA, the ICSI Meeting Recorder Dialogue Act corpus annotates 75 multi-party meetings (105K utterances) with DAs according to 5 domain-specific tags ().", "labels": [], "entities": [{"text": "MRDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8764711618423462}, {"text": "ICSI Meeting Recorder Dialogue Act corpus annotates", "start_pos": 10, "end_pos": 61, "type": "DATASET", "confidence": 0.9110063740185329}]}, {"text": "IEMOCAP, an emotion recognition dataset of 12 hours of dyadic improvisations or scripted scenarios, with eight categorical emotion labels) (10K utterances).", "labels": [], "entities": [{"text": "IEMOCAP", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5884822607040405}]}, {"text": "LEGO, a subset (14K utterances) of the Lets Go bus-information dialogue system corpus () annotated with the ISO 24617-2 standard for conversation functions of task by (.", "labels": [], "entities": [{"text": "LEGO", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5740950703620911}]}, {"text": "Map Task, () is 18 hrs of dialogue where speakers collaborate to complete a map (5K utterances).", "labels": [], "entities": []}, {"text": "To simulate low-resource settings for the larger datasets like SWDA and MRDA, we experiment with different sizes of the training datasets and evaluate on the standard test set for these.", "labels": [], "entities": [{"text": "SWDA", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.7722374200820923}, {"text": "MRDA", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.6978410482406616}]}, {"text": "For LEGO and MapTask, we use 10-fold cross validation.", "labels": [], "entities": [{"text": "MapTask", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9094792008399963}]}, {"text": "We use four different experimental settings to measure the efficacy of our pretrained utterance representations : No ContextWith no conversational encoder (i.e. independently encoding every utterance using ELMo); Random Initialization -with the conversational encoder randomly initialized and trained on only the downstream tagging task; Freeze Network -the conversational encoder initialized using the model pretrained on our bag-of-word objective and kept fixed for downstream task; Pre-trained Initializationthe initialized conversation encoder fine-tuned on the downstream task.", "labels": [], "entities": []}, {"text": "These settings are used to isolate the gains from using (1) contextualized representations, (2) pretraining them and then (3) finetuning them on the downstream task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: SwDA DA categories that improve using pre- trained utterance embeddings with % improvements in  accuracy over other experimental settings.", "labels": [], "entities": [{"text": "SwDA DA", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9300273358821869}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9985464215278625}]}, {"text": " Table 4: Results on LEGO and Map Task", "labels": [], "entities": []}]}