{"title": [{"text": "Arabic Named Entity Recognition: What Works and What's Next", "labels": [], "entities": [{"text": "Arabic Named Entity Recognition", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5693183615803719}]}], "abstractContent": [{"text": "This paper presents the winning solution to the Arabic Named Entity Recognition challenge run by Topcoder.com.", "labels": [], "entities": [{"text": "Arabic Named Entity Recognition challenge", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.7268466353416443}]}, {"text": "The proposed model integrates various tailored techniques together, including representation learning, feature engineering , sequence labeling, and ensemble learning.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.920466810464859}, {"text": "sequence labeling", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7241372168064117}]}, {"text": "The final model achieves a test F 1 score of 75.82% on the AQMAR dataset and outperforms baselines by a large margin.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9860745469729105}, {"text": "AQMAR dataset", "start_pos": 59, "end_pos": 72, "type": "DATASET", "confidence": 0.9799784719944}]}, {"text": "Detailed analyses are conducted to reveal both its strengths and limitations.", "labels": [], "entities": []}, {"text": "Specifically, we observe that (1) representation learning modules can significantly boost the performance but requires a proper pre-processing and (2) the resulting embedding can be further enhanced with feature engineering due to the limited size of the training data.", "labels": [], "entities": []}, {"text": "All implementations and pre-trained models are made public 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Aiming to identify entities in natural language, named entity recognition (NER) serves as one of the fundamental steps in various applications.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7904933641354243}]}, {"text": "In many languages, the performance of NER has been significantly improved because of recent advances in representation learning (.", "labels": [], "entities": [{"text": "NER", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9430590271949768}, {"text": "representation learning", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.901266485452652}]}, {"text": "To promote the development of Arabic NER, a challenge was hosted on Topcoder.com 2 based on the public Arabic NER benchmark dataset (i.e., the AQMAR dataset).", "labels": [], "entities": [{"text": "Arabic NER benchmark dataset", "start_pos": 103, "end_pos": 131, "type": "DATASET", "confidence": 0.6093060299754143}, {"text": "AQMAR dataset", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.9286409616470337}]}, {"text": "Challenge submissions were required to only use annotations from the training set, and manual reviews on the submitted solutions were further conducted to prevent cheating.", "labels": [], "entities": [{"text": "cheating", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9588670134544373}]}, {"text": "Among 137 registrants competing in the challenge , we placed the first by tailoring various techniques and incorporating them all together.", "labels": [], "entities": []}, {"text": "Intuitively, it is hard to only rely on feature engineering to capture textual signals, especially for morphologically rich languages like Arabic.", "labels": [], "entities": []}, {"text": "At the same time, neural networks have demonstrated their great potentials to automate high-quality representation construction in an endto-end manner.", "labels": [], "entities": []}, {"text": "Therefore, we leverage embedding modules to represent words with pre-trained vectors fora better quality.", "labels": [], "entities": []}, {"text": "Besides, we observe that handcrafted features can bring a considerable improvement.", "labels": [], "entities": []}, {"text": "Consuming all these features, we train multiple LSTM-CRF models to construct the mapping from representations to predictions, and further aggregate their outputs with ensemble learning.", "labels": [], "entities": []}, {"text": "Moreover, we incorporate a dictionary-based string matching model and observe that it can improve the recall at some cost of precision, which results in a marginal F 1 -score improvement.", "labels": [], "entities": [{"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9992150068283081}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.999549925327301}, {"text": "F 1 -score", "start_pos": 164, "end_pos": 174, "type": "METRIC", "confidence": 0.9724516868591309}]}, {"text": "Our final ensemble model achieves a test F 1 score of 75.82%, outperforming all other participants as well as the previous state-of-the-arts by significant margins.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9809558590253195}]}, {"text": "We further conduct analyses on our solution to get deeper insights on the task: (1) the effectiveness of representation learning and (2) the role of feature engineering.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.8975143730640411}, {"text": "feature engineering", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.716272696852684}]}, {"text": "The rest of paper is organized as follow.", "labels": [], "entities": []}, {"text": "The next section discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces the problem setting and presents the data analysis.", "labels": [], "entities": []}, {"text": "The proposed framework is presented in Section 4, including model ensemble and dictionary-based model.", "labels": [], "entities": []}, {"text": "Tailored representations modules are introduced in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we discuss the experimental results in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the experimental results on the AQMAR dataset.", "labels": [], "entities": [{"text": "AQMAR dataset", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9776199758052826}]}], "tableCaptions": [{"text": " Table 1: Dataset Statistics of the AQMAR dataset.", "labels": [], "entities": [{"text": "AQMAR dataset", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.9797253608703613}]}, {"text": " Table 2: Model Performance and Ablation Study for  the AQMAR dataset.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9954466223716736}, {"text": "AQMAR dataset", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.9738937020301819}]}]}