{"title": [], "abstractContent": [{"text": "This paper describes the submissions of the eTranslation team to the WMT 2019 news translation shared task.", "labels": [], "entities": [{"text": "WMT 2019 news translation shared task", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.761188417673111}]}, {"text": "The systems have been developed with the aim of identifying and following rather than establishing best practices, under the constraints imposed by a low resource training and decoding environment normally used for our production systems.", "labels": [], "entities": []}, {"text": "Thus most of the findings and results are transfer-able to systems used in the eTranslation service.", "labels": [], "entities": []}, {"text": "Evaluations suggest that this approach is able to produce decent models with good performance and speed without the overhead of using prohibitively deep and complex archi-tectures.", "labels": [], "entities": [{"text": "speed", "start_pos": 98, "end_pos": 103, "type": "METRIC", "confidence": 0.9623783230781555}]}], "introductionContent": [{"text": "The European Commission's eTranslation 1 project, a building block of the Connecting Europe Facility (CEF), has been setup to help European and national public administrations exchange information across language barriers in the EU.", "labels": [], "entities": []}, {"text": "It provides secure access to machine translation (both formatted documents and text snippets) between all 26 official languages of the EU and the EEA for translators and officials in EU and national authorities.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7384946942329407}]}, {"text": "In addition it enables multilinguality in all Digital Service Infrastructures of CEF.", "labels": [], "entities": [{"text": "CEF", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9475048184394836}]}, {"text": "CEF eTranslation builds on the previous machine translation service of the European Commission, MT@EC, developed by the Directorate-General for Translation (DGT) since 2010.", "labels": [], "entities": [{"text": "CEF eTranslation", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9430661797523499}, {"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7487737238407135}, {"text": "MT@EC", "start_pos": 96, "end_pos": 101, "type": "DATASET", "confidence": 0.6378730734189352}]}, {"text": "MT@EC translation engines were trained using the vast Euramis translation memories (), comprising over 1 billion sentences in the 24 official EU languages, produced by the translators of the EU institutions over the past decades.", "labels": [], "entities": [{"text": "MT@EC translation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7768168151378632}, {"text": "Euramis translation memories", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.6792889634768168}]}, {"text": "While this large set of training data provides very good coverage of the type of language used in official EU documents, recent usage of the service is trending towards texts from other domains.", "labels": [], "entities": []}, {"text": "The eTranslation team is working to widen the scope of the service and improve the coverage in more general types of texts.", "labels": [], "entities": []}, {"text": "Given this background, the participation of eTranslation in this year's shared task on news translation is an early, but important step on a longer path towards a more generic MT service.", "labels": [], "entities": [{"text": "news translation", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.6996056586503983}, {"text": "MT service", "start_pos": 176, "end_pos": 186, "type": "TASK", "confidence": 0.9167759120464325}]}, {"text": "We participated in the task with 4 different language pairs: English\u2192German, French\u2192German, English\u2192Lithuanian and Russian\u2192English, in order to find best practices that guarantee the production of a solid system in a constrained resource environment.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran a few experiments with unconstrained models making use of the Euramis () data set.", "labels": [], "entities": [{"text": "Euramis () data set", "start_pos": 69, "end_pos": 88, "type": "DATASET", "confidence": 0.9120817333459854}]}, {"text": "This data contains millions of segments for 3 of the 4 language pairs we worked with and offers itself as a natural resource to build unconstrained models from.", "labels": [], "entities": []}, {"text": "At the same time it is in general quite distant from the news domain.", "labels": [], "entities": []}, {"text": "Thus for the high resource language pairs (En\u2192De, Fr\u2192De) we first tried to use only those subsets which might be closer to the shared task domain.", "labels": [], "entities": []}, {"text": "We extracted additional training data using language models built from monolingual news corpora as reference in-domain text with the XenC toolkit.", "labels": [], "entities": []}, {"text": "For Fr\u2192De we built the language model from the topic modeling based selection and also experimented with extracting Euramis data using the same guided LDA process as described in Section 3.2.1.", "labels": [], "entities": []}, {"text": "We re-ran the trainings of the best constrained models by adding 2M and later 3M Euramis segments to the training data but as we cannot report on any improvement, we stopped this line of experiments and did not submit the unconstrained systems.", "labels": [], "entities": []}, {"text": "For En\u2192Lt, we trained 3 non-constrained models by adding to the best constrained system (i) all our Euramis data, (ii) 1M and (iii) 2M segment subsets selected as described above.", "labels": [], "entities": [{"text": "Euramis data", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.846547931432724}]}, {"text": "This resulted in a very small improvement of less than 0.5 BLEU points for the models with selected Euramis subsets, while the model with the full Euramis data was almost 2 BLEU points worse.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9985659718513489}, {"text": "Euramis data", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.7731776833534241}, {"text": "BLEU", "start_pos": 173, "end_pos": 177, "type": "METRIC", "confidence": 0.9988658428192139}]}, {"text": "We thus decided not to submit any of the 3 systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of segments in the filtered parallel data used for baseline models.", "labels": [], "entities": []}, {"text": " Table 2: Results for En\u2192De models. The 2019 results  are post-submission.", "labels": [], "entities": []}, {"text": " Table 3: Results for Fr\u2192De models. The 2019 results  are post-submission.", "labels": [], "entities": []}, {"text": " Table 4: Results for En\u2192Lt models. The 2019 results  are post-submission.", "labels": [], "entities": []}, {"text": " Table 5: Results for Ru\u2192En models. The 2019 results  are post-submission.", "labels": [], "entities": []}]}