{"title": [{"text": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9171579480171204}, {"text": "BERT", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9849464893341064}]}], "abstractContent": [{"text": "We show that BERT (Devlin et al., 2018) is a Markov random field language model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9971204996109009}]}, {"text": "This formulation gives way to a natural procedure to sample sentences from BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.4511144757270813}]}, {"text": "We generate from BERT and find that it can produce high-quality, fluent generations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.955976665019989}]}, {"text": "Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9524950981140137}]}], "introductionContent": [{"text": "BERT () is a recently released sequence model used to achieve state-of-art results on a wide range of natural language understanding tasks, including constituency parsing and machine translation.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.971977949142456}, {"text": "natural language understanding tasks", "start_pos": 102, "end_pos": 138, "type": "TASK", "confidence": 0.7215401902794838}, {"text": "constituency parsing", "start_pos": 150, "end_pos": 170, "type": "TASK", "confidence": 0.8762396574020386}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.8337929844856262}]}, {"text": "Early work probing BERT's linguistic capabilities has found it surprisingly robust.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.6705047488212585}]}, {"text": "BERT is trained on a masked language modeling objective.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.871188223361969}, {"text": "masked language modeling", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.6590150892734528}]}, {"text": "Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, masked language modeling predicts a word given its left and right context.", "labels": [], "entities": []}, {"text": "Because the model expects context from both directions, it is not immediately obvious how BERT can be used as a traditional language model (i.e., to evaluate the probability of a text sequence) or how to sample from it.", "labels": [], "entities": [{"text": "BERT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9012742042541504}]}, {"text": "We attempt to answer these questions by showing that BERT is a combination of a Markov random field language model (MRF-LM,) with pseudo loglikelihood training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9964250922203064}]}, {"text": "This formulation automatically leads to a sampling procedure based on Gibbs sampling.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments demonstrate the potential of using BERT as a standalone language model rather than as a parameter initializer for transfer learning (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9529210925102234}, {"text": "transfer learning", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.9017885327339172}]}, {"text": "We show that sentences sampled from BERT are well-formed and are assigned high probabilities by an off-theshelf language model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6515958309173584}]}, {"text": "We take pretrained BERT models trained on a mix of Toronto Book Corpus (TBC, and Wikipedia provided by and its PyTorch implementation 4 provided by HuggingFace.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9621838927268982}, {"text": "Toronto Book Corpus (TBC", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.932060706615448}]}, {"text": "We experiment with both the base and large BERT configuations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9678304195404053}]}, {"text": "We consider several evaluation metrics to estimate the quality and diversity of the generations.", "labels": [], "entities": []}, {"text": "Diversity To measure the diversity of each model's generations, we compute self-BLEU (: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9945805668830872}]}, {"text": "Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.", "labels": [], "entities": []}, {"text": "We also evaluate the percentage of n-grams that are unique, when compared to the original data distribution and within the corpus of generations.", "labels": [], "entities": []}, {"text": "We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique n-grams implies higher BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9985185265541077}, {"text": "BLEU", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.9980505704879761}]}, {"text": "Methodology We use the non-sequential sampling scheme with sampling from the top k = 100 most frequent words at each time step, as empirically this led to the most coherent generations.", "labels": [], "entities": []}, {"text": "We show generations from the sequential sampler in: Quality metrics of model generations.", "labels": [], "entities": []}, {"text": "Perplexity (PPL) is measured using an additional language model ().", "labels": [], "entities": [{"text": "Perplexity (PPL)", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8756079077720642}]}, {"text": "For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets.", "labels": [], "entities": [{"text": "WT103", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.9580772519111633}]}, {"text": "Transformer (, which was trained on TBC and has approximately the same number of parameters as the base configuration of BERT.", "labels": [], "entities": [{"text": "TBC", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.8744602799415588}, {"text": "BERT", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9244645833969116}]}, {"text": "For BERT, we pad each input with special symbols and.", "labels": [], "entities": [{"text": "BERT", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9912453293800354}]}, {"text": "For GPT, we start with a start of sentence token and generate left to right.", "labels": [], "entities": [{"text": "GPT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8166776299476624}]}, {"text": "For all models, we generate 1000 uncased sequences of length 40.", "labels": [], "entities": []}, {"text": "Finally, as a trivial baseline, we sample 1000 sentences from TBC and the training split of WT103 and compute all automatic metrics against these samples.", "labels": [], "entities": [{"text": "WT103", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9293355941772461}]}], "tableCaptions": [{"text": " Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText- 103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC  rows, we sample 1000 sentences from the respective datasets.", "labels": [], "entities": [{"text": "WikiText- 103 test set", "start_pos": 104, "end_pos": 126, "type": "DATASET", "confidence": 0.9256901383399964}, {"text": "Toronto Book Corpus", "start_pos": 168, "end_pos": 187, "type": "DATASET", "confidence": 0.9742984374364217}, {"text": "WT103", "start_pos": 205, "end_pos": 210, "type": "DATASET", "confidence": 0.9677776098251343}]}]}