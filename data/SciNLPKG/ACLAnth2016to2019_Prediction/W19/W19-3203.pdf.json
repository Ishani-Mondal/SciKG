{"title": [], "abstractContent": [], "introductionContent": [{"text": "The intent of the #SMM4H shared tasks series is to challenge the community with Natural Language Processing tasks for mining relevant data for health monitoring and surveillance in social media.", "labels": [], "entities": [{"text": "SMM4H shared tasks series", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.9033601731061935}]}, {"text": "Such challenges require processing imbalanced, noisy, real-world, and substantially creative language expressions from social media.", "labels": [], "entities": []}, {"text": "The competing systems should be able to deal with many linguistic variations and semantic complexities in the various ways people express medication-related concepts and outcomes.", "labels": [], "entities": []}, {"text": "It has been shown in past research () that automated systems frequently under-perform when exposed to social media text because of the presence of novel/creative phrases, misspellings and frequent use of idiomatic, ambiguous and sarcastic expressions.", "labels": [], "entities": []}, {"text": "The tasks act as a discovery and verification process of what approaches work best for social media data.", "labels": [], "entities": []}, {"text": "As in previous years, our tasks focused on mining health information from Twitter.", "labels": [], "entities": []}, {"text": "This year we challenged the community with two different problems.", "labels": [], "entities": []}, {"text": "The first problem focuses on performing pharmacovigilance from social media data.", "labels": [], "entities": []}, {"text": "It is now well understood that social media data may contain reports of adverse drug reactions (ADRs) and these reports may complement traditional adverse event reporting systems, such as the FDA adverse event reporting system (FAERS).", "labels": [], "entities": []}, {"text": "However, automatically curating reports from adverse reactions from Twitter requires the application of a series of NLP methods in an end-to-end pipeline.", "labels": [], "entities": [{"text": "curating reports from adverse reactions from Twitter", "start_pos": 23, "end_pos": 75, "type": "TASK", "confidence": 0.8444596358707973}]}, {"text": "The first three tasks of this year's challenge represent three key NLP problems in asocial media based pharmacovigilance pipeline -(i) automatic classification of ADRs, (ii) extraction of spans of ADRs and (iii) normal-ization of the extracted ADRs to standardized IDs.", "labels": [], "entities": [{"text": "automatic classification of ADRs", "start_pos": 135, "end_pos": 167, "type": "TASK", "confidence": 0.6919571757316589}]}, {"text": "The second problem explores the generalizability of predictive models.", "labels": [], "entities": []}, {"text": "In health research using social media, it is often necessary for researchers to build individual classifiers to identify health mentions of a particular disease in a particular context.", "labels": [], "entities": []}, {"text": "Classification models that can generalize to different health contexts would be greatly beneficial to researchers in these fields (e.g.,), as this would allow researchers to more easily apply existing tools and resources to new problems.", "labels": [], "entities": []}, {"text": "Motivated by these ideas, Task 4 was testing tweet classification methods across diverse health contexts, so the test data included a very different health context than the training data.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7508361041545868}]}, {"text": "This setting measures the ability of tweet classifiers to generalize across health contexts.", "labels": [], "entities": []}, {"text": "The fourth iteration of our series follows the same organization as previous iterations.", "labels": [], "entities": []}, {"text": "We collected posts from Twitter, annotated the data for the four tasks proposed and released the posts to the registered teams.", "labels": [], "entities": []}, {"text": "This year, we conducted the evaluation of all participating systems using Codalab, an open source platform facilitating data science competitions.", "labels": [], "entities": []}, {"text": "The performances of the systems were compared on a blind evaluations sets for each task.", "labels": [], "entities": []}, {"text": "All teams registered were allowed to participate to one or multiple tasks.", "labels": [], "entities": []}, {"text": "We provided the participants with two sets of data for each task, a training and a test set.", "labels": [], "entities": []}, {"text": "Participants had a period of six weeks, from March 5 th to April 15 th , for training their systems on our training sets, and 4 days, from the 16 th to 20 th of April, for calibrating their systems on our test sets and submitting their predictions.", "labels": [], "entities": []}, {"text": "In total 34 teams registered and 19 teams submitted at least one run (each team was allowed to submit, at most, three runs per task).", "labels": [], "entities": []}, {"text": "In detail, we received 43 runs for task 1, 24 for task 2, 10 for task 3 and 15 for task 4.", "labels": [], "entities": []}, {"text": "We briefly describe each task and their data in section 2, before discussing the results obtained in section 3.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Task 2. System and resource descriptions for ADR mentions extraction in tweets", "labels": [], "entities": [{"text": "ADR mentions extraction in tweets", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.8772474527359009}]}, {"text": " Table 5: System performances for each team for task 1 of the shared task. F1-score, Precision and Recall over the  ADR class are shown. Top scores in each column are shown in bold.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995635151863098}, {"text": "Precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9993269443511963}, {"text": "Recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9972039461135864}, {"text": "ADR", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.7017062902450562}]}, {"text": " Table 6: System performances for each team for task 2 of the shared task. (Strict/Relaxed) F1-score, Precision  and Recall over the ADR mentions are shown. Top scores in each column are shown in bold.", "labels": [], "entities": [{"text": "Relaxed", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9386219382286072}, {"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.907667338848114}, {"text": "Precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991723299026489}, {"text": "Recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.996561586856842}, {"text": "ADR mentions", "start_pos": 133, "end_pos": 145, "type": "METRIC", "confidence": 0.8384163975715637}]}, {"text": " Table 7: System performances for each team for task 3 of the shared task. (Strict/Relaxed) F1-score, Precision  and Recall over the ADR resolution are shown. Top scores in each column are shown in bold.", "labels": [], "entities": [{"text": "Relaxed", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.945151150226593}, {"text": "F1-score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9430105686187744}, {"text": "Precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991902709007263}, {"text": "Recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9970961809158325}, {"text": "ADR resolution", "start_pos": 133, "end_pos": 147, "type": "METRIC", "confidence": 0.7866218984127045}]}, {"text": " Table 8: System performances for each team for task 4 of the shared task. Accuracy, F1-score, Precision and  Recall over the personal mentions are shown. Top scores in each column are shown in bold.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9993537068367004}, {"text": "F1-score", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9996765851974487}, {"text": "Precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9994440674781799}, {"text": "Recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.998531699180603}]}]}