{"title": [{"text": "Detecting Syntactic Change Using a Neural Part-of-Speech Tagger", "labels": [], "entities": [{"text": "Detecting Syntactic Change", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8670568664868673}, {"text": "Neural Part-of-Speech Tagger", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7173019250233968}]}], "abstractContent": [{"text": "We train a diachronic long short-term memory (LSTM) part-of-speech tagger on a large corpus of American English from the 19th, 20th, and 21st centuries.", "labels": [], "entities": []}, {"text": "We analyze the tagger's ability to implicitly learn temporal structure between years, and the extent to which this knowledge can be transferred to date new sentences.", "labels": [], "entities": []}, {"text": "The learned year embeddings show a strong linear correlation between their first principal component and time.", "labels": [], "entities": []}, {"text": "We show that temporal information encoded in the model can be used to predict novel sentences' years of composition relatively well.", "labels": [], "entities": []}, {"text": "Comparisons to a feedforward baseline suggest that the temporal change learned by the LSTM is syntactic rather than purely lexical.", "labels": [], "entities": []}, {"text": "Thus, our results suggest that our tagger is implicitly learning to model syntactic change in American English over the course of the 19th, 20th, and early 21st centuries.", "labels": [], "entities": []}], "introductionContent": [{"text": "We define a diachronic language task as a standard computational linguistic task where the input includes not just text, but also information about when the text was written.", "labels": [], "entities": []}, {"text": "In particular, diachronic part-of-speech (POS) tagging is the task of assigning POS tags to a sequence of words dated to a specific year.", "labels": [], "entities": [{"text": "diachronic part-of-speech (POS) tagging", "start_pos": 15, "end_pos": 54, "type": "TASK", "confidence": 0.6656627953052521}, {"text": "assigning POS tags to a sequence of words dated to a specific year", "start_pos": 70, "end_pos": 136, "type": "TASK", "confidence": 0.7270171435979697}]}, {"text": "Our goal is to determine the extent to which such a tagger learns a representation of syntactic change in modern American English.", "labels": [], "entities": []}, {"text": "Our method approaches this problem using neural networks, which have seen considerable success in a diverse array of natural language processing tasks over the last few years.", "labels": [], "entities": []}, {"text": "Prior work using deep learning methods to analyze language change * Authors (listed in alphabetical order) contributed equally.", "labels": [], "entities": []}, {"text": "\u2020 Work completed while the author was at Yale University. has focused more on lexical, rather than syntactic, change.", "labels": [], "entities": []}, {"text": "One of these works,, measured linguistic change by evaluating a language model's perplexity on novel documents from different years.", "labels": [], "entities": []}, {"text": "Previous work focusing on syntactic change utilized mathematical simulations rather than empirically trained models.", "labels": [], "entities": [{"text": "syntactic change", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.8063889145851135}]}, {"text": "attempted to build a mathematical model of syntactic change motivated by theories of language contact and acquisition.", "labels": [], "entities": []}, {"text": "They found that their model predicted both gradual and sudden changes in a parameterized grammar depending on the properties of the languages in contact.", "labels": [], "entities": []}, {"text": "In particular, they used their simulation to study how verbsecond (V2) order was gained and lost throughout the history of the French language.", "labels": [], "entities": []}, {"text": "For several toy languages, their model found that contact between languages with and without V2 would lead to gradual adoption of V2 syntax by the entire population.", "labels": [], "entities": []}, {"text": "We use the Corpus of Historical American English (COHA), an LSTM POS tagger, and dimensionality reduction techniques to investigate syntactic change in American English during the 19th through 21st centuries.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6689155697822571}]}, {"text": "Our project takes the POS tagging task as a proxy for diachronic syntax modeling and has three main goals: 1.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.8579212228457133}, {"text": "diachronic syntax modeling", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6874047517776489}]}, {"text": "Assess whether a temporal progression is encoded in the network's learned year embeddings.", "labels": [], "entities": []}, {"text": "2. Verify that the represented temporal change reflects syntax rather than simply word frequency.", "labels": [], "entities": []}, {"text": "3. Determine whether our model can be used to date novel sentences.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Test accuracies for all architectural variants.  The networks differed in whether year information was  included as input and whether the hidden layer had  LSTM or feedforward connections.", "labels": [], "entities": []}, {"text": " Table 2: Training accuracies for all architectural vari- ants.", "labels": [], "entities": []}, {"text": " Table 3: Average distance between each time period's  center and the year that minimizes the perplexity value  of the corresponding LOWESS curve. For the decade- level metric, the \"center\" is the middle year of the  decade (1815 for 1810s). For the year-level metric, the  \"center\" is the year itself (1803 for 1803).", "labels": [], "entities": [{"text": "LOWESS", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9492812156677246}]}]}