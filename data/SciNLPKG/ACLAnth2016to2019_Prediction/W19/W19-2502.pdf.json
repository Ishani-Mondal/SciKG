{"title": [{"text": "Clustering-Based Article Identification in Historical Newspapers", "labels": [], "entities": [{"text": "Clustering-Based Article Identification in Historical Newspapers", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.6539453864097595}]}], "abstractContent": [{"text": "This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page.", "labels": [], "entities": []}, {"text": "We frame the task as a segmentation plus clustering step.", "labels": [], "entities": []}, {"text": "Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words.", "labels": [], "entities": [{"text": "1912 New York Tribune magazine", "start_pos": 27, "end_pos": 57, "type": "DATASET", "confidence": 0.659168940782547}]}, {"text": "Furthermore, the automatic segmenta-tion based on the text results in low scores, due to the low quality of some OCRed documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Historical newspapers are among the \"most important\" and \"most often used\" sources for many historians: Since the rise of regional and local newspaper culture in the late 18th and early 19th centuries, newspapers provide a window into national and global events and debates as well as into local everyday life.", "labels": [], "entities": []}, {"text": "Traditionally, historical newspapers were stored on microfilms in local archives.", "labels": [], "entities": []}, {"text": "Access was manual, required travel and authorization, and was often complicated by poor film quality ().", "labels": [], "entities": []}, {"text": "Digital availability of newspapers has scaled up the accessibility of historical newspapers tremendously and enabled large-scale analysis of phenomena like text re-use ( or ethnic stereotyping (.", "labels": [], "entities": [{"text": "text re-use", "start_pos": 156, "end_pos": 167, "type": "TASK", "confidence": 0.6831144839525223}]}, {"text": "Digital access to the full range of information in a newspaper is challenging, though.", "labels": [], "entities": []}, {"text": "It requires (a), scanning of newspaper pages or microfilms into digital image files; (b), optical character recognition (OCR) to transfer images into text streams; and (c), identification of articles in the text stream.", "labels": [], "entities": [{"text": "optical character recognition (OCR)", "start_pos": 90, "end_pos": 125, "type": "TASK", "confidence": 0.6971973031759262}]}, {"text": "Few historical newspapers have gone through all In this paper, we ignore the issue of metadata extraction. steps.", "labels": [], "entities": [{"text": "metadata extraction", "start_pos": 86, "end_pos": 105, "type": "TASK", "confidence": 0.9066179394721985}]}, {"text": "For example, the vast Chronicling America archive of historical newspapers at the Library of Congress 2 only underwent steps (a) and (b), providing text files at the level of newspaper pages, without manual OCR post-correction (see.", "labels": [], "entities": [{"text": "Chronicling America archive of historical newspapers at the Library of Congress 2", "start_pos": 22, "end_pos": 103, "type": "DATASET", "confidence": 0.843136690557003}]}, {"text": "Due to the multi-column format of almost all newspapers, each text file contain multiple articles.", "labels": [], "entities": []}, {"text": "In addition, many articles span several pages: they are split across text files.", "labels": [], "entities": []}, {"text": "This is an obvious obstacle to any analysis requiring complete articles.", "labels": [], "entities": []}, {"text": "It becomes particularly pressing for articles that span multiple issues (typically days or weeks).", "labels": [], "entities": []}, {"text": "Notable among them are serial stories or serial novels, serialization being among the most important publication strategies for literary works in the 19th and 20th centuries.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the task of article identification across newspaper pages, corresponding to step (c) above.", "labels": [], "entities": [{"text": "article identification across newspaper pages", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.8851502537727356}]}, {"text": "We use only textual information from OCR as input, modelling the task as a sequence of a segmentation and a clustering step.", "labels": [], "entities": []}, {"text": "Whereas most previous work solely uses image data for similar tasks, here, we examine the performance of an approach that uses textual information only.", "labels": [], "entities": []}, {"text": "We introduce and provide anew annotated dataset sampled from the 1912 New York Tribune magazine.", "labels": [], "entities": [{"text": "annotated dataset sampled from the 1912 New York Tribune magazine", "start_pos": 30, "end_pos": 95, "type": "DATASET", "confidence": 0.7702034771442413}]}, {"text": "We find that clustering segments works relatively well for individual issues and becomes substantially more difficult across issues.", "labels": [], "entities": []}, {"text": "Segment similarity based on word embeddings outperforms character n-grams similarities for most cases.", "labels": [], "entities": []}, {"text": "The major challenge of the task is mainly the inferior scan quality which results in poor OCR text output.", "labels": [], "entities": []}], "datasetContent": [{"text": "To our knowledge, there is no standard dataset for article identification in historical newspapers.", "labels": [], "entities": [{"text": "article identification in historical newspapers", "start_pos": 51, "end_pos": 98, "type": "TASK", "confidence": 0.860304057598114}]}, {"text": "Thus, we created such a dataset.", "labels": [], "entities": []}, {"text": "We selected the five March 1912 issues of the New York tribune Sunday magazine 4 for annotation since this dataset contains long articles, some but not all of which are serializations that extend over multiple issues.", "labels": [], "entities": [{"text": "New York tribune Sunday magazine 4", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.8823430140813192}]}, {"text": "We annotated a total of 82 pages.", "labels": [], "entities": []}, {"text": "The annotation was performed by three annotators so that each page was annotated by two different annotators.", "labels": [], "entities": []}, {"text": "We annotated each segment in the OCR output, marking it either as part of an article with a unique ID, or as an advertisement.", "labels": [], "entities": [{"text": "OCR output", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.7957382798194885}]}, {"text": "The high number of short advertisements, combined with the low OCR quality due to very small and artistic typesetting, led to high disagreement on the segmentation annotations.", "labels": [], "entities": [{"text": "OCR quality", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.7990812361240387}]}, {"text": "Since our focus is on articles, we merged all advertisement blocks.", "labels": [], "entities": []}, {"text": "The resulting annotation achieves a Cohen's) kappa score of \u03ba = 0.85, (\"almost perfect\" agreement).", "labels": [], "entities": [{"text": "kappa score", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.864732950925827}, {"text": "agreement", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8902527093887329}]}, {"text": "Subsequently, we manually checked the disagreements and merged the annotations.", "labels": [], "entities": []}, {"text": "In the following experiments, we consider either all pages of one issue (BYISSUE setting), or all pages of all issues (ALLISSUES setting).", "labels": [], "entities": [{"text": "BYISSUE setting", "start_pos": 73, "end_pos": 88, "type": "METRIC", "confidence": 0.9186850190162659}, {"text": "ALLISSUES", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.9415731430053711}]}, {"text": "The BYISSUE dataset contains an average of 37 gold segments corresponding to 12.6 articles.", "labels": [], "entities": [{"text": "BYISSUE dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8275621831417084}]}, {"text": "The AL-LISSUES dataset consists of 53 different articles split among 185 gold segments -i.e., we have an average of 3 to 4 segments per article.", "labels": [], "entities": [{"text": "AL-LISSUES dataset", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7432497292757034}]}, {"text": "We remove all non-alphanumeric characters and transform similarities exponentially for clustering.", "labels": [], "entities": []}, {"text": "The fastText embeddings are trained on all 1912 English-language newspapers available from Library of Congress.", "labels": [], "entities": []}, {"text": "In the first experiment, we use our gold standard (manually annotated) segment boundaries and perform only clustering.", "labels": [], "entities": []}, {"text": "This setup reveals the performance of the clustering method.", "labels": [], "entities": []}, {"text": "The second experiment adopts a more realistic setting and evaluates clustering performance when using automatically predicted segments obtained by TextTiling.", "labels": [], "entities": [{"text": "TextTiling", "start_pos": 147, "end_pos": 157, "type": "DATASET", "confidence": 0.9449466466903687}]}, {"text": "In the first experiment, only the clustering needs to be evaluated.", "labels": [], "entities": []}, {"text": "For the evaluation, we rely on the B-cubed measure, an adaptation of the familiar IR precision/recall/F 1 measure to the clustering setup ().", "labels": [], "entities": [{"text": "B-cubed measure", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.9766020178794861}, {"text": "IR precision/recall/F 1 measure", "start_pos": 82, "end_pos": 113, "type": "METRIC", "confidence": 0.8163510784506798}]}, {"text": "In the second experiment, we additionally evaluate automatic segmentation, for which we report precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9982467889785767}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9984887838363647}]}, {"text": "Using this measure is motivated as when using automatic text segmentation as a preprocessing step, we prefer high recall, resulting in fine-grained segments.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7366655766963959}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9984762072563171}]}, {"text": "Due to the non-deterministic nature of the spectral clustering, we perform each clustering run 5 times and report averages.", "labels": [], "entities": []}, {"text": "First, we inspect the effect of computing similarity in different ways for the BYISSUE setting for 12 clusters, the average number of articles per issue (cf. Section 4).", "labels": [], "entities": [{"text": "BYISSUE setting", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.8568370044231415}]}, {"text": "The results in show that among the Jaccard-based similarities, there is an interesting tendency for relatively long n-grams to work well, with the best results for n=7.", "labels": [], "entities": []}, {"text": "Furthermore, in contrast to our intuition that the word level would suffer from OCR errors, we see better results for words than for n-grams.", "labels": [], "entities": []}, {"text": "The overall best results are achieved by Cosine similarity on fastText embeddings which can be understood as an optimized combination of word and character n-gram information.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 41, "end_pos": 58, "type": "METRIC", "confidence": 0.7078864872455597}]}, {"text": "Next, we vary the number of clusters and retain the three best-performing similarity measures.", "labels": [], "entities": []}, {"text": "(The analysis shown in is robust across numbers of clusters).", "labels": [], "entities": []}, {"text": "For the BYISSUE setting (see), we consider between 10 and 15 clusters.", "labels": [], "entities": [{"text": "BYISSUE", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.5597784519195557}]}, {"text": "We find that Precision generally increases with increased number of clusters, while Recall decreases, as could be expected.", "labels": [], "entities": [{"text": "Precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.985039234161377}, {"text": "Recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.995586633682251}]}, {"text": "The maximum F1 score of just above 68% is obtained for cluster sizes of 14 (fastText-based and 7-gram similarities) and 15 (word-based similarity).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9866393506526947}]}, {"text": "This corresponds closely to, and is a bit higher than, the average number of gold clusters in that dataset (viz., 12.6).", "labels": [], "entities": []}, {"text": "Embedding-based similarity outperforms trigrambased similarity by about 2.8 points F1.", "labels": [], "entities": []}, {"text": "In the ALLISSUES setting, we expect to see around 53 articles and thus explore performance  between 50 and 55 clusters (see).", "labels": [], "entities": []}, {"text": "The F1 scores are generally lower than for the BYISSUE setting, but still substantial.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9997758269309998}, {"text": "BYISSUE setting", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.7867979109287262}]}, {"text": "We find similar tendencies as before (Precision increasing and Recall decreasing with the number of clusters).", "labels": [], "entities": [{"text": "Precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9909380674362183}, {"text": "Recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9968143105506897}]}, {"text": "However, there is more variance than in the BYISSUE setting, so the patterns are less clear.", "labels": [], "entities": [{"text": "variance", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.950890839099884}, {"text": "BYISSUE", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.711644172668457}]}, {"text": "We achieve best performance for 7-gram-based similarity with 55 clusters, for the word-based similarity with 54 and for embedding-based similarity with 54 clusters.", "labels": [], "entities": []}, {"text": "The best performing number of clusters is again close to, and a bit higher than, the true number of articles.", "labels": [], "entities": []}, {"text": "Here, also the 7-gram Jaccard similarity performs better than using words and is essentially on par with the fastText embeddings.", "labels": [], "entities": []}, {"text": "We interpret this finding as showing that long n-gram shared between segments (e.g. person names, place names, etc.) area surprisingly good indicator of article identity, even in the face of noisy OCR output.", "labels": [], "entities": []}, {"text": "We first evaluate TextTiling, our automatic segmentation method (cf. Section 3) and find a low Precision (0.1168) but a comparatively high Recall (0.6602).", "labels": [], "entities": [{"text": "Precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9988477230072021}, {"text": "Recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9992029070854187}]}, {"text": "This means that precise segmentation of the noisy, OCRed historical texts is challenging indeed: TextTiling over-segments the texts.", "labels": [], "entities": [{"text": "OCRed historical texts", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.7183410127957662}]}, {"text": "This happens, for example, when parts of a page \"look different\" in a scan (e.g. due to folds) and OCR introduces systematically different errors.", "labels": [], "entities": []}, {"text": "We still prefer over-to under-segmentation, since over-segmented articles stand a chance of being recombined in the clustering step.", "labels": [], "entities": []}, {"text": "shows the results for article identification on automatically segmented text (we report only results for the previously best numbers of clusters).", "labels": [], "entities": [{"text": "article identification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.8477746844291687}]}, {"text": "As can be expected given the segmentation results, performance drops substantially compared to Experiment 1.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.9784179329872131}]}, {"text": "What is notable is the difference between the BYISSUE and the ALLISSUES settings: For BYISSUE, performance drops moderately from 0.68 to 0.46 F1, while for ALLISSUES we see a huge decrease from 0.55 to 0.28 F1.", "labels": [], "entities": [{"text": "BYISSUE", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.7674062252044678}, {"text": "BYISSUE", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.4819979667663574}, {"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9981963038444519}, {"text": "F1", "start_pos": 207, "end_pos": 209, "type": "METRIC", "confidence": 0.9971952438354492}]}, {"text": "Similarity behaves consistently: fastText performs best for both settings, while word-based similarity yields the lowest scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of similarity measure on clustering  performance for a fixed number of clusters of 12  (BYISSUE setting, gold standard segmentation)", "labels": [], "entities": [{"text": "BYISSUE setting", "start_pos": 105, "end_pos": 120, "type": "METRIC", "confidence": 0.8833593726158142}]}, {"text": " Table 2: Experiment 1: Article identification with gold  standard segments, BYISSUE setting", "labels": [], "entities": [{"text": "Article identification", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.8911720216274261}, {"text": "BYISSUE", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.8519989848136902}]}, {"text": " Table 5: Article identification on pages filtered by OCR  quality (Exp. 1, BYISSUE, B-Cubed F1, 14 clusters)", "labels": [], "entities": [{"text": "Article identification", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.8335398733615875}, {"text": "BYISSUE", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9556316137313843}]}]}