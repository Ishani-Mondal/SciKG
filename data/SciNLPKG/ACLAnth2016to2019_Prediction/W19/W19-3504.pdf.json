{"title": [{"text": "Racial Bias in Hate Speech and Abusive Language Detection Datasets", "labels": [], "entities": [{"text": "Racial Bias in Hate Speech and Abusive Language Detection", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.7801036437352499}]}], "abstractContent": [{"text": "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6610304713249207}]}, {"text": "We examine racial bias in five different sets of Twit-ter data annotated for hate speech and abusive language.", "labels": [], "entities": [{"text": "Twit-ter data annotated", "start_pos": 49, "end_pos": 72, "type": "DATASET", "confidence": 0.9325058062871298}]}, {"text": "We train classifiers on these datasets and compare the predictions of these classi-fiers on tweets written in African-American English with those written in Standard Amer-ican English.", "labels": [], "entities": [{"text": "Amer-ican English", "start_pos": 166, "end_pos": 183, "type": "DATASET", "confidence": 0.8969427645206451}]}, {"text": "The results show evidence of systematic racial bias in all datasets, as classi-fiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates.", "labels": [], "entities": []}, {"text": "If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users.", "labels": [], "entities": []}, {"text": "Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work has shown evidence of substantial bias in machine learning systems, which is typically a result of bias in the training data.", "labels": [], "entities": []}, {"text": "This includes both supervised) and unsupervised natural language processing systems.", "labels": [], "entities": []}, {"text": "Machine learning models are currently being deployed in the field to detect hate speech and abusive language on social media platforms including Facebook, Instagram, and Youtube.", "labels": [], "entities": [{"text": "Youtube", "start_pos": 170, "end_pos": 177, "type": "DATASET", "confidence": 0.9495715498924255}]}, {"text": "The aim of these models is to identify abusive language that directly targets certain individuals or groups, particularly people belonging to protected categories (.", "labels": [], "entities": []}, {"text": "Bias may reduce the accuracy of these models, and at worst, will mean that the models actively discriminate against the same groups they are designed to protect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9990689158439636}]}, {"text": "Our study focuses on racial bias in hate speech and abusive language detection datasets), all of which use data collected from Twitter.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.8088407317797343}]}, {"text": "We train classifiers using each of the datasets and use a corpus of tweets with demographic information to compare how each classifier performs on tweets written in African-American English (AAE) versus Standard American English (SAE).", "labels": [], "entities": []}, {"text": "We use bootstrap sampling to estimate the proportion of tweets in each group that each classifier assigns to each class.", "labels": [], "entities": []}, {"text": "We find evidence of systematic racial biases across all of the classifiers, with AAE tweets predicted as belonging to negative classes like hate speech or harassment significantly more frequently than SAE tweets.", "labels": [], "entities": []}, {"text": "In most cases the bias decreases in magnitude when we condition on particular keywords which may indicate membership in negative classes, yet it still persists.", "labels": [], "entities": []}, {"text": "We expect that these biases will result in racial discrimination if classifiers trained on any of these datasets are deployed in the field.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focus on Twitter, the most widely used data source in abusive language research.", "labels": [], "entities": [{"text": "abusive language research", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.6700368026892344}]}, {"text": "We use all available datasets where tweets are labeled as various types of abuse and are written in English.", "labels": [], "entities": []}, {"text": "We now briefly describe each of these datasets in chronological order.", "labels": [], "entities": []}, {"text": "collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.", "labels": [], "entities": []}, {"text": "They then annotated a sample of these tweets themselves, using guidelines inspired by critical race theory.", "labels": [], "entities": []}, {"text": "These annotators were then reviewed by \"a 25 year old woman studying gender studies and a nonactivist feminist\" to check for bias.", "labels": [], "entities": []}, {"text": "This dataset consists of 16,849 tweets labeled as either racism, sexism, or neither.", "labels": [], "entities": []}, {"text": "Most of the tweets categorized as sexist relate to debates over an Australian TV show and most of those considered as racist are anti-Muslim.", "labels": [], "entities": []}, {"text": "To account for potential bias in the previous dataset, Waseem (2016) relabeled 2876 tweets in the dataset, along with anew sample from the tweets originally collected.", "labels": [], "entities": []}, {"text": "The tweets were anno-tated by \"feminist and anti-racism activists\", based upon the assumption that they are domain-experts.", "labels": [], "entities": []}, {"text": "A fourth category, racism and sexism was also added to account for the presence of tweets which exhibit both types of abuse.", "labels": [], "entities": []}, {"text": "The dataset contains 6,909 tweets.", "labels": [], "entities": []}, {"text": "collected tweets containing terms from the Hatebase, 2 a crowdsourced hate speech lexicon, then had a sample coded by crowdworkers located in the United States.", "labels": [], "entities": [{"text": "Hatebase, 2 a crowdsourced hate speech lexicon", "start_pos": 43, "end_pos": 89, "type": "DATASET", "confidence": 0.9378239884972572}]}, {"text": "To avoid false positives that occurred in prior work which considered all uses of particular terms as hate speech, crowdworkers were instructed not to make their decisions based upon any words or phrases in particular, no matter how offensive, but on the overall tweet and the inferred context.", "labels": [], "entities": []}, {"text": "The dataset consists of 24,783 tweets annotated as hate speech, offensive language, or neither.", "labels": [], "entities": []}, {"text": "selected tweets using ten keywords and phrases related to anti-black racism, Islamophobia, homophobia, anti-semitism, and sexism.", "labels": [], "entities": []}, {"text": "The authors developed a coding scheme to distinguish between potentially offensive content and serious harassment, such as threats or hate speech.", "labels": [], "entities": []}, {"text": "After an initial round of coding, where tweets were assigned to a number of different categories, they simplified their analysis to include a binary harassment or non-harassment label for each tweet.", "labels": [], "entities": []}, {"text": "The dataset consists of 20,360 tweets, each hand-labeled by the authors.", "labels": [], "entities": []}, {"text": "(2018) constructed a dataset intended to better approximate a real-world setting where abuse is relatively rare.", "labels": [], "entities": []}, {"text": "They began with a random sample of tweets then augmented it by adding tweets containing one or more terms from the Hatebase lexicon and that had negative sentiment.", "labels": [], "entities": [{"text": "Hatebase lexicon", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.967028796672821}]}, {"text": "They criticized prior work for defining labels in an ad hoc manner.", "labels": [], "entities": []}, {"text": "To develop a more comprehensive annotation scheme they initially labeled a sample of tweets, allowing each tweet to belong to multiple classes.", "labels": [], "entities": []}, {"text": "After analyzing the overlap between different classes they settled on a coding scheme with four distinct classes: abusive, hateful, spam, and normal.", "labels": [], "entities": []}, {"text": "We use a dataset they published containing 91,951 tweets coded into these categories by crowdworkers.", "labels": [], "entities": []}, {"text": "2 https://hatebase.org/ 3 The paper describes 35k tweets but there were many duplicates in this dataset which were removed from the dataset the authors made available.", "labels": [], "entities": []}, {"text": "They describe 80k tweets in the paper but more tweets were added to the dataset released by the authors.", "labels": [], "entities": []}, {"text": "Some of the tweets in this dataset are duplicates: if all versions of a  We use a dataset of tweets labeled by race from to measure racial biases in these classifiers.", "labels": [], "entities": []}, {"text": "They collected geolocated tweets in the U.S. and matched them with demographic data from the Census on the population of non-Hispanic whites, non-Hispanic blacks, Hispanics, and Asians in the block group where the tweets originated.", "labels": [], "entities": []}, {"text": "They then identified words associated with particular demographics and trained a probabilistic mixed-membership language model.", "labels": [], "entities": []}, {"text": "This model learns demographicallyaligned language models for each of the four demographic categories and is used to calculate the posterior proportion of language from each category in each tweet.", "labels": [], "entities": []}, {"text": "Their validation analyses indicate that tweets with a high posterior proportion of non-Hispanic black language exhibit lexical, phonological, and syntactic variation consistent with prior research on AAE.", "labels": [], "entities": []}, {"text": "Their publiclyavailable dataset contains 59.2 million tweets.", "labels": [], "entities": []}, {"text": "We define a user as likely non-Hispanic black if the average posterior proportion across all of their tweets for the non-Hispanic black language model is \u2265 0.80 (and \u2264 0.10 Hispanic and Asian combined) and as non-Hispanic white using the same formula but for the white language model.", "labels": [], "entities": []}, {"text": "This allows us to restrict our analysis to tweets written by users who predominantly use one of the language models.", "labels": [], "entities": []}, {"text": "Due to space constraints we discard users who predominantly use either the Hispanic or the Asian language model.", "labels": [], "entities": []}, {"text": "This results in a set of 1.1m tweets written by people who generally use non-Hispanic black language and 14.5m tweets written by users who tend to use non-Hispanic white language.", "labels": [], "entities": []}, {"text": "Following Blodgett and O'Connor (2017), we call these datasets black-aligned and white-aligned tweets, reflecting the fact that they contain language associated with either demographic category but which may not all We use this threshold following Blodgett and O'Connor (2017) and after consulting with the lead author.", "labels": [], "entities": []}, {"text": "While these cut-offs should provide high confidence that the users tend to use AAE or SAE, and hence serve as a proxy for race, it is important to note that not all African-Americans use AAE and that not all AAE users are African-American, although use of the AAE dialect suggests asocial proximity to or affinity for African-American communities be produced by members of these categories.", "labels": [], "entities": [{"text": "AAE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.8825539350509644}]}, {"text": "We now describe how we use these data in our experiments.", "labels": [], "entities": []}, {"text": "We examine whether the probability that a tweet is predicted to belong to a particular class varies in relation to the racial alignment of the language it uses.", "labels": [], "entities": []}, {"text": "The null hypothesis of no racial bias is that the probability a tweet will belong to a negative class is independent of the racial group the tweet's author is a member of.", "labels": [], "entities": []}, {"text": "Formally, for class c i , where c i = 1 denotes membership in the class and c i = 0 the opposite, we aim to test H N : P (c i = 1|black) = P (c i = 1|white).", "labels": [], "entities": []}, {"text": "If P (c i = 1|black) > P (c i = 1|white) and the difference is statistically significant then we can reject the null hypothesis H N in favor of the alternative hypothesis H A that black-aligned tweets are classified into c i at a higher rate than white-aligned tweets.", "labels": [], "entities": []}, {"text": "Conversely, if P (c i = 1|black) < P (c i = 1|white) we can conclude that the classifier is more likely to classify white-aligned tweets as c i . We should expect that white-aligned tweets are more likely to use racist language or hate speech than blackaligned tweets, given that African-Americans are often targeted with racism and hate speech by whites.", "labels": [], "entities": []}, {"text": "However for some classes like sexism we have no reason to expect thereto be racial differences in either direction.", "labels": [], "entities": []}, {"text": "To test this hypothesis we use bootstrap sampling ( to estimate the proportion of tweets in each dataset that each classifier predicts to belong to each class.", "labels": [], "entities": []}, {"text": "We drawn random samples with replacement of k tweets from each of the two race corpora, where n = k = 1000.", "labels": [], "entities": []}, {"text": "For each sample we use each classifier to predict the class membership of each tweet, then store the proportion of tweets that were assigned to each class, pi . For each classifier-class pair, we thus obtain a pair of vectors, one for each corpus, each containing n sampled proportions.", "labels": [], "entities": []}, {"text": "We also conduct a second experiment, where we assess whether there is racial bias conditional upon a tweet containing a keyword likely to be associated with a negative class.", "labels": [], "entities": []}, {"text": "While differences in language will undoubtedly remain, this should help to account for the possibility that results in Experiment 1 are driven by differences in the true distribution of the different classes of interest, or of words associated with these classes, in the two corpora.", "labels": [], "entities": []}, {"text": "For classifier c and category i, we evaluate H N : P (c i = 1|black, t) = P (c i = 1|white, t) fora given term t.", "labels": [], "entities": []}, {"text": "We conduct this experiment for two different terms, each of which occurs frequently enough in the data to enable our bootstrapping approach.", "labels": [], "entities": []}, {"text": "We select the term \"n*gga\", since it is a particularly prevalent source of false positives for hate speech detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7944524089495341}]}, {"text": "In this case, we expect that tweets containing the word should be classified as more negative when used by whites, thus H A 1 : P (c i = 1|black, t) < P (c i = 1|white, t).", "labels": [], "entities": []}, {"text": "The other alternative, H A 2 : P (c i = 1|black, t) > P (c i = 1|white, t) would indicate that blackaligned tweets containing the term are penalized at a higher rate than comparable white-aligned tweets.", "labels": [], "entities": []}, {"text": "We also assess the results for the word \"b*tch\" since it is a widely used sexist term, which is often also used casually, but we have no theoretical reason to expect thereto be racial dif- We also planned to conduct the same analysis using the \"-er\" suffix, however the sample was too small, with the word being used in 555 tweets in the white-aligned corpus (0.004%) and 61 in the black-aligned corpus (0.005%).", "labels": [], "entities": []}, {"text": "The term \"n*gga\" was used in around 2.25% of black-aligned and 0.15% of white-aligned tweets.", "labels": [], "entities": []}, {"text": "The term \"b*tch\" was used in 1.7% of black-aligned and 0.5% of whitealigned tweets.", "labels": [], "entities": []}, {"text": "The substantial differences in the distributions for these two terms alone are consistent with our intuition that some of the results in Experiment 1 maybe driven by differences in the frequencies of words associated with negative classes in the training datasets.", "labels": [], "entities": []}, {"text": "Since we are using a subsample of the available data, we use smaller bootstrap samples, drawing k = 100 tweets each time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Experiment 2, t = \"n*gga\"", "labels": [], "entities": []}, {"text": " Table 4: Experiment 2, t = \"b*tch\"", "labels": [], "entities": []}]}