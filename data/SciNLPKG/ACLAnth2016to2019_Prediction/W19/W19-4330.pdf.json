{"title": [{"text": "Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model", "labels": [], "entities": [{"text": "Cross-Lingual Sentence Representations", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.6216207245985667}]}], "abstractContent": [{"text": "The scarcity of labeled training data across many languages is a significant roadblock for multilingual neural language processing.", "labels": [], "entities": [{"text": "multilingual neural language processing", "start_pos": 91, "end_pos": 130, "type": "TASK", "confidence": 0.670168362557888}]}, {"text": "We approach the lack of in-language training data using sentence embeddings that map text written in different languages , but with similar meanings, to nearby embedding space representations.", "labels": [], "entities": []}, {"text": "The representations are produced using a dual-encoder based model trained to maximize the representational similarity between sentence pairs drawn from parallel data.", "labels": [], "entities": []}, {"text": "The representations are enhanced using multitask training and unsupervised monolingual corpora.", "labels": [], "entities": []}, {"text": "The effectiveness of our multilingual sentence embeddings are assessed on a comprehensive collection of monolingual, cross-lingual, and zero-shot/few-shot learning tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence embeddings are broadly useful fora diverse collection of downstream natural language processing tasks.", "labels": [], "entities": [{"text": "Sentence embeddings", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.865315854549408}]}, {"text": "Sentence embeddings evaluated on downstream tasks in prior work have been trained on monolingual data, preventing them from being used for cross-lingual transfer learning.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 139, "end_pos": 170, "type": "TASK", "confidence": 0.8188582261403402}]}, {"text": "However, recent work on learning multilingual sentence embeddings has produced representations that capture semantic similarity even when sentences are written in different languages (.", "labels": [], "entities": []}, {"text": "We explore multi-task extensions of multilingual models for cross-lingual transfer learning.", "labels": [], "entities": [{"text": "cross-lingual transfer learning", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.8136617739995321}]}, {"text": "* equal contribution We present a novel approach for cross-lingual representation learning that combines methods for multi-task learning of monolingual sentence representations () with recent work on dual encoder methods for obtaining multilingual sentence representations for bi-text retrieval (.", "labels": [], "entities": [{"text": "cross-lingual representation learning", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.797095517317454}, {"text": "bi-text retrieval", "start_pos": 277, "end_pos": 294, "type": "TASK", "confidence": 0.7061455100774765}]}, {"text": "By doing so, we learn representations that maintain strong performance on the original monolingual language tasks, while simultaneously obtaining good performance using zeroshot learning on the same task in another language.", "labels": [], "entities": []}, {"text": "For a given language pair, we construct a multitask training scheme using native source language tasks, native target language tasks, and a bridging translation task to encourage sentences with identical meanings, but written in different languages, to have similar embeddings.", "labels": [], "entities": []}, {"text": "We evaluate the learned representations on several monolingual and cross-lingual tasks, and provide a graph-based analysis of the learned representations.", "labels": [], "entities": []}, {"text": "Multi-task training using additional monolingual tasks is found to improve performance over models that only make use of parallel data on both cross-lingual semantic textual similarity (STS)) and cross-lingual eigen-similarity ().", "labels": [], "entities": []}, {"text": "For European languages, the results show that the addition of monolingual data improves the embedding alignment of sentences and their translations.", "labels": [], "entities": []}, {"text": "Further, we find that cross-lingual training with additional monolingual data leads to far better crosslingual transfer learning performance.", "labels": [], "entities": []}, {"text": "1 DOT DOT: Multi-task dual-encoder model with native tasks and a bridging translation task.", "labels": [], "entities": []}, {"text": "The terms PAR, INP, RES refer to parent, input, and response respectively.", "labels": [], "entities": [{"text": "PAR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9814411401748657}, {"text": "INP", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.8903083205223083}, {"text": "RES", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9781724214553833}]}, {"text": "ENC refers to the shared encoder g, FC refers to fully connected layers, and DOT refers to dot product.", "labels": [], "entities": [{"text": "FC", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9141161441802979}]}, {"text": "Finally, FEATURE TRANSFORM refers to the feature vector used for natural language inference.", "labels": [], "entities": [{"text": "FEATURE", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9942547678947449}, {"text": "TRANSFORM", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.6422027945518494}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance on classification transfer tasks from SentEval (", "labels": [], "entities": [{"text": "classification transfer tasks", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.8269786238670349}]}, {"text": " Table 2: Pearson's correlation coefficients on STS Benchmark (dev / test). The first column shows the  results on the original STS Benchmark data in English. French, Spanish", "labels": [], "entities": [{"text": "STS Benchmark", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9114191234111786}, {"text": "STS Benchmark data", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.9324561357498169}]}, {"text": " Table 3: Pearson's r on track 3 (es-es) and track  4(a) (en-es) of the SemEval-2017 STS shared task.", "labels": [], "entities": [{"text": "SemEval-2017 STS shared task", "start_pos": 72, "end_pos": 100, "type": "TASK", "confidence": 0.5738028734922409}]}, {"text": " Table 4: Zero-shot classification accuracy (%) on SNLI-X and XNLI datasets. Cross-lingual transfer  models are training on English only NLI data and then evaluated on French (fr), Spanish (es), German  (de) and Chinese (zh) evaluation sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9720654487609863}, {"text": "SNLI-X", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.8401334285736084}, {"text": "XNLI datasets", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.8221097588539124}, {"text": "Cross-lingual transfer", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.7536274492740631}]}, {"text": " Table 5: Zero-shot sentiment classification accu- racy(%) on non-English Amazon review test data  after training on English only Amazon reviews.", "labels": [], "entities": [{"text": "Zero-shot sentiment classification", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6512078742186228}, {"text": "accu- racy", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.8917543689409891}, {"text": "Amazon review test data", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.7194177433848381}]}, {"text": " Table 6: Sentiment classification accuracy(%) on target language Amazon review test data after training  on English Amazon review data and a portion of French of German data. The second row shows the  percent of French (fr) or German (de) data is used for training in each model.", "labels": [], "entities": [{"text": "Sentiment classification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7936822474002838}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8855544924736023}, {"text": "Amazon review test data", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8882238417863846}, {"text": "Amazon review data", "start_pos": 117, "end_pos": 135, "type": "DATASET", "confidence": 0.9072160522143046}]}]}