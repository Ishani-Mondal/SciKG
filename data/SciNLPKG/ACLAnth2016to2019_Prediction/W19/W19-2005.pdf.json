{"title": [{"text": "Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones.", "labels": [], "entities": [{"text": "multilingual neural translation", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6569403310616811}]}, {"text": "We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language.", "labels": [], "entities": []}, {"text": "The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task.", "labels": [], "entities": []}, {"text": "In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English.", "labels": [], "entities": []}, {"text": "The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation.", "labels": [], "entities": []}, {"text": "This is further supported by a study on paraphrase generation that we also include at the end of the paper.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.9647728502750397}]}], "introductionContent": [{"text": "An appealing property of encoder-decoder models for machine translation is the effect of compressing information into dense vector-based representations to map source language input onto adequate translations in the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7672128081321716}]}, {"text": "However, it is not clear to what extent the model actually needs to model meaning to perform that task; especially for related languages, it is often not necessary to acquire a deep understanding of the input to translate in an adequate way.", "labels": [], "entities": []}, {"text": "The intuition that we would like to explore in this paper is based on the assumption that an increasingly difficult training objective will enforce stronger abstractions.", "labels": [], "entities": []}, {"text": "In particular, we would like to see whether multilingual machine translation models learn representations that are closer to languageindependent meaning representations than bilingual models do.", "labels": [], "entities": [{"text": "multilingual machine translation", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6301415761311849}]}, {"text": "Hence, our hypothesis is that representations learned from multilingual data sets covering a larger linguistic diversity better reflect semantics than representations learned from less diverse material.", "labels": [], "entities": []}, {"text": "This hypothesis is supported by the findings of related work focusing on universal sentence representation learning from multilingual data to be used in natural language inference or other downstream tasks.", "labels": [], "entities": [{"text": "universal sentence representation learning", "start_pos": 73, "end_pos": 115, "type": "TASK", "confidence": 0.6826734840869904}]}, {"text": "In contrast to related work, we are not interested in fixed-size sentence representations that can be fed into external classifiers or regression models.", "labels": [], "entities": []}, {"text": "Instead, we would like to fully explore the use of the encoded information in the attentive recurrent layers as they are produced by the seq2seq model.", "labels": [], "entities": []}, {"text": "Our basic framework consists of a standard attentional sequence-to-sequence model as commonly used for neural machine translation, with the multilingual extension proposed by.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.7354519168535868}]}, {"text": "This extension allows a single system to learn machine translation for several language pairs, and crucially also for language pairs that have not been seen during training.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6964443922042847}]}, {"text": "We use Bible translations for training, in order to keep the genre and content of training data constant across languages, and to enable further studies on increasing levels of linguistic diversity.", "labels": [], "entities": []}, {"text": "We propose different setups, all of which share the characteristics of having some source data in English and some target data in English.", "labels": [], "entities": []}, {"text": "We can then evaluate these models on their capacity of recognizing and generating English paraphrases, i.e. translating English to English without explicitly learning that task.", "labels": [], "entities": []}, {"text": "Starting with abase model using French-English and English-French training data, we select 16 additional languages as auxiliary information that are added to the base model, each of them separately.", "labels": [], "entities": []}, {"text": "There is a large body of related work on paraphrase generation using machine translation () based on parallel monolingual corpora (, pivot-based translation) and paraphrase databased extracted from parallel corpora (.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.9565869569778442}]}, {"text": "Related work on multilingual sentence representation) has focused on fixedsize vector representations that can be used in natural language inference () or other downstream tasks such as bitext mining  or (cross-lingual) document classification ().", "labels": [], "entities": [{"text": "multilingual sentence representation", "start_pos": 16, "end_pos": 52, "type": "TASK", "confidence": 0.6213106413682302}, {"text": "bitext mining", "start_pos": 186, "end_pos": 199, "type": "TASK", "confidence": 0.6484928131103516}, {"text": "cross-lingual) document classification", "start_pos": 205, "end_pos": 243, "type": "TASK", "confidence": 0.6085718795657158}]}], "datasetContent": [{"text": "For our experiments, we apply a standard attentional sequence-to-sequence model with BPEbased segmentation.", "labels": [], "entities": [{"text": "BPEbased segmentation", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.5899744629859924}]}, {"text": "We use the Nematus-style models ( ) as implemented in MarianNMT ().", "labels": [], "entities": [{"text": "MarianNMT", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.960943341255188}]}, {"text": "These models apply gated recurrent units (GRUs) in the encoder and decoder with a bi-directional RNN on the encoder side.", "labels": [], "entities": []}, {"text": "The word embeddings have a dimensionality of 512 and the RNN dimensionality is set to 1,024.", "labels": [], "entities": []}, {"text": "We enable layer normalization and we use one RNN layer in both, encoder and decoder.", "labels": [], "entities": [{"text": "layer normalization", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7431750595569611}]}, {"text": "In training we use dynamic mini-batches to automatically fit the allocated memory (3GB in our case) based on sentence length in the selected sample of data.", "labels": [], "entities": []}, {"text": "The optimization procedure applies Adam () with mean crossentropy as the optimization criterion.", "labels": [], "entities": []}, {"text": "We also enable length normalization, exponential smoothing, scaling dropout for the RNN layers with ratio 0.2 and also apply source and target word dropout with ratio 0.1.", "labels": [], "entities": []}, {"text": "All of these values are recommended settings that have empirically been found in the related literature.", "labels": [], "entities": []}, {"text": "For testing convergence, we use independent development data of roughly 1,000 test examples and BLEU scores to determine the stopping criterion, which is set to five subsequent failures of improving the validation score.", "labels": [], "entities": [{"text": "testing convergence", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6960717737674713}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.999224066734314}, {"text": "stopping criterion", "start_pos": 125, "end_pos": 143, "type": "METRIC", "confidence": 0.9694545567035675}]}, {"text": "The translations are done with abeam search decoder of size 12.", "labels": [], "entities": [{"text": "translations", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9601354002952576}]}, {"text": "The validation frequency is set to run each 2,500 mini-batches.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9496756196022034}]}, {"text": "For the multilingual setup, we follow Johnson) is used to avoid unknown words and to improve generalisations.", "labels": [], "entities": []}, {"text": "Note that in our setup we need to ensure that subword-level segmentations are consistent for each language involved in several translation tasks.", "labels": [], "entities": []}, {"text": "We opted for languagedependent BPE models with 10,000 merge operations for each code table.", "labels": [], "entities": []}, {"text": "The total vocabulary size then depends on the combination of languages that we use in training but the vocabulary stays exactly the same for each language involved in all experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the Bible data in our collec- tion: number of individual Bible translations, number  of verses and number of tokens per language in the  training data sets.", "labels": [], "entities": []}]}