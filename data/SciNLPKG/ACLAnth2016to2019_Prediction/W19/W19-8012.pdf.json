{"title": [{"text": "Recursive LSTM Tree Representation for Arc-Standard Transition-Based Dependency Parsing", "labels": [], "entities": [{"text": "Recursive LSTM Tree Representation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6461884677410126}, {"text": "Arc-Standard Transition-Based Dependency Parsing", "start_pos": 39, "end_pos": 87, "type": "TASK", "confidence": 0.5550879463553429}]}], "abstractContent": [{"text": "We propose a method to represent dependency trees as dense vectors through the recursive application of Long Short-Term Memory networks to build Recursive LSTM Trees (RLTs).", "labels": [], "entities": []}, {"text": "We show that the dense vectors produced by Recursive LSTM Trees replace the need for structural features by using them as feature vectors fora greedy Arc-Standard transition-based dependency parser.", "labels": [], "entities": []}, {"text": "We also show that RLTs have the ability to incorporate useful information from the bi-LSTM contextualized representation used by Cross and Huang (2016) and Kiperwasser and Goldberg (2016b).", "labels": [], "entities": []}, {"text": "The resulting dense vectors are able to express both structural information relating to the dependency tree, as well as sequential information relating to the position in the sentence.", "labels": [], "entities": []}, {"text": "The resulting parser only requires the vector representations of the top two items on the parser stack, which is, to the best of our knowledge, the smallest feature set ever published for Arc-Standard parsers to date, while still managing to achieve competitive results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural network-based dependency parsers have typically relied on combination of raw features, as represented by their dense vector embeddings to represent features of a sentence as well as the parser state).", "labels": [], "entities": [{"text": "Neural network-based dependency parsers", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7465466856956482}]}, {"text": "On the other hand, there has been substantial work on using innovative deep learning architectures to build more informative feature representations.", "labels": [], "entities": []}, {"text": "One approach has been to model an input sentence using bi-directional Long Short-Term Memory Networks (bi-lstms)).", "labels": [], "entities": []}, {"text": "The result is a vector for each word that encodes both its information, and relevant information from other parts of the sentence.", "labels": [], "entities": []}, {"text": "This approach enabled better results with fewer features than was possible before.", "labels": [], "entities": []}, {"text": "Another approach has been to represent the dependency tree itself with some form of recursive network, either bottom-up (, or topdown (.", "labels": [], "entities": []}, {"text": "In this paper we propose anew method of recursively modelling dependency trees using LSTMs, which we call Recursive Tree LSTMs.", "labels": [], "entities": []}, {"text": "Our experiments show that this method of representation is very powerful, and can even be used as an additional layer of encoding over bi-lstm feature representation, which results in a more informative model.", "labels": [], "entities": []}, {"text": "The final parser is capable of achieving competitive results with a feature set consisting of only the top two items on the stack, which is the smallest feature set for an Arc-Standard dependency parser used successfully to date.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our initial set of experiments we trained models that used the top 4 RLTs on the stack, and the front 4 on the buffer as input features to the feed forward hidden layer.", "labels": [], "entities": []}, {"text": "We compare our results initially to those of, who used Stack-LSTMs, and, who used Hierarchical Tree-LSTMs, since they are the closest in the literature to our approach.", "labels": [], "entities": []}, {"text": "We make a more complete comparison with state of the art Transition-based parsers in table 3.", "labels": [], "entities": []}, {"text": "Recursive representation was used by to represent elements on the stack, similar to our approach.", "labels": [], "entities": []}, {"text": "However, their representation is computed through the recursive application of a feedforward composition function that encodes a (head, relation, dependent) tuple, encoding children in the order in which they are reduced.", "labels": [], "entities": []}, {"text": "Kiperwasser and Goldberg (2016a) uses a bottom up recursive approach to build a tree representation as well, but separates the sequence of children into a left and aright sequence, with the head itself being the start of both sequences, and the final representation of the subtree being a concatenation of the output of both sequences.", "labels": [], "entities": []}, {"text": "As in our work, use bi-LSTM vectors to represent words being input to the encoding LSTM.", "labels": [], "entities": []}, {"text": "When setting h v to be the concatenation of the word and pos vectors, the resulting accuracy score, shown in table 1, approaches the performance of and.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994971752166748}]}, {"text": "Using bi-lstm contextualized representation ash v , however, significantly improves accuracy to 94.26/92.01 on the development set and beating both of our baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992942810058594}]}, {"text": "Our second set of experiments were to investigate whether or not RLTs retain the properties of the bilstm representation in addition to its own, i.e., produce an h \u03c4 that can represent a token's special position in a sentence in addition to representing it as the head of its own subtree.", "labels": [], "entities": []}, {"text": "The results shown thus far are the results of a wide feature set, the first 4 items on both structures {s 0\u22123 , b 0\u22123 }, which is comparable to earlier feature sets used by and, but without the need for structural features, such as left-most and right-most dependents which are already encoded in the way a tree vector is produced.", "labels": [], "entities": []}, {"text": "The results in show the performance of our RLT models on increasingly small feature sets.", "labels": [], "entities": []}, {"text": "This second set of experiments show that RLTs are also able to represent contextual information about the node from the bi-lstm layer in addition to its own structural information.", "labels": [], "entities": []}, {"text": "Interestingly the drop in the accuracy of RLTs with the complete removal of buffer features is limited.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994576573371887}, {"text": "RLTs", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.4575888216495514}]}, {"text": "Our minimal feature set here consists of only the top 2 items on the stack {s 0,1 }.", "labels": [], "entities": []}, {"text": "These 2 elements represent the fundamental task of an Arc-Standard parser, which is to decide whether or not these 2 words are related, and so are not themselves contextual features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development set scores on WSJ (SD)  comparing between h v being a concatenation of  the tokens word/pos vectors and h v being a con- catenation of contextualized vectors.", "labels": [], "entities": [{"text": "WSJ (SD)", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.7450617402791977}]}, {"text": " Table 2: Development set scores for different fea- ture sets, using a bi-lstm contextualized vector as  h v , for Forward and Bi-directional encoding.", "labels": [], "entities": []}, {"text": " Table 3: Test set scores on WSJ (SD) for some of the highest scoring Transition-based Dependency  Parsers in current literature. Contextualized vectors refer to the bi-lstm vector representation used for h v ,  and word/pos embeddings refers to the concatenation of these vectors to represent h v . 8 feats. refers to  the use of the top 4 items on the stack and buffer, 2 feats. refers to the use of the top 2 items on the stack.", "labels": [], "entities": [{"text": "WSJ (SD)", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.7477087080478668}]}, {"text": " Table 4: Test set scores on 7 corpuses from the CONLL'18 shared task. These sets use Universal  Dependencies, and use an F1 score calculation for UAS and LAS that includes punctuation.", "labels": [], "entities": [{"text": "CONLL'18 shared task", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.8939688404401144}, {"text": "F1 score calculation", "start_pos": 122, "end_pos": 142, "type": "METRIC", "confidence": 0.9702723423639933}]}]}