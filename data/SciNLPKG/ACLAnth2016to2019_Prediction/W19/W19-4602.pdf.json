{"title": [{"text": "Morphology-Aware Word-Segmentation in Dialectal Arabic Adaptation of Neural Machine Translation", "labels": [], "entities": [{"text": "Dialectal Arabic Adaptation of Neural Machine Translation", "start_pos": 38, "end_pos": 95, "type": "TASK", "confidence": 0.7528115170342582}]}], "abstractContent": [{"text": "Parallel corpora available for building machine translation (MT) models for dialectal Arabic (DA) are rather limited.", "labels": [], "entities": [{"text": "building machine translation (MT)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8202220300833384}, {"text": "dialectal Arabic (DA)", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.6509048819541932}]}, {"text": "The scarcity of resources has prompted the use of Modern Standard Arabic (MSA) abundant resources to complement the limited dialectal resource.", "labels": [], "entities": []}, {"text": "However, clitics often differ between MSA and DA.", "labels": [], "entities": [{"text": "DA", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.8635718822479248}]}, {"text": "This paper compares morphology-aware DA word segmentation to other word segmentation approaches like Byte Pair Encoding (BPE) and Sub-word Regularization (SR).", "labels": [], "entities": [{"text": "DA word segmentation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8068450887997946}, {"text": "Sub-word Regularization (SR)", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.8375056385993958}]}, {"text": "A set of experiments conducted on Egyp-tian Arabic (EA), Levantine Arabic (LA), and Gulf Arabic (GA) show that a sufficiently accurate morphology-aware segmentation used in conjunction with BPE or SR outperforms the other word segmentation approaches.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 222, "end_pos": 239, "type": "TASK", "confidence": 0.7454074919223785}]}], "introductionContent": [{"text": "Building machine translation models for resource constrained languages can benefit from parallel corpora available in related languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7343061864376068}]}, {"text": "Vocabulary adaptation has been used to train statistical and neural machine translation models for Azeri, a resource constrained language, leveraging its similarity to Turkish.", "labels": [], "entities": [{"text": "Vocabulary adaptation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7734255194664001}, {"text": "statistical and neural machine translation", "start_pos": 45, "end_pos": 87, "type": "TASK", "confidence": 0.7376563072204589}]}, {"text": "Projection to a universal representation language () generates high quality machine translation model fora resource constrained language given a set of related resource-rich languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7377746999263763}]}, {"text": "Research in dialectical Arabic translation tried to leverage the resources available in Modern Standard Arabic (MSA) using several techniques.", "labels": [], "entities": [{"text": "dialectical Arabic translation", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.7173321644465128}, {"text": "Modern Standard Arabic (MSA)", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.7165400882562002}]}, {"text": "Starting with statistical and rule-based methods for transforming DA to MSA, and evolving to generating DA data from MSA parallel data using semantic projections (), and multi-task learning of part-of-speech tagging and machine translation to guide the translation model towards leveraging the grammatical roles in translation (.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 193, "end_pos": 215, "type": "TASK", "confidence": 0.7030707895755768}, {"text": "machine translation", "start_pos": 220, "end_pos": 239, "type": "TASK", "confidence": 0.7166570127010345}]}, {"text": "While earlier statistical and rule-based cross-dialectical techniques managed to leverage morphological word segmentation, more recent attempts have largely abandoned morphological segmentation in favor of language agnostic segmentation techniques like Byte Pair Encoding (BPE) ( and Sub-word Regularization (SR).", "labels": [], "entities": [{"text": "Sub-word Regularization (SR)", "start_pos": 284, "end_pos": 312, "type": "TASK", "confidence": 0.8259156107902527}]}, {"text": "In fact, these learned language agnostic word segmentation have proved that they can rival morphological segmentation in neural MT.", "labels": [], "entities": [{"text": "language agnostic word segmentation", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.7201844155788422}]}, {"text": "Ina translation task from language D to language E, if language D (say an Arabic dialect) and language A (say modern standard Arabic) are two closely related languages such that a word WA in language A is semantically equivalent to a word W D in language D.", "labels": [], "entities": []}, {"text": "Moreover, we assume that these two words share a common stem but have different clitics.", "labels": [], "entities": []}, {"text": "So, the two words can be morphologically segmented as follows: WA = PA RS A , and W D = PD RS D where PA is a sequence of zero or more characters forming the prefix of WA . Similarly, SA is a sequence of characters forming the suffix of WA , while PD and SD denote the prefix and suffix of W D , and R is the shared root or stem.", "labels": [], "entities": []}, {"text": "Due to the limited training data for the language pair {D, E}, the root R is one that we hope to learn from the abundant data for the pair {A, E}.", "labels": [], "entities": []}, {"text": "Intuitively, a morphology-aware word segmentation is more likely to produce the correct prefixes and suffixes, making it easier to learn the translation of R to E.", "labels": [], "entities": [{"text": "morphology-aware word segmentation", "start_pos": 15, "end_pos": 49, "type": "TASK", "confidence": 0.740225354830424}, {"text": "translation of R to E", "start_pos": 141, "end_pos": 162, "type": "TASK", "confidence": 0.8029791355133057}]}, {"text": "As clitics tend to occur frequently, the MT model would have learned their translation from the scarce resources for the pair {D, E}; thus, successfully translation an out-of-vocabulary word for the {D, E} pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.8229858875274658}]}, {"text": "For illustration consider the example in Table1 below.", "labels": [], "entities": []}, {"text": "The dialectal Egyptian word \"\u202b\"\u0647\u064a\u0642\u0648\u0644\u0648\u0627\u202c [hayqwlwA] is segmented into four segments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Several experiments were conducted to examine the impact of the dialectical segmenter on the quality of the MT system built with it fora resource constrained languages, and how it compares to other segmentation techniques like BPE and SR.", "labels": [], "entities": [{"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9684510827064514}]}, {"text": "The experiments were carried out using Marian v1.7.6 (Junczys-Dowmunt et al., 2018) a public neural machine translation framework which supports sentence piece tokenization with its two variant BPE and SR (unigram language model) as well as word tokenization which is basically tokenizing the corpus on white spaces.", "labels": [], "entities": [{"text": "public neural machine translation", "start_pos": 86, "end_pos": 119, "type": "TASK", "confidence": 0.5949859991669655}, {"text": "sentence piece tokenization", "start_pos": 145, "end_pos": 172, "type": "TASK", "confidence": 0.615556538105011}, {"text": "BPE", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.8930732011795044}, {"text": "word tokenization", "start_pos": 241, "end_pos": 258, "type": "TASK", "confidence": 0.8150611817836761}]}, {"text": "Most parameters of Marian were the same as the defaults except for the validation set settings which were adapted to each dialect according to the size of its data.", "labels": [], "entities": []}, {"text": "shows the distribution of the training and test data sizes used in the experiments.", "labels": [], "entities": []}, {"text": "For the Gulf and Levantine dialects, 2000 sentences are set aside and equally divided into validation and test.", "labels": [], "entities": []}, {"text": "For Egyptian, the callhome validation  and test split is used after disfluency removal.", "labels": [], "entities": []}, {"text": "The disfluency removal consists of removing incomplete words, filler words, and repeated words.", "labels": [], "entities": [{"text": "disfluency removal", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.786402016878128}]}, {"text": "This processing is necessary because we started with the speech transcripts (LDC97T19, LDC2002T38) which have full verbatim transcripts of the corresponding speech corpora.", "labels": [], "entities": []}, {"text": "As described in Section 3, the base model training merges Arabic dialect sentences and MSA.", "labels": [], "entities": []}, {"text": "Therefore, special care was needed to train the MT system for the Gulf dialect because it has far fewer sentences than MSA we needed to duplicate the Gulf data 10 times in order to make the sizes of the data of the Gulf dialect and other dialects comparable.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9707592129707336}, {"text": "MSA", "start_pos": 119, "end_pos": 122, "type": "DATASET", "confidence": 0.9044281244277954}]}, {"text": "The adaptation uses the dialect data only to fine-tune the base model trained for that dialect at a lower learning rate.", "labels": [], "entities": []}, {"text": "As summarized in, for each dialect, we evaluated five word segmentation approaches: 1.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.7314722239971161}]}, {"text": "The dialectal segmenter as the only segmenter.", "labels": [], "entities": []}, {"text": "2. Byte-Pair Encoding (BPE) as the only segmenter.", "labels": [], "entities": [{"text": "Byte-Pair Encoding (BPE)", "start_pos": 3, "end_pos": 27, "type": "METRIC", "confidence": 0.7545276165008545}]}, {"text": "3. Subword Regularization (SR) as the segmenter.", "labels": [], "entities": [{"text": "Subword Regularization (SR)", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.8088048696517944}]}, {"text": "4. Byte-Pair Encoding applied to dialectically segmented corpora.", "labels": [], "entities": [{"text": "Byte-Pair Encoding", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.648881733417511}]}, {"text": "5. Subword Regularization applied to dialectically segmented copora.", "labels": [], "entities": [{"text": "Subword Regularization", "start_pos": 3, "end_pos": 25, "type": "TASK", "confidence": 0.8834521472454071}]}, {"text": "In all cases, the vocab was kept at 40 K subwords.", "labels": [], "entities": []}, {"text": "For the base models in all three dialects, the best performing word segmentation combined dialectal segmentation with either BPE or SR.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.6962033361196518}, {"text": "dialectal segmentation", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6796204447746277}, {"text": "BPE", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9017996788024902}]}, {"text": "This continued to be the case after adaptation.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9679816961288452}]}, {"text": "The gain attributable to dialectal segmentation 1 was 0.28 BLEU point for Gulf, 1.27 for Levantine, and 0.44 for Egyptian.", "labels": [], "entities": [{"text": "dialectal segmentation", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7259573042392731}, {"text": "BLEU point", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9825215935707092}]}, {"text": "It also worth noting that Subword Regularization has consistently outperformed BPE alone.", "labels": [], "entities": [{"text": "Subword Regularization", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.9497865438461304}, {"text": "BPE", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.3947499096393585}]}, {"text": "The low scores for the Gulf dialect are due to the small size of the test set and the use of a highly dialectal spelling in the data that limited the model's ability to benefit from the MSA training.", "labels": [], "entities": []}, {"text": "While Levantine and Egyptian training data are comparable in size, the BLEU scores reported for Egyptian are based on 4 reference translations, while Levantine scores use a single reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9984784722328186}]}, {"text": "To assess the similarity of word segmentation obtained by the various approach, we computed the Levenshtein edit distances between the segmented sentences fora random subset of 150 dialectal Arabic sentences.", "labels": [], "entities": [{"text": "similarity", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9717492461204529}, {"text": "word segmentation", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.698300763964653}, {"text": "Levenshtein edit distances", "start_pos": 96, "end_pos": 122, "type": "METRIC", "confidence": 0.601399689912796}]}, {"text": "In this set, no two segmentation techniques produced the same word segmentation for all the words in any sentence.", "labels": [], "entities": []}, {"text": "However, applying SR or BPE to a dialectically segmented sentences gives very similar segmentations with an average edit distance of 2.55 per sentence.", "labels": [], "entities": [{"text": "SR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9848132729530334}, {"text": "BPE", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9239452481269836}]}, {"text": "The segmentations obtained by BPE and SR were also relatively similar with an average edit distance of 5.03.", "labels": [], "entities": [{"text": "segmentations", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9712815284729004}, {"text": "BPE", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8058290481567383}, {"text": "SR", "start_pos": 38, "end_pos": 40, "type": "DATASET", "confidence": 0.6894702315330505}, {"text": "edit distance", "start_pos": 86, "end_pos": 99, "type": "METRIC", "confidence": 0.9673772156238556}]}, {"text": "summarizes the average number of edits necessary to map a segmented sentence using one approach to the others.", "labels": [], "entities": []}, {"text": "In the table, DS is the dialectal segmenter.", "labels": [], "entities": []}, {"text": "The relatively large number of edits between the dialectal segmenter and both BPE and SR suggest that these language agnostic approaches have not fully captured the morphological aspects of Arabic dialects.", "labels": [], "entities": [{"text": "BPE", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.6865482926368713}]}, {"text": "1 Calculated as BLEU difference between the best adapted model with dialectal segmentation and the best adapted model without dialectal segmentation", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9996069073677063}]}], "tableCaptions": [{"text": " Table 2: Accuracy of the retrained unified dialectal seg- menter compared to the baseline model (Samih et al.,  2017).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.988430380821228}]}, {"text": " Table 4: The word segmentation technique, base model  BLEU score, and adapted model BLEU score for each  of the three dialects.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7376482486724854}, {"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9139162600040436}, {"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9319728314876556}]}]}