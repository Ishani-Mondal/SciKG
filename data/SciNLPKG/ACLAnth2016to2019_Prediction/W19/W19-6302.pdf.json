{"title": [{"text": "Automatic Generation and Semantic Grading of Esperanto Sentences in a Teaching Context", "labels": [], "entities": [{"text": "Automatic Generation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6375702917575836}, {"text": "Semantic Grading of Esperanto Sentences in a Teaching Context", "start_pos": 25, "end_pos": 86, "type": "TASK", "confidence": 0.7714450591140323}]}], "abstractContent": [{"text": "This paper presents a method for the automatic generation and semantic evaluation of exercise sentences for Esperanto teaching.", "labels": [], "entities": [{"text": "automatic generation and semantic evaluation of exercise sentences", "start_pos": 37, "end_pos": 103, "type": "TASK", "confidence": 0.7552021220326424}]}, {"text": "Our sentence grader exploits both corpus data and lexical resources (verb frames and noun/adjective ontologies) to either generate meaningful sentences from scratch, or to determine the acceptability of a given input sentence.", "labels": [], "entities": []}, {"text": "Individual words receive scores for how well they match the semantic conditions projected onto their place in the syntactic tree.", "labels": [], "entities": []}, {"text": "Ina CALL context, the system works with a lesson-/level-constrained vocabulary and can be integrated into e.g. substitution table or slot filler exercises.", "labels": [], "entities": []}, {"text": "While the method as such is language-independent, we also discuss how morphological clues (affixes) can be exploited for semantic purposes.", "labels": [], "entities": []}, {"text": "When evaluated on out-of-corpus course materials and short stories, the system achieved a rejection precision, in terms of false positives, of 98-99% at the sentence level, and 93-97% at the word level.", "labels": [], "entities": [{"text": "rejection precision", "start_pos": 90, "end_pos": 109, "type": "METRIC", "confidence": 0.6762259602546692}]}], "introductionContent": [{"text": "Automated writing evaluation (AWE) can be a cost-efficient and consistent, albeit controversial, alternative to human grading of L2 student production.", "labels": [], "entities": [{"text": "Automated writing evaluation (AWE)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5871787965297699}]}, {"text": "One possible approach is to focus on learner error detection in terms of spelling and grammar at the sentence level (e.g.), a task for which a wide range of tools and methods is available, covering both rule-based, machine learning (ML) and hybrid approaches (e.g.).", "labels": [], "entities": [{"text": "learner error detection", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.6530231535434723}]}, {"text": "Semantic assessment is usually seen as having a wider scope than the individual sentence, and is mainly used to address properties of a text as a whole.", "labels": [], "entities": [{"text": "Semantic assessment", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8842166364192963}]}, {"text": "In, for instance, machine learning (ML) techniques based on word embeddings are employed to assess textual coherence through the semantic similarity of adjacent sentences.", "labels": [], "entities": []}, {"text": "By contrast, the semantic correctness of the individual sentence is less important in AWE, since human-produced sentences generally do have a coherent meaning, and by and large sentence understanding is quite robust even in the face of multiple spelling and grammatical errors.", "labels": [], "entities": []}, {"text": "Therefore, semantic oddities are usually not independent errors, but either a byproduct of lower-level errors or word pair confusion errors that are recognizable as such in context.", "labels": [], "entities": []}, {"text": "Even beginner-level L2 students know what they want to say.", "labels": [], "entities": []}, {"text": "In the research presented here, however, we focus on the semantics of the individual sentence, and we are interested not only in human-generated sentences, but also in automatically generated random sentences.", "labels": [], "entities": []}, {"text": "In the latter -unlike in AWE -meaning coherence at the sentence level is not governed by an underlying intellect, but has to be controlled and evaluated.", "labels": [], "entities": []}, {"text": "While sentence generatio in our own project is intended for use in language learning exercises with a controlled vocabulary and controlled syntactic complexity, adding a semantic component to random sentence generation is also useful for other tasks, such as creating training data for text-to-speech (TTS) or voice recognition systems.", "labels": [], "entities": [{"text": "sentence generatio", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7175667881965637}]}, {"text": "Thus, describe an HPSG-based generator with a 20-category noun ontology, a lexicon of 2181 wordforms (1100 lemmas) and 39 production rules that achieved a human meaningfulness rating of 3.09 on a 0-6 scale, as opposed to 0.66 fora semantics-free system and 4.52 for human text.", "labels": [], "entities": []}, {"text": "We here adopt a similar approach, matching head words with semantically and categorically constrained slot fillers for valency and modifier slots.", "labels": [], "entities": []}, {"text": "However, we go beyond this framework on several important accounts: \uf0b7 We apply the method to both sentence generation and the evaluation of existing sentences \uf0b7 For evaluation, lexical frames and valency patterns are combined with combinatorial corpus statistics \uf0b7 Vocabulary size and content are parameters controlled by the user, lesson or textbook, and has no upper boundary -in principle, free input sentences can be evaluated, and the semantic ontology has a high coverage even on unabridged text", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7355940192937851}]}], "datasetContent": [{"text": "Obviously, sentence generation (section 3) is more robust than sentence grading (section 4), because the former will only produce what is sanctioned by the input vocabulary and dictionary-based frames.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7565945982933044}]}, {"text": "Sentence grading, however, has to work on unknown sentences, and its performance may suffer from lexical coverage problems, sparse data in the corpus database and, not least, missing or tooconstrained frames.", "labels": [], "entities": [{"text": "Sentence grading", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9075201451778412}]}, {"text": "Especially the latter is a problem in the face of free, creative input from ordinary language users.", "labels": [], "entities": []}, {"text": "Thus, in the controlled environment of course-based substitution table sentences, and with the course itself as part of its database, the tool performed very well, and accepted very few sentences that should have been discarded.", "labels": [], "entities": []}, {"text": "In order to test for false positives (falsely discarded) sentences in a less constrained environment, we removed the course texts from the database, and submitted the entire teaching material 10 (7,363 words) to the sentence grader.", "labels": [], "entities": []}, {"text": "In this run, only 1.4% of the (supposedly correct) sentences received negative scores.", "labels": [], "entities": []}, {"text": "At the word level, there were 3.3% negative scores.", "labels": [], "entities": []}, {"text": "Flags for missing frame support (?-flag) and missing dependency corpus support (*-flag) appeared in 5.3% and 2.4% of cases.", "labels": [], "entities": []}, {"text": "However, neither negative score nor flags area safe marker for unacceptability.", "labels": [], "entities": []}, {"text": "For instance, negative scores can be caused by bad ngram scores even in the face of a frame match, and conversely, corpus \"proof\" can compensate fora missing frame, preventing a negative score.", "labels": [], "entities": []}, {"text": "Therefore, to increase precision and limit the number of false positives, flags and scores should be combined for automatic use.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9995414018630981}]}, {"text": "Thus, at the clause-level, a combined frame failure (?) and corpus evidence failure (*) only occurred in 0.1% of words, and with a double condition for negative score AND a '?' or '*' flag, false positives were down to 0.64% and 1.6%, respectively.", "labels": [], "entities": []}, {"text": "Without a corpus of L2 learner texts, and in the absence of funding for extensive manual evaluation, it is difficult to evaluate the risk of false negatives (i.e. accepted sentences that should have been discarded) in external texts, but it is still possible to use the above method and estimate the prevalence of false positives, even on a larger scale, by simply running the evaluator on text that is not produced by learners, and is supposed to be correct.", "labels": [], "entities": []}, {"text": "For our experiments we used a collection of short stories 11 (61,676 words), that are part of the advanced-learner material on the Esperanto teaching site lernu.net.", "labels": [], "entities": [{"text": "Esperanto teaching site lernu.net", "start_pos": 131, "end_pos": 164, "type": "DATASET", "confidence": 0.6863778308033943}]}, {"text": "As expected, the short stories, with almost 4 times as many morphemes and lemmas, and sentences that were on average 75% longer, were more difficult for the evaluator program, with considerably more false positives, i.e. words with negative scores in supposedly normal sentences.: Evaluation -False positives However, at the sentence level there were fewer false positives, indicating that the individual negative scores were milder, or that there was more corpus support from ngrams.", "labels": [], "entities": []}, {"text": "One reason for the former could be the lower incidence of frame failures 12 , which are punished harder than corpus failures and therefore contribute more to a negative word score than corpus failures.", "labels": [], "entities": [{"text": "incidence", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9708386659622192}]}, {"text": "In any case the unclear balance between ? and * flags is another reason fora 'in dubio pro reo' approach, where words are regarded as problematic only, if they fail on all accounts (both frames and corpus).", "labels": [], "entities": []}, {"text": "With this condition, the rate of false positives is very low even for the more complex short story corpus (0.5%).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall vocabulary size for content  POS, roots vs. words", "labels": [], "entities": []}, {"text": " Table 6: Evaluation -False positives", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9374734163284302}, {"text": "False positives", "start_pos": 22, "end_pos": 37, "type": "METRIC", "confidence": 0.9615917801856995}]}]}