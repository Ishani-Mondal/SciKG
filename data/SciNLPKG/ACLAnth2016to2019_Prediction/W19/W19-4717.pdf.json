{"title": [{"text": "Treat the Word As a Whole or Look Inside? Subword Embeddings Model Language Change and Typology", "labels": [], "entities": []}], "abstractContent": [{"text": "We use a variant of word embedding model that incorporates subword information to characterize the degree of compositionality in lexical semantics.", "labels": [], "entities": []}, {"text": "Our models reveal some interesting yet contrastive patterns of long-term change in multiple languages: Indo-European languages put more weight on subword units in newer words, while conversely Chinese puts less weights on the subwords, but more weight on the word as a whole.", "labels": [], "entities": []}, {"text": "Our method provides novel evidence and methodology that enriches existing theories in evolutionary linguistics.", "labels": [], "entities": []}, {"text": "The resulting word vectors also has decent performance in NLP-related tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "The roles that subword units play in determining word semantics differ across languages.", "labels": [], "entities": []}, {"text": "In typical alphabetic languages, such as English, the smallest grammatical subword unit is morpheme.", "labels": [], "entities": []}, {"text": "A morpheme can be classified as either free or bound: the former stands by itself as a word (e.g., the root of English words), while the latter functions only as part of a word (e.g., affixes such as -ness, un-, etc.).", "labels": [], "entities": []}, {"text": "In EasternAsian languages, however, the distinction between morphemes and words is not as clear.", "labels": [], "entities": []}, {"text": "Particularly in Chinese, the basic subword unit that acts as a morpheme is character (\u5b57), but whether a single morpheme or the combination of morphemes constitute a word is open to debate.", "labels": [], "entities": []}, {"text": "Despite the fact that morphological regularities of words have been extensively applied to improve the dense vector representations of words learned from data, i.e., word embeddings (), the research endeavors so far are less oriented towards linguistic theories about the semantic roles of subword units in word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 307, "end_pos": 321, "type": "TASK", "confidence": 0.7293294966220856}]}, {"text": "In other words, NLP research has optimized towards processing languages such as English, but less so Chinese.", "labels": [], "entities": []}, {"text": "This study provides a first attempt (to the best of our knowledge) that uses word embedding models to explore the roles of subword units in the composition of word meanings.", "labels": [], "entities": []}, {"text": "The source code is available at https://github.com/ innerfirexy/lchange2019.", "labels": [], "entities": []}, {"text": "We have shown that a variant based on the current subwordincorporated models can effectively quantify the semantic weights carried by subword units, with the cost of an moderate number of additional parameters, which have clear interpretations.", "labels": [], "entities": []}, {"text": "Moreover, we have found that these semantic weights demonstrate temporal patterns that are different between Chinese and Indo-European languages, which implies a fundamental difference in the mechanisms of word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 206, "end_pos": 220, "type": "TASK", "confidence": 0.7486391961574554}]}, {"text": "More theoretical motivations are discussed in the following section.", "labels": [], "entities": []}], "datasetContent": [{"text": "Training the DSE model not just results in a lookup table of h w , but also outputs word embeddings.", "labels": [], "entities": [{"text": "DSE", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.8876817226409912}]}, {"text": "In theory, these embeddings should be better representations of the semantic space than the CWE and fastText models, because DSE uses more parameters (h w ).", "labels": [], "entities": []}, {"text": "Here, we compare the quality of word embeddings resulting from DSE models with those from previous models.", "labels": [], "entities": []}, {"text": "Any superiority that DSE could show will indicate that dynamically considering the semantic weight of subword units can be potentially useful in other NLP tasks.", "labels": [], "entities": [{"text": "DSE", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.779032289981842}]}, {"text": "There are several standard lexical semantic tasks commonly used to evaluate the quality of embeddings.", "labels": [], "entities": []}, {"text": "We use two of them, word similarity/relatedness and word analogy, and evaluate the embeddings from Chinese and English.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.759037584066391}]}, {"text": "For word similarity task, Wordsim-296 in) in English are used.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.788306991259257}]}, {"text": "Higher Spearman's correlation score indicates better performance.", "labels": [], "entities": [{"text": "Spearman's correlation score", "start_pos": 7, "end_pos": 35, "type": "METRIC", "confidence": 0.916065126657486}]}, {"text": "For word analogy task, the semantic part of the original dataset 10 developed by and its Chinese translated version are used.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.8102415800094604}]}, {"text": "The total percentage of correctly answered questions is used to measure the performance on this task.", "labels": [], "entities": []}, {"text": "The performance of DSE are shown in compared with CWE and fastText.", "labels": [], "entities": []}, {"text": "Here we do not use the original implementations of CWE and fastText (in C and C++), but use our own implementations with the same programming framework as DSE (By disabling the h w parameters).", "labels": [], "entities": []}, {"text": "This is for the consideration of fair comparisons.", "labels": [], "entities": []}, {"text": "The models are compared within two groups according to  the architecture: DSE-CBOW and CWE are in CBOW group; DSE-SG and fastText are in skipgram group.", "labels": [], "entities": [{"text": "DSE-SG", "start_pos": 110, "end_pos": 116, "type": "DATASET", "confidence": 0.9006476402282715}]}, {"text": "We find that DSE-SG achieves higher score in word analogy task, and overall speaking, DSE models have comparable or slightly lower performance in word similarity task.", "labels": [], "entities": [{"text": "word analogy task", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8588496645291647}, {"text": "word similarity task", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.7998730540275574}]}, {"text": "It is surprising that DSE does not show a significant improvement, which could be due to the redundancy in model parameters or the size of training data.", "labels": [], "entities": [{"text": "DSE", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.5836241841316223}]}, {"text": "That said, what we show here is that the new model performs well enough to be plausible.", "labels": [], "entities": []}, {"text": "We do not attempt to improve upon the state-ofthe-art in these downstream tasks; rather, the main purpose of the model is for linguistic inquiries.", "labels": [], "entities": [{"text": "linguistic inquiries", "start_pos": 126, "end_pos": 146, "type": "TASK", "confidence": 0.7087469100952148}]}], "tableCaptions": [{"text": " Table 2: Performance in lexical semantic tasks.", "labels": [], "entities": []}, {"text": " Table 3: Case study examples. Earlier words on the left are adjectives and verbs, and have smaller h w . Later words  on the right are nouns, and have larger h w . Subword units shared across words are highlighted.", "labels": [], "entities": []}]}