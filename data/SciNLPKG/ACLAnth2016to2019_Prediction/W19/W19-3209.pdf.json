{"title": [{"text": "Identifying adverse drug events mentions in tweets using attentive, collocated, and aggregated medical representation", "labels": [], "entities": []}], "abstractContent": [{"text": "Identifying mentions of medical concepts in social media is challenging because of high variability in free text.", "labels": [], "entities": [{"text": "Identifying mentions of medical concepts in social media", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.9075495898723602}]}, {"text": "In this paper, we propose a novel neural network architecture, the Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), that integrates a bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information from training data to improve the representation of medical concepts.", "labels": [], "entities": []}, {"text": "The collocation and aggregation layers improve the model performance on the task of identifying mentions of adverse drug events (ADE) in tweets.", "labels": [], "entities": [{"text": "identifying mentions of adverse drug events (ADE) in tweets", "start_pos": 84, "end_pos": 143, "type": "TASK", "confidence": 0.7198698087172075}]}, {"text": "Using the dataset made available as part of the workshop shared task, we show that careful selection of neighborhood contexts can help uncover useful local information and improve the overall medical concept representation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multiple studies have analyzed health forums and other social media for drug uses, pharmacovigilance, and effectiveness of medications.", "labels": [], "entities": []}, {"text": "However, research related to drugs and adverse drug effects (ADE) in social media continues to grow rapidly.", "labels": [], "entities": []}, {"text": "Automatically detecting ADE mentions in social media posts has been challenging due to the large variability of free text.", "labels": [], "entities": [{"text": "detecting ADE mentions in social media posts", "start_pos": 14, "end_pos": 58, "type": "TASK", "confidence": 0.8396592821393695}]}, {"text": "One of the main challenges in studying natural language processing (NLP) approaches for medical information extraction is the lack of access to health-related information on social media . Having a robust representation of words is important to train high-performance information extraction approaches.", "labels": [], "entities": [{"text": "medical information extraction", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.6498509148756663}, {"text": "information extraction", "start_pos": 268, "end_pos": 290, "type": "TASK", "confidence": 0.7614911198616028}]}, {"text": "In domain-specific tasks, being able to properly represent domain words or concepts could significantly improve the models.", "labels": [], "entities": []}, {"text": "While many studies have undertaken classifications of ADE mentions in posts with various state-of-the-art techniques, there is still room to improve for the task.", "labels": [], "entities": [{"text": "classifications of ADE mentions in posts", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.7560583353042603}]}, {"text": "For example, in many trained word embedding models (, the embedding of each word is treated as a vector summarizing multiple semantic meanings for each word as independent dimensions.", "labels": [], "entities": []}, {"text": "Indeed, pre-trained embeddings that are trained on a large data corpus usually provide robust representation for common words, compared to traditional feature-based techniques such as bag of words.", "labels": [], "entities": []}, {"text": "Yet, for domainspecific tasks, a drawback of pre-trained embeddings is that representations of domain words may not be sufficiently tuned to be able to represent the expected meaning.", "labels": [], "entities": []}, {"text": "Attempts have been made previously to capture the word embedding for medical concepts from a variety of medical data sources.", "labels": [], "entities": []}, {"text": "Similarly, domain-specific knowledge graphs have been shown effective as external resources for feature expansion to represent medical concepts.", "labels": [], "entities": []}, {"text": "However, even domain-based knowledge graphs sometime contain redundant information stemming from how they are constructed (.", "labels": [], "entities": []}, {"text": "Following prior work by) that show that co-occurring pattern of terms could be beneficial to classification tasks, in this work, we consider an alternate graph-based representation that utilizes local information derived from the training data set.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 93, "end_pos": 113, "type": "TASK", "confidence": 0.8952734768390656}]}, {"text": "We build a collocation graph -a wordbased graph built from the training data set where nodes correspond to vocabulary words and edges between two nodes indicate the co-occurrence of the corresponding words.", "labels": [], "entities": []}, {"text": "We investigate if a model built over the collocation graph could use pre-trained word embeddings and other information to recognize medical concepts from data.", "labels": [], "entities": []}, {"text": "We hypothesize that the representation of a medical word can be further enriched by its neighbors in the collocation graph.", "labels": [], "entities": []}, {"text": "In this paper, we propose Collocated LSTM with Attentive Pooling and Aggregated representation (CLAPA), a novel approach that integrates bidirectional LSTM model with attention and pooling strategy and utilizes the collocation information in the training data set to help enhance the pre-trained word embedding of medical concepts.", "labels": [], "entities": []}, {"text": "We show that our model leads to a significant improvement on an ADE detection task.", "labels": [], "entities": [{"text": "ADE detection task", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.9355106751124064}]}, {"text": "To the best of our knowledge, this is the first attempt that utilizes local collocation information to improve the representation of domain concepts in social media.", "labels": [], "entities": []}, {"text": "To summarize, we make the following contributions in this paper: \u2022 We propose a novel architecture that encodes locally stored domain information into sentence representation.", "labels": [], "entities": []}, {"text": "\u2022 Our work explores the possibility that limited training data could be better exploited by including attentive collocation information.", "labels": [], "entities": []}, {"text": "\u2022 We provide implication for other domainrelated works where better representation of domain terms is important, especially when the data set is highly imbalanced.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Number of ADE and non-ADE tweets in  training and validation data sets.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of models on Precision, Recall,  and F1 measures for the ADE detection task on the val- idation set. The scores in the last two rows are over the  test set of the 2019 SMM4H 2019 shared task 1.", "labels": [], "entities": [{"text": "Precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9936901330947876}, {"text": "Recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9523195624351501}, {"text": "F1", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.9969360828399658}, {"text": "ADE detection task", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.8680370450019836}, {"text": "SMM4H 2019 shared task", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.7785894274711609}]}, {"text": " Table 4: Effects of concept vocabulary on model per- formance", "labels": [], "entities": []}, {"text": " Table 5: Effects of neighborhood selection methods  on F1 scores on both ADE+non-ADE tweets and only  ADE tweets", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9786045551300049}]}]}