{"title": [{"text": "Cross-Lingual Lemmatization and Morphology Tagging with Two-Stage Multilingual BERT Fine-Tuning", "labels": [], "entities": [{"text": "Morphology Tagging", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.6056735068559647}, {"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9395267963409424}]}], "abstractContent": [{"text": "We present our CHARLES-SAARLAND system for the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology , in task 2, Morphological Analysis and Lemmatization in Context.", "labels": [], "entities": [{"text": "CHARLES-SAARLAND", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9257721900939941}, {"text": "Morphological Analysis", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.851886659860611}]}, {"text": "We leverage the multilingual BERT model and apply several fine-tuning strategies introduced by UDify demonstrating exceptional evaluation performance on morpho-syntactic tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.8883835673332214}]}, {"text": "Our results show that fine-tuning multilingual BERT on the concatenation of all available treebanks allows the model to learn cross-lingual information that is able to boost lemmatization and morphology tagging accuracy over fine-tuning it purely monolingually.", "labels": [], "entities": [{"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9641574025154114}, {"text": "morphology tagging", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.6917992979288101}, {"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9508833289146423}]}, {"text": "Unlike UDify, however , we show that when paired with additional character-level and word-level LSTM layers, a second stage of fine-tuning on each treebank individually can improve evaluation even further.", "labels": [], "entities": []}, {"text": "Out of all submissions for this shared task, our system achieves the highest average accuracy and f1 score in morphology tagging and places second in average lemmatization accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9771841764450073}, {"text": "f1 score", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9769090414047241}, {"text": "morphology tagging", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8511207699775696}, {"text": "accuracy", "start_pos": 172, "end_pos": 180, "type": "METRIC", "confidence": 0.9351601004600525}]}], "introductionContent": [{"text": "We focus on track 2 of the SIGMORPHON 2019 Shared Task (, which requires systems to predict lemmas and morphosyntactic descriptions (MSDs) of words given sentences of pre-tokenized words.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 Shared Task", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.7407212108373642}]}, {"text": "The data relies on treebanks provided by the Universal Dependencies (UD) project, where MSDs are converted from UD format to the UniMorph schema (.", "labels": [], "entities": []}, {"text": "Systems must predict from sentences given test data provided in 107 separate treebanks each representing one of 66 different languages.", "labels": [], "entities": []}, {"text": "Recent advances in contextual word representations show that pretraining language models on a large corpus of unsupervised text can be used to", "labels": [], "entities": [{"text": "contextual word representations", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6595712304115295}]}], "datasetContent": [{"text": "We train our system on the provided treebank training data with three separate configurations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A summary of hyperparameters applicable to  each model configuration.", "labels": [], "entities": []}, {"text": " Table 2: A summary of the average results of each  model configuration with a comparison to the baseline  (Malaviya et al., 2019).", "labels": [], "entities": []}, {"text": " Table 3: Main results (part 1 of 4).", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8102552890777588}]}, {"text": " Table 4: Main results (part 2 of 4).", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8183478713035583}]}, {"text": " Table 5: Main results (part 3 of 4).", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7947717308998108}]}, {"text": " Table 6: Main results (part 4 of 4).", "labels": [], "entities": [{"text": "Main", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8110030889511108}]}]}