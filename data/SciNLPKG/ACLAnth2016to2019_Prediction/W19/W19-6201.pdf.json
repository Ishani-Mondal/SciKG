{"title": [{"text": "Mark my Word: A Sequence-to-Sequence Approach to Definition Modeling", "labels": [], "entities": [{"text": "Definition Modeling", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7927302420139313}]}], "abstractContent": [{"text": "Defining words in a textual context is a useful task both for practical purposes and for gaining insight into distributed word representations.", "labels": [], "entities": []}, {"text": "Building on the distribu-tional hypothesis, we argue here that the most natural formalization of definition modeling is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a con-textually appropriate definition for it.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.9422293603420258}]}, {"text": "We implement this approach in a Transformer-based sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "Our proposal allows to train contextualization and definition generation in an end-to-end fashion, which is a conceptual improvement over earlier works.", "labels": [], "entities": [{"text": "definition generation", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.9447572827339172}]}, {"text": "We achieve state-of-the-art results both in contextual and non-contextual definition modeling.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of definition modeling, introduced by, consists in generating the dictionary definition of a specific word: for instance, given the word \"monotreme\" as input, the system would need to produce a definition such as \"any of an order (Monotremata) of egg-laying mammals comprising the platypuses and echidnas\".", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.9637072384357452}]}, {"text": "1 Following the tradition set by lexicographers, we call the word being defined a definiendum (pl. definienda), whereas a word occurring in its definition is called a definiens (pl. definientia).", "labels": [], "entities": []}, {"text": "Definition modeling can prove useful in a variety of applications.", "labels": [], "entities": [{"text": "Definition modeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8914589285850525}]}, {"text": "Systems trained for the task may generate dictionaries for low resource languages, or extend the coverage of existing lexicographic resources where needed, e.g. of domainspecific vocabulary.", "labels": [], "entities": []}, {"text": "Such systems may also be 1 Definition from able to provide reading help by giving definitions for words in the text.", "labels": [], "entities": []}, {"text": "A major intended application of definition modeling is the explication and evaluation of distributed lexical representations, also known as word embeddings.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.9502690136432648}]}, {"text": "This evaluation procedure is based on the postulate that the meaning of a word, as is captured by its embedding, should be convertible into a human-readable dictionary definition.", "labels": [], "entities": []}, {"text": "How well the meaning is captured must impact the ability of the model to reproduce the definition, and therefore embedding architectures can be compared according to their downstream performance on definition modeling.", "labels": [], "entities": []}, {"text": "This intended usage motivates the requirement that definition modeling architectures take as input the embedding of the definiendum and not retrain it.", "labels": [], "entities": []}, {"text": "From a theoretical point of view, usage of word embeddings as representations of meaning (cf., for an overview) is motivated by the distributional hypothesis.", "labels": [], "entities": []}, {"text": "This framework holds that meaning can be inferred from the linguistic context of the word, usually seen as co-occurrence data.", "labels": [], "entities": []}, {"text": "The context of usage is even more crucial for characterizing meanings of ambiguous or polysemous words: a definition that does not take disambiguating context into account will be of limited use (.", "labels": [], "entities": [{"text": "characterizing meanings of ambiguous or polysemous words", "start_pos": 46, "end_pos": 102, "type": "TASK", "confidence": 0.862193192754473}]}, {"text": "We argue that definition modeling should preserve the link between the definiendum and its context of occurrence.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.9769610166549683}]}, {"text": "The most natural approach to this task is to treat it as a sequence-to-sequence task, rather than a word-to-sequence task: given an input sequence with a highlighted word, generate a contextually appropriate definition for it (cf. sections.", "labels": [], "entities": []}, {"text": "We implement this approach in a Transformer-based sequence-to-sequence model that achieves state-of-the-art performances (sections).", "labels": [], "entities": []}], "datasetContent": [{"text": "We implement several sequence to sequence models with the Transformer architecture (, building on the OpenNMT library () with adaptations and modifications when necessary.", "labels": [], "entities": []}, {"text": "Throughout this work, we use GloVe vectors ( and freeze weights of all embeddings fora fairer comparison with previous models; words not in GloVe but observed in train or validation data and missing definienda in our test sets were randomly initialized with components drawn from a normal distribution N (0, 1).", "labels": [], "entities": []}, {"text": "We train a distinct model for each dataset.", "labels": [], "entities": []}, {"text": "We batch examples by 8,192, using gradient accumulation to circumvent GPU limitations.", "labels": [], "entities": []}, {"text": "We optimize the network using Adam with \u03b2 1 = 0.99, \u03b2 2 = 0.998, a learning rate of 2, label smoothing of 0.1, Noam exponential decay with 2000 warmup steps, and dropout rate of 0.4.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 67, "end_pos": 80, "type": "METRIC", "confidence": 0.9656324684619904}, {"text": "dropout rate", "start_pos": 162, "end_pos": 174, "type": "METRIC", "confidence": 0.9622164070606232}]}, {"text": "The parameters are initialized using Xavier.", "labels": [], "entities": []}, {"text": "Models were trained for up to 120,000 steps with checkpoints at each 1000 steps; we stopped training if perplexity on the validation dataset stopped improving.", "labels": [], "entities": []}, {"text": "We report results from checkpoints performing best on validation.", "labels": [], "entities": []}, {"text": "We train our models on three distinct datasets, which are all borrowed or adapted from previous works on definition modeling.", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.9270486533641815}]}, {"text": "As a consequence, our experiments focus on the English language.", "labels": [], "entities": []}, {"text": "The dataset of (henceforth D Nor ) maps definienda to their respective definientia, as well as additional information not used here.", "labels": [], "entities": []}, {"text": "In the dataset of (henceforth D Gad ), each example consists of a definiendum, the definientia for one of its meanings and a contextual cue sentence.", "labels": [], "entities": []}, {"text": "D Nor contains on average shorter definitions than D Gad . Definitions in D Nor have a mean length of 6.6 and a standard deviation of 5.78, whereas those in D Gad have a mean length of 11.01 and a standard deviation of 6.96.", "labels": [], "entities": []}, {"text": "stress that the dataset D Gad includes many examples where the definiendum is absent from the associated cue.", "labels": [], "entities": []}, {"text": "About half of these cues doe not contain an exact match for the corresponding definiendum, but up to 80% contains either an exact match or an inflected form of the definiendum according to lemmatization by the NLTK toolkit ().", "labels": [], "entities": [{"text": "NLTK toolkit", "start_pos": 210, "end_pos": 222, "type": "DATASET", "confidence": 0.9180557131767273}]}, {"text": "To cope with this problematic characteristic, we converted the dataset into the word-in-context format assumed by our model by concatenating the definiendum with the cue.", "labels": [], "entities": []}, {"text": "To illustrate this, consider the actual input from D Gad comprised of the definiendum \"fool\" and its associated cue \"enough horsing around-let's get back to work!\": to convert this into a single sequence, we simply prepend the definiendum to the cue, which results in the sequence \"fool enough horsing around-let's get back to work!\".", "labels": [], "entities": []}, {"text": "Hence the input sequences of D Gad do not constitute linguistically coherent sequences, but it does guarantee that our sequenceto-sequence variants have access to the same input as previous models; therefore the inclusion of this dataset in our experiments is intended mainly for comparison with previous architectures.", "labels": [], "entities": []}, {"text": "We also note that this conversion procedure entails that our examples have a very regular structure: the word marked as a definiendum is always the first word in the input sequence.", "labels": [], "entities": []}, {"text": "Our second strategy was to restrict the dataset by selecting only cues where the definiendum (or its inflected form) is present.", "labels": [], "entities": []}, {"text": "The curated dataset (henceforth D Ctx ) contains 78,717 training examples, 9,413 for validation and 9,812 for testing.", "labels": [], "entities": []}, {"text": "In each example, the first occurrence of the definiendum is annotated as such.", "labels": [], "entities": []}, {"text": "D Ctx thus differs from D Gad in two ways: some definitions have been removed, and the exact citation forms of the definienda are not given.", "labels": [], "entities": [{"text": "D Ctx", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.885157972574234}]}, {"text": "Models trained on D Ctx implicitly need to lemmatize the definiendum, since inflected variants of a given word are to be aligned to a common representation; thus they are not directly comparable with models trained with the citation form of the definiendum that solely use context as a cue-viz..", "labels": [], "entities": []}, {"text": "All this makes D Ctx harder, but at the same time closer to a realistic application than the other two datasets, since each word appears inflected and in a specific sentential context.", "labels": [], "entities": []}, {"text": "For applications of definition modeling, it would only be beneficial to take up these challenges; for example, the output \"monotremes: plural of monotreme\" would not have been self-contained, necessitating a second query for \"monotreme\".", "labels": [], "entities": [{"text": "definition modeling", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9213683009147644}]}], "tableCaptions": []}