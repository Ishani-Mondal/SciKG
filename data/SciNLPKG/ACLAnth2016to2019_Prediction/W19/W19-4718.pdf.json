{"title": [{"text": "Times Are Changing: Investigating the Pace of Language Change in Diachronic Word Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose Word Embedding Networks (WEN), a novel method that is able to learn word embeddings of individual data slices while simultaneously aligning and ordering them without feeding temporal information a priori to the model.", "labels": [], "entities": []}, {"text": "This gives us the opportunity to analyse the dynamics in word embed-dings on a large scale in a purely data-driven manner.", "labels": [], "entities": []}, {"text": "In experiments on two different newspaper corpora, the New York Times (English) and Die Zeit (German), we were able to show that time actually determines the dynamics of semantic change.", "labels": [], "entities": []}, {"text": "However, we find that the evolution does not happen uniformly, but instead we discover times of faster and times of slower change.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vectorial representation of natural language, known as word embeddings, have been widely used in e.g. text classification ( and machine translation ().", "labels": [], "entities": [{"text": "Vectorial representation of natural language", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8097972273826599}, {"text": "text classification", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.7833696007728577}, {"text": "machine translation", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.7991115152835846}]}, {"text": "As in;; and aligned sets of embeddings have also been used to detect changes in vectorial representations of words overtime.", "labels": [], "entities": []}, {"text": "In the past, those changes have mostly been studied at the word-level.", "labels": [], "entities": []}, {"text": "We propose a novel method to investigate the pace of language change based on the entire embedding matrix.", "labels": [], "entities": []}, {"text": "Previous approaches have not been able to carryout this type of analysis, as they have taken the continuous change of language for granted and investigated those dynamics in a supervised manner.", "labels": [], "entities": []}, {"text": "Therefore, we present Word Embedding Networks (WEN), a method that has no knowledge about the chronological order of the slices, so we can investigate semantic changes on the whole vocabulary purely data-driven and unsupervised.", "labels": [], "entities": []}, {"text": "Pairwise relations between embeddings and the embeddings themselves are learned simultaneously without feeding the temporal information a priori into the algorithm.", "labels": [], "entities": []}, {"text": "In that, it is substantially more flexible than those methods mentioned above.", "labels": [], "entities": []}, {"text": "This means that dynamics between any slicing of a text corpus can be learned (especially those where there is no order known) and the result not only contains embeddings for each slice, but an order of slices that corresponds to the dynamics of word meanings.", "labels": [], "entities": []}, {"text": "This method also overcomes the need of a twostep solution for aligned temporal embeddings, as has also been done by -the two-step solution has, according to, its weaknesses especially in the case of non-uniformly distributed amounts of data across the slices.", "labels": [], "entities": []}, {"text": "Closer proximities between embeddings denote time intervals of slower semantic changes, embeddings are farther apart when times are changing faster.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}