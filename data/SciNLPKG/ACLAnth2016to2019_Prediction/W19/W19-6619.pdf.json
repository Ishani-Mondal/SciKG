{"title": [{"text": "Controlling the Reading Level of Machine Translation Output", "labels": [], "entities": [{"text": "Reading Level", "start_pos": 16, "end_pos": 29, "type": "METRIC", "confidence": 0.8661504983901978}, {"text": "Machine Translation Output", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.7884323298931122}]}], "abstractContent": [{"text": "Today's machine translation systems output the same translation fora given input , despite important differences between users.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7676076889038086}]}, {"text": "In practice, translations should be customized for each reader, for instance when translating for children versus in a business setting.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the task of reading level control to machine translation, and provide the first results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8039887249469757}]}, {"text": "Our methods can be used to raise or lower the reading level of output translations.", "labels": [], "entities": []}, {"text": "In our first approach, source-side sentences in the training corpus are tagged based on the reading level (read-ability) of the matching target sentences.", "labels": [], "entities": []}, {"text": "Our second approach alters the traditional encoder-decoder architecture by specifying a joint encoder and separate decoders for simple and complex decoding modes, with training data partitioned by reading level.", "labels": [], "entities": []}, {"text": "We demonstrate control over output readability score on three test sets in the Spanish-English language direction.", "labels": [], "entities": []}], "introductionContent": [{"text": "Though the goal of machine translation is to generate semantically accurate translations from one language to another, there are other factors which affect whether a translation is \"good\".", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7366219758987427}]}, {"text": "One often-neglected factor is the reading level of the translation-different contexts require different reading levels.", "labels": [], "entities": []}, {"text": "When translating for lessskilled readers, one may desire a translation with common vocabulary and simple sentence structures.", "labels": [], "entities": []}, {"text": "Ina professional setting, however, one often requires concise language with advanced vocabulary and syntactic structure.", "labels": [], "entities": []}, {"text": "For instance, when translating a Spanish web page about machine translation to an Englishspeaking 7-year-old, one might output, \"machine translation is away to take a sentence from one language and turn it into a sentence in another language\".", "labels": [], "entities": [{"text": "translating a Spanish web page about machine translation to an Englishspeaking 7-year-old", "start_pos": 19, "end_pos": 108, "type": "TASK", "confidence": 0.6502959554394087}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7292694449424744}]}, {"text": "When advertising new machine translation software to a potential investor, one might explain, \"machine translation is the automated process by which a sentence in a source language can be converted into a sentence in another language\".", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7675563097000122}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7917444705963135}]}, {"text": "Both sentences carry the same meaning and do not require specialist technical knowledge, but decreasing the complexity in the first makes it easier fora child to understand, and increasing the complexity in the second makes it sound more professional and sophisticated.", "labels": [], "entities": []}, {"text": "Furthermore, for native speakers of low-resource languages where machine translation quality may currently be poor but who can read basic phrases in a second language where translation quality is high, they may prefer to read a lower complexity but semantically accurate translation in their second language over an inaccurate, garbled message in their native tongue.", "labels": [], "entities": []}, {"text": "In this paper, we introduce the task of reading level control (readability control) to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.8175607919692993}]}, {"text": "We develop two methodologies that control the reading level of a translation in the Spanish-English language direction, focusing on lexical complexity as a first step.", "labels": [], "entities": []}, {"text": "For professional settings, we aim to produce advanced vocabulary.", "labels": [], "entities": []}, {"text": "For less-skilled readers, the translation should use simple words while maintaining the meaning of the source sentence.", "labels": [], "entities": []}, {"text": "Accordingly, we build a system where a user can specify the reading level (\"sim-ple\" or \"complex\") of the translation they wish to be output.", "labels": [], "entities": []}, {"text": "Future work should examine controlling other factors that affect the readability of a sentence, such as syntactic structure.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use three Spanish-English training sets: the European Parliament Proceedings (Europarl) (), OpenSubtitles2018 (OS) corpus (Lison and Tiedemann, 2016), and ParaCrawl 3 . Europarl contains transcripts of European Parliamentary proceedings, OpenSubtitles2018 is a corpus of movie subtitles, and ParaCrawl consists of data scraped from the web.", "labels": [], "entities": [{"text": "European Parliament Proceedings (Europarl)", "start_pos": 48, "end_pos": 90, "type": "DATASET", "confidence": 0.8892904818058014}, {"text": "OpenSubtitles2018 (OS) corpus", "start_pos": 95, "end_pos": 124, "type": "DATASET", "confidence": 0.688025826215744}]}, {"text": "For training each model and for the preliminary experiments in, we use either: \u223c2 million randomly-selected lines from OpenSubtitles2018, the \u223c2 million line Europarl training set, a concatentaion of the aforementioned two corpora (OS+Europarl), or 14.7 million randomly-selected lines from ParaCrawl.", "labels": [], "entities": [{"text": "OpenSubtitles2018", "start_pos": 119, "end_pos": 136, "type": "DATASET", "confidence": 0.942313551902771}, {"text": "Europarl training set", "start_pos": 158, "end_pos": 179, "type": "DATASET", "confidence": 0.9363972942034403}, {"text": "ParaCrawl", "start_pos": 291, "end_pos": 300, "type": "DATASET", "confidence": 0.9530671238899231}]}, {"text": "Development sets are: 10,000 held-out lines from OpenSubtitles2018 for the OpenSubtitles baseline, newstest2012 for the Europarl baseline, the concatenation of newstest2012 and the OpenSubtitles development set for the OS+Europarl baseline, and 3,000 held-out lines from ParaCrawl for the ParaCrawl baseline.", "labels": [], "entities": [{"text": "Europarl baseline", "start_pos": 120, "end_pos": 137, "type": "DATASET", "confidence": 0.957301914691925}, {"text": "OS+Europarl baseline", "start_pos": 219, "end_pos": 239, "type": "DATASET", "confidence": 0.7689351886510849}, {"text": "ParaCrawl", "start_pos": 271, "end_pos": 280, "type": "DATASET", "confidence": 0.9724367260932922}, {"text": "ParaCrawl baseline", "start_pos": 289, "end_pos": 307, "type": "DATASET", "confidence": 0.9590433537960052}]}, {"text": "Double-decoder models are validated by assessing the performance of each decoder separately on the development set.", "labels": [], "entities": []}, {"text": "The test sets are newstest2013 (3,000 lines), a combined test set of newstest2013 plus 10,000 held-out lines from OpenSubtitles2018, and 3,000 held-out lines from ParaCrawl.", "labels": [], "entities": [{"text": "ParaCrawl", "start_pos": 163, "end_pos": 172, "type": "DATASET", "confidence": 0.9745389223098755}]}, {"text": "We performed human evaluation to determine whether the lower BLEU score observed in more extreme data-partitioning conditions in the doubledecoder approach was the result of true loss in translation quality, or desirable swapping of simple/complex words.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9813746213912964}]}, {"text": "We randomly sampled 50 translations from newstest2013 and obtained the translations from the double-decoder 20-20 partition setting, along with the baseline model and a weaker baseline trained to achieve comparable BLEU to that of the double-decoder approach.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9993041753768921}]}, {"text": "Nine English-speaking adults each scored approximately one-third of the sampled translations on a 10-point scale so that each translation received three scores.", "labels": [], "entities": []}, {"text": "Reviewers were instructed to score how well each translation matched the meaning of the reference, along with the fluency of the translation.", "labels": [], "entities": []}, {"text": "Examples were presented in blocks with the reference translation followed by the four system translations in a random order for each block.", "labels": [], "entities": []}, {"text": "Participants each scored 15 or 20 blocks.", "labels": [], "entities": []}, {"text": "In, we show the average score that translations from each system received.", "labels": [], "entities": []}, {"text": "We observe that while the drop in BLEU in reflects some lowered translation quality as judged by human reviewers, the loss in quality is smaller than the BLEU depreciation makes it seem.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9984664916992188}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.9896872639656067}]}, {"text": "When compared to a baseline model with comparable BLEU to that of the \"simple\" and \"complex\" modes (the \"weaker\" baseline), the double-decoder approaches fair better inhuman evaluation despite having lower BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9969160556793213}, {"text": "BLEU", "start_pos": 206, "end_pos": 210, "type": "METRIC", "confidence": 0.9982777833938599}]}, {"text": "This indicates that BLEU over-penalizes models trained to control readability level, and that readability-controlled translations are better than they appear based on BLEU alone.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9834725856781006}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9623828530311584}]}, {"text": "Note that in this section we only performed human evaluation on outputs from the doubleProceedings of MT Summit XVII, volume 1 Dublin, Aug. 19-23, 2019 | p.", "labels": [], "entities": [{"text": "doubleProceedings of MT Summit XVII, volume 1 Dublin, Aug. 19-23", "start_pos": 81, "end_pos": 145, "type": "DATASET", "confidence": 0.6993791908025742}]}], "tableCaptions": [{"text": " Table 1). We then test the read- ability of each model's translation of WMT new- stest2013 2 (Table 2). Please see Section 5 for im- plementation details and description of datasets.", "labels": [], "entities": [{"text": "read- ability", "start_pos": 28, "end_pos": 41, "type": "METRIC", "confidence": 0.8761498729387919}, {"text": "WMT new- stest2013 2", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.8951815843582154}]}, {"text": " Table 2: Effect of the training corpus on translation readabil- ity for newstest2013. Lower DC score, lower FKG score, and  higher FRE score indicate simpler sentences.", "labels": [], "entities": [{"text": "DC score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9880337715148926}, {"text": "FKG score", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9856935441493988}, {"text": "FRE score", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9923121929168701}]}, {"text": " Table 3: Performance on newstest2013 of data tagging ap- proach trained on ParaCrawl. DC, FKG, and FRE are read- ability measures (lower indicates simpler for DC/FKG, and  higher for FRE). e.g., 7.72 is the average DC score of the  output in simple mode using a 50-50 partition. 15-15*  means oversampling the top/bottom 15% of data (3x). All  DC/FKG/FRE results are significant (p<0.001).", "labels": [], "entities": [{"text": "ParaCrawl", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9314864873886108}, {"text": "FRE", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9990276098251343}, {"text": "FRE", "start_pos": 184, "end_pos": 187, "type": "METRIC", "confidence": 0.9948676824569702}, {"text": "FRE", "start_pos": 352, "end_pos": 355, "type": "METRIC", "confidence": 0.9645913243293762}]}, {"text": " Table 4: Performance on newstest2013 of double-decoder  models trained on ParaCrawl data. In the 50-50* setting, 50%  of data is designated \"simple\", 50% \"complex\", and the most  extreme 15% of simple/complex data are oversampled (3x).  All DC/FKG/FRE results are significant (p<0.001).", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9480894207954407}, {"text": "ParaCrawl data", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.9229402542114258}, {"text": "FRE", "start_pos": 249, "end_pos": 252, "type": "METRIC", "confidence": 0.9376162886619568}]}, {"text": " Table 5: Performance of high-performing models with double-decoder (D) and data tagging (T) approaches on three test sets.  The left/right number is the difference in Dale-Chall score between the baseline and the simple/complex translation. Model  with the larger difference is bolded.", "labels": [], "entities": [{"text": "data tagging (T)", "start_pos": 76, "end_pos": 92, "type": "TASK", "confidence": 0.723901504278183}]}, {"text": " Table 6: Average model score in human evaluation for mod- els trained on Paracrawl. Complex and Simple represent com- plex and simple modes for the double-decoder approach with  a 20-20 data partition.", "labels": [], "entities": [{"text": "Paracrawl", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.918016791343689}]}, {"text": " Table 8: Readability performance of the data tagging method  at five levels of readability, trained on ParaCrawl and tested  on newstest2013.", "labels": [], "entities": [{"text": "data tagging", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.7482475936412811}, {"text": "ParaCrawl", "start_pos": 104, "end_pos": 113, "type": "DATASET", "confidence": 0.9635881185531616}, {"text": "newstest2013", "start_pos": 129, "end_pos": 141, "type": "DATASET", "confidence": 0.962486982345581}]}]}