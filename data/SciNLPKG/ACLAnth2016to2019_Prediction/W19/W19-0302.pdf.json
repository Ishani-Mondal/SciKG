{"title": [{"text": "North S\u00e1mi morphological segmentation with low-resource semi-supervised sequence labeling", "labels": [], "entities": [{"text": "S\u00e1mi morphological segmentation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.5905345976352692}, {"text": "labeling", "start_pos": 81, "end_pos": 89, "type": "TASK", "confidence": 0.4859131872653961}]}], "abstractContent": [{"text": "Semi-supervised sequence labeling is an effective way to train a low-resource morphological segmentation system.", "labels": [], "entities": [{"text": "Semi-supervised sequence labeling", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6111464103062948}]}, {"text": "We show that a feature set augmentation approach, which combines the strengths of generative and discriminative models , is suitable both for graphical models like conditional random field (CRF) and sequence-to-sequence neural models.", "labels": [], "entities": []}, {"text": "We perform a comparative evaluation between three existing and one novel semi-supervised segmentation methods.", "labels": [], "entities": []}, {"text": "All four systems are language-independent and have open-source implementations.", "labels": [], "entities": []}, {"text": "We improve on previous best results for North S\u00e1mi morphological segmentation.", "labels": [], "entities": [{"text": "S\u00e1mi morphological segmentation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.6687238216400146}]}, {"text": "We see a relative improvement in morph boundary F1-score of 8.6% compared to using the generative Morfessor FlatCat model directly and 2.4% compared to a seq2seq baseline.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.5980370044708252}]}, {"text": "Our neural sequence tagging system reaches almost the same performance as the CRF topline.", "labels": [], "entities": [{"text": "neural sequence tagging", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.66143399477005}]}, {"text": "Tiivistelm\u00e4 Puoliohjattu sekvenssiluokitus on tehokas tapa opettaa morfologinen pilkon-taj\u00e4rjestelm\u00e4 kielelle, jolle on saatavilla niukasti lingvistisi\u00e4 resursseja.", "labels": [], "entities": []}, {"text": "Osoitam-me, ett\u00e4 generatiivisen mallin tuottamien piirteiden k\u00e4ytt\u00e4minen soveltuu paitsi graafisille malleille kuten ehdollinen satunnaiskentt\u00e4 (CRF), my\u00f6s sekvenssist\u00e4-sekvenssiin (seq2seq)-neuroverkkomalleille.", "labels": [], "entities": [{"text": "generatiivisen mallin tuottamien piirteiden k\u00e4ytt\u00e4minen soveltuu paitsi graafisille malleille kuten ehdollinen satunnaiskentt\u00e4 (CRF)", "start_pos": 17, "end_pos": 149, "type": "TASK", "confidence": 0.6683005094528198}]}, {"text": "Vertailemme kolmea olemassaole-vaa ja yht\u00e4 uutta puoliohjattua menetelm\u00e4\u00e4.", "labels": [], "entities": []}, {"text": "Kaikki menetelm\u00e4t ovat kieliriippu-mattomia, ja niille on avoimen l\u00e4hdekoodin toteutus.", "labels": [], "entities": []}, {"text": "Parannamme aikaisempia tuloksia pohjoissaamen morfologisen pilkonnan teht\u00e4v\u00e4ss\u00e4.", "labels": [], "entities": [{"text": "Parannamme aikaisempia tuloksia pohjoissaamen morfologisen pilkonnan teht\u00e4v\u00e4ss\u00e4", "start_pos": 0, "end_pos": 79, "type": "TASK", "confidence": 0.5362773495061057}]}, {"text": "Suhteelliset paran-nukset morfirajojen osumien F1-mittaan ovat 8.6% verrattuna generatiiviseen Morfessor FlatCat-malliin ja 2.4% verrattuna seq2seq-verrokkimalliin.", "labels": [], "entities": []}, {"text": "Ehdotta-mamme uusi neuroverkkomalli saavuttaa l\u00e4hes saman tason kuin paras CRF-malli.", "labels": [], "entities": [{"text": "Ehdotta-mamme uusi neuroverkkomalli saavuttaa l\u00e4hes saman tason kuin paras CRF-malli", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.44514823853969576}]}, {"text": "This work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by-nd/4.0/ 16", "labels": [], "entities": []}], "introductionContent": [{"text": "Subword models have enjoyed recent success in many natural language processing (NLP) tasks, such as machine translation and automatic speech recognition (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.8289304077625275}, {"text": "automatic speech recognition", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.6267742911974589}]}, {"text": "Uralic languages have rich morphological structure, making morphological segmentation particularly useful for these languages.", "labels": [], "entities": []}, {"text": "While rule-based morphological segmentation systems can achieve high quality, the large amount of human effort needed makes the approach problematic for low-resource languages.", "labels": [], "entities": [{"text": "rule-based morphological segmentation", "start_pos": 6, "end_pos": 43, "type": "TASK", "confidence": 0.6574153701464335}]}, {"text": "As a fast, cheap and effective alternative, data-driven segmentation can be learned based on a very small amount of human annotator effort.", "labels": [], "entities": []}, {"text": "Using active learning, as little as some hundreds of annotated word types can be enough (.", "labels": [], "entities": []}, {"text": "Adopting neural methods has lead to a large performance gain for many NLP tasks.", "labels": [], "entities": []}, {"text": "However, neural networks are typically data-hungry, reducing their applicability to low-resource languages.", "labels": [], "entities": []}, {"text": "Most research has focused on high-resource languages and large data sets, while the search for new approaches to make neural methods applicable to small data has only recently gained attention.", "labels": [], "entities": []}, {"text": "For example, the workshop Deep Learning Approaches for Low-Resource NLP (DeepLo\u00b9) was arranged first time in the year of writing.", "labels": [], "entities": []}, {"text": "Neural methods have met with success in high-resource morphological segmentation (e.g..", "labels": [], "entities": [{"text": "high-resource morphological segmentation", "start_pos": 40, "end_pos": 80, "type": "TASK", "confidence": 0.7551032304763794}]}, {"text": "We are interested to see if data-hungry neural network models are applicable to segmentation in low-resource settings, in this case for the Uralic language North S\u00e1mi.", "labels": [], "entities": []}, {"text": "Neural sequence-to-sequence (seq2seq) models area very versatile tool for NLP, and are used instate of the art methods fora wide variety of tasks, such as text summarization () and speech synthesis ().", "labels": [], "entities": [{"text": "text summarization", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7555334270000458}, {"text": "speech synthesis", "start_pos": 181, "end_pos": 197, "type": "TASK", "confidence": 0.769890546798706}]}, {"text": "Seq2seq methods are easy to apply, as you can often take e.g. existing neural machine translation software and train it with appropriately preprocessed data.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7498405774434408}]}, {"text": "apply the seq2seq model for low-resource morphological segmentation.", "labels": [], "entities": [{"text": "low-resource morphological segmentation", "start_pos": 28, "end_pos": 67, "type": "TASK", "confidence": 0.6292824149131775}]}, {"text": "However, arbitrary length sequence-to-sequence transduction is not the optimal formulation for the task of morphological surface segmentation.", "labels": [], "entities": [{"text": "morphological surface segmentation", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.6489750842253367}]}, {"text": "We return to formulating it as a a sequence tagging problem instead, and show that this can be implemented with minor modifications to an open source translation system.", "labels": [], "entities": [{"text": "sequence tagging problem", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.7735950946807861}]}, {"text": "Moreover, we show that the semi-supervised training approach of using feature set augmentation can also be applied to neural networks to effectively leverage large unannotated data.", "labels": [], "entities": []}], "datasetContent": [{"text": "The segmentations generated by the model are evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary F 1 -score (see e.g.,).", "labels": [], "entities": [{"text": "boundary precision", "start_pos": 107, "end_pos": 125, "type": "METRIC", "confidence": 0.5942294597625732}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9225777983665466}, {"text": "boundary F 1 -score", "start_pos": 148, "end_pos": 167, "type": "METRIC", "confidence": 0.8100066840648651}]}, {"text": "The boundary F 1 -score equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries).", "labels": [], "entities": [{"text": "boundary F 1 -score", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7625227689743042}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.99932861328125}, {"text": "recall", "start_pos": 156, "end_pos": 162, "type": "METRIC", "confidence": 0.9996733665466309}]}, {"text": "Precision and recall are calculated using macro-averages over the words in the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9914040565490723}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9990050196647644}]}, {"text": "In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.", "labels": [], "entities": []}, {"text": "In order to evaluate boundary precision and recall, a valid segmentation is needed for all words in the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.902904748916626}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9987844824790955}]}, {"text": "The seq2seq model can fail to output a valid segmentation, in which case we replace the output with the input without any segmentation boundaries.", "labels": [], "entities": []}, {"text": "To include an evaluation without this source of error we also report word type level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.49908363819122314}]}, {"text": "A word in the test set is counted as correct if all boundary decisions are correct.", "labels": [], "entities": []}, {"text": "Output that does not concatenate back to the input word is treated as incorrect.", "labels": [], "entities": []}, {"text": "shows results on the full test set.", "labels": [], "entities": []}, {"text": "The semi-supervised CRF shows the best performance both according to F 1 -score and word-type level accuracy.", "labels": [], "entities": [{"text": "CRF", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.855818510055542}, {"text": "F 1 -score", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9755567610263824}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.8055705428123474}]}, {"text": "Semi-supervised seq2seq has high precision but low recall, indicating under-segmentation.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.998805046081543}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9992545247077942}]}, {"text": "The neural sequence tagger shows the opposite behavior, with the highest recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9990265369415283}]}, {"text": "All semi-supervised methods improve on the quality of the semi-supervised FlatCat trained on 200 annotated words which is used as input features.", "labels": [], "entities": []}, {"text": "All three discriminative methods also outperform FlatCat trained on the whole training set, on F 1 and accuracy.", "labels": [], "entities": [{"text": "FlatCat", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8862563967704773}, {"text": "F 1", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.97160804271698}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9985277652740479}]}, {"text": "All three semi-supervised methods outperform their fully supervised variants.", "labels": [], "entities": []}, {"text": "These results show that two-step training is preferable over using only Morfessor FlatCat or one of the discrinative methods.", "labels": [], "entities": [{"text": "Morfessor FlatCat", "start_pos": 72, "end_pos": 89, "type": "DATASET", "confidence": 0.9204247891902924}]}, {"text": "The seq2seq model frequently fails to output a valid segmentation, either generating incorrect characters, stopping too early, or getting stuck repeating a pattern of characters.", "labels": [], "entities": []}, {"text": "For 10.7% of the test set, the seq2seq output does not concatenate back to the input word.", "labels": [], "entities": []}, {"text": "shows results for subsets of the evaluation data.", "labels": [], "entities": []}, {"text": "The subsets include all words were the gold standard category labels follow a particular pattern: No internal structure (STM), uninflected compound (STM+STM), single-suffix inflected word (STM+SUF) and two-suffix inflected word (STM+SUF+SUF).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example input factors with embedding dimension. The example word  biebmor\u00e1hkadeamis is segmented as biebmo/STM r\u00e1hkad/STM eami/SUF s/SUF. Stem  (STM) is abbreviated M, and suffix (SUF) is f.", "labels": [], "entities": []}, {"text": " Table 4: Boundary precision (Pre), recall (Rec), and F 1 -scores for different subsets  of the evaluation data. NST stands for Neural sequence tagger. (s) stands for fully  supervised, (ss) for semi-supervised.", "labels": [], "entities": [{"text": "Boundary precision (Pre)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8744863748550415}, {"text": "recall (Rec)", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9551208466291428}, {"text": "F 1 -scores", "start_pos": 54, "end_pos": 65, "type": "METRIC", "confidence": 0.9900113344192505}, {"text": "Neural sequence tagger", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.561525841554006}]}]}