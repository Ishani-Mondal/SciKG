{"title": [{"text": "Vedavaapi: A Platform for Community-sourced Indic Knowledge Processing at Scale", "labels": [], "entities": []}], "abstractContent": [{"text": "Indic heritage knowledge is embedded in millions of manuscripts at various stages of digitization and analysis.", "labels": [], "entities": []}, {"text": "Numerous powerful tools and techniques have been developed for linguistic analysis of Samskrit and Indic language texts.", "labels": [], "entities": [{"text": "linguistic analysis of Samskrit and Indic language texts", "start_pos": 63, "end_pos": 119, "type": "TASK", "confidence": 0.7715471833944321}]}, {"text": "However, the key challenge today is employing them together on large document collections and building higher level end-user applications to make Indic knowledge texts intelligible.", "labels": [], "entities": [{"text": "Indic knowledge texts intelligible", "start_pos": 146, "end_pos": 180, "type": "TASK", "confidence": 0.6674503237009048}]}, {"text": "We believe the chief hurdle is the lack of an end-to-end, secure, decentralized system platform for (i) composing independently developed tools for higher-level tasks, and (ii) employing human experts in the loop to workaround the limitations of automated tools to ensure curated content always.", "labels": [], "entities": []}, {"text": "Such a platform must define protocols and standards for interoperability and reusability of tools while enabling their autonomous evolution to spur innovation.", "labels": [], "entities": []}, {"text": "This paper describes the architecture of an Internet platform for end-to-end Indic knowledge processing called Vedavaapi that addresses these challenges effectively.", "labels": [], "entities": [{"text": "Indic knowledge processing", "start_pos": 77, "end_pos": 103, "type": "TASK", "confidence": 0.6393365561962128}]}, {"text": "At its core, Vedavaapi is a community-sourced, scalable, multi-layered annotated object network.", "labels": [], "entities": []}, {"text": "It serves as an overlay on Indic documents stored anywhere online by providing textifica-tion, language analysis and discourse analysis as value-added services in a crowd-sourced manner.", "labels": [], "entities": [{"text": "language analysis", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7133925259113312}, {"text": "discourse analysis", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7057351320981979}]}, {"text": "It offers federated deployment of tools as microservices, powerful decentralized user / team management with access control across multiple organizational boundaries.", "labels": [], "entities": []}, {"text": "social-media login and an open architecture with extensible and evolving object schemas.", "labels": [], "entities": []}, {"text": "As its first application, we have developed human-assisted text conversion of handwritten manuscripts such as palm leaf etc leveraging several standards-based open-source tools including ones by IIIT Hyderabad, IIT Kanpur and University of Hyderabad.", "labels": [], "entities": []}, {"text": "We demonstrate how our design choices enabled us to rapidly develop useful applications via extensive reuse of state-of-the-art analysis tools.", "labels": [], "entities": []}, {"text": "This paper offers an approach to standardization of linguistic analysis output, and lays out guidelines for Indic document metadata design and storage.", "labels": [], "entities": [{"text": "standardization of linguistic analysis output", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.8264327049255371}, {"text": "Indic document metadata design", "start_pos": 108, "end_pos": 138, "type": "TASK", "confidence": 0.8186403065919876}]}], "introductionContent": [{"text": "There is growing interest and activity in applying computing technology to unearth the knowledge content of India's heritage literature embedded in Indic languages due to its perceived value to modern society.", "labels": [], "entities": []}, {"text": "This has led to several research efforts to produce analysis tools for Indic language content at various levels -text, syntax, semantics and meaning.", "labels": [], "entities": []}, {"text": "Many of these efforts have so far been addressing algorithmic issues in specific linguistic analysis problems.", "labels": [], "entities": []}, {"text": "However, as the tools mature and proliferate, it becomes imperative to make them interoperable for higher order document analytics involving larger document sets with high performance.", "labels": [], "entities": []}, {"text": "We categorize existing tools for Indic knowledge processing into three buckets -media-to-text (e.g., OCR (image to text), speech recognition (audio to text)), text-to-concept (e.g., syntax-, semanticsand discourse analysis), and concept-to-insight (e.g., knowledge search, mining, inference and decision-making).", "labels": [], "entities": [{"text": "Indic knowledge processing", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.9310958782831827}, {"text": "speech recognition (audio to text))", "start_pos": 122, "end_pos": 157, "type": "TASK", "confidence": 0.8223507489476886}]}, {"text": "For instance, though several alternative linguistic tools exist for Samskrit text analysis (morphological analysis, grammatical checking), they use custom formats to represent input text and analysis outcome, mainly designed for direct human consumption, and not for further machine-processing.", "labels": [], "entities": [{"text": "Samskrit text analysis", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.9256093700726827}, {"text": "grammatical checking", "start_pos": 116, "end_pos": 136, "type": "TASK", "confidence": 0.6993882656097412}]}, {"text": "This inhibits the use of those tools to build end-user applications for cross-correlating texts, glossary indices, concept search etc.", "labels": [], "entities": []}, {"text": "On the other hand, the number of Heritage Indic documents yet to be explored is staggering.", "labels": [], "entities": [{"text": "Heritage Indic documents", "start_pos": 33, "end_pos": 57, "type": "DATASET", "confidence": 0.8001874685287476}]}, {"text": "Data from National Mission for Manuscripts NAMAMI indicate that there are more than 5 million palm leaf manuscripts that are scanned but not catalogued for content, let alone converted into Unicode text to facilitate search.", "labels": [], "entities": [{"text": "National Mission for Manuscripts NAMAMI", "start_pos": 10, "end_pos": 49, "type": "DATASET", "confidence": 0.8522820353507996}]}, {"text": "This is in contrast to less than a million in the rest of the world combined before the advent of print era.", "labels": [], "entities": []}, {"text": "In addition, The Internet Archive project Archive.org (2019) has a huge collection of scanned printed Indic books.", "labels": [], "entities": [{"text": "Internet Archive project Archive.org", "start_pos": 17, "end_pos": 53, "type": "DATASET", "confidence": 0.7912894710898399}]}, {"text": "Very few of them have been converted to text.", "labels": [], "entities": []}, {"text": "There are also thousands of online Unicode Samskrit documents yet to be analyzed linguistically for knowledge mining.", "labels": [], "entities": [{"text": "knowledge mining", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7836041450500488}]}, {"text": "Use of technology is a must to address this scale.", "labels": [], "entities": []}, {"text": "We believe that to take Indic knowledge exploration to the next level, there needs to be a systematic, end-to-end, interoperability-driven architectural effort to store, exchange, parse, analyze and mine Indic documents at large scale.", "labels": [], "entities": [{"text": "Indic knowledge exploration", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9068741003672282}]}, {"text": "Due to lack of standardized data representation and machine interfaces for tools, Indic document analysis is unable to leverage numerous advances in data analytics that are already available for English and other languages.", "labels": [], "entities": [{"text": "Indic document analysis", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8781584898630778}]}, {"text": "Moreover, Indic documents pose unique challenges for processing compared to other ancient document collections due to the unbroken continuity of Indic knowledge tradition spanning more than two thousand years.", "labels": [], "entities": []}, {"text": "First, avast majority of them are handwritten or in often poorly scanned archaic printed modes in dozens of languages, more than thirty evolving scripts and diverse media.", "labels": [], "entities": []}, {"text": "Existing linguistic platforms are inadequate to handle their complexity and diversity.", "labels": [], "entities": []}, {"text": "Second, human feedback and correction in a community-sourced mode is essential to curate Indic document content at scale for further machine processing.", "labels": [], "entities": []}, {"text": "But the architecture of many existing tools is not amenable to incorporating human input and adapting to it.", "labels": [], "entities": []}, {"text": "Finally, Indic knowledge collections and processing tools are fragmented across multiple organizations and administrative boundaries.", "labels": [], "entities": [{"text": "Indic knowledge collections", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.8294697006543478}]}, {"text": "Hence a centralized approach to user authentication, access control and accounting will not be acceptable.", "labels": [], "entities": []}, {"text": "To overcome these challenges, this paper presents Vedavaapi, a novel platform architecture for community-sourced Indic document processing to transform digitized raw Indic content into machine-interpretable knowledge base.", "labels": [], "entities": [{"text": "Indic document processing", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.6505782008171082}]}, {"text": "Through Vedavaapi this paper makes the following contributions to facilitate large-scale Indic knowledge processing: 1.", "labels": [], "entities": [{"text": "Indic knowledge processing", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.8109232584635416}]}, {"text": "A federated RESTful service architecture to support dynamic Indic knowledge processing workflows by leveraging independently evolving services, where each service can be deployed and scaled independently to handle load.", "labels": [], "entities": [{"text": "Indic knowledge processing workflows", "start_pos": 60, "end_pos": 96, "type": "TASK", "confidence": 0.6934845745563507}]}, {"text": "2. A canonical object model to represent document analytics output that enables interoperability between multiple tools in the document processing pipeline and also transparent integration of human feedback at each stage without modifying the tools themselves.", "labels": [], "entities": []}, {"text": "3. A NoSQL-based object store that supports self-describing, versioned schemas to help tool and data evolution overtime.", "labels": [], "entities": [{"text": "data evolution", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.6935201436281204}]}, {"text": "4. A uniform, hierarchical security and access control model for users and object collections that supports decentralization of policies for flexible management across organizational boundaries.", "labels": [], "entities": []}, {"text": "This model also allows individual tool providers to meter usage for chargeback to end-users.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 3, we define the problem of Indic Knowledge processing, its requirements and the scope of our work.", "labels": [], "entities": [{"text": "Indic Knowledge processing", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.8625051776568095}]}, {"text": "In Section 4, we illustrate the challenges in the use of existing tools for Indic knowledge processing to motivate our work.", "labels": [], "entities": [{"text": "Indic knowledge processing", "start_pos": 76, "end_pos": 102, "type": "TASK", "confidence": 0.8238446911176046}]}, {"text": "In Section 5, we present the principles that guide the design of our solution Vedavaapi.", "labels": [], "entities": []}, {"text": "In Section 6, we describe the key architectural aspects of Vedavaapi including its object model, security model and deployment.", "labels": [], "entities": []}, {"text": "In Section 7, we present an overview of our current implementation and a qualitative evaluation against our objectives.", "labels": [], "entities": []}, {"text": "In Section 8, we outline ideas for future work and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have implemented most of the core Vedavaapi functionality in Python using Flask web services framework to provide RESTful API access.", "labels": [], "entities": [{"text": "Flask web services framework", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.8341196030378342}]}, {"text": "The object store is implemented as a python wrapper around MongoDB.", "labels": [], "entities": [{"text": "MongoDB", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9641668200492859}]}, {"text": "The wrapper provides schema validation, user access control and multi-hop navigational queries on the raw objects stored in MongoDB database.", "labels": [], "entities": [{"text": "MongoDB database", "start_pos": 124, "end_pos": 140, "type": "DATASET", "confidence": 0.9550579190254211}]}, {"text": "We have implemented the Vedavaapi web dashboard as a standalone AngularJS application that can connect to multiple Vedavaapi sites via their API.", "labels": [], "entities": [{"text": "Vedavaapi web dashboard", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.8463409940401713}]}, {"text": "The objective of the Vedavaapi platform is to facilitate leveraging existing tools to rapidly create larger and effective IKP workflows.", "labels": [], "entities": []}, {"text": "To evaluate how well our architecture achieves this objective, we have repackaged several existing open-source and private software modules to create an image-to-text conversion pipeline for scanned Indic documents -both printed and handwritten ones.", "labels": [], "entities": []}, {"text": "Unlike existing OCR solutions, our solution enables human intervention to compensate for machine errors as well as OCR retraining for improved effectiveness.", "labels": [], "entities": [{"text": "OCR retraining", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.8686637282371521}]}, {"text": "To do so, we have ported the following existing tools to run as IKP services in the Vedavaapi ecosystem:: Screenshot of Vedavaapi library view showing books imported from archive.org via IIIF importer.", "labels": [], "entities": []}, {"text": "\u2022 IIIF Book importer: This service imports layout and page information of scanned books uploaded to large digitized archives including https://archive.org/.", "labels": [], "entities": [{"text": "IIIF Book importer", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.70877738793691}]}, {"text": "We wrote a python library with a Flask API to import an entire scanned book from archive.org from its url as a Vedavaapi resource hierarchy.", "labels": [], "entities": [{"text": "Flask", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9808924794197083}]}, {"text": "This way, we can offer IKP services on scanned books stored elsewhere.", "labels": [], "entities": []}, {"text": "This took a couple of days of development effort, as Vedavaapi object schema was expressive enough to incorporate their metadata.", "labels": [], "entities": []}, {"text": "shows a screenshot of a book imported via this service.", "labels": [], "entities": []}, {"text": "\u2022 Mirador Book Annotator: Then we ported a sophisticated open-source book viewer and annotator web application (written in JavaScript) called Mirador to operate on Vedavaapihosted books.", "labels": [], "entities": [{"text": "Mirador Book Annotator", "start_pos": 2, "end_pos": 24, "type": "DATASET", "confidence": 0.8258185386657715}]}, {"text": "We achieved this by using our Vedavaapi client-side adapter library in JavaScript as a plugin to Mirador to source its book information and serve it from our site.", "labels": [], "entities": [{"text": "Mirador", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9426397681236267}]}, {"text": "Mirador has a built-in annotation facility that lets users manually identify text segments and also optionally transcript the text.", "labels": [], "entities": [{"text": "Mirador", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9494286775588989}]}, {"text": "We added persistence by storing those annotation on Vedavaapi backend site.", "labels": [], "entities": [{"text": "Vedavaapi backend site", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.9556924700737}]}, {"text": "This took one week of effort.", "labels": [], "entities": []}, {"text": "\u2022 Indic OCR Tools: OCR tools such as Tesseract and Google Vision API service provide both segmentation as well as text recognition from images in an XML-based standard format called hOCR.", "labels": [], "entities": [{"text": "text recognition from images", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.807876244187355}]}, {"text": "We created a wrapper service around them to import and export hOCR formatted data as annotations in Vedavaapi.", "labels": [], "entities": [{"text": "Vedavaapi", "start_pos": 100, "end_pos": 109, "type": "DATASET", "confidence": 0.881073534488678}]}, {"text": "We added a plugin to Mirador to invoke a user-selected OCR service to pre-detect words of a scanned page.", "labels": [], "entities": [{"text": "Mirador", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9417940974235535}]}, {"text": "This took one week of effort and greatly helped jumpstart text conversion for many printed texts available publicly.", "labels": [], "entities": [{"text": "text conversion", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.8056684732437134}]}, {"text": "\u2022 hOCR Editor: We ported an open-source web-based text editor for HOCR-formatted output to ease user experience in text conversion compared to Mirador.", "labels": [], "entities": [{"text": "text conversion", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.720143049955368}]}, {"text": "With our hOCR importer and exporter libraries already in place, this step took a day of effort, mainly to persist edits incrementally on Vedavaapi site.", "labels": [], "entities": [{"text": "Vedavaapi site", "start_pos": 137, "end_pos": 151, "type": "DATASET", "confidence": 0.9561432600021362}]}, {"text": "shows a screenshot of a post-OCR editing session.", "labels": [], "entities": []}, {"text": "We imported an 800-page book called \"Halayudha Kosha\" from archive.org using IIIF importer application into Vedavaapi.", "labels": [], "entities": [{"text": "Vedavaapi", "start_pos": 108, "end_pos": 117, "type": "DATASET", "confidence": 0.9015533924102783}]}, {"text": "We then invoked Tesseract OCR on the 10th page.", "labels": [], "entities": [{"text": "Tesseract OCR", "start_pos": 16, "end_pos": 29, "type": "DATASET", "confidence": 0.7982079088687897}]}, {"text": "We opened the OCR output using the hOCR editor as shown in the figure.", "labels": [], "entities": []}, {"text": "With these applications integrated with Vedavaapi platform, we got a complete solution for text conversion of archive.org books using OCR tools as well as crowd-sourced human correction working within 2 weeks.", "labels": [], "entities": [{"text": "text conversion of archive.org books", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.8111930906772613}, {"text": "crowd-sourced human correction", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.6741624474525452}]}, {"text": "However, the layout detection of existing OCR tools on hand-written palm leaf manuscripts is poor due to irregular and overlapping lines in such documents.", "labels": [], "entities": []}, {"text": "In parallel, a research group at IIIT Hyderabad developed a deep-learning-based layout detector for palm leaf manuscripts called Indiscapes that automatically draws polygons around lines of text, holes, images and other artifacts by training on manual shape annotations.", "labels": [], "entities": []}, {"text": "It requires a machine with GPU for the training step.", "labels": [], "entities": []}, {"text": "\u2022 Layout Detector for Palm leaf Manuscripts: Hence we have created a palm leaf layout detector based on IIIT Hyderabad tool.", "labels": [], "entities": [{"text": "IIIT Hyderabad tool", "start_pos": 104, "end_pos": 123, "type": "DATASET", "confidence": 0.9415934085845947}]}, {"text": "It takes a page image URL from a Vedavaapi site, detects line segments and posts them back as annotations to that page on Vedavaapi with empty text label.", "labels": [], "entities": [{"text": "Vedavaapi", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.9508838653564453}]}, {"text": "The training model file is maintained at IIIT Hyderabad, while the detector runs as Vedavaapi service.", "labels": [], "entities": [{"text": "IIIT Hyderabad", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.7895606458187103}]}, {"text": "Subsequently, we were able to use the hOCR editor to type the text manually, thereby creating a crowd-sourced workflow for online transcription of hand-written text.", "labels": [], "entities": []}, {"text": "Porting the tool to Vedavaapi took 2 days of effort as most of the functionality was in place.", "labels": [], "entities": [{"text": "Vedavaapi", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.8608279824256897}]}, {"text": "shows the screenshot of this service running from within Vedavaapi dashboard.", "labels": [], "entities": [{"text": "Vedavaapi dashboard", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.947166383266449}]}, {"text": "\u2022 Samsaadhanii Linguistic Toolkit: We are currently in the process of incorporating Samsaadhani toolset as an IKP service to be invoked on Vedavaapi-hosted Samskrit text data.", "labels": [], "entities": []}, {"text": "This will test Vedavaapi's ability to leverage community-sourcing to eliminate ambiguity in linguistic analysis output.", "labels": [], "entities": []}, {"text": "This is still a work in progress.", "labels": [], "entities": []}], "tableCaptions": []}