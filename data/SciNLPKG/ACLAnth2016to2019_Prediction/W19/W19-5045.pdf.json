{"title": [{"text": "KU ai at MEDIQA 2019: Domain-specific Pre-training and Transfer Learning for Medical NLI", "labels": [], "entities": [{"text": "KU ai at MEDIQA 2019", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.6325853407382965}, {"text": "Medical NLI", "start_pos": 77, "end_pos": 88, "type": "TASK", "confidence": 0.6103731989860535}]}], "abstractContent": [{"text": "In this paper, we describe our system and results submitted for the Natural Language Inference (NLI) track of the MEDIQA 2019 Shared Task (Ben Abacha et al., 2019).", "labels": [], "entities": [{"text": "MEDIQA 2019 Shared Task (Ben Abacha et al., 2019)", "start_pos": 114, "end_pos": 163, "type": "DATASET", "confidence": 0.6489609827597936}]}, {"text": "As KU ai team, we used BERT (Devlin et al., 2018) as our baseline model and pre-processed the MedNLI dataset to mitigate the negative impact of de-identification artifacts.", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9976992011070251}, {"text": "MedNLI dataset", "start_pos": 94, "end_pos": 108, "type": "DATASET", "confidence": 0.9474144577980042}]}, {"text": "Moreover , we investigated different pre-training and transfer learning approaches to improve the performance.", "labels": [], "entities": []}, {"text": "We show that pre-training the language model on rich biomedical corpora has a significant effect in teaching the model domain-specific language.", "labels": [], "entities": []}, {"text": "In addition, training the model on large NLI datasets such as MultiNLI and SNLI helps in learning task-specific reasoning.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9490939974784851}]}, {"text": "Finally, we ensembled our highest-performing models, and achieved 84.7% accuracy on the unseen test dataset and ranked 10 th out of 17 teams in the official results .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.993070662021637}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) is one of the central problems in artificial intelligence.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7858989387750626}]}, {"text": "It requires understanding two input sentences and forming an inference relationship between them.", "labels": [], "entities": []}, {"text": "Concretely, given a premise sentence p, and a hypothesis sentence h, NLI is the task of determining the inference relationship from p to h.", "labels": [], "entities": []}, {"text": "In MedNLI, this relationship is one of the neutral, entailment and contradiction labels.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.9080722332000732}]}, {"text": "Therefore, our task can be considered as a three-class sentence pair classification problem.", "labels": [], "entities": [{"text": "three-class sentence pair classification", "start_pos": 43, "end_pos": 83, "type": "TASK", "confidence": 0.6557044759392738}]}, {"text": "In previous research, sequence encoders connected with a classifier head have been commonly used as NLI systems (.", "labels": [], "entities": []}, {"text": "Traditionally, the encoder layer has been an RNN-based model such as LSTM (Hochreiter and).", "labels": [], "entities": []}, {"text": "proposed the Transformer as an alternative model to the RNN.", "labels": [], "entities": [{"text": "RNN", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8813446164131165}]}, {"text": "Since the Transformer is based on a self-attention mechanism rather than recurrent layers, it is much faster to train in parallel and can capture distant dependencies better.", "labels": [], "entities": []}, {"text": "Therefore, the recent models originated from Transformer replaced the RNNbased encoders in many systems trained for natural language understanding tasks such as NLI, Question Answering, Common Sense Reasoning (, and Neural Machine Translation (.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.7650090008974075}, {"text": "Question Answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8456515073776245}, {"text": "Common Sense Reasoning", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.695088803768158}, {"text": "Neural Machine Translation", "start_pos": 216, "end_pos": 242, "type": "TASK", "confidence": 0.7451884945233663}]}, {"text": "As a Transformer based model, BERT uses self-attention to capture the relationships within the text during encoding, which can include one or more sentences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9054767489433289}]}, {"text": "Therefore, it can learn a joint representation fora premise-hypothesis pair, which can be fed to a classifier layer to predict the inference relation between them.", "labels": [], "entities": []}, {"text": "Recent studies have explored different ways of inference prediction instead of a straightforward classifier layer.", "labels": [], "entities": [{"text": "inference prediction", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.7210302650928497}]}, {"text": "proposed to use an answer module performing multi-step inference by iteratively refining its prediction.", "labels": [], "entities": []}, {"text": "Likewise, models that aim to solve multiple problems simultaneously have gained attention due to their impressive performances (.", "labels": [], "entities": []}, {"text": "However, we concentrated on a single task, and wanted to keep the prediction layer simple.", "labels": [], "entities": []}, {"text": "Therefore, we used neither of these approaches.", "labels": [], "entities": []}, {"text": "To succeed in NLI, a system must have strong reasoning skills and a good understanding of the language.", "labels": [], "entities": []}, {"text": "If a large annotated dataset is available, the system can be trained from scratch for NLI, learning both the language and reasoning simultaneously.", "labels": [], "entities": []}, {"text": "However, such data is often not available.", "labels": [], "entities": []}, {"text": "Without seeing many syntactic variation and inference relation combinations, learning both is a hard task.", "labels": [], "entities": []}, {"text": "Separating the two by training a language model first, and adjusting it for NLI later is a more effective approach.", "labels": [], "entities": []}, {"text": "Due to the development of powerful language models that can be trained on unlabeled data in an unsupervised manner, many pre-trained contextaware encoders such as BERT ( and GPT ( are publicly available.", "labels": [], "entities": [{"text": "BERT", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9666918516159058}, {"text": "GPT", "start_pos": 174, "end_pos": 177, "type": "DATASET", "confidence": 0.9490386843681335}]}, {"text": "Most notably, showed that BERT can be effectively used on many natural language understanding tasks, including NLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9969243407249451}, {"text": "natural language understanding tasks", "start_pos": 63, "end_pos": 99, "type": "TASK", "confidence": 0.7483285218477249}]}, {"text": "Combining a pre-trained BERT encoder with a task-specific head, and then fine-tuning the entire model on the target task achieved state-of-the-art results on a number of tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.842263400554657}]}, {"text": "Directly applying BERT to NLI yields high accuracy if the dataset is large, and from a general domain (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9894161820411682}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9989235997200012}]}, {"text": "However, the dataset used in this shared task, MedNLI, is based on clinical notes (i.e. patient histories) and limited by size, thus it is a particularly challenging NLI task.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.836479902267456}]}, {"text": "To address this problem, we started with BERT, trained it on large NLI datasets, such as MultiNLI () and SNLI (, to support the inference reasoning, and then trained it further on our target task, MedNLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9904494881629944}, {"text": "MedNLI", "start_pos": 197, "end_pos": 203, "type": "DATASET", "confidence": 0.9272961020469666}]}, {"text": "This kind of intermediate training was shown to be effective when BERT is trained on a target with limited data (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9369993209838867}]}, {"text": "We call our approach twostage transfer learning.", "labels": [], "entities": []}, {"text": "In the first stage, we transfer the knowledge of the task, NLI, into our pretrained model.", "labels": [], "entities": []}, {"text": "During the second stage, we specialize our model on the MedNLI dataset.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9731882512569427}]}, {"text": "We hypothesized that this approach will produce higher accuracy on MedNLI compared to direct application of BERT.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9992764592170715}, {"text": "MedNLI", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9203048348426819}, {"text": "BERT", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.982361376285553}]}, {"text": "By conducting extensive experiments, we explored the impact of different pre-trained model weights and transfer learning strategies.", "labels": [], "entities": []}, {"text": "As a result, we signficantly improved the performance of BERT on MedNLI by initializing it from weights pre-trained on corpora close to clinical domain and applying two-stage transfer learning.", "labels": [], "entities": [{"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9979460835456848}, {"text": "MedNLI", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.9293062686920166}]}], "datasetContent": [{"text": "The main dataset used in this shared task is MedNLI.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 45, "end_pos": 51, "type": "DATASET", "confidence": 0.9323939085006714}]}, {"text": "Additionally, we used MultiNLI and SNLI to improve our accuracy via transfer learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9984157085418701}]}, {"text": "We have conducted a number of experiments to test our model and the effectiveness of different training strategies.", "labels": [], "entities": []}, {"text": "We report the resulting percent accuracies on the MedNLI development dataset.", "labels": [], "entities": [{"text": "MedNLI development dataset", "start_pos": 50, "end_pos": 76, "type": "DATASET", "confidence": 0.9544867078463236}]}, {"text": "To start with, we trained our model on MedNLI directly.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 39, "end_pos": 45, "type": "DATASET", "confidence": 0.9524658918380737}]}, {"text": "We initialized it with various pre-trained weights to compare their performances.", "labels": [], "entities": []}, {"text": "The results of the experiment can be seen from.", "labels": [], "entities": []}, {"text": "It shows that the weights pre-trained on domains related to the task, such as biomedical or scientific data, have a noticeable advantage over the weights obtained from general text corpora, such as Wikipedia.", "labels": [], "entities": []}, {"text": "Moreover, the effect of pre-training is more significant compared to the model size.", "labels": [], "entities": []}, {"text": "BioBERT and SciBERT surpassed BERT LARGE although they are three times smaller.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.6899301409721375}, {"text": "BERT LARGE", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.8263921737670898}]}, {"text": "Next, we tested the two-stage sequential transfer learning method using all combinations of pretrained weights and rich NLI datasets mentioned before.", "labels": [], "entities": [{"text": "sequential transfer learning", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7702819307645162}]}, {"text": "shows the results of this experiment.", "labels": [], "entities": []}, {"text": "As expected, all models benefit from twostage transfer learning.", "labels": [], "entities": []}, {"text": "Moreover, the trend of specialized pre-trained weights having an advantage continues on this experiment as well.", "labels": [], "entities": []}, {"text": "However, BioBERT outperforms SciBERT, unlike the pre-   vious results.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9920598268508911}]}, {"text": "We suspect that since BioBERT is pre-trained starting from BERT BASE , it benefits more from general domain task training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9946151375770569}, {"text": "BASE", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.5600454211235046}]}, {"text": "In order to test the effect of pre-processing described in Section 3.2, we repeated the twostage training experiment without removing the de-identification artifacts.", "labels": [], "entities": []}, {"text": "We compared the results of this experiment with the previous results to see how the accuracy is affected.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9996825456619263}]}, {"text": "The improvements obtained from the pre-processing can be found in.", "labels": [], "entities": []}, {"text": "It increases the accuracy in all cases, except for BERT BASE -SNLI, where the accuracy is unchanged.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995705485343933}, {"text": "BERT BASE -SNLI", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.8628102689981461}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9995853304862976}]}, {"text": "Note that all other experiments are conducted with the pre-processing is enabled.", "labels": [], "entities": []}, {"text": "We have also tested the performance of threestage sequential transfer learning on BioBERT.", "labels": [], "entities": [{"text": "threestage sequential transfer learning", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.6581299155950546}, {"text": "BioBERT", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9184460639953613}]}, {"text": "Training on SNLI and MultiNLI sequentially before MedNLI produced lower accuracies compared to the two-stage transfer learning experiment.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8914597034454346}, {"text": "accuracies", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9805352091789246}]}, {"text": "Moreover, switching the training order of SNLI and MultiNLI did not change the resulting accuracy.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9004238843917847}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9988210797309875}]}, {"text": "We suspect that further training on a second intermediate dataset brings the model closer to a worse local optimum.", "labels": [], "entities": []}, {"text": "A comparison between different training strategies on BioBERT can be found in.", "labels": [], "entities": []}, {"text": "Lastly, we experimented with ensembling.", "labels": [], "entities": []}, {"text": "We tested four ensemble models, one for each pre-   trained BERT variant.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7575588226318359}]}, {"text": "For each of these starting points, we combined the two models obtained from the two-stage transfer learning experiment.", "labels": [], "entities": []}, {"text": "One of these models is trained with MultiNLI, the other with SNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8750895857810974}, {"text": "SNLI", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8519859910011292}]}, {"text": "We tried three different elementwise operations for ensembling, and compared their effects.", "labels": [], "entities": []}, {"text": "shows that this approach yielded better results compared to the two-stage transfer methods.", "labels": [], "entities": []}, {"text": "Therefore, we effectively combined the benefits of training on different, rich datasets.", "labels": [], "entities": []}, {"text": "Among all the experiments, the best result we obtained is 86.1% accuracy by ensembling BioBERT models trained with two-stage transfer learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9996028542518616}]}, {"text": "That accuracy was obtained by combining the output probabilities with element-wise sum operation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9996511936187744}]}, {"text": "Therefore, we participated in the shared task with that ensembled model.", "labels": [], "entities": []}, {"text": "Consequently, our model achieved 84.7% accuracy on the unseen test dataset reserved for the shared task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9993083477020264}]}], "tableCaptions": [{"text": " Table 6: The hyperparameters for different training settings.", "labels": [], "entities": []}, {"text": " Table 8: Development set accuracies achieved by per- forming two-stage sequential transfer learning, utiliz- ing different intermediary datasets, and starting from  various pre-trained weights. The highest accuracy is  indicated with bold.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9990715980529785}]}, {"text": " Table 10: Single model development set accuracies  of different training strategies, starting from BioBERT.  The highest accuracy is indicated with bold.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.7270289063453674}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9990537762641907}]}, {"text": " Table 11: Development set accuracies resulting from  ensembling two models starting from the same pre- trained weights. The highest accuracies are indicated  with bold.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9794452786445618}]}, {"text": " Table 12: Label breakdown of errors made in MedNLI development set by different models. All models were  trained with different strategies starting from BioBERT. \"CON\", \"ENT\" and \"NTR\" label abbreviations mean  \"Contradiction\", \"Entailment\", and \"Neutral\" respectively.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.6037899851799011}, {"text": "CON", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9153597950935364}, {"text": "ENT", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.7514346241950989}]}, {"text": " Table 13: Category breakdown of errors made in MedNLI development set by different models. All models  were trained with different strategies starting from BioBERT. The category abbreviations mean \"Abbreviation\",  \"Medical Knowledge\", \"Numerical Reasoning\", and \"World Knowledge\" respectively.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 157, "end_pos": 164, "type": "DATASET", "confidence": 0.5445351600646973}]}]}