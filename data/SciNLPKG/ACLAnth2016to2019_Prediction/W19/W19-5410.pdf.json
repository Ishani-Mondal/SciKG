{"title": [{"text": "Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings", "labels": [], "entities": [{"text": "Translation Metrics", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.9282946884632111}]}], "abstractContent": [{"text": "We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation.", "labels": [], "entities": [{"text": "sentence-level quality estimation", "start_pos": 83, "end_pos": 116, "type": "TASK", "confidence": 0.6117245058218638}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.6991859972476959}]}, {"text": "In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9912347793579102}, {"text": "LASER", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.7946934103965759}]}, {"text": "In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.8310719847679138}]}, {"text": "Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predict-ing human judgements of translation quality).", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation () aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU () or TER ().", "labels": [], "entities": [{"text": "Quality estimation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6334168910980225}, {"text": "machine translation (MT) outputs", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.8469584981600443}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9945054650306702}, {"text": "TER", "start_pos": 181, "end_pos": 184, "type": "METRIC", "confidence": 0.9170400500297546}]}, {"text": "Most approaches to quality estimation are trained to predict the post-editing effort, i.e. the number of corrections the translators have to make in order to get an adequate translation.", "labels": [], "entities": []}, {"text": "The effort is measured by the HTER metric () applied to human post-edits.", "labels": [], "entities": [{"text": "HTER metric", "start_pos": 30, "end_pos": 41, "type": "METRIC", "confidence": 0.7569823861122131}]}, {"text": "In this paper, we introduce a light-weight neural method with pre-trained embeddings, that means it does not require any pre-training.", "labels": [], "entities": []}, {"text": "The second proposed method is the extension of the first one: besides pre-trained embeddings, it takes log probability from any MT system as an input feature.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9351198673248291}]}, {"text": "In addition to the official datasets provided for this year's WMT sentence level shared task, we analyze the performance of our methods against the extended datasets made from previous years data.", "labels": [], "entities": [{"text": "WMT sentence level shared task", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.7715570211410523}]}, {"text": "Using the extended datasets allows to get a more reliable score and avoid skewed distributions of the predicted metrics.", "labels": [], "entities": []}, {"text": "Besides that we apply our method to predict direct human assessment (DA) (.", "labels": [], "entities": [{"text": "predict direct human assessment (DA)", "start_pos": 36, "end_pos": 72, "type": "TASK", "confidence": 0.6417687833309174}]}, {"text": "In direct human assessment humans compare the machine translation output with a reference translation not seeing a source translation.", "labels": [], "entities": []}, {"text": "Usually MT metrics ( are compared to DA, but we decided to compare our predictions as well, because there is a difference between a number of post-edits and a human assessment.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.8919554948806763}, {"text": "DA", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.8481318354606628}]}, {"text": "For example, if everything in a translation is perfect except one thing: all indefinite articles are missed, the number of post-edits maybe large enough and a score will below whereas humans likely give it a high score.", "labels": [], "entities": []}, {"text": "The main difference between MT metrics and quality estimation is that quality estimation is computing without reference sentences.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9859792590141296}]}], "datasetContent": [{"text": "In this section we analyze the performance of proposed methods on different prediction outputs (HTER and DA) and different datasets and compare them with another neural method DeepQuest () that does not require additional data.", "labels": [], "entities": []}, {"text": "To predict HTER we take a dataset that contains source sentences, their translated outputs and HTER scores.", "labels": [], "entities": []}, {"text": "It is domain-specific: IT or pharmaceutical depending on the language pair.", "labels": [], "entities": []}, {"text": "As there is no large enough corpus with DA labels, we use a dataset that consists only of source sentences and their machine translation output.", "labels": [], "entities": []}, {"text": "The domain of this corpus is more general and source sentences have taken from the open resources.", "labels": [], "entities": []}, {"text": "We have implemented our methods using the Keras toolkit.", "labels": [], "entities": [{"text": "Keras toolkit", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.9169085919857025}]}, {"text": "As a regression model we have used four-layered feed-forward neural network with sigmoid as a final activation function.", "labels": [], "entities": []}, {"text": "To obtain a log probability score, we trained neural MT systems using sockeye toolkit.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9047245383262634}]}, {"text": "We used Transformer () as a network architecture with six layers in encoder and decoder, word vectors of size 512, batch size 50, and Adam ( as optimizer with an initial learning rate of 0.0002.", "labels": [], "entities": []}, {"text": "We present two models with different set of features: \u2022 LABE: embeddings extracted from LASER and BERT \u2022 LABEL: embeddings extracted from LASER and BERT and log probability obtained from Transformer NMT model BERT embeddings are extracted for multilingual cased BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9835125803947449}, {"text": "BERT", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.9775150418281555}]}, {"text": "Only the last layer of embeddings is extracted.", "labels": [], "entities": []}, {"text": "BERT gives 728-dimension embeddings for each word, source and target embeddings are separated by a special token and then average pooling is used to get sentence embeddings for source and target sentences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.49322599172592163}]}], "tableCaptions": [{"text": " Table 1. As  one can see the highest values were obtained by  applying the models LABEL, but the difference of  the computing values is small. The obtained num- bers for En-De and En-Cs are close to each other  whereas the resulting coefficients for De-En are  noticeably higher. Both our models showed the  better performance than deepQuest.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.635481059551239}]}, {"text": " Table 1: Pearson and Spearman correlation coefficients  for the monolingual models LABE and LABEL, and  deepQuest . For models LABE and LABEL we  show PCC and SCC between ensemble of five runs and  HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.7541088461875916}]}, {"text": " Table 2: Pearson and Spearman correlation coefficients  for the monolingual models LABE and LABEL. Test  set: official test set of WMT19. We show PCC and  SCC between ensemble of five runs and HTER.", "labels": [], "entities": [{"text": "WMT19", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.7324457168579102}, {"text": "HTER", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.8895847201347351}]}]}