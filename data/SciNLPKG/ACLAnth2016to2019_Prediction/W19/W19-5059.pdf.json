{"title": [{"text": "ARS NITK at MEDIQA 2019:Analysing Various Methods for Natural Language Inference, Recognising Question Entailment and Medical Question Answering System", "labels": [], "entities": [{"text": "ARS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9313250780105591}, {"text": "NITK", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.4749336540699005}, {"text": "MEDIQA 2019", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8056499063968658}, {"text": "Natural Language Inference", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6645581622918447}, {"text": "Recognising Question Entailment", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.865437388420105}, {"text": "Medical Question Answering", "start_pos": 118, "end_pos": 144, "type": "TASK", "confidence": 0.6253246665000916}]}], "abstractContent": [{"text": "This paper includes approaches we have taken for Natural Language Inference, Question Entailment Recognition and Question-Answering tasks to improve domain-specific Information Retrieval.", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.6190701921780905}, {"text": "Question Entailment Recognition", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.853185753027598}, {"text": "domain-specific Information Retrieval", "start_pos": 149, "end_pos": 186, "type": "TASK", "confidence": 0.5974860886732737}]}, {"text": "Natural Language Inference (NLI) is a task that aims to determine if a given hypothesis is an entailment, contradiction or is neutral to the given premise.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8036627322435379}]}, {"text": "Recognizing Question Entailment (RQE) focuses on identifying entailment between two questions while the objective of Question-Answering (QA) is to filter and improve the ranking of automatically retrieved answers.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8502306540807089}]}, {"text": "For addressing the NLI task, the UMLS Metathesaurus was used to find the synonyms of medical terms in given sentences, on which the InferSent model was trained to predict if the given sentence is an entailment, contradictory or neutral.", "labels": [], "entities": [{"text": "UMLS Metathesaurus", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.903875321149826}]}, {"text": "We also introduce anew Extreme gradient boosting model built on PubMed em-beddings to perform RQE.", "labels": [], "entities": [{"text": "PubMed em-beddings", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9631026685237885}, {"text": "RQE", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.5726454257965088}]}, {"text": "Further, a closed-domain Question Answering technique that uses Bi-directional LSTMs trained on the SquAD dataset to determine relevant ranks of answers fora given question is also discussed.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8125273585319519}, {"text": "Bi-directional", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9460405707359314}, {"text": "SquAD dataset", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9710099995136261}]}, {"text": "Experimental validation showed that the proposed models achieved promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent studies have shown that patient-specific data can be utilized for the development of intelligent Healthcare Information Management Systems (HIMS), that support a wide range of supporting applications that enhance healthcare delivery platforms.", "labels": [], "entities": []}, {"text": "The application of natural language processing, sophisticated data modeling, and predictive algorithms make it a highly interesting area of research.", "labels": [], "entities": []}, {"text": "Patient data is continuously generated in large volume and variety, given the multiple modalities, it is available in (e.g., discharge summaries, physician's notes, clinical reports, lab reports etc).", "labels": [], "entities": []}, {"text": "With an abundance of such diverse information sources available in the medical domain, sophisticated solutions that can adapt to the heterogeneity and specific manifold nature of health-related information area critical requirement for HIMS development.", "labels": [], "entities": [{"text": "HIMS", "start_pos": 236, "end_pos": 240, "type": "TASK", "confidence": 0.9329242706298828}]}, {"text": "In clinical text, a commonly occurring problem would be to understand the correlation and association between various factors like disease, symptoms, diagnoses and treatment.", "labels": [], "entities": []}, {"text": "Clinical text is inherently unstructured and written in natural language, and hence is prone to significant issues in effective interpretability and utilization.", "labels": [], "entities": []}, {"text": "Challenges like paraphrase detection, anaphora resolution, natural language inference etc must be effectively dealt within order to extract useful knowledge that can be used to build intelligent decision support applications.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.8740577399730682}, {"text": "anaphora resolution", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6978590488433838}]}, {"text": "Such support systems require extensive evidence-based analysis, and context-sensitive processing, in order to enable higher-level functionalities like clinical question-answering.", "labels": [], "entities": []}, {"text": "Thus, dealing with such issues is paramount importance.", "labels": [], "entities": []}, {"text": "Natural Language Inference is used to determine whether a given hypothesis can be inferred from a given premise (Ben ).", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.613330751657486}]}, {"text": "The three inference relations to be identified between the statements are Entailment, Neutrality and Contradiction.", "labels": [], "entities": []}, {"text": "If a statement is a true description of the other then it is labelled Entailment.", "labels": [], "entities": []}, {"text": "If it is a false description then it is labelled Contradiction, otherwise, it is considered to be Neutral.", "labels": [], "entities": [{"text": "Contradiction", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9636556506156921}]}, {"text": "The goal of Recognizing Question Entailment(RQE) is to retrieve answers to a premise question by retrieving inferred or entailed questions, called hypothesis questions that already have associated answers.", "labels": [], "entities": [{"text": "Recognizing Question Entailment(RQE)", "start_pos": 12, "end_pos": 48, "type": "TASK", "confidence": 0.8971066077550253}]}, {"text": "Therefore, we define the entailment relation between two questions as: a question A en-tails a question B if every answer to B also correctly answers A (.", "labels": [], "entities": []}, {"text": "RQE is particularly relevant due to the increasing numbers of similar questions posted online ().", "labels": [], "entities": [{"text": "RQE", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5223339200019836}]}, {"text": "For Question Answering, the input ranks are generated by the medical QA system CHiQA.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7902345359325409}, {"text": "medical QA system CHiQA", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.5535014644265175}]}, {"text": "Extracting certain elements of a question like the question type and focus is the main approach in question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8540031909942627}]}, {"text": "If the question happens to contain multiple subquestions then an answer will be considered complete only if all sub-questions are answered.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: Section 2 presents a summarization on relevant existing research done in the area of interest.", "labels": [], "entities": []}, {"text": "We discuss the Proposed Architecture for NLI, RQE and QA in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 presents the results and performance of the various models for each task, followed by error analysis, conclusion and references.", "labels": [], "entities": [{"text": "conclusion", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9290239810943604}]}], "datasetContent": [{"text": "We performed several experiments to benchmark the relative performance of the various proposed models for the three different tasks -NLI, RQE and QA.", "labels": [], "entities": []}, {"text": "We analyzed the accuracy obtained using the standard metrics defined for the three tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993993043899536}]}, {"text": "The datasets provided from the ACL-BioNLP'19 Shared Task ) were used for the experimental studies.", "labels": [], "entities": [{"text": "ACL-BioNLP'19 Shared Task )", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.8595007956027985}]}, {"text": "For the NLI task, we used the MedNLI dataset () built on different word embeddings such as 300D GloVe embeddings () , MIMIC clinic data embeddings) , Wikipedia English embeddings, combination of Wikipedia English and MIMIC clinical data embeddings and even with the combination of 300D GloVe with BioASQ () and MIMIC embeddings.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9761106073856354}, {"text": "MIMIC clinic data embeddings", "start_pos": 118, "end_pos": 146, "type": "DATASET", "confidence": 0.7061720043420792}, {"text": "MIMIC clinical data embeddings", "start_pos": 217, "end_pos": 247, "type": "DATASET", "confidence": 0.7563870996236801}]}, {"text": "All the techniques were set with number of training epochs as 100 and were trained on GPUs.", "labels": [], "entities": []}, {"text": "The observed performance for the MediQA released test dataset is tabulated in.", "labels": [], "entities": [{"text": "MediQA released test dataset", "start_pos": 33, "end_pos": 61, "type": "DATASET", "confidence": 0.9557942599058151}]}, {"text": "The accuracy obtained with different methods is listed in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994947910308838}]}, {"text": "The RNN based model for NLI was trained for 30 epochs and a validation accuracy of 67.1% was achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8191702365875244}]}, {"text": "Further accuracy can be improved by using a Bidirectional LSTM or Bidirectional GRU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9990733861923218}, {"text": "Bidirectional GRU", "start_pos": 66, "end_pos": 83, "type": "METRIC", "confidence": 0.7911352813243866}]}, {"text": "The InferSent Model for NLI with MIMIC embeddings that were trained for 100 epochs gave an accu- In the case of the RQE task, the SVM model was trained using a few features like semantic features, bigram overlap, word movers distance and cosine similarity.", "labels": [], "entities": [{"text": "accu", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9810115098953247}, {"text": "RQE task", "start_pos": 116, "end_pos": 124, "type": "TASK", "confidence": 0.7420230209827423}]}, {"text": "An accuracy of 62% achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9996746778488159}]}, {"text": "The Logistic Regression model was trained using several handcrafted features and an accuracy of 64.5% was achieved.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6379491239786148}, {"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9997758269309998}]}, {"text": "The KNN algorithm was also used for this classification task and an accuracy of 62.4% was obtained with K=47.", "labels": [], "entities": [{"text": "classification task", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.9080643057823181}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9996587038040161}]}, {"text": "The Naive Bayes model was fine-tuned and trained using the constructed feature vector.", "labels": [], "entities": []}, {"text": "This gave an accuracy of 64%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9997933506965637}]}, {"text": "The Naive Bayes model feature vector was modified again to include a feature which will consider the content of both the questions, which improved the accuracy by 1%.", "labels": [], "entities": [{"text": "Naive Bayes model feature vector", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.7114223599433899}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9993693232536316}]}, {"text": "The AdaBoost classifier was used and this ensemble based method performed better than the naive methods and gave an accuracy of 66%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993671774864197}]}, {"text": "The XGBoost method performed the best and gave an accuracy of 66.7% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9996155500411987}]}, {"text": "As can be seen from, the closed domain question answering model gave an accuracy of 53.6% which is much above the baseline fixed at 51%.", "labels": [], "entities": [{"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7406996190547943}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9996683597564697}]}, {"text": "This accuracy was achieved because this method focuses on finding the specific answer in the given context which is more relevant to a given question.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9994441866874695}]}, {"text": "Based on the scores obtained from the closed domain model, the answers have been ranked accordingly.", "labels": [], "entities": []}, {"text": "The model achieved an accuracy of 53.6%, precision of 55.9% and Mean Reciprocal Rank of 62.93%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997908473014832}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9998440742492676}, {"text": "Mean Reciprocal Rank", "start_pos": 64, "end_pos": 84, "type": "METRIC", "confidence": 0.856268564860026}]}], "tableCaptions": [{"text": " Table 1: Performance of the InferSent model for NLI  task when different embeddings are used", "labels": [], "entities": []}, {"text": " Table 2. The RNN based model for  NLI was trained for 30 epochs and a validation ac- curacy of 67.1% was achieved. Further accuracy  can be improved by using a Bidirectional LSTM or  Bidirectional GRU. The InferSent Model for NLI  with MIMIC", "labels": [], "entities": [{"text": "validation ac- curacy", "start_pos": 71, "end_pos": 92, "type": "METRIC", "confidence": 0.8914662450551987}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9993627667427063}, {"text": "MIMIC", "start_pos": 237, "end_pos": 242, "type": "DATASET", "confidence": 0.7872973680496216}]}, {"text": " Table 2: Comparative performance of the proposed ap- proaches for the NLI, RQE and QA tasks", "labels": [], "entities": []}, {"text": " Table 3: Confusion Matrix for NLI", "labels": [], "entities": [{"text": "NLI", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.7155425548553467}]}, {"text": " Table 4: Confusion Matrix for RQE", "labels": [], "entities": [{"text": "RQE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.38550257682800293}]}]}