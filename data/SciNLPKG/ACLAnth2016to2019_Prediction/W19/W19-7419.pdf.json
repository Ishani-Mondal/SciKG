{"title": [{"text": "Compositional pre-training for neural semantic parsing", "labels": [], "entities": [{"text": "neural semantic parsing", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7275040745735168}]}], "abstractContent": [{"text": "Semantic parsing is the process of translating natural language utterances into logical forms, which has many important applications such as question answering and instruction following.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8239882588386536}, {"text": "translating natural language utterances into logical forms", "start_pos": 35, "end_pos": 93, "type": "TASK", "confidence": 0.692004131419318}, {"text": "question answering", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.8897789418697357}, {"text": "instruction following", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.7387665212154388}]}, {"text": "Sequence-to-sequence models have been very successful across many NLP tasks.", "labels": [], "entities": []}, {"text": "However , alack of task-specific prior knowledge can be detrimental to the performance of these models.", "labels": [], "entities": []}, {"text": "Prior work has used frameworks for inducing grammars over the training examples , which capture conditional independence properties that the model can leverage.", "labels": [], "entities": []}, {"text": "Inspired by the recent success stories such as BERT we set out to extend this augmentation framework into two stages.", "labels": [], "entities": [{"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.7997844219207764}]}, {"text": "The first stage is to pre-train using a corpus of augmented examples in an unsupervised manner.", "labels": [], "entities": []}, {"text": "The second stage is to fine-tune to a domain-specific task.", "labels": [], "entities": []}, {"text": "In addition, since the pre-training stage is separate from the training on the main task we also expand the universe of possible augmentations without causing catastrophic forgetting.", "labels": [], "entities": []}, {"text": "We also propose a novel data augmentation strategy that interchanges tokens that co-occur in similar contexts to produce new training pairs.", "labels": [], "entities": []}, {"text": "We demonstrate that the proposed two-stage framework is beneficial for improving the parsing accuracy in a standard dataset called GeoQuery for the task of generating logical forms from a set of questions about the US geography.", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9504213333129883}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9067292213439941}]}], "introductionContent": [{"text": "Semantic parsing is the task of converting natural language into machine-executable logical forms.", "labels": [], "entities": [{"text": "Semantic parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8046273589134216}]}, {"text": "Examples of this parsing include asking questions that are then converted to queries against a database, generating code from natural language, converting natural language instructions to an instruction set that can be followed by a system, and even converting natural language into Python (.", "labels": [], "entities": []}, {"text": "These logical forms can be captured using notions of formal semantics in linguistic such as \u03bb-calculus and a more compact version called lambda dependency-based compositional semantics or \u03bb-DCS.", "labels": [], "entities": []}, {"text": "Traditionally this task has been tackled by a combination of heuristics and search to buildup parsers from large datasets of question-answer pairs or from text that is paired with knowledge base information).", "labels": [], "entities": []}, {"text": "However, with the advent of the sequence-to-sequence () architecture, the majority of the research has shifted towards using this framework.", "labels": [], "entities": []}, {"text": "Many sequence-to-sequence use cases involve converting a sequence of natural language into another sequence of natural language.", "labels": [], "entities": []}, {"text": "Semantic parsing is different in that the decoded sequence need to be constrained by what would constitute a valid logical form.", "labels": [], "entities": []}, {"text": "This additional challenge adds extra complexity to semantic parsing systems.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.8246566355228424}]}, {"text": "Similar to more conventional sequence-tosequence tasks semantic parsing also suffers from the problem that one source sentence can have multiple valid logical forms which introduce a wrinkle in evaluation.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.9360546469688416}]}], "datasetContent": [{"text": "Parsing accuracy (described in an earlier section) is used for evaluation.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9175066947937012}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.7394421100616455}]}], "tableCaptions": [{"text": " Table 2: Augmentation and pre-training experiments", "labels": [], "entities": [{"text": "Augmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9687646627426147}]}, {"text": " Table 3: Hyper-parameter search for best augmentation and pre-train combination", "labels": [], "entities": []}]}