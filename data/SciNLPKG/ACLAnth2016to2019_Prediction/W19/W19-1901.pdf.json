{"title": [{"text": "Effective Feature Representation for Clinical Text Concept Extraction", "labels": [], "entities": [{"text": "Effective Feature Representation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6366283496220907}, {"text": "Clinical Text Concept Extraction", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.666574239730835}]}], "abstractContent": [{"text": "Crucial information about the practice of healthcare is recorded only in free-form text, which creates an enormous opportunity for high-impact NLP.", "labels": [], "entities": []}, {"text": "However, annotated health-care datasets tend to be small and expensive to obtain, which raises the question of how to make maximally efficient uses of the available data.", "labels": [], "entities": []}, {"text": "To this end, we develop an LSTM-CRF model for combining unsupervised word representations and hand-built feature representations derived from publicly available health-care ontologies.", "labels": [], "entities": []}, {"text": "We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text (clinical, social, scientific, commercial).", "labels": [], "entities": []}, {"text": "Each involves the labeling of complex, multi-word spans that pick out different healthcare concepts.", "labels": [], "entities": []}, {"text": "We also introduce anew labeled dataset for identifying the treatment relations between drugs and diseases.", "labels": [], "entities": []}], "introductionContent": [{"text": "The healthcare system generates enormous quantities of data, but its tools for analytics and decisionmaking rely overwhelmingly on a narrow subset of structured fields, especially billing codes for procedures, diagnoses, and tests.", "labels": [], "entities": []}, {"text": "The textual fields in medical records are generally under-utilized or completely ignored.", "labels": [], "entities": []}, {"text": "However, these clinical texts are our only consistent source of information on a wide variety of crucial factors -hypotheses considered and rejected, treatment rationales, obstacles to care, brand recognition, descriptions of uncertainty, social and lifestyle factors, and so forth.", "labels": [], "entities": [{"text": "brand recognition", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.7555676996707916}]}, {"text": "Such information is essential to gaining an accu-: Model diagram.", "labels": [], "entities": []}, {"text": "In our full model, words are represented by pretrained ELMo embeddings, which feed into LSTM cells, and by sparse ontology-derived feature representations, which are fed to a dense layer with dropout to produce a lower-dimensional representation that is concatenated with the hidden states of the LSTM.", "labels": [], "entities": []}, {"text": "The resulting mixed feature representation is fed into a CRF layer that forms the basis for token-level label predictions.", "labels": [], "entities": [{"text": "token-level label predictions", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.6883803009986877}]}, {"text": "We assess this full model against variants without the LSTM or hand-built features to motivate the full version.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.5557519197463989}]}, {"text": "rate picture of the healthcare system and the experiences of individual patients, creating an enormous opportunity for high-impact NLP.", "labels": [], "entities": []}, {"text": "However, annotated clinical text datasets are scarce and tend to be small, for two reasons.", "labels": [], "entities": []}, {"text": "First, data access is usually highly limited because of privacy considerations; the inherent richness of language data means that de-identification is hard or impossible ().", "labels": [], "entities": []}, {"text": "Second, because healthcare concepts are complex, the needed annotations generally must be provided by domain specialists who are trained both in the practice of healthcare and in the interpretation of healthcare records.", "labels": [], "entities": []}, {"text": "Such experts are in high demand, and the annotation work they do is intellectually challenging, so the annotated datasets they produce are, by any measure, very expensive.", "labels": [], "entities": []}, {"text": "The result is that even the largest annotated clinical text datasets are small by comparison with those from other areas of NLP, and this has profound consequences for the kinds of models that are viable in this space.", "labels": [], "entities": []}, {"text": "In this paper, we define a hybrid LSTM-CRF model that is effective for real-world clinical text datasets.", "labels": [], "entities": []}, {"text": "The architecture is sketched in figure 1.", "labels": [], "entities": []}, {"text": "Its crucial property is that it synthesizes two kinds of feature representation: dense representations that can be trained on any large text corpus (not necessarily using clinical text) and sparse, high-dimensional feature representations based on hand-built feature functions.", "labels": [], "entities": []}, {"text": "Hand-built feature functions are especially powerful in healthcare because they can leverage the numerous highquality medical lexicons and ontologies that are publicly available.", "labels": [], "entities": []}, {"text": "As a result, such features can achieve impressive coverage with relatively little additional effort.", "labels": [], "entities": [{"text": "coverage", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.834770917892456}]}, {"text": "We show that this combined model yields superior performance on five datasets of diverse kinds of healthcare text: two clinical, one social media, one scientific, and one commercial/regulatory (official drug labels).", "labels": [], "entities": []}, {"text": "Each task involves the labeling of complex, multi-word spans that pick out diverse healthcare concepts: the ChemicalDisease Relation dataset (CDR;); the Penn Adverse Drug Reaction Twitter dataset (ADR;); anew disease diagnosis dataset; anew prescription reasons dataset that involves identifying complex REASON spans for drug-prescription actions; and anew dataset of 10K drug-disease treatment descriptions, which we release with this paper.", "labels": [], "entities": [{"text": "Penn Adverse Drug Reaction Twitter dataset (ADR", "start_pos": 153, "end_pos": 200, "type": "DATASET", "confidence": 0.8467520326375961}]}], "datasetContent": [{"text": "We report experiments on five different datasets: two from transcribed clinical narratives, one from social media, one from scientific publications, and one from official FDA Drug Labels texts.", "labels": [], "entities": [{"text": "FDA Drug Labels texts", "start_pos": 171, "end_pos": 192, "type": "DATASET", "confidence": 0.6574501097202301}]}, {"text": "For each, the task is to label spans of text that identify particular healthcare concepts.", "labels": [], "entities": []}, {"text": "We are particularly interested in the capacity of our models to identify multi-word expressions in away that is sensitive to the semantics of the environment -for example, to distinguish between a drug prescribed and a drug discontinued, or to distinguish disease mentions as diagnoses, diagnostic concerns, or ruled-out diagnoses.", "labels": [], "entities": []}, {"text": "gives a short illustrative example from each dataset.", "labels": [], "entities": []}, {"text": "gives detailed statistics for each dataset.", "labels": [], "entities": []}, {"text": "Three of the datasets are already partitioned into training and test sets.", "labels": [], "entities": []}, {"text": "For these, we tune the hyperparameters using 5-fold cross-validation on the training set, train the model with tuned hyperparameters on the training set, and then evaluate the performance of the trained model on the test set.", "labels": [], "entities": []}, {"text": "The other two datasets do not have predefined splits.", "labels": [], "entities": []}, {"text": "For these, we divide them equally into five parts.", "labels": [], "entities": []}, {"text": "For each fold, the hyperparameters are tuned on the training data (also using 5-fold cross-validation), and the best model is then applied to the test data for the evaluation.", "labels": [], "entities": []}, {"text": "These experiments are repeated three times to smooth out variation deriving from the random initialization of the model parameters, though we use the hyperparameters selected for each fold in the first run in the subsequent two experiments to save computational resources.", "labels": [], "entities": []}, {"text": "We use the Adam optimizer (, with \u03b2 1 = 0.9 and \u03b2 2 = 0.999, the training batch size set to 16, and the dropout rate set to 0.5 for all the experiments.", "labels": [], "entities": []}, {"text": "The step size \u03b7 and the coefficients of the 1 and 2 regularizers c 1 and c 2 are tuned.", "labels": [], "entities": []}, {"text": "The step size is first tuned by setting both c 1 = c 2 = 0, and then c 1 and c 2 are tuned using random search (Bergstra and Bengio, 2012) for ten settings.", "labels": [], "entities": []}, {"text": "provides additional details on our hyperparameters and evaluation protocol.", "labels": [], "entities": []}, {"text": "The source code for our experiments and models is available.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Relative F1 score improvements of different labels. For each label, we give the number of supporting  examples (Support), the F1 score of our combined model, and the relative improvements over the HB-CRF model.  The F1 scores of minor labels suffer from insufficient training data, and thus have lower values. However, the  combined model shows the largest relative improvements in these categories. ADR results are shown in table A4.", "labels": [], "entities": [{"text": "Relative F1 score", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7159726818402609}, {"text": "F1 score", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.978062778711319}, {"text": "F1", "start_pos": 226, "end_pos": 228, "type": "METRIC", "confidence": 0.9815130233764648}]}]}