{"title": [{"text": "Augmenting Compositional Models for Knowledge Base Completion Using Gradient Representations", "labels": [], "entities": [{"text": "Augmenting Compositional", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9410286843776703}]}], "abstractContent": [{"text": "Neural models of Knowledge Base data have typically employed compositional representations of graph objects: entity and relation embeddings are systematically combined to evaluate the truth of a candidate Knowedge Base entry.", "labels": [], "entities": []}, {"text": "Using a model inspired by Harmonic Grammar, we propose to tokenize triplet embeddings by subjecting them to a process of optimization with respect to learned well-formedness conditions on Knowledge Base triplets.", "labels": [], "entities": []}, {"text": "The resulting model, known as Gradient Graphs, leads to sizable improvements when implemented as a companion to compositional models.", "labels": [], "entities": []}, {"text": "Also, we show that the \"supracompositional\" triplet token embeddings it produces have in-terpretable properties that prove helpful in performing inference on the resulting triplet representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "As they are conventionally analyzed, representations of semantic or linguistic data are \"compositional\": the meanings of complex representations are built up from the meanings of their constituent parts.", "labels": [], "entities": []}, {"text": "This idea has motivated numerous models of graph data deployed in knowledge base completion (KBC), in which embeddings of entities and relations are combined into composite representations-pairs of entities in a particular relation with one another-that are built up systematically from the constituent parts.", "labels": [], "entities": [{"text": "knowledge base completion (KBC)", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.8157344063123068}]}, {"text": "But what happens when the whole is not a simple function of the parts?", "labels": [], "entities": []}, {"text": "A natural case arises in the interpretation of Noun-Noun compounds.", "labels": [], "entities": []}, {"text": "The contrasting senses of vampire cat (a-cat-that-is-a-vampire) and vampire stake (a-stake-used-to-kill-a-vampire) has as much to do with the compatibility of the contituent nouns occurring in a given relation than with the meanings of the individual constituents.", "labels": [], "entities": []}, {"text": "Pursuing this line of thought, we propose Gradient Graphs, a neural network model for KBC built on the principle that compositionallyobtained representations of semantic objects can be optimized to reflect context-specific aspects of the meanings of their constituents.", "labels": [], "entities": []}, {"text": "The issue of context-conditioned, tokenized semantic representations has received little explicit attention in the KBC literature.", "labels": [], "entities": []}, {"text": "However, precedents do exist.", "labels": [], "entities": []}, {"text": "model context-sensitive entity senses by embedding relations as pairs of matrices (R lhs , R rhs ) that linearly transform entity embeddings into pairs of embeddings defined by the relation and the entities' positions within it (the lefthand-side or right-hand-side).", "labels": [], "entities": []}, {"text": "The distances of the resulting embeddings are then compared.", "labels": [], "entities": []}, {"text": "cope with the context-sensitivity of relation meanings by learning a k \u21e5d\u21e5d-dimensional tensor embeddings for each relation, letting their model represent polysemy by learning k versions of the relation represented in the k slices of its embedding tensor.", "labels": [], "entities": []}, {"text": "The intuition underlying this approach is that, for instance, the relation has part has a di\u21b5erent sense when applied to a biological organism than when predicated of a company.", "labels": [], "entities": []}, {"text": "While the former has parts like organs and limbs, the latter has parts like subsidiaries and workers, which occupy very di\u21b5erent parts of the semantic space.", "labels": [], "entities": []}, {"text": "Each relational slice is then responsible for learning the compatibility of arguments within particular semantic subspaces.", "labels": [], "entities": []}, {"text": "In contrast to these other works, our approach is more radical in the sense that our context-sensitive representations of knowledge base entries are not just computed from the entries' constituent elements (entity and relation embeddings), but are instead the result of a representation-optimization procedure that balances compositionally-derived representations with general knowledge about the characteristics of well-formed semantic structures.", "labels": [], "entities": []}, {"text": "We show that this additional \"supracompositional\" processing, in addition to yielding sizable accuracy improvements over the compositional models we apply it to, leads to embeddings of entity tokens with interpretable characteristics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9966048002243042}]}], "datasetContent": [{"text": "We evaluated Gradient Graphs using the standard WN18 and FB15K datasets ()-which are subsets of the WordNet and databases-on the Entity Reconstruction task.", "labels": [], "entities": [{"text": "WN18", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9780800342559814}, {"text": "FB15K datasets", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9229365289211273}, {"text": "WordNet", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9509449601173401}, {"text": "Entity Reconstruction task", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.8707666198412577}]}, {"text": "In Entity Reconstruction, the network ranks completions of triplets ( \u00b7 , r, er ) and (e ` , r, \u00b7 ) with deleted left and right entities.", "labels": [], "entities": [{"text": "Entity Reconstruction", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8726081252098083}]}, {"text": "The model is successful if it ranks the true triplet above other candidate completions.", "labels": [], "entities": []}, {"text": "We report results in the filtered evaluation setting (, in which a test triplet is only ranked against triplets that do not occur in the database.", "labels": [], "entities": []}, {"text": "The rank of a test triplet is thus the rank of the first correct answer to the query.", "labels": [], "entities": []}, {"text": "For both DistMult and HolE, we report the originally reported results alongside results for our reimplementations, comparing these models with our Harmonic variants HDistMult and HHolE with and without optimization of hidden layer representations.", "labels": [], "entities": []}, {"text": "The Harmonic models with \ud97b\udf59 = 1 have the Harmony function H(x, x), i.e. where the hidden representation is just the compositional embedding itself and the Faithfulness penalty in (1) is 0.", "labels": [], "entities": []}, {"text": "Our models used 256-to 512-dimensional embeddings and manually tuned values of the hyperparameter \ud97b\udf59.", "labels": [], "entities": []}, {"text": "In all models, entity and relation embeddings were normalized to kvk = 1.", "labels": [], "entities": []}, {"text": "We do not regularize parameters, but instead set an upper bound \ud97b\udf59\ud97b\udf59\u270f (\u270f a small constant) on the l 2 norm of the weight matrix W, which helps constrain the spectral norm (maximum eigenvalue) of W to remain lower than \ud97b\udf59.", "labels": [], "entities": []}, {"text": "This is equivalent to adopting a uniform prior on weight matrices lying within the n-ball with squared radius \ud97b\udf59 \ud97b\udf59 \u270f.", "labels": [], "entities": []}, {"text": "Importantly, this procedure keeps the matrix V = W \ud97b\udf59 \ud97b\udf59I negativedefinite-a necessary condition for the existence of a unique optimum for H(h, x).", "labels": [], "entities": []}, {"text": "Results from the experiments are reported in.", "labels": [], "entities": []}, {"text": "Overall, we found that models using our quadratic scoring function (1) to perform best across the board.", "labels": [], "entities": []}, {"text": "This e\u21b5ect was particularly seen in more stringent evaluation criteria-Hits@1 and Hits@3-leading to, for instance-a 15% improvement in Hits@1 (accuracy) on Freebase between our DistMult reimplementation and quadratic HDistMult (\ud97b\udf59 = 1).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9616904854774475}]}, {"text": "The bestDistMult models were those with high \ud97b\udf59 values; however, withinmodel comparison of HolE shows dramatic improvements from including the optimization component-a 32% increase in FB15K accuracy between the results of () and our HHolE with a permissive \ud97b\udf59-criterion of 1.0.", "labels": [], "entities": [{"text": "HolE", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.8079308867454529}, {"text": "FB15K", "start_pos": 183, "end_pos": 188, "type": "METRIC", "confidence": 0.9940369129180908}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.7498815655708313}]}], "tableCaptions": [{"text": " Table 3: Performance of HTPR models with and  without optimization (controlled by \ud97b\udf59). For both  models, entities were 5-dimensional and relations  20-dimensional. This trend held across other hy- perparameter settings.", "labels": [], "entities": []}]}