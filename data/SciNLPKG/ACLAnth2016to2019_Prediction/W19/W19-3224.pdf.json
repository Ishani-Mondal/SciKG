{"title": [{"text": "Detection of Adverse Drug Reaction in Tweets Using a Combination of Heterogeneous Word Embeddings", "labels": [], "entities": [{"text": "Detection of Adverse Drug Reaction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8690131545066834}]}], "abstractContent": [{"text": "This paper details our approach to the task of detecting reportage of adverse drug reaction in tweets as part of the 2019 social media mining for healthcare applications shared task.", "labels": [], "entities": [{"text": "detecting reportage of adverse drug reaction in tweets", "start_pos": 47, "end_pos": 101, "type": "TASK", "confidence": 0.7753901556134224}]}, {"text": "We employed a combination of three types of word representations as input to a LSTM model.", "labels": [], "entities": []}, {"text": "With this approach, we achieved an F1 score of 0.5209.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9863711297512054}]}], "introductionContent": [{"text": "The social media mining for healthcare applications shared task aims to provide a benchmark for validating and comparing methods for healthcare applications using social media data).", "labels": [], "entities": []}, {"text": "The focus of task 1 is on identifying adverse drug reaction as a medication related outcome.", "labels": [], "entities": []}, {"text": "Participants on this task are expected to differentiate tweets as reporting adverse drug reaction or not and the performance metric is F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 135, "end_pos": 137, "type": "METRIC", "confidence": 0.9988901019096375}]}, {"text": "This task demands that adverse drug reaction be distinguished from a similar and mostly confounding expression of the indication of a drug.", "labels": [], "entities": []}, {"text": "The former is usually associated with the usage of the drug while the latter is a specification of the reason to use a drug.", "labels": [], "entities": []}, {"text": "In addition, the task of detecting mention of adverse drug reaction is an extremely imbalanced binary classification task.", "labels": [], "entities": [{"text": "detecting mention of adverse drug reaction", "start_pos": 25, "end_pos": 67, "type": "TASK", "confidence": 0.8484166661898295}]}, {"text": "About 1% of the training set are positive examples and approximately 99% are negative examples.", "labels": [], "entities": []}, {"text": "Our approach is based on the combination of three different types of word embedding representations viz: character (, non-contextual(Glove pre-trained on Twitter data), and contextual(BERT)).", "labels": [], "entities": [{"text": "Glove pre-trained on Twitter data", "start_pos": 133, "end_pos": 166, "type": "DATASET", "confidence": 0.6128130316734314}, {"text": "BERT", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9526551961898804}]}, {"text": "The following section gives details of our model and training set-up.", "labels": [], "entities": []}, {"text": "Section 3 shows the results of our experiments while we conclude and speculate on future directions in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We hypothesize that the different types of embeddings capture different relationships and their combination could help in the identification of adverse drug reaction in tweets.", "labels": [], "entities": []}, {"text": "In our experiments, the word representation differs in two dimensions: whether they are pre-trained (Glove and Bert) or not (character embedding) and if they are contextual (Bert) or otherwise (Glove and Character embeddings).", "labels": [], "entities": []}, {"text": "We briefly describe each representation: \u2022 Character embedding -is a 50 dimensional representation of the characters in a word (how are they combined to form an embedding for the word).", "labels": [], "entities": []}, {"text": "This representation is trained together with the model.", "labels": [], "entities": []}, {"text": "It is based on a bidirectional LSTM.", "labels": [], "entities": []}, {"text": "The advantage of character-based representation for social media text is that it eliminates the out-ofvocabulary problem which results from noise in the form of misspellings and abbreviations in word-based representation such as Glove.", "labels": [], "entities": []}, {"text": "Also, this representation is specific to the task and domain of the training set.", "labels": [], "entities": []}, {"text": "\u2022 Glove (twitter) -is a 100 dimensional representation pre-trained on a huge twitter corpus.", "labels": [], "entities": []}, {"text": "We expect this to contribute by reflecting the language of twitter users.", "labels": [], "entities": []}, {"text": "However, the embedding is not a contextual one.", "labels": [], "entities": []}, {"text": "\u2022 BERT (en, base-uncased) -is a general domain contextual word representation where the representation of a word is based on other words in its context (sentence In order to leverage some of the benefits of the representations above, we concatenated these representations fora given word in a tweet.", "labels": [], "entities": [{"text": "BERT", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9981751441955566}]}, {"text": "This combination is of dimension 918.", "labels": [], "entities": []}, {"text": "A linear layer then project this representation into a dimension of 256.", "labels": [], "entities": []}, {"text": "This projection is meant to serve as a distillation step and/or as a fine-tuning step.", "labels": [], "entities": []}, {"text": "The resulting representation is fed into an LSTM layer with hidden size of 512 to sequentially model a tweet.", "labels": [], "entities": []}, {"text": "Finally, a dense layer is used as the classifier.", "labels": [], "entities": []}, {"text": "The model was trained for 100 epochs with learning rate annealing factor of 0.5 using SGD as the optimizer and a batch size of 8.", "labels": [], "entities": [{"text": "learning rate annealing factor", "start_pos": 42, "end_pos": 72, "type": "METRIC", "confidence": 0.8870209157466888}]}, {"text": "We used a train-dev split of 80:20.", "labels": [], "entities": []}, {"text": "shows the number of training, validation, and evaluation examples used in our experiment.", "labels": [], "entities": []}, {"text": "provide details on the collection and annotation of the dataset.", "labels": [], "entities": []}, {"text": "Based on the validation split, a model with the best F1 score is saved during training as the best model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.985807478427887}]}, {"text": "With the best model, we made predictions on the unseen evaluation examples as our first submission (sub1 in).", "labels": [], "entities": []}, {"text": "Our second submission (sub2 in) was based on the model at the 100th epoch or the last epoch as training is terminated if learning rate becomes too small.", "labels": [], "entities": []}, {"text": "Our experiments were performed using the Flair framework).", "labels": [], "entities": [{"text": "Flair framework", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.8760652244091034}]}, {"text": "shows the results obtained on the test set.", "labels": [], "entities": []}, {"text": "We achieved our best submission with the final model with an F1 of 0.5209.", "labels": [], "entities": [{"text": "F1", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.997158408164978}]}, {"text": "This result ranks above the average score of all participants in the task with average F1, precision, and recall of 0.5019, 0.5351, 0.5054 respectively (Weissenbacher et al., 2019).", "labels": [], "entities": [{"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.99980229139328}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9995243549346924}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9997010231018066}]}, {"text": "shows the results obtained from our ablation experiments with respect to the contributions of the different embedding representations and the distillation/finetuning step.", "labels": [], "entities": []}, {"text": "The F1 scores reported are based on the model that achieved the best F1 score on the validation set during training.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9984958171844482}, {"text": "F1 score", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9825915694236755}]}, {"text": "We observed a minimal drop in performance (0.0045) when we re-P R F1 sub1 0.6145 0.4457 0.5167 sub2 0.6203 0.4489 0.5209   moved the fine-tuning layer.", "labels": [], "entities": []}, {"text": "This suggests that the fine-tuning layer either hurts performance or the dimension of the resulting fine-tuned representation is an important parameter to tune with our approach.", "labels": [], "entities": []}, {"text": "We assessed the contribution of the three embedding representations to performance by removing one at a time from the model while keeping our fine-tuning strategy.", "labels": [], "entities": []}, {"text": "When the character embedding word representation is absent, a performance drop of 0.0238 is observed.", "labels": [], "entities": []}, {"text": "When the BERT representation is removed, the performance improved by 0.0025.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9961477518081665}]}, {"text": "Without the Glove embedding, the performance increased by 0.0005.", "labels": [], "entities": []}, {"text": "This result is consistent with our perceived advantages and disadvantages of the three embedding representations.", "labels": [], "entities": []}, {"text": "With the character embedding contributing the most to the model performance.", "labels": [], "entities": []}, {"text": "Remarkably, the removal of BERT and Glove leads to improved performance.", "labels": [], "entities": [{"text": "BERT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9990763664245605}, {"text": "Glove", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9952943921089172}]}, {"text": "This can be attributed to the out-of-vocabulary problem with Glove and domain mismatch in the case of BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.47699567675590515}]}], "tableCaptions": [{"text": " Table 1: Details of the Data", "labels": [], "entities": []}, {"text": " Table 2: Performance on the Test Set (Scores as pro- vided by the organizers)", "labels": [], "entities": [{"text": "Test Set", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.927773505449295}]}, {"text": " Table 3: Performance of Model Variants on the Valida- tion Split", "labels": [], "entities": []}]}