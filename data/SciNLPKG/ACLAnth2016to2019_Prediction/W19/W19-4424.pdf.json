{"title": [{"text": "Neural and FST-based approaches to grammatical error correction", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6285523474216461}]}], "abstractContent": [{"text": "In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction.", "labels": [], "entities": [{"text": "BEA 2019 shared task", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.7805500030517578}, {"text": "grammatical error correction", "start_pos": 73, "end_pos": 101, "type": "TASK", "confidence": 0.5277722477912903}]}, {"text": "We present a system pipeline that utilises both error detection and correction models.", "labels": [], "entities": [{"text": "error detection", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6941876858472824}]}, {"text": "The input text is first corrected by two complementary neural machine translation systems: one using convo-lutional networks and multi-task learning, and another using a neural Transformer-based system.", "labels": [], "entities": []}, {"text": "Training is performed on publicly available data, along with artificial examples generated through back-translation.", "labels": [], "entities": []}, {"text": "The n-best lists of these two machine translation systems are then combined and scored using a finite state transducer (FST).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7065738290548325}]}, {"text": "Finally, an unsupervised re-ranking system is applied to the n-best output of the FST.", "labels": [], "entities": [{"text": "FST", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8470270037651062}]}, {"text": "The re-ranker uses a number of error detection features to re-rank the FST n-best list and identify the final 1-best correction hypothesis.", "labels": [], "entities": [{"text": "re-ranker", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9342607855796814}, {"text": "FST n-best list", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.5536415775616964}]}, {"text": "Our system achieves 66.75% F 0.5 on error correction (ranking 4th), and 82.52% F 0.5 on token-level error detection (ranking 2nd) in the restricted track of the shared task.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9886208176612854}, {"text": "error correction", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7906516790390015}, {"text": "F 0.5", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9873725175857544}, {"text": "token-level error detection", "start_pos": 88, "end_pos": 115, "type": "TASK", "confidence": 0.5518289705117544}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in written text.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8254536191622416}, {"text": "automatically correcting grammatical errors in written text", "start_pos": 50, "end_pos": 109, "type": "TASK", "confidence": 0.7531312618936811}]}, {"text": "In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction, where participating teams are constrained to using only the provided datasets as training data.", "labels": [], "entities": [{"text": "BEA 2019 shared task", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.7995700687170029}, {"text": "grammatical error correction", "start_pos": 97, "end_pos": 125, "type": "TASK", "confidence": 0.5205199619134268}]}, {"text": "Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors.", "labels": [], "entities": []}, {"text": "Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (), anew annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, with a wide coverage of language proficiency levels for the latter, ranging from elementary to advanced.", "labels": [], "entities": [{"text": "GEC", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.6612766981124878}]}, {"text": "Neural machine translation (NMT) systems for GEC have drawn growing attention in recent years (), as they have been shown to achieve state-of-the-art results.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7591921935478846}, {"text": "GEC", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.7140679359436035}]}, {"text": "Within this framework, error correction is cast as a monolingual translation task, where the source is a sentence (written by a language learner) that may contain errors, and the target is its corrected counterpart in the same language.", "labels": [], "entities": [{"text": "error correction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.6902018785476685}]}, {"text": "Due to the fundamental differences between a \"true\" machine translation task and the error correction task, previous work has investigated the adaptation of NMT for the task of GEC.", "labels": [], "entities": [{"text": "machine translation task", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.8020785252253214}, {"text": "error correction task", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.7778610090414683}, {"text": "GEC", "start_pos": 177, "end_pos": 180, "type": "DATASET", "confidence": 0.917159914970398}]}, {"text": "Byte pair encoding (BPE)) and a copying mechanism () have been introduced to deal with the \"noisy\" input text in GEC and the non-standard language used by learners.", "labels": [], "entities": [{"text": "Byte pair encoding (BPE))", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7206651866436005}, {"text": "GEC", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.7822495698928833}]}, {"text": "Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives ( or by re-ranking machinetranslation-system correction hypotheses.", "labels": [], "entities": [{"text": "machinetranslation-system correction", "start_pos": 154, "end_pos": 190, "type": "TASK", "confidence": 0.6989644765853882}]}, {"text": "To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation, fluency boost learning (, and pre-training with denoising autoencoders (.", "labels": [], "entities": [{"text": "artificial error generation", "start_pos": 182, "end_pos": 209, "type": "TASK", "confidence": 0.6884116431077322}]}, {"text": "Previous work has shown that a GEC system targeting all errors may not necessarily be the best approach to the task, and that different GEC systems maybe better suited to correcting different types of errors, and can therefore be complementary.", "labels": [], "entities": []}, {"text": "As such, hybrid systems that combine different approaches have been shown to yield improved performance).", "labels": [], "entities": []}, {"text": "In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features.", "labels": [], "entities": [{"text": "NMT-based error correction", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.6254857778549194}]}, {"text": "The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 presents our official results on the shared task test set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the restricted track, participating teams were constrained to use only the provided learner datasets: 4 \u2022 Cambridge English W&I corpus \u2022 LOCNESS The LOCNESS 7 corpus (Granger, 1998) consists of essays written by native English students.", "labels": [], "entities": [{"text": "Cambridge English W&I corpus", "start_pos": 109, "end_pos": 137, "type": "DATASET", "confidence": 0.942489097515742}, {"text": "LOCNESS 7 corpus (Granger, 1998)", "start_pos": 152, "end_pos": 184, "type": "DATASET", "confidence": 0.8117262497544289}]}, {"text": "A subsection of 100 essays has been manually annotated, and equally partitioned into development and test sets.", "labels": [], "entities": []}, {"text": "\u2022 FCE \u2022 NUCLE The National University of Singapore Corpus of Learner English (NUCLE) () contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers.", "labels": [], "entities": [{"text": "FCE", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.8566099405288696}, {"text": "NUCLE The National University of Singapore Corpus of Learner English (NUCLE)", "start_pos": 8, "end_pos": 84, "type": "DATASET", "confidence": 0.8986015273974493}]}, {"text": "\u2022 Additional resources used in our system include: \u2022 English Wikipedia corpus The English Wikipedia corpus (2, 405, 972, 890 tokens in 110, 698, 467 sentences) is used to pre-train word embeddings for the convolutional neural system.", "labels": [], "entities": [{"text": "English Wikipedia corpus", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.8939401507377625}, {"text": "English Wikipedia corpus", "start_pos": 82, "end_pos": 106, "type": "DATASET", "confidence": 0.7835172315438589}]}, {"text": "We also use it as error-free native data for artificial error generation (see Section 2.1).", "labels": [], "entities": [{"text": "artificial error generation", "start_pos": 45, "end_pos": 72, "type": "TASK", "confidence": 0.6826492249965668}]}, {"text": "\u2022 In order to cover the full range of English levels and abilities, the official development set consists of 300 essays from W&I (A: 130, B:100, and C:70) and 50 essays from LOCNESS (86, 973 tokens in 4, 384 sentences).", "labels": [], "entities": [{"text": "W&I", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.9076699415842692}, {"text": "LOCNESS", "start_pos": 174, "end_pos": 181, "type": "DATASET", "confidence": 0.8719034194946289}]}, {"text": "The ERRANT scorer) is used as the official scorer for the shared task.", "labels": [], "entities": [{"text": "ERRANT scorer", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9768836796283722}]}, {"text": "System performance is evaluated in terms of spanlevel correction using F 0.5 , which emphasises precision twice as much as recall.", "labels": [], "entities": [{"text": "spanlevel correction", "start_pos": 44, "end_pos": 64, "type": "METRIC", "confidence": 0.853661835193634}, {"text": "F 0.5", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9477233588695526}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9992862343788147}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.998248815536499}]}, {"text": "Our submission to the shared task is the result of our best hybrid system, described in Section 2 and summarised in.", "labels": [], "entities": []}, {"text": "Similar to the official development set, the test set comprises 350 texts (85, 668 tokens in 4, 477 sentences) written by native and non-native English learners.", "labels": [], "entities": []}, {"text": "Systems were evaluated using the ERRANT scorer, with span-based correction F 0.5 as the primary measure.", "labels": [], "entities": [{"text": "ERRANT scorer", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9742817580699921}, {"text": "span-based correction F 0.5", "start_pos": 53, "end_pos": 80, "type": "METRIC", "confidence": 0.9617253243923187}]}, {"text": "In the restricted track, where participants were constrained to use only the provided training sets, our submitted system ranked fourth 9 out of 21 participating teams.", "labels": [], "entities": []}, {"text": "The official results of our submission in terms of span-level correction, span-level detection and token-level detection, including our system rankings, are reported in.", "labels": [], "entities": [{"text": "span-level detection", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.6861695796251297}, {"text": "token-level detection", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7013234198093414}]}, {"text": "It is worth noting that our correction system yielded particularly high performance on error detection tasks, ranking third on span-level detection and second on token-level detection.", "labels": [], "entities": [{"text": "error detection tasks", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.7833220859368643}, {"text": "span-level detection", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.6523134112358093}, {"text": "token-level detection", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.7528242766857147}]}, {"text": "We believe that much of the success in error detection can be credited to the error detection auxiliary objectives introduced in the convolutional neural sys-tem (see Section 2.1) and the error detection features used in our final re-ranking system (see Section 2.4).", "labels": [], "entities": [{"text": "error detection", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8346350193023682}, {"text": "error detection", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.6309072524309158}]}, {"text": "We also report span-level correction performance in terms of different CEFR levels (A, B, and C), as well as on the native texts only (N) in.", "labels": [], "entities": [{"text": "span-level correction", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.5798171609640121}, {"text": "CEFR", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.707211434841156}]}, {"text": "Our final error correction system performs best on advanced learner data (C), achieving an F 0.5 score of 73.28, followed by intermediate learner data (B), native data (N), and lastly beginner learner data (A).", "labels": [], "entities": [{"text": "F 0.5 score", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9865596493085226}]}, {"text": "The difference between the highest and lowest F 0.5 scores is 8.12 points.", "labels": [], "entities": [{"text": "F 0.5 scores", "start_pos": 46, "end_pos": 58, "type": "METRIC", "confidence": 0.9838337302207947}]}, {"text": "We also note that the system seems to be handling errors made by native students effectively even though it has not been trained on any native parallel data.", "labels": [], "entities": []}, {"text": "Overall, we observe that our system generalises well across native and non-native data, as well as across different proficiency/CEFR levels.", "labels": [], "entities": []}, {"text": "In order to better understand the performance of our hybrid error correction system, we perform a detailed error analysis.", "labels": [], "entities": [{"text": "error correction", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.6913524121046066}]}, {"text": "This helps us understand the strengths and weaknesses of the system, as well as identify areas for future work.", "labels": [], "entities": []}, {"text": "Error type-specific performance is presented in Table 5.", "labels": [], "entities": [{"text": "Error", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9307051301002502}]}, {"text": "We can see that our system achieves the highest results on VERB:INFL (verb inflection) errors with an F 0.5 of 93.75.", "labels": [], "entities": [{"text": "VERB", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9789957404136658}, {"text": "INFL (verb inflection) errors", "start_pos": 64, "end_pos": 93, "type": "METRIC", "confidence": 0.6831735819578171}, {"text": "F 0.5", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9830087721347809}]}, {"text": "However, the result is not truly representative as there are only 8 verb inflection errors in the test data, and our system successfully corrects 6 of them.", "labels": [], "entities": []}, {"text": "The error type that follows is ORTH (orthography), which comprises case and/or whitespace errors.", "labels": [], "entities": [{"text": "ORTH", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9678634405136108}]}, {"text": "A high precision score of 89.11 is observed, suggesting that our system is particularly suitable for these kind of errors.", "labels": [], "entities": [{"text": "precision score", "start_pos": 7, "end_pos": 22, "type": "METRIC", "confidence": 0.9833659827709198}]}, {"text": "We also observe that our system is effective at correcting VERB:SVA (subject-verb agreement) errors, achieving an F 0.5 of 80.08.", "labels": [], "entities": [{"text": "correcting VERB:SVA (subject-verb agreement)", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.6680891923606396}, {"text": "F 0.5", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9895668923854828}]}, {"text": "Results for ADJ:FORM (adjective form; F 0.5 =78.95) and CONTR (contraction; F 0.5 =77.92) are high; however, these error types only account for small fractions of the test set (0.188% and 0.245% respectively).", "labels": [], "entities": [{"text": "FORM", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9982849955558777}, {"text": "F 0.5 =78.95", "start_pos": 38, "end_pos": 50, "type": "METRIC", "confidence": 0.9435617178678513}, {"text": "CONTR", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9982802867889404}, {"text": "contraction; F 0.5 =77.92", "start_pos": 63, "end_pos": 88, "type": "METRIC", "confidence": 0.8201933205127716}]}, {"text": "The worst performance is observed for type CONJ (conjunction), with an F 0.5 of 28.46.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 71, "end_pos": 76, "type": "METRIC", "confidence": 0.9874777495861053}]}, {"text": "Our system successfully corrected 7 conjunction errors, while missed 20 and made 17 unnecessary changes.", "labels": [], "entities": []}, {"text": "We note that our system is less effective at correcting open-class errors", "labels": [], "entities": [{"text": "correcting open-class", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8596232235431671}]}], "tableCaptions": [{"text": " Table 1: Span-level correction results for individual  systems on the development set. TP: true positives, FP:  false positives, FN: false negatives, P: precision, R: re- call.", "labels": [], "entities": [{"text": "precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9530585408210754}, {"text": "R: re- call", "start_pos": 165, "end_pos": 176, "type": "METRIC", "confidence": 0.7835165381431579}]}, {"text": " Table 3. It is worth noting that our correction  system yielded particularly high performance on  error detection tasks, ranking third on span-level  detection and second on token-level detection. We  believe that much of the success in error detection  can be credited to the error detection auxiliary ob- jectives introduced in the convolutional neural sys-tem (see Section 2.1) and the error detection fea- tures used in our final re-ranking system (see Sec- tion 2.4).", "labels": [], "entities": [{"text": "error detection tasks", "start_pos": 99, "end_pos": 120, "type": "TASK", "confidence": 0.7880712648232778}, {"text": "span-level  detection", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.6276896148920059}, {"text": "token-level detection", "start_pos": 175, "end_pos": 196, "type": "TASK", "confidence": 0.6718773394823074}, {"text": "error detection", "start_pos": 238, "end_pos": 253, "type": "TASK", "confidence": 0.7276102751493454}]}, {"text": " Table 2: Span-level correction results for different system pipelines on the development set.", "labels": [], "entities": [{"text": "Span-level correction", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5658443570137024}]}, {"text": " Table 3: Official results of our submitted system on the  test set.", "labels": [], "entities": []}, {"text": " Table 4: Proficiency level-specific span-level correc- tion performance of our submitted system on the test  set. A: CEFR beginner; B: CEFR intermediate; C:  CEFR advanced; N: native.", "labels": [], "entities": [{"text": "Proficiency level-specific span-level correc- tion", "start_pos": 10, "end_pos": 60, "type": "METRIC", "confidence": 0.850189596414566}]}, {"text": " Table 5: Error type-specific span-level correction per- formance of our submitted system on the test set.", "labels": [], "entities": [{"text": "Error type-specific span-level correction", "start_pos": 10, "end_pos": 51, "type": "METRIC", "confidence": 0.8912173509597778}]}]}