{"title": [{"text": "Improving Translations by Combining Fuzzy-Match Repair with Automatic Post-Editing", "labels": [], "entities": [{"text": "Improving Translations", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9380248486995697}, {"text": "Fuzzy-Match Repair", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7839498817920685}]}], "abstractContent": [{"text": "Two of the more predominant technologies that professional translators have at their disposal for improving productivity are machine translation (MT) and computer-aided translation (CAT) tools based on translation memories (TM).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.8199761092662812}, {"text": "computer-aided translation (CAT)", "start_pos": 154, "end_pos": 186, "type": "TASK", "confidence": 0.8331284165382385}]}, {"text": "When translators use MT, they can use automatic post-editing (APE) systems to automate part of the post-editing work and get further productivity gains.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9185418486595154}]}, {"text": "When they use TM-based CAT tools, productivity may improve if they rely on fuzzy-match repair (FMR) methods.", "labels": [], "entities": [{"text": "TM-based CAT", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.7299754321575165}]}, {"text": "In this paper we combine FMR and APE: first a FMR proposal is produced from the translation unit proposed by the TM, then this proposal is further improved by an APE system specially tuned for this purpose.", "labels": [], "entities": [{"text": "APE", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.7769771814346313}]}, {"text": "Experiments conducted on the translation of English texts into German show that, with the two combined technologies, the quality of the translations improves up to 23% compared to a pure MT system.", "labels": [], "entities": [{"text": "translation of English texts into German", "start_pos": 29, "end_pos": 69, "type": "TASK", "confidence": 0.8472225467363993}, {"text": "MT", "start_pos": 187, "end_pos": 189, "type": "TASK", "confidence": 0.9494191408157349}]}, {"text": "The improvement over a pure FMR system is of 16%, showing the effectiveness of our joint solution.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent times, research has shown that translators can be more productive when applying state-ofthe-art post-editing techniques.", "labels": [], "entities": [{"text": "translators", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.9702969193458557}]}, {"text": "In many cases, the state-of-the-art techniques are applied to improve translation proposals from a translation memory (TM) or directly produced by a ma- chine translation (MT) system.", "labels": [], "entities": [{"text": "ma- chine translation (MT)", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.7560071519442967}]}, {"text": "Post-editing techniques can be automated and seamlessly integrated into the typical translation pipeline for productivity gains.", "labels": [], "entities": []}, {"text": "Two such techniques: fuzzy-match repair (FMR) () and automatic post-editing (APE) () have shown to be effective without the initial intervention of the translator by offering a repaired translation proposal from a TM in the case of FMR, and an improved MT output in the case of APE.", "labels": [], "entities": [{"text": "fuzzy-match repair (FMR)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.7598012387752533}, {"text": "automatic post-editing (APE)", "start_pos": 53, "end_pos": 81, "type": "METRIC", "confidence": 0.7744812726974487}]}, {"text": "FMR is an automatic post-editing technique typically used with TM-based computer-aided translation (CAT) tools.", "labels": [], "entities": [{"text": "TM-based computer-aided translation (CAT)", "start_pos": 63, "end_pos": 104, "type": "TASK", "confidence": 0.8071514020363489}]}, {"text": "In TM-based CAT, the translator is offered a translation proposal that comes from a translation unit (a pair of parallel segments) whose source segment is similar to the segment to be translated.", "labels": [], "entities": [{"text": "TM-based CAT", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.7988256514072418}]}, {"text": "When the source segment in the translation unit and the segment to be translated are not identical, which happens very often, the translation proposal needs to be post-edited in order to create the final translation.", "labels": [], "entities": []}, {"text": "FMR aims to provide repaired translation hypotheses to reduce the postediting effort of the original translation proposals by using another source of bilingual information such as an MT system.", "labels": [], "entities": [{"text": "FMR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8823223114013672}]}, {"text": "Some approaches to FMR, like the one by, heavily depend on the specific MT system type being used for repairing.", "labels": [], "entities": [{"text": "FMR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9754770398139954}]}, {"text": "Others, such as the one by use an agnostic, black-box, MT system in such away that the user would only choose from several repaired hypothesis proposals.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9141790270805359}]}, {"text": "APE aims to correct the errors present in a machine-translated text before showing it to the translator or post-editor.", "labels": [], "entities": []}, {"text": "As motivated by, an APE system can help to improve MT output by exploiting information that is not available during translation, or by performing a deeper text analysis, and by adapting the output of a general-purpose MT system to the lexicon/style requested in a specific application domain.", "labels": [], "entities": [{"text": "MT output", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.9215188920497894}]}, {"text": "In doing so, APE aims to provide professional translators with improved MT output quality to reduce (human) post-editing effort.", "labels": [], "entities": [{"text": "MT output", "start_pos": 72, "end_pos": 81, "type": "TASK", "confidence": 0.8593460321426392}]}, {"text": "In this paper, we show that APE could be used to improve sentence-level proposals from FMR when FMR is used as a device to create new translations from a TM.", "labels": [], "entities": [{"text": "APE", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.8759911060333252}]}, {"text": "As shown in, FMR is first used to produce a repaired translation proposal and then APE is used as a tool to improve the quality of the proposal.", "labels": [], "entities": [{"text": "FMR", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6272032856941223}, {"text": "APE", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9958993792533875}]}, {"text": "We demonstrate that the combination of these two techniques can significantly boost translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9713405966758728}]}, {"text": "It outperforms both a competitive neural MT system and FMR alone, and its performance reaches nearly that of methods relying on the reference (i.e. oracle) translations.", "labels": [], "entities": []}, {"text": "Our work provides an in-depth analysis of which technique would work best under \"typical\" translation scenarios by testing several combinations of the two post-editing techniques.", "labels": [], "entities": []}, {"text": "Our analysis includes various checkpoints of evaluation including industry standards and human-level reviews.", "labels": [], "entities": []}, {"text": "In order to better describe our process, we organize the paper as follows.", "labels": [], "entities": []}, {"text": "First, in Section 2 we review the relevant work where both technologies (FMR and APE) have been used.", "labels": [], "entities": [{"text": "APE", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.8904354572296143}]}, {"text": "Second, in Section 3, we dig deeper into the motivation and methodology of our work and show how the two technologies could be \"glued\" together to form anew system that is added in a modular way to a traditional CAT pipeline.", "labels": [], "entities": [{"text": "CAT pipeline", "start_pos": 212, "end_pos": 224, "type": "TASK", "confidence": 0.9110152125358582}]}, {"text": "Third, in Section 4 we describe our experimental settings in detail.", "labels": [], "entities": []}, {"text": "Fourth, we present our results in Section 5.", "labels": [], "entities": []}, {"text": "We use BLEU and TER as metrics to evaluate the quality of our translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9986138343811035}, {"text": "TER", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.995756208896637}]}, {"text": "We also perform error analysis and human reviews.", "labels": [], "entities": [{"text": "error analysis", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.798052579164505}]}, {"text": "Then, we measure the systems quantitatively using a word-measurement like word-error rate to show performance.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we give some conclusions and plan on doing in the future.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with various combinations of FMR and APE using a phrase-based MT system as a SBI for FMR.", "labels": [], "entities": [{"text": "APE", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9629234671592712}]}, {"text": "In addition, we use APE on the output of two MT systems, a phrase-based MT system and a neural MT system, as a point of comparison.", "labels": [], "entities": [{"text": "APE", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.7380799055099487}]}, {"text": "This section goes over the details of the data and systems we used.", "labels": [], "entities": []}, {"text": "One of our goals in this paper is to show that by using freely-available data found on the Internet, which is the case for small businesses that do not have in-house data and cannot afford more expensive data sets, our system achieves good results despite results from previous work () that have shown that training MT systems on in-domain data, especially in the case of a neural MT system, can be advantageous.", "labels": [], "entities": [{"text": "MT", "start_pos": 316, "end_pos": 318, "type": "TASK", "confidence": 0.9803213477134705}]}, {"text": "For evaluating the combination of FMR and APE, we use two major metrics: BLEU () and translation edit rate (TER)).", "labels": [], "entities": [{"text": "APE", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.5988681316375732}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9995250701904297}, {"text": "translation edit rate (TER))", "start_pos": 85, "end_pos": 113, "type": "METRIC", "confidence": 0.8636251588662466}]}, {"text": "We report on BLEU because it is a centerpiece of the development of MT systems, and on TER because it is the primary evaluation metric at the WMT APE shared task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.99864262342453}, {"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9823708534240723}, {"text": "TER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9950001239776611}, {"text": "WMT APE shared task", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.710425391793251}]}, {"text": "In addition to automatic evaluation metrics, we introduce a human evaluator: a native German speaker.", "labels": [], "entities": []}, {"text": "This evaluator is not a translator; yet, does have a background in natural language processing and evaluation.", "labels": [], "entities": []}, {"text": "We report the evaluator's overall evaluation on the best performing systems in our results and offer it as an extra evaluation metric of performance.", "labels": [], "entities": []}, {"text": "The hope is to better understand the target language and how well the various systems perform under a native eye.", "labels": [], "entities": []}, {"text": "We provided a random set of 1,000 samples to the evaluator, where each sample is made of a sentence pair and its translations provided by each system presented in.", "labels": [], "entities": []}, {"text": "Each sentence pair is rated by assigning quality scores on a 5-point scale (1 being the worst and 5 the best).", "labels": [], "entities": []}, {"text": "The evaluator was told to rate the quality of translations and, thus, was given the final translation from the four systems but not the original human reference translation.", "labels": [], "entities": []}, {"text": "Additionally, the evaluator was asked to provide an explanation of why each system's translation did not seem correct.", "labels": [], "entities": []}, {"text": "Correctness was determined as a system's translation being exactly what was expected for the source sentence (a 5-star rating) or not at all (a 1-star rating).", "labels": [], "entities": [{"text": "Correctness", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9658479690551758}]}], "tableCaptions": [{"text": " Table 2: Performance of three baseline approaches (use of a  phrase-based MT system, use of a neural MT system, and use  of the TM proposal without repairing), of the use of APE to  better the MT outputs, the use of FMR alone when the trans- lation hypothesis is selected at random or using an oracle, and  of different combinations of FMR and APE.", "labels": [], "entities": [{"text": "APE", "start_pos": 175, "end_pos": 178, "type": "METRIC", "confidence": 0.9715322852134705}]}, {"text": " Table 3: Average human evaluation for the best system com- bining FMR and APE. Translations were rated using a 5-point  scale, 1 being the worst and 5 the best.", "labels": [], "entities": [{"text": "bining FMR", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.4838241785764694}, {"text": "APE", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.7754101753234863}, {"text": "Translations", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.9715465903282166}]}]}