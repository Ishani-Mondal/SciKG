{"title": [{"text": "Character-level Annotation for Chinese Surface-Syntactic Universal Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents anew schema to annotate Chinese Treebanks on the character level.", "labels": [], "entities": [{"text": "Chinese Treebanks", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9506770968437195}]}, {"text": "The original Universal Dependencies (UD) and Surface-Syntactic Universal Dependencies (SUD) projects provide token-level resources with rich morphosyntactic language details.", "labels": [], "entities": []}, {"text": "However, without any commonly accepted word definition for Chinese, the dependency parsing always faces the dilemma of word segmentation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7823381721973419}, {"text": "word segmentation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7020113468170166}]}, {"text": "Therefore we present a character-level annotation schema integrated into the existing Universal Dependencies schema as an extension.", "labels": [], "entities": []}], "introductionContent": [{"text": "With its writing system being a \u200b Scriptua Continua\u200b , Chinese is a language without explicit word delimiters and thus the \"wordhood\" is a particularly unclear notion.", "labels": [], "entities": []}, {"text": "Yet, the vast majority of downstream NLP tasks for any language are based on \"tokens\", which mostly boils down to some kind of spelling-based tokenizer.", "labels": [], "entities": []}, {"text": "Yet, in the case of Chinese, this step requires a preprocessing step called \"word segmentation\", whose performance has an non-neglectable influence on the final results.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7166043370962143}]}, {"text": "While the F-score of the segmentation task of general texts is in the high nineties since more than 10 years) and results have even been slightly improved by recent neural models ( ), these numbers drop to below 10% for Out-of-Vocabulary terms, i.e. where the system has to take educated guesses on where the word borders are.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9986458420753479}, {"text": "segmentation task of general texts", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.8821591258049011}]}, {"text": "This leads to catastrophic results for domain specialized texts that use a great number of neologisms unknown to the system, such as patent texts.", "labels": [], "entities": []}, {"text": "Since (Zhao 2009) proposed the first method for character-level dependencies parsing on the Chinese Penn Treebank, a series of research involving the character-based annotation) have already shown the usefulness of the word-internal structures in Chinese syntactic parsing by obtaining limited but real improvements by means of extra character-level information (character POS, head character position and word internal dependency relation). and) have annotated a large-scale word list on Penn Treebank (PTB) and constituent Chinese Treebank (CTB) on the morphological level.", "labels": [], "entities": [{"text": "character-level dependencies parsing", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.6253731350104014}, {"text": "Chinese Penn Treebank", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.6969123681386312}, {"text": "syntactic parsing", "start_pos": 255, "end_pos": 272, "type": "TASK", "confidence": 0.6587027460336685}, {"text": "Penn Treebank (PTB)", "start_pos": 489, "end_pos": 508, "type": "DATASET", "confidence": 0.9650515198707581}, {"text": "Chinese Treebank (CTB)", "start_pos": 525, "end_pos": 547, "type": "DATASET", "confidence": 0.9241265773773193}]}, {"text": "Other character-based parsing attempts are generally based on these two annotated corpora.", "labels": [], "entities": [{"text": "character-based parsing", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.6942594051361084}]}, {"text": "In this work, we report on the integration of character-level annotations into the Chinese UD treebanks with the goal to find a joint segmentation-parsing method, which enables a multi-granularity analysis on Chinese sentences.", "labels": [], "entities": [{"text": "Chinese UD treebanks", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.6370221972465515}]}, {"text": "Besides the final goal to improve the performance of the dependency parser with character-level information, in particular on out-of-domain texts, this work can also be regarded as anew Chinese word segmentation method: As we distinguish the morphological and syntactic relations between characters by a different set of dependency relation labels, we can ultimately fuse the character parsing results into a simple word segmentation, which can be compared to the original UD word segmentation.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.767753392457962}, {"text": "Chinese word segmentation", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.6269752383232117}, {"text": "character parsing", "start_pos": 376, "end_pos": 393, "type": "TASK", "confidence": 0.7870470285415649}, {"text": "UD word segmentation", "start_pos": 473, "end_pos": 493, "type": "TASK", "confidence": 0.6321978668371836}]}, {"text": "The character-level parse tree can thus also be projected onto a dependency tree on the words, which allows us to compare our parsing results with a simple token-based model.", "labels": [], "entities": []}, {"text": "In Section 2 we will briefly introduce various internal structures of Chinese words before presenting our annotation scheme for character-level POS and word internal dependency structures.", "labels": [], "entities": []}, {"text": "The experiments and the results obtained are shown in Section 3, followed by the conclusion in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have worked on the four Chinese UD treebanks converted into SUD format and simplified characters when necessary: The Traditional Chinese Universal Dependencies Treebank annotated by Google (GSD), the Parallel Universal Dependencies treebanks created for the CoNLL 2017 shared task on Multilingual Parsing (PUD), the Traditional Chinese treebank of film subtitles and of legislative proceedings of Hong Kong (HK), and the essays written by learners of Mandarin Chinese as a foreign language (CFL), also proposed by the City University of Hong Kong.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task on Multilingual Parsing (PUD)", "start_pos": 261, "end_pos": 313, "type": "TASK", "confidence": 0.7961418747901916}]}, {"text": "To train the character-based POS tagger and SUD parser, we choose the Graph-based Neural Dependency Parser developed by Timothy Dozat at Stanford University for its character-based LSTM word representation.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.705893874168396}, {"text": "LSTM word representation", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.6872652570406595}]}, {"text": "This parser contains a tagger training network and a dependency parser training network, but unfortunately these two training processes are separated, meaning that to obtain a corpus tagged and parsed, first we have to train a tagger, use it to tag our corpus, then train a parser and use it to parse our tagged corpus.", "labels": [], "entities": []}, {"text": "Before the training process, we have also prepared a character vector file which is trained by BERT, a word embedding model developed by Google with a pre-trained character based Chinese model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9964442849159241}]}, {"text": "Our experiments consist of using Dozat Parser to train the word-based (WB) tagger and parser, as well as the character-based (CB) tagger and parser.", "labels": [], "entities": []}, {"text": "Then by applying them to tag and parse our test corpus we can obtain two versions of our treebank: a word-based and a character-based treebank (see \u200b Annex 2\u200b ), so that we can perform systematic tests of comparison on the combined Chinese SUD treebanks and evaluate the performance of our character-based tagger and parser.", "labels": [], "entities": [{"text": "Chinese SUD treebanks", "start_pos": 232, "end_pos": 253, "type": "DATASET", "confidence": 0.6034828325112661}]}, {"text": "To sum up, we need to go through at least four training processes: WB tagger training, WB parser training, CB tagger training and CB parser training.", "labels": [], "entities": [{"text": "WB tagger training", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8868687550226847}, {"text": "WB parser training", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.936475396156311}, {"text": "CB tagger training", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8632150093714396}, {"text": "CB parser training", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8532784978548685}]}, {"text": "Therefore, we have prepared our training data as following: for WB tagger and parser training, we extract the last 10% of the four former mentioned Chinese SUD treebanks to serve as the testing set and the developing set, and we combine the rest 90% to serve as a training set; for CB tagger and parser training, we carryout the exact same arrangement, except this time all the treebanks are converted from word level to character level.", "labels": [], "entities": [{"text": "WB tagger", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.8890753984451294}, {"text": "Chinese SUD treebanks", "start_pos": 148, "end_pos": 169, "type": "DATASET", "confidence": 0.7313100099563599}, {"text": "CB tagger", "start_pos": 282, "end_pos": 291, "type": "TASK", "confidence": 0.7687663733959198}]}, {"text": "Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.", "labels": [], "entities": [{"text": "tagger", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.9678547382354736}, {"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9989938139915466}, {"text": "WB", "start_pos": 71, "end_pos": 73, "type": "DATASET", "confidence": 0.9746277332305908}]}, {"text": "The \u200b ) and CB tagging after the recombination (\u200b  As we can see from these two tables above, the training on a character base has greatly improved the performance of the tagger.", "labels": [], "entities": [{"text": "CB tagging", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.873964250087738}]}, {"text": "However for some most common POS, like ADJ and NOUN, there's an obvious decline of f-score.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.5373103618621826}, {"text": "f-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9203642010688782}]}, {"text": "One of the possible reasons is that there's an inconsistency between the word level POS and character level POS in Chinese.", "labels": [], "entities": []}, {"text": "For example, \u6d3b\u52a8 (NOUN, '\u200b activity\u200b ') is composed by two verbal character \"\u6d3b\" (VERB, '\u200b living\u200b ') and \"\u52a8\" (VERB, '\u200b moving\u200b ').", "labels": [], "entities": [{"text": "NOUN", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8315302729606628}, {"text": "VERB", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9326361417770386}]}, {"text": "But by reviewing our data, we noticed that there's also an inconsistency on the POS annotation of the same words between different treebanks, even if in a similar context.", "labels": [], "entities": []}, {"text": "This problem may have a bigger influence on both tagger and parser.", "labels": [], "entities": [{"text": "tagger", "start_pos": 49, "end_pos": 55, "type": "TASK", "confidence": 0.9644203186035156}]}, {"text": "Since we haven't totally finished the pretreatment process, there's a problem of inconsistency in our data, with the same word in the same context but having different POS or different internal structure annotated.", "labels": [], "entities": []}, {"text": "We can also separately measure the performance on the syntactic and morphological dependencies (\u200b ).", "labels": [], "entities": []}, {"text": "This method has a special function, that is the performance of the segmentation can be evaluated by concerning only about the two main groups of dependency relations: Morphe (relations annotated with m: at the beginning) and Deprel (the original dependency relations in SUD).", "labels": [], "entities": [{"text": "Morphe", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.7542732357978821}]}], "tableCaptions": [{"text": " Table 3 F-score of word level POS (UPOS) for our  word-based tagger", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9929458498954773}, {"text": "word level POS (UPOS)", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.6773103177547455}]}]}