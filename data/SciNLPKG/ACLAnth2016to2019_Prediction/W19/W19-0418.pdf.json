{"title": [{"text": "Evaluating the Representational Hub of Language and Vision Models", "labels": [], "entities": []}], "abstractContent": [{"text": "The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the \"Hub and Spoke\" architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs.", "labels": [], "entities": []}, {"text": "In particular, the Hub is implemented as a neural network encoder.", "labels": [], "entities": [{"text": "Hub", "start_pos": 19, "end_pos": 22, "type": "DATASET", "confidence": 0.9358429312705994}]}, {"text": "We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue.", "labels": [], "entities": [{"text": "question answering", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.6851019859313965}, {"text": "visual reference resolution", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6149440904458364}]}, {"text": "To measure the quality of the representations learned by the encoder, we use two kinds of analyses.", "labels": [], "entities": []}, {"text": "First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing diagnostic task designed to assess multi-modal semantic understanding.", "labels": [], "entities": [{"text": "multi-modal semantic understanding", "start_pos": 136, "end_pos": 170, "type": "TASK", "confidence": 0.6411127944787344}]}, {"text": "Second, we carryout a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, a lot of progress has been made within the emerging field at the intersection of computational linguistics and computer vision thanks to the use of deep neural networks.", "labels": [], "entities": [{"text": "computer vision", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.7081083357334137}]}, {"text": "The most common strategy to move the field forward has been to propose different multimodal tasks-such as visual question answering (, visual question generation (, visual reference resolution (, and visual dialogue ()-and to develop task-specific models.", "labels": [], "entities": [{"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.6853393465280533}, {"text": "visual question generation", "start_pos": 135, "end_pos": 161, "type": "TASK", "confidence": 0.6122235357761383}, {"text": "visual reference resolution", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.6290867626667023}]}, {"text": "The benchmarks developed so far have put forward complex and distinct neural architectures, but in general they all share a common backbone consisting of an encoder which learns to merge the two types of representation to perform a certain task.", "labels": [], "entities": []}, {"text": "This resembles the bottom-up processing in the 'Hub and Spoke' model proposed in Cognitive Science to represent how the brain processes and combines multi-sensory inputs.", "labels": [], "entities": []}, {"text": "In this model, a 'hub' module merges the input processed by the sensor-specific 'spokes' into a joint representation.", "labels": [], "entities": []}, {"text": "We focus our attention on the encoder implementing the 'hub' in artificial multimodal systems, with the goal of assessing its ability to compute multimodal representations that are useful beyond specific tasks.", "labels": [], "entities": []}, {"text": "While current visually grounded models perform remarkably well on the task they have been trained for, it is unclear whether they are able to learn representations that truly merge the two modalities and whether the skill they have acquired is stable enough to be transferred to other tasks.", "labels": [], "entities": []}, {"text": "In this paper, we investigate these questions in detail.", "labels": [], "entities": []}, {"text": "To do so, we evaluate an encoder trained on different multimodal tasks on an existing diagnostic task-FOIL ()-designed to assess multimodal semantic understanding and carryout an in-depth analysis to study how the encoder merges and exploits the two modalities.", "labels": [], "entities": []}, {"text": "We also exploit two techniques to investigate the structure of the learned semantic spaces: Representation Similarity Analysis (RSA) ( and Nearest Neighbour overlap (NN).", "labels": [], "entities": [{"text": "Representation Similarity Analysis (RSA", "start_pos": 92, "end_pos": 131, "type": "TASK", "confidence": 0.7087166249752045}]}, {"text": "We use RSA to compare the outcome of the various encoders given the same vision-and-language input and NN to compare the multimodal space produced by an encoder with the ones built with the input visual and language embeddings, respectively, which allows us to measure the relative weight an encoder gives to the two modalities.", "labels": [], "entities": [{"text": "RSA", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.8932469487190247}]}, {"text": "In particular, we consider three visually grounded tasks: visual question answering (VQA) (, where the encoder is trained to answer a question about an image; visual resolution of referring expressions (ReferIt) (), where the model has to pickup the referent object of a description in an image; and GuessWhat (de, where the model has to identify the object in an image that is the target of a goal-oriented question-answer dialogue.", "labels": [], "entities": [{"text": "visual question answering (VQA)", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.7467398842175802}, {"text": "visual resolution of referring expressions (ReferIt)", "start_pos": 159, "end_pos": 211, "type": "TASK", "confidence": 0.6915167085826397}, {"text": "GuessWhat", "start_pos": 300, "end_pos": 309, "type": "METRIC", "confidence": 0.9744018316268921}]}, {"text": "We make sure the datasets used in the pre-training phase are as similar as possible in terms of size and image complexity, and use the same model architecture for the three pre-training tasks.", "labels": [], "entities": []}, {"text": "This guarantees fair comparisons and the reliability of the results we obtain.", "labels": [], "entities": [{"text": "reliability", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9895685315132141}]}, {"text": "We show that the multimodal encoding skills learned by pre-training the model on GuessWhat and ReferIt are more stable and transferable than the ones learned through VQA.", "labels": [], "entities": [{"text": "GuessWhat", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.9772309064865112}, {"text": "VQA", "start_pos": 166, "end_pos": 169, "type": "DATASET", "confidence": 0.8866745233535767}]}, {"text": "This is reflected in the lower number of epochs and the smaller training data size they need to reach their best performance on the FOIL task.", "labels": [], "entities": [{"text": "FOIL task", "start_pos": 132, "end_pos": 141, "type": "TASK", "confidence": 0.6529143750667572}]}, {"text": "We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder.", "labels": [], "entities": [{"text": "VQA encoder", "start_pos": 175, "end_pos": 186, "type": "DATASET", "confidence": 0.9133808314800262}]}, {"text": "Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide details on the data sets and the implementation settings we use in our experiments.", "labels": [], "entities": []}, {"text": "Pre-training datasets For the three visually grounded tasks, we use the VQA.v1 dataset by, the RefCOCO dataset by, and the GuessWhat?!", "labels": [], "entities": [{"text": "VQA.v1 dataset", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.9761391878128052}, {"text": "RefCOCO dataset", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.9011334180831909}, {"text": "GuessWhat?!", "start_pos": 123, "end_pos": 134, "type": "METRIC", "confidence": 0.6878259579340616}]}, {"text": "dataset by de as our starting point.", "labels": [], "entities": []}, {"text": "All these datasets have been developed with images from MS-COCO ().", "labels": [], "entities": [{"text": "MS-COCO", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9588794112205505}]}, {"text": "We construct common image datasets for by taking the intersection of the images in the three original datasets.", "labels": [], "entities": []}, {"text": "This results in a total of 14,458 images.", "labels": [], "entities": []}, {"text": "An image can be part of several data points, i.e, it can be paired with more than one linguistic input.", "labels": [], "entities": []}, {"text": "Indeed, the 14,458 common images correspond to 43,374 questions for the VQA task, 104,227 descriptions for the ReferIt task, and 35,467 dialogues for the GuessWhat task.", "labels": [], "entities": [{"text": "VQA", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7378902435302734}, {"text": "GuessWhat", "start_pos": 154, "end_pos": 163, "type": "DATASET", "confidence": 0.8937479853630066}]}, {"text": "To obtain datasets of equal size per task that are as similar as possible, we filter the resulting data points according to the following procedure: 1.", "labels": [], "entities": []}, {"text": "For each image, we check how many linguistic items are present in the three datasets and fix the minimum number (k) to be our target number of linguistic items paired with that image.", "labels": [], "entities": []}, {"text": "2. We select n data points where the descriptions in ReferIt and dialogues in GuessWhat concern the same target object (with n \u2264 k).", "labels": [], "entities": [{"text": "GuessWhat", "start_pos": 78, "end_pos": 87, "type": "DATASET", "confidence": 0.8980653882026672}]}, {"text": "3. Among then data points selected in the previous step, we select them data points in VQA where the question or the answer mention the same target object (computed by string matching).", "labels": [], "entities": [{"text": "VQA", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.7590314745903015}]}, {"text": "4. We make sure all the images in each task-specific dataset are paired with exactly k linguistic items; if not, we select additional ones randomly until this holds..", "labels": [], "entities": []}, {"text": "All the images in the test set do not occur either in the FOIL training and validation set, nor in the common image dataset described above and used to pre-train the models.", "labels": [], "entities": [{"text": "FOIL training and validation set", "start_pos": 58, "end_pos": 90, "type": "DATASET", "confidence": 0.6717244625091553}]}, {"text": "Implementation details All models are trained using supervised learning with ground truth data.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.8484333157539368}]}, {"text": "We use the same parameters for all models: batch size of 256 and Adam optimizer) with learning rate 0.0001.", "labels": [], "entities": []}, {"text": "All the parameters are tuned on the validation set.", "labels": [], "entities": []}, {"text": "Early stopping is used while training, i.e., training is stopped when there is no improvement on the validation loss for 10 consecutive epochs or a maximum of 100 epochs, and the best model is taken based on the validation loss.", "labels": [], "entities": [{"text": "Early stopping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7204102277755737}]}], "tableCaptions": [{"text": " Table 1: Statistics of the datasets used for the pre-training tasks and the FOIL task.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.6936319470405579}]}, {"text": " Table 2: Accuracy on the FOIL task for the best model of each training setting.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9974898099899292}, {"text": "FOIL", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.861302375793457}]}, {"text": " Table 3: Average nearest neighbour overlap between the encoder multimodal representations and the  ResNet152 and USE embeddings, respectively.", "labels": [], "entities": [{"text": "USE embeddings", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.8968476057052612}]}]}