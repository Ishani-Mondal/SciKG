{"title": [{"text": "Neural Machine Translation for English-Kazakh with Morphological Segmentation and Synthetic Data", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7323736548423767}]}], "abstractContent": [{"text": "This paper presents the systems submitted by the University of Groningen to the English-Kazakh language pair (both translation directions) for the WMT 2019 news translation task.", "labels": [], "entities": [{"text": "WMT 2019 news translation task", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.8198542356491089}]}, {"text": "We explore potential benefits from using (i) morphological segmentation (both un-supervised and rule-based), given the agglu-tinative nature of Kazakh, (ii) data from two additional languages (Turkish and Russian), given the scarcity of English-Kazakh data, and (iii) synthetic data, both for the source and for the target language.", "labels": [], "entities": []}, {"text": "Our best submissions ranked second for Kazakh\u2192English and third for English\u2192Kazakh in terms of the BLEU automatic evaluation metric.", "labels": [], "entities": [{"text": "BLEU automatic evaluation metric", "start_pos": 99, "end_pos": 131, "type": "METRIC", "confidence": 0.8665617853403091}]}], "introductionContent": [{"text": "This paper presents the neural machine translation (NMT) systems submitted by the University of Groningen to the WMT 2019 news translation task.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7476016084353129}, {"text": "WMT 2019 news translation task", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.8673722863197326}]}, {"text": "We participated in the English\u2194Kazakh (henceforth referred to as EN\u2194KK) constrained tasks.", "labels": [], "entities": []}, {"text": "Because of the inherent characteristics of this language pair and the current state-of-the-art of related techniques, we focused on two main research questions (RQs): \u2022 RQ1.", "labels": [], "entities": []}, {"text": "Recent research in NMT for agglutinative languages found that morphological segmentation outperforms the most widely used segmentation technique, byte-pair encoding (BPE, using character sequence frequencies) (.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 62, "end_pos": 88, "type": "TASK", "confidence": 0.7102602869272232}]}, {"text": "Rule-based segmentation improved English-to-Finnish translation (S\u00e1nchez-Cartagena and Toral, http://www.statmt.org/wmt19/ translation-task.html 2016) and unsupervised segmentation improved Turkish-to-English translation.", "labels": [], "entities": [{"text": "Turkish-to-English translation", "start_pos": 190, "end_pos": 220, "type": "TASK", "confidence": 0.717330127954483}]}, {"text": "Because Kazakh belongs to the same language family as Turkish, the work by is particularly relevant.", "labels": [], "entities": []}, {"text": "Their training data had fewer than 300,000 sentence pairs and they trained an NMT system under the recurrent sequenceto-sequence with attention paradigm.", "labels": [], "entities": []}, {"text": "Our training data is considerably bigger and we use a nonrecurrent attention-based system (.", "labels": [], "entities": []}, {"text": "Does the advantage of morphological segmentation over BPE also hold in our experimental setup?", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.699499323964119}]}, {"text": "Does the use of additional languages improve outcomes?", "labels": [], "entities": []}, {"text": "Due to the scarcity of parallel data for EN-KK, we investigate if using data from two additional languages is useful, Russian (RU) and Turkish (TR).", "labels": [], "entities": []}, {"text": "Even though RU is not related to either EN or KK, it seems a sensible choice due to the availability of large amounts of EN-RU and RU-KK parallel data.", "labels": [], "entities": []}, {"text": "TR is related to KK and there are limited amounts of EN-TR data available.", "labels": [], "entities": [{"text": "TR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.5883760452270508}]}, {"text": "Does this additional data improve the performance, and is more data from an unrelated language (RU) better than less data from a related language (TR)?", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the datasets and tools used.", "labels": [], "entities": []}, {"text": "Then Section 3 details our experiments.", "labels": [], "entities": []}, {"text": "Finally, Section 4 outlines our conclusions and plans for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We preprocessed all the corpora used (training, validation and test sets) with scripts from the Moses toolkit (.", "labels": [], "entities": []}, {"text": "The following operations were performed sequentially: punctuation normalisation, tokenisation, 2 truecasing and escaping of problematic characters.", "labels": [], "entities": [{"text": "punctuation normalisation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7985082268714905}, {"text": "tokenisation", "start_pos": 81, "end_pos": 93, "type": "TASK", "confidence": 0.9687272310256958}]}, {"text": "The truecaser was lexicon-based and it was trained on all the monolingual data available for each language.", "labels": [], "entities": []}, {"text": "In addition, we removed sentence pairs where either side was empty or longer than 80 tokens from the parallel corpora . show the parallel datasets used for training for each translation direction after preprocessing.", "labels": [], "entities": []}, {"text": "The corpora Kazakhtv (EN-KK) and crawl (KK-RU) were provided with sentence-level scores; we sorted their files according to these scores and a native KK speaker proficient in both EN and RU identified a threshold where alignments were roughly 90% correct.", "labels": [], "entities": []}, {"text": "This led to discarding the bottom 27% of the data for EN-KK's Kazakhtv and the bottom 3% for KK-RU's crawl.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Preprocessed EN-KK parallel training data.", "labels": [], "entities": []}, {"text": " Table 2: Preprocessed EN-RU parallel training data.", "labels": [], "entities": []}, {"text": " Table 3: Preprocessed KK-RU parallel training data.", "labels": [], "entities": []}, {"text": " Table 5: BLEU scores on the development set for  KK\u2192EN using additional EN-TR data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992695450782776}]}, {"text": " Table 6: Performance of MT systems with and without  backtranslation for EN\u2192KK (CHRF) and KK\u2192EN  (BLEU).", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9885745644569397}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9814027547836304}]}, {"text": " Table 7: Cosine similarity thresholds used to filter out  EN-RU corpora and resulting corpus sizes after all fil- tering steps are applied.", "labels": [], "entities": [{"text": "Cosine similarity thresholds", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8096667925516764}]}, {"text": " Table 8: BLEU scores for KK\u2192EN systems adding  one filtering mechanism at a time. The table also shows  the number of sentence pairs (millions) that make up  the training data for each system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991433620452881}]}, {"text": " Table 9: Sentence pairs left in the EN'-KK dataset after  filtering.", "labels": [], "entities": [{"text": "EN'-KK dataset", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.6703031659126282}]}, {"text": " Table 10: Sentence pairs left in the EN-KK' dataset  after filtering using the similarity thresholds 0.75 and  0.8.", "labels": [], "entities": [{"text": "EN-KK' dataset", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.8163866599400839}]}, {"text": " Table 12: Performance of MT systems using differ- ent segmentations (BPE, LVMR and Apertium) for  EN\u2192KK (CHRF) and KK\u2192EN (BLEU). Apertium  was not used for the KK\u2192EN due to time constraints.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9885572195053101}, {"text": "BPE", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8877763748168945}, {"text": "Apertium", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9894875288009644}, {"text": "BLEU", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.9554483294487}]}]}