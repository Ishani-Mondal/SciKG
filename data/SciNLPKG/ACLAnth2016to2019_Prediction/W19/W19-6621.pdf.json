{"title": [{"text": "The Impact of Preprocessing on Arabic-English Statistical and Neural Machine Translation", "labels": [], "entities": [{"text": "Statistical and Neural Machine Translation", "start_pos": 46, "end_pos": 88, "type": "TASK", "confidence": 0.6094142258167267}]}], "abstractContent": [{"text": "Neural networks have become the state-of-the-art approach for machine translation (MT) in many languages.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.8638665437698364}]}, {"text": "While linguistically-motivated tokenization techniques were shown to have significant effects on the performance of statistical MT, it remains unclear if those techniques are well suited for neural MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.8472838401794434}, {"text": "MT", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.7871319055557251}]}, {"text": "In this paper , we systematically compare neural and statistical MT models for Arabic-English translation on data preprecossed by various prominent tokenization schemes.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9561867117881775}]}, {"text": "Furthermore , we consider a range of data and vocabulary sizes and compare their effect on both approaches.", "labels": [], "entities": []}, {"text": "Our empirical results show that the best choice of tokeniza-tion scheme is largely based on the type of model and the size of data.", "labels": [], "entities": []}, {"text": "We also show that we can gain significant improvements using a system selection that combines the output from neural and statistical MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 133, "end_pos": 135, "type": "TASK", "confidence": 0.898955225944519}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has been rapidly attracting the attention of the research community for its promising results (;.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8709724148114523}]}, {"text": "NMT is composed of two neural networks, an encoder and a decoder, where the encoder is fed a sentence from the source language and the decoder generates its translation, word byword, in the target language.", "labels": [], "entities": []}, {"text": "Recently, NMT has been shown to outperform other MT systems in many language pairs, e.g. German-English, French-English and Basque-English (.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9690499305725098}]}, {"text": "While Arabic MT has been mostly developed under statistical MT (SMT), NMT has also been applied and studied recently).", "labels": [], "entities": [{"text": "MT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.7581562995910645}, {"text": "statistical MT (SMT", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.6533488556742668}]}, {"text": "Linguistically-motivated tokenization has shown to have a significant effect on SMT, particularly in the case of morphologically rich languages like Arabic ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9955546259880066}]}, {"text": "However, it remains unclear if such techniques are well suited for NMT, where language-agnostic tokenizations, e.g. byte-pair encoding (BPE) (, are widely used. has looked into Arabic SMT and NMT, achieving the highest accuracy using the Penn Arabic Treebank (ATB) tokenization, with 51.2 and 49.7 BLEU points for SMT and NMT, respectively.", "labels": [], "entities": [{"text": "NMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9191877841949463}, {"text": "SMT", "start_pos": 184, "end_pos": 187, "type": "TASK", "confidence": 0.7669102549552917}, {"text": "accuracy", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9982911944389343}, {"text": "Penn Arabic Treebank (ATB)", "start_pos": 238, "end_pos": 264, "type": "DATASET", "confidence": 0.9471502006053925}, {"text": "BLEU", "start_pos": 298, "end_pos": 302, "type": "METRIC", "confidence": 0.9984419941902161}, {"text": "SMT", "start_pos": 314, "end_pos": 317, "type": "TASK", "confidence": 0.9629512429237366}]}, {"text": "In this paper, we study the impact of different preprocessing techniques in Arabic-English MT on both SMT and NMT, by examining various prominent tokenization schemes.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.8650028109550476}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9862629175186157}]}, {"text": "We conduct learning curve experiments to study the interaction between data size and the choice of tokenization scheme.", "labels": [], "entities": []}, {"text": "We study the performance under morphology-based and frequency-based tokenization schemes, provided by MADAMIRA () and BPE, respectively, on in-domain data.", "labels": [], "entities": [{"text": "BPE", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.7272701859474182}]}, {"text": "In addition, we evaluate the best performing models on out-of-domain data.", "labels": [], "entities": []}, {"text": "Our results show that the utilization of BPE for SMT can be effective and allows achieving a good performance even with a small vocabulary size of 20K.", "labels": [], "entities": [{"text": "SMT", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9898275136947632}]}, {"text": "Moreover, the results show that the performance of NMT is especially sensitive to the size of data.", "labels": [], "entities": []}, {"text": "We notice that NMT suffers with long sentences, and thus, we utilize system selection, which yields significant improvements over both approaches.", "labels": [], "entities": []}, {"text": "Our best system significantly outperforms previous results reported on the same in-domain test data by +4 BLEU points (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9992972612380981}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The related work is presented in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes our proposed approach.", "labels": [], "entities": []}, {"text": "Section 4 illustrates the experimental settings.", "labels": [], "entities": []}, {"text": "The results are reported in Section 5.", "labels": [], "entities": []}, {"text": "In Section 6, we discuss our findings.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper and mention the future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation results are reported in case insensitive BLEU scores () with their confidence intervals (CI) and p-values.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9906342029571533}, {"text": "confidence intervals (CI)", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.9156577706336975}]}, {"text": "Bootstrap resampling is used to compute statistical significance intervals).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparing Raw, ATB and D3 Tokenized cases without/with BPE on in-domain test (MT05), in terms of BLEU  scores, where the Confidence Interval (CI) and P -value are reported. Bold font highlights best results by SMT and NMT.", "labels": [], "entities": [{"text": "BPE", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9374734163284302}, {"text": "BLEU", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9990248680114746}, {"text": "Confidence Interval (CI)", "start_pos": 131, "end_pos": 155, "type": "METRIC", "confidence": 0.8867823004722595}, {"text": "P -value", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9472384850184122}, {"text": "SMT", "start_pos": 220, "end_pos": 223, "type": "TASK", "confidence": 0.8211991786956787}, {"text": "NMT", "start_pos": 228, "end_pos": 231, "type": "DATASET", "confidence": 0.9156973361968994}]}, {"text": " Table 2: BLEU score of the length-based system selection (using best models of SMT and NMT) when applied on in-domain  test (MT05).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995161294937134}, {"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.904972493648529}, {"text": "MT05", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.8025597929954529}]}, {"text": " Table 3: BLEU score of the length-based system selection (using best models of SMT and NMT) when applied on out-of- domain test (MT12).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995942711830139}, {"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8889884352684021}, {"text": "MT12", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.6916638016700745}]}]}