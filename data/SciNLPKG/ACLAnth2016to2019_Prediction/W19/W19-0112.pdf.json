{"title": [{"text": "Jabberwocky Parsing: Dependency Parsing with Lexical Noise", "labels": [], "entities": []}], "abstractContent": [{"text": "Parsing models have long benefited from the use of lexical information, and indeed current state-of-the art neural network models for dependency parsing achieve substantial improvements by benefiting from distributed representations of lexical information.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9381000399589539}, {"text": "dependency parsing", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.829536497592926}]}, {"text": "At the same time, humans can easily parse sentences with unknown or even novel words, as in Lewis Carroll's poem Jabberwocky.", "labels": [], "entities": []}, {"text": "In this paper, we carryout jabberwocky parsing experiments, exploring how robust a state-of-the-art neu-ral network parser is to the absence of lexical information.", "labels": [], "entities": []}, {"text": "We find that current parsing models, at least under usual training reg-imens, are in fact overly dependent on lexical information, and perform badly in the jab-berwocky context.", "labels": [], "entities": []}, {"text": "We also demonstrate that the technique of word dropout drastically improves parsing robustness in this setting, and also leads to significant improvements in out-of-domain parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 76, "end_pos": 83, "type": "TASK", "confidence": 0.9727783203125}]}], "introductionContent": [{"text": "Since the earliest days of statistical parsing, lexical information has played a major role).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8230820298194885}]}, {"text": "While some of the performance gains that had been derived from lexicalization can begotten in other ways (, thereby avoiding increases in model complexity and problems in data sparsity, recent neural network models of parsing across a range of formalisms continue to use lexical information to guide parsing decisions (constituent parsing); dependency parsing:;;; CCG parsing:; TAG parsing:; Shi and \u21e4 Work done at Yale University.).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 341, "end_pos": 359, "type": "TASK", "confidence": 0.8037477731704712}, {"text": "CCG parsing", "start_pos": 364, "end_pos": 375, "type": "TASK", "confidence": 0.6139588057994843}, {"text": "TAG parsing", "start_pos": 378, "end_pos": 389, "type": "TASK", "confidence": 0.6559114009141922}]}, {"text": "These models exploit lexical information in away that avoids some of the data sparsity issues, by making use of distributed representations (i.e., word embeddings) that support generalization across different words.", "labels": [], "entities": []}, {"text": "While humans certainly make use of lexical information in sentence processing, it is also clear that we are able to analyze sentences in the absence of known words.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.722794383764267}]}, {"text": "This can be seen most readily by our ability to understand Lewis Carroll's poem, Jabberwocky, in which open class items are replaced by non-words.", "labels": [], "entities": []}, {"text": "Twas brillig, and the slithy toves Did gyre and gimble in the wabe; All mimsy were the borogoves, And the mome raths outgrabe Work in neurolinguistics and psycholinguistics has demonstrated the human capacity for unlexicalized parsing experimentally, showing that humans can analyze syntactic structure even in presence of pseudo-words (.", "labels": [], "entities": []}, {"text": "The word embeddings used by current lexicalized parsers are of no help in sentences with nonce words.", "labels": [], "entities": []}, {"text": "Yet, it is at present unknown the degree to which these parsers are dependent on the information contained in these embeddings.", "labels": [], "entities": []}, {"text": "Parsing evaluation on such nonce sentences is, therefore, critical to bridge the gap between cognitive models and data-driven machine learning models in sentence processing.", "labels": [], "entities": []}, {"text": "Moreover, understanding the degree to which parsers are dependent upon lexical information is also of practical importance.", "labels": [], "entities": []}, {"text": "It is advantageous fora syntactic parser to generalize well across different domains.", "labels": [], "entities": []}, {"text": "Yet, heavy reliance upon lexical information could have detrimental effects on out-of-domain parsing because lexical input will carry genre-specific information.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the contribution of lexical information (via distributed lexical representations) by focusing on a state-of-the-art graphbased dependency parsing model) in a series of controlled experiments.", "labels": [], "entities": []}, {"text": "Concretely, we simulate jabberwocky parsing by adding noise to the representation of words in the input and observe how parsing performance varies.", "labels": [], "entities": [{"text": "jabberwocky parsing", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.643647238612175}]}, {"text": "We test two types of noise: one in which words are replaced with an out-of-vocabulary word without a lexical representation, and a second in which words are replaced with others (with associated lexical representations) that match in their Penn TreeBank (PTB)-style fine-grained part of speech.", "labels": [], "entities": [{"text": "Penn TreeBank (PTB)-style fine-grained part of speech", "start_pos": 240, "end_pos": 293, "type": "DATASET", "confidence": 0.9337555587291717}]}, {"text": "The second approach is similar to the method that propose to assess syntactic generalization in LSTM language models.", "labels": [], "entities": [{"text": "syntactic generalization", "start_pos": 68, "end_pos": 92, "type": "TASK", "confidence": 0.6973468214273453}]}, {"text": "In both cases, we find that the performance of the state-of-the-art graph parser dramatically suffers from the noise.", "labels": [], "entities": []}, {"text": "In fact, we show that the performance of a lexicalized graph-based parser is substantialy worse than an unlexicalized graphbased parser in the presence of lexical noise, even when the lexical content of frequent or function words is preserved.", "labels": [], "entities": []}, {"text": "This dependence on lexical information presents a severe challenge when applying the parser to a different domain or heterogeneous data, and we will demonstrate that indeed parsers trained on the PTB WSJ corpus achieve much lower performance on the Brown corpus.", "labels": [], "entities": [{"text": "PTB WSJ corpus", "start_pos": 196, "end_pos": 210, "type": "DATASET", "confidence": 0.9280170599619547}, {"text": "Brown corpus", "start_pos": 249, "end_pos": 261, "type": "DATASET", "confidence": 0.8906437456607819}]}, {"text": "On the positive side, we find that word dropout (, applied more aggressively than is commonly done, remedies the susceptibility to lexical noise.", "labels": [], "entities": []}, {"text": "Furthermore, our results show that models trained on the PTB WSJ corpus with word dropout significantly outperform those trained without word dropout in parsing the out-of-domain Brown corpus, confirming the practical significance of jabberwocky parsing experiments.", "labels": [], "entities": [{"text": "PTB WSJ corpus", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9373401602109274}, {"text": "jabberwocky parsing", "start_pos": 234, "end_pos": 253, "type": "TASK", "confidence": 0.656152680516243}]}], "datasetContent": [{"text": "We test trained parsers on input that contains two types of lexical noise, designed to assess their ability to abstract away from idiosyncratic/collocational properties of lexical items: 1) colorless green noise and 2) jabberwocky noise.", "labels": [], "entities": []}, {"text": "The former randomly exchanges words with PTB POS preserved, and the latter zeroes outs the embeddings for words (i.e. replacing words with an out-of-vocabulary word).", "labels": [], "entities": []}, {"text": "In either case, we keep POS input to the parsers intact.", "labels": [], "entities": []}, {"text": "Colorless Green Experiments propose a framework to evaluate the generalization ability of LSTM language models that abstracts away from idiosyncratic properties of words or collocational information.", "labels": [], "entities": []}, {"text": "In particular, they generate nonce sentences by randomly replacing words in the original sentences while preserving part-of-speech and morphological features.", "labels": [], "entities": []}, {"text": "This can bethought of as a computational approach to producing sentences that are \"grammatical\" yet meaningless, exemplified by the famous example \"colorless green ideas sleep furiously\".", "labels": [], "entities": []}, {"text": "Concretely, for each PTB POS category, we pick the 50 most frequent words of that category in the training set and replace each word win the test set by a word uniformly drawn from the 50 most frequent words for w's POS category.", "labels": [], "entities": []}, {"text": "We consider three situations: 1) full colorless green experiments where all words are replaced by random words, 2) top 100 colorless green experiments where all words but the 100 most frequent words are replaced by random words, and 3) open class colorless green experiments where the input word is replaced by a random word if and only if the word is an open class word.", "labels": [], "entities": []}, {"text": "1 Jabberwocky Experiments One potential shortcoming with the approach above is that it produces sentences which might violate constraints that are imposed by specific lexical items, but which are not represented by the POS category.", "labels": [], "entities": []}, {"text": "For instance, this approach could generate a sentence like \"it stays the shuttle\" in which the intransitive verb \"stay\" takes an object (Gulordava et al., 2018).", "labels": [], "entities": []}, {"text": "2 Such a violation of argument structure constraints could mislead parsers (as well as language models studied in Gulordava et al.) and we will show that is indeed the case.", "labels": [], "entities": []}, {"text": "To address this issue, we also experiment with jabberwocky noise, in which input word vectors are zeroed out.", "labels": [], "entities": []}, {"text": "This noise is equivalent to replacing words with an out-of-vocabulary word by construction.", "labels": [], "entities": []}, {"text": "Because fine-grained POS information is retained in the input to the parser, the parser is still able to benefit from the kind of morphological information present in Carroll's poem.", "labels": [], "entities": []}, {"text": "We again consider three situations 1) full jabberwocky experiments where all word embeddings are zeroed out, 2) top 100 jabberwocky experiments where word embeddings for all but the most frequent 100 words are zeroed out, and 3) open class jabberwocky experiments where the input word vector is zeroed out if and only if the word is an open class word.", "labels": [], "entities": []}, {"text": "Open class jabberwocky experiments are the closest to the situation when humans read Lewis Carroll's Jabberwocky.", "labels": [], "entities": []}, {"text": "4 Out-of-domain experiments We also explore a practical aspect of our experiments with lexical noise.", "labels": [], "entities": []}, {"text": "We apply our parsers that are trained on the WSJ corpus to the Brown corpus and observe how parsers with various configurations perform.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9681699872016907}, {"text": "Brown corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.8625898361206055}]}, {"text": "Prior work showed that parsers trained on WSJ yield degraded performance on the Brown corpus) despite the fact that the average sentence length is shorter in the Brown corpus (23.85 tokens for WSJ; 20.56 for Brown).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.9004025757312775}, {"text": "Brown corpus", "start_pos": 162, "end_pos": 174, "type": "DATASET", "confidence": 0.8303460478782654}, {"text": "WSJ", "start_pos": 193, "end_pos": 196, "type": "DATASET", "confidence": 0.8914939761161804}]}, {"text": "We show that robustness to lexical noise improves outof-domain parsing.", "labels": [], "entities": []}, {"text": "Baseline Parsers Lexical information is clearly useful for certain parsing decisions, such as PPattachment.", "labels": [], "entities": [{"text": "PPattachment", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.7147111296653748}]}, {"text": "As a result, a lexicalized parser clearly should make use of such information when Prior work in psycholinguistics argued that verbs can in fact be used in novel argument structure constructions, and assigned coherent interpretations on the fly.", "labels": [], "entities": []}, {"text": "Our colorless green parsing experiments can be interpreted as a simulation for such situations.", "labels": [], "entities": []}, {"text": "An anonymous reviewer notes that because of its greater complexity, human performance on a jabberwocky version of the WSJ corpus may not beat the level we find when reading the sentences of Lewis Carroll's poem or in the psycholinguistic work that has explored human ability to process jabberwocky-like sentences.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.8911959230899811}]}, {"text": "We leave it for future work to explore whether human performance in such complex cases is indeed qualitatively different, and also whether the pattern of results changes if we restrict our focus to a syntactically simpler corpus, given a suitable notion of simplicity.", "labels": [], "entities": []}, {"text": "We initially intended to apply our trained parsers to the Universal Dependency corpus () as well for out-of-domain experiments, but we found annotation inconsistency and the problem of conversion from phrase structures to universal dependencies.", "labels": [], "entities": [{"text": "Universal Dependency corpus", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.586532195409139}]}, {"text": "We leave this problem for future.", "labels": [], "entities": []}, {"text": "it is available, and may well perform less well when it is not.", "labels": [], "entities": []}, {"text": "In fact, in jabberwocky and colorless green settings, the absence of lexical information may lead to an underdetermination of the parse by the POS or word sequence, so that there is no non-arbitrary \"gold standard\" parse.", "labels": [], "entities": []}, {"text": "As a result, simply observing a performance drop of a parser in the face of lexical noise does not help to establish an appropriate baseline with respect to how well a parser can be expected to perform in a lexically noisy setting.", "labels": [], "entities": []}, {"text": "We propose three baseline parsers: 1) an unlexicalized parser where the network input is only POS tags, 2) a \"top 100\" parser where the network input is only POS tags and lexical information for the 100 most frequent words and 3) a \"function word\" parser where the network input is only POS tags and lexical information for function words.", "labels": [], "entities": []}, {"text": "Each baseline parser can bethought of as specialized to the corresponding colorless green and jabberwocky experiments.", "labels": [], "entities": []}, {"text": "For example, the unlexicalized parser gives us an upper bound for full colorless green and jabberwocky experiments because the parser is ideally adapted to the unlexicalized situation, as it has no dependence on lexical information.", "labels": [], "entities": []}, {"text": "Experimental Setup We use Universal Dependency representations obtained from converting the Penn Treebank () using Stanford CoreNLP (ver. 3.8.0) ( ).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.9957351088523865}, {"text": "Stanford CoreNLP (ver. 3.8.0", "start_pos": 115, "end_pos": 143, "type": "DATASET", "confidence": 0.9063610911369324}]}, {"text": "We follow the standard data split: sections 2-21, 22, and 23 for training, dev, and test sets respectively.", "labels": [], "entities": [{"text": "standard data split", "start_pos": 14, "end_pos": 33, "type": "DATASET", "confidence": 0.8211151560147604}]}, {"text": "For the out-of-domain experiments, we converted the Brown corpus in PTB again using Stanford CoreNLP into Universal Dependency representations.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 52, "end_pos": 64, "type": "DATASET", "confidence": 0.8038778901100159}, {"text": "PTB", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.5045617818832397}, {"text": "Stanford CoreNLP", "start_pos": 84, "end_pos": 100, "type": "DATASET", "confidence": 0.8928504288196564}]}, {"text": "We only use gold POS tags in training for simplicity, but we conduct experiments with both gold and predicted POS tags.", "labels": [], "entities": []}, {"text": "Experiments with gold POS tags allow us to isolate the effect of lexical noise from POS tagging errors, while those with predicted POS tags simulate more practical situations where POS input is not fully reliable.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, however, we find that relative performance patterns do not change even when using predicted POS tags.", "labels": [], "entities": []}, {"text": "All pre-dicted POS tags are obtained from a BiLSTM POS tagger with character CNNs, trained on the same training data (sections 2-21) with hyperparameters from and word embeddings initialized with GloVe vectors ().", "labels": [], "entities": []}, {"text": "We train 5 parsing models for each training configuration with 5 different random initializations and report the mean and standard deviation.", "labels": [], "entities": []}, {"text": "We use the CoNLL 2017 official script for evaluation.", "labels": [], "entities": [{"text": "CoNLL 2017 official script", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.97069351375103}]}, {"text": "Out-of-domain Parsing  mance in the Brown corpus.", "labels": [], "entities": [{"text": "Parsing  mance", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.814522385597229}, {"text": "Brown corpus", "start_pos": 36, "end_pos": 48, "type": "DATASET", "confidence": 0.9381477534770966}]}, {"text": "In contrast, our results show that lexicalization of the neural network dependency parsers via distributed representations facilitates parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 135, "end_pos": 142, "type": "TASK", "confidence": 0.9645664691925049}]}, {"text": "Most of the gains from lexicalization stem from the 100 most frequent words (row 3) and function words (row 4), but we get a significant improvement by performing relatively aggressive word dropout (\u21b5 = 40).", "labels": [], "entities": []}, {"text": "This confirms the practical significance of jabberwocky parsing experiments.", "labels": [], "entities": [{"text": "jabberwocky parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7300449311733246}]}], "tableCaptions": [{"text": " Table 1: Normal Parsing Results on the Dev Set. The  subscripts indicate the standard deviations.", "labels": [], "entities": [{"text": "Dev Set", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.8004620671272278}]}, {"text": " Table 2: Full Colorless Green and Jabberwocky Exper- iments on the Dev Set.", "labels": [], "entities": [{"text": "Dev Set", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9152434468269348}]}, {"text": " Table 3: Top 100 Colorless Green and Jabberwocky  Experiments on the Dev Set.", "labels": [], "entities": [{"text": "Dev Set", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8960444331169128}]}, {"text": " Table 4: Open Class Colorless Green and Jabberwocky  Experiments on the Dev Set.", "labels": [], "entities": [{"text": "Dev Set", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9043258726596832}]}, {"text": " Table 5: Brown CF Results.", "labels": [], "entities": [{"text": "Brown CF Results", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.9366077184677124}]}, {"text": " Table 6: Performance Breakdown by Dependency Re- lation.", "labels": [], "entities": [{"text": "Dependency Re- lation", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.6105750724673271}]}]}