{"title": [{"text": "TMU Transformer System Using BERT for Re-ranking at BEA 2019 Grammatical Error Correction on Restricted Track", "labels": [], "entities": [{"text": "BERT", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9947836995124817}, {"text": "Re-ranking", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.8796457648277283}, {"text": "BEA 2019 Grammatical", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.7607136766115824}]}], "abstractContent": [{"text": "We introduce our system that is submitted to the restricted track of the BEA 2019 shared task on grammatical error correction 1 (GEC).", "labels": [], "entities": [{"text": "BEA 2019 shared task", "start_pos": 73, "end_pos": 93, "type": "DATASET", "confidence": 0.8553179651498795}, {"text": "grammatical error correction 1 (GEC)", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.6374741920403072}]}, {"text": "It is essential to select an appropriate hypothesis sentence from the candidates list generated by the GEC model.", "labels": [], "entities": [{"text": "GEC", "start_pos": 103, "end_pos": 106, "type": "DATASET", "confidence": 0.8575667142868042}]}, {"text": "A re-ranker can evaluate the naturalness of a corrected sentence using language models trained on large corpora.", "labels": [], "entities": []}, {"text": "On the other hand, these language models and language representations do not explicitly take into account the grammatical errors written by learners.", "labels": [], "entities": []}, {"text": "Thus, it is not straightforward to utilize language representations trained from a large corpus, such as Bidirectional Encoder Representations from Transformers (BERT), in a form suitable for the learner's grammatical errors.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 105, "end_pos": 167, "type": "TASK", "confidence": 0.6697894185781479}]}, {"text": "Therefore, we propose to fine-tune BERT on learner corpora with grammatical errors for re-ranking.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9903783798217773}]}, {"text": "The experimental results of the W&I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance.", "labels": [], "entities": [{"text": "W&I+LOCNESS development dataset", "start_pos": 32, "end_pos": 63, "type": "DATASET", "confidence": 0.8303508332797459}, {"text": "BERT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.994126558303833}]}], "introductionContent": [{"text": "Grammatical error correction (GEC) systems maybe used for language learning to detect and correct grammatical errors in text written by language learners.", "labels": [], "entities": [{"text": "Grammatical error correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7837716986735662}]}, {"text": "GEC has grown in importance over the past few years due to the increasing need for people to learn new languages.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6436236500740051}]}, {"text": "GEC has been addressed in the Helping Our Own (HOO) () and Conference on Natural Language Learning (CoNLL) ( shared tasks between 2011 and 2014.", "labels": [], "entities": [{"text": "GEC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6504384875297546}]}, {"text": "Recent research has demonstrated the effectiveness of the neural machine translation model for GEC.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.7135205467542013}, {"text": "GEC", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9162430167198181}]}, {"text": "There are three main types of neural network models for GEC, namely, recurrent neural networks (), a multi-layer convolutional model based on convolutional neural networks) and a transformer model based on self-attention . We follow the best practices to develop our system based on the transformer model, which has achieved better performance for GEC (.", "labels": [], "entities": [{"text": "GEC", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8133285641670227}, {"text": "GEC", "start_pos": 348, "end_pos": 351, "type": "DATASET", "confidence": 0.8914831876754761}]}, {"text": "Re-ranking using a language model trained on large-scale corpora contributes to the improved hypotheses of the GEC model).", "labels": [], "entities": [{"text": "GEC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.8068380951881409}]}, {"text": "Typically, a language model is trained by maximizing the log-likelihood of a sentence.", "labels": [], "entities": []}, {"text": "Hence, such models observe only the positive examples of a raw corpus.", "labels": [], "entities": []}, {"text": "However, these models may not be sufficient to take into account the grammatical errors written by language learners.", "labels": [], "entities": []}, {"text": "Therefore, we fine-tune these models trained from large-scale raw data on learner corpora to explicitly take into account grammatical errors to re-rank the hypotheses for the GEC tasks.", "labels": [], "entities": [{"text": "GEC tasks", "start_pos": 175, "end_pos": 184, "type": "TASK", "confidence": 0.6364942193031311}]}, {"text": "Bidirectional Encoder Representations from Transformer (BERT) can consider information of large-scale raw corpora and task specific information by fine-tuning on the target task corpora.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformer (BERT", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.6897497602871486}]}, {"text": "Moreover, BERT is known to be effective in the distinction of grammatical sentences from ungrammatical sentences (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979614019393921}, {"text": "distinction of grammatical sentences from ungrammatical sentences", "start_pos": 47, "end_pos": 112, "type": "TASK", "confidence": 0.8149299791881016}]}, {"text": "They proposed a grammatical error detection (GED) model based on BERT that achieved state-of-the-art results in word-level GED tasks.", "labels": [], "entities": [{"text": "grammatical error detection (GED)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7761881748835245}, {"text": "BERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9268607497215271}]}, {"text": "Therefore, we use BERT, pre-trained with large-scale raw corpora, and fine-tune it with learner corpora for re-ranking the hypotheses of our GEC model to utilize not only the large-scale raw corpora but also a set of information on grammatical errors.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9981614947319031}]}, {"text": "The main contribution of this study is that the experimental results demonstrate that BERT, which considers both the representations trained on large-scale and learners corpora, is effective for re-ranking the hypotheses for GEC tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9985272884368896}, {"text": "GEC tasks", "start_pos": 225, "end_pos": 234, "type": "TASK", "confidence": 0.8400355279445648}]}, {"text": "Additionally, we demonstrated that BERT based on self-attention can re-rank sentences corrected from the GEC model by capturing long distance information.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9947722554206848}]}], "datasetContent": [{"text": "In the restricted track, we only used the corpora listed in  The systems submitted to the shared task were evaluated using the ERRANT 7 scorer.", "labels": [], "entities": [{"text": "ERRANT 7 scorer", "start_pos": 127, "end_pos": 142, "type": "METRIC", "confidence": 0.9453555941581726}]}, {"text": "This metric is an improved version of the MaxMatch scorer", "labels": [], "entities": [{"text": "MaxMatch", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9299319982528687}]}], "tableCaptions": [{"text": " Table 1: Number of sentences in corpora on GEC  shared task for restricted track.", "labels": [], "entities": [{"text": "GEC  shared task", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.8474839131037394}]}, {"text": " Table 2: Hyperparameter values of our transformer  GEC model.", "labels": [], "entities": []}, {"text": " Table 3: Results of GEC systems with the highest P, R  and F 0.5 overall vs TMU on restricted track on official  W&I test data.", "labels": [], "entities": [{"text": "P, R  and F 0.5", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.7633695403734843}, {"text": "TMU", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.861011266708374}, {"text": "W&I test data", "start_pos": 114, "end_pos": 127, "type": "DATASET", "confidence": 0.9418950319290161}]}]}