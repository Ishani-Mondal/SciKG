{"title": [{"text": "\"Caption\" as a Coherence Relation: Evidence and Implications", "labels": [], "entities": [{"text": "Caption\"", "start_pos": 1, "end_pos": 9, "type": "TASK", "confidence": 0.9328438639640808}, {"text": "Coherence Relation", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7525337636470795}]}], "abstractContent": [{"text": "We study verbs in image-text corpora, contrasting caption corpora, where texts are explicitly written to characterize image content, with depiction corpora, where texts and images may stand in more general relations.", "labels": [], "entities": []}, {"text": "Captions show a distinctively limited distribution of verbs, with strong preferences for specific tense, aspect, lexical aspect, and semantic field.", "labels": [], "entities": []}, {"text": "These limitations, which appear in data elicited by a range of methods, restrict the utility of caption corpora to inform image retrieval, multimodal document generation, and perceptually-grounded semantic models.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.7126763463020325}, {"text": "multimodal document generation", "start_pos": 139, "end_pos": 169, "type": "TASK", "confidence": 0.6258633534113566}]}, {"text": "We suggest that these limitations reflect the discourse constraints in play when subjects write texts to accompany imagery, so we argue that future development of image-text corpora should work to increase the diversity of event descriptions, while looking explicitly at the different ways text and imagery can be coherently related.", "labels": [], "entities": []}], "introductionContent": [{"text": "Researchers interested in modeling relations between language and the world are increasingly starting from multimodal corpora that combine text with visual information; see for review.", "labels": [], "entities": []}, {"text": "A key benchmark problem, which we explore here, is to learn to produce an appropriate text caption to accompany an image.", "labels": [], "entities": []}, {"text": "This problem brings fundamental scientific and engineering challenges, and has immediate applications, particularly in making online content more accessible.", "labels": [], "entities": []}, {"text": "At the same time, the problem lends itself to appealing high-level characterizations-learning to describe in words what's happening in an image-which suggests that the line of research affords sweeping insights into depiction, image retrieval, and realworld commonsense inference.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 227, "end_pos": 242, "type": "TASK", "confidence": 0.7270100861787796}, {"text": "commonsense inference", "start_pos": 258, "end_pos": 279, "type": "TASK", "confidence": 0.6662539094686508}]}, {"text": "In this paper, we offer a theoretically-situated but empirically-motivated critique of this broader understanding of captioning.", "labels": [], "entities": []}, {"text": "We argue that current image-caption corpora systematically suffer from key deficits in coverage, and therefore cannot underpin general models for linking images and text.", "labels": [], "entities": [{"text": "coverage", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9564532041549683}]}, {"text": "Instead, we suggest that these deficits might be remedied through attention to different corpora and different image-text relationships.", "labels": [], "entities": []}, {"text": "Our starting point is the observation that images and text in multimodal documents are used coherently together: like all contributions to discourse, they stand in particular relations to one another, which guide readers toward the inferential connections intended by the author.", "labels": [], "entities": []}, {"text": "Captioning, we argue, is such a relation.", "labels": [], "entities": [{"text": "Captioning", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9620818495750427}]}, {"text": "A text that is presented as the caption to an image presents restricted kinds of information about the image and adopts a distinctive perspective.", "labels": [], "entities": []}, {"text": "In particular, we suggest, captions characteristically describe imagery as though what we see has been going on indefinitely in the past, is happening now, and will continue indefinitely into the future.", "labels": [], "entities": []}, {"text": "We justify this account of captioning with an empirical study of action descriptions in English image captioning corpora.", "labels": [], "entities": [{"text": "captioning", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9740692973136902}, {"text": "English image captioning corpora", "start_pos": 88, "end_pos": 120, "type": "TASK", "confidence": 0.7034035250544548}]}, {"text": "Our central finding is that they are disproportionately atelic, meaning that they describe an ongoing process in a general way, without invoking its possible goal, endpoint or culmination; see.", "labels": [], "entities": []}, {"text": "This is the difference between painting an advertisement (telic) and using oils (atelic); performing their hit song (telic) and performing onstage (atelic); running a 5K (telic) and simply running (atelic).", "labels": [], "entities": []}, {"text": "Of course, captions frequently feature stative descriptions, which evoke conditions rather than activities: names are etched on a wall, the building towers over the skyline.", "labels": [], "entities": []}, {"text": "Captioning is just one of many possible coherence relations connecting text and imagery: we, which suggests that the photo catches the moment that makes the captions true.", "labels": [], "entities": [{"text": "Captioning", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9432222843170166}]}, {"text": "Many other cases, we argue, are best analyzed in terms of an illustration relation connecting text to an accompanying image.", "labels": [], "entities": []}, {"text": "As shown in(e) and (f), from (Yagcioglu et al., 2018), illustration relations allow for diverse verbs-telic, atelic and stative alike-to be described in the text.", "labels": [], "entities": []}, {"text": "Thus, where vision-language applications involve this illustration relation, as is plausible in many cases of image retrieval, document synthesis, and grounded language use, caption corpora will systematically lack the full range of action descriptions that general solutions must handle.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 110, "end_pos": 125, "type": "TASK", "confidence": 0.7385984361171722}, {"text": "document synthesis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7589018940925598}]}, {"text": "We conclude by arguing that future researchers should focus on naturally-occurring examples, where text and images connect in diverse ways, and should explicitly model the coherence relationships between text and images.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Fraction of verbal part-of-speech tokens accounted for by top K verb lemmas, by corpus. Frequent verbs  disproportionately dominate in captions.", "labels": [], "entities": []}, {"text": " Table 2: Verbs occurring at least 100 times per mil- lion words in COCO (Lin et al., 2014) or Flickr (Young  et al., 2014), shown in their most frequent forms: be  and have (simple present), plus 17 verbs we call the  Frequent Caption Verbs (FCVs) (present participle).", "labels": [], "entities": [{"text": "COCO", "start_pos": 68, "end_pos": 72, "type": "DATASET", "confidence": 0.8644711971282959}, {"text": "Flickr", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.8218711614608765}, {"text": "Frequent Caption Verbs (FCVs)", "start_pos": 219, "end_pos": 248, "type": "TASK", "confidence": 0.7079052974780401}]}, {"text": " Table 4: Grammatical tense and aspect across corpora. Progressive and non-past dominate in Flickr and COCO  whereas the simple form dominates in Recipe, ANC and FS. The dataset from the image-text corpora that is the  closest to ANC with respect to aspect is the Recipe dataset.", "labels": [], "entities": [{"text": "Flickr", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8271136283874512}, {"text": "Recipe dataset", "start_pos": 264, "end_pos": 278, "type": "DATASET", "confidence": 0.968331515789032}]}, {"text": " Table 5: Relative frequency of non-past and progres- sive in verbs produced by eight captioning models  trained on COCO.", "labels": [], "entities": [{"text": "Relative frequency", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9271958470344543}, {"text": "COCO", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8807832598686218}]}, {"text": " Table 7: Corpus frequencies of select verbs (per million words) and counts from the Google Ngram dataset. The  frequencies of worry and wonder are low in both image-text and the Google Ngram datasets. However, the  frequencies of build and draw, while low in image-text corpora, are high in the Google Ngram dataset.", "labels": [], "entities": [{"text": "Google Ngram dataset", "start_pos": 85, "end_pos": 105, "type": "DATASET", "confidence": 0.9429097374280294}, {"text": "Google Ngram datasets", "start_pos": 179, "end_pos": 200, "type": "DATASET", "confidence": 0.9220089515050253}, {"text": "Google Ngram dataset", "start_pos": 296, "end_pos": 316, "type": "DATASET", "confidence": 0.9523044029871622}]}, {"text": " Table 8: Counts of telic verbs out of 500 randomly se- lected sentences from each dataset. Pairwise compar- isons of datasets suggest that every datasets is signifi- cantly different from others with the exception of two  pairs; COCO and Flickr as well as Recipe and ANC. In  general, the caption corpora contain fewer telic verbs  in comparison to ANC and Recipe.", "labels": [], "entities": []}]}