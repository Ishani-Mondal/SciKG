{"title": [{"text": "Distilling weighted finite automata from arbitrary probabilistic models", "labels": [], "entities": [{"text": "Distilling weighted finite automata", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8510848134756088}]}], "abstractContent": [{"text": "Weighted finite automata (WFA) are often used to represent probabilistic models, such as n-gram language models, since they are efficient for recognition tasks in time and space.", "labels": [], "entities": []}, {"text": "The probabilistic source to be represented as a WFA, however, may come in many forms.", "labels": [], "entities": []}, {"text": "Given a generic probabilistic model over sequences , we propose an algorithm to approximate it as a weighted finite automaton such that the Kullback-Leibler divergence between the source model and the WFA target model is minimized.", "labels": [], "entities": []}, {"text": "The proposed algorithm involves a counting step and a difference of convex optimization , both of which can be performed efficiently.", "labels": [], "entities": []}, {"text": "We demonstrate the usefulness of our approach on some tasks including distilling n-gram models from neural models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Given a sequence of symbols x 1 , x 2 , . .", "labels": [], "entities": []}, {"text": ", x n\u22121 , where symbols are drawn from the alphabet \u03a3, a probabilistic model S assigns to the next symbol x n \u2208 \u03a3 the conditional probability p s [x n | x n\u22121 . .", "labels": [], "entities": []}, {"text": "Such a model might be Markovian, where p s [x n | x n\u22121 . .", "labels": [], "entities": []}, {"text": "x 1 ] = p s [x n | x n\u22121 . .", "labels": [], "entities": []}, {"text": "x n\u2212k+1 ], such as a k-gram language model (LM)) or it might be non-Markovian such as along short-term memory (LSTM) neural network language model.", "labels": [], "entities": []}, {"text": "Our goal is to approximate a probabilistic model as a weighted finite automaton (WFA) such that the weight assigned by the WFA is close to the probability assigned by the source model.", "labels": [], "entities": []}, {"text": "Specifically, we will seek to minimize the Kullback weighting in the approximation.", "labels": [], "entities": []}, {"text": "In some applications, the topology maybe unknown.", "labels": [], "entities": []}, {"text": "In such cases, one choice is to build a kgram deterministic finite automaton (DFA) topology from a corpus drawn from S (.", "labels": [], "entities": []}, {"text": "This could be from an existing corpus or from random samples drawn from S.", "labels": [], "entities": []}, {"text": "Figure 1 shows a trigram topology for the very simple corpus aab.", "labels": [], "entities": []}, {"text": "This representation makes use of failure transitions.", "labels": [], "entities": []}, {"text": "These allow modeling strings unseen in the corpus (e.g., abab) in a compact way by failing or backing-off to states that correspond to lower-order histories.", "labels": [], "entities": []}, {"text": "Such models can be made more elaborate if some transitions represent classes, such as names or numbers, that are themselves represented by sub-automata.", "labels": [], "entities": []}, {"text": "As mentioned previously, we will mostly assume we have a topology either pre-specified or inferred by some means and focus on how to weight that topology to best approximate the source distribution.", "labels": [], "entities": []}, {"text": "In previous work, there have been various approaches for estimating weighted automata.", "labels": [], "entities": [{"text": "estimating weighted automata", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8873264789581299}]}, {"text": "Methods include state merging and weight estimation from a prefix tree data representation), the EM algorithm applied to fully connected HMMs or specific topologies and spectral methods applied to automata).", "labels": [], "entities": [{"text": "state merging", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7263931632041931}]}, {"text": "For approximating neural network (NN) models as WFAs, methods have been proposed to build n-gram models from RNN samples), from DNNs trained at different orders (, and from RNNs with quantized hidden states.", "labels": [], "entities": []}, {"text": "Our paper is distinguished in several respects from previous work.", "labels": [], "entities": []}, {"text": "First, our general approach does not depend on the form the source distribution.", "labels": [], "entities": []}, {"text": "Second, our targets area wide class of deterministic automata with failure transitions.", "labels": [], "entities": []}, {"text": "Third, we search for the minimal KL divergence between the source and target distributions, given a fixed target topology.", "labels": [], "entities": []}, {"text": "We remark that if the source probabilistic model is represented as a WFA, our approximation will in general give a different solution than forming the finite-state intersection with the topology and weight-pushing to normalize the result.", "labels": [], "entities": []}, {"text": "Our approximation has the same states as the topology whereas a weightpushed intersection could have many more states and and is not an approximation, but an exact representation, of the source distribution.", "labels": [], "entities": []}, {"text": "Before presenting and validating algorithms fora minimum KL divergence approximation when either the source itself is finite-state or not (in which case sampling is involved), we next present the theoretical formulation of the problem and the minimum KL divergence approximation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now provide experimental evidence of the theory's validity and show its usefulness in various applications.", "labels": [], "entities": []}, {"text": "For the ease of notation, we use WFA-APPROX to denote the exact counting algorithm described in Section 3.1.1 followed by the KL-MINIMIZATION algorithm of Section 3.2.", "labels": [], "entities": []}, {"text": "Similarly, we use WFA-SAMPLEAPPROX(N) to denote the sampled counting described in Section 3.1.2 with N sampled sentences followed by KL-MINIMIZATION.", "labels": [], "entities": []}, {"text": "We first give experimental evidence that supports the theory in Section 4.1.", "labels": [], "entities": []}, {"text": "We then show how to approximate neural models as WFAs in Section 4.2.", "labels": [], "entities": []}, {"text": "We also use the proposed method to provide lower bounds on the perplexity given a target topology in Section 4.3.", "labels": [], "entities": []}, {"text": "For all the experiments we use the 1996 CSR Hub4 Language Model data, LDC98T31 from the Broadcast News (BN) task.", "labels": [], "entities": [{"text": "1996 CSR Hub4 Language Model data", "start_pos": 35, "end_pos": 68, "type": "DATASET", "confidence": 0.8102869242429733}, {"text": "Broadcast News (BN) task", "start_pos": 88, "end_pos": 112, "type": "DATASET", "confidence": 0.8950186371803284}]}, {"text": "We use the processed form of the corpus and further process it to downcase all the words and remove punctuation.", "labels": [], "entities": []}, {"text": "The resulting dataset has 132M words in the training set, 20M words in the test set, and has 240K unique words.", "labels": [], "entities": []}, {"text": "From this, we create a vocabulary of approximately 32K words consisting of all words that appeared more than 50 times in the training corpus.", "labels": [], "entities": []}, {"text": "Using this vocabulary, we create a trigram Katz model and prune it to contain 2M n-grams using entropy pruning), which we use as a baseline in all our experiments.", "labels": [], "entities": []}, {"text": "We use Katz smoothing since it is amenable to pruning.", "labels": [], "entities": [{"text": "Katz smoothing", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.6734743565320969}]}, {"text": "The perplexity of this model on the test set is 144.4. 7 All algorithms were implemented using the open-source OpenFst and OpenGrm n-gram and stochastic automata (SFst) libraries 8 with the last library including these implementations).", "labels": [], "entities": [{"text": "OpenFst", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9510610103607178}]}], "tableCaptions": []}