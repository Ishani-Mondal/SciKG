{"title": [{"text": "UU TAILS at MEDIQA 2019: Learning Textual Entailment in the Medical Domain", "labels": [], "entities": [{"text": "UU", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7190585732460022}, {"text": "TAILS", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.7482185363769531}, {"text": "MEDIQA 2019", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.7749145030975342}, {"text": "Learning Textual Entailment", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.6059371630350748}]}], "abstractContent": [{"text": "This article describes the participation of the UU TAILS team in the 2019 MEDIQA challenge intended to improve domain-specific models in medical and clinical NLP.", "labels": [], "entities": [{"text": "UU TAILS", "start_pos": 48, "end_pos": 56, "type": "DATASET", "confidence": 0.8354757130146027}]}, {"text": "The challenge consists of 3 tasks: medical language inference (NLI), recognizing textual entail-ment (RQE) and question answering (QA).", "labels": [], "entities": [{"text": "medical language inference (NLI)", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.6824593196312586}, {"text": "question answering (QA)", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.7816674530506134}]}, {"text": "Our team participated in tasks 1 and 2 and our best runs achieved a performance accuracy of 0.852 and 0.584 respectively for the test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9886848330497742}]}, {"text": "The models proposed for task 1 relied on BERT embeddings and different ensemble techniques.", "labels": [], "entities": [{"text": "BERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.963176429271698}]}, {"text": "For the RQE task, we trained a traditional multilayer perceptron network based on embeddings generated by the universal sentence encoder.", "labels": [], "entities": [{"text": "RQE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9580985903739929}]}], "introductionContent": [{"text": "Detecting semantic relations between sentence pairs is a long-standing challenge for computational semantics.", "labels": [], "entities": [{"text": "Detecting semantic relations between sentence pairs", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.9046302239100138}]}, {"text": "Given two snippets of text: Premise P and Hypothesis H, textual entailment recognition determines if the meaning of H can be inferred from that of P (.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.7663928667704264}]}, {"text": "The significance of modeling text inference is evident since it evaluates the capability of Natural language Processing (NLP) to grasp meaning and interprets the linguistic variability of the language.", "labels": [], "entities": []}, {"text": "Natural language inference (NLI) tasks, also known as Recognizing Textual Entailment (RTE) require a deep understanding of the semantic similarity between the hypothesis and the premise.", "labels": [], "entities": [{"text": "Natural language inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7156435151894888}, {"text": "Recognizing Textual Entailment (RTE", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.6430070877075196}]}, {"text": "Moreover, they overlap with other linguistic problems such as question answering and semantic text similarity.", "labels": [], "entities": [{"text": "question answering", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.8836236298084259}, {"text": "semantic text similarity", "start_pos": 85, "end_pos": 109, "type": "TASK", "confidence": 0.6661941309769949}]}, {"text": "The recent years witnessed regular organization of shared tasks targeting the RTE/NLI task, which consequently led to advances in the field.", "labels": [], "entities": [{"text": "RTE/NLI task", "start_pos": 78, "end_pos": 90, "type": "TASK", "confidence": 0.6052067279815674}]}, {"text": "More complex models were developed that rely on deep neural networks, this was feasible with the availability of large amounts of annotated datasets such as SNLI and MultiNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 166, "end_pos": 174, "type": "DATASET", "confidence": 0.9113779664039612}]}, {"text": "However, most models fail to generalize across different NLI benchmarks.", "labels": [], "entities": []}, {"text": "Additionally, they do not perform accurately on domain-specific datasets.", "labels": [], "entities": []}, {"text": "This is specifically true in the medical and clinical domain.", "labels": [], "entities": []}, {"text": "Compared to open domain data, the language used to describe biomedical events is usually complex, rich in clinical semantics and contains conceptual overlap.", "labels": [], "entities": []}, {"text": "And hence, it is difficult to adapt any of the former models directly.", "labels": [], "entities": []}, {"text": "The MEDIQA challenge addresses the above limitations through its three proposed tasks.", "labels": [], "entities": []}, {"text": "The first task aims at identifying inference relations between clinical sentence pairs and introduces the medical natural language inference benchmark dataset.", "labels": [], "entities": [{"text": "medical natural language inference benchmark dataset", "start_pos": 106, "end_pos": 158, "type": "DATASET", "confidence": 0.5529755155245463}]}, {"text": "Its creation process is similar to the creation of the gold-standard SNLI dataset with adaptation to the clinical domain.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8359985947608948}]}, {"text": "Expert annotators were presented 4,638 premises extracted from the MIMIC-III database and were asked to write three hypotheses with a true, false and neutral description of the premise.", "labels": [], "entities": [{"text": "MIMIC-III database", "start_pos": 67, "end_pos": 85, "type": "DATASET", "confidence": 0.9112736284732819}]}, {"text": "The final dataset comprises 14,049 sentence pairs divided into 11,232, 1,395 and 1,422 for training, development and testing respectively.", "labels": [], "entities": []}, {"text": "An additional test batch was provided by the challenge organizers with 405 unlabelled instances.", "labels": [], "entities": []}, {"text": "Similarly, the second task, Recognizing Question Entailment (RQE), tackles the problem of finding duplicate questions by labeling questions based on their similarity.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.8156198114156723}]}, {"text": "Extending the earlier NLI definition, the authors define question entailment as \"Question A entails Question B if every answer to B is also a correct answer to A exactly or partially\".", "labels": [], "entities": [{"text": "question entailment", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.8249744176864624}]}, {"text": "The dataset is specifically designed to find the most similar frequently asked question (FAQ) to a given question.", "labels": [], "entities": []}, {"text": "The training set was constructed from the questions provided by family doctors on the National Library of Medicine (NLM) platform resulting in 8,588 question pairs where 54.2% are positive pairs.", "labels": [], "entities": [{"text": "National Library of Medicine (NLM) platform", "start_pos": 86, "end_pos": 129, "type": "DATASET", "confidence": 0.9658940210938454}]}, {"text": "For validation, two sources of questions were used: validated questions from the NLM collections and FAQs retrieved from the National Institutes of Health (NIH) website.", "labels": [], "entities": [{"text": "validation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9727635383605957}, {"text": "NLM collections", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.7990117073059082}, {"text": "FAQs", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.7383890151977539}]}, {"text": "The validation set has 302 pairs of questions with 42.7% pairs positively labelled.", "labels": [], "entities": []}, {"text": "The test set for the challenge was balanced and comprised of 230 question pairs.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 briefly discusses related work.", "labels": [], "entities": []}, {"text": "We limit our summary to textual inference research in the biomedical domain only.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our proposed model and the implementation details for both tasks.", "labels": [], "entities": []}, {"text": "In Section 4, we show the experiment results of our proposed models.", "labels": [], "entities": []}, {"text": "Finally, we conclude our analysis of the challenge, as well as some additional discussions of the future directions in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results of our team runs on the MEDIQA challenge for the NLI task.", "labels": [], "entities": [{"text": "MEDIQA challenge", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.6152624487876892}]}, {"text": " Table 3: Results of our team runs on the MEDIQA challenge for the RQE task.", "labels": [], "entities": [{"text": "RQE task", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.6809474527835846}]}]}