{"title": [{"text": "Encoding Position Improves Recurrent Neural Text Summarizers", "labels": [], "entities": [{"text": "Encoding Position Improves Recurrent Neural Text Summarizers", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.7785900064877102}]}], "abstractContent": [{"text": "Modern text summarizers are big neural networks (recurrent, convolutional, or transformers) trained end-to-end under an encoder-decoder framework.", "labels": [], "entities": [{"text": "text summarizers", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7214529514312744}]}, {"text": "These networks equipped with an attention mechanism, that maintains a memory of their source hidden states, are able to generalize well to long text sequences.", "labels": [], "entities": []}, {"text": "In this paper, we explore how the different modules involved in an encoder-decoder structure affect the produced summary quality as measured by ROUGE score in the widely used CNN/Daily Mail and Gigaword summariza-tion datasets.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9776864349842072}, {"text": "CNN/Daily Mail and Gigaword summariza-tion datasets", "start_pos": 175, "end_pos": 226, "type": "DATASET", "confidence": 0.8330270275473595}]}, {"text": "We find that encoding the position of the text tokens before feeding them to a recurrent text summarizer gives a significant, in terms of ROUGE, gain to its performance on the former but not the latter dataset.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.9976279139518738}]}], "introductionContent": [{"text": "Within NLP a number of tasks involve generating text conditioned on some input information (machine translation, image caption generation, headline generation, single and multi-document summarization).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7692492008209229}, {"text": "image caption generation", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.843255082766215}, {"text": "headline generation", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8246311843395233}]}, {"text": "To accomplish the task of text summarization, a system needs the ability to capture the semantic content of the source text and then predict its grammatical, faithful and coherent summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 26, "end_pos": 44, "type": "TASK", "confidence": 0.66081902384758}]}, {"text": "Since the structure of the system summary has to be closely related to the structure of the input text a central challenge to this task is the problem of alignment, i.e. the problem of how to relate subelements of the input to sub-elements of the output (.", "labels": [], "entities": []}, {"text": "Similar to a human-produced summary that intuitively is as good as the clarity of her thoughts and goals, a machine-generated summary depends heavily on the quality of its internal information.", "labels": [], "entities": [{"text": "clarity", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9839531779289246}]}, {"text": "For neural network summarizers that is equivalent to strong representations of the source document and of the summary generated so far, both kept as vectors, respectively, in their encoder and decoder hidden states.", "labels": [], "entities": []}, {"text": "Deep learning methods, employing end-to-end trained neural network models, have recently achieved significant, although not robust, ability in generating reasonable multi-sentence abstractive summaries of long news articles.", "labels": [], "entities": [{"text": "multi-sentence abstractive summaries of long news articles", "start_pos": 165, "end_pos": 223, "type": "TASK", "confidence": 0.7991213585649218}]}, {"text": "Extending the sequence-to-sequence framework, already adapted in other sequence transduction tasks, these models mostly consist of three cooperating modules, whose parameters are learned jointly through gradient descent or reinforcement learning techniques.", "labels": [], "entities": []}, {"text": "First, an encoder mechanism that produces hidden representations of the source document; second, an attention network that selects its salient information; and third, a decoder module that produces the model summary.", "labels": [], "entities": []}, {"text": "This decoder module is often an autoregressive 1 network that splits high dimensional data into a sequence of small pieces and then predicts each piece from those before.", "labels": [], "entities": []}, {"text": "For most languages, these neural models perform summarization in a left to right manner, one word at a time, until a special stop token is generated, which ends the summary.", "labels": [], "entities": []}, {"text": "This information processing pipeline can be seen as a four step process \"embed -encode -attend -predict\".", "labels": [], "entities": []}, {"text": "In the \"embed\" step lexical tokens are converted from indices in a vocabulary to dense vectors, encoding distributional semantics.", "labels": [], "entities": []}, {"text": "Then, in the \"encode\" step information is passed through hidden neu-ral connections (either recurrent, convolutional or feed-forward cells) building the source document matrix representation.", "labels": [], "entities": []}, {"text": "Each row of this matrix encodes the \"meaning\" of each token in the context of its surrounding tokens.", "labels": [], "entities": []}, {"text": "Next, in the \"attend\" step the previous matrix is reduced to a vector while ensuring this reduction comes with minimal information loss, reflecting the goal of the attention mechanism to select the most important element from each time step.", "labels": [], "entities": []}, {"text": "The final \"predict\" step reduces this vector to a prediction of the next token in the summary.", "labels": [], "entities": []}, {"text": "Recently, convolutional and self-attentive purely feed-forward () networks have proven able to match the performance of recurrent neural networks () in the role of encoder and decoder modules, replacing them on several sequence generation tasks.", "labels": [], "entities": [{"text": "sequence generation", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.7173129618167877}]}, {"text": "Used as summarizers, these models can produce not only general but also topic-aware ( , query-based (, or user-controllable summaries.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9682899713516235}]}, {"text": "However, in this work we choose to only focus on general summaries.", "labels": [], "entities": []}, {"text": "Creating summaries from documents, seen as a sequential decision making problem for the decoder-agent, is also amenable to reinforcement learning techniques.", "labels": [], "entities": []}, {"text": "In this setting, the model at each step learns to make a decision of the next token to generate while optimizing a sequence-level objective, the full sequence ROUGE score).", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 159, "end_pos": 170, "type": "METRIC", "confidence": 0.9106780886650085}]}, {"text": "Here, arises the issue of the explorationexploitation tradeoff, a problem but also an opportunity for the agent to generate a more diverse, hence more abstract and human-like summary).", "labels": [], "entities": []}, {"text": "In the standard supervised setting, the model needs labeled summaries in the training phase to provide the appropriate learning signal.", "labels": [], "entities": []}, {"text": "In an unsupervised setting, a model could potentially learn to summarize documents without having access to ground-truth summaries in the learning phase ().", "labels": [], "entities": [{"text": "summarize documents", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.9014691114425659}]}, {"text": "The quality of the produced system summaries can be rated both by automatic metrics (ROUGE, Meteor) and by human raters.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.9782801866531372}]}, {"text": "Intuitively, a high quality summary should be a concise text that captures the salient and rejects the secondary information of the source document.", "labels": [], "entities": []}, {"text": "It would use grammatical language structures and include a signifi-: Example of different model generated twosentence summaries of the same input text (source document).", "labels": [], "entities": []}, {"text": "Reference denotes the ground-truth summary.", "labels": [], "entities": []}, {"text": "With position encoding (our model) we see more abstractive ability, while without position encoding (baseline model) we see less paraphrasing and more copying from input text.", "labels": [], "entities": []}, {"text": "cant amount of novel words and phrases not found in the source text.", "labels": [], "entities": []}, {"text": "The key contribution of this work is the novel use of the token-position information in a recurrent neural text summarizer.", "labels": [], "entities": [{"text": "recurrent neural text summarizer", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.6034729182720184}]}, {"text": "We show that our neural network approach, while requiring fewer learnable parameters than a transformer model, outperforms it on the CNN/Daily Mail dataset ( and performs on par with it on the Gigaword corpus ( . These results suggest we do not need the computationheavy self-attention processing of the transformer architecture in neural text summarizers.", "labels": [], "entities": [{"text": "CNN/Daily Mail dataset", "start_pos": 133, "end_pos": 155, "type": "DATASET", "confidence": 0.9463363289833069}, {"text": "Gigaword corpus", "start_pos": 193, "end_pos": 208, "type": "DATASET", "confidence": 0.9470407366752625}]}], "datasetContent": [{"text": "We perform experiments on the CNN/Daily-Mail news articles summarization dataset) and the Gigaword sentence summarization/headline generation corpus ( , which are both standard corpora for long and short document summarization.", "labels": [], "entities": [{"text": "CNN/Daily-Mail news articles summarization dataset", "start_pos": 30, "end_pos": 80, "type": "DATASET", "confidence": 0.8445528958524976}, {"text": "Gigaword sentence summarization/headline generation", "start_pos": 90, "end_pos": 141, "type": "TASK", "confidence": 0.693815658489863}]}, {"text": "For the CNN/Daily-Mail train and validation splits, we truncate source text to 400 tokens and target summaries to 100 tokens, following standard practice.", "labels": [], "entities": [{"text": "CNN/Daily-Mail train", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.905182957649231}, {"text": "validation splits", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9672967493534088}]}, {"text": "We limit both input and output vocabulary to the 50000 most frequent words, and replace the rest with UNK tokens.", "labels": [], "entities": []}, {"text": "For training on the Gigaword dataset we follow the pre-processing steps of ( , replacing all digit characters with # and tokens seen less than five times with UNK.", "labels": [], "entities": [{"text": "Gigaword dataset", "start_pos": 20, "end_pos": 36, "type": "DATASET", "confidence": 0.9671334028244019}]}, {"text": "shows the main statistics for both corpora.", "labels": [], "entities": []}, {"text": "For both CNN/DM and Gigaword datasets, we report the full length F-1 scores of the ROUGE-1, ROUGE-2 and ROUGE-L metrics and their average (R-AVG).", "labels": [], "entities": [{"text": "CNN/DM", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.9096043705940247}, {"text": "Gigaword datasets", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.9372815489768982}, {"text": "F-1", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.930361270904541}, {"text": "R-AVG", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.9645377397537231}]}], "tableCaptions": [{"text": " Table 1: Dataset statistics. DL and SL denote average  number of tokens in source document and summary,  respectively.", "labels": [], "entities": [{"text": "DL", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9859461188316345}, {"text": "SL", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.925316333770752}]}, {"text": " Table 2: Rouge scores on the CNN/DM test set.", "labels": [], "entities": [{"text": "Rouge", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9585098624229431}, {"text": "CNN/DM test set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.9581528186798096}]}, {"text": " Table 3: Rouge scores on the Gigaword test set.", "labels": [], "entities": [{"text": "Rouge", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9444802403450012}, {"text": "Gigaword test set", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9618128935496012}]}]}