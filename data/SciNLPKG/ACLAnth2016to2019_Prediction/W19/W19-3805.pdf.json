{"title": [{"text": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings", "labels": [], "entities": [{"text": "Evaluating the Underlying Gender Bias", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7209564447402954}]}], "abstractContent": [{"text": "Gender bias is highly impacting natural language processing applications.", "labels": [], "entities": []}, {"text": "Word embed-dings have clearly been proven both to keep and amplify gender biases that are present in current data sources.", "labels": [], "entities": []}, {"text": "Recently, contextual-ized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in.", "labels": [], "entities": []}, {"text": "In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias.", "labels": [], "entities": []}, {"text": "Our analysis includes different measures previously applied in the literature to standard word em-beddings.", "labels": [], "entities": []}, {"text": "Our findings suggest that contextu-alized word embeddings are less biased than standard ones even when the latter are debi-ased.", "labels": [], "entities": []}], "introductionContent": [{"text": "Social biases in machine learning, in general and in natural language processing (NLP) applications in particular, are raising the alarm of the scientific community.", "labels": [], "entities": []}, {"text": "Examples of these biases are evidences such that face recognition systems or speech recognition systems work better for white men than for ethnic minorities.", "labels": [], "entities": [{"text": "face recognition", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.8442965149879456}, {"text": "speech recognition", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7240548431873322}]}, {"text": "Examples in the area of NLP are the case of machine translation that systems tend to ignore the coreference information in benefit of a stereotype or sentiment analysis where higher sentiment intensity prediction is biased fora particular gender.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7965060770511627}, {"text": "sentiment intensity prediction", "start_pos": 182, "end_pos": 212, "type": "TASK", "confidence": 0.592524786790212}]}, {"text": "In this work we focus on the particular NLP area of word embeddings (, which represent words in a numerical vector space.", "labels": [], "entities": []}, {"text": "Word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words (e.g. man is to woman as king is to queen).", "labels": [], "entities": []}, {"text": "Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (.", "labels": [], "entities": []}, {"text": "Pre-trained word embeddings are used in many NLP downstream tasks, such as natural language inference (NLI), machine translation (MT) or question answering (QA).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.8315335035324096}, {"text": "question answering (QA)", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.8743287324905396}]}, {"text": "Recent progress in word embedding techniques has been achieved with contextualized word embeddings ( which provide different vector representations for the same word in different contexts.", "labels": [], "entities": []}, {"text": "While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (, it is not the case for the latest techniques of contextualized word embeddings.", "labels": [], "entities": []}, {"text": "Only just recently, present a first analysis on the topic based on the proposed methods in.", "labels": [], "entities": []}, {"text": "In this paper, we further analyse the presence of gender biases in contextualized word embeddings by means of the proposed methods in.", "labels": [], "entities": []}, {"text": "For this, in section 2 we provide an overview of the relevant work on which we build our analysis; in section 3 we state the specific request questions addressed in this work, while in section 4 we describe the experimental framework proposed to address them and in section 5 we present the obtained and discuss the results; finally, in section 6 we draw the conclusions of our work and propose some further research.", "labels": [], "entities": []}], "datasetContent": [{"text": "As follows, we define the data and resources that we use for performing our experiments.", "labels": [], "entities": []}, {"text": "The approach motivation is applying the experiments on contextualized word embeddings.", "labels": [], "entities": []}, {"text": "We worked with the English-German news corpus from the WMT18 1 . We used the English side with 464,947 lines and 1,004,6125 tokens.", "labels": [], "entities": [{"text": "English-German news corpus from the WMT18 1", "start_pos": 19, "end_pos": 62, "type": "DATASET", "confidence": 0.8206289197717395}]}, {"text": "To perform our analysis, we used a set of lists from previous work.", "labels": [], "entities": []}, {"text": "We refer to the list of definitional pairs 2 as 'Definitonal List' (e.g. shehe, girl-boy).", "labels": [], "entities": []}, {"text": "We refer to the list of female and male professions 3 as 'Professional List' (e.g. accountant, surgeon).", "labels": [], "entities": []}, {"text": "The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token).", "labels": [], "entities": []}, {"text": "This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( \u2212 \u2192 he-\u2212\u2192 she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male).", "labels": [], "entities": []}, {"text": "The 'Extended Biased List' is the list used in classification experiment, which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List . A note to be considered, is that the lists we used in our experiments (and obtained from and) may contain words that are missing in our corpus and so we cannot obtain contextualized embeddings for them.", "labels": [], "entities": []}, {"text": "Among different approaches to contextualized word embeddings (mentioned in section 2), we choose ELMo () as contextualized word embedding approach.", "labels": [], "entities": []}, {"text": "The motivation for using ELMo instead of other approaches like BERT () is that ELMo provides word-level representations, as opposed to BERT's subwords.", "labels": [], "entities": [{"text": "BERT", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8181061148643494}]}, {"text": "This makes it possible to study the word-level semantic traits directly, without resorting to extra steps to compose word-level information from the subwords that could interfere with our analyses.", "labels": [], "entities": []}, {"text": "There is no standard measure for gender bias, and even less for such the recently proposed contextualized word embeddings.", "labels": [], "entities": []}, {"text": "In this section, we adapt gender bias measures for word embedding methods from previous work ( and to be applicable to contextualized word embeddings.", "labels": [], "entities": []}, {"text": "We start by computing the gender subspace from the ELMo vector representations of genderdefining words, then identify the presence of direct bias in the contextualized representations.", "labels": [], "entities": []}, {"text": "We then proceed to identify gender information by means of clustering and classifications techniques.", "labels": [], "entities": []}, {"text": "We compare our results to previous results from debiased and non-debiased word embeddings . In the case of contextualized embeddings, there is not just a single representation for each word, but its representation depends on the sentence it appears in.", "labels": [], "entities": []}, {"text": "Hence, in order to compute the gender subspace we take the representation of words by randomly sampling sentences that contain words from the Definitional List and, for each of them, we swap the definitional word with its pair-wise equivalent from the opposite gender.", "labels": [], "entities": []}, {"text": "We then obtain the ELMo representation of the definintional word in each sentence pair, computing their difference.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9535043239593506}]}, {"text": "On the set of difference vectors, we compute their principal components to verify the presence of bias.", "labels": [], "entities": []}, {"text": "In order to have a reference, we computed the principal components of representation of random words.", "labels": [], "entities": []}, {"text": "Similarly to, shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings.", "labels": [], "entities": []}, {"text": "In the right graph of the figure, we can easily note the difference in the case of random, where the data is not concentrated in a specific direction, as the weight is spread among all components.", "labels": [], "entities": []}, {"text": "A similar conclusion was stated in the recent work () where the authors applied the same approach, but for gender swapped variants of sentences with professions.", "labels": [], "entities": []}, {"text": "They computed the difference between the vectors of occupation words in corresponding sentences and got a skewed graph where the first component represent the gender information while the second component groups the male and female related words.", "labels": [], "entities": []}, {"text": "Direct Bias Direct Bias is a measure of how close a certain set of words are to the gender vector.", "labels": [], "entities": [{"text": "Direct Bias Direct Bias", "start_pos": 0, "end_pos": 23, "type": "METRIC", "confidence": 0.587498851120472}]}, {"text": "To compute it, we extracted from the training data the sentences that contain words in the Professional List.", "labels": [], "entities": [{"text": "Professional List", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.9310744106769562}]}, {"text": "We excluded the sentences that have both a professional token and definitional gender word to avoid the influence of the latter over the presence of bias in the former.", "labels": [], "entities": []}, {"text": "We applied the definition of direct bias from on the ELMo representations of the professional words in these sentences.", "labels": [], "entities": []}, {"text": "where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.", "labels": [], "entities": []}, {"text": "We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in.", "labels": [], "entities": []}, {"text": "This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings.", "labels": [], "entities": []}, {"text": "Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.", "labels": [], "entities": []}, {"text": "Male and female-biased words clustering.", "labels": [], "entities": []}, {"text": "In order to study if biased male and female words cluster together when applying contextualized embeddings, we used k-means to generate 2 clusters of the embeddings of tokens from the Biased list.", "labels": [], "entities": []}, {"text": "Note that we cannot use several representations for each word, since it would not make any sense to cluster one word as male and female at the same time.", "labels": [], "entities": []}, {"text": "Therefore, in order to make use of the advantages of the contextualized embeddings, we repeated 10 independent experiments, each with a different random sentence of each word from the list of biased male and female words.", "labels": [], "entities": []}, {"text": "Among these 10 experiments, we got a minimum accuracy of 69.1% and a maximum of 71.3%, with average accuracy of 70.1%, much lower than in the case of biased and debiased word embeddings which were 99.9 and 92.5, respectively, as stated in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9265521764755249}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9354533553123474}]}, {"text": "Based on this criterion, even if there is still bias information to be removed from contextualized embeddings, it is much less than in case of standard word embeddings, even if debiased.", "labels": [], "entities": []}, {"text": "The clusters (for one particular experiment out of the 10 of them) are shown in after applying UMAP ( ) to the contextualized embeddings.", "labels": [], "entities": []}, {"text": "Classification Approach In order to study if contextualized embeddings learn to generalize bias, we trained a Radial Basis Function-kernel Support Vector Machine classifier on the embeddings of random 1000 biased words from the Extended Biased List.", "labels": [], "entities": [{"text": "Radial Basis Function-kernel Support Vector Machine classifier", "start_pos": 110, "end_pos": 172, "type": "TASK", "confidence": 0.7664179376193455}, {"text": "Extended Biased List", "start_pos": 228, "end_pos": 248, "type": "DATASET", "confidence": 0.7797768115997314}]}, {"text": "After that, we evaluated the generalization on the other random 4000 biased tokens.", "labels": [], "entities": []}, {"text": "Again, we performed 10 independent experiments, to guarantee randomization of word representations.", "labels": [], "entities": []}, {"text": "Among these 10 experiments, we got a minimum accuracy of 83.33% and a maximum of 88.43%, with average accuracy of 85.56%.", "labels": [], "entities": [{"text": "minimum", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9406342506408691}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9094704389572144}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9254496693611145}]}, {"text": "This number shows that the bias is learned in these embeddings with high rate.", "labels": [], "entities": []}, {"text": "However, it learns in a lower rate than the normal embeddings, whose classification reached 88.88% and 98.25% for debiased and biased versions, respectively.", "labels": [], "entities": []}, {"text": "stereotype the professions as the normal embeddings.", "labels": [], "entities": []}, {"text": "This can be shown by the nearest neighbors of the female and male stereotyped professions, for example 'receptionist' and 'librarian' for female and 'architect' and 'philosopher' for male.", "labels": [], "entities": []}, {"text": "We applied the k nearest neighbors on the Professional List, to get the nearest k neighbor to each profession.", "labels": [], "entities": [{"text": "Professional List", "start_pos": 42, "end_pos": 59, "type": "DATASET", "confidence": 0.8657205104827881}]}, {"text": "We used a random representation for each token of the profession list, after applying the k nearest neighbor algorithm on each profession, we computed the percentage of female and male stereotyped professions among the k nearest neighbor of each profession token.", "labels": [], "entities": []}, {"text": "Afterwards, we computed the Pearson correlation of this percentage with the original bias of each profession.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 28, "end_pos": 47, "type": "METRIC", "confidence": 0.9412011206150055}]}, {"text": "Once again, to assure randomization of tokens, we performed 10 experiments, each with different random sentences for each profession, therefore with different word representations.", "labels": [], "entities": []}, {"text": "The minimum Pearson correlation is 0.801 and the maximum is 0.961, with average of 0.89.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 12, "end_pos": 31, "type": "METRIC", "confidence": 0.8192545771598816}]}, {"text": "All these correlations are significant with p-values smaller than 1 \u00d7 10 \u221240 . This experiment showed the highest influence of bias compared to 0.606 for debiased embeddings and 0.774 for biased.", "labels": [], "entities": []}, {"text": "demonstrates this influence of bias by showing that female biased words (e.g. nanny) has higher percent of female words than male ones and viceversa for male biased words (e.g. philosopher).", "labels": [], "entities": []}], "tableCaptions": []}