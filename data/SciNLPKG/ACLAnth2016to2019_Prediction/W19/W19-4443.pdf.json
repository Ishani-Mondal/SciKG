{"title": [{"text": "Measuring text readability with machine comprehension: a pilot study", "labels": [], "entities": []}], "abstractContent": [{"text": "This article studies the relationship between text readability levels and automatic machine understanding systems.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the simpler a text is, the better it should be understood by a machine.", "labels": [], "entities": []}, {"text": "We thus expect a strong correlation between readability levels on the one hand, and performance of automatic reading systems on the other hand.", "labels": [], "entities": []}, {"text": "We test this hypothesis with several understanding systems based on language models of varying strengths, measuring this correlation on two corpora of journalistic texts.", "labels": [], "entities": []}, {"text": "Our results suggest that this correlation is quite small and that existing comprehension systems are far to reproduce the gradual improvement of their performance on texts of decreasing complexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "The automatic evaluation of the readability of texts is an old subject (see, for example, for an historical account), which continues to arouse great interest from the Natural Language Processing (NLP) community; a recent analysis of the state-of-the-art is given by.", "labels": [], "entities": []}, {"text": "Checking that a written document is expressed in a language that is accessible to its target audience is essential in many situations: for instance to ensure that the information conveyed by the text is properly understood or to allow sufficient engagement of the reader in the reading activity).", "labels": [], "entities": []}, {"text": "These issues are relevant both for the dissemination of general purpose information (e.g. news articles) and more targeted information such as drug leaflets, administrative texts or legal documents.", "labels": [], "entities": []}, {"text": "They apply to various readerships: children in first or second language learning situations, adults with varying levels of education or with disabilities, etc.", "labels": [], "entities": []}, {"text": "The classical measures of readability are based on crude approximations of the syntactic complexity (using the average sentence length as a proxy) and lexical complexity (average length in characters or syllables of words in a sentence).", "labels": [], "entities": []}, {"text": "One of the most well-known measure along these lines is the Flesch-Kincaid readability index, which combines these two measures into a global score.", "labels": [], "entities": [{"text": "Flesch-Kincaid readability index", "start_pos": 60, "end_pos": 92, "type": "METRIC", "confidence": 0.902720034122467}]}, {"text": "This approach has recently been renewed by the use of supervised statistical learning methods capable of integrating into the prediction of readability a very large number of linguistic characteristics) aimed at capturing readability indices at the lexical, syntactic, semantic and even discursive levels.", "labels": [], "entities": []}, {"text": "It can be argued that these enhanced feature sets are able to take into account so-called cognitive factors.", "labels": [], "entities": []}, {"text": "However, these approaches depend on the availability of texts annotated with their difficulty levels, which are often defined in relation to a particular task or readership.", "labels": [], "entities": []}, {"text": "The elicitation of these annotations is a complex operation, which requires either the implementation of understanding measurement protocols on controlled populations, using for example cloze tests to evaluate understanding; or the work of highly qualified experts, at the risk of observing disagreements between annotators (Petersen and Ostendorf, 2009).", "labels": [], "entities": []}, {"text": "They also require automatically extracting linguistic features from texts, which existing NLP tools only partially achieve, fora limited number of languages.", "labels": [], "entities": []}, {"text": "In this paper, we study an alternative method that could help assess the readability level of texts in an unsupervised manner.", "labels": [], "entities": []}, {"text": "Our main hypothesis, developed in \u00a7 2, is that automatic text understanding systems (machine reading) having made remarkable progress (), 1 it might become possible to use them to assess the readability of texts.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6893921792507172}]}, {"text": "The preliminary question of interest here is therefore whether the ability of automatic comprehension systems to respond to gap-filling questions correlates with the difficulty of the text, and can be help to measure readability.", "labels": [], "entities": []}, {"text": "To answer this question, we study several simplistic machine understanding systems, described in \u00a7 2.4 and empirically examine the correlation between their performance and the actual complexity of texts, measured by humans (see \u00a7 3).", "labels": [], "entities": []}, {"text": "Our main findings are that when comprehension is evaluated using cloze tests, all the systems make very little difference, if any, between texts of varying complexities, suggesting that we should reconsider our evaluation scheme, or our set of comprehension systems, or both, to achieve a behavior that would be more similar to human's performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We stick hereto a much simpler form of cloze testing, based on a uniform random strategy to select the deleted words.", "labels": [], "entities": []}, {"text": "We leave for future work the use of more sophisticated methods specifically designed to generate difficult tests.", "labels": [], "entities": []}, {"text": "In practice, each test document is automatically divided into N passages of the same size; in each passage, M positions are randomly selected uniformly and correspond to the words that will be blanked out.", "labels": [], "entities": []}, {"text": "These words will have to be recovered by the comprehension system, which has access to the complete left context of the gap since the beginning of the passage.", "labels": [], "entities": []}, {"text": "In our experiments, we use N = 5 and M = 3.", "labels": [], "entities": [{"text": "M", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9468281865119934}]}, {"text": "One interesting property of cloze tests is that they provide ways to analyze the complexity of gap fillings with respect to arbitrary linguistic lexical descriptors; they also provide ways to compute difficulty levels separately for each sentence and check for instance its dependency with respect to the length of the context.", "labels": [], "entities": []}, {"text": "shows examples of Cloze tests given to our language models.", "labels": [], "entities": []}, {"text": "Gap filling performance is measured as the average number of words that are correctly predicted in a text (noted p@1) in the tables below.", "labels": [], "entities": [{"text": "Gap filling", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8178708553314209}]}, {"text": "This metric being very strict, we also report the number of times the correct word appears in the first 5, 25, and 50 candidates.", "labels": [], "entities": []}, {"text": "Many other techniques could be entertained to weaken the p@1 metric, for example by considering the similarity (formal or distributional) between the predicted word and the reference.", "labels": [], "entities": []}, {"text": "Again, we leave the study of these alternative metrics to future work.", "labels": [], "entities": []}, {"text": "In this section, we compare the automatic comprehension measures produced by our language models with readability scores produced by humans, as well as with other automatic standard indices of readability.", "labels": [], "entities": []}, {"text": "We start with details regarding the implementation of our language models.", "labels": [], "entities": []}, {"text": "The reference annotations used to validate our method mostly come from two sources: Weebit (Vajjala and Meurers, 2012) and OneStopEnglish (OSE for short).", "labels": [], "entities": [{"text": "Weebit (Vajjala and Meurers, 2012)", "start_pos": 84, "end_pos": 118, "type": "DATASET", "confidence": 0.8790196552872658}]}, {"text": "The first identifies 5 levels of complexity in educational news articles published online (on the WeeklyReader and BBC-Bitesize Web sites), covering learners aged from 7 to 16.", "labels": [], "entities": [{"text": "WeeklyReader", "start_pos": 98, "end_pos": 110, "type": "DATASET", "confidence": 0.9850673675537109}, {"text": "BBC-Bitesize Web sites", "start_pos": 115, "end_pos": 137, "type": "DATASET", "confidence": 0.9408240715662638}]}, {"text": "As a merger of two sources, the distribution of categories is quite unbalanced, with more texts for the fifth level than for all the other categories taken together.", "labels": [], "entities": []}, {"text": "Taken with their associated API from https://github.com/huggingface/ pytorch-pretrained-BERT The second test set 9 also contains extracts of journalistic texts originally from the newspaper The Guardian.", "labels": [], "entities": [{"text": "The Guardian", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.881968230009079}]}, {"text": "Each document has been rewritten twice by experts to correspond to two less advanced reading levels, and thus distinguishes three levels of readability.", "labels": [], "entities": []}, {"text": "Statistics on these test corpora are in. displays the distribution of the FleschKincaid Grade Level (FKGL) for these two corpora.", "labels": [], "entities": [{"text": "FleschKincaid Grade Level (FKGL)", "start_pos": 74, "end_pos": 106, "type": "METRIC", "confidence": 0.9654714564482371}]}, {"text": "Complexity levels are distributed nearly as expected for the Weebit corpus apart from levels 3 and 4 where the latter seems simpler than foreseen, with large overlapping spans.", "labels": [], "entities": [{"text": "Weebit corpus", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9497125148773193}]}, {"text": "It was already noted by) that the actual readability level of each test was difficult to predict accurately based on the sole FKGL.", "labels": [], "entities": [{"text": "FKGL", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.9269258975982666}]}, {"text": "OSE complexity levels are also in agreement with the Flesch-Kincaid index, and in agreement with the numbers reported in (Vajjala and Lucic, 2018); again we see a large overlap between levels for this index.", "labels": [], "entities": []}, {"text": "Overall, OSE texts are somewhat more complex than Weebit's, with OSE level 1 comparable in difficulty to Weebit level 4.", "labels": [], "entities": [{"text": "OSE", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7665538191795349}, {"text": "OSE level 1", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.6856080492337545}, {"text": "difficulty", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.9844471216201782}]}, {"text": "The test datasets used in this study are quite similar in content, and all correspond to news articles intended for readers with variable levels of proficiency.", "labels": [], "entities": []}, {"text": "In this subsection, we run a small confirmation study with an alternative test set based on literary texts.", "labels": [], "entities": []}, {"text": "We use extracts from classical fictional books (see list in where we also report the associated readability level and completion rate).", "labels": [], "entities": [{"text": "completion rate", "start_pos": 118, "end_pos": 133, "type": "METRIC", "confidence": 0.975397378206253}]}, {"text": "Kendall rank coefficient between these rates and their respective levels is equal to \u22120.28.", "labels": [], "entities": [{"text": "Kendall rank coefficient", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.810330867767334}]}, {"text": "Here again, levels are not distinguishable based on the observed completion rates.", "labels": [], "entities": [{"text": "completion", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9522504806518555}]}, {"text": "However, as seen above in section \u00a7 3.3 our systems perform better when we release the exact match constraint for the precision@N metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9906472563743591}]}, {"text": "We expect to increase the Kendall rank metric on larger experiments with this dataset in our future work.", "labels": [], "entities": [{"text": "Kendall rank metric", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9175933400789896}]}], "tableCaptions": [{"text": " Table 4: Basic statistics of test corpora", "labels": [], "entities": []}, {"text": " Table 5: Completion rates broken down per readability level for gap filling systems of variable strength tested on  the OSE dataset. Kendall tau-b correlation is reported as \u03c4 .", "labels": [], "entities": [{"text": "Completion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9913907051086426}, {"text": "gap filling", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7666687667369843}, {"text": "OSE dataset", "start_pos": 121, "end_pos": 132, "type": "DATASET", "confidence": 0.9769971668720245}, {"text": "Kendall tau-b correlation", "start_pos": 134, "end_pos": 159, "type": "METRIC", "confidence": 0.9245215654373169}]}, {"text": " Table 6: Completion rates broken down per readability level for gap filling systems of variable strength tested on  the Weebit dataset. Kendall tau-b correlation is reported as \u03c4 .", "labels": [], "entities": [{"text": "Completion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9935601949691772}, {"text": "gap filling", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.7792457044124603}, {"text": "Weebit dataset", "start_pos": 121, "end_pos": 135, "type": "DATASET", "confidence": 0.9915777742862701}, {"text": "Kendall tau-b correlation", "start_pos": 137, "end_pos": 162, "type": "METRIC", "confidence": 0.9038318594296774}]}, {"text": " Table 7: Completion rates broken down per readability  level for the GPT-2 gap filling system pre-trained on  WebText and tested on the Weebit and OSE datasets.  Kendall tau-b correlation is reported as \u03c4 .", "labels": [], "entities": [{"text": "Completion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9928430318832397}, {"text": "GPT-2 gap filling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.8024331132570902}, {"text": "WebText", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9524396061897278}, {"text": "Weebit and OSE datasets", "start_pos": 137, "end_pos": 160, "type": "DATASET", "confidence": 0.7948560118675232}, {"text": "Kendall tau-b correlation", "start_pos": 163, "end_pos": 188, "type": "METRIC", "confidence": 0.9326201478640238}]}, {"text": " Table 8: Completion rates (\u03bb with p@5) broken down  per model and POS tag and Kendall-Tau (\u03c4 ) correlation  between completion rates and their respective level on  the OSE dataset.", "labels": [], "entities": [{"text": "Completion", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9815945625305176}, {"text": "POS tag", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.8833579421043396}, {"text": "Kendall-Tau (\u03c4 ) correlation", "start_pos": 79, "end_pos": 107, "type": "METRIC", "confidence": 0.878839635848999}, {"text": "OSE dataset", "start_pos": 169, "end_pos": 180, "type": "DATASET", "confidence": 0.9380667507648468}]}]}