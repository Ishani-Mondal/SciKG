{"title": [], "abstractContent": [{"text": "We propose a novel method, Modality-based Redundancy Reduction Fusion (MRRF), for understanding and modulating the relative contribution of each modality in multimodal inference tasks.", "labels": [], "entities": [{"text": "Modality-based Redundancy Reduction Fusion (MRRF)", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.782534122467041}]}, {"text": "This is achieved by obtaining an (M + 1)-way tensor to consider the high-order relationships between M modalities and the output layer of a neural network model.", "labels": [], "entities": []}, {"text": "Applying a modality-based tensor factorization method, which adopts different factors for different modalities, results in removing information present in a modality that can be compensated by other modalities, with respect to model outputs.", "labels": [], "entities": []}, {"text": "This helps to understand the relative utility of information in each modality.", "labels": [], "entities": []}, {"text": "In addition it leads to a less complicated model with less parameters and therefore could be applied as a regularizer avoiding overfitting.", "labels": [], "entities": []}, {"text": "We have applied this method to three different multimodal datasets in sentiment analysis, personality trait recognition, and emotion recognition.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.9629932045936584}, {"text": "personality trait recognition", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6970043381055196}, {"text": "emotion recognition", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7560358047485352}]}, {"text": "We are able to recognize relationships and relative importance of different modalities in these tasks and achieves a 1% to 4% improvement on several evaluation measures compared to the state-of-the-art for all three tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal data fusion is a desirable method for many machine learning tasks where information is available from multiple source modalities, typically achieving better predictions through integration of information from different modalities.", "labels": [], "entities": [{"text": "Multimodal data fusion", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6931606531143188}]}, {"text": "Multimodal integration can handle missing data from one or more modalities.", "labels": [], "entities": [{"text": "Multimodal integration", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8093083202838898}]}, {"text": "Since some modalities can include noise, it can also lead to more robust prediction.", "labels": [], "entities": []}, {"text": "Moreover, since some information may not be visible in some modalities or a single modality may not be powerful enough fora specific task, considering multiple modalities often improves performance (.", "labels": [], "entities": []}, {"text": "For example, humans assign personality traits to each other, as well as to virtual characters by inferring personality from diverse cues, both behavioral and verbal, suggesting that a model to predict personality should take into account multiple modalities such as language, speech, and visual cues.", "labels": [], "entities": []}, {"text": "Our method, Modality-based Redundancy Reduction multimodal Fusion (MRRF), builds on recent work in mutimodal fusion utilizing first an outer product tensor of input modalities to better capture inter-modality dependencies ( ) and a recent approach to reduce the number of elements in the resulting tensor through low rank factorization (.", "labels": [], "entities": [{"text": "Modality-based Redundancy Reduction multimodal Fusion (MRRF)", "start_pos": 12, "end_pos": 72, "type": "TASK", "confidence": 0.8270751088857651}, {"text": "mutimodal fusion", "start_pos": 99, "end_pos": 115, "type": "TASK", "confidence": 0.7364981472492218}]}, {"text": "Whereas the factorization used in () utilizes a single compression rate across all modalities, we instead use Tuckers tensor decomposition (see the Methodology section), which allows different compression rates for each modality.", "labels": [], "entities": []}, {"text": "This allows the model to adapt to variations in the amount of useful information between modalities.", "labels": [], "entities": []}, {"text": "Modality-specific factors are chosen by maximizing performance on a validation set.", "labels": [], "entities": []}, {"text": "Applying a modality-based factorization method results in removing redundant information duplicated across modalities and leading to fewer parameters with minimal information loss.", "labels": [], "entities": []}, {"text": "Through maximizing performance on a validation set, our method can work as a regularizer, leading to a less complicated model and reducing overfitting.", "labels": [], "entities": []}, {"text": "In addition, our modality-based factorization approach helps to understand the differences in useful information between modalities for the task at hand.", "labels": [], "entities": []}, {"text": "We evaluate the performance of our approach using sentiment analysis, personality detection, and emotion recognition from audio, text and video frames.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.9740397334098816}, {"text": "personality detection", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.7669891715049744}, {"text": "emotion recognition", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.7368013560771942}]}, {"text": "The method reduces the number of pa-rameters which requires fewer training samples, providing efficient training for the smaller datasets, and accelerating both training and prediction.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that the proposed approach can make notable improvements, in terms of accuracy, mean average error (MAE), correlation, and F 1 score, especially for the applications with more complicated inter-modality relations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9994316697120667}, {"text": "mean average error (MAE)", "start_pos": 117, "end_pos": 141, "type": "METRIC", "confidence": 0.9634250899155935}, {"text": "correlation", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.996752142906189}, {"text": "F 1 score", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9900328119595846}]}, {"text": "We further study the effect of different compression rates for different modalities.", "labels": [], "entities": []}, {"text": "Our results on the importance of each modality for each task supports the previous results on the usefulness of each modality for personality recognition, emotion recognition and sentiment analysis.", "labels": [], "entities": [{"text": "personality recognition", "start_pos": 130, "end_pos": 153, "type": "TASK", "confidence": 0.7821895778179169}, {"text": "emotion recognition", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7477510273456573}, {"text": "sentiment analysis", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.9472688734531403}]}, {"text": "In the sequel, we first describe related work.", "labels": [], "entities": []}, {"text": "We elaborate on the details of our proposed method in Methodology section.", "labels": [], "entities": []}, {"text": "In the following section we goon to describe our experimental setup.", "labels": [], "entities": []}, {"text": "In the Results section, we compare the performance of MRRF and state-of-the-art baselines on three different datasets and discuss the effect of compression rate on each modality.", "labels": [], "entities": [{"text": "MRRF", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.4304245114326477}]}, {"text": "Finally, we provide a brief conclusion of the approach and the results.", "labels": [], "entities": []}, {"text": "Supplementary materials describe the methodology in greater detail.", "labels": [], "entities": []}, {"text": "Notation The operator \u2297 is the outer product operator where z 1 \u2297 . .", "labels": [], "entities": []}, {"text": "\u2297 z M for z i \u2208 Rd i leads to a M-way tensor in Rd 1 \u00d7...\u00d7d M . The operator \u00d7 k , fora given k, is k-mode product of a tensor R \u2208 R r 1 \u00d7r 2 \u00d7...\u00d7r M and a matrix W \u2208 Rd k \u00d7r k as W \u00d7 k R, which results in a tensor \u00af R \u2208 R r 1 \u00d7...\u00d7r k\u22121 \u00d7d k \u00d7r k+1 \u00d7...\u00d7r M .", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our experiments on the following multimodal datasets: CMU-MOSI (), POM (), and IEMOCAP () for sentiment analysis, speaker traits recognition, and emotion recognition, respectively.", "labels": [], "entities": [{"text": "POM", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9403411149978638}, {"text": "IEMOCAP", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9837164878845215}, {"text": "sentiment analysis", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.959184318780899}, {"text": "speaker traits recognition", "start_pos": 125, "end_pos": 151, "type": "TASK", "confidence": 0.7393909494082133}, {"text": "emotion recognition", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7255201935768127}]}, {"text": "These tasks can be done by integrating both verbal and nonverbal behaviors of the persons.", "labels": [], "entities": []}, {"text": "The CMU-MOSI dataset is annotated on a sevenstep scale as highly negative, negative, weakly negative, neutral, weakly positive, positive, highly positive which can be considered as a 7 class classification problem with 7 labels in the range  interval ( . In order to perform time alignment across modalities, the three modalities are aligned using P2FA) at the word level.", "labels": [], "entities": [{"text": "CMU-MOSI dataset", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9734424948692322}]}, {"text": "Pre-trained 300-dimensional Glove word embeddings (  were used to extract the language feature representations, which encodes a sequence of the transcribed words into a sequence of vectors.", "labels": [], "entities": []}, {"text": "Visual features for each frame (sampled at 30Hz) are extracted using the library Facet 1 which includes 20 facial action units, 68 facial landmarks, head pose, gaze tracking and HOG features ().", "labels": [], "entities": [{"text": "gaze tracking", "start_pos": 160, "end_pos": 173, "type": "TASK", "confidence": 0.7267920672893524}]}, {"text": "COVAREP acoustic analysis framework) is used to extract low-level acoustic features, including 12 Mel frequency cepstral coefficients (MFCCs), pitch, voiced/unvoiced segmentation, glottal source, peak slope, and maxima dispersion quotient features.", "labels": [], "entities": [{"text": "COVAREP acoustic analysis framework", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.7797149866819382}, {"text": "Mel frequency cepstral coefficients (MFCCs)", "start_pos": 98, "end_pos": 141, "type": "METRIC", "confidence": 0.7464313975402287}]}, {"text": "To evaluate model generalization, all datasets are split into training, validation, and test sets such that the splits are speaker independent, i.e., no speakers from the training set are present in the test sets.", "labels": [], "entities": []}, {"text": "We compared our proposed method with three baseline methods.", "labels": [], "entities": []}, {"text": "Concat fusion (CF) ( proposes a simple concatenation of the different modalities followed by a linear combination.", "labels": [], "entities": [{"text": "Concat fusion (CF)", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8504687786102295}]}, {"text": "The tensor fusion approach (TF) ( ) computes a tensor including uni-modal, bi-modal, and tri-modal combination information.", "labels": [], "entities": [{"text": "tensor fusion", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.6976374685764313}]}, {"text": "LMF () is a tensor fusion method that performs tensor factorization using the same rank for all the modalities in order to reduce the number of parameters.", "labels": [], "entities": [{"text": "tensor fusion", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7625702321529388}]}, {"text": "Our proposed method aims to use different factors for each modality.", "labels": [], "entities": []}, {"text": "In, we present mean average error (MAE), the correlation between prediction and true scores, binary accuracy (Acc-2), multi-class accuracy (Acc-7) and F1 measure.", "labels": [], "entities": [{"text": "mean average error (MAE)", "start_pos": 15, "end_pos": 39, "type": "METRIC", "confidence": 0.956263393163681}, {"text": "binary accuracy (Acc-2)", "start_pos": 93, "end_pos": 116, "type": "METRIC", "confidence": 0.7183869361877442}, {"text": "multi-class accuracy (Acc-7)", "start_pos": 118, "end_pos": 146, "type": "METRIC", "confidence": 0.6804664969444275}, {"text": "F1 measure", "start_pos": 151, "end_pos": 161, "type": "METRIC", "confidence": 0.9877845644950867}]}, {"text": "The proposed approach outperforms baseline approaches in nearly all metrics, with marked improvements in Happy and Neutral recognition.", "labels": [], "entities": [{"text": "Happy", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9808423519134521}, {"text": "Neutral recognition", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.8550873100757599}]}, {"text": "The reason is that the inter-modality information for these emotions is more complicated than the other emotions and requires a non-diagonal core tensor to extract the complicated information.", "labels": [], "entities": []}, {"text": "It is worth to note that for the equivalent setting and equal ranks for all the modalities, the result of the proposed method is always marginally better than LMF method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for Sentiment Analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait  recognition on POM. (CF, TF, and LMF stand for concat, tensor and low-rank fusion respectively).", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.919192761182785}, {"text": "CMU-MOSI", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.9357420206069946}, {"text": "emotion recognition", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7043749988079071}, {"text": "IEMOCAP", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.6488736867904663}, {"text": "personality trait  recognition", "start_pos": 89, "end_pos": 119, "type": "TASK", "confidence": 0.5910165707270304}]}]}