{"title": [{"text": "Relating RNN layers with the spectral WFA ranks in sequence modelling", "labels": [], "entities": []}], "abstractContent": [{"text": "We analyse Recurrent Neural Networks (RNNs) to understand the significance of multiple LSTM layers.", "labels": [], "entities": []}, {"text": "We argue that the Weighted Finite-state Automata (WFA) trained using a spectral learning algorithm are helpful to analyse RNNs.", "labels": [], "entities": []}, {"text": "Our results suggest that multiple LSTM layers in RNNs help learning distributed hidden states, but have a smaller impact on the ability to learn long-term dependencies.", "labels": [], "entities": []}, {"text": "The analysis is based on the empirical results, however relevant theory (whenever possible) was discussed to justify and support our conclusions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence prediction is a problem that involves using historical sequence data (i.e. context) to predict the next symbol or symbols in the sequence.", "labels": [], "entities": [{"text": "Sequence prediction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9515846967697144}]}, {"text": "Weighted Finite-state Automata (WFA) and Recurrent Neural Networks (RNNs) provide a general framework for the representation of functions that map strings (i.e. sequential data) to real numbers.", "labels": [], "entities": []}, {"text": "Nondeterministic Weighted Finite-state Automata (WFA) map input words to real numbers and are not guaranteed to be tractable (.", "labels": [], "entities": []}, {"text": "In general, WFA use hidden states and learning is usually done by the Expectation-Maximisation (EM) algorithm, which is computationally expensive and does not come with a guarantee of global optimality.", "labels": [], "entities": [{"text": "WFA", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9720965623855591}]}, {"text": "Spectral learning algorithms for WFA () provide an alternative to EM that is both computationally efficient and statistically consistent.", "labels": [], "entities": []}, {"text": "On the other hand, RNNs are remarkably expressive models.", "labels": [], "entities": []}, {"text": "Even a single-layer RNN network has powerful sequence modelling capacity.", "labels": [], "entities": [{"text": "sequence modelling", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7260845303535461}]}, {"text": "RNNs are also Turing complete and can represent any computable function (, but the theoretical analysis of even a single-layer RNN is difficult.", "labels": [], "entities": []}, {"text": "Existing research shows that multilayer RNNs are advantageous for efficient sequence modelling (.", "labels": [], "entities": [{"text": "sequence modelling", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.6833483427762985}]}, {"text": "However, it is hard to analyse such models theoretically.", "labels": [], "entities": []}, {"text": "As a result, in spite of competitive empirical results, it is not clear what kind of additional modelling power is gained by a deep architecture (i.e. more than one hidden layer in RNNs).", "labels": [], "entities": []}, {"text": "Stacking RNN layers (in space) is inspired by the multilayer perceptron (MLP) and the hypothesis that multiple layers allow the model to have greater complexity by incorporating complex feature representations of each time step.", "labels": [], "entities": []}, {"text": "This allows each recurrent level to operate at a different time-scale.", "labels": [], "entities": []}, {"text": "For the non-recurrent networks, hypothesise that a deep, hierarchical model can be exponentially more efficient at representing some functions than a shallow one.", "labels": [], "entities": []}, {"text": "Theoretical) and empirical () work on non-recurrent networks agrees with the above hypothesis.", "labels": [], "entities": []}, {"text": "Based on these results, assumed that the MLP-based hypothesis proposed by is also true for the recurrent neural networks.", "labels": [], "entities": []}, {"text": "The earlier work attempted at capturing large context and reducing the training time by using multilayer RNNs.", "labels": [], "entities": []}, {"text": "For example, El assumed that the layers increase the capacity of learning the context by capturing the improved long-term history, whereas argues that the stacked RNN requires less computation per time-step and far fewer training sequences than a single-layer RNN.", "labels": [], "entities": []}, {"text": "Elman (1990) introduced the notion of 'memory' to capture non-fixed long-term contexts through the recurrent layer.", "labels": [], "entities": []}, {"text": "When stacking the RNNs, the transition between the consecutive recurrent layers is still shallow ().", "labels": [], "entities": []}, {"text": "Thus, stacking the RNNs does not extend the hypothesis of () to the recurrent layer that is dedicated for long-term context capture.", "labels": [], "entities": [{"text": "context capture", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.7119785845279694}]}, {"text": "The empirical results of; suggested that multilayer RNNs improve sequence modelling.", "labels": [], "entities": [{"text": "sequence modelling", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.7909035086631775}]}, {"text": "We show empirical evidence that indicates that a multilayer RNN does capture better context as shown by, but that is achieved across stacked layers instead of the timescale (i.e. instead of recurrent layer).", "labels": [], "entities": []}, {"text": "Better learning depends on capturing the improved input representation at each time step and capturing improved long-term dependency from the previous time-steps in a sequence.", "labels": [], "entities": []}, {"text": "In this paper, we investigate RNN learning from the formal language perspective using the WFA models, and we show that adding more layers may not be sufficient if the model has to deal with long-term dependencies.", "labels": [], "entities": [{"text": "RNN learning", "start_pos": 30, "end_pos": 42, "type": "TASK", "confidence": 0.9451596736907959}]}, {"text": "WFA-based models are used for both theoretical studies and sequence prediction tasks including language modelling ().", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.7537685036659241}, {"text": "language modelling", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7688537538051605}]}, {"text": "Evaluating their performance on real and synthetic data can help us to understand the model's hidden state relationship with the RNN layers.", "labels": [], "entities": []}, {"text": "In the existing literature, stacking multiple RNN layers (in space) is used to obtain improved accuracy on sequence prediction tasks, but this is done without deeply-justified reasons of such choices.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.996190071105957}, {"text": "sequence prediction tasks", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.8218861619631449}]}, {"text": "Our experiments and analysis show that the hidden states of a process can be modelled efficiently using multiple layers, but multiple layers may not be sufficient to model long-term dependencies in sequential observations.", "labels": [], "entities": []}, {"text": "In this paper, we use two types of RNN models (one is a single-layer and another is a two-layer stacked RNN network) and a WFA.", "labels": [], "entities": [{"text": "WFA", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.6924142241477966}]}, {"text": "All methods were evaluated on fifteen datasets to answer the following question \"what is the impact of multiple RNN layers in sequence modelling?\".", "labels": [], "entities": []}, {"text": "To answer this question, we contrasted the impact of the LSTM layers in RNNs with the rank (i.e. the number of hidden states) in the corresponding WFA models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Sequence PredictIction ChallengE (SPiCe) () was an on-line competition to predict the next element of a sequence.", "labels": [], "entities": []}, {"text": "The competition scored methods on their performance on both real and synthetic data (see Sec. 2).", "labels": [], "entities": []}, {"text": "Training datasets consist of whole sequences and the aim is to learn a model that allows the ranking of potential next symbols fora given test sequence (prefix or context), that is, the most likely options fora single next symbol.", "labels": [], "entities": []}, {"text": "Once rankings for all prefixes were submitted by the participants, the score (N DCG 5 explained below) of the submission was computed.", "labels": [], "entities": [{"text": "N DCG 5", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.6000482638676962}]}, {"text": "The score is a ranking metric based on normalised discounted cumulative gain computed from the ranking of 5 potential next symbols starting from the most probable one.", "labels": [], "entities": []}, {"text": "Suppose the test set is made of prefixes y 1 , . .", "labels": [], "entities": []}, {"text": ", y M and the distinct next symbols ranking submitted for y i is (\u02c6 a i 1 , . .", "labels": [], "entities": []}, {"text": ", \u02c6 a i 5 ) sorted from more likely to least likely.", "labels": [], "entities": []}, {"text": "The target probability distribution of possible next symbols given the prefix y i , p(.|y i ), was known to the organisers.", "labels": [], "entities": []}, {"text": "Thus, the exact measure for prefix y i could be computed using the following equation: where p 1 \u2265 p 2 \u2265 . .", "labels": [], "entities": []}, {"text": "\u2265 p 5 are the top 5 values in the distribution p(.|y i ).", "labels": [], "entities": []}, {"text": "More details on this evaluation can be found in ().", "labels": [], "entities": []}, {"text": "The hyperparameters (see Tab. 1) for the WFA were tuned based on the approach described in Sec.", "labels": [], "entities": [{"text": "WFA", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.6630000472068787}]}, {"text": "4.1 to find the best results obtained in this paper.", "labels": [], "entities": []}, {"text": "For the neural models, the weights were initialised with Gaussian samples, each of which has zero mean and standard deviation 1 in size , wherein size is the dimension of input vectors.", "labels": [], "entities": []}, {"text": "The LSTM has 600 hidden nodes, the size of the embedding layer was set to 100, and when a nonlinear layer is used between the LSTM and Softmax layers (for the baseline replication), the output dimension was set to 300.", "labels": [], "entities": []}, {"text": "These values were set based on the baseline study, in which two hidden layer sizes (400 and 600) were used.", "labels": [], "entities": []}, {"text": "In Shibata and Heinz (2017, Tables 1 and 2), hidden layer size did not have critical effect on the results.", "labels": [], "entities": []}, {"text": "Considering SPiCe'16 datasets and the existing literature, 600 hidden nodes make a large RNN network.", "labels": [], "entities": [{"text": "SPiCe'16 datasets", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.8382742702960968}]}, {"text": "We have used two-layer and one-layer LSTM networks in our experiments.", "labels": [], "entities": []}, {"text": "The dropout rate was set to 0.5 for all non-recurrent layers, which is known to be close to optimal fora wide range of networks and tasks ().", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9571536779403687}]}, {"text": "Following the baseline study, for optimisation, we used stochastic gradient descent (SGD) with momentum of 0.9.", "labels": [], "entities": []}, {"text": "The learning rate decreased gradually from 0.1 to 0.001, where the number of iterations is 45 and the mini-batch size is 128.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9631223082542419}]}, {"text": "The synthetic dataset 12 was the biggest and arguably the most challenging problem in SPiCe 2016.", "labels": [], "entities": []}, {"text": "It was initially generated for another competition (PAutomaC) using the PAutomaC data generator).", "labels": [], "entities": [{"text": "PAutomaC data generator", "start_pos": 72, "end_pos": 95, "type": "DATASET", "confidence": 0.9547102650006613}]}, {"text": "The best performing WFA scored 0.8113 on this dataset with n = 95 and nR = nC = 4.", "labels": [], "entities": [{"text": "WFA", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.7782349586486816}]}, {"text": "Although WFA is a Markov model 2 (i.e., a model that may require l-th order representation, which makes predictions based on l the most recent observations, to learn long-range dependencies), on dataset 12, WFA was as good as the RNN models.", "labels": [], "entities": [{"text": "WFA", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.844805896282196}, {"text": "WFA", "start_pos": 207, "end_pos": 210, "type": "DATASET", "confidence": 0.6348578333854675}]}, {"text": "Our one-layer neural model scored 0.7116 and the twolayer neural model improved the result to 0.8508.", "labels": [], "entities": []}, {"text": "So, we can clearly see that on this large dataset, two layers improve the results.", "labels": [], "entities": []}, {"text": "We argue that we can use WFA results to explain the improvement of our two-layer neural model.", "labels": [], "entities": [{"text": "WFA", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8864140510559082}]}, {"text": "For that we will focus on the rank of WFA (i.e. parameter n) and the maximum length of substrings in its basis (i.e. nR and nC).", "labels": [], "entities": []}, {"text": "To score high on dataset 12, WFA had to use 95 hidden states, which is a large number of hidden states fora traditional Baum-Welch algorithm (.", "labels": [], "entities": [{"text": "WFA", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.6226389408111572}]}, {"text": "This means that in order to solve this problem, a Markov model requires a relatively large number of states.", "labels": [], "entities": []}, {"text": "This fact can explain why our two-layer neural model outperformed a single-layer model because the second LSTM layer increased the number of hidden states in the neural model.", "labels": [], "entities": []}, {"text": "Moreover, the obtained WFA's score is based on short substrings (i.e. nR = nC = 4) in its basis.", "labels": [], "entities": []}, {"text": "Therefore, it is fair to expect that dataset 12 does not have longterm dependencies since short substring statistics are sufficient to capture the data-generating distribution in this model.", "labels": [], "entities": []}, {"text": "We believe that we can make this claim because our WFA with short substrings works very well on this data.", "labels": [], "entities": []}, {"text": "All this means that dataset 12 requires a relatively large number of hidden states, but it does not have long-term dependencies.", "labels": [], "entities": []}, {"text": "Our two-layer neural model is sufficient for such problems because two layers increase the number of hidden states, whereas the long-term dependencies are not an issue.", "labels": [], "entities": []}, {"text": "To support our discussion above, we should add that in () the hidden units of the second-order RNN were shown to be related to the rank or the hidden states of WFA.", "labels": [], "entities": [{"text": "WFA", "start_pos": 160, "end_pos": 163, "type": "DATASET", "confidence": 0.8689813613891602}]}, {"text": "Note that second-order and higher-order RNNs have their recurrence depth increased by explicit, higherorder multiplicative interactions between the hidden states at previous time steps and input at the current time step.", "labels": [], "entities": []}, {"text": "It was shown that any function that can be computed by a linear second-order RNN () with n hidden units on sequences of one-hot vectors (i.e. canonical basis vectors) can be computed by a WFA with n states.", "labels": [], "entities": []}, {"text": "A higher-order RNN has additional connections from multiple previous time steps whereas the classic RNN has connection from one previous time step only.", "labels": [], "entities": []}, {"text": "Higher order RNNs allow a direct mapping to a finite-state machine (.", "labels": [], "entities": []}, {"text": "However, a similar connection is not available for classic RNNs and WFA and more importantly for multilayer RNNs and WFA.", "labels": [], "entities": [{"text": "WFA", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.8524186611175537}]}, {"text": "Based on these theoretical results and our empirical investigation, we can conjecture that the improved score on dataset 12 by using two LSTM layers indicates that the multiple layers helped to model the hidden states more efficiently.", "labels": [], "entities": []}, {"text": "Low Rank To support our arguments about the rank (i.e. the number of hidden states) in our discussion about dataset 12, we can identify a complementary relationship in other results.", "labels": [], "entities": []}, {"text": "In particular, when we consider all datasets on which WFA did well having a small rank (this is in contrast to dataset 12 which required a high rank for WFA), a two-layer network does not lead to significant improvement.", "labels": [], "entities": []}, {"text": "This pattern can be seen on datasets 1, 2, and 3, and this complements our previous arguments about dataset 12.", "labels": [], "entities": []}, {"text": "Dataset 5 and Long Context Dataset 5 is areal dataset on which the best performing WFA model scored 0.568 with rank n = 450 and substring lengths nR = nC = 5.", "labels": [], "entities": []}, {"text": "This dataset is large for spectral learning and increasing nR and nC above 5 made the method intractable.", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8673090636730194}]}, {"text": "Our one-layer neural model scored 0.7988 and a two-layer model showed a small improvement scoring 0.8107.", "labels": [], "entities": []}, {"text": "This means that on this dataset adding more layers did not change the score significantly.", "labels": [], "entities": []}, {"text": "We will attempt to explain the lack of a big improvement of a twolayer neural model using our WFA results.", "labels": [], "entities": []}, {"text": "Dataset 5 corresponds to the NLP character language modelling benchmark from Penn Treebank ().", "labels": [], "entities": [{"text": "NLP character language modelling", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.581678107380867}, {"text": "Penn Treebank", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9913111329078674}]}, {"text": "The other NLP datasets are 4, 8, 11, and 13.", "labels": [], "entities": [{"text": "NLP datasets", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9525662660598755}]}, {"text": "Similar to dataset 5, increasing the number of RNN layers did not significantly improve the score on those datasets.", "labels": [], "entities": []}, {"text": "Most NLP data (including dataset 5) have longterm dependencies because there are many training examples of word agreements (with different long-range regularities) which span a large portion of a sentence ).", "labels": [], "entities": []}, {"text": "WFA with discrete states have limited memory capacity which gets consumed by having to deal with all the intervening regularities in the sequence.", "labels": [], "entities": []}, {"text": "We can clearly see this in our results because in our experiments on WFA, we have many hidden states (n = 450).", "labels": [], "entities": [{"text": "WFA", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.7505335211753845}]}, {"text": "We can see that a large number of hidden states was not sufficient to solve this problem using WFA when nR = nC = 5, i.e., when substrings are short.", "labels": [], "entities": []}, {"text": "In order to capture long-term dependencies, our WFA would need to be trained on longer substrings (higher nR and nC), but this is infeasible to do on this large dataset because the method becomes intractable.", "labels": [], "entities": []}, {"text": "This problem requires the learning algorithm to take care of the long-term context.", "labels": [], "entities": []}, {"text": "We can provide a theoretical justification as to why long substrings (i.e. prefixes and suffixes that define the basis of a Hankel matrix) can lead to a better model given a particular number of hidden states, n.", "labels": [], "entities": []}, {"text": "Note that the number of hidden states n corresponds to the number of dimensions that are kept after the SVD of the Henkel matrix.", "labels": [], "entities": []}, {"text": "This means that n most informative latent dimensions (i.e., those that carry the most variance) are used as hidden states.", "labels": [], "entities": []}, {"text": "If, given a particular value of n, one model has better performance than another model, it means that it's best n dimensions capture more variance than the best n latent dimensions of the alternative model.", "labels": [], "entities": []}, {"text": "This argument explains why one basis of a Hankel matrix can lead to a better model than another basis.", "labels": [], "entities": []}, {"text": "Intuitively, it is also natural to expect that long substrings can lead to a better set of hidden states because they can capture longer interactions between input symbols, which should naturally lead to more informative hidden states.", "labels": [], "entities": []}, {"text": "If it was computationally feasible to evaluate dataset 5 with larger substring lengths, we could investigate the spectral norm of the empirical Hankel matrix with the increasing length of substrings (i.e. nR, nC).", "labels": [], "entities": []}, {"text": "This would shed some light on the quality of the first hidden state in compared models.", "labels": [], "entities": []}, {"text": "Theoretically, a one-layer RNN model can capture infinite context (, but due to training difficulties (e.g. the vanishing gradient problem), the context capture capacity of RNNs is limited.", "labels": [], "entities": []}, {"text": "Despite this difficulty, it was shown in the literature that RNNs can capture previous context of up to 200 tokens.", "labels": [], "entities": []}, {"text": "However, other researchers ( argue that stacking RNN layers (like in our two-layer model) does not increase the capacity of the model to capture longer contexts.", "labels": [], "entities": []}, {"text": "This means that our two-layer model cannot deal with long contexts even when we add more layers.", "labels": [], "entities": []}, {"text": "Since WFA did not perform well on dataset 5 having short context and a large number of hidden states, we conjecture that this dataset requires along context and for this reason two-layers in a neural model do not help.", "labels": [], "entities": [{"text": "WFA", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.4983707368373871}]}, {"text": "Our results are consistent with other results where long-term contexts are captured by the recurrent layer ().", "labels": [], "entities": []}, {"text": "According to the distributed hypothesis () stacking multiple layers allows for learning distributed features but not for capturing long-term contexts.", "labels": [], "entities": []}, {"text": "Consequently, we assume that the long-term contexts are more important for dataset 5 to make efficient prediction than the pure increase in the number of the hidden states across the space.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The hyperparameters of WFA, the scores of WFA and neural models, and the score improvement by the  best neural model compared to WFA", "labels": [], "entities": [{"text": "WFA", "start_pos": 139, "end_pos": 142, "type": "DATASET", "confidence": 0.5448172092437744}]}]}