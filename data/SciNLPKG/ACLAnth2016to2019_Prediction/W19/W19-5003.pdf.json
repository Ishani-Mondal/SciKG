{"title": [{"text": "A Paraphrase Generation System for EHR Question Answering", "labels": [], "entities": [{"text": "Paraphrase Generation", "start_pos": 2, "end_pos": 23, "type": "TASK", "confidence": 0.9513416588306427}, {"text": "EHR Question Answering", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7567924857139587}]}], "abstractContent": [{"text": "This paper proposes a dataset and method for automatically generating paraphrases for clinical questions relating to patient-specific information in electronic health records (EHRs).", "labels": [], "entities": []}, {"text": "Crowdsourcing is used to collect 10,578 unique questions across 946 semantically distinct paraphrase clusters.", "labels": [], "entities": []}, {"text": "This corpus is then used with a deep learning-based question paraphrasing method utilizing variational autoen-coder and LSTM encoder/decoder.", "labels": [], "entities": [{"text": "deep learning-based question paraphrasing", "start_pos": 32, "end_pos": 73, "type": "TASK", "confidence": 0.6522659435868263}]}, {"text": "The ultimate use of such a method is to improve the performance of automatic question answering methods for EHRs.", "labels": [], "entities": [{"text": "question answering", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.7250571250915527}]}], "introductionContent": [{"text": "The useful information present in electronic health records (EHRs) is hard to access due to many of its usability issues ().", "labels": [], "entities": []}, {"text": "Question answering (QA) systems have the potential to reduce the time it takes for users to access information present in the EHRs.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8855692744255066}, {"text": "EHRs", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.9053162932395935}]}, {"text": "However, the effectiveness of such QA systems largely depends on the variety of questions they are capable of handling.", "labels": [], "entities": []}, {"text": "Automated paraphrasing techniques are known to improve the performance of QA models in general domain by generating different variations of a question).", "labels": [], "entities": []}, {"text": "Thus, automatic generation of high quality paraphrases for patient-specific EHR questions has the potential to improve performance of the clinical QA systems.", "labels": [], "entities": []}, {"text": "Paraphrasing is a technique of rewording a given phrase such that its lexical and syntactic structure is different but its semantic information is retained.", "labels": [], "entities": []}, {"text": "For instance, the following two questions can be considered as paraphrases of each other.", "labels": [], "entities": []}, {"text": "\u2022 What medications am I currently taking?", "labels": [], "entities": []}, {"text": "\u2022 What are my current medications?", "labels": [], "entities": []}, {"text": "Such EHR-related questions are usually targeted toward specific clinical information).", "labels": [], "entities": []}, {"text": "For example, the aforementioned questions are intended to get information regarding medications.", "labels": [], "entities": []}, {"text": "In such a scenario, paraphrases can be considered as different ways of accessing the same medical data.", "labels": [], "entities": []}, {"text": "As such, automatic clinical paraphrase generation can help in increasing the breadth of questions for training a clinical QA system.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.7391555905342102}, {"text": "breadth", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9697303771972656}]}, {"text": "While automated paraphrase generation is wellstudied in the general domain, very few studies have focused on clinical paraphrasing ().", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.9027016758918762}]}, {"text": "On the other hand, clinical text simplification, which aims at generating easier to read paraphrases, has received relatively more attention.", "labels": [], "entities": [{"text": "clinical text simplification", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6948137482007345}]}, {"text": "However, these works in the clinical domain are not representative of QA needs as the usefulness of paraphrases is largely application-specific.", "labels": [], "entities": []}, {"text": "Also, existing datasets for clinical paraphrasing consist of either short phrases ( or webpage title texts (, both of which are not suitable to build a paraphrase generator for QA.", "labels": [], "entities": []}, {"text": "One can resort to using external tools such as Google Translate for generating question paraphrases (, but these general-purpose tools are not tailored to the medical domain (.", "labels": [], "entities": []}, {"text": "In this paper, we propose a clinical paraphrasing corpus CLINIQPARA with questions which can be answered using EHR data . We further propose a deep learning-based automated clinical paraphrasing system utilizing a variational autoencoder (VAE) and along short-term memory recurrent neural network (LSTM) (.", "labels": [], "entities": [{"text": "EHR data", "start_pos": 111, "end_pos": 119, "type": "DATASET", "confidence": 0.7666463851928711}]}, {"text": "To our knowledge, this is the first work aimed at automatically generating paraphrases without using any external resource for questions specifically focused on retrieving patient-specific information from EHRs.", "labels": [], "entities": []}, {"text": "Our main contributions are summarized as follows: \u2022 Crowdsourcing a large paraphrasing corpus of questions which are answerable using the data from EHR.", "labels": [], "entities": [{"text": "EHR", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.967279314994812}]}, {"text": "\u2022 Application of VAE in context to clinical paraphrasing task.", "labels": [], "entities": [{"text": "VAE", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.5322523713111877}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 explores related work in the domain of clinical paraphrasing.", "labels": [], "entities": [{"text": "clinical paraphrasing", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.5409213602542877}]}, {"text": "Then, Sections 3 and 4 discuss our dataset generation and model implementation details respectively.", "labels": [], "entities": [{"text": "dataset generation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.6877923011779785}]}, {"text": "Next, Section 5 evaluates the results of our clinical paraphrasing system.", "labels": [], "entities": []}, {"text": "Finally, Section 6 discusses our findings, and Section 7 provides a concluding summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to quickly and efficiently collect hundreds of paraphrases, we utilized the crowdsourcing platform Amazon Mechanical Turk (AMT).", "labels": [], "entities": []}, {"text": "Instead of prompting AMT workers with a question and directly asking for paraphrases-which could prime the workers and bias them toward very similar paraphrases-we presented them with a short, 3-6 sentence imaginary scenario that placed them in a situation where a specific piece of information was required (such as their current medications).", "labels": [], "entities": []}, {"text": "The workers were then asked to provide questions directed to their doctor to answer that information need.", "labels": [], "entities": []}, {"text": "After the crowd-sourced questions were collected, they were manually organized into distinct paraphrase clusters.", "labels": [], "entities": []}, {"text": "This was necessary because some questions address the information need but are not logically equivalent paraphrases.", "labels": [], "entities": []}, {"text": "These steps are discussed in more detail below.", "labels": [], "entities": []}, {"text": "The paraphrased questions generated by the model are re-incorporated with the concept, person names, and digits which were extracted during the preprocessing step.", "labels": [], "entities": []}, {"text": "The paraphrases are evaluated using standard paraphrase evaluation metrics such as BLEU (), METEOR, and TER (), which are shown to work well for the paraphrase identification task (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9991320967674255}, {"text": "METEOR", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9948083758354187}, {"text": "TER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9978285431861877}, {"text": "paraphrase identification task", "start_pos": 149, "end_pos": 179, "type": "TASK", "confidence": 0.8729405999183655}]}, {"text": "BLEU score assesses the lexical similarity of generated paraphrases with the reference ones using exact matching while METEOR additionally takes into account the word stems and synonyms.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9649990499019623}, {"text": "METEOR", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9734062552452087}]}, {"text": "TER score measures the edit distance (number of edits required to convert one sentence into another) between generated and reference paraphrases.", "labels": [], "entities": [{"text": "TER score", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9646511375904083}, {"text": "edit distance", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.8409471809864044}]}, {"text": "So, higher BLEU and METEOR scores are better whereas a lower TER score is preferable.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9995641112327576}, {"text": "METEOR", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9958707690238953}, {"text": "TER score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9859679341316223}]}, {"text": "Since we have multiple paraphrases for each question in our corpus, we calculate these metrics for the generated paraphrases against all the available ground truth paraphrases.", "labels": [], "entities": []}, {"text": "To evaluate the performance measures on all the parts of CLINIQPARA dataset, we perform 10-fold cross validation.", "labels": [], "entities": [{"text": "CLINIQPARA dataset", "start_pos": 57, "end_pos": 75, "type": "DATASET", "confidence": 0.9722868204116821}]}, {"text": "Specifically, we split our dataset by scenarios (into 10 groups each containing 5 scenarios) and sequentially test the performance of model on each group of 5 scenarios after training it on the other 45.", "labels": [], "entities": []}, {"text": "We report the individual and average scores from all these runs in our results.", "labels": [], "entities": [{"text": "individual and average scores", "start_pos": 14, "end_pos": 43, "type": "METRIC", "confidence": 0.7712884396314621}]}, {"text": "We further evaluate the performance of our model on the Quora dataset 2 , which contains over 400k pairs of questions of which around 150k pairs are paraphrases.", "labels": [], "entities": [{"text": "Quora dataset 2", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.9519934256871542}]}, {"text": "We train on 90% of these paraphrase pairs and test on the remaining 10%.", "labels": [], "entities": []}, {"text": "We also perform human evaluation of the gen-   erated paraphrases for quantifying the aspects not covered solely by the automated evaluation metrics.", "labels": [], "entities": []}, {"text": "For the CLINIQPARA dataset, we randomly select a set of 300 questions from all the scenarios.", "labels": [], "entities": [{"text": "CLINIQPARA dataset", "start_pos": 8, "end_pos": 26, "type": "DATASET", "confidence": 0.9369920492172241}]}, {"text": "For each of these questions, we further choose aground truth paraphrase as well as a system generated paraphrase in a random fashion.", "labels": [], "entities": []}, {"text": "This result in a total of 600 pairs of question paraphrases, 300 from the gold dataset and 300 generated by the paraphrasing system.", "labels": [], "entities": [{"text": "gold dataset", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.7597298622131348}]}, {"text": "The constructed set is separately evaluated by two annotators who are asked to rate the paraphrases based on two parameters: fluency of the questions as natural language and their relevance to the original question.", "labels": [], "entities": []}, {"text": "Both of these scores range from 1 (worse) to 5 (best).", "labels": [], "entities": []}, {"text": "For each paraphrase, the final score is calculated by averaging the scores provided by the two annotators.", "labels": [], "entities": []}, {"text": "The fact that a paraphrase is ground truth or generated by the model is hidden from the annotators to avoid bias.", "labels": [], "entities": []}, {"text": "For the Quora dataset, we directly report the human evaluation results from.", "labels": [], "entities": [{"text": "Quora dataset", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9658375382423401}]}], "tableCaptions": [{"text": " Table 3: Performance of our paraphrasing system using  automated evaluation metrics.", "labels": [], "entities": []}, {"text": " Table 4: Results on CLINIQPARA using automated  evaluation metrics for 10-fold cross validation. Each  fold contains 5 scenarios over which the model is tested  after being trained on the other 45 scenarios.", "labels": [], "entities": []}, {"text": " Table 5: Results of human evaluation. Range of scores  is between 1 (worst) and 5 (best).", "labels": [], "entities": [{"text": "Range of scores", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.9568740129470825}]}]}