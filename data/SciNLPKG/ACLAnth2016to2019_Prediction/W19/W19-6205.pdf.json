{"title": [{"text": "Multilingual Probing of Deep Pre-Trained Contextual Encoders", "labels": [], "entities": [{"text": "Probing of Deep Pre-Trained Contextual Encoders", "start_pos": 13, "end_pos": 60, "type": "TASK", "confidence": 0.6434092173973719}]}], "abstractContent": [{"text": "Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora.", "labels": [], "entities": []}, {"text": "Earlier work on evaluating fixed-length sentence representations has included the use of 'probing' tasks, that use diagnostic clas-sifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena.", "labels": [], "entities": []}, {"text": "The principle of probing has also resulted in extended evaluations that include relatively newer word-level pre-trained encoders.", "labels": [], "entities": []}, {"text": "We build on probing tasks established in the literature and comprehensively evaluate and analyse-from a typological perspective amongst others-multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages.", "labels": [], "entities": []}, {"text": "Specifically , we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT's cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM).", "labels": [], "entities": [{"text": "BERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8584603667259216}]}], "introductionContent": [{"text": "Recent trends in NLP have demonstrated the utility of pre-trained deep contextual representations in numerous downstream NLP tasks, where they have almost consistently resulted in significant performance improvements.", "labels": [], "entities": []}, {"text": "Detailed evaluations have naturally followed: these have either been follow-up works to papers describing contextual representation systems, such as, or novel works evaluating abroad class of encoders on abroad variety of tasks (.", "labels": [], "entities": []}, {"text": "This paper is an example of the latter sort; we perform a comprehensive, large-scale evaluation of what linguistic phenomena these sequential encoders capture across a diverse set of languages.", "labels": [], "entities": []}, {"text": "This has often been referred to in the literature as probing; we use this terminology throughout this work.", "labels": [], "entities": []}, {"text": "Briefly, our goals are to probe our encoders in a multilingual setting -i.e., we use a series of probing tasks to quantify what sort of linguistic information our encoders retain, and how this information varies across language, across encoder, and across task.", "labels": [], "entities": []}, {"text": "As such, our experiments do not attempt to attain 'state-of-the-art' results; instead, we attempt to use a comparable experimental setting across each experiment, to quantify differences between settings rather than absolute results.", "labels": [], "entities": []}, {"text": "In Section 2, we describe prior work in multiple strands of research: specifically, on deep neural pre-training, on multilingualism in pre-training, and on evaluation.", "labels": [], "entities": []}, {"text": "Section 3 describes both the linguistic features we probe our representations for, and how we generated our probing corpus.", "labels": [], "entities": []}, {"text": "In Section 4, we describe and motivate our choice of encoders, as well as describe our infrastructural details.", "labels": [], "entities": []}, {"text": "The bulk of our contribution is in Section 5, where we describe and analyse our results.", "labels": [], "entities": []}, {"text": "Finally, we conclude with a discussion of the implications of these results and future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of contextual representations goes beyond merely deep representations; not too far in the past, work on evaluating shallow sentence representations was encouraged by the release of the SentEval toolkit (, which provided an easy-to-use framework that sentence representations could be 'plugged' into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 449, "end_pos": 474, "type": "TASK", "confidence": 0.8330811659495035}, {"text": "caption/image retrieval", "start_pos": 480, "end_pos": 503, "type": "TASK", "confidence": 0.7035867869853973}]}, {"text": "Relevant to our paper is set of 'probing tasks', a variant on the theme of diagnostic classification, that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations.", "labels": [], "entities": [{"text": "diagnostic classification", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7959571480751038}]}, {"text": "Based in part on, focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks.", "labels": [], "entities": []}, {"text": "Along similar lines, and contemporaneously with this work, probe similar deep pre-trained to the ones we do, on a set of 'sixteen diverse probing tasks'.) probe deep pre-trained encoders for sentence structure.", "labels": [], "entities": [{"text": "sentence structure", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7171372473239899}]}, {"text": "On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated corpora.", "labels": [], "entities": []}, {"text": "A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data.", "labels": [], "entities": []}, {"text": "Within the realm of evaluating multilingual sentence representations, describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset.", "labels": [], "entities": [{"text": "XNLI dataset", "start_pos": 83, "end_pos": 95, "type": "DATASET", "confidence": 0.8550364375114441}, {"text": "MultiNLI inference dataset", "start_pos": 175, "end_pos": 201, "type": "DATASET", "confidence": 0.6018150250116984}]}, {"text": "This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping non-English representations to an English representation space.", "labels": [], "entities": []}], "tableCaptions": []}