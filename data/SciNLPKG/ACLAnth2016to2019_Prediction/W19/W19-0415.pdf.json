{"title": [], "abstractContent": [{"text": "Concreteness of words has been studied extensively in psycholinguistic literature.", "labels": [], "entities": []}, {"text": "A number of datasets has been created with average values for perceived concreteness of words.", "labels": [], "entities": []}, {"text": "We show that we can train a regression model on these data, using word embeddings and morphological features.", "labels": [], "entities": []}, {"text": "We evaluate the model on 7 publicly available datasets and show that concreteness and imagery values can be predicted with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9887125492095947}]}, {"text": "Furthermore, we analyse typical contexts of abstract and concrete words and review the potentials of concreteness prediction for image annotation.", "labels": [], "entities": [{"text": "concreteness prediction", "start_pos": 101, "end_pos": 124, "type": "TASK", "confidence": 0.7545035183429718}]}, {"text": "1 Motivation Concreteness and imagery of words has been studied for several decades in the field of psycho-linguistics and psychology.", "labels": [], "entities": []}, {"text": "Values for concreteness and imagery of words are obtained by instructing and asking experimentees to score words on a numeric scale for these aspects.", "labels": [], "entities": []}, {"text": "We assume that concrete nouns occur in other contexts than abstract nouns do and that nouns with a high imagery value occur together with other words than nouns with a low imagery.", "labels": [], "entities": []}, {"text": "This is not as simple as it might sound, since most words can be used in different senses: e.g., nouns with high imagery might be accompanied by colour adjectives, but colours also fit perfectly with political parties and ideas.", "labels": [], "entities": []}, {"text": "Nevertheless, we expect that there are differences in the distribution of abstract and concrete words and words with high and low imagery.", "labels": [], "entities": []}, {"text": "If these differences indeed exist, and if concreteness and imagery are important aspects of the meaning of a word, we would expect that the characteristics of the context of concrete words are present in learned distributional representations of word meanings.", "labels": [], "entities": []}, {"text": "This finally, can be verified quite easily and is exactly what we will test in the following: it should be possible to read off the concreteness and imagery of a word from its distributional representation.", "labels": [], "entities": []}, {"text": "If it is possible to predict the concreteness and imagery of a word from its distributional representation, this also has a very practical aspect: Retrieving these values from experiments is an expensive and time consuming task.", "labels": [], "entities": []}, {"text": "If these values are needed fora psycholinguistic experiment or for some application it would bean advantage if we could compute them instead.", "labels": [], "entities": []}, {"text": "A practical application, that in fact was our initial motivation for this research, is the annotation of images (Charbonnier et al., 2018).", "labels": [], "entities": []}, {"text": "We need to find terms in the caption (and surrounding text) of an image that describe that image.", "labels": [], "entities": []}, {"text": "We expect that nouns with high concreteness and imagery values are much more likely to refer to concepts depicted in the image than abstract words do.", "labels": [], "entities": []}, {"text": "Our basic intuition is illustrated by the image caption pair shown in Fig. 1. Here the noun robot in the caption is very concrete, while the other nouns (systems, platform, research, development) are much more abstract.", "labels": [], "entities": []}, {"text": "The most concrete noun in this example describes quite well what is shown by the image.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: Section 2 gives an overview of common definitions of concreteness and imagery.", "labels": [], "entities": []}, {"text": "In section 3 we review the most relevant literature on this topic.", "labels": [], "entities": []}, {"text": "Section 4 gives an overview of the available datasets with human judgements on", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We used four datasets for evaluation and one, the largest, for training.", "labels": [], "entities": []}, {"text": "All datasets have values for concreteness, three of them also have values for imagery.", "labels": [], "entities": []}, {"text": "To avoid confusion, we will treat the datasets with imagery and concreteness values as different datasets.", "labels": [], "entities": []}, {"text": "Thus we have a total number of seven datasets.", "labels": [], "entities": []}, {"text": "gives an overview of the size of these datasets and their pairwise overlap.", "labels": [], "entities": []}, {"text": "The table also shows for how many of the words in each of the datasets we find word embeddings in two common used resources of pretrained embeddings.", "labels": [], "entities": []}, {"text": "Though we do not have enough imagery data to train a good model, we can, given the high correlation between imagery and concreteness values, also evaluate our concreteness model on imagery data.", "labels": [], "entities": []}, {"text": "An overview of the correlation for words occurring in different data sets is provided in.", "labels": [], "entities": []}, {"text": "We trained a SVM to build a regression model.", "labels": [], "entities": []}, {"text": "For the training we used \u03b3 = 0.01, C = 1.0 and an rbf kernel as parameters, found by grid search.", "labels": [], "entities": []}, {"text": "We use the training corpus described above to train the regression model.", "labels": [], "entities": []}, {"text": "We evaluated the classifier using different sets of features with tenfold cross validation on the training data.", "labels": [], "entities": []}, {"text": "Using all available features we evaluated the classifier on other datasets as well.", "labels": [], "entities": []}, {"text": "In most datasets used for evaluation, there area small number of words for which there are no pretrained word embeddings (see).", "labels": [], "entities": []}, {"text": "For these words the SVM cannot predict a value.", "labels": [], "entities": []}, {"text": "Hence, we will predict the value 3 (neutral, neither concrete nor abstract) for these words in the evaluation.", "labels": [], "entities": []}, {"text": "For the evaluation of all datasets with concreteness values we use Pearson's rand Kendall's \u03c4 to measure the correlation between the true values and the model's predictions.", "labels": [], "entities": [{"text": "Pearson's rand Kendall's \u03c4", "start_pos": 67, "end_pos": 93, "type": "METRIC", "confidence": 0.6047376294930776}]}, {"text": "We took the output of our regression model as is, even if its prediction is outside of the target interval of our training data.", "labels": [], "entities": []}, {"text": "The dataset from Newcombe contains only binary data.", "labels": [], "entities": [{"text": "Newcombe", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.7111732959747314}]}, {"text": "Here we can order the words according to the predicted concreteness value and use the Area under the ROC Curve (AUC) as evaluation measure.", "labels": [], "entities": [{"text": "ROC Curve (AUC)", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.8788851737976074}]}, {"text": "gives the distribution of predicted concreteness values for the concrete and abstract words from Newcombe and shows that the predicted values can distinguish quite well between concrete and abstract words.", "labels": [], "entities": [{"text": "Newcombe", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9870156049728394}]}, {"text": "The AUC for this binary classification is 0.990 using fastText embeddings, POS and suffixes and 0.981 using GoogNews embeddings, POS and suffixes.", "labels": [], "entities": [{"text": "AUC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9921553730964661}]}], "tableCaptions": [{"text": " Table 1: Size of datasets and the overlap with other data set.", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9542666673660278}]}, {"text": " Table 2: Pearson correlation between the values for concreteness and imagery for the words in  the intersection of two datasets.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9266102313995361}]}, {"text": " Table 3: Pearson (r) and Kendall (\u03c4) correlation results using our training corpus with cross- validation and different features.", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9427231550216675}, {"text": "Kendall (\u03c4) correlation", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.884977149963379}]}, {"text": " Table 4: Pearson (r) and Kendall (\u03c4) correlation between our concreteness estimations on the  concreteness values of the TWP and PYM datasets using fastText and GoogleNews embeddings.", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9604631811380386}, {"text": "Kendall (\u03c4) correlation", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.9148433089256287}, {"text": "TWP and PYM datasets", "start_pos": 122, "end_pos": 142, "type": "DATASET", "confidence": 0.7513805255293846}]}, {"text": " Table 5: Pearson (r) and Kendall (\u03c4) correlation between our imagery estimations on the imagery  values of the TWP, PYM and CP datasets using fastText and GoogleNews embeddings.", "labels": [], "entities": [{"text": "Pearson (r)", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9581695050001144}, {"text": "Kendall (\u03c4) correlation", "start_pos": 26, "end_pos": 49, "type": "METRIC", "confidence": 0.9252390027046203}, {"text": "TWP", "start_pos": 112, "end_pos": 115, "type": "DATASET", "confidence": 0.9315173625946045}, {"text": "PYM and CP datasets", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.6691080629825592}]}, {"text": " Table 6: Overview of high, medium and low concreteness for words from MT40K with their  original and our predicted values.", "labels": [], "entities": [{"text": "MT40K", "start_pos": 71, "end_pos": 76, "type": "DATASET", "confidence": 0.9193193316459656}]}, {"text": " Table 7: 30 words with highest pointwise mutual information in ukWaC with prototype of  abstract and concrete words, resp.", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9894862174987793}]}]}