{"title": [{"text": "Convolutional neural networks for low-resource morpheme segmentation: baseline or state-of-the-art?", "labels": [], "entities": [{"text": "low-resource morpheme segmentation", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7111099163691202}]}], "abstractContent": [{"text": "We apply convolutional neural networks to the task of shallow morpheme segmentation using low-resource datasets for 5 different languages.", "labels": [], "entities": [{"text": "shallow morpheme segmentation", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.6900373796621958}]}, {"text": "We show that both in fully supervised and semi-supervised settings our model beats previous state-of-the-art approaches.", "labels": [], "entities": []}, {"text": "We argue that convolutional neural networks reflect local nature of morpheme segmentation better than other neural approaches.", "labels": [], "entities": []}, {"text": "Morpheme segmentation consists in dividing a given word to meaningful individual units, morphs, which are surface realizations of underlying abstract morphemes.", "labels": [], "entities": [{"text": "Morpheme segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8797893822193146}]}, {"text": "For example, a word unexpectedly could be segmented as un-expected -ly, and the morpheme-ed maybe also realized as-t like in learnt.", "labels": [], "entities": []}, {"text": "The generated segmentation maybe used as input representation for machine translation (Mager et al., 2018) or morphological tagging (Matteson et al., 2018) or for automatic annotation of digital linguistic resources.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7195081412792206}, {"text": "morphological tagging", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7187901437282562}]}, {"text": "Briefly, information about internal morpheme structure makes the data less sparse since an out-of-vocabulary word may share its morphemes with other words already present in the training set.", "labels": [], "entities": []}, {"text": "This helps to recover semantic and morphological properties of an unknown word, which otherwise will be unacces-sible.", "labels": [], "entities": []}, {"text": "The task of morpheme segmentation is especially important for agglutinative languages, such as Finnish or Turkish, where a word is formed by attaching a sequence of affixes to its stem.", "labels": [], "entities": [{"text": "morpheme segmentation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.721793919801712}]}, {"text": "This affixes reflect both derivational and inflectional processes.", "labels": [], "entities": []}, {"text": "A common example from Turkish is ev-lerinizden 'from your houses', which is decomposed as: ev ler iniz den house +PL your+PL +ABL The task of morpheme segmentation is even harder for polysynthetic languages: while in ag-glutinative languages morphemes are usually in one-to-one correspondence with morphological features, for polysynthetic languages this matching is more complex with no clear bound between compound words and sentences.", "labels": [], "entities": [{"text": "ABL", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9644801020622253}, {"text": "morpheme segmentation", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.7270947694778442}]}, {"text": "For example, in Chuckchi language the whole phrase 'The house broke' can be expressed as Ga ra semat \u00ecen +PF housebreak +PF+3SG Consequently, polysynthetic language demonstrate extremely high morpheme-to-word ratio, which leads to high type-token ratio, which makes their automatic processing harder.", "labels": [], "entities": []}, {"text": "Even further, this processing is performed in low-resource setting since most polysynthetic languages have only few hundreds or thousands of speakers and consequently tend to lack annotated digital resources.", "labels": [], "entities": []}, {"text": "Hence, the algorithms initially designed for less complex languages with more data (mostly for English) may change significantly their properties when applied to low-resource polysynthetic data.", "labels": [], "entities": []}, {"text": "That is especially the case for neural methods, which are (often erroneously 1) believed to be more data-hungry than earlier approaches.", "labels": [], "entities": []}, {"text": "However, in 2019 it is insufficient to just say \"neural networks\" in case of NLP, since there are various neural networks whose properties may differ significantly.", "labels": [], "entities": []}, {"text": "Leaving aside the immense diversity of network architectures, they can be separated in three main categories: the convo-lutional ones (CNNs), where convolutional windows capture local regularities; the recurrent ones, where GRUs and LSTMs memorize potentially unbounded context; and sequence-to-sequence (seq2seq) models, which perform string transduc-tions using encoder-decoder approach.", "labels": [], "entities": []}, {"text": "Among the three, convolutional neural networks are the least 1 see (Zeman et al., 2018) and (Cotterell et al., 2017) that show that both in morphological tagging and automatic word inflection neural networks are clearly superior, though their architecture should be adapted for the lack of data.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 140, "end_pos": 161, "type": "TASK", "confidence": 0.6639891564846039}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Size of the datasets used for evaluation.", "labels": [], "entities": []}, {"text": " Table 3: Results of our extended CNN models in comparison with the basic one and state-of-the-art. Results for  Yuto-Aztecan languages are from Kann et al. (2018), for North S\u00e1mi from Gr\u00f6nroos et al. (2019).", "labels": [], "entities": []}]}