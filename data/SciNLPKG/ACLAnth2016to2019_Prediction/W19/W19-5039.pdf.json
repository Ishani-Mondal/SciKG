{"title": [{"text": "Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8123824596405029}, {"text": "Question Entailment", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7499989867210388}, {"text": "Question Answering", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8307879865169525}]}], "abstractContent": [{"text": "This paper presents the MEDIQA 2019 shared task organized at the ACL-BioNLP workshop.", "labels": [], "entities": [{"text": "MEDIQA 2019 shared task", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6881128400564194}, {"text": "ACL-BioNLP workshop", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.8637833595275879}]}, {"text": "The shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entail-ment in the medical domain, and their application to improve domain specific information retrieval and question answering systems.", "labels": [], "entities": [{"text": "domain specific information retrieval", "start_pos": 184, "end_pos": 221, "type": "TASK", "confidence": 0.6588553190231323}, {"text": "question answering", "start_pos": 226, "end_pos": 244, "type": "TASK", "confidence": 0.8267837464809418}]}, {"text": "MEDIQA 2019 includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and Question Answering (QA) in the medical domain.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 68, "end_pos": 105, "type": "TASK", "confidence": 0.6779507746299108}, {"text": "Question Answering (QA)", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.8430812001228333}]}, {"text": "72 teams participated in the challenge, achieving an accuracy of 98% in the NLI task, 74.9% in the RQE task, and 78.3% in the QA task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9995224475860596}, {"text": "RQE task", "start_pos": 99, "end_pos": 107, "type": "TASK", "confidence": 0.4875137209892273}]}, {"text": "In this paper, we describe the tasks, the datasets, and the participants' approaches and results.", "labels": [], "entities": []}, {"text": "We hope that this shared task will attract further research efforts in textual inference, question entailment, and question answering in the medical domain.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7387076616287231}, {"text": "question entailment", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8411699533462524}, {"text": "question answering", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.8994183242321014}]}], "introductionContent": [{"text": "The first open-domain challenge in Recognizing Textual Entailment (RTE) was launched in 2005 () and has prompted the development of a wide range of approaches (.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 35, "end_pos": 71, "type": "TASK", "confidence": 0.8614232937494913}]}, {"text": "Recently, large-scale datasets such as SNLI () and MultiNLI () were introduced for the task of Natural Language Inference (NLI) targeting three relations between sentences: Entailment, Neutral, and Contradiction.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 95, "end_pos": 127, "type": "TASK", "confidence": 0.6739612321058909}]}, {"text": "Few efforts have studied the benefits of RTE and NLI in other NLP tasks such as text exploration), identifying evidence for eligibility criteria satisfaction in clinical trials (, and the summarization of PMC articles (.", "labels": [], "entities": [{"text": "text exploration", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.809078186750412}, {"text": "summarization of PMC", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.8278648853302002}]}, {"text": "NLI can also be beneficial for Question Answering (QA).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8941906452178955}]}, {"text": "presented entailment-based methods to filter and rank answers and showed that RTE can enhance the performance of open-domain QA systems and provide the inferential information needed to validate the answers.", "labels": [], "entities": []}, {"text": "C \u00b8 elikyilmaz et al. presented a graph-based semi-supervised method for QA exploiting entailment relations between questions and candidate answers and demonstrated that the use of unlabeled entailment data can improve answer ranking.", "labels": [], "entities": [{"text": "QA", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.965610921382904}]}, {"text": "Ben noted that the requirements of question entailment in QA are different from general question similarity, and introduced the task of Recognizing Question Entailment (RQE) in order to answer new questions by retrieving entailed questions with pre-existing answers.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.7748955190181732}]}, {"text": "Ben Abacha and Demner-Fushman (2019) proposed a novel QA approach based on RQE, with the introduction of the MedQuAD medical question-answer collection, and showed empirical evidence supporting question entailment for QA.", "labels": [], "entities": [{"text": "RQE", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7990871667861938}, {"text": "MedQuAD medical question-answer collection", "start_pos": 109, "end_pos": 151, "type": "DATASET", "confidence": 0.775319293141365}, {"text": "QA", "start_pos": 218, "end_pos": 220, "type": "TASK", "confidence": 0.9644887447357178}]}, {"text": "Although the idea of using entailment in QA has been introduced, research investigating methods to incorporate textual inference and question entailment into QA systems is still limited in the literature.", "labels": [], "entities": []}, {"text": "Moreover, despite a few recent efforts to design RTE methods and datasets from MEDLINE abstracts and to create the MedNLI dataset from clinical data, the entailment and inference tasks remain less studied in the medical domain.", "labels": [], "entities": [{"text": "MEDLINE abstracts", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.8973456621170044}, {"text": "MedNLI dataset from clinical data", "start_pos": 115, "end_pos": 148, "type": "DATASET", "confidence": 0.8702233791351318}]}, {"text": "MEDIQA 2019 1 aims to highlight further the NLI and RQE tasks in the medical domain, and their applications in QA and NLP.", "labels": [], "entities": [{"text": "MEDIQA 2019 1", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8573548595110575}]}, {"text": "presents the MEDIQA tasks in the AIcrowd platform 2 . For the QA task, participants were tasked to filter and re-rank the provided answers.", "labels": [], "entities": [{"text": "AIcrowd platform 2", "start_pos": 33, "end_pos": 51, "type": "DATASET", "confidence": 0.8820600708325704}]}, {"text": "Reuse of the systems developed in the first and second tasks was highly encouraged.", "labels": [], "entities": []}, {"text": "The first task focuses on Natural Language Inference (NLI) in the medical domain.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.8207367161909739}]}, {"text": "We use three labels for the relation between two sentences: Entailment, Neutral and Contradiction.", "labels": [], "entities": []}], "datasetContent": [{"text": "The MEDIQA-NLI test set consists of 405 texthypothesis pairs.", "labels": [], "entities": [{"text": "MEDIQA-NLI test set", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8077405293782552}]}, {"text": "The training set is the MedNLI dataset, which includes 14,049 clinical sentence pairs derived from the MIMIC-III database.", "labels": [], "entities": [{"text": "MedNLI dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9807374179363251}, {"text": "MIMIC-III database", "start_pos": 103, "end_pos": 121, "type": "DATASET", "confidence": 0.920120358467102}]}, {"text": "Both datasets are publicly available .  The MEDIQA-RQE test set consists of 230 pairs of Consumer Health Questions (CHQs) received by the U.S. National Library of Medicine (NLM) and Frequently Asked Questions (FAQs) from NIH institutes.", "labels": [], "entities": [{"text": "MEDIQA-RQE test set", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.8334109584490458}]}, {"text": "The collection was created automatically and double validated manually by medical experts.", "labels": [], "entities": []}, {"text": "presents positive and negative examples from the test set.", "labels": [], "entities": []}, {"text": "The RQE training and validation sets contain respectively 8,890 and 302 medical question pairs created by) using a collection of clinical questions () for the training set and pairs of CHQs and FAQs pairs for the validation set.", "labels": [], "entities": [{"text": "RQE training and validation sets", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.6434974312782288}, {"text": "FAQs", "start_pos": 194, "end_pos": 198, "type": "METRIC", "confidence": 0.7994461059570312}]}, {"text": "All the RQE training, validation and test sets are publicly available 6 .  The MEDIQA-QA training, validation and test sets were created by submitting medical questions to the consumer health QA system CHiQA , and then rating and re-ranking the retrieved answers manually by medical experts to provide reference ranks (1 to 11) and scores (4: Excellent Answer, 3: Correct but Incomplete, 2: Related, 1: Incorrect).", "labels": [], "entities": [{"text": "Excellent Answer", "start_pos": 343, "end_pos": 359, "type": "METRIC", "confidence": 0.8168730139732361}]}, {"text": "We provided two training sets for the QA task: \u2022 104 consumer health questions from the TREC-2017-LiveQA medical data) covering different topics such as diseases and drugs, and 839 associated answers retrieved by CHiQA and manually rated and re-ranked.", "labels": [], "entities": [{"text": "TREC-2017-LiveQA medical data", "start_pos": 88, "end_pos": 117, "type": "DATASET", "confidence": 0.8931797742843628}]}, {"text": "\u2022 104 simple questions about the most frequent diseases (dataset named Alexa), and 862 associated answers.", "labels": [], "entities": []}, {"text": "The evaluation of the NLI and RQE tasks was based on accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9986419081687927}]}, {"text": "In the QA task, participants https://github.com/abachaa/ MEDIQA2019/tree/master/MEDIQA_Task3_QA were tasked to filter and re-rank the provided answers.", "labels": [], "entities": []}, {"text": "The QA evaluation was based on accuracy, Mean Reciprocal Rank (MRR), Precision, and Spearmans Rank Correlation Coefficient (Spearman's rho).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9997740387916565}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.9681555430094401}, {"text": "Precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9994035959243774}, {"text": "Spearmans Rank Correlation Coefficient", "start_pos": 84, "end_pos": 122, "type": "METRIC", "confidence": 0.7273893728852272}]}], "tableCaptions": [{"text": " Table 3: Official Results of the MEDIQA-NLI Task", "labels": [], "entities": [{"text": "MEDIQA-NLI Task", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.7803697288036346}]}, {"text": " Table 4: Official Results of the MEDIQA-RQE Task", "labels": [], "entities": [{"text": "MEDIQA-RQE Task", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.7712347507476807}]}, {"text": " Table 5: Official Results of the MEDIQA-QA Task", "labels": [], "entities": [{"text": "MEDIQA-QA Task", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.7723962366580963}]}]}