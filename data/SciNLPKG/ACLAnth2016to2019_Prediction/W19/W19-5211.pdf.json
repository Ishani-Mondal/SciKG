{"title": [{"text": "Widening the Representation Bottleneck in Neural Machine Translation with Lexical Shortcuts", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.641463170448939}]}], "abstractContent": [{"text": "The transformer is a state-of-the-art neural translation model that uses attention to iteratively refine lexical representations with information drawn from the surrounding context.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.8253074586391449}]}, {"text": "Lexical features are fed into the first layer and propagated through a deep network of hidden layers.", "labels": [], "entities": []}, {"text": "We argue that the need to represent and propagate lexical features in each layer limits the model's capacity for learning and representing other information relevant to the task.", "labels": [], "entities": []}, {"text": "To alleviate this bottleneck, we introduce gated shortcut connections between the embedding layer and each subsequent layer within the encoder and decoder.", "labels": [], "entities": []}, {"text": "This enables the model to access relevant lexical content dynamically, without expending limited resources on storing it within intermediate states.", "labels": [], "entities": []}, {"text": "We show that the proposed modification yields consistent improvements over a baseline transformer on standard WMT translation tasks in 5 translation directions (0.9 BLEU on average) and reduces the amount of lexical information passed along the hidden layers.", "labels": [], "entities": [{"text": "WMT translation tasks", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.9193816383679708}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9971987009048462}]}, {"text": "We furthermore evaluate different ways to integrate lexical connections into the transformer architecture and present ablation experiments exploring the effect of proposed shortcuts on model behavior.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since it was first proposed, the transformer model () has quickly established itself as a popular choice for neural machine translation, where it has been found to deliver state-ofthe-art results on various translation tasks (.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 109, "end_pos": 135, "type": "TASK", "confidence": 0.6825547417004904}]}, {"text": "Its success can be attributed to the model's high parallelizability allowing for significantly faster training compared to recurrent neural Our code is publicly available to aid the reproduction of the reported results: https://github.com/demelin/ transformer_lexical_shortcuts networks (, superior ability to perform lexical disambiguation, and capacity for capturing long-distance dependencies on par with existing alternatives (.", "labels": [], "entities": []}, {"text": "Recently, several studies have investigated the nature of features encoded within individual layers of neural translation models (.", "labels": [], "entities": []}, {"text": "One central finding reported in this body of work is that, in recurrent architectures, different layers prioritize different information types.", "labels": [], "entities": []}, {"text": "As such, lower layers appear to predominantly perform morphological and syntactic processing, whereas semantic features reach their highest concentration towards the top of the layer stack.", "labels": [], "entities": []}, {"text": "One necessary consequence of this distributed learning is that different types of information encoded within input representations received by the translation model have to be transported to the layers specialized in exploiting them.", "labels": [], "entities": []}, {"text": "Within the transformer encoder and decoder alike, information exchange proceeds in a strictly sequential manner, whereby each layer attends over the output of the immediately preceding layer, complemented by a shallow residual connection.", "labels": [], "entities": []}, {"text": "For input features to be successfully propagated to the uppermost layers, the translation model must therefore store them in its intermediate representations until they can be processed.", "labels": [], "entities": []}, {"text": "By retaining lexical content, the model is unable to leverage its full representational capacity for learning new information from other sources, such as the surrounding sentence context.", "labels": [], "entities": []}, {"text": "We refer to this limitation as the representation bottleneck.", "labels": [], "entities": []}, {"text": "To alleviate this bottleneck, we propose extending the standard transformer architecture with lexical shortcuts which connect the embedding layer with each subsequent self-attention sub-layer in both encoder and decoder.", "labels": [], "entities": []}, {"text": "The shortcuts are defined as gated skip connections, allowing the model to access relevant lexical information at any point, instead of propagating it upwards from the embedding layer along the hidden states.", "labels": [], "entities": []}, {"text": "We evaluate the resulting model's performance on multiple language pairs and varying corpus sizes, showing a consistent improvement in translation quality over the unmodified transformer baseline.", "labels": [], "entities": []}, {"text": "Moreover, we examine the distribution of lexical information across the hidden layers of the transformer model in its standard configuration and with added shortcut connections.", "labels": [], "entities": []}, {"text": "The presented experiments provide quantitative evidence for the presence of a representation bottleneck in the standard transformer and its reduction following the integration of lexical shortcuts.", "labels": [], "entities": []}, {"text": "While our experimental efforts are centered around the transformer, the proposed components are compatible with other multi-layer NMT architectures.", "labels": [], "entities": []}, {"text": "The contributions of our work are as follows: 1.", "labels": [], "entities": []}, {"text": "We propose the use of lexical shortcuts as a simple strategy for alleviating the representation bottleneck in NMT models.", "labels": [], "entities": []}, {"text": "2. We demonstrate significant improvements in translation quality across multiple language pairs as a result of equipping the transformer with lexical shortcut connections.", "labels": [], "entities": []}, {"text": "3. We conduct a series of ablation studies, showing that shortcuts are best applied to the self-attention mechanism in both encoder and decoder.", "labels": [], "entities": []}, {"text": "4. We report a positive impact of our modification on the model's ability to perform word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7822853128115336}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU scores for the EN\u2192DE news translation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995249509811401}, {"text": "EN\u2192DE news translation task", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6854443748792013}]}, {"text": " Table 2: Effect of lexical shortcuts on translation quality for different language pairs, as measured by sacreBLEU.", "labels": [], "entities": []}, {"text": " Table 3: sacreBLEU scores for small EN\u2192DE models;  'test mean' denotes the average of test-sets in table (1).", "labels": [], "entities": []}, {"text": " Table 4: sacreBLEU for shortcut variants of EN\u2192DE  models; 'test mean' averages over test-sets in table (1).", "labels": [], "entities": []}]}