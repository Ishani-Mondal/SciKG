{"title": [{"text": "Bootstrapping a Neural Morphological Analyzer for St. Lawrence Island Yupik from a Finite-State Transducer", "labels": [], "entities": []}], "abstractContent": [{"text": "Morphological analysis is a critical enabling technology for polysynthetic languages.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9332228004932404}]}, {"text": "We present a neural morphological analyzer for case-inflected nouns in St. Lawrence Island Yupik, an endangered polysythetic language in the Inuit-Yupik language family, treating morphological analysis as a recurrent neural sequence-to-sequence task.", "labels": [], "entities": []}, {"text": "By utilizing an existing finite-state morphological analyzer to create training data, we improve analysis coverage on attested Yupik word types from approximately 75% for the existing finite-state analyzer to 100% for the neural analyzer.", "labels": [], "entities": [{"text": "coverage", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.887489378452301}]}, {"text": "At the same time, we achieve a substantially higher level of accuracy on a held-out testing set, from 78.9% accuracy for the finite-state analyzer to 92.2% accuracy for our neural an-alyzer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9994327425956726}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9970487952232361}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9987020492553711}]}], "introductionContent": [{"text": "St. Lawrence Island Yupik, henceforth Yupik, is an endangered polysynthetic language spoken on St. Lawrence Island, Alaska and the Chukotka Peninsula of Russia.", "labels": [], "entities": []}, {"text": "Members of the Yupik community on St. Lawrence Island have expressed interest in language revitalization and conservation.", "labels": [], "entities": [{"text": "language revitalization", "start_pos": 81, "end_pos": 104, "type": "TASK", "confidence": 0.7276028543710709}]}, {"text": "Recent work by resulted in a finite-state morphological analyzer for Yupik implemented in foma.", "labels": [], "entities": [{"text": "foma", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.9113604426383972}]}, {"text": "That analyzer implements the grammatical and morphophonological rules documented in A Practical Grammar of the St. Lawrence Island / Siberian Yupik Eskimo Language.", "labels": [], "entities": [{"text": "St. Lawrence Island / Siberian Yupik Eskimo Language", "start_pos": 111, "end_pos": 163, "type": "DATASET", "confidence": 0.49252739548683167}]}, {"text": "In this work, we test the coverage of the finitestate analyzer against a corpus of digitized Yupik texts and find that the analyzer fails to return any analysis for approximately 25% of word types (see \u00a72 and).", "labels": [], "entities": []}, {"text": "We present a higher-coverage neural morphological analyzer for case-inflected Yupik nouns that involve no derivational morphology, using the previously-developed finitestate analyzer to generate large amounts of labeled training data ( \u00a73).", "labels": [], "entities": []}, {"text": "We evaluate the performance of the finite-state and neural analyzers, and find that the neural analyzer results in higher coverage and higher accuracy ( \u00a75), even when the finite-state analyzer is augmented with a guessing module to hypothesize analyzes for out-of-vocabulary words ( \u00a74).", "labels": [], "entities": [{"text": "coverage", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9806671142578125}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9985106587409973}]}, {"text": "We thus find that a robust high-accuracy morphological analyzer can be successfully bootstrapped from an existing lower-coverage finitestate morphological analyzer ( \u00a76), a result which has implications for the development of language technologies for Yupik and other morphologicallyrich languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "The finite-state morphological analyzer of implements the grammatical and morphophonological rules documented in A Practical Grammar of the St. Lawrence Island / Siberian Yupik Eskimo Language) using the foma finite-state toolkit.", "labels": [], "entities": [{"text": "finite-state morphological analyzer", "start_pos": 4, "end_pos": 39, "type": "TASK", "confidence": 0.6033028264840444}]}, {"text": "In order to evaluate the percentage of attested Yupik word forms for which the finite-state analyzer produces any analysis, we began by digitizing several hundred Yupik sentences presented in as examples to be translated by the reader.", "labels": [], "entities": []}, {"text": "We next assembled, digitized, and manually validated seven texts that each consist of a collection of Yupik stories along with corresponding English translations.", "labels": [], "entities": []}, {"text": "The texts include four anthologies of Yupik stories, legends, and folk tales, along with three leveled elementary primers prepared by the Bering Strait School District in the 1990s (.", "labels": [], "entities": [{"text": "Bering Strait School District", "start_pos": 138, "end_pos": 167, "type": "DATASET", "confidence": 0.7695484608411789}]}, {"text": "Of the four anthologies, three comprise a trilogy known as The Lore of St. Lawrence Is-, Ungi is an abbreviation for Ungipaghaghlanga, and Lvl1 -Lvl3 refer to the elementary Yupik primers).", "labels": [], "entities": [{"text": "The Lore of St. Lawrence Is-", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.7573934921196529}]}, {"text": "land (, while the last is a stand-alone text, Ungipaghaghlanga.", "labels": [], "entities": []}, {"text": "Together, these texts represent the largest known collection of written Yupik.", "labels": [], "entities": []}, {"text": "After digitizing each text, we analyzed each Yupik word in that text using the finite-state morphological analyzer.", "labels": [], "entities": []}, {"text": "We then calculated the percentage of tokens from each text for which the finite-state analyzer produced at least one analysis.", "labels": [], "entities": []}, {"text": "We call this number coverage, and report this result for each text in.", "labels": [], "entities": [{"text": "coverage", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8926520943641663}]}, {"text": "The mean coverage over the entire set of texts was 77.56%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9328970313072205}]}, {"text": "The neural morphological analyzer described in the subsequent section was developed in large part to provide morphological analyses for the remaining 22.44% of heretofore unanalyzed Yupik tokens.", "labels": [], "entities": []}, {"text": "Using Marian, we used the data tokenized by characters and trained a shallow neural network model that implemented an attentional encoder-decoder model () with early stopping and holdout cross validation.", "labels": [], "entities": []}, {"text": "We used the parameters described in, where the encoder and decoder consisted of one hidden layer each, of size 1024.", "labels": [], "entities": []}, {"text": "Of the 109,395 items in the final test set, this shallow neural model achieved 100% coverage and 59.67% accuracy on the test set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9974679946899414}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9995589852333069}]}, {"text": "Error analysis revealed a substantial amount of underspecification and surface form ambiguity as a result of syncretism in the nominal paradigm.", "labels": [], "entities": []}, {"text": "As exemplified in (3a) and (3b), inflectional suffixes in Yupik may share the same underlying phonological form as well as the same morphophonological rules associated with that suffix.", "labels": [], "entities": []}, {"text": "Since these surface forms are only distinguishable through grammatical context, and our neural analyzer was not trained to consider context, it was made to guess which underlying form to return, and as suggested by the low accuracy score of 59.67%, the analyzer's guesses were often incorrect.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 223, "end_pos": 237, "type": "METRIC", "confidence": 0.9802584052085876}]}, {"text": "We did not think it was proper to penalize the analyzer for wrong answers in instances of syncretism, and consequently implemented a postprocessing step to account for this phenomenon.", "labels": [], "entities": []}, {"text": "This step was performed after the initial calculation of the neural analyzer's accuracy score, and provided an estimated or adjusted accuracy score that considered the syncretic forms equivalent.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.9785700738430023}, {"text": "accuracy score", "start_pos": 133, "end_pos": 147, "type": "METRIC", "confidence": 0.9797264337539673}]}, {"text": "It iterated through all outputs of the neural analyzer that were initially flagged as incorrect for differing from their test set counterparts.", "labels": [], "entities": []}, {"text": "Using the finitestate analyzer, the surface forms for each output and its corresponding test set item were then generated to verify whether or not their surface forms matched.", "labels": [], "entities": []}, {"text": "If they matched, the neural analyzer's output was instead counted as correct (see on the following page for examples).", "labels": [], "entities": []}, {"text": "Assessed in this way, the shallow model achieved an adjusted accuracy score of 99.90%.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9825087487697601}]}, {"text": "We trained four models on the inflected nouns dataset, experimenting with the shallow versus deep neural network architectures and the two tokenization methods: by characters and by redoubled graphemes.", "labels": [], "entities": []}, {"text": "The shallow neural models were identical to those described in \u00a73.2.2 and \u00a73.2.3.", "labels": [], "entities": []}, {"text": "The deep neural models used four hidden layers and LSTM cells, following.", "labels": [], "entities": []}, {"text": "As before, all models were trained to convergence and evaluated with holdout cross validation on the same test set.", "labels": [], "entities": []}, {"text": "Results are presented in on the next page, along with the accuracy scores before and after resolving all surface ambiguities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9996098875999451}]}, {"text": "While all models reached over 99% adjusted accuracy, the deep models outperformed their shallow counterparts, and the models trained on data tokenized by redoubled graphemes fared marginally better than those trained on data tokenized by individual characters.", "labels": [], "entities": [{"text": "adjusted", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9527074098587036}, {"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.8565347790718079}]}, {"text": "The latter may result from the fact that some inflections operate on full graphemes, for instance, gh# \u2192 q# during inflection for the unpossesed absolutive singular.", "labels": [], "entities": []}, {"text": "The percentage improvement is so slight, however, that this may not be of much consequence.", "labels": [], "entities": [{"text": "percentage", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.965893030166626}]}, {"text": "The deep model trained on redoubled graphemes was most accurate, peaking at 99.96%.", "labels": [], "entities": []}, {"text": "Despite comparable accuracy scores, there did not appear to be a discernible pattern with respect to errors among the four models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.999267041683197}]}], "tableCaptions": [{"text": " Table 1: For each Yupik text, the percentage of types  and tokens for which the Yupik finite-state analyzer of  Chen and Schwartz (2018) returns an analysis, along  with the total number of tokens per text. Ref refers  to Yupik examples taken from the Jacobson (2001)  reference grammar, SLI1 -SLI3 refer to the Lore of  St. Lawrence Island, volumes 1-3 (Apassingok et al.,", "labels": [], "entities": [{"text": "Lore of  St. Lawrence Island", "start_pos": 313, "end_pos": 341, "type": "DATASET", "confidence": 0.7846681237220764}]}, {"text": " Table 2: List of documented morphophonological rules  in Yupik and their lexicalized symbols. For more de- tails see Jacobson (2001) and Badten et al. (2008).", "labels": [], "entities": []}, {"text": " Table 4: Contrasts the two tokenization methods introduced in  \u00a7 3.2.1 (tokenization by character and tokeniza- tion by orthographically transparent Yupik grapheme) on the surface form qikmiq (dog) and its underlying form  qikmigh[N][ABS][UNPD][SG].", "labels": [], "entities": [{"text": "UNPD][SG]", "start_pos": 240, "end_pos": 249, "type": "DATASET", "confidence": 0.7977725118398666}]}, {"text": " Table 5: Number of morphotactically possible Yupik  word forms formed using 0-7 derivational suffixes.", "labels": [], "entities": []}, {"text": " Table 7: Accuracy and adjusted accuracy scores on the  generated test data from  \u00a73.2.1 (before and after resolv- ing all surface ambiguity) for each model. The bolded  percentage indicates the highest-performing model on  the heldout test data.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983872175216675}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9038229584693909}]}, {"text": " Table 8: Comparison of coverage and accuracy scores  on the blind test set ( \u00a75.1), contrasting the finite-state  and neural analyzers. Accuracy is calculated over all  types and tokens.", "labels": [], "entities": [{"text": "coverage", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9729391932487488}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9974457025527954}, {"text": "Accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9949033260345459}]}]}