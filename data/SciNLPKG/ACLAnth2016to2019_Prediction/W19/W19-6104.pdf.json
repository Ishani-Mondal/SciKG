{"title": [{"text": "Gender Bias in Pretrained Swedish Embeddings", "labels": [], "entities": [{"text": "Gender Bias in Pretrained Swedish Embeddings", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6283873071273168}]}], "abstractContent": [{"text": "This paper investigates the presence of gender bias in pretrained Swedish embed-dings.", "labels": [], "entities": []}, {"text": "We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task.", "labels": [], "entities": []}, {"text": "Our experiments show some significant differences between the pretrained embed-dings, with word-based methods showing the most bias and contextualized language models showing the least.", "labels": [], "entities": []}, {"text": "We also demonstrate that a previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.", "labels": [], "entities": []}], "introductionContent": [{"text": "The motivation for this study is the currently widespread practice of using pretrained embeddings as building blocks for NLP-related tasks.", "labels": [], "entities": []}, {"text": "More specifically, we are concerned about such usage by actors in the public sector, for instance government agencies and public organizations.", "labels": [], "entities": []}, {"text": "It is obvious how the presence of (gender or racial) bias would be potentially serious in applications where embeddings are used as input to decision support systems in the public sector.", "labels": [], "entities": []}, {"text": "As an example, in Sweden limited companies must be approved and registered by the Swedish Companies Registration Office.", "labels": [], "entities": [{"text": "Swedish Companies Registration Office", "start_pos": 82, "end_pos": 119, "type": "DATASET", "confidence": 0.7458537966012955}]}, {"text": "One important (and internationally unique) step in this registration procedure is the approval of the company name, which is decided by case handlers at the Registration Office.", "labels": [], "entities": []}, {"text": "Their decision is based on several factors, one of which is the appropriateness of the company name in relation to the company description.", "labels": [], "entities": []}, {"text": "Now, imagine the hypothetical use casein which the case handlers use a decision support system that employs pretrained embeddings to quantify the similarity between a suggested company name and its company description.", "labels": [], "entities": []}, {"text": "exemplifies what the results might look like.", "labels": [], "entities": []}, {"text": "In this fictive example, the company description states that the company will do business with cars, and the name suggestions are composed of a person name in genitive and the word \"cars\" (i.e. \"Fredrik's cars\").", "labels": [], "entities": []}, {"text": "We use pretrained Swedish ELMo embeddings ( to compute the distance between the name suggestion and the company description.", "labels": [], "entities": []}, {"text": "The results demonstrate that male person names (\"Magnus\" and \"Fredrik\") are closer to \"cars\" in the ELMo similarity space than female person names (\"Maria\" and \"Anna\").", "labels": [], "entities": [{"text": "ELMo similarity space", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.7798470457394918}]}, {"text": "If such results are used as input to a decision support system for deciding on the appropriateness of a company name suggestion in relation to a company description, we might introduce gender bias into the decision process.", "labels": [], "entities": []}, {"text": "We subscribe to the view that such bias would be unfair and problematic.", "labels": [], "entities": []}, {"text": "The point of this paper is therefore to investigate gender bias when using existing and readily available pretrained embeddings for tasks relating to names and occupations.", "labels": [], "entities": []}, {"text": "We include both word-based embeddings produced using word2vec and fastText, as well as characterbased (and WordPiece-based) contextualized embeddings produced using ELMo and the multilingual BERT.", "labels": [], "entities": [{"text": "BERT", "start_pos": 191, "end_pos": 195, "type": "METRIC", "confidence": 0.7289313077926636}]}, {"text": "The next section covers related work.", "labels": [], "entities": []}, {"text": "We then discuss the various embeddings in Section 3, before we then turn to some experimental evidence of bias in the embeddings, and we also show that the previously proposed debiasing method is unable to handle gender bias in our scenario.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a first experiment, we compute the similarity between the names and the occupations using the different embeddings.", "labels": [], "entities": []}, {"text": "We do this by computing the similarity between each name and each occupation.", "labels": [], "entities": [{"text": "similarity", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9731724262237549}]}, {"text": "shows the percentage of female and male names that are on average more similar to a female vs. male occupation.", "labels": [], "entities": []}, {"text": "Numbers in parentheses are based on only the most similar oc-: Percentage of female and male names that are on average more similar to a female vs. male occupation.", "labels": [], "entities": []}, {"text": "The similarities are calculated based on the original embeddings, before the application of the debiasing step described in Section 6.", "labels": [], "entities": []}, {"text": "Numbers in parentheses only count the single most similar occupation for each name.", "labels": [], "entities": []}, {"text": "As an example, imagine we only have two female and male occupations, and that the name \"Anna\" has the similarities 0.47 and 0.78 to the female occupations, and the similarities 0.12 and 0.79 to the male occupations.", "labels": [], "entities": []}, {"text": "In this example, \"Anna\" would be closer to the female occupations when counting the average similarities (0.625 vs. 0.455), but closer to the male occupations when only considering the most similar examples (0.79 vs. 0.78).", "labels": [], "entities": []}, {"text": "There are several ways in which an embedding could show bias in this setting.", "labels": [], "entities": []}, {"text": "The arguably most detrimental effect would be if the embedding grouped male names with male occupations and female names with female occupations.", "labels": [], "entities": []}, {"text": "Somewhat less severe, but still problematic, would be if the embedding grouped all names with female or male occupations.", "labels": [], "entities": []}, {"text": "A completely unbiased model would not show any difference between the female and male names with respect to female and male occupations.", "labels": [], "entities": []}, {"text": "The numbers in demonstrate some interesting differences between the different embeddings.", "labels": [], "entities": []}, {"text": "word2vec shows a clear tendency to group both male and female names with male occupations.", "labels": [], "entities": [{"text": "word2vec", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9094724059104919}]}, {"text": "fastText, on the other hand, shows a bias for female occupations for male names, and for male occupations for female names.", "labels": [], "entities": [{"text": "fastText", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.960034966468811}]}, {"text": "This is a very interesting difference, given that the only algorithmic difference between these models is the inclusion of character n-grams in the latter model.", "labels": [], "entities": []}, {"text": "The results for ELMo and BERT show some interesting differences too.", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9946998357772827}]}, {"text": "ELMo groups the male names with the male occupations, but is less biased for the female names.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8666046261787415}]}, {"text": "When counting only the single most similar occupation, ELMo shows a similar tendency as word2vec and groups both male and female names with male occupations.", "labels": [], "entities": []}, {"text": "BERT, on the other hand, seems slightly more balanced, with a tendency similar to fastText when counting the average similarities.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9807426929473877}, {"text": "fastText", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9356704950332642}]}, {"text": "When only counting the single most similar occupation, BERT is almost perfectly balanced between female and male occupations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9942924976348877}]}, {"text": "We repeat the experiment described in Section 5, but using the debiased embeddings.", "labels": [], "entities": []}, {"text": "It is clear that the debiasing method does not have any impact on the results in these experiments.", "labels": [], "entities": []}, {"text": "The tendencies for the wordbased embeddings word2vec and fastText are more or less identical before and after debiasing.", "labels": [], "entities": []}, {"text": "The most striking differences between Table 5 and are the results for ELMo and BERT, which become less balanced after applying the debiasing method.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.5258480310440063}, {"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9957746863365173}]}, {"text": "ELMo actually shows a clearer gender distinction after debiasing, with male names being more similar to male occupations, and female names being more similar to feamong all vectors and decreasing the influence of the gender specific direction.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9228034019470215}]}, {"text": "BERT also becomes less balanced after debiasing, grouping male names with female occupations, and female names with male occupations, when considering the average similarities.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9454593658447266}]}, {"text": "When counting only the most similar occupation per name, BERT is still well balanced after debiasing.", "labels": [], "entities": [{"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9975758194923401}]}, {"text": "The experiments in the previous sections are admittedly somewhat simplistic considering the scenario discussed in the Introduction: quantifying the similarity between a company name and a company description.", "labels": [], "entities": []}, {"text": "In particular the contextualized language models are not primarily designed for generating token embeddings, and it is neither clear what kind of quality we can expect from such un-contextualized token embeddings, nor whether they are susceptible to the debiasing operation discussed in Section 6.", "labels": [], "entities": []}, {"text": "In order to provide a more realistic scenario, we also include experiments where we compute the similarity between a set of actual company descriptions and a set of fictive company names generated from the lists of male and female names by adding the term \"Aktiebolag\" (in English limited company) after each name.", "labels": [], "entities": []}, {"text": "The company descriptions are provided by the Swedish Companies Registration Office, and contain approximately 10 company descriptions for each of the sectors construction work, vehicles and transportation, information technologies, health and healthcare, education, and economy.", "labels": [], "entities": [{"text": "Swedish Companies Registration Office", "start_pos": 45, "end_pos": 82, "type": "DATASET", "confidence": 0.8131542056798935}]}, {"text": "Based on, we consider the descriptions from the first three sectors to be representative of typically male occupations, and the descriptions from the latter three sectors to be representative 29 71 30: Percentage of female and male names that are on average more similar to a female vs. male occupation.", "labels": [], "entities": []}, {"text": "The similarities are calculated based on the original embeddings, using the names and occupations in context.", "labels": [], "entities": []}, {"text": "Numbers in parentheses only count the single most similar occupation for each name. of typically female occupations.", "labels": [], "entities": []}, {"text": "We generate vectors for each of the descriptions and for each fictive company name (i.e. a male or female name, followed by \"Aktiebolag\").", "labels": [], "entities": []}, {"text": "For the word-based models (word2vec and fastText), we take the average of the embeddings of the words in the descriptions and the name.", "labels": [], "entities": []}, {"text": "For the contextualized language models (ELMo and BERT), we generate vectors for each description and each fictive name.", "labels": [], "entities": [{"text": "BERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9904733300209045}]}, {"text": "In the case of ELMo we take the average over the three LSTM layers, and for BERT we use the output embedding for the token for each of the input sequences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9954853653907776}]}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "It is clear that these results are significantly more balanced than the results using tokens only.", "labels": [], "entities": []}, {"text": "Even so, there are still some interesting differences between the embeddings.", "labels": [], "entities": []}, {"text": "Contrary to the results in, word2vec now shows a bias for female occupations, and fastText now shows a bias for male occupations.", "labels": [], "entities": []}, {"text": "ELMo and BERT seem more balanced, with ELMo showing almost perfectly balanced results, and BERT showing a slight bias for female occupations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9975231289863586}, {"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9963756203651428}]}, {"text": "Even though the biases apparently are different when considering tokens in comparison with considering texts, there are still biases in all models in both cases.", "labels": [], "entities": []}, {"text": "The only exception in our experiments is ELMo, when used for texts instead of tokens.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8480001091957092}]}, {"text": "We hypothesize that the results for BERT are negatively affected by artefacts of the WordPiece tokenization, as discussed in Section 3.", "labels": [], "entities": [{"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9776805639266968}, {"text": "WordPiece", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.8828164339065552}]}], "tableCaptions": [{"text": " Table 2: The pre-trained embeddings and models included in these experiments were downloaded in  April 2019 from the following URLs. word2vec: vectors.nlpl.eu/repository/11/69.zip,  fastText: dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.sv.300.bin.gz,  ELMo: vectors.nlpl.eu/repository/11/173.zip, BERT: storage.googleapis.  com/bert models/2018 11 23/multi cased L-12 H-768 A-12.zip", "labels": [], "entities": [{"text": "BERT", "start_pos": 303, "end_pos": 307, "type": "METRIC", "confidence": 0.9957596659660339}]}, {"text": " Table 3: The 20 most common occupations for Swedish women in 2016 according to Statistics Sweden  (www.scb.se).", "labels": [], "entities": []}, {"text": " Table 8: The number of appropriate, stereotypical, and uncertain analogies in the top 150 pairs for the  original and debiased embeddings. The numbers are the analogy pairs for which the annotators agree on  the category.", "labels": [], "entities": []}]}