{"title": [], "abstractContent": [{"text": "In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the Ger-man chancellor, etc.).", "labels": [], "entities": [{"text": "information extraction for named entities", "start_pos": 41, "end_pos": 82, "type": "TASK", "confidence": 0.81666858792305}]}, {"text": "It is therefore important that coreference resolution systems are able to link these different types of mentions to the correct entity name.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9307148158550262}]}, {"text": "We evaluate state-of-the-art coreference resolution systems for the task of resolving all mentions to named entities.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.9125852882862091}]}, {"text": "Our analysis reveals that standard coreference metrics do not reflect adequately the requirements in this task: they do not penalize systems for not identifying any mentions by name to an entity and they reward systems even if systems find correctly mentions to the same entity but fail to link these to a proper name (she-the student-no name).", "labels": [], "entities": []}, {"text": "We introduce new metrics for evaluating named entity corefer-ence that address these discrepancies and show that for the comparisons of competitive systems , standard coreference evaluations could give misleading results for this task.", "labels": [], "entities": []}, {"text": "We are, however, able to confirm that the state-of-the art system according to traditional evaluations also performs vastly better than other systems on the named entity coreference task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution is the task of identifying all expressions in text that refer to the same entity.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9196842908859253}]}, {"text": "In this paper we set out to provide an in-depth analysis of the task specifically for named entities: finding all references-either by name, pronoun or nominal-to a named entity in the text.", "labels": [], "entities": [{"text": "finding all references-either by name, pronoun or nominal-to a named entity in the text", "start_pos": 102, "end_pos": 189, "type": "TASK", "confidence": 0.552196576197942}]}, {"text": "Many language technology tasks focus on entities and our work is oriented towards practical uses of the results of coreference resolution in downstream tasks.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.9177825450897217}]}, {"text": "Named entities are often targets for * equal contribution information extraction (), biography summarization () and knowledge base completion tasks).", "labels": [], "entities": [{"text": "equal contribution information extraction", "start_pos": 39, "end_pos": 80, "type": "TASK", "confidence": 0.6064838394522667}, {"text": "biography summarization", "start_pos": 85, "end_pos": 108, "type": "TASK", "confidence": 0.6610499024391174}, {"text": "knowledge base completion tasks", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.6739534065127373}]}, {"text": "More relevant information can be extracted for these tasks if we also know which pronouns and nominals refer to the entity.", "labels": [], "entities": []}, {"text": "Similarly, creation of proper noun ontologies ( can use patterns other than (proper noun-common noun) if other references to the entity are known.", "labels": [], "entities": []}, {"text": "Recent work has shown that standard coreference datasets are biased and high performance on these need not mean high performance in downstream tasks.", "labels": [], "entities": []}, {"text": "We argue that the standard coreference metrics are not suitable either from the perspective of downstream applications.", "labels": [], "entities": []}, {"text": "Since applications require information about entities and entities are usually identified by their names, the evaluation metrics should focus on the resolution of mentions to the correct name.", "labels": [], "entities": []}, {"text": "If all the pronouns referring to an entity are resolved correctly to each other but are not linked to any name or are linked to a wrong name, the results would not be useful for downstream tasks.", "labels": [], "entities": []}, {"text": "Standard coreference metrics do not incorporate these aspects and hence give high performance for results unsuitable for further use.", "labels": [], "entities": []}, {"text": "We also show that the existing metrics are not sensitive to finding any mention to an entity at all.", "labels": [], "entities": []}, {"text": "They give higher performance for systems that do not find a large number of entities but do good coreference resolution on the subset of entities they find.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 97, "end_pos": 119, "type": "TASK", "confidence": 0.7532583475112915}]}, {"text": "This problem of coreference chains without any named mentions being unsuitable has previously been discussed in).", "labels": [], "entities": [{"text": "coreference chains", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.8957229852676392}]}, {"text": "The authors argued that a name is more informative than a nominal, which is more informative than a pronoun so they assign different weights to co-reference links (mention-antecedent pairs) in a chain depending on the type of mentions the link contains.", "labels": [], "entities": []}, {"text": "They assign a higher weight to a link having a name than one that doesn't and also higher weight to a link having a nominal than a link that contains just pronouns.", "labels": [], "entities": []}, {"text": "Similarly,) perform an error analysis for co-reference by choosing an antecedent that is a name or a nominal in this order because they are more informative than a pronoun.", "labels": [], "entities": []}, {"text": "However, we argue that we should view the coreference chains as a whole instead of individual links when evaluating systems for downstream application.", "labels": [], "entities": []}, {"text": "If a chain contains even one named mention, it should be sufficient for using it in applications and we need not consider the mention type in each link within the chain.", "labels": [], "entities": []}, {"text": "We introduce metrics focused on Named Entity Coreference (NEC) which separate the identification of entities and resolution of different mention types, thus tackling the above issue and transparently tracking areas of system improvement.", "labels": [], "entities": []}], "datasetContent": [{"text": "Shared tasks on coreference ) use the average of three F1 scores as their official evaluation: MUC (, B 3 (Bagga and Baldwin, 1998) and CEAFE (.", "labels": [], "entities": [{"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9983230233192444}, {"text": "MUC", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.810741126537323}, {"text": "CEAFE", "start_pos": 136, "end_pos": 141, "type": "DATASET", "confidence": 0.661832869052887}]}, {"text": "Prior work () discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score.", "labels": [], "entities": [{"text": "link entity aware (LEA) score", "start_pos": 81, "end_pos": 110, "type": "METRIC", "confidence": 0.7011093880449023}]}, {"text": "Below we describe each score in the context of downstream tasks.", "labels": [], "entities": []}, {"text": "Let K be the set of key (gold) clusters, and let R be the set of response clusters.", "labels": [], "entities": []}, {"text": "MUC The recall for an entity is the minimum number of links that would have to be added in the predicted clusters containing any mention of this entity, to make them connected and part of the same cluster.", "labels": [], "entities": [{"text": "MUC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7864689826965332}, {"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9954624772071838}]}, {"text": "Precision is computed by reversing the role of gold and predicted clusters.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9642654657363892}]}, {"text": "where p(k i ) is the partition of k i generated by intersecting k i with the response entities.", "labels": [], "entities": []}, {"text": "Gold: {JohnDoe, he1, he2, he3} {RichardRoe, he4, he5} Solution 1: {JohnDoe, he1, he2} {RichardRoe, he4} Solution 2: {he1, he2, he3}{he4, he5} B-cubed B 3 works on the mention level.", "labels": [], "entities": [{"text": "B-cubed B", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9196611940860748}]}, {"text": "It iterates overall gold-standard mentions of an entity, averaging the recall of its gold cluster in its predicted cluster.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9994138479232788}]}, {"text": "It computes precision by reversing the role of gold and predicted clusters.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9993138313293457}]}, {"text": "CEAF CEAF first maps each gold cluster to a predicted cluster.", "labels": [], "entities": [{"text": "CEAF CEAF", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8928043842315674}]}, {"text": "It then computes recall as the number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9985384941101074}]}, {"text": "Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9952439665794373}]}, {"text": "Numbers are reported either per mention (CEAFm), or per entity (CEAFe).", "labels": [], "entities": []}, {"text": "where K * is the set of key entities in the optimal one-to-one mapping and \u03c6(\u00b7, \u00b7) is a similarity measure fora pair of entities.", "labels": [], "entities": [{"text": "\u03c6", "start_pos": 75, "end_pos": 76, "type": "METRIC", "confidence": 0.9616551399230957}]}, {"text": "In CEAFm, \u03c6(k i , r j ) = |k i \u2229 r j |, and in CEAFe, LEA Recall is computed as the fraction of correctly resolved links between mentions.", "labels": [], "entities": [{"text": "CEAFm", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.9317266345024109}, {"text": "\u03c6", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9637854099273682}, {"text": "CEAFe", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9507965445518494}, {"text": "LEA Recall", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.8644441366195679}]}, {"text": "Results for each entity are weighted by its number of mentions, so that resolving correctly an entity with more mentions contributes more to the overall score.", "labels": [], "entities": []}, {"text": "Precision is computed by reversing the role of gold and predicted clusters.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9642654657363892}]}, {"text": "where for any set S, link(S) denotes the number of links between elements of S (so link(S) = |S|\u00b7 (|S| \u2212 1)/2).", "labels": [], "entities": []}, {"text": "The goal of NEC is to link all mentions referring to a named entity to the correct name.", "labels": [], "entities": [{"text": "NEC", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.8730823993682861}]}, {"text": "There are two entities, each with one named mention and a few pronouns.", "labels": [], "entities": []}, {"text": "Both solutions find the same number of correct mentions pairs.", "labels": [], "entities": []}, {"text": "However, solution 1 has a named mention in each cluster but solution 2 has only pronouns.", "labels": [], "entities": []}, {"text": "Standard evaluations have the same values for both solutions (see) because they do not consider the types of mentions.", "labels": [], "entities": []}, {"text": "The above example highlights the potential deficiencies of standard coreference evaluations when applied to NEC.", "labels": [], "entities": []}, {"text": "Here we introduce a set of taskspecific criteria for the evaluation of NEC.", "labels": [], "entities": [{"text": "NEC", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8937242031097412}]}, {"text": "We make use of the relevant part of OntoNotes coreference corpus () and gold-standard annotations for named entities on the same data to quantify the patterns in coreference of different named entity types (see the table in the supplementary material) and to evaluate systems on the newswire, broadcast news and magazine documents for PER, ORG and GPE entities.", "labels": [], "entities": [{"text": "OntoNotes coreference corpus", "start_pos": 36, "end_pos": 64, "type": "DATASET", "confidence": 0.772959291934967}, {"text": "PER, ORG and GPE entities", "start_pos": 335, "end_pos": 360, "type": "DATASET", "confidence": 0.6259788622458776}]}, {"text": "Patterns in Coreference Named people, organizations and locations makeup 38% of all coreference clusters in OntoNotes (), yet 54% of all mentions that require coreference resolution are mentions of these types.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.833117812871933}]}, {"text": "All named entities are on average much less likely to be singletons than atypical entity, mentioned only once in the text and not requiring coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.8763141930103302}]}, {"text": "People, organizations and locations are most likely to be mentioned repeatedly: 68% of people, 51% of organizations and 52% of locations named in text have at least one other coreferent mention to them.", "labels": [], "entities": []}, {"text": "Named entities have a large portion of references that are not by name.", "labels": [], "entities": []}, {"text": "Nominals account for less than 5% of the mentions in all genres for PER, while the remaining mentions are split almost equally between names and pronouns.", "labels": [], "entities": [{"text": "PER", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.5001168847084045}]}, {"text": "For ORG, roughly half of the mentions are named, the remaining are equally split between pronouns and nominals.", "labels": [], "entities": [{"text": "ORG", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.794199526309967}]}, {"text": "For GPE, roughly 70% of the mentions are named and others are mostly pronouns.", "labels": [], "entities": [{"text": "GPE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6396101713180542}]}, {"text": "Systems We evaluate the Stanford coreference system, with its deterministic (, statistical and neural versions, and the neural end-to-end systems of ( and () on traditional and NEC metrics.", "labels": [], "entities": [{"text": "Stanford coreference system", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.8372622529665629}]}, {"text": "These general coreference systems find coreferring expressions of any type and produce coreference chains for all mentioned entities.", "labels": [], "entities": []}, {"text": "In NEC, the goal is to find all mentions to an entity that has been referred to by name at least once in the document.", "labels": [], "entities": []}, {"text": "The output of off-the-shelf coreference systems has to be filtered to keep only chains that contain at least one mention noun phrase with a syntactic head that is a entity's name.", "labels": [], "entities": []}, {"text": "For our evaluation, we use the spaCy dependency parsing system ( to detect whether a name is the head of a mention, by checking that no other word in the mention is an ancestor of the name in the dependency parse tree.", "labels": [], "entities": [{"text": "spaCy dependency parsing", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.7596015731493632}]}, {"text": "In evaluation, we use gold NER tags to determine if the head is a name.", "labels": [], "entities": []}, {"text": "Note that the dependency parsing and gold NER are not given to the systems but are used to process their output.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6891187429428101}, {"text": "gold NER", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8340855836868286}]}, {"text": "Many system NEC chains did not have any named mentions.", "labels": [], "entities": []}, {"text": "() does not have a named mention in about 30% of the coreference chains on PER that do contain a personal or possessive third person pronoun.", "labels": [], "entities": [{"text": "PER", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.823967695236206}]}, {"text": "This number is about 20% for the CoreNLP neural system.", "labels": [], "entities": [{"text": "CoreNLP neural system", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9232078989346822}]}, {"text": "shows the standard and NEC F1 on all the systems.", "labels": [], "entities": [{"text": "NEC", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.6017869114875793}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.699737012386322}]}, {"text": "For PER, there are three notable leaps of improvement according to the standard coref evaluation: between the statistical and rulebased CoreNLP systems, between their statistical and neural systems and between the two versions of the AllenNLP systems.", "labels": [], "entities": [{"text": "PER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6714615225791931}, {"text": "AllenNLP", "start_pos": 234, "end_pos": 242, "type": "DATASET", "confidence": 0.9789971113204956}]}, {"text": "Some of these improvements contradict actual performance on NEC, notably for the difference between the rule-based and statistical systems.", "labels": [], "entities": [{"text": "NEC", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.5969893932342529}]}, {"text": "The other two improvements in Coref F1 translate to improvements in NEC metrics.", "labels": [], "entities": []}, {"text": "The difference between the statistical and rule-based system is also falsely reflected in standard F1 for ORG and ORG entities.", "labels": [], "entities": [{"text": "F1", "start_pos": 99, "end_pos": 101, "type": "METRIC", "confidence": 0.9963650703430176}]}, {"text": "As expected, () outperforms all the systems, with () as a close second.", "labels": [], "entities": []}, {"text": "Both perform much better than ( and.,b) does slightly better than () on PER entities.", "labels": [], "entities": []}, {"text": "Notably, ( misses less than 10% of the chains for all entity types compared to 20-40% by other systems.", "labels": [], "entities": []}, {"text": "Note that the performance varies considerably across entity types.", "labels": [], "entities": []}, {"text": "A top NER system such as) that focus on PER, ORG and GPE does not find a single named entity in just 4.67%, 5.7% and 1.1% of chains respectively.", "labels": [], "entities": [{"text": "PER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.8046020269393921}, {"text": "ORG", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.968656599521637}, {"text": "GPE", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.7002809047698975}]}, {"text": "However, the percentage of chains not found is much higher.", "labels": [], "entities": []}, {"text": "It is possible that the non-named mentions were resolved to each other but not to any names so such chains got filtered out for the NEC task.", "labels": [], "entities": [{"text": "NEC task", "start_pos": 132, "end_pos": 140, "type": "TASK", "confidence": 0.80628901720047}]}, {"text": "Future work involves developing coreference systems driven by NER and producing results more suitable for downstream tasks.", "labels": [], "entities": []}, {"text": "We also separate the performance of the systems by mention type.", "labels": [], "entities": []}, {"text": "The second panel of Table 3 reveals that () outperforms all the systems on each mention type for all the three types of entities.", "labels": [], "entities": []}, {"text": "Detection of named mentions can be done with high accuracy by named entity recognition systems () and the matching of names can also be done accurately via string matching (.", "labels": [], "entities": [{"text": "Detection of named mentions", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9008562117815018}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9940767288208008}, {"text": "named entity recognition", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6983327269554138}]}, {"text": "In spite of this, most systems do not perform well on names.", "labels": [], "entities": []}, {"text": "The mistakes on pronouns and nominals are much higher as expected.", "labels": [], "entities": []}, {"text": "While () gets a better F1 on the standard coreference metrics used as well, it improves on many aspects of performance.", "labels": [], "entities": [{"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.9997087121009827}]}, {"text": "It finds more chains and even performs better resolution of each mention type, making it more suitable for downstream tasks.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of the hypothetical examples in Ta- ble 1. NEC is the new metric introduced in Section 3.", "labels": [], "entities": [{"text": "Ta- ble 1", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.7927650213241577}, {"text": "NEC", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.4437977373600006}]}, {"text": " Table 3: Performance of systems. Chains not found and NEC F1 refer to the new named entity focused metrics.  Coref F1 refers to the evaluation combining MUC, B 3 and CEAFE, on test data.", "labels": [], "entities": [{"text": "NEC F1", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.622162789106369}, {"text": "F1", "start_pos": 116, "end_pos": 118, "type": "METRIC", "confidence": 0.6532314419746399}, {"text": "CEAFE", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.657918393611908}]}, {"text": " Table 4: NEC F1 by type of mention. The errors on names are high, though it is possible to resolve these with  NER and string matching or similarity. Pronoun errors are high as expected.", "labels": [], "entities": [{"text": "NEC", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.8550512194633484}, {"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.5370573997497559}]}]}