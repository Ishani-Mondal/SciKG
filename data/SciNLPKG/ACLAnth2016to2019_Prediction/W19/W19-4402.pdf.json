{"title": [{"text": "Predicting the Difficulty of Multiple Choice Questions in a High-stakes Medical Exam", "labels": [], "entities": []}], "abstractContent": [{"text": "Predicting the construct-relevant difficulty of Multiple-Choice Questions (MCQs) has the potential to reduce cost while maintaining the quality of high-stakes exams.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for estimating the difficulty of MCQs from a high-stakes medical exam, where all questions were deliberately written to a common reading level.", "labels": [], "entities": [{"text": "MCQs", "start_pos": 68, "end_pos": 72, "type": "TASK", "confidence": 0.9379724264144897}]}, {"text": "To accomplish this, we extract a large number of linguistic features and embedding types, as well as features quantifying the difficulty of the items for an automatic question-answering system.", "labels": [], "entities": []}, {"text": "The results show that the proposed approach outperforms various baselines with a statistically significant difference.", "labels": [], "entities": []}, {"text": "Best results were achieved when using the full feature set, where embeddings had the highest predictive power, followed by linguistic features.", "labels": [], "entities": []}, {"text": "An ablation study of the various types of linguistic features suggested that information from all levels of linguistic processing contributes to predicting item difficulty, with features related to semantic ambiguity and the psycholinguistic properties of words having a slightly higher importance.", "labels": [], "entities": [{"text": "predicting item difficulty", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.7838615576426188}]}, {"text": "Owing to its generic nature, the presented approach has the potential to generalize over other exams containing MCQs.", "labels": [], "entities": []}], "introductionContent": [{"text": "For many years, approaches from Natural Language Processing (NLP) have been applied to estimating reading difficulty, but relatively fewer attempts have been made to measure conceptual difficulty or question difficulty beyond linguistic complexity.", "labels": [], "entities": []}, {"text": "In addition to expanding the horizons of NLP research, estimating the constructrelevant difficulty of test questions has a high practical value because ensuring that exam questions are appropriately difficult is both one of the most important and one of the most costly tasks within the testing industry.", "labels": [], "entities": []}, {"text": "For example, test questions that are too easy or too difficult are less able to distinguish between different levels of examinee ability (or between examinee ability and a defined cut-score of some kind -e.g., pass/fail).", "labels": [], "entities": [{"text": "pass", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9530720710754395}]}, {"text": "This is especially important when scores are used to make consequential decisions such as those for licensure, certification, college admission, and other high-stakes applications . To address these issues, we propose a method for predicting the difficulty of multiple choice questions (MCQs) from a highstakes medical licensure exam, where questions of varying difficulty may not necessarily vary in terms of reading levels.", "labels": [], "entities": [{"text": "predicting the difficulty of multiple choice questions (MCQs)", "start_pos": 231, "end_pos": 292, "type": "TASK", "confidence": 0.6100899755954743}]}, {"text": "Owing to the criticality of obtaining difficulty estimates for items (exam questions) prior to their use for scoring, current best practices require newly-developed items to be pretested.", "labels": [], "entities": []}, {"text": "Pretesting typically involves administering new items to a representative sample of examinees (usually between a few hundred and a few thousand), and then using their responses to estimate various statistical characteristics.", "labels": [], "entities": [{"text": "Pretesting", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.742393970489502}]}, {"text": "Ideally, pretest data are collected by embedding new items within a standard live exam, although sometimes special data collection efforts may also be needed.", "labels": [], "entities": []}, {"text": "Based on the responses, items that are answered correctly by a proportion of examinees below or above certain thresholds (i.e. items that are too easy or too difficult for almost all examinees) are discarded.", "labels": [], "entities": []}, {"text": "While necessary, this procedure has a high financial and administrative cost, in addition to the time required to obtain the data from a sufficiently large sample of examinees.", "labels": [], "entities": []}, {"text": "Here, we propose an approach for estimating the difficulty of expert-level MCQs, where the A 55-year-old woman with small cell carcinoma of the lung is admitted to the hospital to undergo chemotherapy.", "labels": [], "entities": [{"text": "MCQs", "start_pos": 75, "end_pos": 79, "type": "TASK", "confidence": 0.7204195261001587}]}, {"text": "Six days after treatment is started, she develops a temperature of 38C (100.4F).", "labels": [], "entities": []}, {"text": "Physical examination shows no other abnormalities.", "labels": [], "entities": []}, {"text": "Laboratory studies show a leukocyte count of 100/mm3 (5% segmented neutrophils and 95% lymphocytes).", "labels": [], "entities": []}, {"text": "Which of the following is the most appropriate pharmacotherapy to increase this patient's leukocyte count?: An example of a practice item gold standard of item difficulty is defined through large-scale pretesting and is based on the responses of hundreds of highly-motivated examinees.", "labels": [], "entities": []}, {"text": "Being able to automatically predict item difficulty from item text has the potential to save significant resources by eliminating or reducing the need to pretest the items.", "labels": [], "entities": []}, {"text": "These savings are of even greater importance in the context of some automatic item generation strategies, which can produce tens of thousands of items with no feasible way to pretest them or identify which items are most likely to succeed.", "labels": [], "entities": [{"text": "item generation", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.748753547668457}]}, {"text": "Furthermore, understanding what makes an item difficult other than manipulating its reading difficulty has the potential to aid the item-writing process and improve the quality of the exam.", "labels": [], "entities": []}, {"text": "Last but not least, automatic difficulty prediction is relevant to automatic item generation as an evaluation measure of the quality of the produced output.", "labels": [], "entities": [{"text": "automatic difficulty prediction", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.5859295924504598}, {"text": "automatic item generation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6115975677967072}]}, {"text": "Contributions i) We develop and test the predictive power of a large number of different types of features (e.g. embeddings and linguistic features), including innovative metrics that measure the difficulty of MCQs for an automatic questionanswering system.", "labels": [], "entities": []}, {"text": "The latter produced empirical evidence on whether parallels exist between question difficulty for humans and machines.", "labels": [], "entities": [{"text": "question difficulty", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.6927895992994308}]}, {"text": "ii) The results outperform a number of baselines, showing that the proposed approach measures a notion of difficulty that goes beyond linguistic complexity.", "labels": [], "entities": []}, {"text": "iii) We analyze the most common errors produced by the models, as well as the most important features, providing insight into the effects that various item characteristics have on the success of predicting item difficulty.", "labels": [], "entities": [{"text": "predicting item difficulty", "start_pos": 195, "end_pos": 221, "type": "TASK", "confidence": 0.8251194953918457}]}, {"text": "iv) Owing to the generic nature of the features, the presented approach is potentially generalizable to other MCQbased exams.", "labels": [], "entities": []}, {"text": "We make our code available 2 at: https://bit.ly/2EaTFNN.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experiments on predicting the P-value.", "labels": [], "entities": [{"text": "P-value", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.8685472011566162}]}, {"text": "First, we randomly divide the full data set into training (60%), validation (20%) and test (20%) sets for the purpose of evaluating a number of different algorithms 9 on the validation set.", "labels": [], "entities": []}, {"text": "This was done using all features.", "labels": [], "entities": []}, {"text": "The most notable results on algorithm selection are presented in.", "labels": [], "entities": [{"text": "algorithm selection", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.713137149810791}]}, {"text": "As can be seen from the table, the best results are obtained using the Random Forests (RF) algorithm, which was selected for use in subsequent experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Baseline results using 10-fold cross validation on the training set and evaluating the models on the test set  (r = correlation coefficient, MAE = Mean Absolute Error, RMSE = Root Mean Squared Error).", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 126, "end_pos": 149, "type": "METRIC", "confidence": 0.9634131789207458}, {"text": "MAE = Mean Absolute Error", "start_pos": 151, "end_pos": 176, "type": "METRIC", "confidence": 0.8402686715126038}, {"text": "RMSE = Root Mean Squared Error", "start_pos": 178, "end_pos": 208, "type": "METRIC", "confidence": 0.8335362474123637}]}, {"text": " Table 4: Results for the training (10-fold CV) and test sets for various feature combinations.", "labels": [], "entities": []}, {"text": " Table 5: Changes in RMSE after removing individual  feature classes", "labels": [], "entities": [{"text": "RMSE", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8451300859451294}]}]}