{"title": [{"text": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets", "labels": [], "entities": [{"text": "Transfer Learning in Biomedical Natural Language Processing", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7422707378864288}, {"text": "BERT", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9965618252754211}, {"text": "ELMo", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.8741832971572876}]}], "abstractContent": [{"text": "Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain.", "labels": [], "entities": [{"text": "Biomedical Language Understanding Evaluation (BLUE)", "start_pos": 101, "end_pos": 152, "type": "METRIC", "confidence": 0.5618632776396615}]}, {"text": "The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties.", "labels": [], "entities": []}, {"text": "We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results.", "labels": [], "entities": [{"text": "BERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9928312301635742}, {"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9941697120666504}]}, {"text": "We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark.", "labels": [], "entities": [{"text": "BLUE", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9684370756149292}]}], "introductionContent": [{"text": "With the growing amount of biomedical information available in textual form, there have been significant advances in the development of pretraining language representations that can be applied to a range of different tasks in the biomedical domain, such as pre-trained word embeddings, sentence embeddings, and contextual representations (.", "labels": [], "entities": []}, {"text": "In the general domain, we have recently observed that the General Language Understanding Evaluation (GLUE) benchmark () has been successfully promoting the development of language representations of general purpose (.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, however, there is no publicly available benchmarking in the biomedicine domain.", "labels": [], "entities": []}, {"text": "To facilitate research on language representations in the biomedicine domain, we present the Biomedical Language Understanding Evaluation (BLUE) benchmark, which consists of five different biomedicine text-mining tasks with ten corpora.", "labels": [], "entities": [{"text": "Biomedical Language Understanding Evaluation (BLUE)", "start_pos": 93, "end_pos": 144, "type": "METRIC", "confidence": 0.5972086616924831}]}, {"text": "Here, we rely on preexisting datasets because they have been widely used by the BioNLP community as shared tasks.", "labels": [], "entities": []}, {"text": "These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges.", "labels": [], "entities": []}, {"text": "We expect that the models that perform better on all or most tasks in BLUE will address other biomedicine tasks more robustly.", "labels": [], "entities": [{"text": "BLUE", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.6702308058738708}]}, {"text": "To better understand the challenge posed by BLUE, we conduct experiments with two baselines: One makes use of the BERT model) and one makes use of.", "labels": [], "entities": [{"text": "BLUE", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.6029874086380005}, {"text": "BERT", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9905571937561035}]}, {"text": "Both are state-of-the-art language representation models and demonstrate promising results in NLP tasks of general purpose.", "labels": [], "entities": []}, {"text": "We find that the BERT model pre-trained on PubMed abstracts ( and MIMIC-III clinical notes) achieves the best results, and is significantly superior to other models in the clinical domain.", "labels": [], "entities": [{"text": "BERT", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.989248514175415}, {"text": "PubMed abstracts", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9381954669952393}, {"text": "MIMIC-III clinical notes", "start_pos": 66, "end_pos": 90, "type": "DATASET", "confidence": 0.7911702195803324}]}, {"text": "This demonstrates the importance of pre-training among different text genres.", "labels": [], "entities": []}, {"text": "In summary, we offer: (i) five tasks with ten biomedical and clinical text-mining corpora with different sizes and levels of difficulty, (ii) codes for data construction and model evaluation for fair comparisons, (iii) pretrained BERT models on PubMed abstracts and MIMIC-III, and (iv) baseline results.", "labels": [], "entities": [{"text": "data construction", "start_pos": 152, "end_pos": 169, "type": "TASK", "confidence": 0.7190851271152496}, {"text": "BERT", "start_pos": 230, "end_pos": 234, "type": "METRIC", "confidence": 0.896652102470398}, {"text": "MIMIC-III", "start_pos": 266, "end_pos": 275, "type": "DATASET", "confidence": 0.6737805604934692}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Baseline performance on the BLUE task test sets.", "labels": [], "entities": [{"text": "BLUE task test sets", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.8569653183221817}]}]}