{"title": [{"text": "Building a Production Model for Retrieval-Based Chatbots", "labels": [], "entities": []}], "abstractContent": [{"text": "Response suggestion is an important task for building human-computer conversation systems.", "labels": [], "entities": [{"text": "Response suggestion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9049104154109955}]}, {"text": "Recent approaches to conversation mod-eling have introduced new model architectures with impressive results, but relatively little attention has been paid to whether these models would be practical in a production setting.", "labels": [], "entities": []}, {"text": "In this paper, we describe the unique challenges of building a production retrieval-based conversation system, which selects outputs from a whitelist of candidate responses.", "labels": [], "entities": []}, {"text": "To address these challenges, we propose a dual encoder architecture which performs rapid inference and scales well with the size of the whitelist.", "labels": [], "entities": []}, {"text": "We also introduce and compare two methods for generating whitelists, and we carryout a comprehensive analysis of the model and whitelists.", "labels": [], "entities": []}, {"text": "Experimental results on a large, proprietary help desk chat dataset, including both offline metrics and a human evaluation, indicate production-quality performance and illustrate key lessons about conversation mod-eling in practice.", "labels": [], "entities": []}], "introductionContent": [{"text": "Predicting a response given conversational context is a critical task for building open-domain chatbots and dialogue systems.", "labels": [], "entities": [{"text": "Predicting a response given conversational context", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8284559647242228}]}, {"text": "Recently developed conversational systems typically use either a generative or a retrieval approach for producing responses (.", "labels": [], "entities": []}, {"text": "While both of these approaches have demonstrated strong performance in the literature, retrieval methods often enjoy better control over response quality than generative approaches.", "labels": [], "entities": []}, {"text": "In particular, such methods select outputs from a whitelist of candidate responses, which can be pre-screened and revised \u2020 Work done primarily while an intern at ASAPP, Inc.", "labels": [], "entities": []}, {"text": "for desired qualities such as sentence fluency and diversity.", "labels": [], "entities": []}, {"text": "Most previous work on retrieval models has concentrated on designing neural architectures to improve response selection.", "labels": [], "entities": []}, {"text": "For instance, several works have improved model performance by encoding multi-turn conversation context instead of single-turn context).", "labels": [], "entities": []}, {"text": "More recent efforts ( have explored using more advanced architectures, such as the Transformer (), to better learn the mapping between the context and the candidate responses.", "labels": [], "entities": []}, {"text": "Relatively little effort, however, has been devoted to the practical considerations of using such models in a real-world production setting.", "labels": [], "entities": []}, {"text": "For example, one critical consideration rarely discussed in the literature is the inference speed of the deployed model.", "labels": [], "entities": []}, {"text": "While recent methods introduce rich computation, such as cross-attention (, to improve the modeling between the conversational context and candidate response, the model outputs must be re-computed for every pair of context and response.", "labels": [], "entities": []}, {"text": "As a consequence, these models are not well-suited to a production setting where the size of the response whitelist can easily extend into the thousands.", "labels": [], "entities": []}, {"text": "Another critical concern is the whitelist selection process and the associated retrieval evaluation.", "labels": [], "entities": [{"text": "whitelist selection process", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.8149466117223104}]}, {"text": "Most prior work have reported Recall@k on a small set of randomly selected responses which include the true response sent by the agent ().", "labels": [], "entities": []}, {"text": "However, this oversimplified evaluation may not provide a useful indication of performance in production, where the whitelist is not randomly selected, is significantly larger, and may not contain the target response.", "labels": [], "entities": []}, {"text": "In this paper, we explore and evaluate model and whitelist design choices for building retrievalbased conversation systems in production.", "labels": [], "entities": []}, {"text": "We present a dual encoder architecture that is optimized to select among as many as 10,000 responses within a couple tens of milliseconds.", "labels": [], "entities": []}, {"text": "The model makes use of a fast recurrent network implementation () and multiheaded attention ( ) and achieves over a 4.1x inference speedup over traditional encoders such as LSTM (Hochreiter and Schmidhuber, 1997).", "labels": [], "entities": []}, {"text": "The independent dual encoding allows pre-computing the embeddings of candidate responses, thereby making the approach highly scalable with the size of the whitelist.", "labels": [], "entities": []}, {"text": "In addition, we compare two approaches for generating the response candidates, and we conduct a comprehensive analysis of our model and whitelists on a large, real-world help desk dataset, using human evaluation and metrics that are more relevant to use in a production setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our model and whitelists on a large, proprietary help desk chat dataset using several offline metrics and a human evaluation.", "labels": [], "entities": [{"text": "help desk chat dataset", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.6001031622290611}]}, {"text": "We particularly emphasize metrics relevant to production, such as inference speed and Recall@k from a large candidate set.", "labels": [], "entities": [{"text": "Recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9296137690544128}]}, {"text": "The human evaluation illustrates how our model and whitelists compare to each other and to the responses sent by areal human agent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary statistics for the propriety help desk dataset.", "labels": [], "entities": [{"text": "propriety help desk dataset", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.8784159868955612}]}, {"text": " Table 3: AUC and AUC@p of our model on the propri- ety help desk dataset.", "labels": [], "entities": [{"text": "AUC", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5805672407150269}, {"text": "propri- ety help desk dataset", "start_pos": 44, "end_pos": 73, "type": "DATASET", "confidence": 0.9230542778968811}]}, {"text": " Table 4: Recall@k from n response candidates for dif- ferent values of n using random whitelists. Each ran- dom whitelist includes the correct response along with  n \u2212 1 randomly selected responses.", "labels": [], "entities": []}, {"text": " Table 5: Recall@k for random, frequency, and cluster- ing whitelists of different sizes. The \"+\" indicates that  the true response is added to the whitelist.", "labels": [], "entities": []}, {"text": " Table 6: Recall@1 versus coverage for frequency and  clustering whitelists.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9935205578804016}, {"text": "coverage", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9520599842071533}]}, {"text": " Table 8: Inference time (milliseconds) of our model to  encode a context using an SRU or an LSTM encoder on  a single CPU core. The last row shows the extra time  needed to compare the response encoding to 10,000  cached candidate response encodings in order to find  the best response.", "labels": [], "entities": [{"text": "Inference time", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9403347373008728}]}, {"text": " Table 9: An ablation study showing the effect of different model architectures and training regimes on performance  on the proprietary help desk dataset.", "labels": [], "entities": []}]}