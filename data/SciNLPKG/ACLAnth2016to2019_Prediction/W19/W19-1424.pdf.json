{"title": [{"text": "Comparing Pipelined and Integrated Approaches to Dialectal Arabic Neural Machine Translation", "labels": [], "entities": [{"text": "Dialectal Arabic Neural Machine Translation", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.7474484324455262}]}], "abstractContent": [{"text": "When translating diglossic languages such as Arabic, situations may arise where we would like to translate a text but do not know which dialect it is.", "labels": [], "entities": []}, {"text": "A traditional approach to this problem is to design dialect identification systems and dialect-specific machine translation systems.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7360982000827789}, {"text": "dialect-specific machine translation", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.6448361078898112}]}, {"text": "However, under the recent paradigm of neural machine translation, shared multi-dialectal systems have become a natural alternative.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.7479451894760132}]}, {"text": "Here we explore under which conditions it is beneficial to perform dialect identification for Arabic neural machine translation versus using a general system for all dialects.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 67, "end_pos": 89, "type": "TASK", "confidence": 0.7375341355800629}, {"text": "Arabic neural machine translation", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.5560222268104553}]}], "introductionContent": [{"text": "Arabic exhibits a linguistic phenomenon called diglossia-speakers use Modern Standard Arabic (MSA) for formal settings and local dialects for informal settings.", "labels": [], "entities": []}, {"text": "There are broad categories of dialects by region, such as Levantine or Maghrebi.", "labels": [], "entities": []}, {"text": "However, dialects also vary at a finer-grained level, even within individual countries.", "labels": [], "entities": []}, {"text": "An additional complication is that code-switching, i.e. mixing MSA and dialect, is a common occurrence ( ).", "labels": [], "entities": []}, {"text": "To put the importance of handling Arabic dialects in perspective, Ethnologue lists Arabic as having the 5th highest number of L1 speakers, spread over 21 regional dialects.", "labels": [], "entities": [{"text": "Ethnologue", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9090210199356079}]}, {"text": "The bulk of work on translating Arabic dialects uses rule-based and statistical machine translation, and much of it is translating between dialects and MSA.", "labels": [], "entities": [{"text": "translating Arabic dialects", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.8830123941103617}, {"text": "statistical machine translation", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.6858084003130595}]}, {"text": "Generally, this work builds systems for specific dialects, with substantial amounts of information about the dialects themselves builtin.", "labels": [], "entities": []}, {"text": "In the meantime, neural machine translation has become the dominant paradigm, and with it multi-1 https://www.ethnologue.com/statistics/size lingual systems have become a more natural possibility ().", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6710736652215322}]}, {"text": "These systems know nothing about the specific languages involved, but use shared embedding spaces and parameters to yield benefits especially with lower-resource languages.", "labels": [], "entities": []}, {"text": "It is a natural extension to apply this to the space of Arabic dialects.", "labels": [], "entities": []}, {"text": "There are many possibilities of what exactly a multilingual system might look like, but we focus on one particular decision: Suppose we want to be able to translate a test sentence from an unknown dialect.", "labels": [], "entities": []}, {"text": "Is it better to perform dialect identification and then translate with a finely tuned system for that dialect (i.e. a pipelined approach)?", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7262986898422241}]}, {"text": "Or is it better to throw everything into one integrated, multilingual system 2 which we use for all input regardless of dialect?", "labels": [], "entities": []}, {"text": "And how accurate does our dialect identification have to be for the pipeline approach to be useful?", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7768705487251282}]}, {"text": "We perform a set of exploratory experiments quantifying this trade-off on LDC data consisting of MSA, Levantine, and Egyptian bitexts, using a standard Transformer architecture (.", "labels": [], "entities": []}, {"text": "The experimental setup is illustrated in Figure 1 and described in detail in Section 4.", "labels": [], "entities": []}, {"text": "To explore the effect of quality of dialect identification, we perform a set of artificial experiments where we add increasing amounts of random noise to reduce language identification accuracy.", "labels": [], "entities": [{"text": "dialect identification", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.707098051905632}, {"text": "language identification", "start_pos": 161, "end_pos": 184, "type": "TASK", "confidence": 0.7213035374879837}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.7971053719520569}]}, {"text": "Our results show that in some scenarios, depending on the language identification accuracy, there is a cross-over point where the pipelined approach outperforms the integrated, multilingual approach in terms of BLEU scores, and vice versa.", "labels": [], "entities": [{"text": "language identification", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.6856947690248489}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.6675657033920288}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9979590177536011}]}, {"text": "We then propose avenues for future work in this direction, based on our initial observations.", "labels": [], "entities": []}, {"text": "Here, a test sentence of unknown dialect either gets run through a pipeline, where it is classified as Egyptian and then run through an Egyptian-tuned system, or is run through an integrated, multidialectal system.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform experiments comparing multidialectal and dialect-tuned approaches, and then focus on the effect of misclassified dialects with a set of experiments adding synthetic noise to our language classification.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Multidialectal and dialect-tuned approaches for different BPE sizes. In this experiment, we assume the  dialect of the test sentences are known so that the correct Dialect-Tuned models can be applied.", "labels": [], "entities": []}, {"text": " Table 3: How each model performs on each test set.", "labels": [], "entities": []}, {"text": " Table 4: How well the pipelined approach does with langid.py as dialect ID.", "labels": [], "entities": []}]}