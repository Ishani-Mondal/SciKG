{"title": [{"text": "Pentagon at MEDIQA 2019: Multi-task Learning for Filtering and Re-ranking Answers using Language Inference and Question Entailment", "labels": [], "entities": [{"text": "MEDIQA", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8316019773483276}, {"text": "Re-ranking Answers", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.675209254026413}, {"text": "Question Entailment", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.6631161868572235}]}], "abstractContent": [{"text": "Parallel deep learning architectures like fine-tuned BERT and MT-DNN, have quickly become the state of the art, bypassing previous deep and shallow learning methods by a large margin.", "labels": [], "entities": [{"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9873510003089905}]}, {"text": "More recently, pre-trained models from large related datasets have been able to perform well on many downstream tasks by just fine-tuning on domain-specific datasets (similar to transfer learning).", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.8813511431217194}]}, {"text": "However, using powerful models on non-trivial tasks, such as ranking and large document classification, still remains a challenge due to input size limitations 1 of parallel architecture and extremely small datasets (insuffi-cient for fine-tuning).", "labels": [], "entities": [{"text": "large document classification", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6070708135763804}]}, {"text": "In this work, we introduce an end-to-end system , trained in a multi-task setting, to filter and re-rank answers in medical domain.", "labels": [], "entities": []}, {"text": "We use task-specific pre-trained models as deep feature extractors.", "labels": [], "entities": [{"text": "deep feature extractors", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.676179567972819}]}, {"text": "Our model achieves the highest Spearman's Rho and Mean Reciprocal Rank of 0.338 and 0.9622 respectively, on the ACL-BioNLP workshop MediQA Question Answering shared-task.", "labels": [], "entities": [{"text": "Spearman's Rho and Mean Reciprocal Rank", "start_pos": 31, "end_pos": 70, "type": "METRIC", "confidence": 0.8250936950956073}, {"text": "MediQA Question Answering shared-task", "start_pos": 132, "end_pos": 169, "type": "TASK", "confidence": 0.7902978807687759}]}], "introductionContent": [{"text": "In this work, we study the problem of re-ranking and filtering in medical domain Information Retrieval (IR) systems.", "labels": [], "entities": [{"text": "re-ranking and filtering in medical domain Information Retrieval (IR)", "start_pos": 38, "end_pos": 107, "type": "TASK", "confidence": 0.7260492769154635}]}, {"text": "Historically, re-ranking is generally treated as a 'Learning to Rank' problem while filtering is posed as a 'Binary Classification' problem.", "labels": [], "entities": []}, {"text": "Traditional methods have used handcrafted features to train such systems.", "labels": [], "entities": []}, {"text": "However, recently deep learning methods have gained * * Equal contribution, randomly sorted.", "labels": [], "entities": []}, {"text": "Karan and Shefali took ownership of the NLI module while Sheetal and Prashant worked on the RQE module.", "labels": [], "entities": [{"text": "NLI module", "start_pos": 40, "end_pos": 50, "type": "DATASET", "confidence": 0.8181990683078766}, {"text": "RQE module", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.8400754332542419}]}, {"text": "Hemant researched and implemented the Question-Answering system including baseline and multi-task learning.", "labels": [], "entities": []}, {"text": "Sheetal and Hemant worked on scraping data from icliniq.", "labels": [], "entities": [{"text": "icliniq", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9017001986503601}]}, {"text": "Karan and Prashant helped with integration of NLI and RQE module respectively into the multi-task system.", "labels": [], "entities": []}, {"text": "1 https://github.com/google-research/bert/issues/27 popularity in the Information retrieval (IR) domain.", "labels": [], "entities": [{"text": "popularity", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9294776916503906}, {"text": "Information retrieval (IR)", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.7615715265274048}]}, {"text": "The ACL-BioNLP workshop MediQA shared task ) aims to develop relevant techniques for inference and entailment in medical domain to improve domain specific IR and QA systems.", "labels": [], "entities": [{"text": "ACL-BioNLP workshop MediQA shared task", "start_pos": 4, "end_pos": 42, "type": "DATASET", "confidence": 0.8667594432830811}]}, {"text": "The challenge consists of three tasks which are evaluated separately.", "labels": [], "entities": []}, {"text": "The first task is the Natural Language Inference (NLI) task which focuses on determining whether a natural language hypothesis can be inferred from a natural language premise.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI) task", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.7969022818974086}]}, {"text": "The second task is to recognize question entailment (RQE) between a pair of questions.", "labels": [], "entities": [{"text": "recognize question entailment (RQE) between a pair of questions", "start_pos": 22, "end_pos": 85, "type": "TASK", "confidence": 0.8409359834410928}]}, {"text": "The third task is to filter and improve the ranking of automatically retrieved answers.", "labels": [], "entities": []}, {"text": "For the NLI and RQE tasks, we use transfer learning on prevalent pre-trained models like BERT ( and MT-DNN ( ).", "labels": [], "entities": [{"text": "BERT", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9904412031173706}]}, {"text": "These models play a pivotal role to gain deeper semantic understanding of the content for the final task (filtering and re-ranking) of the challenge.", "labels": [], "entities": []}, {"text": "Besides using usual techniques for candidate answer selection and re-ranking, we use features obtained from NLI and RQE models.", "labels": [], "entities": [{"text": "candidate answer selection", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6557488640149435}]}, {"text": "We majorly concentrate on the novel multi-task approach in this paper.", "labels": [], "entities": []}, {"text": "We also succinctly describe our NLI and RQE models and their performance on the final leaderboard.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset for re-ranking and filtering has been provided by the MediQA Shared task  For the answer classification task, answers with scores 1 and 2 are considered as incorrect (label 0), and answers with scores 3 and 4 are considered as correct (label 1).", "labels": [], "entities": [{"text": "MediQA Shared task", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.8245773315429688}, {"text": "answer classification task", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.9025142391522726}]}, {"text": "The evaluation metrics for filtering task is Accuracy and Precision while metrics for re-ranking task is Mean Reciprocal Rank (MRR) and Spearman's Rank Correlation Coefficient.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9991809725761414}, {"text": "Precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9807419776916504}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 105, "end_pos": 131, "type": "METRIC", "confidence": 0.9600967963536581}]}, {"text": "To train the Natural Language Inference and Question Entailment module of our system we again use the data from MediQA shared task   ).", "labels": [], "entities": [{"text": "Question Entailment", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7191835939884186}, {"text": "MediQA shared task", "start_pos": 112, "end_pos": 130, "type": "DATASET", "confidence": 0.8484143018722534}]}, {"text": "We are provided labels for whether FAQ entails CHQ or not.", "labels": [], "entities": [{"text": "FAQ entails CHQ", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.5565400322278341}]}, {"text": "The RQE training dataset consists of 8,588 medical question pairs.", "labels": [], "entities": [{"text": "RQE training dataset", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.7366017699241638}]}, {"text": "The validation set comprises of 302 pairs.", "labels": [], "entities": []}, {"text": "The evaluation metric used for RQE is accuracy.", "labels": [], "entities": [{"text": "RQE", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9208405613899231}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9991081357002258}]}, {"text": "We also augment the data from a popular medical expert answering website called 3 icliniq.", "labels": [], "entities": []}, {"text": "It is a forum where users can delineate their medical issues, which are then paraphrased as short queries by medical experts.", "labels": [], "entities": []}, {"text": "The user queries are treated as CHQs whereas the paraphrased queries are treated as FAQs.", "labels": [], "entities": [{"text": "FAQs", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.8798818588256836}]}, {"text": "We extract 9,958 positive examples and generate an equal number of negative examples by random sampling.", "labels": [], "entities": []}, {"text": "The average CHQ length is 180 tokens whereas the average FAQ length is 11 tokens.", "labels": [], "entities": [{"text": "CHQ length", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.6397784799337387}, {"text": "FAQ length", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9761140048503876}]}, {"text": "In addition, the expert answers are used to augment the MediQUAD corpus (Ben Abacha and Demner-Fushman, 2019).", "labels": [], "entities": [{"text": "MediQUAD corpus", "start_pos": 56, "end_pos": 71, "type": "DATASET", "confidence": 0.8829406797885895}]}, {"text": "For this task we perform multiple experiments on feature-engineered system in Section 4.4 to asses the usefulness of the designed features.", "labels": [], "entities": []}, {"text": "These experiments later help us incorporate these features into Metadata Embedding defined in Section 4.5.", "labels": [], "entities": []}, {"text": "Firstly we run the experiments on Metedata features, BoW, Coarse-grained RQE and NLI scores.", "labels": [], "entities": [{"text": "BoW", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.5006767511367798}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We later incorporate the RQE embeddings from RQE system and the results are shown in.", "labels": [], "entities": []}, {"text": "Here we evaluate the system with different number of RQE candidates at different threshold settings.", "labels": [], "entities": []}, {"text": "Previous experiments were conducted on the filtering task only . For ranking task we train SVM-Rank) based systems to learn pair-wise ranking, using the same features as the filtering task.", "labels": [], "entities": []}, {"text": "Experiments with SVM-Rank) were performed with N=3 RQE candidates and the results are shown in.", "labels": [], "entities": [{"text": "SVM-Rank", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.7793126702308655}]}, {"text": "Moving to jointly learning system introduced in Section 4.5, we train it with different parameter settings.", "labels": [], "entities": []}, {"text": "Due to lack of resources, we could evaluate only a few hyperparameter settings where N is the most number of RQE candidates considered while training and T is the the threshold for retrieving the candidates.", "labels": [], "entities": []}, {"text": "In addition we also evaluate the results with augmented datasets from icliniq.", "labels": [], "entities": []}, {"text": "We share the results on validation data in and results on test set in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results with features except RQE embeddings  using Logistic Regression", "labels": [], "entities": []}, {"text": " Table 2: Accuracy obtained on including RQE embed- dings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991530179977417}]}, {"text": " Table 3: Coverage of Validation set based on RQE  threshold for Task 3.", "labels": [], "entities": [{"text": "Coverage of Validation", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7897062102953593}, {"text": "RQE  threshold", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.8403151333332062}]}, {"text": " Table 5: Multi-Task learning results with different parameter settings on Test data for Task 3.", "labels": [], "entities": []}, {"text": " Table 6: Multi-Task learning results with different parameter settings on Validation data for Task 3.", "labels": [], "entities": []}]}