{"title": [], "abstractContent": [{"text": "Stack Long Short-Term Memory (StackL-STM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 85, "end_pos": 92, "type": "TASK", "confidence": 0.9766104221343994}, {"text": "string-to-tree neural machine translation", "start_pos": 97, "end_pos": 138, "type": "TASK", "confidence": 0.5841292217373848}]}, {"text": "In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations.", "labels": [], "entities": []}, {"text": "Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tree-structured representation of language has been successfully applied to various applications including dependency parsing (, sentiment analysis) and neural machine translation (.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8740041255950928}, {"text": "sentiment analysis", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.915048360824585}, {"text": "neural machine translation", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.6393473545710245}]}, {"text": "However, most of the neural network architectures used to build tree-structured representations are notable to exploit full parallelism of GPUs by minibatched training, as the computation that happens for each instance is conditioned on the input/output structures, and hence cannot be na\u00efvely grouped together as a batch.", "labels": [], "entities": []}, {"text": "This lack of parallelism is one of the major hurdles that prevent these representations from wider adoption practically (e.g., neural machine translation), as many natural language processing tasks currently require the ability to scale up to very large training corpora in order to reach state-of-theart performance.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 127, "end_pos": 153, "type": "TASK", "confidence": 0.717282791932424}]}, {"text": "We seek to advance the state-of-the-art of this problem by proposing a parallelization scheme for one such network architecture, the Stack Long Short-Term Memory (StackLSTM) proposed in.", "labels": [], "entities": []}, {"text": "This architecture has been successfully applied to dependency parsing () and syntax-aware neural machine translation () in the previous research literature, but none of these research results were produced with minibatched training.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.847578227519989}, {"text": "syntax-aware neural machine translation", "start_pos": 77, "end_pos": 116, "type": "TASK", "confidence": 0.6079579144716263}]}, {"text": "We show that our parallelization scheme is feasible in practice by showing that it scales up near-linearly with increasing batch size, while reproducing a set of results reported in ().", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Dependency parsing result with various train- ing batch size b and without composition function.  The results marked with asterisks were reported in the  Ballesteros et al. (2017).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8121513426303864}]}]}