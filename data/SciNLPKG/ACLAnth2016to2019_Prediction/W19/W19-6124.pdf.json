{"title": [{"text": "Projecting named entity recognizers without annotated or parallel corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Named entity recognition (NER) is a task extensively researched in the field of NLP.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8279034793376923}]}, {"text": "NER typically requires large annotated corpora for training usable models.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9591013193130493}]}, {"text": "This is a problem for languages which lack large annotated corpora, such as Finnish.", "labels": [], "entities": []}, {"text": "We propose an approach to create a named entity recognizer for Finnish by leveraging pre-existing strong NER models for English, with no manually annotated data and no parallel corpora.", "labels": [], "entities": []}, {"text": "We automatically gather a large amount of chronologically matched data in the two languages, then project named entity annotations from the English documents onto the Finnish ones, by resolving the matches with simple linguistic rules.", "labels": [], "entities": []}, {"text": "We use this \"artificially\" annotated data to train a BiLSTM-CRF NER model for Finnish.", "labels": [], "entities": [{"text": "BiLSTM-CRF", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.8265926837921143}]}, {"text": "Our results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9911819696426392}]}], "introductionContent": [{"text": "The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8167460660139719}]}, {"text": "The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems.", "labels": [], "entities": [{"text": "information retrieval (IR) or information extraction (IE)", "start_pos": 82, "end_pos": 139, "type": "TASK", "confidence": 0.7985574792731892}]}, {"text": "Various approaches have been proposed to tackle the NER task, including ().", "labels": [], "entities": [{"text": "NER task", "start_pos": 52, "end_pos": 60, "type": "TASK", "confidence": 0.9373757541179657}]}, {"text": "These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English.", "labels": [], "entities": []}, {"text": "However, not all languages are as resource-rich as English.", "labels": [], "entities": []}, {"text": "There are significantly fewer resources for languages such as Finnish.", "labels": [], "entities": []}, {"text": "Further, very few NER taggers or corpora are publicly available online.", "labels": [], "entities": [{"text": "NER taggers or corpora", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.9094448536634445}]}, {"text": "The FiNER tagger from the Language Bank of Finnish 1 is one of the few, but we found no documentation of its performance.", "labels": [], "entities": [{"text": "FiNER tagger from the Language Bank of Finnish 1", "start_pos": 4, "end_pos": 52, "type": "DATASET", "confidence": 0.952727841006385}]}, {"text": "Automatically annotating corpora for training NER models is one solution to this problem.", "labels": [], "entities": []}, {"text": "Several approaches have been proposed for building such corpora for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.7250626683235168}]}, {"text": "Most of these rely on the Wikipedia corpus, (Al-.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9584443271160126}]}, {"text": "However, the amount of Wikipedia documents in Finnish is also relatively small.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data.", "labels": [], "entities": []}, {"text": "This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language.", "labels": [], "entities": []}, {"text": "The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotated English data with Finnish data by resolving the base form of names.", "labels": [], "entities": []}, {"text": "proposed an idea of model projection similar to the one in the this work.", "labels": [], "entities": [{"text": "model projection", "start_pos": 20, "end_pos": 36, "type": "TASK", "confidence": 0.7254788875579834}]}, {"text": "However, rather than resolving the base form of named entities in target language internally as we do, they used machine translation as the basis for projection.", "labels": [], "entities": []}, {"text": "This allows them to project models between different languages, including in languages with different writing systems, such as Russian and English.", "labels": [], "entities": []}, {"text": "However, this assumes the existence of a high-quality machine translation system, and token binding between the languages, which determine the quality of the NER training dataset.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7377099394798279}, {"text": "NER training dataset", "start_pos": 158, "end_pos": 178, "type": "DATASET", "confidence": 0.8086389700571696}]}, {"text": "Using the resulting annotated data, we train an BiLSTM-CRF model on the basis of (, and evaluate it on a manually annotated dataset.", "labels": [], "entities": []}, {"text": "Our results show that training models on data annotated in this way achieves improved performance for Finnish NER tagging over models trained on the publicly available data alone.", "labels": [], "entities": [{"text": "NER tagging", "start_pos": 110, "end_pos": 121, "type": "TASK", "confidence": 0.8216693997383118}]}, {"text": "This suggests that our approach works well for annotating a corpus with named entities automatically, and enables using this corpus to learn good-quality NER models for less-resourced languages.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we briefly introduce a few terms and key concepts used throughout the paper.", "labels": [], "entities": []}, {"text": "In section 3 we describe the data sources, pre-processing steps and the rulebased annotation pipeline.", "labels": [], "entities": []}, {"text": "Section 4 describes our model architecture, as well as the parameters used in training.", "labels": [], "entities": []}, {"text": "In section 5 we discuss the results obtained from the experiments.", "labels": [], "entities": []}, {"text": "Section 6 concludes with current directions of research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we report the performance for the automatic projection pipeline and the NER model.", "labels": [], "entities": [{"text": "NER model", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.8380371332168579}]}, {"text": "F1-score is used as the evaluation metric.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9860835671424866}]}, {"text": "The overall F1-score is the weighted average F1-score of each category.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9987609386444092}, {"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.991162896156311}]}], "tableCaptions": [{"text": " Table 2: Table of hyper-parameter for experiments", "labels": [], "entities": []}, {"text": " Table 4: Quality of Finnish data projected from  BiLSTM-CRF-W2V model: evaluated on 1,000  sentences, annotated manually.", "labels": [], "entities": [{"text": "Quality", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9668139815330505}, {"text": "BiLSTM-CRF-W2V model", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.6968198418617249}]}, {"text": " Table 5: Quality of Finnish data projected from the  BiLSTM-CRF-GloVe model: evaluated on 1,000  sentences, annotated manually.", "labels": [], "entities": [{"text": "Quality", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9633157253265381}]}, {"text": " Table 6: Validation scores on 2018-04 to 2018-05. \"PULS pattern-based\" and \"BiLSTM-CRF-*\" refer  to the Finnish NER models that are projected from our PULS pattern-based NER tagger and English  BiLSTM-CRF NER tagger respectively. \"GloVe\" and \"W2V\" indicates the embedding that English NER  taggers use.", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9639676213264465}, {"text": "BiLSTM-CRF", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9270918369293213}, {"text": "GloVe", "start_pos": 232, "end_pos": 237, "type": "METRIC", "confidence": 0.9344492554664612}]}, {"text": " Table 7: Test evaluation. \"FiNER-data\" refer to the Finnish NER model trained with data from FiNER-data.  \"Polyglot\" entry illustrates the performance of their model on our test dataset", "labels": [], "entities": [{"text": "FiNER-data", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9340507388114929}, {"text": "FiNER-data", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8655126094818115}]}, {"text": " Table 8: Performance of the Finnish NER tagger  for each category. The tagger is projected from the  pattern-based English NER tagger in Join-Viterbi  mode, first line in", "labels": [], "entities": [{"text": "NER tagger", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.5338490158319473}]}, {"text": " Table 9: Detailed performance of Finnish NER  tagger for category \"Kaupunki\" (\"The City\") in", "labels": [], "entities": [{"text": "NER  tagger", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.7432136833667755}]}]}