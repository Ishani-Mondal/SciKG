{"title": [{"text": "The Challenges of Using Neural Machine Translation for Literature", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.691541314125061}]}], "abstractContent": [{"text": "In this paper, we adapt state-of-the-art neu-ral machine translation (NMT) systems to literary content and use them to translate fiction stories from English to Russian and from German to English.", "labels": [], "entities": [{"text": "neu-ral machine translation (NMT)", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.7412219196557999}]}, {"text": "We show that such adapted systems have richer vocabulary and lead to improved automatic evaluation metrics on literary prose as compared to general domain NMT systems, including Google's online MT.", "labels": [], "entities": []}, {"text": "We propose anew error classification scheme for NMT output that is specifically tailored to literary translation and let a bilingual evaluator analyze translated excerpts from two fiction stories.", "labels": [], "entities": [{"text": "literary translation", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.7636987268924713}]}, {"text": "The results show that up to 30% of machine-translated sentences have acceptable quality.", "labels": [], "entities": []}, {"text": "We observe very few severe syntactic errors even on complex sentences , but the meaning errors for ambiguous words are still numerous.", "labels": [], "entities": []}, {"text": "A separate classification of consistency, pronoun resolution , and tone/register error types reveals a high potential of MT quality improvement by considering the context of previous sentences or even the whole story.", "labels": [], "entities": [{"text": "consistency", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9931300282478333}, {"text": "pronoun resolution", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6610910445451736}, {"text": "MT quality", "start_pos": 121, "end_pos": 131, "type": "TASK", "confidence": 0.8390469253063202}]}, {"text": "A preliminary experiment aimed at reducing pronoun translation errors confirms this potential.", "labels": [], "entities": [{"text": "pronoun translation errors", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.6500566601753235}]}], "introductionContent": [{"text": "Recent advances in neural machine translation led to a greater acceptance of MT technology, even among professional translators.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7224450508753458}, {"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9932677745819092}]}, {"text": "However, it is hard to find anyone who would dare to use NMT for the professional translation of literature.", "labels": [], "entities": [{"text": "professional translation of literature", "start_pos": 69, "end_pos": 107, "type": "TASK", "confidence": 0.7710818648338318}]}, {"text": "Yet we believe that the challenges of literature translation could be tackled with NMT.", "labels": [], "entities": [{"text": "literature translation", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7354455441236496}]}, {"text": "In this work, we adapted a baseline general domain NMT system, described in Section 3, to the style and diverse vocabulary of literary translations.", "labels": [], "entities": []}, {"text": "The details of the adaptation process are given in Section 4.", "labels": [], "entities": [{"text": "adaptation", "start_pos": 19, "end_pos": 29, "type": "TASK", "confidence": 0.964674174785614}]}, {"text": "This was carried out on two language pairs: English-to-Russian and Germanto-English.", "labels": [], "entities": []}, {"text": "We then computed automatic error measures for translations of entire short novels and were able to show improvements as compared to the baseline model and Google's online MT (Section 5).", "labels": [], "entities": [{"text": "translations of entire short novels", "start_pos": 46, "end_pos": 81, "type": "TASK", "confidence": 0.8857274651527405}]}, {"text": "Next, we performed a thorough manual evaluation of both human and automatic translation quality on an excerpt from each novel.", "labels": [], "entities": []}, {"text": "For better insights into the shortcomings of NMT and potential improvements, we devised a novel error classification scheme, as described in Section 6.2, intended to tackle errors characteristic of neural MT systems, including cohesion and inter-sentence context issues which are prominent in literary translation.", "labels": [], "entities": [{"text": "error classification", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.6981271356344223}, {"text": "literary translation", "start_pos": 293, "end_pos": 313, "type": "TASK", "confidence": 0.706351637840271}]}, {"text": "Sections 6.3 and 6.4 describe these experiments in detail and also provide a quantitative comparison between Google's online and AppTek's adapted NMT for each error type.", "labels": [], "entities": []}, {"text": "We conclude the paper with a discussion on the possible applications of NMT for literature and underline the challenges, but also the opportunities associated with state-of-the-art NMT technology and its future developments.", "labels": [], "entities": []}, {"text": "the technology made significant progress in the last decade, initial research in this direction appeared, although non-computational linguists remain largely skeptical.", "labels": [], "entities": []}, {"text": "identified that incorporating discourse features above the sentence level is an important requirement for literary translation because of the greater referential cohesion of literary texts, but did not run any MT experiments with systems adapted to such content.", "labels": [], "entities": [{"text": "literary translation", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7313164472579956}, {"text": "MT", "start_pos": 210, "end_pos": 212, "type": "TASK", "confidence": 0.9642487168312073}]}, {"text": "Ina pilot study, trained a phrase-based statistical MT system for translating a short story from English to French, concluding that a faster literary translation with post-editing can be achieved at the expense of translator creativity and freedom of expression.", "labels": [], "entities": [{"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9040640592575073}, {"text": "translating a short story from English to French", "start_pos": 66, "end_pos": 114, "type": "TASK", "confidence": 0.8569755181670189}]}, {"text": "compared phrase-based statistical MT with neural MT when translating literary content using automatic and human evaluation.", "labels": [], "entities": [{"text": "phrase-based statistical MT", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.5065796772638956}]}, {"text": "They concluded that neural MT significantly outperforms phrasebased SMT in this genre, but \"fills the gap\" to the human quality level only by 20%.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.7605019211769104}, {"text": "phrasebased SMT", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.5720074772834778}]}, {"text": "In that work, a vanilla NMT architecture for English-to-Catalan MT was used, not described in detail.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.776336669921875}]}, {"text": "The authors built a relatively large in-domain parallel corpus of human-translated fiction, and also use synthetic parallel data, for which Catalan novels are translated using a phrase-based system into English.", "labels": [], "entities": []}, {"text": "In contrast, in our work we use the latest and best NMT architecture both for back-translation of large volumes of novels, and for the actual MT experiments; with only a very small parallel fiction corpus we are still able to obtain improvements over a strong general-domain NMT baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9585682153701782}]}, {"text": "Other related work important to literary translation include style transfer ( and personalization, number and gender disambiguation, document-level translation (.", "labels": [], "entities": [{"text": "literary translation", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7012739181518555}, {"text": "style transfer", "start_pos": 61, "end_pos": 75, "type": "TASK", "confidence": 0.7764779031276703}, {"text": "number and gender disambiguation", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.6080274507403374}, {"text": "document-level translation", "start_pos": 133, "end_pos": 159, "type": "TASK", "confidence": 0.6924229711294174}]}, {"text": "This work focuses on translation of prose; however, there have also been attempts to automatically translate poetry, with rhyming and rhythmical constraints, starting from the seminal work of for phrase-based SMT.", "labels": [], "entities": [{"text": "translation of prose", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.8855378230412801}, {"text": "SMT", "start_pos": 209, "end_pos": 212, "type": "TASK", "confidence": 0.6398189663887024}]}, {"text": "Recently, neural architectures were also proposed for this task ().", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we review the automatic scores for the generated literature translations.", "labels": [], "entities": []}, {"text": "We compute case-insensitive BLEU and TER scores (: Automatic MT quality measurements for Germanto-English literary translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9824277758598328}, {"text": "TER scores", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9679248929023743}, {"text": "Germanto-English literary translation", "start_pos": 89, "end_pos": 126, "type": "TASK", "confidence": 0.5732180078824362}]}], "tableCaptions": [{"text": " Table 1: Automatic MT quality measurements for English- to-Russian literary translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9669796228408813}, {"text": "English- to-Russian literary translation", "start_pos": 48, "end_pos": 88, "type": "TASK", "confidence": 0.6641228139400482}]}, {"text": " Table 2: Automatic MT quality measurements for German- to-English literary translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.9327920079231262}, {"text": "German- to-English literary translation", "start_pos": 48, "end_pos": 87, "type": "TASK", "confidence": 0.5949543356895447}]}, {"text": " Table 3: Human error analysis and BLEU and TER scores in % on the first 114 segments of A. Conan-Doyle's The Lift of  Google's online MT and AppTek's literature-adapted NMT. The acronyms of the error categories are explained in Section 6.2.", "labels": [], "entities": [{"text": "Human error analysis", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.5950633684794108}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9992503523826599}, {"text": "TER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9952257871627808}, {"text": "The Lift of  Google's online MT", "start_pos": 106, "end_pos": 137, "type": "DATASET", "confidence": 0.5258065845285144}, {"text": "AppTek's literature-adapted NMT", "start_pos": 142, "end_pos": 173, "type": "DATASET", "confidence": 0.6866072863340378}]}, {"text": " Table 4: Human error analysis and BLEU and TER scores in % on the first 114 segments of F. Kafka's Die Verwandlung of  Google's online MT and AppTek's literature-adapted NMT. The acronyms of the error categories are explained in Section 6.2.", "labels": [], "entities": [{"text": "Human error analysis", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.6091787616411845}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9992146492004395}, {"text": "TER", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9948201179504395}, {"text": "AppTek's literature-adapted NMT", "start_pos": 143, "end_pos": 174, "type": "DATASET", "confidence": 0.6997687816619873}]}]}