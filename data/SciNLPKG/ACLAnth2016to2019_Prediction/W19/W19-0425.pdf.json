{"title": [{"text": "Frame Identification as Categorization: Exemplars vs Prototypes in Embeddingland", "labels": [], "entities": [{"text": "Frame Identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.836884468793869}]}], "abstractContent": [{"text": "Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization.", "labels": [], "entities": []}, {"text": "Despite the fact that many semantic tasks involve categorization, theories of categorization do not play a major role in contemporary research in computational linguistics.", "labels": [], "entities": []}, {"text": "This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories.", "labels": [], "entities": []}, {"text": "The benefit is a group of models that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions.", "labels": [], "entities": []}, {"text": "We instantiate this idea on the frame-semantic frame identification task.", "labels": [], "entities": [{"text": "frame-semantic frame identification task", "start_pos": 32, "end_pos": 72, "type": "TASK", "confidence": 0.6970398426055908}]}, {"text": "We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input, and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization.", "labels": [], "entities": []}, {"text": "We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 17, "end_pos": 37, "type": "TASK", "confidence": 0.8539730906486511}]}, {"text": "Our prototype-based, fine-tuned model, which combines the best choices over these variables, establishes anew state-of-the-art in frame identification.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.8570948541164398}]}], "introductionContent": [{"text": "Categorization is the process of forming categories and assigning objects to them, and is a central capability of human cognition.", "labels": [], "entities": []}, {"text": "Not surprisingly, cognitive psychology has shown substantial interest in theories of categorization.", "labels": [], "entities": []}, {"text": "Two such prominent theories are prototype and exemplar models.", "labels": [], "entities": []}, {"text": "In prototype theory, categories are characterized in terms of a single representation, the prototype, which is an abstraction over individual objects and captures the 'essence' of the category.", "labels": [], "entities": []}, {"text": "In computational models, the prototype is often computed as the centroid of the objects of a category, and new objects are classified by their similarity to different categories' prototypes.", "labels": [], "entities": []}, {"text": "As a result, the decision boundary between every pair of categories is linear.", "labels": [], "entities": []}, {"text": "In contrast, exemplar theories represent categories in terms of the potentially large set of objects, called exemplars, that instantiate the category.", "labels": [], "entities": []}, {"text": "New objects are classified by similarity to nearby exemplars, so in a computational model this becomes similar to a nearest-neighbor classification.", "labels": [], "entities": []}, {"text": "In exemplar models, the decision boundary between categories can become non-linear, enabling more complex behavior to be captured, but at the cost of higher training data requirements.", "labels": [], "entities": []}, {"text": "Prototype and exemplar theories are typically not at the center of attention in contemporary computational linguistics.", "labels": [], "entities": []}, {"text": "One reason is arguably that, due to their origin in psychology, they tend to restrict themselves to cognitively plausible parameters and learning mechanisms (, whereas the focus of computational linguistics is very much on the use of novel machine learning techniques for applications.", "labels": [], "entities": []}, {"text": "We nevertheless believe that categorization theory is still relevant for computational linguistics, and lexical semantics in particular.", "labels": [], "entities": [{"text": "categorization theory", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7384045422077179}]}, {"text": "In fact, the emergence of distributed representations (embeddings) as a dominant representational paradigm has had a unifying effect on work in lexical semantics.", "labels": [], "entities": []}, {"text": "The properties of high-dimensional embeddings provide a good match with the assumption behind much of categorization theory -namely, that categories arise naturally from the similarity structure of individual objects.", "labels": [], "entities": []}, {"text": "Given this context, the exemplar-prototype dichotomy is a useful dimension on which models can be situated in terms of how much they generalize over objects: low for exemplar-inspired, but high for prototype-inspired models.", "labels": [], "entities": []}, {"text": "Regarding the representation of word meaning in context, for example, the additive models first considered by fall into the prototype camp, while propose exemplar-based models, and explore dynamic generalization in what they called 'multi-prototype' categorization models.", "labels": [], "entities": []}, {"text": "However, for many tasks, such comparisons -on a level playing field -are missing.", "labels": [], "entities": []}, {"text": "An interesting recent development in the embedding literature is the emergence of the distinction between pre-training and fine-tuning (e.g., in BERT (), OpenAI's GPT (, or ULM (): pre-training constructs embeddings that are supposedly general and are robust across many tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9778555035591125}, {"text": "OpenAI's GPT", "start_pos": 154, "end_pos": 166, "type": "DATASET", "confidence": 0.6991869807243347}]}, {"text": "Fine-tuning can then further optimize embeddings for one particular task, at the cost of robustness.", "labels": [], "entities": []}, {"text": "Importantly, pre-training takes advantage of massive amounts of unlabeled data, while fine-tuning can leverage small amounts of task-specific labeled data.", "labels": [], "entities": []}, {"text": "This distinction ties in nicely with open questions in the categorization literature concerning the respective roles of \"bottom-up\" similarity information and \"top-down\" theory information: task-independent pre-training embeddings, and their similarities which shape the categorization process, can be understood as \"bottom-up\" information, while the transformations that fine-tuning introduces to optimize these embeddings fora specific task, arguably represent \"top-down\" information.", "labels": [], "entities": []}, {"text": "Notably, such transformations can be understood equivalently as learning task-specific similarity metrics.", "labels": [], "entities": []}, {"text": "By learning general representations in a bottom-up pre-training phase and then comparing performance with additional top-down fine-tuning, we can discriminate how much general semantic knowledge is necessary to perform a categorization task and how much task-specific learning is required.", "labels": [], "entities": []}, {"text": "In this paper, we investigate a lexical semantic task, specifically the identification of frame-semantic frames () in running text, from this categorization perspective.", "labels": [], "entities": []}, {"text": "Frames can be understood as semantic classes that are sensitive both to the topic of the context and to specific properties of the predicate-argument structure.", "labels": [], "entities": []}, {"text": "We present four categorization models for the task, all of which are based on the state-of-the-art BERT model) but which differ in how they use its embeddings.", "labels": [], "entities": [{"text": "BERT", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9872375726699829}]}, {"text": "Two models are prototype-based (i.e., compute a representation for each frame), and two are exemplar-based (i.e., represent a frame solely in terms of its instances).", "labels": [], "entities": []}, {"text": "Within each group, we compare the use of embeddings without fine-tuning (\"bottom-up\") and with fine-tuning (\"top-down\").", "labels": [], "entities": []}, {"text": "This setup enables us to gauge, on a lexical semantic analysis task, (a) whether generalization helps, and what the size of the effect is; (b) whether there are benefits of top-down task adaptation; (c) whether there is an interaction between generalization and adaptation.", "labels": [], "entities": []}, {"text": "We find that generalization indeed helps, as does top-down adaptation.", "labels": [], "entities": [{"text": "generalization", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.9866118431091309}]}, {"text": "Overall, our best model establishes anew state-of-the-art in frame identification.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8692870736122131}]}, {"text": "In Section 2, we provide details on frame semantics and frame identification, as well as the current work in distributed semantic representations.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.8861369788646698}]}, {"text": "We additionally outline the architecture of BERT its pre-training and fine-tuning steps.", "labels": [], "entities": [{"text": "BERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.6719847321510315}]}, {"text": "Section 3 defines the four models that we experiment with, and Section 4 describes the experimental setup.", "labels": [], "entities": []}, {"text": "Finally, we describe and discuss results and analysis in Sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We work with the dataset sampled by from the FrameNet Release 1.5 full-text annotations.", "labels": [], "entities": [{"text": "FrameNet Release 1.5 full-text annotations", "start_pos": 45, "end_pos": 87, "type": "DATASET", "confidence": 0.8396028041839599}]}, {"text": "This dataset contains a total of 78 documents with frame-annotated sentences drawn from the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 92, "end_pos": 115, "type": "DATASET", "confidence": 0.9311251242955526}]}, {"text": "In total, 39 documents were selected for training and 16 for development with a total of 19,582 target predicates, and 23 documents for testing with 4,458 target predicate annotations.", "labels": [], "entities": []}, {"text": "This is the standard dataset used for evaluation of frame identification systems.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.7941402494907379}]}, {"text": "The general evaluation metric for frame identification is accuracy: the relative frequency of correct assignments to predicates.", "labels": [], "entities": [{"text": "frame identification", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.8703621923923492}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9993247985839844}]}, {"text": "Since the task of frame identification is moot for single-frame lexical units, frame identification systems standardly ( report accuracy on two different subsets of the data: (1) all instances from the test set, called \"Full Lexicon\", because it includes lexical units that are unambiguous; and (2) only instances of predicates from the test set that can evoke multiple frames, called \"Ambiguous\".", "labels": [], "entities": [{"text": "frame identification", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.802253931760788}, {"text": "frame identification", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7746657729148865}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9978051781654358}, {"text": "Ambiguous", "start_pos": 386, "end_pos": 395, "type": "METRIC", "confidence": 0.9171006679534912}]}, {"text": "In the data set we use, the test partition contains 2,029 ambiguous predicates out of a total of 4,458 predicate instances.", "labels": [], "entities": []}, {"text": "In addition, some prior work reports specific metrics on infrequent predicates, for which prediction is particularly challenging.", "labels": [], "entities": []}, {"text": "\"Unseen\" reports accuracy for predicates that are completely unseen in the training data and their predictions overall possible frames -meaning the frame lexicon is not used for evaluation attest time . \"Rare\" reports accuracy on predicates that occur less than 11 times in the training shows the performance of the four models as well as prior results from recent literature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9992572665214539}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9937339425086975}]}, {"text": "Regarding the impact of the exemplar and prototype dimensions that we introduced in Section 3, we find that the exemplar model does worse overall than the prototype model in both configurations (overall \"Full Lexicon\" accuracy: 2% for bottom-up, 7% for bottom-up plus top-down).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.7466620206832886}]}, {"text": "This indicates that the prototype setup appears better suited to the task than the exemplar one, at least on the data we experimented with.", "labels": [], "entities": []}, {"text": "Second, we see a substantial effect of top-down processing (fine-tuning): 1.5% for exemplars, over 6% for prototypes.", "labels": [], "entities": []}, {"text": "The clear winner is the bottom-up plus top-down (fine-tuned) prototype model: with an accuracy of 91.26%, it outperforms the previous state of the art (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995478987693787}]}, {"text": "This shows that frame categorization can indeed profit from task-based optimization.", "labels": [], "entities": []}, {"text": "That being said, it is worth noting that even the bottom-up prototype model with only generic pre-training performs at or above the level of the supervised SEMAFOR model () which incorporated linguistic and ontological features in a log-linear model.", "labels": [], "entities": []}, {"text": "Thus, the bottom-up vector space models do have a claim to robust performance.", "labels": [], "entities": []}, {"text": "Accuracy on \"Ambiguous\" predicates largely mirrors the patterns we find on \"Full Lexicon\" accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.969543993473053}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.715131938457489}]}, {"text": "They bolster the interpretation that both prototype representation and fine-tuning lead to clear gains.", "labels": [], "entities": []}, {"text": "Results on \"Rare\" and \"Unseen\" predicates are more difficult to compare due to lack of reported results (marked as NA).", "labels": [], "entities": [{"text": "NA", "start_pos": 115, "end_pos": 117, "type": "METRIC", "confidence": 0.852881908416748}]}, {"text": "The numbers for \"Rare\", again, seem to follow the \"Full Lexicon\" trend, and outperform the state of the art.", "labels": [], "entities": [{"text": "Rare\"", "start_pos": 17, "end_pos": 22, "type": "DATASET", "confidence": 0.8630677759647369}]}, {"text": "The results for the \"Unseen\" category do so too, but are below the previously reported results.", "labels": [], "entities": []}, {"text": "The reason is that employ additional processing to unseen predicates based on a context similarity graph.", "labels": [], "entities": []}, {"text": "For simple supervised classification without the extra component, comparable to our 30.20% setting, they report an Accuracy of 23.08%.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9996331930160522}]}], "tableCaptions": [{"text": " Table 1: Accuracy results for Frame Identification on Das et al. (2014) benchmark dataset (test partition)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9984197616577148}, {"text": "Frame Identification", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.8776751756668091}, {"text": "Das et al. (2014) benchmark dataset", "start_pos": 55, "end_pos": 90, "type": "DATASET", "confidence": 0.6649515496359931}]}, {"text": " Table 2: Accuracies for top 5 frames from Bottom-up+Top-down Prototype model across all four model", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9994418025016785}]}, {"text": " Table 3: Accuracies for top 5 predicates from Bottom-up+Top-down Prototype model across four model", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9988062381744385}]}]}