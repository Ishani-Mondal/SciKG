{"title": [], "abstractContent": [{"text": "This paper presents anew schema to annotate Chinese Treebanks on the character level.", "labels": [], "entities": [{"text": "Chinese Treebanks", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9506770968437195}]}, {"text": "The original Universal Dependencies (UD) and Surface-Syntactic Universal Dependencies (SUD) projects provide token-level resources with rich morphosyntactic language details.", "labels": [], "entities": []}, {"text": "However, without any commonly accepted word definition for Chinese, the dependency parsing always faces the dilemma of word segmentation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7823381721973419}, {"text": "word segmentation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7020113468170166}]}, {"text": "Therefore we present a character-level annotation schema integrated into the existing Universal Dependencies schema as an extension.", "labels": [], "entities": []}], "introductionContent": [{"text": "With its writing system being a \u200b Scriptua Continua\u200b , Chinese is a language without explicit word delimiters and thus the \"wordhood\" is a particularly unclear notion.", "labels": [], "entities": []}, {"text": "Yet, the vast majority of downstream NLP tasks for any language are based on \"tokens\", which mostly boils down to some kind of spelling-based tokenizer.", "labels": [], "entities": []}, {"text": "Yet, in the case of Chinese, this step requires a preprocessing step called \"word segmentation\", whose performance has an non-neglectable influence on the final results.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7166043370962143}]}, {"text": "While the F-score of the segmentation task of general texts is in the high nineties since more than 10 years) and results have even been slightly improved by recent neural models ( ), these numbers drop to below 10% for Out-of-Vocabulary terms, i.e. where the system has to take educated guesses on where the word borders are.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9986458420753479}, {"text": "segmentation task of general texts", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.8821591258049011}]}, {"text": "This leads to catastrophic results for domain specialized texts that use a great number of neologisms unknown to the system, such as patent texts.", "labels": [], "entities": []}, {"text": "Since (Zhao 2009) proposed the first method for character-level dependencies parsing on the Chinese Penn Treebank, a series of research involving the character-based annotation) have already shown the usefulness of the word-internal structures in Chinese syntactic parsing by obtaining limited but real improvements by means of extra character-level information (character POS, head character position and word internal dependency relation).", "labels": [], "entities": [{"text": "character-level dependencies parsing", "start_pos": 48, "end_pos": 84, "type": "TASK", "confidence": 0.6234085162480673}, {"text": "Chinese Penn Treebank", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.867517371972402}, {"text": "syntactic parsing", "start_pos": 255, "end_pos": 272, "type": "TASK", "confidence": 0.7127261459827423}]}, {"text": "(Zhao 2009) and ) have annotated a large-scale word list on Penn Treebank (PTB) and constituent Chinese Treebank (CTB) on the morphological level.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 60, "end_pos": 79, "type": "DATASET", "confidence": 0.9674509048461915}, {"text": "Chinese Treebank (CTB)", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.9345452904701232}]}, {"text": "Other character-based parsing attempts are generally based on these two annotated corpora.", "labels": [], "entities": [{"text": "character-based parsing", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.6942594051361084}]}, {"text": "In this work, we report on the integration of character-level annotations into the Chinese UD treebanks with the goal to find a joint segmentation-parsing method, which enables a multi-granularity analysis on Chinese sentences.", "labels": [], "entities": [{"text": "Chinese UD treebanks", "start_pos": 83, "end_pos": 103, "type": "DATASET", "confidence": 0.6370221972465515}]}, {"text": "Besides the final goal to improve the performance of the dependency parser with character-level information, in particular on out-of-domain texts, this work can also be regarded as anew Chinese word segmentation method: As we distinguish the morphological and syntactic relations between characters by a different set of dependency relation labels, we can ultimately fuse the character parsing results into a simple word segmentation, which can be compared to the original UD word segmentation.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.767753392457962}, {"text": "Chinese word segmentation", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.6269752383232117}, {"text": "character parsing", "start_pos": 376, "end_pos": 393, "type": "TASK", "confidence": 0.7870470285415649}, {"text": "UD word segmentation", "start_pos": 473, "end_pos": 493, "type": "TASK", "confidence": 0.6321887075901031}]}, {"text": "The character-level parse tree can thus also be projected onto a dependency tree on the words, which allows us to compare our parsing results with a simple token-based model.", "labels": [], "entities": []}, {"text": "In Section 2 we will briefly introduce various internal structures of Chinese words before presenting our annotation scheme for character-level POS and word internal dependency structures.", "labels": [], "entities": []}, {"text": "The experiments and the results obtained are shown in Section 3, followed by the conclusion in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the Common Crawl corpus) of English web text.", "labels": [], "entities": [{"text": "Common Crawl corpus) of English web text", "start_pos": 11, "end_pos": 51, "type": "DATASET", "confidence": 0.8876192048192024}]}, {"text": "We filtered the corpus to contain mostly meaningful linguistic utterances and to remove irrelevant web boilerplate text.", "labels": [], "entities": []}, {"text": "We parsed and POS-tagged 10% of the filtered corpus using SyntaxNet (.", "labels": [], "entities": []}, {"text": "The final dataset used in this paper consists of a total of 320 million tokens of parsed text.", "labels": [], "entities": []}, {"text": "SyntaxNet produces function-word-headed dependencies, rather than content-head dependencies, so our results reflect syntactic dependencies rather than semantic dependencies.", "labels": [], "entities": []}, {"text": "The neural transition-based dependency parser of De Kok and Hinrichs (2016) serves as the baseline for the experiments.", "labels": [], "entities": []}, {"text": "Words, part-of-speech tags and characters are represented as vectors that were trained with structured skip-gram (.", "labels": [], "entities": []}, {"text": "Topological fields are used as additional input features.", "labels": [], "entities": []}, {"text": "The parser does pseudo-projective parsing and was trained on the shuffled T\u00fcBa-D/Z () that contains 105K sentences and 1.9M tokens of manually labeled data from the Berliner Tageszeitung (taz).", "labels": [], "entities": [{"text": "Berliner Tageszeitung (taz)", "start_pos": 165, "end_pos": 192, "type": "DATASET", "confidence": 0.8788013458251953}]}, {"text": "Non-gold part-of-speech tags were trained via 10-fold jackknifing on the T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "T\u00fcBa-D/Z", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9472018678983053}]}, {"text": "The data was split in a 7:1:2 ratio for respectively training, development and testing.", "labels": [], "entities": []}, {"text": "Association scores are retrieved for lowercased word forms to increase lexical coverage.", "labels": [], "entities": []}, {"text": "Common and proper nouns are typically capitalized in German and were therefore not lowercased.", "labels": [], "entities": []}, {"text": "Results are presented as labeled (LAS) and unlabeled attachment scores (UAS) including punctuation.", "labels": [], "entities": [{"text": "labeled (LAS) and unlabeled attachment scores (UAS)", "start_pos": 25, "end_pos": 76, "type": "METRIC", "confidence": 0.716334814375097}]}, {"text": "Accuracies for inversion and prepositions indicate performance on resolving ambiguities.", "labels": [], "entities": []}, {"text": "Inversion accuracy reports correct labeling of subjects and objects in clauses with fronted object.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.6270391345024109}]}, {"text": "Preposition accuracy comprises all correct heads and labels of prepositional phrases and objects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.7967931032180786}]}, {"text": "The test set contains 1,887 cases of inversion (5.82 percent of all clauses) and 31,687 prepositional phrases and objects.: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with PMI-based association scores.", "labels": [], "entities": [{"text": "Parser", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9727990627288818}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9183064103126526}, {"text": "neural dependency parsing", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6440643668174744}]}, {"text": "The NPMI model with minimum frequency 5 achieves the best overall performance.", "labels": [], "entities": []}, {"text": "We have worked on the four Chinese UD treebanks converted into SUD format and simplified characters when necessary: The Traditional Chinese Universal Dependencies Treebank annotated by Google (GSD), the Parallel Universal Dependencies treebanks created for the CoNLL 2017 shared task on Multilingual Parsing (PUD), the Traditional Chinese treebank of film subtitles and of legislative proceedings of Hong Kong (HK), and the essays written by learners of Mandarin Chinese as a foreign language (CFL), also proposed by the City University of Hong Kong.", "labels": [], "entities": [{"text": "CoNLL 2017 shared task on Multilingual Parsing (PUD)", "start_pos": 261, "end_pos": 313, "type": "TASK", "confidence": 0.7961418747901916}]}, {"text": "To train the character-based POS tagger and SUD parser, we choose the Graph-based Neural Dependency Parser developed by Timothy Dozat at Stanford University for its character-based LSTM word representation.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.705893874168396}, {"text": "LSTM word representation", "start_pos": 181, "end_pos": 205, "type": "TASK", "confidence": 0.6872652570406595}]}, {"text": "This parser contains a tagger training network and a dependency parser training network, but unfortunately these two training processes are separated, meaning that to obtain a corpus tagged and parsed, first we have to train a tagger, use it to tag our corpus, then train a parser and use it to parse our tagged corpus.", "labels": [], "entities": []}, {"text": "Before the training process, we have also prepared a character vector file which is trained by BERT, a word embedding model developed by Google with a pre-trained character based Chinese model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9964442849159241}]}, {"text": "Our experiments consist of using Dozat Parser to train the word-based (WB) tagger and parser, as well as the character-based (CB) tagger and parser.", "labels": [], "entities": []}, {"text": "Then by applying them to tag and parse our test corpus we can obtain two versions of our treebank: a word-based and a character-based treebank (see \u200b Annex 2\u200b ), so that we can perform systematic tests of comparison on the combined Chinese SUD treebanks and evaluate the performance of our character-based tagger and parser.", "labels": [], "entities": [{"text": "Chinese SUD treebanks", "start_pos": 232, "end_pos": 253, "type": "DATASET", "confidence": 0.6034828325112661}]}, {"text": "To sum up, we need to go through at least four training processes: WB tagger training, WB parser training, CB tagger training and CB parser training.", "labels": [], "entities": [{"text": "WB tagger training", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8868687550226847}, {"text": "WB parser training", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.936475396156311}, {"text": "CB tagger training", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.8632150093714396}, {"text": "CB parser training", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8532784978548685}]}, {"text": "Therefore, we have prepared our training data as following: for WB tagger and parser training, we extract the last 10% of the four former mentioned Chinese SUD treebanks to serve as the testing set and the developing set, and we combine the rest 90% to serve as a training set; for CB tagger and parser training, we carryout the exact same arrangement, except this time all the treebanks are converted from word level to character level.", "labels": [], "entities": [{"text": "WB tagger", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.8890753984451294}, {"text": "Chinese SUD treebanks", "start_pos": 148, "end_pos": 169, "type": "DATASET", "confidence": 0.7313100099563599}, {"text": "CB tagger", "start_pos": 282, "end_pos": 291, "type": "TASK", "confidence": 0.7687663733959198}]}, {"text": "Concerning the tagger, we compare the F-score of the tagger trained on WB and on CB.", "labels": [], "entities": [{"text": "tagger", "start_pos": 15, "end_pos": 21, "type": "TASK", "confidence": 0.9678547382354736}, {"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9989938139915466}, {"text": "WB", "start_pos": 71, "end_pos": 73, "type": "DATASET", "confidence": 0.9746277332305908}]}, {"text": "The \u200b display the direct result of the CB tagging.", "labels": [], "entities": [{"text": "CB tagging", "start_pos": 39, "end_pos": 49, "type": "TASK", "confidence": 0.6373505294322968}]}, {"text": "As we can see, the Dozat parser achieved a rather high score on CB tagging.", "labels": [], "entities": [{"text": "CB tagging", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.8170413076877594}]}, {"text": "Some POS, because of it's absence on a character level, doesn't have a remarkable score, like SCONJ, but regardless of that we believe this tagger can satisfy our basic need in CB tagging.", "labels": [], "entities": [{"text": "CB tagging", "start_pos": 177, "end_pos": 187, "type": "TASK", "confidence": 0.8661409616470337}]}, {"text": "However, we cannot compare directly this result with the result of WB tagging, since the words in the treebank for CB tagging has been split into characters, and thus we don't have the exact same number of POS in the WB and CB tagged treebanks.", "labels": [], "entities": [{"text": "WB tagging", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.7936681509017944}, {"text": "CB tagging", "start_pos": 115, "end_pos": 125, "type": "TASK", "confidence": 0.7828878164291382}, {"text": "WB and CB tagged treebanks", "start_pos": 217, "end_pos": 243, "type": "DATASET", "confidence": 0.7469976425170899}]}, {"text": "Therefore a recombination of the CB treebank after the tagging is necessary.", "labels": [], "entities": [{"text": "CB treebank", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.8683160841464996}]}, {"text": "To facilitate the recombination, we use the XPOS column in our treebank (under Conll-U format) to record the word level POS.", "labels": [], "entities": []}, {"text": "When we split a word into characters during the preparation of treebanks for tagger training, we insert the character level POS into the UPOS column, and copy the word's original POS to the XPOS column of each character.", "labels": [], "entities": [{"text": "tagger training", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.9306733012199402}]}, {"text": "And since in Dozat Parser the prediction of XPOS is dependent on the prediction of UPOS, we can thus train a tagger that can tag WB POS based on the CB POS.", "labels": [], "entities": []}, {"text": "The following are the results of WB tagging (\u200b ) and CB tagging after the recombination (\u200b F-score of word level POS (XPOS) for our character-based tagger after the recombination As we can see from these two tables above, the training on a character base has greatly improved the performance of the tagger.", "labels": [], "entities": [{"text": "WB tagging", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.8814732134342194}, {"text": "CB tagging", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.6992680430412292}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.99399733543396}, {"text": "word level POS (XPOS)", "start_pos": 102, "end_pos": 123, "type": "METRIC", "confidence": 0.7934553424517313}]}, {"text": "However for some most common POS, like ADJ and NOUN, there's an obvious decline of f-score.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.5373103618621826}, {"text": "f-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9203642010688782}]}, {"text": "One of the possible reasons is that there's an inconsistency between the word level POS and character level POS in Chinese.", "labels": [], "entities": []}, {"text": "For example, \u6d3b\u52a8 (NOUN, '\u200b activity\u200b ') is composed by two verbal character \"\u6d3b\" (VERB, '\u200b living\u200b ') and \"\u52a8\" (VERB, '\u200b moving\u200b ').", "labels": [], "entities": [{"text": "NOUN", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.8315302729606628}, {"text": "VERB", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9326361417770386}]}, {"text": "But by reviewing our data, we noticed that there's also an inconsistency on the POS annotation of the same words between different treebanks, even if in a similar context.", "labels": [], "entities": []}, {"text": "This problem may have a bigger influence on both tagger and parser.", "labels": [], "entities": [{"text": "tagger", "start_pos": 49, "end_pos": 55, "type": "TASK", "confidence": 0.9644203186035156}]}, {"text": "Since we haven't totally finished the pretreatment process, there's a problem of inconsistency in our data, with the same word in the same context but having different POS or different internal structure annotated.", "labels": [], "entities": []}, {"text": "We can also separately measure the performance on the syntactic and morphological dependencies (\u200b ).", "labels": [], "entities": []}, {"text": "This method has a special function, that is the performance of the segmentation can be evaluated by concerning only about the two main groups of dependency relations: Morphe (relations annotated with m: at the beginning) and Deprel (the original dependency relations in SUD).", "labels": [], "entities": [{"text": "Morphe", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.7542732357978821}]}, {"text": "Both the PMI-based and embedding-based models perform better than the baseline.", "labels": [], "entities": []}, {"text": "Overall performance will improve by more correctly solved ambiguous attachments.", "labels": [], "entities": []}, {"text": "Lexical associations between more than two tokens maybe necessary to further improve ambiguity resolution.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.740287721157074}]}, {"text": "For PP attachment, the compatibility between the preposition, its modifier noun and the verbal or nominal head candidate of the PP have to be modeled.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9340215921401978}]}, {"text": "De have shown that trilexical preferences help to better capture attachment preferences of the preposition.", "labels": [], "entities": []}, {"text": "It can also be beneficial to make competing attachment sites available to the parser.", "labels": [], "entities": []}, {"text": "Currently, association scores are only computed for the two attachment candidates for any given parser state.", "labels": [], "entities": []}, {"text": "With beam search, several attachment candidates can compete in different analyses.", "labels": [], "entities": [{"text": "beam search", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8709270060062408}]}, {"text": "The best candidate can then be chosen from all or then best candidates (.", "labels": [], "entities": []}, {"text": "In order to preliminary evaluate the similarities between the three datasets, we performed an evaluation of UDPipe using the TWITTIR`OTWITTIR` TWITTIR`O-UD gold corpus as a test set.", "labels": [], "entities": [{"text": "TWITTIR`O-UD gold corpus", "start_pos": 143, "end_pos": 167, "type": "DATASET", "confidence": 0.5947067588567734}]}, {"text": "The following three settings were exploited.", "labels": [], "entities": []}, {"text": "1) training UDPipe using only UD Italian (UD It), 2) training UDPipe using only PoSTWITA-UD (PoSTW), 3) and training UDPipe using both resources (UD It+PoSTW).", "labels": [], "entities": []}, {"text": "For evaluation we used the script made available for the CoNLL 2018 Shared Task 5 10 with the default setting parameters.", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task 5 10", "start_pos": 57, "end_pos": 84, "type": "DATASET", "confidence": 0.932793656984965}]}, {"text": "surveys the resulting scores for precision (P), recall (R) and averaged F1-score (F1).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 33, "end_pos": 46, "type": "METRIC", "confidence": 0.9442202150821686}, {"text": "recall (R)", "start_pos": 48, "end_pos": 58, "type": "METRIC", "confidence": 0.9500472843647003}, {"text": "averaged", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9682124257087708}, {"text": "F1-score (F1)", "start_pos": 72, "end_pos": 85, "type": "METRIC", "confidence": 0.8356699645519257}]}, {"text": "First of all, it is interesting to notice the variation of the Unlabelled Attachment Score (UAS) and Labelled Attachment Score (LAS).", "labels": [], "entities": [{"text": "Unlabelled Attachment Score (UAS)", "start_pos": 63, "end_pos": 96, "type": "METRIC", "confidence": 0.802227571606636}, {"text": "Labelled Attachment Score (LAS)", "start_pos": 101, "end_pos": 132, "type": "METRIC", "confidence": 0.8046881655852}]}, {"text": "For what concerns UAS, the first setup, where only the data from UD Italian have been used for training, allowed a better result than the second one, where PoSTWITA-UD is the training dataset.", "labels": [], "entities": [{"text": "UAS", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.607437789440155}]}, {"text": "But the opposite can be seen for LAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8727023005485535}]}, {"text": "We can hypothesize that the larger amount of data in UD Italian allowed to build a more representative statistical model.", "labels": [], "entities": [{"text": "UD Italian", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9354497790336609}]}, {"text": "Nevertheless, training on a resource which includes the same typology of data maybe crucial for collecting an adequate knowledge about the specific relations exploited.", "labels": [], "entities": []}, {"text": "This motivates the best scores for LAS an UAS, which were obtained in the third setup benefiting of both the resources for training.", "labels": [], "entities": [{"text": "LAS", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.5467928051948547}, {"text": "UAS", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.6828898787498474}]}, {"text": "This encourages us to develop more and better gold standard treebanks also for social media to be used for training.", "labels": [], "entities": []}, {"text": "The experiment is conducted as a dialogue supported by a special program.", "labels": [], "entities": []}, {"text": "The program takes as input a sentence in the form of a string of characters and splits it into words.", "labels": [], "entities": []}, {"text": "The dialogue consists of N-1 steps numbered 2, 3, ...", "labels": [], "entities": []}, {"text": ", N, where N is the number of words in the sentence.", "labels": [], "entities": []}, {"text": "At step K the subject is presented with a text file which shows the first K words of the sentence (with the adjacent punctuation) plus the right context, that is, a fixed number of words following the word K.", "labels": [], "entities": []}, {"text": "The syntactic links are also shown that were created at previous steps between the words of the left context (1, ... , K-1).", "labels": [], "entities": []}, {"text": "When the last word of the sentence is shown, it is accompanied by the message [end of sentence]; until this message appears, the subject has no information about the length of the sentence.", "labels": [], "entities": []}, {"text": "| 9 --> 3 is | ------------------------------------------------------------  The sentences for the experiments were taken from the two sets of sentences dev.csv and train.csv offered as training material for the competition \"Automatic Gapping Resolution for Russian\" held in association with the conference Dialogue 2019 (Dialogue Evaluation / AGRR-2019).", "labels": [], "entities": [{"text": "Automatic Gapping Resolution", "start_pos": 225, "end_pos": 253, "type": "TASK", "confidence": 0.7116548220316569}]}, {"text": "These sets contain over 20,000 sentences of various genres, about one third of which are marked as elliptical.", "labels": [], "entities": []}, {"text": "For our experiments, non-elliptical sentences were selected that satisfied the following additional requirements: (1) the number of words does not exceed 30; (2) the first alphanumeric character is a Russian capital letter; (3) the last character is a small Russian letter or full stop; (4) the proportion of small Russian letters among all alphanumeric characters is at least 90%.", "labels": [], "entities": []}, {"text": "The aim of these requirements was to restrict experimental material to \"ordinary narrative Russian sentences of average length\".", "labels": [], "entities": []}, {"text": "As a result, a set of about 7,700 sentences was formed; the sentences for the experiments were taken from it without replacement using pseudorandom numbers.", "labels": [], "entities": []}, {"text": "The distribution of sentence length in this set is rather \"flat\" on the segment from 7 to 30, with a mean of 17.4 and a standard deviation of 6.4.", "labels": [], "entities": []}, {"text": "Hence, in a random sample of 100 sentences the average length has the same mean and a standard deviation of 0.64.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 8. Some of the patterns for the rules can be found in Tables 4, 5 and 6. The matcher  also returns the position of the head_word and the comp_word if applicable. As shown, the matches are  mostly correct for all three types of patterns, meaning that our query language and our patterns are robust  enough to describe and find the syntactic phenomena that we aimed to identify.", "labels": [], "entities": []}, {"text": " Table 8: Seven sentences from our test set, their ideal matches and the matches that the algorithm re- turned. Rules that were incorrectly matched/not matched are marked in bold.", "labels": [], "entities": []}, {"text": " Table 9: Matcher results on the development  set with gold standard parses and parse results  from the parsers.", "labels": [], "entities": []}, {"text": " Table 10: Matcher results on the test set with  gold standard parses and parse results from the  parsers.", "labels": [], "entities": [{"text": "Matcher", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9305347800254822}]}, {"text": " Table 2: Test set accuracy for target languages (UAS, LAS). \u2212Target = cross-lingual models trained  without target language data. +Target = models trained on target language data; monolingual (first row)  and multilingual.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9830779433250427}]}, {"text": " Table 1: Word Order Overview", "labels": [], "entities": []}, {"text": " Table 2: Predictors of Argument Position", "labels": [], "entities": [{"text": "Predictors of Argument Position", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.6601729691028595}]}, {"text": " Table 3: Predictors of Argument Position by Grammatical Function", "labels": [], "entities": [{"text": "Predictors of Argument Position by Grammatical Function", "start_pos": 10, "end_pos": 65, "type": "TASK", "confidence": 0.6473331792013985}]}, {"text": " Table 4: Observed values and predictions of the random forest.", "labels": [], "entities": [{"text": "Observed", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9786888360977173}]}, {"text": " Table 6: Logistic Regression model of Argument Placement (reference level: pre-verbal).", "labels": [], "entities": [{"text": "Argument Placement", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.6679096072912216}]}, {"text": " Table 2: MDD and MHD scores of YNU data", "labels": [], "entities": [{"text": "YNU data", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.859381914138794}]}, {"text": " Table 3: Outline of the current corpus data", "labels": [], "entities": [{"text": "Outline", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9118942022323608}]}, {"text": " Table 4: Method of calculating DD and HD  \u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b1\u0080\u00ed \u00b5\u00ed\u00b1\u0080(the sentence) =", "labels": [], "entities": []}, {"text": " Table 5. This means that  the increase may reflect their syntactic complexity development as their Japanese learning progressed.", "labels": [], "entities": []}, {"text": " Table 5: SL, MDD and MHD comparison of C2, C3 and JP", "labels": [], "entities": []}, {"text": " Table 6: Brunner-Munzel Test and Cliff's delta of the MDD and MHD", "labels": [], "entities": [{"text": "Brunner-Munzel Test", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9185386002063751}, {"text": "MHD", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.5010438561439514}]}, {"text": " Table 1: Dependency parsing scores (+ average sentence per second on CPU) using the PoS-tag based  encoding for the different learning strategies (best in bold;  \u2020 marks statistical significance; T-test with  p<0.05). STR19 scores are reported from Strzyz et al. (2019) (besides from Tamil for which they use  gold PoS-tags).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6630653440952301}, {"text": "T-test", "start_pos": 197, "end_pos": 203, "type": "METRIC", "confidence": 0.9865431785583496}, {"text": "STR19", "start_pos": 219, "end_pos": 224, "type": "METRIC", "confidence": 0.8176397085189819}]}, {"text": " Table 2: Dependency parsing scores (+ precision on heads) with the different dependency encodings,  using the combined learning strategy (best in bold;  \u2020 marks statistical significance).", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7244388610124588}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9928804039955139}]}, {"text": " Table 1: Subfunctors for localizations \"within the given place\" (of LOC functor).", "labels": [], "entities": []}, {"text": " Table 2: Czech prepositions for localization \"within the given place\" and their English equivalents.", "labels": [], "entities": []}, {"text": " Table 1. Number of sentence containing the 10 verbs", "labels": [], "entities": []}, {"text": " Table 1: Five most frequent parser errors by dependency label of the parser by De Kok and Hinrichs", "labels": [], "entities": []}, {"text": " Table 2: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with  PMI-based association scores. The NPMI model with minimum frequency 5 achieves the best overall  performance.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.978609025478363}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8753945231437683}, {"text": "neural dependency parsing", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.6351583302021027}]}, {"text": " Table 3: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with  embedding-based association scores. The overall best model uses projectivized, fully typed dependency  embeddings with a binary indicator.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9804831743240356}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9008970260620117}, {"text": "neural dependency parsing", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.6009583274523417}]}, {"text": " Table 4: PMI and embedding-based scores for random and incorrectly attached dependency triples.", "labels": [], "entities": [{"text": "PMI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6335314512252808}]}, {"text": " Table 1: Distribution of deprel labels for hashtags and mentions.", "labels": [], "entities": []}, {"text": " Table 2: Dependency relations' distribution across the three main Italian treebanks. The values are  expressed in percentage %.", "labels": [], "entities": [{"text": "Italian treebanks", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.8008346557617188}]}, {"text": " Table 3: Evaluation of UDPipe.", "labels": [], "entities": [{"text": "UDPipe", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.6217535734176636}]}, {"text": " Table 1. The results of the experiments.", "labels": [], "entities": []}, {"text": " Table 3 F-score of word level POS (UPOS) for our  word-based tagger", "labels": [], "entities": [{"text": "F-score", "start_pos": 9, "end_pos": 16, "type": "METRIC", "confidence": 0.9929458498954773}, {"text": "word level POS (UPOS)", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.6773106505473455}]}]}