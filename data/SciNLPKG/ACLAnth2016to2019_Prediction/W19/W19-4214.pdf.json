{"title": [{"text": "A Little Linguistics Goes a Long Way: Unsupervised Segmentation with Limited Language Specific Guidance", "labels": [], "entities": []}], "abstractContent": [{"text": "We present de-lexical segmentation, a linguistically motivated alternative to greedy or other unsupervised methods, requiring language specific knowledge, but no direct supervision.", "labels": [], "entities": [{"text": "de-lexical segmentation", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7438241541385651}]}, {"text": "Our technique involves creating a small grammar of closed-class affixes which can be written in a few hours.", "labels": [], "entities": []}, {"text": "The grammar over generates analyses for word forms attested in a raw corpus which are disam-biguated based on features of the linguistic base proposed for each form.", "labels": [], "entities": []}, {"text": "Extending the grammar to cover orthographic, morpho-syntactic or lexical variation is simple, making it an ideal solution for challenging corpora with noisy, dialect-inconsistent, or otherwise non-standard content.", "labels": [], "entities": []}, {"text": "We demonstrate the utility of de-lexical segmentation on several dialects of Arabic.", "labels": [], "entities": [{"text": "de-lexical segmentation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7481500208377838}]}, {"text": "We consistently outper-form competitive unsupervised baselines and approach the performance of state-of-the-art supervised models trained on large amounts of data, providing evidence for the value of linguistic input during preprocessing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Non-standard domains, dialectal variation, and unstandardized spelling make segmentation challenging, though morphologically rich languages require good segmentation to enable downstream applications from syntactic parsing to machine translation (MT).", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 205, "end_pos": 222, "type": "TASK", "confidence": 0.7511657178401947}, {"text": "machine translation (MT)", "start_pos": 226, "end_pos": 250, "type": "TASK", "confidence": 0.8211200773715973}]}, {"text": "For domains lacking sufficient annotated data to train segmenters, one must resort to language specific greedy techniques or language agnostic unsupervised techniques.", "labels": [], "entities": []}, {"text": "Greedy techniques use maximum matching to identify base words, leveraging large dictionaries).", "labels": [], "entities": []}, {"text": "Yet such dictionaries are often unavailable or too expensive for low resource languages.", "labels": [], "entities": []}, {"text": "Language agnostic unsupervised options like MOR-FESSOR () and byte pair encoding (BPE) () assume no resources beyond raw text but can yield lower performance on downstream tasks ().", "labels": [], "entities": [{"text": "MOR-FESSOR", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9035633206367493}, {"text": "byte pair encoding (BPE)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.6139437705278397}]}, {"text": "They also suffer from typological biases and favor intended applications at the expense of others.", "labels": [], "entities": []}, {"text": "To this end, we present De-lexical Segmentation (DESEG), a slightly more expensive but powerful alternative to language agnostic morphological segmentation, realizing most of the benefits of supervised segmentation at far less a cost.", "labels": [], "entities": [{"text": "De-lexical Segmentation (DESEG", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.7308035343885422}, {"text": "language agnostic morphological segmentation", "start_pos": 111, "end_pos": 155, "type": "TASK", "confidence": 0.631721593439579}]}, {"text": "DE-SEG requires language specific input in the form of a small grammar describing the combinatorics of closed-class affixes.", "labels": [], "entities": [{"text": "DE-SEG", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7706840634346008}]}, {"text": "We demonstrate that such a grammar can be constructed easily and rapidly fora new language or dialect.", "labels": [], "entities": []}, {"text": "Hence, DESEG addresses the scenario in which there is no supervised segmenter available fora given language or dialect (or no segmenter trained on a domain with sufficient lexical overlap with the target domain in its training data), but the user does have linguistic knowledge of the target language/dialect.", "labels": [], "entities": [{"text": "DESEG", "start_pos": 7, "end_pos": 12, "type": "TASK", "confidence": 0.6478720903396606}]}, {"text": "The user-provided grammar is employed in conjunction with a large, raw corpus.", "labels": [], "entities": []}, {"text": "The grammar over generates analyses for all words therein, allowing for maximal recall not only of the possible affix combinations, but also variant spellings and dialectal idiosyncrasies.", "labels": [], "entities": [{"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9242504239082336}]}, {"text": "The preferred analysis is disambiguated based on the fertility with which its proposed base attaches to different affixes in analyses of other words throughout the corpus.", "labels": [], "entities": [{"text": "fertility", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9779589176177979}]}, {"text": "This follows from the logic that valid bases are more likely to productively combine with more exponents 1 ().", "labels": [], "entities": []}, {"text": "By leveraging language specific resources but learning to disambiguate empirically without supervision, we mitigate much of the sparsity inherent in processing non-standard domains.", "labels": [], "entities": []}, {"text": "Using a corpus of several Arabic dialects exhibiting rich and complex morphology, unstandardized spelling, and variation bordering on mutual unintelligibility, we evaluate DESEG intrinsically on language modeling (LM) and extrinsically on MT.", "labels": [], "entities": [{"text": "DESEG", "start_pos": 172, "end_pos": 177, "type": "TASK", "confidence": 0.7249801158905029}, {"text": "MT", "start_pos": 239, "end_pos": 241, "type": "TASK", "confidence": 0.6629796028137207}]}, {"text": "DESEG consistently outperforms MORFESSOR and BPE while only costing a few hours of grammar-building labor; and in some environments it outperforms state-of-the-art supervised Arabic tokenizers MADAMIRA () and FARASA ().", "labels": [], "entities": [{"text": "BPE", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.7817966938018799}, {"text": "FARASA", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.9856389164924622}]}, {"text": "The success of such a simple model is strong evidence for the value of linguistic input during preprocessing.", "labels": [], "entities": []}, {"text": "DESEG is publicly available at github.", "labels": [], "entities": [{"text": "DESEG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7290015816688538}]}, {"text": "com/CAMeL-Lab/deSeg.", "labels": [], "entities": [{"text": "CAMeL-Lab/deSeg", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.6307656367619833}]}], "datasetContent": [{"text": "We compare DESEG to several alternative segmentation models.", "labels": [], "entities": [{"text": "DESEG", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.4805873930454254}]}, {"text": "We use the CORPUS6 dev set to pick the optimal minimum base length on an intrinsic LM perplexity evaluation, and then perform an extrinsic MT evaluation on the test set.", "labels": [], "entities": [{"text": "CORPUS6 dev set", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.827344020207723}]}, {"text": "We conduct MT experiments translating Arabic dialects to English in three environments.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.9919015765190125}]}, {"text": "Pooled-pooled trains segmenters (only trainable segmenters) on the monolingual corpus with all dialects pooled and the MT system on all the dialects pooled.", "labels": [], "entities": []}, {"text": "Individual-individual trains six segmenters on relevant subsections of the monolingual data and six MT systems on the relevant partitions of CORPUS6.", "labels": [], "entities": [{"text": "MT", "start_pos": 100, "end_pos": 102, "type": "TASK", "confidence": 0.9387425184249878}, {"text": "CORPUS6", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9660146832466125}]}, {"text": "Individual-pooled trains individual segmenters but one pan-Arabic MT system, which is reasonable to reduce the over generation of the morphological model but leverage shared information during MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.94549560546875}, {"text": "MT", "start_pos": 193, "end_pos": 195, "type": "TASK", "confidence": 0.9750635027885437}]}, {"text": "Neural MT has been used with dialects), but given the extreme scarcity of in-domain data, statistical MT () is the better choice) for comparing quality of segmentation in our setting.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.5139399617910385}, {"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.8976258635520935}]}, {"text": "DESEG consistently outperforms unsupervised alternatives BPE and MORFESSOR in while approaching and even beating state-of-the-art systems FARASA and MADAMIRA in the individual-pooled environment.", "labels": [], "entities": [{"text": "DESEG", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.4391210377216339}, {"text": "BPE", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.5414928793907166}, {"text": "MORFESSOR", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.687387228012085}, {"text": "FARASA", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.8077030777931213}]}, {"text": "The Fertility-based model DESEG f 2 outperforms its greedy counterpart, supporting the argument that base fertility plays a meaningful role in morphological organization.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Calculating fertility in a toy Arabic corpus of three words given all possible candidate analyses of the  input corpus vocabulary. Correct analyses are depicted in bold.", "labels": [], "entities": []}, {"text": " Table 3: Out of vocabulary (OOV) and perplexity for all tokenization models in the pooled dialects environment.", "labels": [], "entities": []}, {"text": " Table 4: Macro BLEU scores for each tokenization model on CORPUS6 in three environments distinguishing how  dialects are pooled or treated separately when training the tokenizer and MT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9825038313865662}, {"text": "CORPUS6", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9406577348709106}]}]}