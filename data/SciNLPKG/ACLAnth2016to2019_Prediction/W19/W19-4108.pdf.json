{"title": [{"text": "End-to-End Neural Context Reconstruction in Chinese Dialogue", "labels": [], "entities": [{"text": "End-to-End Neural Context Reconstruction in Chinese Dialogue", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.6548867225646973}]}], "abstractContent": [{"text": "We tackle the problem of context reconstruction in Chinese dialogue, where the task is to replace pronouns, zero pronouns, and other referring expressions with their referent nouns so that sentences can be processed in isolation without context.", "labels": [], "entities": [{"text": "context reconstruction", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7719593942165375}]}, {"text": "Following a standard decomposition of the context reconstruction task into referring expression detection and coref-erence resolution, we propose a novel end-to-end architecture for separately and jointly accomplishing this task.", "labels": [], "entities": [{"text": "context reconstruction task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.8016034762064616}, {"text": "referring expression detection", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.7481022278467814}, {"text": "coref-erence resolution", "start_pos": 110, "end_pos": 133, "type": "TASK", "confidence": 0.7655785083770752}]}, {"text": "Key features of this model include POS and position encoding using CNNs and a novel pronoun masking mechanism.", "labels": [], "entities": [{"text": "position encoding", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.6698559671640396}]}, {"text": "One perennial problem in building such models is the paucity of training data, which we address by augmenting previously-proposed methods to generate a large amount of realistic training data.", "labels": [], "entities": []}, {"text": "The combination of more data and better models yields accuracy higher than the state-of-the-art method in coreference resolution and end-to-end context reconstruction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9994986057281494}, {"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.9722776412963867}, {"text": "context reconstruction", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7260846197605133}]}], "introductionContent": [{"text": "The chatbot is claimed to become a platform for the next generation of the human-computer interface.", "labels": [], "entities": []}, {"text": "Recent researches on open-domain chatting systems (, open-domain question answering systems) have shown promising results on single-round conversations.", "labels": [], "entities": [{"text": "open-domain question answering", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.5829236110051473}]}, {"text": "Meanwhile, most of these systems require the input question to be syntactically and semantically complete sentences.", "labels": [], "entities": []}, {"text": "However, due to the language nature of humans, facing more than one round of conversation, we need to tackle the problem of contextual relationship where coreference and ellipsis occur frequently in dialogues leaving the sentence incomplete.", "labels": [], "entities": []}, {"text": "The goal of context reconstruction in dialogues is to load context information from a multi-round dialogue, and remove the dependency on the previous contexts in the sentences, so that each sentence have complete and independent semantic meanings, so are answerable and processible by down-stream dialogue or question answering systems.", "labels": [], "entities": [{"text": "context reconstruction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7522867321968079}, {"text": "question answering", "start_pos": 309, "end_pos": 327, "type": "TASK", "confidence": 0.6970652490854263}]}, {"text": "In this paper, we addressed the context reconstruction problem, which includes referring expression detection and coreference resolution in the dialogue domain.", "labels": [], "entities": [{"text": "context reconstruction", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7920444011688232}, {"text": "referring expression detection", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.7592602173487345}, {"text": "coreference resolution", "start_pos": 114, "end_pos": 136, "type": "TASK", "confidence": 0.9426673948764801}]}, {"text": "We present our part-ofspeech (POS) tagging based deep neural network, including both the step-by-step models and the end-to-end model, for the detections and resolutions of coreference and ellipsis.", "labels": [], "entities": [{"text": "detections and resolutions", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.7364673018455505}]}, {"text": "Our coreference and ellipsis detection model reasons over the input sequence to detect the positions of coreference and ellipsis in the sentence.", "labels": [], "entities": [{"text": "coreference and ellipsis detection", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7085412368178368}]}, {"text": "Our resolution model ranks the candidate entities with the input sentence where coreference and/or ellipsis are annotated.", "labels": [], "entities": []}, {"text": "We also present an end-to-end detectionresolution network which consumes only the nonannotated input sentence and candidate entities.", "labels": [], "entities": []}, {"text": "Our models utilize both the syntactic and semantic information by employing word embedding, convolution layers, and Long-short-term-memory (LSTM) units.", "labels": [], "entities": []}, {"text": "Due to the lack of large wellannotated data, in this paper, we proposed a novel approach to construct annotated data in dialogue domain.", "labels": [], "entities": []}, {"text": "We summarize our contribution in this paper with three points: 1) We formulate the problem definition of context reconstruction in dialogue into one detection problem and one ranking problem and present the difference between it and traditional tasks such as pronoun and zero pronoun detection and mention candidate selection; 2) We present the analysis of the application of deep neural work for contextual resolution in dialogue, including both step-by-step and end-to-end approaches; 3) We propose away to effectively construct a huge amount of silver data for the con-text reconstruction task.", "labels": [], "entities": [{"text": "context reconstruction in dialogue", "start_pos": 105, "end_pos": 139, "type": "TASK", "confidence": 0.8161484450101852}, {"text": "pronoun and zero pronoun detection", "start_pos": 259, "end_pos": 293, "type": "TASK", "confidence": 0.7178125381469727}, {"text": "contextual resolution", "start_pos": 397, "end_pos": 418, "type": "TASK", "confidence": 0.7309315651655197}, {"text": "con-text reconstruction task", "start_pos": 568, "end_pos": 596, "type": "TASK", "confidence": 0.8067794243494669}]}], "datasetContent": [{"text": "We conduct all of our experiments on Chinese datasets.", "labels": [], "entities": [{"text": "Chinese datasets", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.8706411421298981}]}, {"text": "Note all of our models used in this pa-: Statistics of the generated CQA dataset per are language-independent.", "labels": [], "entities": [{"text": "CQA dataset", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9266037940979004}]}, {"text": "We have evaluated our models on three datasets.", "labels": [], "entities": []}, {"text": "The statistics of all datasets is shown in.", "labels": [], "entities": []}, {"text": "\u2022 CONLL2012: To get a fair comparison with the previous methods, we applied POSNet-R to the zero pronoun resolution task on the CONLL2012 benchmark dataset following and's processing methods.", "labels": [], "entities": [{"text": "CONLL2012", "start_pos": 2, "end_pos": 11, "type": "DATASET", "confidence": 0.9331116080284119}, {"text": "zero pronoun resolution", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.727899173895518}, {"text": "CONLL2012 benchmark dataset", "start_pos": 128, "end_pos": 155, "type": "DATASET", "confidence": 0.9631404081980387}]}, {"text": "Note this is the dataset annotated with the coreference of zero pronouns in a general domain and this task assumes the pre-known location of zero pronouns so we apply POSNet-R as a comparison.", "labels": [], "entities": []}, {"text": "\u2022 OntoNote (BC/TC): Since there is no known end-to-end evaluation benchmark for Chinese context reconstruction, we extracted data from the BC (broadcast conversation) and TC (telephone conversation) subsets from OnotoNote 5.0 corpus (which is the same source of CONLL2012) and build the end-to-end training and evaluation dataset for zero pronoun resolution.", "labels": [], "entities": [{"text": "Chinese context reconstruction", "start_pos": 80, "end_pos": 110, "type": "TASK", "confidence": 0.6136003732681274}, {"text": "OnotoNote 5.0 corpus", "start_pos": 212, "end_pos": 232, "type": "DATASET", "confidence": 0.8227427204449972}, {"text": "CONLL2012", "start_pos": 262, "end_pos": 271, "type": "DATASET", "confidence": 0.9503595232963562}]}, {"text": "We apply basic cleaning on the corpus such as removing the cataphoric reference and filling multiple coreferences in one sentence.", "labels": [], "entities": []}, {"text": "For each sentence with a zero pronoun, we sample one negative candidate from the last sentence and use this sentence as a context sentence.", "labels": [], "entities": []}, {"text": "\u2022 CQA: Since CONLL2012 and OntoNote are either too small to evaluate the performance of neural network or too domain-specific to provide a satiated training and evaluation on a general domain, we collected and built new training and testing set from Chinese CQA (community question answering website) websites including BaiduZhidao 1 , SosoWenwen 2 , which contains over 300,000,000 QA pairs.", "labels": [], "entities": [{"text": "CONLL2012", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.9328760504722595}]}, {"text": "We generated time, location, people and noun phrase examples.", "labels": [], "entities": []}, {"text": "Each subset is divided into the training data and the testing data at the ratio of 9:1.", "labels": [], "entities": []}, {"text": "We use this generated data to mimic the coreference in the real data and we will show this generated data contributes to both general evaluation and external assistance to a specific domain.", "labels": [], "entities": []}, {"text": "Contextual resolution on dialogue corpus requires large-scale and annotated training data.", "labels": [], "entities": [{"text": "Contextual resolution", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8022447824478149}]}, {"text": "Obtaining such a data set is the key to this problem.", "labels": [], "entities": []}, {"text": "We introduce our three-phases data generation method as follows: data collection, keywords detection, and data splitting.", "labels": [], "entities": [{"text": "data collection", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.7296777963638306}, {"text": "keywords detection", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7735918164253235}, {"text": "data splitting", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7177608758211136}]}, {"text": "Data Collection: Sentences in dialogues have the features of being short and containing only one or two entities.", "labels": [], "entities": []}, {"text": "Corpus from CQA websites fit our purpose perfectly since 1).", "labels": [], "entities": [{"text": "Corpus from CQA websites", "start_pos": 0, "end_pos": 24, "type": "DATASET", "confidence": 0.6613221764564514}]}, {"text": "these questions and answers tend to be short and precise; 2).", "labels": [], "entities": []}, {"text": "large user groups provide a huge corpus of data; 3).", "labels": [], "entities": []}, {"text": "these single round question-answering dialogues share some language features with chatting dialogues.", "labels": [], "entities": []}, {"text": "Initially, QA pairs from the internet are collected.", "labels": [], "entities": []}, {"text": "These are our raw data.", "labels": [], "entities": []}, {"text": "These raw data are mostly precise, complete, short, and independent sentences and contain no coreferences to the context.", "labels": [], "entities": []}, {"text": "Keyword Detection: First of all, we detect and label words that refer to time, location, people or noun phrases.", "labels": [], "entities": [{"text": "Keyword Detection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7009623795747757}]}, {"text": "We parse questions using the Parser to generate syntax trees annotated with POS taggings.", "labels": [], "entities": [{"text": "POS taggings", "start_pos": 76, "end_pos": 88, "type": "TASK", "confidence": 0.6061246395111084}]}, {"text": "The POS taggings provide syntactic information that helps guide the data generation rules.", "labels": [], "entities": [{"text": "POS taggings", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7354004383087158}]}, {"text": "Then, we use the Stanford named entity recognizer () to tag tokens that refer to time, location or people entities, named marked words.", "labels": [], "entities": []}, {"text": "Data Splitting: Our goal is to transform short sentences from dialogues into positive examples of coreference and ellipsis.", "labels": [], "entities": [{"text": "Data Splitting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6351019144058228}]}, {"text": "The main challenge in generating those is to identify segments that can be omitted or replaced with a pronoun so that the resulting sentence is both grammatical and natural.", "labels": [], "entities": []}, {"text": "Our method splits complete sentences into sentences that contain pronoun or zero pronoun according to the self-defined syntactic pattern: 1) Pronoun samples: Since pronouns actually refer to an entity from the context, we can reverse the process and create coreference cases by replacing entities with pronouns in sentences.", "labels": [], "entities": []}, {"text": "It is feasible also because fora certain entity type (e.g. time), the corresponding pronouns are limited.", "labels": [], "entities": []}, {"text": "2) Zero pronoun samples: For the same reason as above, the process of understanding zero pronouns could be reversed.", "labels": [], "entities": []}, {"text": "We can create ellipsis cases by omitting entities in sentences.", "labels": [], "entities": []}, {"text": "Therefore, we create ellipsis cases by deleting the marked words in the sentence directly.", "labels": [], "entities": []}, {"text": "3) Negative samples: There are two types of negative samples in this problem.", "labels": [], "entities": []}, {"text": "The first type is a sentence without generated pronoun or zero pronoun.", "labels": [], "entities": []}, {"text": "In order to provide competitive samples for training, negative examples are randomly sampled out of the whole CQA corpus.", "labels": [], "entities": [{"text": "CQA corpus", "start_pos": 110, "end_pos": 120, "type": "DATASET", "confidence": 0.9598298966884613}]}, {"text": "In addition, a number of complete sentences that contain pronouns and zero pronouns already are added.", "labels": [], "entities": []}, {"text": "It could enhance our model's ability to distinguish real coreference and \"fake\" coreference.", "labels": [], "entities": []}, {"text": "The second negative samples are the mention candidates that are not referred to.", "labels": [], "entities": []}, {"text": "We randomly sample mentions from the same session or document to make the negative samples challenging.", "labels": [], "entities": []}, {"text": "End-to-end model is tested on two datasets: the generated CQA and the extracted OntoNote.", "labels": [], "entities": [{"text": "OntoNote", "start_pos": 80, "end_pos": 88, "type": "DATASET", "confidence": 0.8762515187263489}]}, {"text": "This model is trained with the original sentence as well as the correct NP and 9 sampled negative NPs.", "labels": [], "entities": []}, {"text": "The output consists of two parts, the coreference and ellipsis detection of the sentence, and the ranking score of the mention candidate.", "labels": [], "entities": []}, {"text": "The experiment results of the end-to-end evaluation on CQA and OntoNote datasets are shown in and Table 7.", "labels": [], "entities": [{"text": "CQA", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.9415531754493713}, {"text": "OntoNote datasets", "start_pos": 63, "end_pos": 80, "type": "DATASET", "confidence": 0.832239955663681}]}, {"text": "Comparing the results of the joint model with the, we found that the endto-end model has improvements on the F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9856670200824738}]}, {"text": "We find that it is because the precision score increases while the recall score drops a little.", "labels": [], "entities": [{"text": "precision score", "start_pos": 31, "end_pos": 46, "type": "METRIC", "confidence": 0.9877544045448303}, {"text": "recall score", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.9898254871368408}]}, {"text": "This result shows that involving candidate phrase information, the ability to detect the correct coreference and ellipsis is improved.", "labels": [], "entities": []}, {"text": "Comparing to the joint  model with the POSNet-R, we found that the top 1 accuracy is slightly improved, while top 2 and top 3 accuracies are dropped.", "labels": [], "entities": [{"text": "POSNet-R", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9120213389396667}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9930679798126221}, {"text": "accuracies", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9530700445175171}]}, {"text": "The drops are expected as the position information of coreference and ellipsis are not given.", "labels": [], "entities": [{"text": "coreference", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9426682591438293}]}, {"text": "Since there is no known end-to-end Chinese context reconstruction model for the dialogue corpus, we compare POSNet with two step-by-step baselines: POSNet-D for the detection first, and's methods for the ranking next.", "labels": [], "entities": [{"text": "Chinese context reconstruction", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6495206356048584}]}, {"text": "Comparing to the joint model with the baselines, we can see that step-by-step approach will cause serious cascade error if one step cannot perform well.", "labels": [], "entities": []}, {"text": "In contrast, our model joint performs reasonably well considering the returned top 3 candidates.", "labels": [], "entities": []}, {"text": "However, to better help the down-stream natural language understanding task, we should mainly aim at transforming a sentence extracted from the dialogue corpus to an independent sentence.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.66315225760142}]}, {"text": "So accuracy at top 1 is the most important evaluation metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9993595480918884}]}, {"text": "We shows the results on OntoNote dataset in Table 7.", "labels": [], "entities": [{"text": "OntoNote dataset", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9225721061229706}]}, {"text": "From the result of these two small data sets we can see it is important to 1).", "labels": [], "entities": []}, {"text": "learn a general knowledge by pretraining on a large corpus; 2).", "labels": [], "entities": []}, {"text": "fine tune on a domain-specific dataset to get the downstream information such as common terms, common grammar, etc.", "labels": [], "entities": []}, {"text": "In addition, by looking at and 7 together, we can see that coreference detection, especially zero pronoun detection, is the bottleneck of the end-to-end context reconstruction system.", "labels": [], "entities": [{"text": "coreference detection", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9560151696205139}, {"text": "zero pronoun detection", "start_pos": 93, "end_pos": 115, "type": "TASK", "confidence": 0.7047921419143677}, {"text": "context reconstruction", "start_pos": 153, "end_pos": 175, "type": "TASK", "confidence": 0.7528727054595947}]}], "tableCaptions": [{"text": " Table 1: Statistics of the CONLL2012 and the  OntoNote datasets", "labels": [], "entities": [{"text": "CONLL2012", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.9580772519111633}, {"text": "OntoNote datasets", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9226507842540741}]}, {"text": " Table 2: Statistics of the generated CQA dataset", "labels": [], "entities": [{"text": "CQA dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9317062199115753}]}, {"text": " Table 1 and Table 2.", "labels": [], "entities": []}, {"text": " Table 3: Results of POSNet-D for referring expression  detection on CQA dataset", "labels": [], "entities": [{"text": "referring expression  detection", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.873696486155192}, {"text": "CQA dataset", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.9322783052921295}]}, {"text": " Table 4: Results of mention candidate ranking on the  CQA dataset", "labels": [], "entities": [{"text": "CQA dataset", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.9791242480278015}]}, {"text": " Table 5: Results of the end-to-end evaluation for coref- erence resolution on the CQA dataset", "labels": [], "entities": [{"text": "coref- erence resolution", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5886431783437729}, {"text": "CQA dataset", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9772916734218597}]}, {"text": " Table 6: Results of mention candidate ranking for zero  pronouns on the CONLL2012 dataset", "labels": [], "entities": [{"text": "CONLL2012 dataset", "start_pos": 73, "end_pos": 90, "type": "DATASET", "confidence": 0.9718084931373596}]}, {"text": " Table 7: Results of end-to-end zero pronoun resolution  on OntoNote dataset", "labels": [], "entities": [{"text": "end-to-end zero pronoun resolution", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.7201305329799652}, {"text": "OntoNote dataset", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.888243705034256}]}, {"text": " Table 8: Alation study of the end-to-end contexual res- olution on the CQA dataset", "labels": [], "entities": [{"text": "Alation", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.944826602935791}, {"text": "CQA dataset", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9773037731647491}]}]}