{"title": [{"text": "Learning to Explain: Answering Why-Questions via Rephrasing", "labels": [], "entities": [{"text": "Learning to Explain: Answering Why-Questions", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6823919167121252}, {"text": "Rephrasing", "start_pos": 49, "end_pos": 59, "type": "TASK", "confidence": 0.7865452170372009}]}], "abstractContent": [{"text": "Providing plausible responses to why questions is a challenging but critical goal for language based human-machine interaction.", "labels": [], "entities": []}, {"text": "Explanations are challenging in that they require many different forms of abstract knowledge and reasoning.", "labels": [], "entities": [{"text": "Explanations", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9521666169166565}]}, {"text": "Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations.", "labels": [], "entities": []}, {"text": "They are also often limited to ranking pre-existing explanation choices.", "labels": [], "entities": []}, {"text": "In our work, we contribute to the under-explored area of generating natural language explanations for general phenomena.", "labels": [], "entities": []}, {"text": "We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to generate natural language explanations.", "labels": [], "entities": []}, {"text": "We compare different training strategies and evaluate their performance using both automatic scores and human ratings.", "labels": [], "entities": []}, {"text": "We demonstrate that our strategy is sufficient to generate highly plausible explanations for general open-domain phenomena compared to other models trained on different datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Allowing machines to provide human acceptable explanations has long been a difficult task for natural language interaction.", "labels": [], "entities": []}, {"text": "In order to provide explanations, systems need to acquire sophisticated domainknowledge, conduct causal reasoning over complex set of events and over narrative chains, and apply commonsense knowledge (.", "labels": [], "entities": []}, {"text": "Past work has demonstrated that by leveraging human-curated structured knowledge bases such as WordNet or ConceptNet (), a system can learn to rank or choose between multiple plausible explanations Phenomenon The city councilmen refused the demonstrators a permit because ? Original The city councilmen feared violence.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 95, "end_pos": 102, "type": "DATASET", "confidence": 0.935197114944458}]}, {"text": "L2E-Seq2Seq (greedy): They were not allowed to march in the city.", "labels": [], "entities": []}, {"text": "L2E-Seq2Seq (beam): They did not have a permit.", "labels": [], "entities": []}, {"text": "LM-1B: They were not allowed to use the Cape Town airport.", "labels": [], "entities": []}, {"text": "L2W: It was the only thing in the city that could be done.", "labels": [], "entities": []}, {"text": "Open-Subtitle: I don't know.: We show the original Winograd schema sentence, the original offered explanation, and generated responses from our models. and reach high accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9987159967422485}]}, {"text": "Recent successes have also shown that structured knowledge is not needed if one can train a language model on a large quantity of text.", "labels": [], "entities": []}, {"text": "Such model can rank explanations based on the probability that each explanation might appear in natural text.", "labels": [], "entities": []}, {"text": "While ranking explanations is an important task, the nature of explanation is more general than this.", "labels": [], "entities": []}, {"text": "For one phenomenon, there might be many acceptable, natural, and useful explanations.", "labels": [], "entities": []}, {"text": "In our work, instead of simply ranking or choosing explanations generated by humans, we propose to advance this important domain by directly generating the explanation.", "labels": [], "entities": []}, {"text": "We measure success based on whether the generated sequence is grammatically correct and is a fluent, natural, and plausible explanation.", "labels": [], "entities": []}, {"text": "This task has two advantages.", "labels": [], "entities": []}, {"text": "First, it allows us to explore whether such a task is computationally feasible given the current learning framework.", "labels": [], "entities": []}, {"text": "Second, answering open-domain why-questions with plausible answers can make chitchat dialogue system more engaging, especially in response to \"why\" questions (which previous systems typically answer with degenerate responses such as \"I don't know\").", "labels": [], "entities": []}, {"text": "We show that simply training a language model on previously existing datasets is not enough.", "labels": [], "entities": []}, {"text": "However, by leveraging dependency parsing patterns, we are able to construct two new datasets that will allow modern neural networks to learn to generate general-domain explanations plausible to humans.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7747134268283844}]}, {"text": "These new datasets of naturally occurring self-explanations (statements with \"because\", unprompted by a question) provide excellent training signal for generating novel explanations fora given phenomenon.", "labels": [], "entities": []}, {"text": "We conduct human experiments on the important features that contribute to plausible explanations, and we describe a simple procedure that can rephrase Whyquestions into a statement so our model can also function as a single-round chitchat chatbot that can answer Why-questions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use automatic metrics to evaluate the 8 models' performance on the News Commentary dataset.", "labels": [], "entities": [{"text": "News Commentary dataset", "start_pos": 70, "end_pos": 93, "type": "DATASET", "confidence": 0.9565160870552063}]}, {"text": "Even though this is a non-overlapping held-out dataset to our news training data, it is still   within the same domain.", "labels": [], "entities": [{"text": "news training data", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.6373831331729889}]}, {"text": "We find that L2E/L2EC based models obtained higher scores across all automatic metrics in.", "labels": [], "entities": []}, {"text": "Our results also demonstrate that context matters for explanation.", "labels": [], "entities": []}, {"text": "The L2EC task models, trained on context, can generate higher quality explanations than contextfree L2E task models.", "labels": [], "entities": []}, {"text": "Ranking Explanations We evaluate the models' relative performance on generating explanations through a survey with human evaluators.", "labels": [], "entities": []}, {"text": "75 participants were recruited using Amazon's Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 37, "end_pos": 67, "type": "DATASET", "confidence": 0.9124577982085091}]}, {"text": "Each evaluator saw 10 prompts from a single dataset, and ranked 7 to 9 explanations: the original explanation extracted from the dataset and the explanations generated by different models.", "labels": [], "entities": []}, {"text": "30 participants saw prompts from our Winograd dataset, 30 participants saw prompts from News Commentary, and 15 participants saw prompts from COPA.", "labels": [], "entities": [{"text": "Winograd dataset", "start_pos": 37, "end_pos": 53, "type": "DATASET", "confidence": 0.9793512523174286}, {"text": "COPA", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.9390634894371033}]}, {"text": "We report the results of this evaluation in the Human Ranking subsection of.", "labels": [], "entities": [{"text": "Human Ranking subsection", "start_pos": 48, "end_pos": 72, "type": "DATASET", "confidence": 0.7434202829996744}]}, {"text": "Rating Explanations Ina followup survey, 60 human evaluators on AMT rated explanations generated by the L2E-Seq2Seq model with beam search and the original (between participants).", "labels": [], "entities": []}, {"text": "Ratings were from 0 (extremely bad) to 1 (extremely good) along various dimensions of explanation quality.", "labels": [], "entities": []}, {"text": "Results of this study are shown in.", "labels": [], "entities": []}, {"text": "Generated explanations overall were rated worse than human explanations, but tended to be more good than bad (\u2265 0.5) on all measures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Top are training datasets and bottom are eval- uation datasets for each task. We report the average  length of sentences for each dataset (S1 and S2 com- bined). News Commentary with context has 156.3  words on average.", "labels": [], "entities": []}, {"text": " Table 2: Example pairs from our highest performing models with the original sentence as a reference. Human  ranking score lower is better. We provide examples of especially poor-rated generations in the Appendix.", "labels": [], "entities": []}, {"text": " Table 3: We report the best per-token accuracy and  perplexity evaluated for each tuned architecture on the  L2E/L2EC validation dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9871446490287781}, {"text": "L2E/L2EC validation dataset", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.6453731179237365}]}, {"text": " Table 4: BLUE, ROGUE, METEOR are evaluated on News Commentary test data. Any model with C in the name  is evaluated with full context. Models with  \u2020 are pre-trained models from other work. Only L2E-Seq2Seq uses the  Transformer architecture, the rest LSTM. In human ranking, we report the average rank across participants. Top  ranking is 0 and lowest ranking is 1.", "labels": [], "entities": [{"text": "BLUE", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9982181191444397}, {"text": "ROGUE", "start_pos": 16, "end_pos": 21, "type": "METRIC", "confidence": 0.9784402847290039}, {"text": "METEOR", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9770484566688538}, {"text": "News Commentary test data", "start_pos": 47, "end_pos": 72, "type": "DATASET", "confidence": 0.8993048220872879}]}]}