{"title": [{"text": "A Comparison of Context-sensitive Models for Lexical Substitution", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks.", "labels": [], "entities": []}, {"text": "Embedding approaches differ as to whether and how they account for the context surrounding a word.", "labels": [], "entities": []}, {"text": "We present a comparison of different word and context representations on the task of proposing substitutes fora target word in context (lexical substitution).", "labels": [], "entities": []}, {"text": "We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word.", "labels": [], "entities": []}, {"text": "We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution.", "labels": [], "entities": []}, {"text": "This is better handled by models trained with this objective in mind, where the interdependence between word and context representations is explicitly modeled during training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Contextualized word representations model complex characteristics of word usage, and give state-of-theart performance in a variety of NLP tasks involving syntactic and semantic processing.", "labels": [], "entities": []}, {"text": "Each proposed model accounts for context in a different way depending on the underlying architecture, and might account for local or long-distance phenomena.", "labels": [], "entities": []}, {"text": "In this work, we compare different word representations on the lexical substitution (LexSub) task, which involves proposing meaning-preserving substitutes for words in specific contexts.", "labels": [], "entities": []}, {"text": "The importance of context in defining the meaning of word instances, and selecting the substitutes that best fit specific sentences, makes of the LexSub task an ideal testbed fora direct comparison of the contextualized representations built by different models.", "labels": [], "entities": []}, {"text": "We compare representations that model context in different ways: they exploit context embeddings generated within the skip-gram model, learn a generic context embedding function using a bidirectional Long Short-Term Memory (LSTM) network, or use vectors that are learned functions of the internal states of a deep bidirectional language model (biLM) ().", "labels": [], "entities": []}, {"text": "Additionally, we experiment with away to tune these state-of-the-art contextsensitive representations to sense-specific contexts of use, using a dataset of sentences containing each LexSub target word that are carefully chosen to reflect the senses of their potential substitutes.", "labels": [], "entities": []}, {"text": "We explore the impact of this tuning on the LexSub task.", "labels": [], "entities": [{"text": "LexSub", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.911566436290741}]}, {"text": "Finally, we compare the performance of contextualized models to baseline models that exploit standard word embedding representations for measuring semantic similarity without directly accounting for context, such as Glove () and FastText (.", "labels": [], "entities": []}, {"text": "The results of this study highlight the importance of the architecture used for model training in capturing information relevant for lexical substitution.", "labels": [], "entities": [{"text": "lexical substitution", "start_pos": 133, "end_pos": 153, "type": "TASK", "confidence": 0.681162416934967}]}, {"text": "We show that contextualized representations that Substitutes Sentences shoot The panther fired at the bridge and hit a truck.", "labels": [], "entities": []}, {"text": "sack (5), dismiss (1) While both he and the White House deny he was fired, Frum is so insistent on the fact that he quit on his own that it really makes you wonder.", "labels": [], "entities": [{"text": "dismiss", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9986391663551331}]}, {"text": "trainer (3), teacher (2), instructor (1), tutor As a coach, we speak and listen with the intent of helping people surface, question and reframe assumptions.", "labels": [], "entities": []}, {"text": "bus (5), carriage We hopped back onto the coach -now for the boulangerie!: Examples of manually proposed substitutes for the verb fire and the noun coach in the SemEval-2007 Lexical Substitution dataset.", "labels": [], "entities": [{"text": "SemEval-2007 Lexical Substitution dataset", "start_pos": 161, "end_pos": 202, "type": "DATASET", "confidence": 0.6645215302705765}]}, {"text": "Numbers in brackets indicate the number of annotators who proposed each substitute.", "labels": [], "entities": []}, {"text": "have been shown to be very powerful in other semantics-related tasks perform less well in the LexSub task, while others that explicitly model the inter-dependence of words and their context manage to propose the best substitutes as measured by comparing their choices to human annotations in a gold standard dataset.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the performance of the proposed models on a ranking task, where models assign scores to all candidate substitutes fora target word (S = {s 1 , s 2 , ..., s n }) according to their suitability in new contexts.", "labels": [], "entities": []}, {"text": "For evaluation, we use the dataset from the SemEval-2007 Lexical Substitution task.", "labels": [], "entities": [{"text": "SemEval-2007 Lexical Substitution task", "start_pos": 44, "end_pos": 82, "type": "TASK", "confidence": 0.5807641223073006}]}, {"text": "The full dataset consists of 2,010 sentences, 10 for each of 201 target words (nouns, verbs, adjectives and adverbs), extracted from the English Internet Corpus, and annotated by five native English speakers.", "labels": [], "entities": [{"text": "English Internet Corpus", "start_pos": 137, "end_pos": 160, "type": "DATASET", "confidence": 0.9304977258046468}]}, {"text": "Words in this lexical sample were selected to ensure variety of senses.", "labels": [], "entities": []}, {"text": "We filter the test set to preserve target words and substitutes present in PPDB 2.0 (XXL) and having a vector available in all tested models, to ensure all methods use exactly the same substitute pool per target word.", "labels": [], "entities": []}, {"text": "Target words for which none or only one substitute was left were removed.", "labels": [], "entities": []}, {"text": "The filtered test set used in our experiments includes 158 target words and 1,584 sentences.", "labels": [], "entities": []}, {"text": "The ranking performed by each model is compared to the gold ranking by means of Generalized Average Precision (GAP) (.", "labels": [], "entities": [{"text": "Generalized Average Precision (GAP)", "start_pos": 80, "end_pos": 115, "type": "METRIC", "confidence": 0.9564655522505442}]}, {"text": "GAP measures the quality of a ranking by comparing the resulting ranked list with the gold standard annotation, using substitution frequency as weights (i.e. number of annotators that suggested each substitute).", "labels": [], "entities": []}, {"text": "GAP scores range between 0 and 1.", "labels": [], "entities": [{"text": "GAP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8594854474067688}]}, {"text": "A score of 1 indicates a perfect ranking where all correct substitutes precede all incorrect ones, and high-weight substitutes precede low-weight ones.", "labels": [], "entities": [{"text": "perfect ranking", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.9391087293624878}]}, {"text": "We use the GAP implementation in .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of the substitute ranking experiment with all methods and embedding types. For AddCos  models, c refers to the size of the window.", "labels": [], "entities": []}]}