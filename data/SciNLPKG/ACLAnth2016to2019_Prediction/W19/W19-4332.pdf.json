{"title": [{"text": "Leveraging Pre-Trained Embeddings for Welsh Taggers", "labels": [], "entities": [{"text": "Welsh Taggers", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7201191484928131}]}], "abstractContent": [{"text": "While the application of word embedding models to downstream Natural Language Processing (NLP) tasks has been shown to be successful, the benefits for low-resource languages is somewhat limited due to lack of adequate data for training the models.", "labels": [], "entities": []}, {"text": "However , NLP research efforts for low-resource languages have focused on constantly seeking ways to harness pre-trained models to improve the performance of NLP systems built to process these languages without the need to re-invent the wheel.", "labels": [], "entities": []}, {"text": "One such language is Welsh and therefore, in this paper, we present the results of our experiments on learning a simple multi-task neural network model for part-of-speech and semantic tagging for Welsh using a pre-trained embedding model from Fast-Text.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.7284762859344482}]}, {"text": "Our model's performance was compared with those of the existing rule-based stand-alone taggers for part-of-speech and semantic taggers.", "labels": [], "entities": []}, {"text": "Despite its simplicity and capacity to perform both tasks simultaneously, our tagger compared very well with the existing taggers.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Welsh language can easily be classified as low resourced in the context of natural language processing because the lack of the commonly used resources in language research such as large annotated corpora as well as the standard computational tools and techniques for processing these resources.", "labels": [], "entities": []}, {"text": "There is still along way to go for Welsh, but the situation is improving.", "labels": [], "entities": [{"text": "Welsh", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.7672995924949646}]}, {"text": "For instance, Welsh is fortunate to have a fund that supports an on-going inter-disciplinary and multi-institutional project, the National Corpus of Contemporary Welsh (Corpws Cenedlaethol Cymraeg CyfoesCorCenCC) , which has been building a large- Existing Welsh part-of-speech (sections 2.1) and semantic (section 2.2) taggers produce good results, but their heavy dependence on handcrafted rules and hard-coded resources may pose a maintenance challenge in future.", "labels": [], "entities": [{"text": "National Corpus of Contemporary Welsh (Corpws Cenedlaethol Cymraeg CyfoesCorCenCC)", "start_pos": 130, "end_pos": 212, "type": "DATASET", "confidence": 0.9585667631842874}]}, {"text": "Also, considering the speed with which languages evolve, especially on the internet, and the huge amount of unannotated corpora that can be collected from the web, we urgently need a system that is capable of learning from unstructured text in order to guarantee the generalisability and scalability of tagging tools.", "labels": [], "entities": []}, {"text": "Given the potential challenges with the existing approaches and considering the similarities between the tasks of part-of-speech (POS) and semantic (SEM) annotation, we propose to train a single neural network model that can jointly learn both of the tasks.", "labels": [], "entities": []}, {"text": "We aim at requiring as little human annotation effort as possible and leveraging the linguistic patterns acquired from unsupervised language models like word embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CyTag and the CySemTagger are separate tools that use rule-based methods to achieve their results.", "labels": [], "entities": []}, {"text": "The semantic tagger relies heavily on a part-of-speech tagger to function.", "labels": [], "entities": [{"text": "semantic tagger", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7072469294071198}]}, {"text": "The key aim of this paper is to implement a tagging system that: \u2022 learns from unstructured data, \u2022 leverages available embedding models, \u2022 performs both tasks, POS and semantic tagging, simultaneously using a multi-task learning setup.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 169, "end_pos": 185, "type": "TASK", "confidence": 0.7188872694969177}]}, {"text": "As mentioned earlier in section 2.1, the instances for training the POS and semantic taggers were extracted from the manually annotated gold standard evaluation corpus that has been constructed in the CorCenCC project, i.e. the data used for the CyTag and CySemTagger development.", "labels": [], "entities": []}, {"text": "This training data comprises 611 tagged sentences (14,876 tokens) stored in eight input files that contain excerpts from a variety of existing Welsh corpora, including Kynulliad314 (Welsh Assembly proceedings), Meddalwedd15 (translations of software instructions), Kwici16 (Welsh Wikipedia articles), LERBIML17 (multi-domain spoken corpora) and some short abstracts of three additional Welsh Wikipedia articles.", "labels": [], "entities": [{"text": "Welsh Assembly proceedings)", "start_pos": 182, "end_pos": 209, "type": "DATASET", "confidence": 0.92851223051548}]}, {"text": "The fully manually checked version of the gold standard data, i.e. with the POS and SEM tags, will be released along with the multi-task model for parts-of-speech and semantic tagging.", "labels": [], "entities": [{"text": "POS and SEM tags", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.6610526964068413}, {"text": "semantic tagging", "start_pos": 167, "end_pos": 183, "type": "TASK", "confidence": 0.6782810091972351}]}, {"text": "The dataset used for training the multi-task model was built with the data instances extracted from the fully tagged version of the gold standard data.", "labels": [], "entities": []}, {"text": "These data instances do not contain unambiguous tokens (e.g. punctuation and numbers) and those categorised as unknown are removed from the training data.", "labels": [], "entities": []}, {"text": "The basic statistics from the data used in our experiment are shown in Table 1.", "labels": [], "entities": []}, {"text": "Although the data used in this experiment is comparatively smaller than what is often used by typical neural network projects, we assume it is sufficient for an exploratory research that aims to build a prototypical framework to support further developments for the Welsh language tools.", "labels": [], "entities": []}, {"text": "The key input data to our pipeline consists of the 611 sentences that are jointly annotated with the POS and semantic tags.", "labels": [], "entities": []}, {"text": "The combination of the annotation tags on the gold standard data makes it possible to extract the data in the different formats, as shown in.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.8382997910181681}]}, {"text": "However, the format used for this experiment is the last one, 3-BOTH, in which each token is tagged with a concatenation of the POS and semantic tags.", "labels": [], "entities": []}, {"text": "The extraction of the instance features for each token is carried out in two stage process which involves the chunking of the target word along with its three previous tokens (i.e. 4 words in total), as well as the vectorisation of the features.", "labels": [], "entities": []}, {"text": "The chunking process proceeds with a sliding window along the sentence, with the target word being the rightmost in the chunk.", "labels": [], "entities": []}, {"text": "The vectorisation then replaces each word in the chunk with its vector representation from a word-embedding model, forming a matrix of values that represent each training instance.", "labels": [], "entities": []}, {"text": "The label for each instance is the tag-ID i.e. a unique integer number assigned to each of the tags.", "labels": [], "entities": []}, {"text": "With the accuracy of 93.64% and the F1 of 95.06% reported previously for the CyTag, it represented the state-of-the-art in Welsh POStagging.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9996435642242432}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9997099041938782}, {"text": "CyTag", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.960985541343689}, {"text": "Welsh POStagging", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.8219099044799805}]}, {"text": "Also, although the CySemTagger did not report those specific metrics, it is currently the only semantic tagger for Welsh language that we are aware of.", "labels": [], "entities": [{"text": "CySemTagger", "start_pos": 19, "end_pos": 30, "type": "DATASET", "confidence": 0.9695369005203247}]}, {"text": "Therefore, the evaluation results from the multi-tagger builtin this experiment, which simultaneously performs both POSand SEM-tagging, were compared against these tools.", "labels": [], "entities": [{"text": "POSand SEM-tagging", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.3814880847930908}]}, {"text": "The effects of dropout regularisation and batch normalisation were examined with the previously selected parameters for vector size=100, minibatches=8 and dropout rate=30%.", "labels": [], "entities": [{"text": "dropout regularisation", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.8335627317428589}, {"text": "batch normalisation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.6922090947628021}, {"text": "minibatches", "start_pos": 137, "end_pos": 148, "type": "METRIC", "confidence": 0.9898809790611267}]}, {"text": "As shown in, the results indicate that, at the detriment of accuracy, both dropout and the batch normalisation achieved significant reductions in evaluation loss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996304512023926}]}, {"text": "Without them, the training accuracy and loss scores for the multi-task tagger are 99.23% and 0.021 respectively while the evaluation scores are 95.24% and 6.161.", "labels": [], "entities": [{"text": "training", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9236733913421631}, {"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9008417725563049}, {"text": "loss", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.8381341099739075}, {"text": "multi-task tagger", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7335125803947449}]}, {"text": "However, with only dropout, training accuracy and loss scores are 98.36% and 0.050 while those of evaluation are 94.89% and 4.880.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.924068808555603}, {"text": "loss", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.8681322336196899}]}, {"text": "Batch normalisation without dropout produced accuracy and loss scores of 95.51% and 0.144 respectively while those of evaluation produced 92.57% and 3.837 respectively.", "labels": [], "entities": [{"text": "Batch normalisation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5794420540332794}, {"text": "accuracy and loss scores", "start_pos": 45, "end_pos": 69, "type": "METRIC", "confidence": 0.7577415406703949}, {"text": "evaluation", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9647251963615417}]}, {"text": "The combination of them achieved a significant reduction in evaluation loss (2.682), but with relatively poorer accuracy scores for training (88.88%) and evaluation (86.66%).", "labels": [], "entities": [{"text": "evaluation loss", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.972930908203125}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9993095397949219}]}, {"text": "Figures 4 and 5 show that, as used in this experiment, the batch normalisation had a more regularising effect than the dropout, thereby slowing down convergence and avoiding over-fitting.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic statistics from the training data and em- bedding model used in this experiment.", "labels": [], "entities": []}, {"text": " Table 2: Parameter optimisation: Training and Evalua- tion of scores on Accuracy and Loss. Parameter values  in bold were chosen.", "labels": [], "entities": [{"text": "Training", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9714769124984741}, {"text": "Evalua- tion", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9692358374595642}, {"text": "Accuracy and Loss", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.7118059396743774}]}, {"text": " Table 4: Result summary for training and evaluation of accuracy and loss with or without dropout  and batch normalisation", "labels": [], "entities": [{"text": "Result", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8722999691963196}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9993484616279602}]}]}