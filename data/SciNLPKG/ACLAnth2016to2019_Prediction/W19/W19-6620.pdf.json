{"title": [{"text": "A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.6526622374852499}]}], "abstractContent": [{"text": "Most neural machine translation systems are built upon subword units extracted by methods such as Byte-Pair Encoding (BPE) or wordpiece.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.759939968585968}]}, {"text": "However, the choice of number of merge operations is generally made by following existing recipes.", "labels": [], "entities": []}, {"text": "In this paper , we conduct a systematic exploration on different numbers of BPE merge operations to understand how it interacts with the model architecture, the strategy to build vocabularies and the language pair.", "labels": [], "entities": [{"text": "BPE merge", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.7511278688907623}]}, {"text": "Our exploration could provide guidance for selecting proper BPE configurations in the future.", "labels": [], "entities": [{"text": "BPE", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.8184412717819214}]}, {"text": "Most prominently: we show that for LSTM-based architectures, it is necessary to experiment with a wide range of different BPE operations as there is no typical optimal BPE configuration, whereas for Transformer architectures, smaller BPE size tends to be a typically optimal choice.", "labels": [], "entities": []}, {"text": "We urge the community to make prudent choices with subword merge operations, as our experiments indicate that a sub-optimal BPE configuration alone could easily reduce the system performance by 3-4 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9987982511520386}]}], "introductionContent": [{"text": "While achieving state-of-the-art results, it is a common constraint that Neural Machine Translation (NMT) () systems are only capable of generating a closed set of The authors.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 73, "end_pos": 105, "type": "TASK", "confidence": 0.8161696592966715}]}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND. symbols.", "labels": [], "entities": []}, {"text": "Systems with large vocabulary sizes are too hard to fit onto GPU for training, as the word embedding is generally the most parameter-dense component in the NMT architecture.", "labels": [], "entities": []}, {"text": "For that reason, subword methods, such as Byte-Pair Encoding (BPE) (, are very widely used for building NMT systems.", "labels": [], "entities": []}, {"text": "The general idea of these methods is to exploit the pre-defined vocabulary space optimally by performing a minimum amount of word segmentations in the training set.", "labels": [], "entities": []}, {"text": "However, very few existing literature carefully examines what is the best practice regarding application of subword methods.", "labels": [], "entities": []}, {"text": "As hyper-parameter search is expensive, there is a tendency to simply use existing recipes.", "labels": [], "entities": []}, {"text": "This is especially true for the number of merge operations when people are using BPE, although this configuration is closely correlated with the granularity of the segmentation on the training corpus, thus having direct influence on the final system performance.", "labels": [], "entities": []}, {"text": "Prior to this work, recommended 32k BPE merge operation in their work on trustable baselines for NMT, while contradicted their study by showing that character-based models outperform 32k BPE.", "labels": [], "entities": []}, {"text": "Both of these studies are based on the LSTM-based architectures (.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, there is no work that looks into the same problem for the Transformer architecture extensively.", "labels": [], "entities": []}, {"text": "In this paper, we aim to provide guidance for this hyper-parameter choice by examining the interaction between MT system performance with the choice of BPE merge operations under the low- resource setting.", "labels": [], "entities": [{"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9882190227508545}]}, {"text": "We conjecture that lower resource systems will be more prone to the performance variance introduced by this choice, and the effect might vary with the choice of model architectures and languages.", "labels": [], "entities": []}, {"text": "To verify this, we conduct experiments with 5 different architecture setup on 4 language pairs of IWSLT 2016 dataset.", "labels": [], "entities": [{"text": "IWSLT 2016 dataset", "start_pos": 98, "end_pos": 116, "type": "DATASET", "confidence": 0.9675110379854838}]}, {"text": "In general, we discover that there is no typical optimal choice of merge operations for LSTM-based architectures, but for Transformer architectures, the optimal choice lays between 0-4k, and systems using the traditional 32k merge operations could lose as much as 4 points in BLEU score compared to the optimal choice.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 276, "end_pos": 286, "type": "METRIC", "confidence": 0.9809870421886444}]}], "datasetContent": [{"text": "Our experiments are conducted with the all the data from IWSLT 2016 shared task, covering translation of English (en) from and into Arabic (ar), Czech (cs), French (fr) and German (de).", "labels": [], "entities": [{"text": "IWSLT 2016 shared task", "start_pos": 57, "end_pos": 79, "type": "DATASET", "confidence": 0.9419225603342056}, {"text": "translation of English (en)", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.8693364361921946}]}, {"text": "As this dataset contains multiple dev and test sets, we concatenate all the dev sets into a single dev set and do the same for the test set as well.", "labels": [], "entities": []}, {"text": "To increase language coverage, we also conduct extra experiments with 6 more language pairs from the TED corpus ().", "labels": [], "entities": [{"text": "TED corpus", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.855485200881958}]}, {"text": "We use Brazilian Portuguese (pt), Hebrew (he), Russian (ru), Turkish (tr), Polish (pl) and Hungarian (hu) as our extra languages, paired with English.", "labels": [], "entities": []}, {"text": "All the data are tokenized and truecased using the accompanying script from Moses decoder () before training and applying BPE models.", "labels": [], "entities": []}, {"text": "We use subword-nmt 3 to train and apply BPE to our data.", "labels": [], "entities": []}, {"text": "Unless otherwise specified, all of our BPE models are trained on the concatenation of the source and target training corpus, i.e. the joint BPE scheme in.", "labels": [], "entities": []}, {"text": "We use SacreBLEU to compute BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9747381806373596}]}], "tableCaptions": [{"text": " Table 1: Information of the 5 architectures used for analysis. bi-dir is a boolean representing whether the encoder is bi- directional. denc, d dec and d emb are dimension of encoder, decoder and source/target word embedding, respectively. l is the  number of encoder/decoder layers. N h is the number of attention heads, while Np is the number of parameters of the model at  8k BPE merge operations.", "labels": [], "entities": []}, {"text": " Table 2: BLEU score for Transformer architectures with multiple BPE configurations. Each score is color-coded by its rank  among scores from different BPE configurations in the same row. \u03b4 is the difference between the best and worst BLEU score of  each row.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9703995585441589}, {"text": "BLEU", "start_pos": 235, "end_pos": 239, "type": "METRIC", "confidence": 0.994697093963623}]}, {"text": " Table 3: BLEU score for LSTM architectures with multiple BPE configurations. Each score is color-coded by its rank among  scores from different BPE configurations in the same row. \u03b4 is the difference between the best and worst BLEU score of each  row.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9684634804725647}, {"text": "BLEU", "start_pos": 228, "end_pos": 232, "type": "METRIC", "confidence": 0.9935677647590637}]}, {"text": " Table 5: Best and worst BLEU score with tiny-lstm and  deep-transformer for joint and separate BPE models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9990912675857544}]}, {"text": " Table 6: BLEU score for the 6 extra language pairs in  multilingual-TED dataset with deep-transformer archi- tecture.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999313235282898}]}, {"text": " Table 7: Coefficient from regression analysis and their corre- sponding standard error and p-values. f1 and f2 are source  and target type/token ratio, respectively. f3 is alignment ratio.  f4-f6 are binary features for source-side morphological type  (fusional, introflexive and agglutinative) and f7-f9 are the  same for target.", "labels": [], "entities": []}, {"text": " Table 8: BLEU score for deep-transformer architecture under high-resource setting, with multiple BPE configurations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989915490150452}]}]}