{"title": [{"text": "Guardian (2), Independent (1)", "labels": [], "entities": [{"text": "Guardian (2)", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.8935710340738297}, {"text": "Independent (1)", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.8285972625017166}]}], "abstractContent": [{"text": "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019.", "labels": [], "entities": [{"text": "Machine Translation (WMT) 2019", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.8604124287764231}]}, {"text": "Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.75652876496315}]}, {"text": "The main metric for this task is human judgment of translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.770783007144928}]}, {"text": "The task was also opened up to additional test suites to probe specific aspects of translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.9588744640350342}]}], "introductionContent": [{"text": "The Fourth Conference on Machine Translation (WMT) held at ACL 2019 1 hosts a number of shared tasks on various aspects of machine translation.", "labels": [], "entities": [{"text": "Machine Translation (WMT)", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8517949461936951}, {"text": "ACL 2019 1", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.6608721613883972}, {"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7979598939418793}]}, {"text": "This conference builds on 13 previous editions of WMT as workshops and conferences (.", "labels": [], "entities": [{"text": "WMT", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.659245491027832}]}, {"text": "This year we conducted several official tasks.", "labels": [], "entities": []}, {"text": "We report in this paper on the news and similar translation tasks.", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9608882069587708}]}, {"text": "Additional shared tasks are described in separate papers in these proceedings: \u2022 biomedical translation ( \u2022 automatic post-editing ( \u2022 metrics ( \u2022 quality estimation ( \u2022 parallel corpus filtering ( \u2022 robustness ( In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (\"constrained\" condition).", "labels": [], "entities": [{"text": "biomedical translation", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7436118721961975}, {"text": "news translation task", "start_pos": 220, "end_pos": 241, "type": "TASK", "confidence": 0.7646572887897491}]}, {"text": "We held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian.", "labels": [], "entities": []}, {"text": "New this year were Gujarati\u2194English and Kazakh\u2194English.", "labels": [], "entities": []}, {"text": "Both pose a lesser resourced data condition on challenging language pairs.", "labels": [], "entities": []}, {"text": "System outputs for each task were evaluated both automatically and manually.", "labels": [], "entities": []}, {"text": "This year the news translation task had two additional sub-tracks: an unsupervised language pair (German\u2192Czech) and a language pair not involving English (German\u2194French).", "labels": [], "entities": [{"text": "news translation task", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7486860354741415}]}, {"text": "Both sub-tracks were included into the general list of news translation submissions and are described in more detail in the corresponding subsections of Section 2.", "labels": [], "entities": [{"text": "news translation submissions", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.7331675489743551}]}, {"text": "The human evaluation (Section 3) involves asking human judges to score sentences output by anonymized systems.", "labels": [], "entities": []}, {"text": "We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered.", "labels": [], "entities": []}, {"text": "In addition, we used Mechanical Turk to collect further evaluations.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.8443259596824646}]}, {"text": "This year, the official manual evaluation metric is again based on judgments of adequacy on a 100-point scale, a method we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems.", "labels": [], "entities": []}, {"text": "The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation.", "labels": [], "entities": [{"text": "WMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.8719257116317749}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.8308209776878357}, {"text": "machine translation", "start_pos": 242, "end_pos": 261, "type": "TASK", "confidence": 0.8069436848163605}]}, {"text": "As before, all of the data, translations, and collected human judgments are publicly available.", "labels": [], "entities": []}, {"text": "We hope these datasets serve as a valuable resource for research into data-2 driven machine translation, automatic evaluation, or prediction of translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7114471644163132}, {"text": "prediction of translation", "start_pos": 130, "end_pos": 155, "type": "TASK", "confidence": 0.8257317543029785}]}, {"text": "News translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT-ComparEval (.", "labels": [], "entities": [{"text": "MT-ComparEval", "start_pos": 142, "end_pos": 155, "type": "DATASET", "confidence": 0.8687995672225952}]}, {"text": "In order to gain further insight into the performance of individual MT systems, we organized a call for dedicated \"test suites\", each focussing on some particular aspect of translation quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.9765219688415527}]}, {"text": "A brief overview of the test suites is provided in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9729273915290833}]}, {"text": "English to German assessment from the human evaluation campaign.", "labels": [], "entities": []}, {"text": "The annotator is presented with the machine translation output segment randomly selected from competing systems (anonymized) and is asked to rate the translation on a sliding scale.", "labels": [], "entities": []}, {"text": "This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year.", "labels": [], "entities": []}, {"text": "In terms of the News translation task manual evaluation, a total of 263 individual researcher accounts were involved, and 766 turker accounts.", "labels": [], "entities": [{"text": "News translation task", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7694211999575297}]}, {"text": "Researchers in the manual evaluation contributed judgments of 242,424 translations, while 487,674 translation assessment scores were submitted in total by the crowd, of which 224,046 were provided by workers who passed quality control.", "labels": [], "entities": []}, {"text": "Under ordinary circumstances, each assessed translation would correspond to a single individual scored segment.", "labels": [], "entities": []}, {"text": "However, since distinct systems can produce the same output fora particular input sentence, in previous years we were often able to take advantage of this and use a single assessment for multiple systems.", "labels": [], "entities": []}, {"text": "For example, last year we combined human assessment of identical translations produced by multiple systems and were able to getup to 17% saving in terms of evaluation resources.", "labels": [], "entities": []}, {"text": "However, since our evaluation now includes document context, deduplication of system outputs was not possible for most of the configurations run this year.", "labels": [], "entities": []}, {"text": "German-to-English (Avramidis et al., 2019) The test suite by DFKI covers 107 grammatical phenomena organized into 14 categories.", "labels": [], "entities": [{"text": "DFKI", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9213467836380005}]}, {"text": "The test suite is very closely related to the one used last year (, which allows an evaluation overtime.", "labels": [], "entities": []}, {"text": "The test suite is evaluated semi-automatically on a large set of sentences (over 25k) illustrating each of the examined phenomenon and equipped with automatic checks for anticipated good and bad translations.", "labels": [], "entities": []}, {"text": "The outputs of these checks are manually verified and refined.", "labels": [], "entities": []}, {"text": "The cross-year comparison is naturally affected by the different set of systems participating in each of the evaluations, but some trends are still observed, namely the improvement in function words, non-verbal agreement and punctuation.", "labels": [], "entities": []}, {"text": "The least improvement is seen in terminology and named entities.", "labels": [], "entities": []}, {"text": "Overall, MT system still translate on average about 25% of the tested sentences wrongly.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9695519208908081}]}, {"text": "The worst performance is seen for idioms (88% wrong) and complex German verbal grammar (72-77% wrong).", "labels": [], "entities": []}, {"text": "Specific terminology and some grammat-ical phenomena reach about 50%.", "labels": [], "entities": []}, {"text": "The paper also indicates phenomena with error rate below 10%, e.g. negation or several cases of verb conjugation.", "labels": [], "entities": [{"text": "error rate", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9642864167690277}]}], "tableCaptions": [{"text": " Table 2: Statistics of the French\u2194German dev and test sets", "labels": [], "entities": [{"text": "French\u2194German dev and test sets", "start_pos": 28, "end_pos": 59, "type": "DATASET", "confidence": 0.8166977337428502}]}, {"text": " Table 3: French\u2192German Meteor scores.", "labels": [], "entities": [{"text": "French\u2192German Meteor scores", "start_pos": 10, "end_pos": 37, "type": "DATASET", "confidence": 0.8624101519584656}]}, {"text": " Table 4: German\u2192French Meteor scores. Green cells high-", "labels": [], "entities": [{"text": "French Meteor scores", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.5613758166631063}]}, {"text": " Table 6: Model and training features frequently reported for", "labels": [], "entities": []}, {"text": " Table 8: Amount of data collected in the WMT19 manual evaluation campaign (after removal of quality control items). The", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9865663647651672}, {"text": "WMT19 manual evaluation campaign", "start_pos": 42, "end_pos": 74, "type": "DATASET", "confidence": 0.8402330726385117}]}, {"text": " Table 11: Official results of WMT19 News Translation Task. Systems ordered by DA score z-score; systems within a cluster", "labels": [], "entities": [{"text": "WMT19 News Translation Task", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.7708839029073715}, {"text": "DA", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.8983713388442993}]}, {"text": " Table 12: Official results of WMT19 German to Czech Unsupervised News Translation Task. Systems ordered by DA score", "labels": [], "entities": [{"text": "WMT19 German to Czech Unsupervised News Translation", "start_pos": 31, "end_pos": 82, "type": "TASK", "confidence": 0.5843644908496312}, {"text": "DA", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.842879056930542}]}, {"text": " Table 13: Official results of WMT19 German to French and French to German News Translation Task for which the topic was", "labels": [], "entities": [{"text": "WMT19 German to French and French to German News Translation", "start_pos": 31, "end_pos": 91, "type": "TASK", "confidence": 0.602805244922638}]}, {"text": " Table 14: Document Rating+Document Context (DR+DC) results of WMT19 News Translation Task for subset of language", "labels": [], "entities": [{"text": "WMT19 News Translation", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7658767501513163}]}, {"text": " Table 15: Segment Rating+Document Context (SR+DC)", "labels": [], "entities": [{"text": "Segment Rating+Document Context", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.8613053441047669}]}, {"text": " Table 17: Europarl v9 Parallel Corpus", "labels": [], "entities": [{"text": "Europarl v9 Parallel Corpus", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.8956682682037354}]}, {"text": " Table 18: Wiki Titles v1 Parallel Corpus", "labels": [], "entities": [{"text": "Wiki Titles v1 Parallel Corpus", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.5869897186756134}]}, {"text": " Table 19: JRC-Acquis Parallel Corpus", "labels": [], "entities": [{"text": "JRC-Acquis Parallel Corpus", "start_pos": 11, "end_pos": 37, "type": "DATASET", "confidence": 0.811516543229421}]}, {"text": " Table 20: News Commentary v14 Parallel Corpus", "labels": [], "entities": [{"text": "News Commentary v14 Parallel Corpus", "start_pos": 11, "end_pos": 46, "type": "DATASET", "confidence": 0.8541069507598877}]}, {"text": " Table 21: GNOME, Ubuntu, KDE Parallel Corpus", "labels": [], "entities": []}, {"text": " Table 22: Europarl v9 Monolingual Corpus", "labels": [], "entities": [{"text": "Europarl v9 Monolingual Corpus", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.9050718694925308}]}, {"text": " Table 23: News Crawl Monolingual Corpus", "labels": [], "entities": [{"text": "News Crawl Monolingual", "start_pos": 11, "end_pos": 33, "type": "DATASET", "confidence": 0.864702065785726}]}, {"text": " Table 24: News Commentary v14 Monolingual Corpus", "labels": [], "entities": [{"text": "News Commentary v14 Monolingual Corpus", "start_pos": 11, "end_pos": 49, "type": "DATASET", "confidence": 0.8511420726776123}]}, {"text": " Table 25: The teams that participated in the Similar Translation Task.", "labels": [], "entities": [{"text": "Similar Translation Task", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9271026253700256}]}, {"text": " Table 26: Results for Portuguese to Spanish Translation", "labels": [], "entities": [{"text": "Portuguese to Spanish Translation", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.6801561117172241}]}, {"text": " Table 27: Results for Spanish to Portuguese Translation", "labels": [], "entities": [{"text": "Spanish to Portuguese Translation", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7732674181461334}]}, {"text": " Table 28: Results for Hindi to Nepali Translation", "labels": [], "entities": [{"text": "Hindi to Nepali Translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.7665969133377075}]}, {"text": " Table 29: Results for Nepali to Hindi Translation", "labels": [], "entities": [{"text": "Nepali to Hindi Translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.8735504299402237}]}, {"text": " Table 30: Results for Czech to Polish Translation", "labels": [], "entities": [{"text": "Czech to Polish Translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6871443092823029}]}, {"text": " Table 31: Results for Polish to Czech Translation", "labels": [], "entities": [{"text": "Polish to Czech Translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5723610147833824}]}, {"text": " Table 36: Head to head comparison for English\u2192German systems", "labels": [], "entities": []}]}