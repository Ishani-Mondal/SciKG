{"title": [{"text": "To Combine or Not To Combine? A Rainbow Deep Reinforcement Learning Agent for Dialog Policy", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore state-of-the-art deep reinforcement learning methods such as prioritized experience replay, double deep Q-Networks, duel-ing network architectures, distributional learning methods for dialog policy.", "labels": [], "entities": [{"text": "dialog policy", "start_pos": 195, "end_pos": 208, "type": "TASK", "confidence": 0.8674528002738953}]}, {"text": "Our main findings show that each individual method improves the rewards and the task success rate but combining these methods in a Rainbow agent, which performs best across tasks and environments, is a non-trivial task.", "labels": [], "entities": []}, {"text": "We, therefore , provide insights about the influence of each method on the combination and how to combine them to form the Rainbow agent.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dialog system can be designed for generic purposes, e.g. smalltalk or a specific task such as finding restaurants or booking flights (.", "labels": [], "entities": []}, {"text": "This paper focuses on task-oriented dialog systems, which interact with a user to aid achieving their goals.", "labels": [], "entities": []}, {"text": "The systems have several modules which solve different subtasks () starting with natural language understanding (NLU) module.", "labels": [], "entities": [{"text": "natural language understanding (NLU)", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.7874339818954468}]}, {"text": "Its output is then passed to a belief tracking module ) that holds the state of the dialog, i.e. all relevant information provided by the user.", "labels": [], "entities": []}, {"text": "This belief state is then passed to the dialog policy module which has to decide how the system should reply.", "labels": [], "entities": []}, {"text": "Depending on the ontology of the task, e.g. the restaurant search, the size of the input space for the policy can quickly become very large.", "labels": [], "entities": []}, {"text": "Furthermore, the belief state might be wrong due to noisy inputs, e.g. the user could be misunderstood because of NLU errors or in general, language ambiguity.", "labels": [], "entities": []}, {"text": "Therefore, building such policies by hand is rather time consuming.", "labels": [], "entities": []}, {"text": "Reinforcement learning (RL) can alleviate this task by allowing to learn such policies automatically) with a user simulator such as proposed in within a task (, between task and non-task () and also in multimodal dialog systems (.", "labels": [], "entities": [{"text": "Reinforcement learning (RL", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8099348917603493}]}, {"text": "Deep RL has been proven to be successful with Deep Q-Learning (DQN) () introducing the idea of using neural networks as a Q-function approximator.", "labels": [], "entities": []}, {"text": "It has been widely used in the context of dialog policy learning.", "labels": [], "entities": [{"text": "dialog policy learning", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.8773371378580729}]}, {"text": "However according to a recent comparison) in the context of dialog policy learning, it performed worse than other RL methods such as Gaussian Process in many testing conditions.", "labels": [], "entities": [{"text": "dialog policy learning", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8818220297495524}]}, {"text": "Recently, several advances in deep RL such as distributional RL ( , dueling network architectures ( and their combination () -a Rainbow agent -have been shown to be promising for further improvements of deep RL agents in benchmark environments, e.g. Atari 2600.", "labels": [], "entities": []}, {"text": "However, it is still unclear whether these methods could advance dialog policies.", "labels": [], "entities": []}, {"text": "This paper attempts to provide insights motivated from dialog policy modeling perspectives how to use state-of-the-art deep RL methods such as prioritized experience replay (, double DQN (Van Hasselt et al., 2016), dueling network architecture, distributional learning method and how to combine them to train the Rainbow agent for dialog policy learning . Moreover, we explore the influence of each method w.r.t the resulting rewards and the number of successful dialogs, highlighting methods with the biggest and the smallest impact.", "labels": [], "entities": [{"text": "dialog policy learning", "start_pos": 331, "end_pos": 353, "type": "TASK", "confidence": 0.8631894389788309}]}], "datasetContent": [{"text": "Training and evaluation with the PyDial user simulator follows the PyDial benchmarking tasks, where each task (see Table 1) is trained on 10000 dialogs split into ten training iterations of 1000 dialogs each.", "labels": [], "entities": [{"text": "PyDial benchmarking", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.8267717063426971}]}, {"text": "We evaluate policies after each training iteration on 1000 test dialogs.", "labels": [], "entities": []}, {"text": "All of the following results were obtained by averaging over the outcome often different random seeds using the parameters described in appendix A.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Benchmark domains with #slots the user can  provide or #request from the system as well as #values  of each requestable slot (Casanueva et al., 2017).", "labels": [], "entities": []}, {"text": " Table 3: Rewards per task and agent ( 1 GP-SARSA, 2 eNAC, 3 DQN).", "labels": [], "entities": [{"text": "Rewards", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9965803027153015}]}, {"text": " Table 4: Success rates per task and agent ( 1 GP-SARSA, 2 eNAC, 3 DQN).", "labels": [], "entities": [{"text": "Success", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9900117516517639}]}, {"text": " Table 5: Success rates and rewards per domain ( 1 GP- SARSA, 2 DQN).", "labels": [], "entities": []}]}