{"title": [{"text": "Modeling Acoustic-Prosodic Cues for Word Importance Prediction in Spoken Dialogues", "labels": [], "entities": [{"text": "Word Importance Prediction in Spoken Dialogues", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.7429587543010712}]}], "abstractContent": [{"text": "Prosodic cues in conversational speech aid listeners in discerning a message.", "labels": [], "entities": []}, {"text": "We investigate whether acoustic cues in spoken dialogue can be used to identify the importance of individual words to the meaning of a conversation turn.", "labels": [], "entities": []}, {"text": "Individuals who are Deaf and Hard of Hearing often rely on real-time captions in live meetings.", "labels": [], "entities": []}, {"text": "Word error rate, a traditional metric for evaluating automatic speech recognition (ASR), fails to capture that some words are more important fora system to transcribe correctly than others.", "labels": [], "entities": [{"text": "Word error rate", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.6328277289867401}, {"text": "evaluating automatic speech recognition (ASR)", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.8027698738234383}]}, {"text": "We present and evaluate neural architectures that use acoustic features for 3-class word importance prediction.", "labels": [], "entities": [{"text": "3-class word importance prediction", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.6710463985800743}]}, {"text": "Our model performs competitively against state-of-the-art text-based word-importance prediction models, and it demonstrates particular benefits when operating on imperfect ASR output.", "labels": [], "entities": [{"text": "word-importance prediction", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.6518099009990692}, {"text": "ASR output", "start_pos": 172, "end_pos": 182, "type": "TASK", "confidence": 0.8946264982223511}]}], "introductionContent": [{"text": "Not all words are equally important to the meaning of a spoken message.", "labels": [], "entities": []}, {"text": "Identifying the importance of words is useful fora variety of tasks including text classification and summarization.", "labels": [], "entities": [{"text": "Identifying the importance of words", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8618300914764404}, {"text": "text classification", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7717784345149994}, {"text": "summarization", "start_pos": 102, "end_pos": 115, "type": "TASK", "confidence": 0.9850666522979736}]}, {"text": "Considering the relative importance of words can also be valuable when evaluating the quality of output of an automatic speech recognition (ASR) system for specific tasks, such as caption generation for Deaf and Hard of Hearing (DHH) participants in spoken meetings (.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 110, "end_pos": 144, "type": "TASK", "confidence": 0.7898516952991486}, {"text": "caption generation for Deaf and Hard of Hearing (DHH) participants in spoken meetings", "start_pos": 180, "end_pos": 265, "type": "TASK", "confidence": 0.7520183980464935}]}, {"text": "As described by, interlocutors may submit audio of individual utterances through a mobile device to a remote ASR system, with the text output appearing on an app for DHH users.", "labels": [], "entities": []}, {"text": "With ASR being applied to new tasks such as this, it is increasingly important to evaluate ASR output effectively.", "labels": [], "entities": [{"text": "ASR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9848948121070862}, {"text": "ASR", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9870414733886719}]}, {"text": "Traditional Word Error Rate (WER)-based evaluation assumes that all word transcription errors equally impact the quality of the ASR output fora user.", "labels": [], "entities": [{"text": "Word Error Rate (WER", "start_pos": 12, "end_pos": 32, "type": "METRIC", "confidence": 0.7067748725414276}, {"text": "ASR", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9620166420936584}]}, {"text": "However, this is less helpful for various applications).", "labels": [], "entities": []}, {"text": "In particular, found that metrics with differential weighting of errors based on word importance correlate better with human judgment than WER does for the automatic captioning task.", "labels": [], "entities": [{"text": "WER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.794442355632782}, {"text": "automatic captioning task", "start_pos": 156, "end_pos": 181, "type": "TASK", "confidence": 0.704390267531077}]}, {"text": "However, prior models based on text features for word importance identification face challenges when applied to conversational speech: \u2022 Difference from Formal Texts: Unlike formal texts, conversational transcripts may lack capitalization or punctuation, use informal grammatical structures, or contain disfluencies (e.g. incomplete words or edits, hesitations, repetitions), filler words, or more frequent out-of-vocabulary (and invented) words).", "labels": [], "entities": [{"text": "word importance identification", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7841985921065012}]}, {"text": "\u2022 Availability and Reliability: Text transcripts of spoken conversations require a human transcriptionist or an ASR system, but ASR transcription is not always reliable or even feasible, especially for noisy environments, nonstandard language use, or low-resource languages, etc.", "labels": [], "entities": [{"text": "ASR transcription", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.9306295812129974}]}, {"text": "While spoken messages include prosodic cues that focus a listener's attention on the most important parts of the message), such information maybe omitted from a text transcript, as in, in which the speaker pauses after \"right\" (suggesting a boundary) and uses rising intonation on \"from\" (suggesting a question).", "labels": [], "entities": []}, {"text": "Moreover, there are application scenarios where transcripts of spoken messages are not always available or fully reliable.", "labels": [], "entities": []}, {"text": "In such cases, models based on a speech signal (without a text transcript) might be preferred.", "labels": [], "entities": []}, {"text": "With this motivation, we investigate modeling acoustic-prosodic cues for predicting the importance of words to the meaning of a spoken dialogue.", "labels": [], "entities": [{"text": "predicting the importance of words to the meaning of a spoken dialogue", "start_pos": 73, "end_pos": 143, "type": "TASK", "confidence": 0.5223756035168966}]}, {"text": "Our goal is to explore the versatility of speech-based (text-independent) features for word importance modeling.", "labels": [], "entities": [{"text": "word importance modeling", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.8644123872121176}]}, {"text": "In this work, we frame the task of word importance prediction as sequence labeling and utilize a bi-directional Long ShortTerm Memory (LSTM)-based neural architecture for context modeling on speech.", "labels": [], "entities": [{"text": "word importance prediction", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.787751038869222}]}], "datasetContent": [{"text": "We utilized a portion of the Switchboard corpus () that had been manually annotated with word importance scores, as apart of the Word Importance Annotation project . That annotation covers 25,048 utterances spoken by 44 different English speakers, containing word-level timestamp information along with a numeric score (in the range of) assigned to each word from the speakers.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 29, "end_pos": 47, "type": "DATASET", "confidence": 0.8713919222354889}]}, {"text": "These numeric importance scores have three natural ordinal ranges [0 -0.3), [0.3, 0.6), [0.6, 1] that the annotators had used during the annotation to indicate the importance of a word in understanding an utterance.", "labels": [], "entities": []}, {"text": "The ordinal range represents low importance (LI), medium importance (MI) and high importance (HI) of words, respectively.", "labels": [], "entities": [{"text": "medium importance (MI) and high importance (HI)", "start_pos": 50, "end_pos": 97, "type": "METRIC", "confidence": 0.7920330437746915}]}, {"text": "Our models were trained and evaluated using this data, treating the problem as a ordinal classification problem with the labels ordered as (LI < MI < HI).", "labels": [], "entities": []}, {"text": "We created a 80%, 10% and 10% split of our data for training, validation, and testing.", "labels": [], "entities": [{"text": "validation", "start_pos": 62, "end_pos": 72, "type": "TASK", "confidence": 0.9509942531585693}]}, {"text": "The prediction performance of our model was primarily evaluated using the Root Mean Square (RMS) measure, to account for the ordinal nature of labels.", "labels": [], "entities": [{"text": "Root Mean Square (RMS) measure", "start_pos": 74, "end_pos": 104, "type": "METRIC", "confidence": 0.8683058364050729}]}, {"text": "Additionally, our evaluation includes F-score and accuracy results to measure classification performance.", "labels": [], "entities": [{"text": "F-score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9984965324401855}, {"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9992629885673523}]}, {"text": "As our baseline, we used various textbased importance prediction models trained and evaluated on the same data split, as described in Section 6.3.", "labels": [], "entities": [{"text": "textbased importance prediction", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.5894800225893656}]}, {"text": "For training, we explored various architectural parameters to find the best-working setup for our models: Our input layer of GRU-cells, used as word-based speech representation, had a dimension of 64.", "labels": [], "entities": []}, {"text": "The LSTM units, used for generating contextualized representation of a spoken word, had a dimension of 128.", "labels": [], "entities": []}, {"text": "We used the Adam optimizer with an initialized learning rate of 0.001 for training.", "labels": [], "entities": []}, {"text": "Each training batch had a maximum of 20 dialogue-turn utterances, and the model was trained until no improvement was observed in 7 consecutive iterations.", "labels": [], "entities": []}, {"text": "Tables 1, 2 and 3 summarize the performance of our models on the word importance prediction task.", "labels": [], "entities": [{"text": "word importance prediction task", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.859053447842598}]}, {"text": "The performance scores reported in the tables are the average performance across 5 different trials, to account for possible bias due to random initialization of the model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of our speech-based models on  the test data under different projection layers. Best per- forming scores highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 2: Speech feature ablation study. The minus sign  indicates the feature group removed from the model  during training. Markers ( and  \u2020) indicate the biggest  and the second-biggest change in model performance  for each metric, respectively.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of our speech-based model with a  prior text-based model, under different word error rate  conditions.", "labels": [], "entities": []}]}