{"title": [{"text": "Derivational Morphological Relations in Word Embeddings", "labels": [], "entities": [{"text": "Derivational Morphological Relations in Word Embeddings", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.7015534043312073}]}], "abstractContent": [{"text": "Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes.", "labels": [], "entities": [{"text": "Derivation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9640123248100281}]}, {"text": "In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language.", "labels": [], "entities": []}, {"text": "We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmata into deriva-tional trees.", "labels": [], "entities": []}, {"text": "For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors.", "labels": [], "entities": []}, {"text": "Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation 'bake-baker' belongs to category 'actor', and a correct clustering puts it into the same cluster as 'govern-governor').", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings area way of representing discrete words in a continuous space.", "labels": [], "entities": []}, {"text": "Embeddings are used in neural networks trained for various tasks, e.g. in neural machine translation (NMT), or can be pre-trained in various versions of language models to be used as continuous representations of words for other tasks.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.8566722373167673}]}, {"text": "One of the most popular frameworks for training word embeddings is word2vec (.", "labels": [], "entities": []}, {"text": "In this paper, we examine whether the word embeddings (trained on the whole words, not using any subword units or individual characters) capture derivational relations.", "labels": [], "entities": []}, {"text": "We do this to better understand what different neural networks represent about words and to provide abase for further development of derivational networks.", "labels": [], "entities": []}, {"text": "Derivation is a type of word-formation process which creates new words from existing ones by \u02c7 zivit Verb \"to nourish\" \u02c7 ziven\u00b4yziven\u00b4y Adj \"nourished\" o\u017eivit Verb \"to revive\" o\u017eiven\u00b4o\u017eiven\u00b4y Adj \"revived\" o\u017eivovat Verb \"to be reviving\" \u02c7 zivn\u00b4yzivn\u00b4y Adj \"nutrient\" (e.g. substrate) \u02c7 zivnost Noun \"craft\" \u02c7 zivnostn\u00edk Noun \"craftsman\" Figure 1: An excerpt from a derivational family rooted in the word \"\u02c7 zivit\" (to nourish, to feed).", "labels": [], "entities": []}, {"text": "Note that the word \"o\u017eiven\u00b4o\u017eiven\u00b4y\" (revived, rejuvenated), which can be derived from either \"o\u017eivit\" (to revive) or \"\u02c7 ziven\u00b4yziven\u00b4y\" (nourished, fed), is arbitrarily connected only to the former, in order to simplify the derivational family to a rooted tree.", "labels": [], "entities": []}, {"text": "adding, changing or deleting affixes.", "labels": [], "entities": []}, {"text": "For example, the word \"collide\" can be used as abase for deriving e.g. the words \"collider\" or \"collision\".", "labels": [], "entities": []}, {"text": "The derived word \"collision\" can be, in turn, used as abase for \"collisional\".", "labels": [], "entities": []}, {"text": "Words derived from a single root create derivational families, which can be approximated by directed acyclic graphs or (with some loss of information) trees; see for an example.", "labels": [], "entities": []}, {"text": "Derivational relations have two sides: formbased and semantic.", "labels": [], "entities": []}, {"text": "For a pair of words to be considered derivationally related, the two words must be related both by their phonological or orthographical forms and by their meaning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of different clustering methods on  differences of normalized and non-normalized word- vectors trained on Czech National Corpus and cluster- ing into 21 clusters. The results are ordered according  to V-measure.", "labels": [], "entities": [{"text": "Czech National Corpus", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.9487819075584412}, {"text": "V-measure", "start_pos": 222, "end_pos": 231, "type": "METRIC", "confidence": 0.9387301206588745}]}, {"text": " Table 3: Effect of number of clusters with K-means  (averaged over 10 runs).", "labels": [], "entities": []}, {"text": " Table 4: Results on vectors learned by the NMT models  compared to word2vec. K-means clustering with 21  clusters. The results are averaged over 10 independent  runs.", "labels": [], "entities": []}, {"text": " Table 5: Precision and recall for the derivation classes. We sampled 250 examples for each class from the data and  clustered them with K-means on word2vec embeddings trained on th\u011b  CNK. Results presented here are averaged  over 5 runs.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9707003235816956}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9985705614089966}]}]}