{"title": [{"text": "Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available.", "labels": [], "entities": [{"text": "dependency-based semantic role labeling", "start_pos": 74, "end_pos": 113, "type": "TASK", "confidence": 0.6490710079669952}]}, {"text": "Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features.", "labels": [], "entities": [{"text": "dependency parse trees", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.7745049397150675}]}, {"text": "Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features.", "labels": [], "entities": []}, {"text": "Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.", "labels": [], "entities": [{"text": "Universal Proposition Bank", "start_pos": 131, "end_pos": 157, "type": "DATASET", "confidence": 0.7365943491458893}]}], "introductionContent": [{"text": "Despite considerable efforts on developing semantically annotated resources for semantic role labeling (SRL) (, majority of languages do not have such annotated resources.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.7935864379008611}]}, {"text": "The lack of annotated resources for SRL has led to a growing interest in transfer methods for developing semantic role labeling systems.", "labels": [], "entities": [{"text": "SRL", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9824872612953186}, {"text": "semantic role labeling", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.6232380568981171}]}, {"text": "The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest.", "labels": [], "entities": []}, {"text": "Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7700041234493256}]}, {"text": "Recent work on annotation projection for SRL ( presumes the availability of accurate supervised features such as lemmas, part-of-speech (POS) tags and syntactic parse trees.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 15, "end_pos": 36, "type": "TASK", "confidence": 0.7635001540184021}, {"text": "SRL", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9555949568748474}]}, {"text": "However, this is not a realistic assumption for truly low-resource languages, for which (accurate) supervised features are hardly available.", "labels": [], "entities": []}, {"text": "This paper considers the problem of annotation projection of dependency-based SRL in a scenario for which only parallel data is available for the target language.", "labels": [], "entities": [{"text": "annotation projection of dependency-based SRL", "start_pos": 36, "end_pos": 81, "type": "TASK", "confidence": 0.760742974281311}]}, {"text": "Recent state-of-the-art SRL systems have shown a significant reliance on the predicate lemma information while in a low-resource language, a lemmatizer might not be available.", "labels": [], "entities": [{"text": "SRL", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9701574444770813}]}, {"text": "We first demonstrate that unsupervised stems can be used as an alternative to supervised lemma features.", "labels": [], "entities": []}, {"text": "We further show that we can obtain a robust and simple SRL model for the target language without relying on any explicit linguistic feature (including lemmas), either supervised or unsupervised.", "labels": [], "entities": [{"text": "SRL", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9763117432594299}]}, {"text": "We achieve this goal by changing the structure of a state-of-the-art deep SRL system () to make it independent of supervised features.", "labels": [], "entities": []}, {"text": "Our model solely rely on word and character level features in the target language.", "labels": [], "entities": []}, {"text": "The main contribution of this work is on applying annotation projection without relying on supervised features in the target language of interest.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7484979629516602}]}, {"text": "To the best of our knowledge, this is the first study that builds a cross-lingual SRL transfer model in the absence of any explicit linguistic information in the target language.", "labels": [], "entities": [{"text": "SRL transfer", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.9618797600269318}]}, {"text": "We make use of the recently released Universal Proposition Banks (: An example of annotation projection for an English-German sentence pair from the Europarl corpus (.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 149, "end_pos": 164, "type": "DATASET", "confidence": 0.9865207672119141}]}, {"text": "Supervised predicate-argument structure of the English sentence (edges on top) is generated using our supervised SRL system trained on PropBank 3 ().", "labels": [], "entities": []}, {"text": "Dashed lines in the middle show intersected word alignments from Giza++ (.", "labels": [], "entities": []}, {"text": "Dashed edges at the bottom show the projected predicate-arguments.", "labels": [], "entities": []}, {"text": "a semi-automatically annotated data that unifies the annotation scheme for all languages.", "labels": [], "entities": []}, {"text": "We show the effectiveness of our method on a range of languages, namely German, Spanish, Finnish, French, Italian, Portuguese, and Chinese.", "labels": [], "entities": []}, {"text": "We compare our model to a state-of-the-art baseline that uses a rich set of supervised features and show that our model outperforms on six out of seven languages in the Universal Proposition Banks.", "labels": [], "entities": []}, {"text": "Furthermore, for Finnish, a morphologically rich language, our model with unsupervised features improves over the model that relies on a supervised lemmatizer.", "labels": [], "entities": []}, {"text": "This paper is structured as the following: \u00a72 briefly overviews the dependency-based SRL task and annotation projection, \u00a73 describes our approach, \u00a74 shows the experimental results and analysis, \u00a75 gives overviews about the related work, and \u00a76 concludes the paper and proposes suggestions for future work.", "labels": [], "entities": [{"text": "SRL task and annotation projection", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.7511773109436035}]}], "datasetContent": [{"text": "Datasets and Tools We use English as the source language and project SRL annotations to the following languages: German, Spanish, Finnish, French, Italian, Portuguese, and Chinese.", "labels": [], "entities": []}, {"text": "We use the Europarl parallel corpus () for the European languages and a random sample of 2 million sentence pairs from the MultiUN corpus) for Chinese.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9107324282328287}, {"text": "MultiUN corpus", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.9499380886554718}]}, {"text": "We use the Giza++ tool) with its default setting for word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.7700764536857605}]}, {"text": "We run Giza++ in source-to-target and the reverse: A graphical depiction of our joint argument identification and classification model without using part-ofspeech tags, lemmas, and syntax.", "labels": [], "entities": [{"text": "argument identification and classification", "start_pos": 86, "end_pos": 128, "type": "TASK", "confidence": 0.8130947798490524}]}, {"text": "In this example, the predicate-specific encoder considers word eats as the sentence predicate and the goal is to score the assignment of argument apple with label A 0 . Our model contains three different character BiLSTMs; at the bottom, a character BiLSTM is run to acquire a character-based representation for all the words in the sentence in the absence of POS tags.", "labels": [], "entities": []}, {"text": "There are two character BiLSTMs for predicate lemma: one in the encoder level (next to the second word) to model predicate lemma in the input layer and the other in the decoder level (top left).", "labels": [], "entities": []}, {"text": "In this example, we just show one layer of BiLSTM but we use a deep BiLSTM in our experiments.", "labels": [], "entities": []}, {"text": "We implement our deep network using the Dynet library (.", "labels": [], "entities": [{"text": "Dynet library", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8900492191314697}]}, {"text": "We use the dimension of 100 for word embeddings, 50 for characters, 512 for LSTM encoders, 128 for role and lemma embeddings in the decoder, and 100 for decoder lemma embedding.", "labels": [], "entities": []}, {"text": "We pick random minibatches of size 1000 with a fixed learning rate of 0.001 for learning the parameter values with the Adam optimizer ().", "labels": [], "entities": []}, {"text": "The depth of BiLSTM network is set to one for character representation (x char ) and three for predicate-specific representations (x le , u l ).", "labels": [], "entities": []}, {"text": "Predicate Disambiguation Our model is agnostic to predicate senses but since our automatic evaluation relies on automatic predicate senses, we need a disambiguation module.", "labels": [], "entities": [{"text": "Predicate Disambiguation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7905819416046143}]}, {"text": "Predicate disambiguation systems typically contains separate classifiers for each predicate lemma).", "labels": [], "entities": [{"text": "Predicate disambiguation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9336639046669006}]}, {"text": "Since we do not have a reliable lemmatizer in the target language, we train a single classifier for all predicates.", "labels": [], "entities": []}, {"text": "We encode a sentence with a three-layer deep BiLSTM and run a softmax layer on top of each predicate to disambiguate the predicate sense of each predicate.", "labels": [], "entities": []}, {"text": "Predicate identification on the source side For projection experiments, first of all we need to identify predicates in the source language.", "labels": [], "entities": [{"text": "Predicate identification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.900039553642273}]}, {"text": "Input to our predicate identifier is the concatenation of word embedLang.", "labels": [], "entities": []}, {"text": "#Tokens #Types #Pred.", "labels": [], "entities": [{"text": "Pred", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9637899398803711}]}, {"text": "de: Sizes of the projected data.", "labels": [], "entities": []}, {"text": "ding, pre-trained fixed word embedding, POS embedding 3 , and character representation (obtained from a character BiLSTM) for every token in the sentence.", "labels": [], "entities": []}, {"text": "We use a deep BiLSTM to get the final representation for each token.", "labels": [], "entities": []}, {"text": "The ultimate predictions are made by performing an affine transform on the BiLSTM hidden output.", "labels": [], "entities": [{"text": "BiLSTM hidden output", "start_pos": 75, "end_pos": 95, "type": "DATASET", "confidence": 0.8936746716499329}]}, {"text": "Our supervised SRL system is a reimplementation of the model of.", "labels": [], "entities": [{"text": "SRL", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9617123603820801}]}, {"text": "We generate automatic English predicate senses using a system similar to the predicate disambiguation module of except that we replace the logistic regression classifier with the averaged Perceptron algorithm.", "labels": [], "entities": [{"text": "English predicate senses", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.6334032714366913}]}, {"text": "In order to comply with the Universal Proposition Bank annotation scheme, we convert the argument spans in the English PropBank v3 () to dependencybased arguments by labeling the syntactic head of each span.", "labels": [], "entities": [{"text": "English PropBank v3", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.7995559175809225}]}, {"text": "For annotation projection, we define density of alignments to find sentences with relatively-dense alignments: where l i is the length of the ith target sentence in parallel data, a j is the alignment index for the jth word in the target sentence, and I(a (i) j > 0) is an indicator fora non-NULL alignment.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8272621631622314}]}, {"text": "We prune the target sentence pairs with density less than 80% for all European languages.", "labels": [], "entities": []}, {"text": "We set this threshold to 60% for Chinese in order to obtain a comparable number of sentences to the European languages.", "labels": [], "entities": []}, {"text": "summarizes the sizes of projected datasets after applying the density filter.", "labels": [], "entities": []}, {"text": "We set the number of training epochs to 2 for all languages based on development results obtained from the English to German projections.", "labels": [], "entities": []}, {"text": "Since the original model of heavily relies on the predicate lemma information for making robust prediction, we further assess the influence of using explicit linguistic features in our model by using a) supervised lemma from the UDPipe pre-trained models, and b) unsupervised stems obtained from unsupervised morphological analyzer.", "labels": [], "entities": []}, {"text": "We use the unsupervised morphological analyzer of, and obtain morpheme classes by running Morfessor FlatCat () on the output of the analyzer.", "labels": [], "entities": []}, {"text": "We run the fixed-affix finite-state machine of () to obtain a single stem for all words including the out-of-vocabularies.", "labels": [], "entities": []}, {"text": "Results We compare our character-based approach (CModel) with three different models: 1) The crosslingual model of Aminian et al.", "labels": [], "entities": []}, {"text": "(2017) (Bootstrap) that uses a rich set of supervised features including supervised lemmas, POS tags, and dependency parse information, 2) a variant of our model that uses supervised lemmas (SLem) generated by a lemmatizer to represent predicate lemmas in the input and the decode layers, and 3) a model similar to the second model but using unsupervised stems (UStem) generated by an unsupervised morphological analyzer to represent predicate lemmas.", "labels": [], "entities": []}, {"text": "Here, we aim to asses: Results of projection experiments using our character based model (CModel) on the Universal PropBank test sets compared to different baselines: the SRL system of, SLem that shows the results of our model when supervised lemma is used and UStem that show the results of our model with unsupervised stem.", "labels": [], "entities": [{"text": "Universal PropBank test sets", "start_pos": 105, "end_pos": 133, "type": "DATASET", "confidence": 0.7873411327600479}, {"text": "UStem", "start_pos": 261, "end_pos": 266, "type": "DATASET", "confidence": 0.7489257454872131}]}, {"text": "Numbers in parenthesis show results with automatic predicate senses.", "labels": [], "entities": []}, {"text": "the effects of using different levels of explicit linguistic features ranging from fully specified supervised features to unsupervised features in our model.", "labels": [], "entities": []}, {"text": "The Bootstrap model uses an iterative bootstrapping approach by utilizing a special cost function and benefiting from a rich set of supervised lexico-syntactic features, thereby, it is considered a hard baseline.", "labels": [], "entities": []}, {"text": "Since Bootstrap has a large number of features, the model is not memory-wise scalable to our projection data sizes.", "labels": [], "entities": [{"text": "Bootstrap", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.8926546573638916}]}, {"text": "Therefore we train the Bootstrap model on a random sample of 20K sentences.", "labels": [], "entities": []}, {"text": "This number is similar to the number of sentences used in the original experiments.", "labels": [], "entities": []}, {"text": "shows labeled F-scores using both gold and automatic predicate senses on the test portion of the Universal Proposition Banks.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9770744442939758}, {"text": "Universal Proposition Banks", "start_pos": 97, "end_pos": 124, "type": "DATASET", "confidence": 0.7125349144140879}]}, {"text": "The last raw in the table shows results from the supervised SRL systems trained on the training portion of the Universal Proposition Banks for each language, thereby can servr as an upper bound for our model.", "labels": [], "entities": []}, {"text": "As shown in, our model (CModel) outperforms the Bootstrap model for all languages except French.", "labels": [], "entities": []}, {"text": "Additionally, our model performs on par to the supervised lemma and unsupervised stem models.", "labels": [], "entities": []}, {"text": "This demonstrates the power of our approach even though our model has access to fewer linguistic features in the target language.", "labels": [], "entities": []}, {"text": "Using unsupervised stems outperforms supervised lemma on all languages except Portuguese and Italian.", "labels": [], "entities": []}, {"text": "This further highlights the reliance of the model on the accuracy of lemmatizer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9995939135551453}]}, {"text": "Analysis As shown in, using automatic predicate senses leads to a significant reduction inaccuracy.", "labels": [], "entities": []}, {"text": "This degradation is caused by two reasons.", "labels": [], "entities": []}, {"text": "First, training a single classifier for all predicates in the absence of explicit predicate lemma information, and second, using unified predicate senses for all languages leads to lower precision for out-of-vocabulary words.", "labels": [], "entities": [{"text": "precision", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9966061115264893}]}, {"text": "This happens due to the fact that we cannot make use of the default sense of predicate (lemma.01).", "labels": [], "entities": []}, {"text": "Among all the languages in our experiments, French is the only language that our model underperforms the Bootstrap model.", "labels": [], "entities": []}, {"text": "Our analysis on French shows that our model has not been able to correctly predict A0 and A1 arguments in 20% and 30% of cases, and labeled them as NULL.", "labels": [], "entities": [{"text": "French", "start_pos": 16, "end_pos": 22, "type": "DATASET", "confidence": 0.9594177007675171}, {"text": "A0", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.8975446224212646}, {"text": "A1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.5123265981674194}]}], "tableCaptions": []}