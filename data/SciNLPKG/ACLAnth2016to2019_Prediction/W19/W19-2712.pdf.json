{"title": [{"text": "RST-Tace A tool for automatic comparison and evaluation of RST trees", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents RST-Tace, a tool for automatic comparison and evaluation of RST trees.", "labels": [], "entities": []}, {"text": "RST-Tace serves as an implementation of Iruskieta's comparison method, which allows trees to be compared and evaluated without the influence of decisions at lower levels in a tree in terms of four factors: constituent, attachment point, nuclearity as well as relation.", "labels": [], "entities": [{"text": "RST-Tace", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7453871369361877}]}, {"text": "RST-Tace can be used regardless of the language or the size of rhetorical trees.", "labels": [], "entities": []}, {"text": "This tool aims to measure the agreement between two annotators.", "labels": [], "entities": []}, {"text": "The result is reflected by F-measure and inter-annotator agreement.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9854108095169067}]}, {"text": "Both the comparison table and the result of the evaluation can be obtained automatically.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rhetorical Structure Theory () is intended to describe discourse structure and text organization by labeling the discourse relations that hold between elementary discourse units (EDU) or between larger spans of text.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8171737591425577}]}, {"text": "It is widely used throughout the discourse community as a theory for discourse analysis.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8116329312324524}]}, {"text": "RST is defined as the reconstruction of the author's plan from the perspective of the reader, that is to say it implies a certain subjectivity.", "labels": [], "entities": [{"text": "RST", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9029821157455444}]}, {"text": "According to this view, different annotators might very well produce different analyses, which can nonetheless be equally legitimate (.", "labels": [], "entities": []}, {"text": "However, differences in the analysis based on the legitimate scope of explication ought to be distinguished from unexpected errors or ambiguities resulting from unclear annotation guidelines.", "labels": [], "entities": []}, {"text": "In order to assess and ensure the accuracy and reliability of the annotation, it is crucial to measure the agreement between the annotators.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9989150762557983}, {"text": "reliability", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9561643004417419}]}, {"text": "Compared with other types of annotation, evaluating rhetorical structures and calculating the inter-annotator agreement are not trivial.", "labels": [], "entities": []}, {"text": "There are several challenges: 1) RST tree parsing, 2) finding an appropriate method for comparison and evaluation, 3) applying this method efficiently.", "labels": [], "entities": [{"text": "RST tree parsing", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.916987140973409}]}, {"text": "So far, Marcu's (2000) method for the comparison of RST annotations by several annotators is widely-used.", "labels": [], "entities": [{"text": "comparison of RST annotations", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.702330082654953}]}, {"text": "Building on Marcu's method, developed RSTeval in order to obtain the results of comparison automatically.", "labels": [], "entities": []}, {"text": "While being widely used, the method has also been criticized.", "labels": [], "entities": []}, {"text": "For instance, da argue that it amalgamates agreement coming from different sources, with the result that decisions at lower levels in the tree significantly affect agreement at the upper rhetorical relations in a tree (, and relations cannot be able to be compared where constituents do not coincide.", "labels": [], "entities": []}, {"text": "In this regard, proposed an evaluation method which accepts that constituents do not need to coincide in their entirety to be compared.", "labels": [], "entities": []}, {"text": "Iruskieta's method provides a qualitative description of dispersion annotation while allowing quantitative evaluation (details are introduced in section 2).", "labels": [], "entities": [{"text": "dispersion annotation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.91398024559021}]}, {"text": "Nevertheless, using this method to evaluate discourse structures manually is an extremely time-and resource-consuming task.", "labels": [], "entities": []}, {"text": "Thus, inspired by RSTeval, we have developed RST-Tace as a tool for automatic comparison and evaluation of RST trees based on Iruskieta's method.", "labels": [], "entities": []}, {"text": "17-18|26 17-18|26 10|15 11|15 This research paper focuses on the theoretical foundations of RST-Tace as well as the implementation process.", "labels": [], "entities": [{"text": "RST-Tace", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.925747275352478}]}, {"text": "In addition, an example of using RST-Tace to compare and evaluate rhetorical trees (extracted from a self-built RST treebank) by two linguists will be presented in the final section.", "labels": [], "entities": []}], "datasetContent": [{"text": "After the annotations of both RST trees have been associated, all annotation pairs are compared according to Iruskieta's method, i.e. their nuclearities (N), relations (R), constituents (C), and attachment points (A) are compared and marked as equal or non-equal.", "labels": [], "entities": []}, {"text": "These values are then used to calculate F-measure and inter-annotator agreement.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9745434522628784}]}, {"text": "RST-Tace also offers the possiblity to process a whole batch of RST tree pairs and calculate the equivalence scores and inter-annotator agreement over a whole dataset.", "labels": [], "entities": [{"text": "RST-Tace", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9067618250846863}]}, {"text": "In this section, we provide an example of using RST-Tace to compare and evaluate RST trees.", "labels": [], "entities": []}, {"text": "Extracted from RST German Learner Treebank 8 , two annotations 9 on the same German text by two linguists are compared and evaluated.", "labels": [], "entities": [{"text": "RST German Learner Treebank 8", "start_pos": 15, "end_pos": 44, "type": "DATASET", "confidence": 0.9588816881179809}]}, {"text": "A part of the two RST trees where the annotations are different is shown in; the comparison table and the results of evaluation are presented in.", "labels": [], "entities": [{"text": "RST", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9030109643936157}]}], "tableCaptions": [{"text": " Table 2: Matching table of Figure 1", "labels": [], "entities": [{"text": "Matching", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9792141318321228}]}]}