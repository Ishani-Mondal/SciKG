{"title": [{"text": "Dr.Quad at MEDIQA 2019: Towards Textual Inference and Question Entailment using contextualized representations", "labels": [], "entities": [{"text": "MEDIQA 2019", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.8112817108631134}, {"text": "Textual Inference", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7292482703924179}, {"text": "Question Entailment", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8115772902965546}]}], "abstractContent": [{"text": "This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain.", "labels": [], "entities": [{"text": "Textual Inference and Question Entailment in the Medical Domain", "start_pos": 90, "end_pos": 153, "type": "TASK", "confidence": 0.706497761938307}]}, {"text": "Our system is based on the prior work Liu et al.", "labels": [], "entities": []}, {"text": "(2019) which uses a multi-task objective function for textual en-tailment.", "labels": [], "entities": []}, {"text": "In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain.", "labels": [], "entities": [{"text": "generalizing state-of-the-art language understanding", "start_pos": 50, "end_pos": 102, "type": "TASK", "confidence": 0.6116052493453026}]}, {"text": "Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed by specialized domains such as medicine.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ACL-BioNLP 2019  shared task focuses on improving the following three tasks for medical domain: 1) Natural Language Inference (NLI) 2) Recognizing Question Entailment (RQE) and 3) Question-Answering reranking system.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE", "start_pos": 139, "end_pos": 175, "type": "TASK", "confidence": 0.7321034014225006}]}, {"text": "Our team has made submissions to all the three tasks.", "labels": [], "entities": []}, {"text": "We note that in this work we focus more on the task 1 and task 2 as improvements in these two tasks reflect directly on the task 3.", "labels": [], "entities": []}, {"text": "However, as per the shared task guidelines, we do submit one model for the task 3 to complete our submission.", "labels": [], "entities": []}, {"text": "Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding model MT-DNN (, which combines the strength of multi-task learning (MTL) and language model pre-training.", "labels": [], "entities": [{"text": "natural language understanding", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.716923455397288}]}, {"text": "MTL in deep networks has shown performance gains when related tasks are trained together resulting in better generalization to new domains.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9424746632575989}]}, {"text": "Recent works such as BERT (), ELMO () have shown * equal contribution the efficacy of learning universal language representations in providing a decent warm start to a task-specific model, by leveraging large amounts of unlabeled data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9911455512046814}, {"text": "ELMO", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8111245036125183}]}, {"text": "MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers.", "labels": [], "entities": [{"text": "MT-DNN", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8856375217437744}, {"text": "BERT", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9961590766906738}]}, {"text": "This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI (), SciTail ( and hence forms the basis of our approach.", "labels": [], "entities": [{"text": "natural language understanding tasks", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.7348747849464417}]}, {"text": "For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in \u00a72.5.", "labels": [], "entities": []}, {"text": "As discussed above, state-of-the-art models using deep neural networks have shown significant performance gains across various natural language processing (NLP) tasks.", "labels": [], "entities": []}, {"text": "However, their generalization to specialized domains such as the medical domain still remains a challenge.", "labels": [], "entities": []}, {"text": "introduce anew dataset MedNLI, a natural language inference dataset for the medical domain and show the importance of incorporating domain-specific resources.", "labels": [], "entities": [{"text": "MedNLI", "start_pos": 23, "end_pos": 29, "type": "DATASET", "confidence": 0.8243917226791382}]}, {"text": "Inspired by their observations, we explore several techniques of augmenting domain-specific features with the state-of-the-art methods.", "labels": [], "entities": []}, {"text": "We hope that the deep neural networks will help the model learn about the task itself and the domain-specific features will assist the model in tacking the issues associated with such specialized domains.", "labels": [], "entities": []}, {"text": "For instance, the medical domain has a distinct sublanguage) and it presents challenges such as abbreviations, inconsistent spellings, relationship between drugs, diseases, symptoms.", "labels": [], "entities": []}, {"text": "Our resulting models perform fairly on the unseen test data of the ACL-MediQA shared task.", "labels": [], "entities": []}, {"text": "On Task 1, our best model achieves +14.1 gain above the baseline.", "labels": [], "entities": []}, {"text": "On Task 2, our five-model ensemble achieved +12.6 gain over the baseline and for Task 3 our model achieves a a +4.9 gain.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: The results reported in the table is mean and  variance of the models averaged on 3 runs using differ- ent random seeds.", "labels": [], "entities": [{"text": "variance", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.5608006119728088}]}, {"text": " Table 3: Confusion matrix for NLI classes for Infersent  model. Rows denote the true labels and columns denote  the model predictions.", "labels": [], "entities": []}, {"text": " Table 4: NLI results on the validation set.", "labels": [], "entities": []}, {"text": " Table 6: Qualitative analysis of the outputs produced by our model. We categorize the errors into different buckets  and provide cherry-picked examples to demonstrate each category.", "labels": [], "entities": []}, {"text": " Table 7: The number of train and validation instances  in each of the categories of the RQE dataset.", "labels": [], "entities": [{"text": "RQE dataset", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9052563607692719}]}, {"text": " Table 9: Results on the RQE validation set.", "labels": [], "entities": [{"text": "RQE validation set", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.7524867256482443}]}, {"text": " Table 10: Results on the RQE test set.", "labels": [], "entities": [{"text": "RQE test set", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.8660384019215902}]}, {"text": " Table 11: Dataset statistics for re-ranking task.", "labels": [], "entities": []}, {"text": " Table 12: Accuracy for task 3 on both validation set  (top) and test set (bottom).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9985342025756836}]}]}