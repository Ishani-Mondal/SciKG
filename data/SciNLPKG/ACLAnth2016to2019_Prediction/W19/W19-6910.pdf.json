{"title": [{"text": "A Character-Level LSTM Network Model for Tokenizing the Old Irish text of the W\u00fcrzburg Glosses on the Pauline Epistles", "labels": [], "entities": [{"text": "Tokenizing the Old Irish text of the W\u00fcrzburg Glosses on the Pauline Epistles", "start_pos": 41, "end_pos": 118, "type": "DATASET", "confidence": 0.8241081283642695}]}], "abstractContent": [{"text": "This paper examines difficulties inherent in tokenization of Early Irish texts and demonstrates that a neural-network-based approach may provide a viable solution for historical texts which contain unconventional spacing and spelling anomalies.", "labels": [], "entities": [{"text": "tokenization of Early Irish texts", "start_pos": 45, "end_pos": 78, "type": "TASK", "confidence": 0.8570847630500793}]}, {"text": "Guidelines for tokenizing Old Irish text are presented and the creation of a character-level LSTM network is detailed, its accuracy assessed, and efforts at optimising its performance are recorded.", "labels": [], "entities": [{"text": "tokenizing Old Irish text", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.6837932765483856}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9988654851913452}]}, {"text": "Based on the results of this research it is expected that a character-level LSTM model may provide a viable solution for tokenization of historical texts where the use of Scriptio Continua, or alternative spacing conventions, makes the automatic separation of tokens difficult.", "labels": [], "entities": [{"text": "tokenization of historical texts", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.8732838481664658}]}], "introductionContent": [{"text": "Dating from about the middle of the 8 th century), the W\u00fcrzburg glosses on the Pauline epistles provide one of the earliest examples of Irish text contained in manuscript contemporary with the Old Irish period of \"roughly the beginning of the 8th century to the middle of the 10th century A.D.\".", "labels": [], "entities": []}, {"text": "Aside from the W\u00fcrzburg collection, the later Milan and St. Gall glosses account for the only other large collections of Irish text in manuscripts from the period.", "labels": [], "entities": [{"text": "Milan and St. Gall glosses", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.8717168569564819}]}, {"text": "As such, the contents of these glosses are of immense cultural significance, preserving some of the earliest dated writings in the language of the Irish people.", "labels": [], "entities": []}, {"text": "All three sets of glosses have been collected in the two-volume Thesaurus Palaeohibernicus, where the relatively diplomatic editing of the text has retained orthographic features and information from the original manuscript content . Along with faithful reproduction of the text, however, come faithful reproductions of anomalies in word spacing and spelling.", "labels": [], "entities": [{"text": "Thesaurus Palaeohibernicus", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.8786294460296631}]}, {"text": "Section two of this paper will detail the difficulties associated with tokenizing the W\u00fcrzburg glosses as they appear in Thesaurus Palaeohibernicus (TPH), and of tokenizing Old Irish text more generally.", "labels": [], "entities": [{"text": "tokenizing the W\u00fcrzburg", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.8131943345069885}, {"text": "Thesaurus Palaeohibernicus (TPH)", "start_pos": 121, "end_pos": 153, "type": "DATASET", "confidence": 0.6643946051597596}, {"text": "tokenizing Old Irish text", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.7895747274160385}]}, {"text": "Section three will address the existence of comparable tokenization issues in modern languages, and research which has been carried out in order to provide solutions in these areas.", "labels": [], "entities": []}, {"text": "Section four will provide a rationale for the creation of tokenization guidelines specifically for use with Old Irish text in a natural language processing (NLP) context, as well as discussing the results of an inter-annotator agreement experiment which has been carried out to assess these guidelines.", "labels": [], "entities": []}, {"text": "Finally, section five will address the creation of a character-level, long short-term memory (LSTM) based recurrent neural network (RNN) model for tokenizing Old Irish, the effects of training the model on different standards of Old Irish text, and an evaluation of its performance at the task of tokenizing the W\u00fcrzburg glosses.", "labels": [], "entities": [{"text": "tokenizing Old Irish", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.7897037466367086}]}, {"text": "variation being diachronic, \"the result of morphological development\".", "labels": [], "entities": []}, {"text": "Despite this, the text is not as orthographically consistent as readers of Modern Irish will be accustomed to, and there are certain peculiarities to be observed.", "labels": [], "entities": []}, {"text": "These peculiarities impact the potential to carryout even rudimentary pre-processing of text by conventional means for NLP purposes, and raise questions as to how different morphemes should be combined or separated to form tokens in the first place.", "labels": [], "entities": []}, {"text": "It is noted by Stifter that \"the orthography of Irish changed over the course of time ...", "labels": [], "entities": []}, {"text": "so that you may find in a manuscript one word written in Old Irish, the next in Modern Irish spelling and the third in a completely odd attempt at combining different standards\".", "labels": [], "entities": [{"text": "Old Irish", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9024842083454132}]}, {"text": "While this is more evident in later manuscripts, McCone has identified features more suggestive of Middle Irish than Old in manuscripts as early as that of the W\u00fcrzburg glosses, and there are linguistic differences evident between the three scribal hands of the W\u00fcrzburg codex, with the text of the prima manus suggesting a more archaic form of Irish than that of the second and third hands.", "labels": [], "entities": []}, {"text": "Additionally, the division of words in Old Irish manuscripts is not directly comparable to Modern Irish.", "labels": [], "entities": []}, {"text": "Instead, word separation is based on linguistic stress patterns with spaces occurring between accentual units.", "labels": [], "entities": [{"text": "word separation", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7812501788139343}]}, {"text": "In accordance with this spacing convention, \"all words which are grouped round a single chief stress and have a close syntactic connexion with each other are written as one in the manuscripts\".", "labels": [], "entities": []}, {"text": "As such it is common for conjunctions to fall together with a following verb (articfea = ar ticfea, \"[it] will come\"), for the article to fall together with a following noun (indindocb\u00e1l = ind indocb\u00e1l, \"the glorification\"), for the copula to fall together with a following predicate (isdiasom = is dia-som, \"he is God\"), as well as a variety of other combinations.", "labels": [], "entities": []}, {"text": "There are also rarer instances where separate morphemes of what maybe considered the same part of speech will be separated.", "labels": [], "entities": []}, {"text": "Take, for example, the gloss, .i. is inse \u1e45duit nit\u00fa nodnail acht ish\u00e9 not ail (Wb.", "labels": [], "entities": []}, {"text": "5b28), \"i.e. it is impossible for you (sg.); it is not you (sg.) that nourishes it, but it is it that nourishes you (sg.).\"", "labels": [], "entities": []}, {"text": "In this example the verb, ailid, \"to nourish\", is used twice, in both cases combined with the empty prefix, no, used to infix a pronoun.", "labels": [], "entities": []}, {"text": "While the infixed pronoun changes between the first usage, nodnail, \"nourishes it\", and the second, not ail, \"nourishes you\", the spacing introduced between the pronoun and the verbal root in the second instance is the more notable difference.", "labels": [], "entities": []}, {"text": "What this example demonstrates is that, not only can spacing be lacking in Old Irish manuscripts where it would be desirable to inform tokenization at the boundaries of different parts of speech, but it can also be inserted within constituent parts of a verb.", "labels": [], "entities": []}, {"text": "An automatic tokenizer capable of processing manuscript text will need, therefore, not only to introduce spacing where it does not already exist within the text, but also to remove it where it has been employed within one part of speech to separate two accentual units.", "labels": [], "entities": []}, {"text": "A final consideration, related to the previous example, should be given to the phenomenon of infixing pronouns within compound verbs in Old Irish.", "labels": [], "entities": []}, {"text": "A variety of Old Irish verbs are formed by prefixing one or more preverbal particles to a following verbal root).", "labels": [], "entities": []}, {"text": "The simple verb, beirid, \"to carry\", forms the root of the compound verbs, dobeir, \"to give\", and asbeir, \"to say\", for example.", "labels": [], "entities": []}, {"text": "refers to the preverbal particles as prepositions, this being their historical origin, however, the prepositional function of these particles is often obscured by combination with the verbal root.", "labels": [], "entities": []}, {"text": "In this sense Old Irish compound verbs might be compared to Modern English counterparts such as \"oversee\" and \"withdraw\", where the combination takes on anew sense of its own as a completely separate verb in meaning, whereby that meaning would be lost if the verbal root were to be split from the preposition element.", "labels": [], "entities": []}, {"text": "In such cases the compound verb is typically considered to be a word in its own right, rather than the combination of its constituent parts, and hence, it requires its own token.", "labels": [], "entities": []}, {"text": "This poses a minor problem as regards automatically tokenizing Irish compound verbs in that a tokenizer must not split these apart when encountered.", "labels": [], "entities": [{"text": "tokenizing Irish compound verbs", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.8380652815103531}]}, {"text": "A more challenging problem is presented, however, in the way Old Irish deals with pronouns which form the objects of these compound verbs.", "labels": [], "entities": []}, {"text": "These are infixed between the preverbal particle and the verbal root, effectively splitting what might ideally be considered a single token and requiring that another token be placed within it.", "labels": [], "entities": []}, {"text": "To exemplify this issue, where the verb mentioned above, dobeir, \"he gives\", appears with the first singular infixed pronoun, -m, it becomes dombeir, \"he gives me\".", "labels": [], "entities": []}, {"text": "Webster and Kit (1992) make the point that the \"simplicity of recognising words in English from the existence of space marks as explicit delimiters\".", "labels": [], "entities": []}, {"text": "It is, perhaps based on this same notion that H\u00f4ng claim \"a tokenizer which simply replaces blanks with word boundaries ... is already quite accurate\" for alphabetic scripts.", "labels": [], "entities": []}, {"text": "Unfortunately, for the reasons outlined above, such an approach is not necessarily feasible with Old Irish texts.", "labels": [], "entities": []}, {"text": "Before tokenization can be carried out decisions must be made regarding the treatment of issues outlined in this paper.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 7, "end_pos": 19, "type": "TASK", "confidence": 0.988381564617157}]}, {"text": "These decisions will necessarily depend on the ultimate goal of the NLP tasks for which tokenization is to take place.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.9681286215782166}]}, {"text": "It will, in any case, be necessary to decide whether to separate parts of speech which have been combined into accentual units, or to leave the manuscript spacing stand.", "labels": [], "entities": []}, {"text": "It will also be important to consider how compound verbs, especially those bearing infixed pronouns, should be tokenized.", "labels": [], "entities": []}, {"text": "The treatment of such issues, for the purposes of this paper, will be discussed further in section four.", "labels": [], "entities": []}], "datasetContent": [{"text": "In developing guidelines for tokenization Old Irish, a balance must be struck between tailoring tokens to account for the complex morphology of the language and tailoring them to account for the relative scarcity of text resources which are digitally available.", "labels": [], "entities": [{"text": "tokenization Old Irish", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.8735407988230387}]}, {"text": "The lack of a large, universally standardised, corpus of Old Irish text limits the amount of data with which to train statistical or neural network models.", "labels": [], "entities": []}, {"text": "As such, the guidelines for tokenization listed below have been developed so as to avoid creating a wide variety of infrequently occurring tokens.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9723115563392639}]}, {"text": "As such, frequently occurring affixes such as demonstrative and emphatic suffixes are always separated from preceding tokens and considered to be tokens themselves.", "labels": [], "entities": []}, {"text": "An exception to this rule is made for preverbal particles, which are instead taken to be a constituent part of a following verb.", "labels": [], "entities": []}, {"text": "While this will create a larger variety of verbal tokens, it has been shown above that the separation of these particles is not always feasible, particularly where they are compounded or reduced.", "labels": [], "entities": []}, {"text": "The case of verbs containing infixed pronouns requires particular attention.", "labels": [], "entities": []}, {"text": "These guidelines recommend treating the entire verbal complex as an individual token.", "labels": [], "entities": []}, {"text": "This will allow for verbs with infixed pronouns to be treated as morphological variants of the base verb form in part-of-speech tagging, which is necessary as the inclusion of an infixed pronoun can affect the morphology of the preverbal particle in some instances.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.6802446842193604}]}, {"text": "Thurneysen points out that \"the -o of ro, no, do, fo is lost before initial a\".", "labels": [], "entities": []}, {"text": "For example, dogn\u00edu, \"I do\", loses the -o of the preverbal particle, do, and becomes dagn\u00edu, \"I do it\", with the third person, singular, neuter pronoun, a, infixed.", "labels": [], "entities": []}, {"text": "This morphological change to the particle constitutes an alteration of the verb, and therefore would require the entry of an alternative form in a lexicon.", "labels": [], "entities": []}, {"text": "However, as this form cannot occur without the infixed pronoun which is causing it, the entire complex should betaken as being the alternate form.", "labels": [], "entities": []}, {"text": "Future work will look at part-ofspeech tagging, and the possibility of extracting infixed pronouns and tagging them separately at that stage will be explored.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7214134335517883}]}, {"text": "In the current work, however, they will be treated, as outlined above, as internalised tokens.", "labels": [], "entities": []}, {"text": "Aside from internalised tokens, the guidelines account for one more form of specialised token.", "labels": [], "entities": []}, {"text": "Where forms of a significant part of speech such as the article, the copula, or possessive pronouns occur in reduced or altered form when combined with other tokens, these forms are considered to be conjoined tokens.", "labels": [], "entities": []}, {"text": "For example, where the article is preceded by prepositions such as co, i and fri, giving rise to combined forms such as cosin, isnaib and frisna, the separated forms of the article, -sin, -snaib, and -sna are conjoined tokens.", "labels": [], "entities": []}, {"text": "Similarly, when possessive pronouns precede or follow vowels, they take on a conjoined form, with examples such as id, \"in your\" and manam (Wb.", "labels": [], "entities": []}, {"text": "17c4a), \"my soul\", containing the conjoined tokens -d and m-respectively.", "labels": [], "entities": []}, {"text": "While conjoined tokens in the guidelines are displayed with a hyphen to demonstrate their dependency on a preceding or following token, this is removed in implementation, hence, manam should be rendered m anam.", "labels": [], "entities": []}, {"text": "Aside from the token types outlined in this section and those parts of speech mentioned earlier in this paper, there are few common disagreements in editorial standards.", "labels": [], "entities": []}, {"text": "It is hoped that the guidelines outlined here will provide a reasonable baseline for measuring success in automatic tokenization, however, on the basis of varying requirements for varying tasks, a different style of tokenization maybe required, and so, alteration to these guidelines.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 116, "end_pos": 128, "type": "TASK", "confidence": 0.7972883582115173}]}, {"text": "It was determined that the text of the W\u00fcrzburg glosses (Doyle, 2018) contained fifty-two characters once all Latin text, and all editorial punctuation, commentary and brackets had been removed.", "labels": [], "entities": [{"text": "W\u00fcrzburg glosses (Doyle, 2018)", "start_pos": 39, "end_pos": 69, "type": "DATASET", "confidence": 0.937652621950422}]}, {"text": "An arbitrary, out of vocabulary character was introduced for use in padding sequences, bringing the character count to fiftythree.", "labels": [], "entities": []}, {"text": "The only remaining punctuation in the glosses occurs in abbreviations such as .i. and \u026b.", "labels": [], "entities": []}, {"text": "In these instances, the punctuation which occurs is taken to be part of the token, hence, such punctuation was not removed in pre-processing.", "labels": [], "entities": []}, {"text": "It is also noteworthy that, with the exception of some roman numerals and Latin names, all of which had been removed by this point in the processing, very few upper-case letters are used throughout the glosses.", "labels": [], "entities": []}, {"text": "The forty-one glosses utilised in the interannotator agreement experiment were removed from the corpus to be used as a test set in a later evaluation stage.", "labels": [], "entities": []}, {"text": "At this point the remaining glosses were concatenated to form a single string.", "labels": [], "entities": []}, {"text": "This string was the first of two training sets used in this experiment.", "labels": [], "entities": []}, {"text": "The second training set was drawn from texts available on CELT (F\u00e4rber, 1997).", "labels": [], "entities": [{"text": "CELT (F\u00e4rber, 1997)", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9167941808700562}]}, {"text": "Ten texts were selected which were deemed both to be reasonably long and also to be edited to a standard comparable to one another: The texts were concatenated together to form one string, and all characters were changed to lower-case.", "labels": [], "entities": []}, {"text": "A number of characters which did not transfer cleanly into UTF-8 format had to be manually corrected.", "labels": [], "entities": []}, {"text": "Other alterations included the automatic removal of editorial notes and folio information, editions which use the letter v, which does not occur in the W\u00fcrzburg character-set, were altered and the letter u was substituted in its place.", "labels": [], "entities": [{"text": "W\u00fcrzburg character-set", "start_pos": 152, "end_pos": 174, "type": "DATASET", "confidence": 0.9113404750823975}]}, {"text": "Finally, in an attempt to align the various editorial standards with the tokenization guidelines, a script was written using regular expressions to identify common preverbal particles which had been separated from a following verb and attach them to it, and similarly, to find common suffixes attached to preceding words by hyphenation and detach them.", "labels": [], "entities": []}, {"text": "This approach runs the risk of accidentally splitting genuine tokens where part of the token matches the regular expression used.", "labels": [], "entities": []}, {"text": "It would be preferable to train on a corpus where the editor had deliberately edited using this standard, however, this was the most feasible solution with the available editions.", "labels": [], "entities": []}, {"text": "With the two separate training corpora having been created, the following steps were applied to each before training on them.", "labels": [], "entities": []}, {"text": "The training corpora were sequenced into strings often.", "labels": [], "entities": []}, {"text": "For every string of eleven characters in the training corpus, the first ten characters were added to a list of training strings, and the eleventh was added to a set of associated labels.", "labels": [], "entities": []}, {"text": "Each label, therefore, is the character which directly follows the preceding string often characters.", "labels": [], "entities": []}, {"text": "Finally, each sequence of characters, and label, were converted into one-hot vectors with a length of fifty-three to account for each character.", "labels": [], "entities": []}, {"text": "Before training, ten percent of sequences and labels were set aside.", "labels": [], "entities": []}, {"text": "During the training process these were used to validate the accuracy of the model by testing it on unseen sequences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9984404444694519}]}, {"text": "This step helped to prevent overfitting of the model.", "labels": [], "entities": []}, {"text": "During the training of models, a wide variety of parameters were experimented within order to produce the best possible model.", "labels": [], "entities": []}, {"text": "At this stage training accuracy was measured using TensorFlow's built-in TensorBoard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9971311092376709}]}, {"text": "This also enabled loss to be measured over the time taken to train a given model.", "labels": [], "entities": []}, {"text": "As mentioned above, ten percent of all training sequences were split off and used to validate accuracy and loss scores by periodically testing the model-in-training on unseen sequences and labels.", "labels": [], "entities": [{"text": "accuracy and loss scores", "start_pos": 94, "end_pos": 118, "type": "METRIC", "confidence": 0.7457153424620628}]}, {"text": "At the point in training when validation loss began to increase, training was stopped in order to prevent overfitting.", "labels": [], "entities": []}, {"text": "This generally occurred at about 24 epochs when training on sequences from the first training set drawn from the glosses, and at about 8 epochs when training on the larger collection of texts of the second training set.", "labels": [], "entities": []}, {"text": "It is also notable that the accuracy for Model 1 was consistently lower than that of Model 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9997754693031311}]}, {"text": "The highest validation accuracy score for Model 1 peaked at about 36%, while that of Model 2 reached a peak of 92% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9634391665458679}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9976778626441956}]}, {"text": "These scores were not apparently affected by the training set used, and both training sets used with Model 1 reached the 92% accuracy score on the validation set.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 125, "end_pos": 139, "type": "METRIC", "confidence": 0.9787684082984924}]}, {"text": "This suggests that the task of predicting word endings only was easier for models than the task of predicting any of the potential fifty-two characters.", "labels": [], "entities": [{"text": "predicting word endings", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.8650833169619242}]}, {"text": "While an accuracy of 92% is reasonably high for an RNN trained on a limited amount of text, it should be remembered that a tokenizer built on a model with this accuracy score would insert or remove a space incorrectly about once for every ten characters in a given string.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9994605183601379}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9942733645439148}]}, {"text": "This may explain why the performance of the forward only tokenizer design was unsatisfactory.", "labels": [], "entities": []}, {"text": "In any case, the accuracy score of a model is not necessarily an accurate indicator of how well a tokenizer built on that model will work.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 17, "end_pos": 31, "type": "METRIC", "confidence": 0.9851424694061279}]}, {"text": "This is especially true in the case of tokenizers built on Model 1, where the tokenizer function ignores all character predictions other than ones which would introduce or remove a space.", "labels": [], "entities": []}, {"text": "Tokenization accuracy, therefore, needs to be measured by separate means to those described above for evaluating LSTM models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9443223476409912}]}, {"text": "For this purpose, four tokenizers were used to tokenize the forty-one glosses used in the inter-annotator agreement assessment.", "labels": [], "entities": []}, {"text": "Information regarding the model and training set used to create each tokenizer can be seen in  In order to quantify the success of each tokenizer the output of each model was compared against the work of each annotator, again using Cohen's kappa coefficient (see).", "labels": [], "entities": []}, {"text": "These results show that no tokenizer performed worse than the two human annotators working without guidelines, while the better performing tokenizers show a higher score than at least one pairing of human annotators working with guidelines.", "labels": [], "entities": []}, {"text": "This seems to suggest that a neural approach may provide a feasible solution for automatic word segmentation in unedited Old Irish texts.", "labels": [], "entities": [{"text": "automatic word segmentation in unedited Old Irish texts", "start_pos": 81, "end_pos": 136, "type": "TASK", "confidence": 0.7765954099595547}]}, {"text": "It is interesting that the best performing tokenizer (T1) was trained on the glosses themselves, rather than on a larger amount of text which has been edited to a desirable standard.", "labels": [], "entities": []}, {"text": "It maybe the case that out-of-vocabulary terminology in the glosses reduces the effectiveness of models trained on prose text.", "labels": [], "entities": []}, {"text": "Future work, therefore, will focus on applying a bootstrapping approach to tokenization of the glosses.", "labels": [], "entities": []}, {"text": "Models will be periodically trained on manually tokenized glosses and tested against this same test set until an improvement is noted over the current models.", "labels": [], "entities": []}, {"text": "It is expected also that training on a corpus of edited gloss material will increase performance, therefore, going forward, attempts will be made to improve the techniques detailed hereby training similar tokenizers on the text of the St. Gall glosses.", "labels": [], "entities": [{"text": "St. Gall glosses", "start_pos": 235, "end_pos": 251, "type": "DATASET", "confidence": 0.8584750493367513}]}, {"text": "Further improvements maybe gleaned by the addition of a simple rule-based output layer which would make sure that easily identifiable features, such as common particles, abbreviations, and initial mutations, are appropriately bounded by spacing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement Cohen's kappa  scores for each pair of annotators, and average", "labels": [], "entities": []}, {"text": " Table 2: Hyperparameters for the RNN", "labels": [], "entities": [{"text": "RNN", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.6425422430038452}]}, {"text": " Table 4: Measurement of annotators' work (A1-4)  compared against output of tokenizer models (T1-4)  using Cohen's kappa", "labels": [], "entities": []}]}