{"title": [], "abstractContent": [{"text": "This paper describes POSTECH's submission to the WMT 2019 shared task on Automatic Post-Editing (APE).", "labels": [], "entities": [{"text": "WMT 2019 shared task on Automatic Post-Editing (APE)", "start_pos": 49, "end_pos": 101, "type": "TASK", "confidence": 0.681655079126358}]}, {"text": "In this paper, we propose anew multi-source APE model by extending Transformer.", "labels": [], "entities": [{"text": "APE", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.7877242565155029}]}, {"text": "The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the en-coder and the decoder state in the decoder.", "labels": [], "entities": []}, {"text": "Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias.", "labels": [], "entities": []}, {"text": "Finally, we adopt the ensemble technique across variations of our model.", "labels": [], "entities": []}, {"text": "Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline.", "labels": [], "entities": [{"text": "WMT19 English-German APE data set", "start_pos": 19, "end_pos": 52, "type": "DATASET", "confidence": 0.9096919775009156}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9994723200798035}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9856042861938477}]}, {"text": "Our primary submission achieves-0.73 in TER and +1.49 in BLEU compared to the base-line, and ranks second among all submitted systems.", "labels": [], "entities": [{"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9991655349731445}, {"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9993709921836853}]}], "introductionContent": [{"text": "Automatic Post-Editing (APE) is the task of automatically correcting errors in a given the machine translation (MT) output to generate a better translation ( . Because APE can be regarded as a sequence-to-sequence problem, MT techniques have been previously applied to this task.", "labels": [], "entities": [{"text": "Automatic Post-Editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5586963653564453}, {"text": "MT", "start_pos": 223, "end_pos": 225, "type": "TASK", "confidence": 0.9492167234420776}]}, {"text": "Subsequently, it is only natural that neural APE has been proposed following the appearance of neural machine translation (NMT).", "labels": [], "entities": [{"text": "neural APE", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.5308559536933899}, {"text": "machine translation (NMT)", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.8284906506538391}]}, {"text": "Among the initial approaches to neural APE, a log-linear combination model) that combines bilingual and monolingual translations yielded the best results.", "labels": [], "entities": [{"text": "neural APE", "start_pos": 32, "end_pos": 42, "type": "TASK", "confidence": 0.5830424726009369}]}, {"text": "Since then, In order to leverage information from both MT outputs (mt) and its corresponding source sentences (src), a multi-encoder model () based on multi-source translation has become the prevalent approach (.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 55, "end_pos": 65, "type": "TASK", "confidence": 0.8253460228443146}]}, {"text": "Recently, with the advent of Transformer (), most of the participants in the WMT18 APE shared task proposed Transformerbased multi-encoder APE models ( . Previous multi-encoder APE models employ separate encoders for each input (src, mt), and combine their outputs in various ways: by 1) sequentially applying attention between the hidden state of the decoder and the two outputs) or 2) simply concatenating them (.", "labels": [], "entities": [{"text": "WMT18 APE shared task", "start_pos": 77, "end_pos": 98, "type": "DATASET", "confidence": 0.7927999645471573}]}, {"text": "However, these approaches seem to overlook one of the key differences between general multi-source translation and APE.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.6645852327346802}, {"text": "APE", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.6661078333854675}]}, {"text": "Because the errors mt may contain are dependent on the MT system, the encoding process for mt should reflect its relationship with the source sentence.", "labels": [], "entities": []}, {"text": "Furthermore, we believe that it would be helpful to incorporate information from the source sentence, which should ideally be error-free, in addition to the jointly encoded mt in generating post-edited sentence.", "labels": [], "entities": []}, {"text": "From these points of view, we propose a multisource APE model by extending Transformer to contain a joint multi-source encoder and a decoder that involves a multi-source attention layer to combine the outputs of the encoder.", "labels": [], "entities": [{"text": "multisource APE", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.5047315657138824}]}, {"text": "Apart from that, we apply various teacher-forcing ratios at training time to alleviate exposure bias.", "labels": [], "entities": []}, {"text": "Finally, we ensemble model variants for our submission.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 describes our model architecture.", "labels": [], "entities": []}, {"text": "Section 3 summarizes the experimental results, and Section 4 gives the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the WMT19 official English-German APE dataset (Chatterjee et al., 2018) which consists of a training and development set.", "labels": [], "entities": [{"text": "WMT19 official English-German APE dataset", "start_pos": 12, "end_pos": 53, "type": "DATASET", "confidence": 0.9059196233749389}]}, {"text": "In addition, we adopted the eSCAPE NMT dataset ( ) as additional training data.", "labels": [], "entities": [{"text": "eSCAPE NMT dataset", "start_pos": 28, "end_pos": 46, "type": "DATASET", "confidence": 0.9277388453483582}]}, {"text": "We extracted sentence triplets from the eSCAPE-NMT dataset according to the following criteria, to which the official training dataset mostly adheres.", "labels": [], "entities": [{"text": "eSCAPE-NMT dataset", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.9763211607933044}]}, {"text": "Selected triplets have no more than 70 words in each sentence, a TER less than or equal to 75, and a reciprocal length ratio within the monolingual pair (mt, pe) less than 1.4.", "labels": [], "entities": [{"text": "TER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9986937642097473}, {"text": "reciprocal length ratio", "start_pos": 101, "end_pos": 124, "type": "METRIC", "confidence": 0.7829166054725647}]}, {"text": "summarizes the statistic of the datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset statistics -number of sentence tri- plets (src, mt, pe) and TER score.", "labels": [], "entities": [{"text": "TER score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9829109907150269}]}, {"text": " Table 2: Results of training variants -the columns correspond to their architectures and the rows correspond to  their teacher-forcing ratios. The bold values indicate the best result in the metrics for each architecture. \"w/o  tuning\" refer to generic model.", "labels": [], "entities": []}, {"text": " Table 3: Results of ensemble models -\"Submission Name\" indicates the names (types) for the submission.", "labels": [], "entities": []}]}