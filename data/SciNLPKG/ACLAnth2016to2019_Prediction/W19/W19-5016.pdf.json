{"title": [{"text": "Constructing large scale biomedical knowledge bases from scratch with rapid annotation of interpretable patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowledge base construction is crucial for summarising, understanding and inferring relationships between biomedical entities.", "labels": [], "entities": [{"text": "Knowledge base construction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5892857611179352}, {"text": "summarising, understanding and inferring relationships between biomedical entities", "start_pos": 43, "end_pos": 125, "type": "TASK", "confidence": 0.5802713599469926}]}, {"text": "However , for many practical applications such as drug discovery, the scarcity of relevant facts (e.g. gene X is therapeutic target for disease Y) severely limits a domain expert's ability to create a usable knowledge base, either directly or by training a relation extraction model.", "labels": [], "entities": [{"text": "drug discovery", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.725901797413826}, {"text": "relation extraction", "start_pos": 257, "end_pos": 276, "type": "TASK", "confidence": 0.7472513318061829}]}, {"text": "In this paper, we present a simple and effective method of extracting new facts with a pre-specified binary relationship type from the biomedical literature, without requiring any training data or hand-crafted rules.", "labels": [], "entities": []}, {"text": "Our system discovers, ranks and presents the most salient patterns to domain experts in an inter-pretable form.", "labels": [], "entities": []}, {"text": "By marking patterns as compatible with the desired relationship type, experts indirectly batch-annotate candidate pairs whose relationship is expressed with such patterns in the literature.", "labels": [], "entities": []}, {"text": "Even with a complete absence of seed data, experts are able to discover thousands of high-quality pairs with the desired relationship within minutes.", "labels": [], "entities": []}, {"text": "When a small number of relevant pairs do exist-even when their relationship is more general (e.g. gene X is biologically associated with disease Y) than the relationship of interest-our system leverages them in order to i) learn a better ranking of the patterns to be annotated or ii) generate weakly labelled pairs in a fully automated manner.", "labels": [], "entities": []}, {"text": "We evaluate our method both intrinsically and via a downstream knowledge * Equal contribution.", "labels": [], "entities": []}, {"text": "Theo-dosia proposed and coordinated the research project, built the early prototypes and contributed the different methods for extracting and lexicalising patterns.", "labels": [], "entities": []}, {"text": "Ashok provided conceptual work on the metrics for ranking simplifications and for the intrinsic evaluation, developed the simplification extraction module, ran the experiments for the automated workflow (with all the parameter variations) and performed all the ex-trinsic evaluations.", "labels": [], "entities": [{"text": "simplification extraction", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.821250706911087}]}, {"text": "Julien was mainly responsible for the system architecture and workflow, the intrinsic evaluation (in-cluding interacting with the experts), handling negation and speculation and the clustering algorithm.", "labels": [], "entities": []}, {"text": "base completion task, and show that it is an effective way of constructing knowledge bases when few or no relevant facts are already available .", "labels": [], "entities": []}], "introductionContent": [{"text": "In many important biomedical applications, experts seek to extract facts that are often complex and tied to particular tasks, hence data that are truly fit for purpose are scarce or simply nonexistent.", "labels": [], "entities": []}, {"text": "Even when only binary relations are sought, useful facts tend to be more specific (e.g. mutation of gene X has a causal effect on disease Y in an animal model) than associations typically found in widely available knowledge bases.", "labels": [], "entities": []}, {"text": "Extracting facts with a pre-specified relationship type from the literature in the absence of training data often relies on handcrafted rules, which are laborious, ad-hoc and hardly reusable for other types of relations.", "labels": [], "entities": [{"text": "Extracting facts", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9044896364212036}]}, {"text": "Recent attempts to create relational data from scratch by denoising the output of multiple hand-written rules or by augmenting existing data through the induction of new black-box heuristics are still dependent on ad-hoc human effort or preexisting data.", "labels": [], "entities": []}, {"text": "Our approach involves discovering and recommending, rather than prescribing, rules.", "labels": [], "entities": []}, {"text": "Importantly, our rules are presented as text-like patterns whose meaning is transparent to human annotators, enabling integration of an automatic data generation (or augmentation) system with a domain expert feedback loop.", "labels": [], "entities": []}, {"text": "In this work, we make the following contributions: \u2022 We propose a number of methods for extracting patterns from a sentence in which two eligible entities co-occur; different types of patterns have different trade-offs between expressive power and coverage.", "labels": [], "entities": []}, {"text": "\u2022 We propose a simple method for presenting patterns in a readable way, enabling faster, more reliable human annotation \u2022 For cases where a small number of seed pairs are already available, we propose a method which utilises these seed pairs to rank newly discovered patterns in terms of their compatibility with the existing data.", "labels": [], "entities": []}, {"text": "The resulting patterns can be used with or without a human in the loop.", "labels": [], "entities": []}, {"text": "The rest of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes some related work.", "labels": [], "entities": []}, {"text": "Section 3 explains the relationship between patterns and labelling rules and presents some pattern types along with techniques for rendering them interpretable.", "labels": [], "entities": []}, {"text": "Section 4 provides a high-level overview of the system and covers details of our different workflows (with and without seed data; with and without human feedback).", "labels": [], "entities": []}, {"text": "Section 5 explains how we measure the system's performance both intrinsically and via a downstream knowledge base completion task.", "labels": [], "entities": []}, {"text": "In section 6, we report the details of our main experiments while in sections 7 and 8 we present some analysis along with further experiments.", "labels": [], "entities": []}, {"text": "The paper ends with conclusions and proposals for further work in section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implement two evaluation frameworks.", "labels": [], "entities": []}, {"text": "The first is an intrinsic evaluation of the quality of the new extracted pairs.", "labels": [], "entities": []}, {"text": "The second is extrinsic; we consider how the inclusion of the new pairs discovered by our system affects the performance of a downstream knowledge base completion task.", "labels": [], "entities": []}, {"text": "*  Pair-level Our aim in this subsection is to construct an intrinsic evaluation framework which can directly measure the quality of the discovered pairs.", "labels": [], "entities": []}, {"text": "We do this by holding out a fraction of the gold standard positive pairs and the negative pairs (under the closed world assumption) to be used as a test set.", "labels": [], "entities": []}, {"text": "The remaining fraction is used as training data.", "labels": [], "entities": []}, {"text": "We evaluate our system by measuring its recall, specificity (true negative rate), precision, and F-score against this test set.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9995792508125305}, {"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.999281108379364}, {"text": "F-score", "start_pos": 97, "end_pos": 104, "type": "METRIC", "confidence": 0.9996415376663208}]}, {"text": "In more detail, the new pairs discovered by our selected simplifications are taken to be the positive pairs predicted by our system.", "labels": [], "entities": []}, {"text": "The overlap between these new pairs and the positive test set are the true positives (T P ) while the overlap with the negative test set are the false positives (F P ).", "labels": [], "entities": []}, {"text": "Recall and specificity take their standard definitions.", "labels": [], "entities": []}, {"text": "Again, we consider a precision score which is normalised to correct for the imbalance in numbers between positive and negative pairs, precision = |T P |/N P |T P |/N P +|F P |/N N , where NP and N N are the numbers of positive and negative pairs respectively present in the test set (as described in section 4.2).", "labels": [], "entities": [{"text": "precision score", "start_pos": 21, "end_pos": 36, "type": "METRIC", "confidence": 0.983967125415802}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9994431138038635}]}, {"text": "We take the F-score to be the harmonic mean of this precision variant and recall.", "labels": [], "entities": [{"text": "F-score", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9953552484512329}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9990561604499817}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9978651404380798}]}, {"text": "Simplification-level For the manual workflows, we also consider the expert annotations while assessing the quality of the simplifications.", "labels": [], "entities": []}, {"text": "We report M SP , the manual simplification precision, based on NY es , N No and NM aybe , the number of simplifications that the expert has annotated as \"Yes\", \"No\" and \"Maybe\".", "labels": [], "entities": [{"text": "M SP", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.8663930892944336}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.8316922783851624}, {"text": "NY es", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.8960568904876709}]}, {"text": "We expect MSP to be as high as possible.", "labels": [], "entities": [{"text": "MSP", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5674242973327637}]}, {"text": "Extrinsic evaluation via knowledge base completion The setup for our extrinsic evaluation framework is straightforward and intuitive.", "labels": [], "entities": []}, {"text": "The initial gold standard set of positive pairs is split into training and test data.", "labels": [], "entities": []}, {"text": "A graph completion model is then trained using the training data and evaluated to determine whether it can predict the existence of the pairs in the test data.", "labels": [], "entities": [{"text": "graph completion", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.7570144236087799}]}, {"text": "To determine whether our knowledge base construction system can add value, we use the new pairs found from our system to augment the training data for the graph completion model, and observe whether this improves its performance against the test set.", "labels": [], "entities": []}, {"text": "We use), a well-established tensor factorisation model, as our knowledge base completion model.", "labels": [], "entities": []}, {"text": "We provide standard information retrieval metrics to quantify the performance of the graph completion model.", "labels": [], "entities": []}, {"text": "These are the precision, P (k), and recall, R(k), calculated for the top k predictions along with the mean average precision (mAP ).", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9994397759437561}, {"text": "recall, R(k)", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.8321540753046671}, {"text": "mean average precision (mAP )", "start_pos": 102, "end_pos": 131, "type": "METRIC", "confidence": 0.9474461376667023}]}, {"text": "For gene-disease entity pairs, for example, mAP = 1 where the sum is over the diseases d with Nd being the total number of diseases, and AveP = k P (k) (R(k) \u2212 R(k \u2212 1)) with P (k) and R(k) as defined above.", "labels": [], "entities": [{"text": "AveP", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.8990157246589661}]}, {"text": "6 Main experiments and results  For all the following experiments, our data was drawn from the following datasets: DisGeNET ( and Comparative Toxicogenomics Database (CTD) (.", "labels": [], "entities": [{"text": "DisGeNET", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.697931706905365}]}, {"text": "CTD contains two relation types: 'marker/mechanism' and 'therapeutic'.", "labels": [], "entities": []}, {"text": "We use both the entire CTD dataset and the subset of therapeutic gene-disease pairs which we refer to as CTD therapeutic.", "labels": [], "entities": [{"text": "CTD dataset", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.7524360716342926}]}, {"text": "The datasets above are first restricted to human genes and then to the gene-disease pairs which appear in our corpus of sentences; this corpus consists of sentences from PubMed articles which have been restricted, for simplicity, to sentences which contain just one gene-disease pair each.", "labels": [], "entities": []}, {"text": "With these restrictions in effect, the CTD dataset has 8828 gene-disease pairs, CTD therapeutic has 169 pairs, and Disgenet has 33844 pairs.", "labels": [], "entities": [{"text": "CTD dataset", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7953665256500244}]}, {"text": "In table 1, we report the pair-level metrics (see section 5.1) for our three proposed workflows and a baseline (see section 4).", "labels": [], "entities": []}, {"text": "We also report the expertbased metric M SP (see section 5.1) for the two manual workflows.", "labels": [], "entities": [{"text": "expertbased metric M SP", "start_pos": 19, "end_pos": 42, "type": "METRIC", "confidence": 0.619232289493084}]}, {"text": "The CTD therapeutic dataset was the most suitable dataset for this evaluation because i) it is very relevant to crucial domains of application such as drug discovery, and ii) its small size makes it a good candidate for expansion.", "labels": [], "entities": [{"text": "CTD therapeutic dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.650954524676005}, {"text": "drug discovery", "start_pos": 151, "end_pos": 165, "type": "TASK", "confidence": 0.7919094562530518}]}, {"text": "In each session, the expert annotated 200 simplifications accompanied by 20 sentences.", "labels": [], "entities": []}, {"text": "It took the expert about 3 hours to annotate the first session, which is a rapid way to generate thousands of new pairs from scratch.", "labels": [], "entities": []}, {"text": "We find that our three main proposed workflows ('expert -with labels', 'expert -no labels', and the fully automated 'no expert but labels') all discover a significant number of new gene-disease therapeutic pairs.", "labels": [], "entities": []}, {"text": "As confirmed by both pair-level and user-based metrics, incorporating the use of domain expert's time and the use of labelled data results in higher precision at the expense of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9991555213928223}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9989655017852783}]}, {"text": "In table 2, we list the results of the downstream knowledge base completion task for the fully automated workflow and the baseline.", "labels": [], "entities": []}, {"text": "We compare the performance of our knowledge base completion model when trained with just the initial seed training data versus the seed training data augmented with the new pairs discovered by our fully automated workflow (and baseline workflow).", "labels": [], "entities": []}, {"text": "The addition of new pairs from the fully automated workflow gives us a higher mean average precision (mAP ) than with just the seed dataset.", "labels": [], "entities": [{"text": "mean average precision (mAP )", "start_pos": 78, "end_pos": 107, "type": "METRIC", "confidence": 0.9049961268901825}]}, {"text": "We obtain a higher precision (for the top 100 and top 1000 predictions) while maintaining the same: Intrinsic evaluation results for the main experiments on the CTD therapeutic dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9974932670593262}, {"text": "Intrinsic", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9640980958938599}, {"text": "CTD therapeutic dataset", "start_pos": 161, "end_pos": 184, "type": "DATASET", "confidence": 0.7729853590329488}]}, {"text": "This was carried outwith a train/valid/test split of 0.4/0.1/0.5, and precision threshold of 0.6 for the 'expert with labels' and 'no expert but labels' workflows.", "labels": [], "entities": [{"text": "precision threshold", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.9809654057025909}]}, {"text": "MSP is our \"manual simplification precision\" metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.8581571578979492}]}, {"text": "The precision and F-scores reported here are normalised as described in the section 4.2.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996882677078247}, {"text": "F-scores", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9964537620544434}]}, {"text": "For the baseline workflow, mAP is higher but with lower precision (for the top 100 and 1000 predictions respectively).", "labels": [], "entities": [{"text": "mAP", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.7129313945770264}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9990938901901245}]}, {"text": "We performed several other experiments using our fully automated workflow to evaluate the quality of the new pairs discovered as we varied our experiment parameters.", "labels": [], "entities": []}, {"text": "We consider three dimensions of variation: varying the precision threshold for selecting simplifications, varying the size of the seed training set, and varying the expressiveness of the simplification (for example, by including the SEN-TENCE ROOT or restricting to simplifications with at least a specified number of words).", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.996607780456543}, {"text": "SEN-TENCE ROOT", "start_pos": 233, "end_pos": 247, "type": "METRIC", "confidence": 0.80585977435112}]}, {"text": "The intrinsic evaluation results for these experiments are listed in tables 4, 5, and 6.", "labels": [], "entities": []}, {"text": "In all cases, as we make our system more selective either by raising the precision threshold, by starting with fewer seeds pairs, or by restricting to more informative simplifications, we unsurprisingly obtain higher precision at the expense of lower recall.", "labels": [], "entities": [{"text": "precision threshold", "start_pos": 73, "end_pos": 92, "type": "METRIC", "confidence": 0.9752395749092102}, {"text": "precision", "start_pos": 217, "end_pos": 226, "type": "METRIC", "confidence": 0.9972963929176331}, {"text": "recall", "start_pos": 251, "end_pos": 257, "type": "METRIC", "confidence": 0.9986666440963745}]}, {"text": "The extrinsic evaluation framework is less sensitive to these changes but improvements were observed (without any noticeable trend) for all these parameter changes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Intrinsic evaluation results for the main experiments on the CTD therapeutic dataset. This was carried out  with a train/valid/test split of 0.4/0.1/0.5, and precision threshold of 0.6 for the 'expert with labels' and 'no expert  but labels' workflows. MSP is our \"manual simplification precision\" metric. The precision and F-scores reported  here are normalised as described in the section 4.2.", "labels": [], "entities": [{"text": "CTD therapeutic dataset", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.6631728907426199}, {"text": "precision threshold", "start_pos": 168, "end_pos": 187, "type": "METRIC", "confidence": 0.9806718230247498}, {"text": "precision", "start_pos": 297, "end_pos": 306, "type": "METRIC", "confidence": 0.8730859160423279}, {"text": "precision", "start_pos": 320, "end_pos": 329, "type": "METRIC", "confidence": 0.9995076656341553}, {"text": "F-scores", "start_pos": 334, "end_pos": 342, "type": "METRIC", "confidence": 0.9779005646705627}]}, {"text": " Table 3: Top 10 simplifications for CTD Therapeutic  annotated \"Yes\" (left) and \"No\" (right) by the expert.", "labels": [], "entities": []}, {"text": " Table 4: Intrinsic evaluation results (Recall, Specificity,  Precision and F-score) on CTD and DisGeNET (DG)", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9543020725250244}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9967824220657349}, {"text": "Precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9959549903869629}, {"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9939047694206238}]}, {"text": " Table 5: Intrinsic evaluation results (Recall, Specificity,  Precision and F-score) for CTD as we vary size of the  seed training data for the 'no expert but labels' work- flow. Experiments are done with a precision threshold  of 0.8 and we restrict to simplifications with at least 5  words to ensure that they are reasonably specific.", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9829913377761841}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9966815114021301}, {"text": "Precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9955562949180603}, {"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9985629916191101}, {"text": "CTD", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.8877900838851929}, {"text": "precision threshold", "start_pos": 207, "end_pos": 226, "type": "METRIC", "confidence": 0.9728818833827972}]}, {"text": " Table 6: Intrinsic evaluation results (Recall, Specificity,  Precision and F-score) for CTD as we vary simplifi- cation expressive power (minimum length) for the 'no  expert but labels' workflow. Experiments are done  with a train/valid/test split of 0.8/0.1/0.1 and a preci- sion threshold of 0.8.", "labels": [], "entities": [{"text": "Intrinsic", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9727007150650024}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9950005412101746}, {"text": "Precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9961748123168945}, {"text": "F-score", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9978452920913696}]}]}