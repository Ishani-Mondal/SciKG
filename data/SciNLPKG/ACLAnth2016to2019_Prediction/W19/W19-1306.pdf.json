{"title": [{"text": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations", "labels": [], "entities": [{"text": "Negation and Sentiment in Customer Service Conversations", "start_pos": 61, "end_pos": 117, "type": "TASK", "confidence": 0.6160661024706704}]}], "abstractContent": [{"text": "Twitter customer service interactions have recently emerged as an effective platform to respond and engage with customers.", "labels": [], "entities": []}, {"text": "In this work, we explore the role of negation in customer service interactions, particularly applied to sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.9593557715415955}]}, {"text": "We define rules to identify true negation cues and scope more suited to conversational data than existing general review data.", "labels": [], "entities": []}, {"text": "Using semantic knowledge and syntactic structure from constituency parse trees, we propose an algorithm for scope detection that performs comparable to state of the art BiLSTM.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 108, "end_pos": 123, "type": "TASK", "confidence": 0.8641954660415649}]}, {"text": "We further investigate the results of negation scope detection for the sentiment prediction task on customer service conversation data using both a traditional SVM and a Neural Network.", "labels": [], "entities": [{"text": "negation scope detection", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.9341947436332703}, {"text": "sentiment prediction", "start_pos": 71, "end_pos": 91, "type": "TASK", "confidence": 0.9677301645278931}]}, {"text": "We propose an antonym dictionary based method for negation applied to a CNN-LSTM combination model for sentiment analysis.", "labels": [], "entities": [{"text": "negation", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.9797367453575134}, {"text": "sentiment analysis", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.9498756527900696}]}, {"text": "Experimental results show that the antonym-based method outperforms the previous lexicon-based and neural network methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "Negation has been described as a polarity influencer ( and therefore it has to betaken into consideration while designing a sentiment prediction system, but how important it is in twitter customer service conversations?", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.9344765841960907}]}, {"text": "For example, both the customer service tweets in have an explicit negation cue but the effect of cue words on the polarity differ.", "labels": [], "entities": []}, {"text": "The first tweet has a negation cue that changes the positive polarity of the words in the scope [think you do understand].", "labels": [], "entities": []}, {"text": "Additionally, tweet 1 has a hashtag which could be a strong negative signal on its own.", "labels": [], "entities": []}, {"text": "The second tweet has a cue word but it does not negate the words in that sentence or change their polarity.", "labels": [], "entities": []}, {"text": "The negation cue in the second tweet is not a true negation cue, and hence it has no scope.", "labels": [], "entities": []}, {"text": "Negation can be expressed in different ways in natural language.", "labels": [], "entities": []}, {"text": "It maybe through the use of explicit negation cues such as no, not and never that have a morphologic indication of a negative meaning.", "labels": [], "entities": []}, {"text": "This also includes a group of broad or semi negatives words (e.g. barely, hardly, and seldom) that have a negative meaning but are without any morphological negative.", "labels": [], "entities": []}, {"text": "This has been also referred to as clausal or syntactic negation.", "labels": [], "entities": []}, {"text": "These cue words are often used to negate a statement or an assertion that expresses a judgment or an opinion.", "labels": [], "entities": []}, {"text": "However in some contexts, these cue words function as exclamations, and not as true negation cues.", "labels": [], "entities": []}, {"text": "These false cues do not change the sentiment polarity of the following expression, and hence do not have any associated scope.", "labels": [], "entities": []}, {"text": "We define rules to identify true negation cues and their scopes more suited to conversational data than existing general review data.", "labels": [], "entities": []}, {"text": "The impact of negation has been studied in domains such as biomedical, literary texts, and online reviews (; however, none of the previous corpora are conversational in nature.", "labels": [], "entities": []}, {"text": "Scope definitions may depend on the domain.", "labels": [], "entities": []}, {"text": "showed that negation scope algo-rithm trained on atwitter domain struggled when tested on a medical domain.", "labels": [], "entities": []}, {"text": "Majority of the previous work in scope detection has been dominated by SVMs or Neural Networks, which require expensive annotated training data.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.921016126871109}]}, {"text": "Scope annotation is costly and time-intensive as all the scope conflicts have to be resolved by mutual discussion amongst expert annotators.", "labels": [], "entities": [{"text": "Scope annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9550828039646149}]}, {"text": "Our main motivation is to create a system that does not require a huge amount of training data for scope detection, but has comparable performance to machine learning models that require annotated training data.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 99, "end_pos": 114, "type": "TASK", "confidence": 0.9371238052845001}]}, {"text": "The proposed method uses constituency parse trees and semantic knowledge to predict scope.", "labels": [], "entities": [{"text": "constituency parse trees", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.802041212717692}]}, {"text": "The results in show that the method is comparable to state of the art BiLSTM model from) on gold negation cues for scope prediction.", "labels": [], "entities": [{"text": "scope prediction", "start_pos": 115, "end_pos": 131, "type": "TASK", "confidence": 0.8920582234859467}]}, {"text": "Since our method does not need expensive training data, we could also use this method to predict on other negation data sets.", "labels": [], "entities": []}, {"text": "However, our aim here was first to test if the predicted negation scope improves sentiment in conversations.", "labels": [], "entities": []}, {"text": "For areal time sentiment prediction system, we need both a cue prediction system to determine the true negation cues, and scope detection.", "labels": [], "entities": [{"text": "areal time sentiment prediction", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.6781883463263512}, {"text": "scope detection", "start_pos": 122, "end_pos": 137, "type": "TASK", "confidence": 0.8424889445304871}]}, {"text": "As a first step, we use a data based approach to train an SVM to predict true negation cues.", "labels": [], "entities": []}, {"text": "It's much faster and simpler to get annotated data for cue prediction, a binary task as compared to scope detection, which is a sequence labeling task.", "labels": [], "entities": [{"text": "cue prediction", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.8115433752536774}, {"text": "scope detection", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.7714212238788605}, {"text": "sequence labeling task", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.7170852820078532}]}, {"text": "This is followed by a second step of constituency treebased negation scope detection for predicted cues.", "labels": [], "entities": [{"text": "constituency treebased negation scope detection", "start_pos": 37, "end_pos": 84, "type": "TASK", "confidence": 0.628665429353714}]}, {"text": "The last step applies negation prediction coupled with antonym dictionary to improve the sentiment performance fora combination CNN-LSTM model.", "labels": [], "entities": [{"text": "negation prediction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.963472306728363}]}, {"text": "The contributions of this paper are: \u2022 Negation scope rules more suited to conversational data.", "labels": [], "entities": []}, {"text": "\u2022 A constituency-tree based approach for scope detection that uses both semantic and structural information, and does not require annotated data for scope.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.8709223866462708}]}, {"text": "\u2022 An antonym based negation applied to a combination CNN-LSTM model for sentiment prediction in conversations.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.9388205409049988}]}, {"text": "We begin with a discussion of related work in Section 2, followed by negation corpus in Section 3, and negation cue and scope detection experiments in Section 4.", "labels": [], "entities": [{"text": "negation cue and scope detection", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.884919011592865}]}, {"text": "Next, we show the effect of introducing negation detection for the sentiment task in Section 5.", "labels": [], "entities": [{"text": "negation detection", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.962024986743927}]}, {"text": "We then compare and contrast the twitter conversational sentiment data to previous datasets in Section 6.", "labels": [], "entities": []}, {"text": "Finally, Section 7 presents the conclusions and future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We divided the dataset into train and test sets giving a training set of 2317 cues and test set of 604 cues to train both a cue detection and BiLSTM scope prediction.", "labels": [], "entities": [{"text": "cue detection", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.7343175560235977}, {"text": "BiLSTM scope prediction", "start_pos": 142, "end_pos": 165, "type": "TASK", "confidence": 0.5857666532198588}]}, {"text": "The algorithm is evaluated using two different measures; token-level and scope-level.", "labels": [], "entities": []}, {"text": "Every token can be either in-scope or out of scope.", "labels": [], "entities": []}, {"text": "We report the F-score for both in-scope and out-ofscope tokens.", "labels": [], "entities": [{"text": "F-score", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9988805651664734}]}, {"text": "Since the output is a sequence, Fscore metrics maybe insufficient as it just considers individual tokens.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.6461663842201233}]}, {"text": "We also report percentage of correct scopes (PCS).", "labels": [], "entities": [{"text": "correct scopes (PCS)", "start_pos": 29, "end_pos": 49, "type": "METRIC", "confidence": 0.9160306811332702}]}, {"text": "Results are given in Table 7.", "labels": [], "entities": []}, {"text": "The out-of-scope token has a higher F-score  From our tweet collection, we discarded tweets containing images and Non-English characters and anonymized all user and company handles, giving a dataset of 21746 tweets.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9989956021308899}]}, {"text": "A sentiment annotation task was run on a data annotation platform.", "labels": [], "entities": [{"text": "sentiment annotation task", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.9072168072064718}]}, {"text": "Each tweet was initially annotated by 5 annotators using a 4 point (0 to 3) Likert scale indicating Not-At-All, Slight, Moderate and Very about their perception on the sentiment fora given tweet.", "labels": [], "entities": [{"text": "Slight", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9715608358383179}, {"text": "Moderate", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9822832942008972}]}, {"text": "We used a set of gold standard questions to filter out the bad annotators, computed the average score for each label, and assigned the maximum score.", "labels": [], "entities": []}, {"text": "A tweet is assigned a sentiment label if the maximum score for that label is greater than 1 else it is discarded from the study, giving a labeled dataset of 17779 tweets.", "labels": [], "entities": []}, {"text": "To compute the inter-annotator agreement, first we measured what percentage of the annotators out of 5 contributed to the final sentiment label and then took the average overall the tweets giving a 78.8% inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Libsvm () is used to implement the SVM classifier.", "labels": [], "entities": []}, {"text": "The tweet annotated dataset was divided into a train and test set (see for the distribution).", "labels": [], "entities": []}, {"text": "The training set was further split into a ratio of 85:15 for the validation set.", "labels": [], "entities": []}, {"text": "The three parameters w1, and C were tuned using the validation set.", "labels": [], "entities": []}, {"text": "The variables w1 and w2 are the penalty associated to a class and C is the regularization.", "labels": [], "entities": []}, {"text": "shows the evaluation metric using Precision, Recall and F measure for each class in the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9990847110748291}, {"text": "Recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9837747812271118}, {"text": "F", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9954761862754822}]}, {"text": "We do not find a major difference for SVMs( w/o negation).", "labels": [], "entities": []}, {"text": "This is in spite of using the standard features such as prefixing the tokens in scope with a keyword NOT_ and changing the polarity of the sentiment-bearing words using sentiment lexicons as described in previous work.", "labels": [], "entities": [{"text": "NOT", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.8876392245292664}]}, {"text": "A possible reason is that customer service domain is more negative as compared to general review domain See Section 6 for detailed analysis.", "labels": [], "entities": []}, {"text": "Baseline 1: Our first baseline is a single layer CNN as used in).", "labels": [], "entities": []}, {"text": "The model consists of a 1D convolution layer of window size 2 and 64 different filters.", "labels": [], "entities": []}, {"text": "The convolution layer takes as input the GloVe embeddings.", "labels": [], "entities": []}, {"text": "Max pooling layer is used to reduce the output dimensionality but keep the most salient information.", "labels": [], "entities": []}, {"text": "Baseline 2: () presented a jointed CNN and LSTM architecture.", "labels": [], "entities": []}, {"text": "The features generated from convolution and pooling operation can be viewed as local features similar to ngrams but cannot handle long term dpendencies.", "labels": [], "entities": []}, {"text": "LSTM can handle CNN's limitation by preserving historical information fora long period of time.", "labels": [], "entities": []}, {"text": "Using this as a motivation, we included a convolutional layer and max pooling layer before the input is fed into an LSTM.", "labels": [], "entities": []}, {"text": "A bidirectional LSTM layer is stacked on the convolutions layer and the tweet representation is taken to the fully connected network.", "labels": [], "entities": []}, {"text": "Proposed Negation + Antonym CNN-LSTM : We modified the sentence representation learned by replacing a word in the negation scope with it's antonym.", "labels": [], "entities": []}, {"text": "Using antonyms would reduce the Outof-Vocabulary words as compared to prefixing a word with \"NOT_\" for learning word representations.", "labels": [], "entities": []}, {"text": "Replacing all the words upto punctuation with antonyms could entirely change the sentence meaning and hence this required a more restricted and accurate scope detection.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 153, "end_pos": 168, "type": "TASK", "confidence": 0.7402244210243225}]}, {"text": "We get the predicted scopes from the scope detection model described in Section 4.", "labels": [], "entities": [{"text": "scope detection", "start_pos": 37, "end_pos": 52, "type": "TASK", "confidence": 0.8530786335468292}]}, {"text": "The antonym list is obtained from AntNET ( For the NN-based approaches, 20% data is used for validation and we save the model weights only if the validation accuracy improves.", "labels": [], "entities": [{"text": "AntNET", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.7513315081596375}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8924347162246704}]}, {"text": "The outputs of the LSTM are fed through a sigmoid layer for binary classification.", "labels": [], "entities": []}, {"text": "Regularization is performed by using a drop-out rate of 0.2 in the drop-out layer.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9327600002288818}]}, {"text": "The model is optimized using the) optimizer.", "labels": [], "entities": []}, {"text": "The deep network was implemented using the Keras package.", "labels": [], "entities": [{"text": "Keras package", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.9287780523300171}]}, {"text": "Hyper-parameter optimization for the neural network is performed using Hyperas, a python package, based on hyperopt (.", "labels": [], "entities": []}, {"text": "Results in show that the antonym based learned representations are more useful for sentiment task as compared to prefixing with NOT_.", "labels": [], "entities": [{"text": "sentiment task", "start_pos": 83, "end_pos": 97, "type": "TASK", "confidence": 0.9041589200496674}]}, {"text": "The proposed CNN-LSTM-Our-negAnt improves upon the simple CNN-LSTM-w/o neg.", "labels": [], "entities": []}, {"text": "baseline with F1 scores improving from 0.72 to 0.78 for positive sentiment and from 0.83 to 0.87 for negative sentiment.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9996150732040405}]}, {"text": "Hence Negation coupled with antonyms improves the sentiment prediction fora customer service domain.", "labels": [], "entities": [{"text": "sentiment prediction", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.9192955493927002}]}], "tableCaptions": [{"text": " Table 3: Cue and token distribution in the conversa- tional negation corpus.", "labels": [], "entities": []}, {"text": " Table 4: Cue classification on the test set.", "labels": [], "entities": [{"text": "Cue classification", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.842121809720993}]}, {"text": " Table 7: Negation classifier performance for scope de- tection with gold cues and scope.", "labels": [], "entities": [{"text": "Negation classifier", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.735590398311615}]}, {"text": " Table 8: Sentiment classification evaluation, using dif- ferent classifiers on the test set.", "labels": [], "entities": [{"text": "Sentiment classification evaluation", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.9554816881815592}]}]}