{"title": [], "abstractContent": [{"text": "We describe the work of Johns Hopkins University for the shared task of news translation organized by the Fourth Conference on Machine Translation (2019).", "labels": [], "entities": [{"text": "news translation organized by the Fourth Conference on Machine Translation (2019)", "start_pos": 72, "end_pos": 153, "type": "TASK", "confidence": 0.7040431086833661}]}, {"text": "We submitted systems for both directions of the English-German language pair.", "labels": [], "entities": []}, {"text": "The systems combine multiple techniques-sampling, filtering, iterative backtranslation, and continued training-previously used to improve performance of neural machine translation models.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 153, "end_pos": 179, "type": "TASK", "confidence": 0.6984052062034607}]}, {"text": "At submission time, we achieve a BLEU score of 38.1 for De-En and 42.5 for En-De translation directions on newstest2019.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9804523587226868}, {"text": "newstest2019", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.9545038938522339}]}, {"text": "Post-submission, the score is 38.4 for De-En and 42.8 for En-De.", "labels": [], "entities": []}, {"text": "Various experiments conducted in the process are also described.", "labels": [], "entities": []}, {"text": "1 https://ParaCrawl.eu/index.html 2", "labels": [], "entities": [{"text": "ParaCrawl.eu", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.957944393157959}]}], "introductionContent": [{"text": "This paper describes the Johns Hopkins University (JHU) submission to the Fourth Conference on Machine Translation (WMT19) news translation shared task ().", "labels": [], "entities": [{"text": "Johns Hopkins University (JHU) submission to the Fourth Conference on Machine Translation (WMT19) news translation shared task", "start_pos": 25, "end_pos": 151, "type": "TASK", "confidence": 0.8102832635243734}]}, {"text": "We built systems for both German-English and EnglishGerman.", "labels": [], "entities": []}, {"text": "Our attempts are based on previous year's submissions by Edinburgh (model architectures), Microsoft (data filtering)), Facebook (backtranslation using sampling) (, and JHU (continued training on previous years' test sets) ( . Our models leverage several techniques popular in neural machine translation -backtranslation, continued training ( and sentence filtering.", "labels": [], "entities": [{"text": "Edinburgh", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.9774153828620911}, {"text": "JHU", "start_pos": 168, "end_pos": 171, "type": "DATASET", "confidence": 0.8814554810523987}, {"text": "neural machine translation", "start_pos": 276, "end_pos": 302, "type": "TASK", "confidence": 0.7464633782704672}, {"text": "sentence filtering", "start_pos": 346, "end_pos": 364, "type": "TASK", "confidence": 0.7371479421854019}]}, {"text": "We use Transformer-big () models trained on available bitext to generate backtranslations via sampling.", "labels": [], "entities": []}, {"text": "These backtranslations are then scored and filtered using dual conditional cross-entropy and cross-entropy difference scores, then added to upsampled bitext (x2).", "labels": [], "entities": []}, {"text": "ParaCrawl 1 and Common Crawl 2 are filtered similarly, and added to form the training set for the final models.", "labels": [], "entities": []}, {"text": "We refine each final model by performing continued training on the test sets of previous years of WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.5557291507720947}]}, {"text": "We then perform ensemble decoding using multiple models for each language.", "labels": [], "entities": []}, {"text": "Finally, translations are reranked using separately-trained models to obtain the final output.", "labels": [], "entities": []}, {"text": "In the De-En direction, scores from a language model also contribute to reranking.", "labels": [], "entities": []}, {"text": "In the automatic evaluation, we scored 38.1 on De-En and 42.5 on En-De at submission time.", "labels": [], "entities": []}, {"text": "Post-submission, we ensembled more similar models and scored 38.4 on De-En and 42.8 on En-De.", "labels": [], "entities": []}, {"text": "We built our systems using the Marian and Fairseq toolkits.", "labels": [], "entities": [{"text": "Marian and Fairseq toolkits", "start_pos": 31, "end_pos": 58, "type": "DATASET", "confidence": 0.8347467929124832}]}], "datasetContent": [{"text": "A critical component of our system is continued training (CT).", "labels": [], "entities": [{"text": "continued training (CT)", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.692753118276596}]}, {"text": "To demonstrate the effectiveness of this method, we continue training using newstest2014-18, excluding newstest2017, using the learning rates mentioned in the previous section.", "labels": [], "entities": [{"text": "newstest2014-18", "start_pos": 76, "end_pos": 91, "type": "DATASET", "confidence": 0.9347436428070068}, {"text": "newstest2017", "start_pos": 103, "end_pos": 115, "type": "DATASET", "confidence": 0.9499331712722778}]}, {"text": "The scores presented in   to improve performance of a NMT system.", "labels": [], "entities": []}, {"text": "In     For submission, we perform continued training using newstest2014-18 and ensemble multiple models with the same vocabulary for translation.", "labels": [], "entities": [{"text": "newstest2014-18", "start_pos": 59, "end_pos": 74, "type": "DATASET", "confidence": 0.9409462213516235}]}, {"text": "We then employ reranking models on the 12-best lists produced from the ensembles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Reproduction of Microsoft's replication of  the University of Edinburgh's submission to WMT17,  using the Transformer-base model. Scores are re- ported on newstest2017. Our single model perfor- mance ranged from 28.3-28.6.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.516460657119751}, {"text": "newstest2017", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9708293080329895}]}, {"text": " Table 2: FAIR 2018 Replication", "labels": [], "entities": [{"text": "FAIR 2018", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.8710120022296906}, {"text": "Replication", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.40581467747688293}]}, {"text": " Table 3: Effect of continued training and ensembling,  reported on newstest2017.", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9766432642936707}]}, {"text": " Table 4: Results of ensembling En-De models, re- ported on newstest2017. Ensembling with the lower- performing model #2 (M2) degrades performance ver- sus ensembling only models #1 and #3 (M1 and M3).", "labels": [], "entities": [{"text": "newstest2017", "start_pos": 60, "end_pos": 72, "type": "DATASET", "confidence": 0.9759081602096558}]}, {"text": " Table 5: BLEU-cased score on newstest2019.", "labels": [], "entities": [{"text": "BLEU-cased score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9750783145427704}, {"text": "newstest2019", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.965285062789917}]}, {"text": " Table 6: BLEU-cased score on newstest2019.", "labels": [], "entities": [{"text": "BLEU-cased score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9744415879249573}, {"text": "newstest2019", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9644204378128052}]}]}