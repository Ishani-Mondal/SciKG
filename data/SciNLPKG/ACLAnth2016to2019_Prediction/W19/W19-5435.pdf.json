{"title": [{"text": "Low-Resource Corpus Filtering using Multilingual Sentence Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task.", "labels": [], "entities": [{"text": "WMT19 low-resource parallel corpus filtering shared task", "start_pos": 49, "end_pos": 105, "type": "TASK", "confidence": 0.6745872582708087}]}, {"text": "Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations.", "labels": [], "entities": [{"text": "Language-Agnostic SEntence Representations)", "start_pos": 49, "end_pos": 92, "type": "TASK", "confidence": 0.7957553043961525}]}, {"text": "We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function.", "labels": [], "entities": []}, {"text": "We contrast our approach to other promising methods and show that LASER yields strong results.", "labels": [], "entities": [{"text": "LASER", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.8444116115570068}]}, {"text": "Finally, we produce an ensemble of different scoring methods and obtain additional gains.", "labels": [], "entities": []}, {"text": "Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively , as compared to the second best systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 137, "end_pos": 141, "type": "METRIC", "confidence": 0.9973370432853699}]}, {"text": "Moreover, our experiments show that this technique is promising for low and even no-resource scenarios.", "labels": [], "entities": []}], "introductionContent": [{"text": "The implications of low availability of training data for parallel-scoring methods is not known yet.", "labels": [], "entities": []}, {"text": "For the task of low-resource filtering ( , we are provided with a very noisy 40.6 million-word (English token count) NepaliEnglish corpus and a 59.6 million-word SinhalaEnglish corpus crawled from the web as part of the Paracrawl project.", "labels": [], "entities": [{"text": "low-resource filtering", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.70981365442276}, {"text": "NepaliEnglish corpus", "start_pos": 117, "end_pos": 137, "type": "DATASET", "confidence": 0.8938329219818115}]}, {"text": "The challenge consists of providing scores for each sentence pair in both noisy parallel sets.", "labels": [], "entities": []}, {"text": "The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words.", "labels": [], "entities": []}, {"text": "The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based () and the neural machine translation system fairseq ( ) trained on this data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.6360923747221628}]}, {"text": "The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala-English and Nepali-English from the flores dataset . In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER 2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7434855997562408}, {"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9994107484817505}, {"text": "flores dataset", "start_pos": 203, "end_pos": 217, "type": "DATASET", "confidence": 0.8475650250911713}]}, {"text": "Our experiments demonstrate that the proposed approach outperforms other existing approaches.", "labels": [], "entities": []}, {"text": "Moreover we make use of an ensemble of multiple scoring functions to further boost the filtering performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with various methods using a setup that closely mirrors the official scoring of the shared task.", "labels": [], "entities": []}, {"text": "All methods are trained on the provided clean parallel data (see).", "labels": [], "entities": []}, {"text": "We did not use the given monolingual data.", "labels": [], "entities": []}, {"text": "For development purposes, we used the provided flores dev set.", "labels": [], "entities": []}, {"text": "For evaluation, we trained machine translation systems on the selected subsets (1M, 5M) of the noisy parallel training data using fairseq with the default flores training parameter configuration.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7720380425453186}]}, {"text": "We report SacreBLEU scores on the flores devtest set.", "labels": [], "entities": []}, {"text": "We selected our main system based on the best scores on the devtest set for the 1M condition.", "labels": [], "entities": []}, {"text": "si-en ne-en hi-en Sentences 646k 573k 1.5M English words 3.7M 3.7M 20.7M: Available bitexts to train the filtering approaches.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: SacreBLEU scores on the flores devtest set.", "labels": [], "entities": [{"text": "flores devtest set", "start_pos": 34, "end_pos": 52, "type": "DATASET", "confidence": 0.577431857585907}]}, {"text": " Table 3: Official results of the main and secondary  submissions on the flores test set evaluated with the  NMT configuration. For comparison, we include the  best scores by another system.", "labels": [], "entities": [{"text": "flores test set", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.8140635093053182}]}]}