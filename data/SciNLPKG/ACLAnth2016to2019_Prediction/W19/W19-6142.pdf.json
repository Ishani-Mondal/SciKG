{"title": [{"text": "Towards High Accuracy Named Entity Recognition for Icelandic", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9844234585762024}, {"text": "Entity Recognition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6937106102705002}, {"text": "Icelandic", "start_pos": 51, "end_pos": 60, "type": "TASK", "confidence": 0.42216119170188904}]}], "abstractContent": [{"text": "We report on work in progress which consists of annotating an Icelandic corpus for named entities (NEs) and using it for training a named entity recognizer based on a Bidirectional Long Short-Term Memory model.", "labels": [], "entities": []}, {"text": "Currently, we have annotated 7,538 NEs appearing in the first 200,000 tokens of a 1 million token corpus, MIM-GOLD, originally developed for serving as a gold standard for part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 172, "end_pos": 194, "type": "TASK", "confidence": 0.7464897334575653}]}, {"text": "Our best performing model, trained on this subset of MIM-GOLD, and enriched with external word embeddings, obtains an overall F 1 score of 81.3% when categorizing NEs into the following four categories: persons, locations, organizations and miscellaneous.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9925196170806885}]}, {"text": "Our preliminary results are promising, especially given the fact that 80% of MIM-GOLD has not yet been used for training.", "labels": [], "entities": [{"text": "MIM-GOLD", "start_pos": 77, "end_pos": 85, "type": "TASK", "confidence": 0.5991591215133667}]}], "introductionContent": [{"text": "Named Entity Recognition (NER) is the task of identifying named entities (NEs) in text and labeling them by category.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.799168661236763}]}, {"text": "Before the work presented in this paper, no labeled data sets for NER existed for Icelandic.", "labels": [], "entities": []}, {"text": "On the other hand, NER data sets exist for various other languages, e.g. for Spanish and Dutch), for English and German, and for seven Slavic languages.", "labels": [], "entities": [{"text": "NER data sets", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.8590090870857239}]}, {"text": "In all these data sets, NEs have been categorized into the following four categories: PER (person), LOC (location), ORG (organization), and MISC (miscellaneous), according to the CoNLL shared task conventions).", "labels": [], "entities": [{"text": "PER", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9884228110313416}, {"text": "LOC", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9371572732925415}, {"text": "ORG", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9813608527183533}, {"text": "MISC", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9535484313964844}]}, {"text": "The work in progress described in this paper is twofold.", "labels": [], "entities": []}, {"text": "The first part consists of categorizing NEs in an Icelandic corpus, MIM-GOLD, containing about 1 million tokens, that has been developed to serve as a gold standard for training and evaluating part-of-speech (PoS) taggers).", "labels": [], "entities": [{"text": "part-of-speech (PoS) taggers", "start_pos": 193, "end_pos": 221, "type": "TASK", "confidence": 0.6352656364440918}]}, {"text": "In the second part, MIM-GOLD is used to train and evaluate a named entity recognizer by applying a Bidirectional Long ShortTerm Memory (BiLSTM) model.", "labels": [], "entities": [{"text": "MIM-GOLD", "start_pos": 20, "end_pos": 28, "type": "TASK", "confidence": 0.8970089554786682}]}, {"text": "Our work will result in the first annotated Icelandic training corpus for NER and the first named entity recognizer for Icelandic based on machine learning (ML).", "labels": [], "entities": [{"text": "Icelandic based on machine learning (ML)", "start_pos": 120, "end_pos": 160, "type": "TASK", "confidence": 0.5887215882539749}]}, {"text": "Currently, we have categorized 7,538 NEs appearing in the first 200,000 (200K) tokens of MIM-GOLD with the commonly used four NE categories: PER, LOC, ORG and MISC.", "labels": [], "entities": [{"text": "PER", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9791609644889832}, {"text": "LOC", "start_pos": 146, "end_pos": 149, "type": "METRIC", "confidence": 0.8024170398712158}, {"text": "ORG", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9673744440078735}, {"text": "MISC", "start_pos": 159, "end_pos": 163, "type": "DATASET", "confidence": 0.755691647529602}]}, {"text": "Our best performing BiLSTM model, trained on this subset of MIM-GOLD, and enriched with external word embeddings (representations of words in ndimensional space), obtains an overall F 1 score of 81.3%.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 182, "end_pos": 191, "type": "METRIC", "confidence": 0.9918249050776163}]}, {"text": "Given the fact that 80% of MIM-GOLD has not yet been used for training, this preliminary result is promising and indicates that we maybe able to develop a high accuracy named entity recognizer for Icelandic.", "labels": [], "entities": [{"text": "accuracy named entity recognizer", "start_pos": 160, "end_pos": 192, "type": "TASK", "confidence": 0.5958939790725708}]}], "datasetContent": [{"text": "The training corpus was arranged into two sets of different sizes, 100K and 200K tokens, each split into training (80%), validation (10%) and test  (10%) sets.", "labels": [], "entities": []}, {"text": "Four different models were trained and evaluated, for the two different training set sizes and for both implicitly and externally trained word embeddings.", "labels": [], "entities": []}, {"text": "We pre-trained our own word embeddings of 200 dimensions using about 543 million tokens from a large unlabelled corpus, the Icelandic Gigaword Corpus (, using a Word2Vec architecture (.", "labels": [], "entities": [{"text": "Icelandic Gigaword Corpus", "start_pos": 124, "end_pos": 149, "type": "DATASET", "confidence": 0.8907785614331564}]}, {"text": "All the parameters in NeuroNER's configuration file, controlling the structure of the model, along with the hyperparameters directed towards the learning process, were left at their default values.", "labels": [], "entities": [{"text": "NeuroNER's configuration file", "start_pos": 22, "end_pos": 51, "type": "DATASET", "confidence": 0.7502181529998779}]}, {"text": "The only exception to this is the token_embedding_dimension parameter, controlling the length of the word vectors.", "labels": [], "entities": []}, {"text": "This value was increased from 100 to 200 for the external word embeddings.", "labels": [], "entities": []}, {"text": "In the training, early stop was applied by default when no improvement had been seen on the validation set for ten consecutive epochs.", "labels": [], "entities": [{"text": "early stop", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9293673038482666}]}, {"text": "The model used is based on the network weights taken from the epoch where the F 1 score last peaked for the validation set.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9738150636355082}]}, {"text": "Evaluation was done automatically by Neuro-NER according to CoNLL practices, which means that to score a true positive, both the NE category and the token boundaries need to be correct.", "labels": [], "entities": []}, {"text": "The F 1 scores for the models of the four training configurations, i.e. for the two training corpora sizes, with implicit and external word embeddings, are shown in.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9587779641151428}]}, {"text": "The best performing model is the one trained on 200K tokens and using external pre-trained word embeddings, achieving an overall F 1 score of 81.3%.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9917749961217245}]}], "tableCaptions": [{"text": " Table 1: Number of NEs in the 200K token train- ing corpus.", "labels": [], "entities": [{"text": "200K token train- ing corpus", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.7777579029401144}]}, {"text": " Table 2: F 1 scores (%) of four different training  configurations.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9806025425593058}]}]}