{"title": [{"text": "Toward Automated Content Feedback Generation for Non-native Spontaneous Speech", "labels": [], "entities": [{"text": "Automated Content Feedback Generation", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.6070302426815033}]}], "abstractContent": [{"text": "In this study, we developed an automated algorithm to provide feedback about the specific content of non-native English speakers' spoken responses.", "labels": [], "entities": []}, {"text": "The responses were spontaneous speech, elicited using integrated tasks where the language learners listened to and/or read passages and integrated the core content in their spoken responses.", "labels": [], "entities": []}, {"text": "Our models detected the absence of key points considered to be important in a spoken response to a particular test question, based on two different models: (a) a model using word-embedding based content features and (b) a state-of-the art short response scoring engine using traditional n-gram based features.", "labels": [], "entities": []}, {"text": "Both models achieved a substantially improved performance over the majority baseline, and the combination of the two models achieved a significant further improvement.", "labels": [], "entities": []}, {"text": "In particular, the models were robust to automated speech recognition (ASR) errors, and performance based on the ASR word hypotheses was comparable to that based on manual transcriptions.", "labels": [], "entities": [{"text": "automated speech recognition (ASR)", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.813159853219986}]}, {"text": "The accuracy and F-score of the best model for the questions included in the train set were 0.80 and 0.68, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999810516834259}, {"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9994309544563293}]}, {"text": "Finally, we discussed possible approaches to generating targeted feedback about the content of a language learner's response , based on automatically detected missing key points.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this study, we propose an automated algorithm which provides feedback about the specific content of non-native English speakers' spoken responses.", "labels": [], "entities": []}, {"text": "It is designed to help language learners preparing fora speaking test that is part of an assessment of English proficiency for academic purposes.", "labels": [], "entities": []}, {"text": "The speaking test includes questions eliciting spontaneous speech.", "labels": [], "entities": []}, {"text": "In particular, the items require language learners to read and/or listen to stimulus materials and then integrate and reproduce the key content from the source materials into their speaking performances (hereafter, integrated tasks).", "labels": [], "entities": []}, {"text": "Research in integrated task performance has shown that human raters pay substantial attention to test-takers' speech content.", "labels": [], "entities": []}, {"text": "A speaker's performance is evaluated by the content completeness and accuracy of the reproduced information, in addition to linguistic criteria including fluency, pronunciation, grammar, and vocabulary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9986380934715271}]}, {"text": "The current study investigated automated feedback through the dimension of content completeness.", "labels": [], "entities": []}, {"text": "This content-aspect of speech performance refers to the degree to which an individual can process, select, integrate, and reproduce key source information into a subsequent oral response.", "labels": [], "entities": []}, {"text": "The ability to reproduce complete content represents a critical aspect of integrated speaking task performance and is evaluated by the number of key points reproduced from the input materials.", "labels": [], "entities": []}, {"text": "Key points are brief descriptions of content elements that test developers determine to be important in responses to a particular test question.", "labels": [], "entities": []}, {"text": "Providing feedback on content aspects of speech can help language learners discern the quality of their speech performance beyond linguistic dimensions such as fluency or grammar.", "labels": [], "entities": []}, {"text": "This type of feedback is particularly relevant and crucial when we consider integrated task performance, because the ability to accurately and adequately recreate the source materials is an essential language skill required in real-world academic or workplace contexts.", "labels": [], "entities": []}, {"text": "Despite the importance of content as a component of speech, few studies have explored automated content feedback.", "labels": [], "entities": []}, {"text": "To address this gap, we aim to develop a content feedback algorithm.", "labels": [], "entities": []}, {"text": "In this study, we trained automated models to detect the absence of key points that are the core content expected incorrect answers.", "labels": [], "entities": []}, {"text": "Next, we discussed possible ways to generate content feedback based on the output of the automated models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The speakers were partitioned into two sets: train (49%), and test sets (51%).", "labels": [], "entities": []}, {"text": "All responses from the same speaker belonged to one set, and thus the train and test sets did not share any speakers.", "labels": [], "entities": []}, {"text": "The percentage of each form and speakers' proficiency levels were similar in each set.", "labels": [], "entities": []}, {"text": "In order to investigate the impact of a question-specific training dataset, we conducted 4-fold cross-validation.", "labels": [], "entities": []}, {"text": "As described in Section 4, the data was comprised of four forms (with three questions on each form).", "labels": [], "entities": []}, {"text": "For each fold, three forms were used as the \"seen form\", and the remaining form was used as the \"unseen form\".", "labels": [], "entities": []}, {"text": "The model was trained only on the seen form responses in the training partition.", "labels": [], "entities": []}, {"text": "We also trained a separate regression model for each question of each Key Point, resulting in 6 \u00d7 12 = 72 models (question-specific models).", "labels": [], "entities": []}, {"text": "Because the overall performance of the question-specific models were not superior to the generic models, we reported only the generic modelbased results.", "labels": [], "entities": []}, {"text": "Ina future study using a much larger numbers of questions, we will conduct more rigorous comparisons between the generic models and the question-specific models and select the final models.", "labels": [], "entities": []}, {"text": "During evaluation, the model was evaluated on the seen form responses and the unseen form responses, separately.", "labels": [], "entities": []}, {"text": "In the results section, we report the average of the four-folds.", "labels": [], "entities": []}, {"text": "We used two different transcription methods: manual transcriptions by professional transcribers and automated transcriptions by an ASR system trained on non-native speakers' speech.", "labels": [], "entities": []}, {"text": "We used a gender-independent acoustic model (AM) trained on 800 hours of spoken responses covering over 100 native languages across 8,900 speakers using the Kaldi toolkit.", "labels": [], "entities": []}, {"text": "A DNN-HMM model was adapted to test takers with fM-LLR and i-vectors.", "labels": [], "entities": [{"text": "fM-LLR", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.911832869052887}]}, {"text": "The language model (LM) was a trigram model trained using the same dataset used for AM training.", "labels": [], "entities": [{"text": "AM training", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9413532316684723}]}, {"text": "This ASR system achieved a Word Error Rate of 18.5% on 600 held-out responses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9524706602096558}, {"text": "Word Error Rate", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.7751500010490417}]}, {"text": "Detailed information about the ASR system is provided in.", "labels": [], "entities": [{"text": "ASR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9721413850784302}]}, {"text": "In order to compare the performance of the content features with c-rater, we trained three models: EMB (model based on word-embedding features), c-rater (model based on the c-rater engine), and CMB (combination of two models).", "labels": [], "entities": [{"text": "EMB", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8229835033416748}]}, {"text": "For CMB, we averaged the probabilities generated by EMB and c-rater with 0.5 as a decision boundary.", "labels": [], "entities": [{"text": "EMB", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.7657756805419922}]}, {"text": "Finally, for each transcription mode, we trained 18 binary classifiers.", "labels": [], "entities": []}, {"text": "provides performance of the models on the seen forms where all questions in the test set appeared in the train set.", "labels": [], "entities": []}, {"text": "The models were evaluated in terms of accuracy, F-score, and Cohen's kappa for detecting absence of the Key Points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9996269941329956}, {"text": "F-score", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9989871382713318}]}, {"text": "We reported the average performance for 6 Key Points.", "labels": [], "entities": []}, {"text": "In this study, the accuracy of the majority class baseline (classifying all responses as the Key Point presented) was 64% since the proportion of the responses without Key Point was 36% on av-  For the experiment using the manual transcriptions, both the EMB and c-rater models achieved substantial improvement over the majority baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9995414018630981}, {"text": "EMB", "start_pos": 255, "end_pos": 258, "type": "DATASET", "confidence": 0.8156377077102661}]}, {"text": "The performance of the EMB model was comparable to the c-rater model, and the combination of the two models resulted in further improvement.", "labels": [], "entities": [{"text": "EMB", "start_pos": 23, "end_pos": 26, "type": "DATASET", "confidence": 0.8423916101455688}]}, {"text": "The accuracy and F-score of the CMB model were 0.79 and 0.69, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9998942613601685}, {"text": "F-score", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.999610960483551}]}], "tableCaptions": [{"text": " Table 1: Data size and Key Point (KP) distribution by proficiency levels", "labels": [], "entities": [{"text": "Key Point (KP)", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.8594812989234925}]}, {"text": " Table 2: Average performance of six Key Points on  seen form", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.99118971824646}]}, {"text": " Table 3: Average performance of six Key Points on un- seen form", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9954841136932373}]}]}