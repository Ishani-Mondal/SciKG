{"title": [{"text": "Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations", "labels": [], "entities": [{"text": "Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations", "start_pos": 0, "end_pos": 93, "type": "TASK", "confidence": 0.7370936274528503}]}], "abstractContent": [{"text": "In this paper we introduce anew natural language processing dataset and benchmark for predicting prosodic prominence from written text.", "labels": [], "entities": [{"text": "predicting prosodic prominence from written text", "start_pos": 86, "end_pos": 134, "type": "TASK", "confidence": 0.8448672990004221}]}, {"text": "To our knowledge this will be the largest publicly available dataset with prosodic labels.", "labels": [], "entities": []}, {"text": "We describe the dataset construction and the resulting benchmark dataset in detail and train a number of different models ranging from feature-based classifiers to neural network systems for the prediction of dis-cretized prosodic prominence.", "labels": [], "entities": [{"text": "prediction of dis-cretized prosodic prominence", "start_pos": 195, "end_pos": 241, "type": "TASK", "confidence": 0.7776307940483094}]}, {"text": "We show that pre-trained contextualized word representations from BERT outperform the other models even with less than 10% of the training data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.8706264495849609}]}, {"text": "Finally we discuss the dataset in light of the results and point to future research and plans for further improving both the dataset and methods of predicting prosodic prominence from text.", "labels": [], "entities": [{"text": "predicting prosodic prominence from text", "start_pos": 148, "end_pos": 188, "type": "TASK", "confidence": 0.9038166761398315}]}, {"text": "The dataset and the code for the models are publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Prosodic prominence, i.e., the amount of emphasis that a speaker gives to a word, has been widely studied in phonetics and speech processing.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7186127305030823}]}, {"text": "However, the research on text-based natural language processing (NLP) methods for predicting prosodic prominence is somewhat limited.", "labels": [], "entities": [{"text": "predicting prosodic prominence", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.9134551088015238}]}, {"text": "Even in the text-to-speech synthesis domain, with many recent methodological advances, work on symbolic prosody prediction has lagged behind.", "labels": [], "entities": [{"text": "text-to-speech synthesis", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.7344401776790619}, {"text": "symbolic prosody prediction", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.736123263835907}]}, {"text": "We believe that this is mainly due to the lack of suitable datasets.", "labels": [], "entities": []}, {"text": "Existing, publicly available annotated speech corpora, are very small by current standards.", "labels": [], "entities": []}, {"text": "In this paper we introduce anew NLP dataset and benchmark for predicting prosodic prominence from text which is based on the recently published LibriTTS corpus (, containing automatically generated prosodic prominence labels for over 260 hours or 2.8 million words of English audio books, read by 1230 different speakers.", "labels": [], "entities": [{"text": "NLP dataset", "start_pos": 32, "end_pos": 43, "type": "DATASET", "confidence": 0.6953019797801971}, {"text": "LibriTTS corpus", "start_pos": 144, "end_pos": 159, "type": "DATASET", "confidence": 0.9600358307361603}]}, {"text": "To our knowledge this will be the largest publicly available dataset with prosodic annotations.", "labels": [], "entities": []}, {"text": "We first give some background about prosodic prominence and related research in Section 2.", "labels": [], "entities": [{"text": "prosodic prominence", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.9412990212440491}]}, {"text": "We then describe the dataset construction and annotation method in Section 3.", "labels": [], "entities": [{"text": "dataset construction", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.6676256954669952}]}, {"text": "Prosody prediction can be turned into a sequence labeling task by giving each word in a text a discrete prominence value based on the amount of emphasis the speaker gives to the word when reading the text.", "labels": [], "entities": [{"text": "Prosody prediction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9112547338008881}, {"text": "sequence labeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6777327209711075}]}, {"text": "In Section 4 we explain the experiments and the experimental results using a number of different sequence labeling approaches and show that pre-trained contextualized word representations from BERT ( outperform our other baselines even with less than 10% of the training data.", "labels": [], "entities": [{"text": "BERT", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9662620425224304}]}, {"text": "Although BERT has been previously applied in various sequence labeling tasks, like named entity recognition, to the best of our knowledge, this is the first application of BERT in the task of predicting prosodic prominence.", "labels": [], "entities": [{"text": "BERT", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9963725805282593}, {"text": "named entity recognition", "start_pos": 83, "end_pos": 107, "type": "TASK", "confidence": 0.6110834081967672}, {"text": "BERT", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9921886920928955}, {"text": "predicting prosodic prominence", "start_pos": 192, "end_pos": 222, "type": "TASK", "confidence": 0.893689493338267}]}, {"text": "We analyse the results in Section 5, comparing BERT to a bidirectional long shortterm memory (BiLSTM) model and looking at the types of errors made by these selected models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9765899777412415}]}, {"text": "We find that BERT outperforms the BiLSTM model across all the labels.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9968718886375427}]}, {"text": "Finally in Section 6 we discuss the methods in light of the experimental results and highlight areas that are known to negatively impact the results.", "labels": [], "entities": []}, {"text": "We also discuss the relevance of pre-training for the task of predicting prosodic prominence.", "labels": [], "entities": [{"text": "predicting prosodic prominence", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.9052848021189371}]}, {"text": "We conclude by pointing to future research both in developing better methods for predicting prosodic prominence but also to further improve the quality of the dataset.", "labels": [], "entities": [{"text": "predicting prosodic prominence", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.9019665718078613}]}, {"text": "The dataset and the PyTorch code for the models are available on GitHub: https://github.com/ Helsinki-NLP/prosody.", "labels": [], "entities": []}], "datasetContent": [{"text": "We introduce, automatically generated, high quality prosodic annotations for the recently published LibriTTS corpus (: Example sentence with the annotation from the dataset.", "labels": [], "entities": [{"text": "LibriTTS corpus", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9668113887310028}]}, {"text": "Discrete prominence values were used in the experiments of this paper.", "labels": [], "entities": []}, {"text": "The real-valued labels are used for generation of the discrete labels, however, they could also be used directly for prominence prediction. with the Montreal forced aligner, using a pronunciation lexicon and acoustic models trained on the LibriSpeech dataset.", "labels": [], "entities": [{"text": "prominence prediction.", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.7772950828075409}, {"text": "Montreal forced aligner", "start_pos": 149, "end_pos": 172, "type": "DATASET", "confidence": 0.9565616448720297}, {"text": "LibriSpeech dataset", "start_pos": 239, "end_pos": 258, "type": "DATASET", "confidence": 0.9687641263008118}]}, {"text": "The aligned sentences were then prosodically annotated with word-level acoustic prominence labels.", "labels": [], "entities": []}, {"text": "For the annotation, we used the Wavelet Prosody Analyzer toolkit 2 , which implements the method described in (.", "labels": [], "entities": []}, {"text": "Briefly, the method consists of 1) the extraction of pitch and energy signals from the speech data and duration from the word level alignments, 2) filling the unvoiced gaps in extracted signals by interpolation followed by smoothing and normalizing, 3) combining the normalized signals by summing or multiplication, and 4) performing a continuous wavelet transform (CWT) on the composite signal and extracting continuous prominence values as lines of maximum amplitude across wavelet scales (see.", "labels": [], "entities": [{"text": "duration", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9548500776290894}]}, {"text": "Essentially, the method assumes that the louder, the longer, and the higher, the more prominent.", "labels": [], "entities": []}, {"text": "On top of this, the wavelet transform provides multi-resolution contextual information; the more the word stands out from its environment in various time scales, the more prominent the word is perceived.", "labels": [], "entities": []}, {"text": "For the current study, continuous prominence values were discretized to two (non-prominent, prominent) or three (non prominent, somewhat prominent, very prominent) classes.", "labels": [], "entities": [{"text": "continuous prominence", "start_pos": 23, "end_pos": 44, "type": "METRIC", "confidence": 0.8086254298686981}]}, {"text": "The binary case is closely related to the pitch accent detection task, aiming for results comparable with the majority of the literature on the topic.", "labels": [], "entities": [{"text": "pitch accent detection task", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7584869638085365}]}, {"text": "The weights in constructing the composite signal and discretization thresholds were adjusted based on The Boston University radio news corpus, containing manually annotated pitch accent labels.", "labels": [], "entities": [{"text": "Boston University radio news corpus", "start_pos": 106, "end_pos": 141, "type": "DATASET", "confidence": 0.9569355130195618}]}, {"text": "This corpus is often used in the evaluation of pitch accent annotation and prediction quality, with the current annotation method yielding state-of-the-art accuracy in word level acoustic-based accent detection, 85.3%, us-ing weights 1.0, 0.5 and 1.0 for F0, energy and duration respectively, and using multiplication of these features in signal composition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9975257515907288}, {"text": "word level acoustic-based accent detection", "start_pos": 168, "end_pos": 210, "type": "TASK", "confidence": 0.665942245721817}, {"text": "F0", "start_pos": 255, "end_pos": 257, "type": "METRIC", "confidence": 0.966543436050415}, {"text": "duration", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.9857153296470642}]}, {"text": "For threeway discretization, the non-prominent / prominent cut-off was maintained and the prominent class was split to two classes of roughly equal size.", "labels": [], "entities": []}, {"text": "Statistics of the resulting dataset are described in table 1.", "labels": [], "entities": []}, {"text": "The full dataset is available for download here: https://github.com/ Helsinki-NLP/prosody.", "labels": [], "entities": [{"text": "Helsinki-NLP", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.8905514478683472}]}, {"text": "Although not discussed in this paper, the described acoustic annotation and text-based prediction methods can be applied to prosodic boundaries too, and the boundary labels will be included in the dataset at a later stage.", "labels": [], "entities": []}, {"text": "In this section we describe the experimental setup and the results from our experiments in predicting discrete prosodic prominence labels from text using the corpus described above.", "labels": [], "entities": [{"text": "predicting discrete prosodic prominence labels from text", "start_pos": 91, "end_pos": 147, "type": "TASK", "confidence": 0.8210736002240863}]}, {"text": "We performed experiments with the following models: \u2022 The models were selected so that they cover a wide variety of different architectures from feature-based statistical approaches to neural networks and pre-trained language models.", "labels": [], "entities": []}, {"text": "The models are described in more detail below.", "labels": [], "entities": []}, {"text": "We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library, 3 which we further fine-tune during training.", "labels": [], "entities": [{"text": "BERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9884933829307556}]}, {"text": "We take the last hidden layer of BERT and train a single fully-connected classifier layer on top of it, mapping the representation of each word to the labels.", "labels": [], "entities": [{"text": "BERT", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.838756799697876}]}, {"text": "For our experiments we use the smaller BERT-base model using the uncased alternative.", "labels": [], "entities": [{"text": "BERT-base", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9961315393447876}]}, {"text": "We use a batch size of 32 and fine-tune the model for 2 epochs.", "labels": [], "entities": []}, {"text": "For BiLSTM we use pre-trained 300D GloVe 840B word embeddings ().", "labels": [], "entities": []}, {"text": "The initial word embeddings are fine-tuned during training.", "labels": [], "entities": []}, {"text": "As with BERT, we add one fullyconnected classifier layer on top of the BiLSTM, mapping the representation of each word to the labels.", "labels": [], "entities": []}, {"text": "We use a dropout of 0.2 between the layers of the BiLSTM.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8954971432685852}]}, {"text": "We use a batch size of 64 and train the model for 5 epochs.", "labels": [], "entities": []}, {"text": "For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D GloVe 840B word embeddings as features, with context-size 1, i.e. including the previous and the next word in the context.", "labels": [], "entities": []}, {"text": "For the conditional random field (CRF) model we use MarMot 5 by with the default configuration.", "labels": [], "entities": []}, {"text": "The model applies standard feature templates that are used for part-ofspeech tagging such as surrounding words as well as suffix and prefix features.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.7152222692966461}]}, {"text": "We did not optimize the feature model nor any of the other hyperparameters.", "labels": [], "entities": []}, {"text": "All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub: https://github.com/ Helsinki-NLP/prosody.", "labels": [], "entities": [{"text": "Minitagger", "start_pos": 23, "end_pos": 33, "type": "DATASET", "confidence": 0.9124308228492737}]}, {"text": "For the experiments we used the larger train-360 training set.", "labels": [], "entities": [{"text": "train-360 training set", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.6746822694937388}]}, {"text": "We report both 2-way and 3-way classification results.", "labels": [], "entities": []}, {"text": "In the 2-way classification task we take the three prominence labels and merge labels 1 and 2 into a single prominent class.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example sentence with the annotation from the dataset. Discrete prominence values were used  in the experiments of this paper. The real-valued labels are used for generation of the discrete labels,  however, they could also be used directly for prominence prediction.", "labels": [], "entities": [{"text": "prominence prediction", "start_pos": 255, "end_pos": 276, "type": "TASK", "confidence": 0.7842512726783752}]}, {"text": " Table 3: Experimental results (%) for the 2 and 3-way classification tasks.", "labels": [], "entities": []}]}