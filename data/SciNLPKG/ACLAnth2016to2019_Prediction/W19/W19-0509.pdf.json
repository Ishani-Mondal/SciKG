{"title": [{"text": "Sentiment Independent Topic Detection in Rated Hospital Reviews", "labels": [], "entities": [{"text": "Sentiment Independent Topic Detection", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7444366738200188}]}], "abstractContent": [{"text": "We present a simple method to find topics in user reviews that accompany ratings for products or services.", "labels": [], "entities": []}, {"text": "Standard topic analysis will perform sub-optimal on such data since the word distributions in the documents are not only determined by the topics but by the sentiment as well.", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7903409898281097}]}, {"text": "We reduce the influence of the sentiment on the topic selection by adding two explicit topics, representing positive and negative sentiment.", "labels": [], "entities": []}, {"text": "We evaluate the proposed method on a set of over 15,000 hospital reviews.", "labels": [], "entities": []}, {"text": "We show that the proposed method, Latent Semantic Analysis with explicit word features, finds topics with a much smaller bias for sentiments than other similar methods.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.5609489182631174}]}], "introductionContent": [{"text": "There are many websites that collect user opinions and ratings on products or reviews.", "labels": [], "entities": []}, {"text": "In this paper we study a collection of reviews and ratings of orthopedic treatments in hospitals.", "labels": [], "entities": []}, {"text": "On the leading German social media website for hospital rating www.klinikbewertungen.de users may rate and comment about 3000 hospitals.", "labels": [], "entities": []}, {"text": "On this website, in principle it is possible to see what topics are criticized and which ones are valued.", "labels": [], "entities": []}, {"text": "To do so, we need to do a topic analysis on the comments.", "labels": [], "entities": []}, {"text": "Since many texts in our corpus have a strong polarity, a standard topic analysis, using Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) also tries to account for the words associated with positive and negative sentiment.", "labels": [], "entities": []}, {"text": "Most likely topics and sentiments will be mixed up.", "labels": [], "entities": []}, {"text": "E.g. the topic pain is usually associated with negative feelings.", "labels": [], "entities": []}, {"text": "Thus negative opinion and pain get mixed up in one topic.", "labels": [], "entities": []}, {"text": "Consequently, the topic Pain might be found fora document that contains negative words but is not about pain and, vice versa, a document talking about a positive experience on pain treatment will not be associated with the topic Pain.", "labels": [], "entities": []}, {"text": "Thus we have to model the sentiment and the topic independently.", "labels": [], "entities": []}, {"text": "A straightforward way to make the topic analysis sentiment independent would be to treat comments that come with positive and those that come with negative ratings separately.", "labels": [], "entities": []}, {"text": "However, we would end up with incomparable topics for positive and negative comments.", "labels": [], "entities": []}, {"text": "Joint topic-sentiment models are designed to find topics and polarity of each document, while we already have the polarity of each document.", "labels": [], "entities": []}, {"text": "Moreover, these models are designed to optimize sentiment analysis and not to make the topics less biased towards some sentiment.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.9409026503562927}]}, {"text": "The solution we present in Section 2 is basically a simplified formulation of the method proposed by.", "labels": [], "entities": []}, {"text": "We use Latent Semantic Analysis (LSA) and add fixed topics for positive and negative sentiment to the set of topics that have to be learned.", "labels": [], "entities": []}, {"text": "Thus much of the positive and negative words are explained by these dimensions and less of these words are explained by the other topics.", "labels": [], "entities": []}, {"text": "In order to keep the influence of positive or negative opinion out of the topic modeling, we add two fixed topics representing these sentiments to the LSA model.", "labels": [], "entities": []}, {"text": "These topics are initialized with values calculated before and not updated in the learning phase.", "labels": [], "entities": []}, {"text": "As values for these fixed dimensions we either take the ratings for each document or we compute the polarity of each word.", "labels": [], "entities": []}, {"text": "LSA) is a simple but effective method for topic analysis: a termdocument matrix is decomposed into two smaller matrices.", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9730327725410461}]}, {"text": "The rows of the first matrix can be interpreted as the topic distributions of the documents while the second matrix gives the word distribution for these topics.", "labels": [], "entities": []}, {"text": "The decomposition is usually realized by Singular Value Decomposition.", "labels": [], "entities": []}, {"text": "In the following we will use Non-negative Matrix Factorization (NMF) for the decomposition, which makes the weights easier to interpret and can be seen as a variant of PLSA).", "labels": [], "entities": []}, {"text": "We start with the term-document matrix TD of size m \u00d7 n, with m the number of documents and n the number of terms.", "labels": [], "entities": []}, {"text": "Each element TD i,j is the weight of word j for document i.", "labels": [], "entities": []}, {"text": "Now we assume that there are k (latent) topics (with 0 < k n) such that TD can be decomposed into a document-topic matrix U of size m \u00d7 k and a word-topic matrix V of size n \u00d7 k Since we do not know the topics, we choose some k, initialize U and V randomly and use the stochastic gradient descent algorithm to minimize ||TD \u2212 U \u00b7 VT || F ro 2 . Furthermore, we require that the row vectors of VT have magnitude 1.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare three variants: LSA (with NMF), LSA-ExplDF and LSA-ExplWF.", "labels": [], "entities": []}, {"text": "In all cases we set the number of topics k to 20 plus the number of fixed topics.", "labels": [], "entities": []}, {"text": "Since the goal is to make the topics independent of the sentiment, we will use exactly this as an evaluation criterion.", "labels": [], "entities": []}, {"text": "For each document we determine the two most prominent topics, assuming that there are at least two topics in each text.", "labels": [], "entities": []}, {"text": "The results, however, do not depend on the number of topics chosen.", "labels": [], "entities": []}, {"text": "Subsequently, we count the number of times each topic is assigned to a negative and to a positive document.", "labels": [], "entities": []}, {"text": "If the topics would be completely independent, the ratio of positive and negative documents would be the same for each topic.", "labels": [], "entities": []}, {"text": "Thus we take the variance of the fraction of negative documents for each topic as criterion for success: the lower the variance the more independent the topics are from the sentiment.", "labels": [], "entities": []}, {"text": "Of course the topics are not independent of the sentiment.", "labels": [], "entities": []}, {"text": "Nevertheless a smaller variance indicates that topics ans sentiments are better seperated.", "labels": [], "entities": []}, {"text": "Since the results are not deterministic we use averages of 10 runs.", "labels": [], "entities": []}, {"text": "gives the average fraction of negative documents and the variance for each method.", "labels": [], "entities": [{"text": "variance", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9481989741325378}]}, {"text": "A lower variance shows that the fraction is more similar for each topic, indicating that the topics are more independent of the sentiment of the texts.", "labels": [], "entities": []}, {"text": "We clearly see that LSA-ExplWF give the best result and impressively reduces the variance between the percentage of negative document per topic., second column for typical words for the first two topics.", "labels": [], "entities": []}, {"text": "To get an impression of the topics found, gives the five most prominent words for each topic found by one run of LSA-ExplWF.", "labels": [], "entities": []}, {"text": "Though the results differ slightly across two runs, most topics are found in each run and many topics found by one method also are found by another method.", "labels": [], "entities": []}, {"text": "E.g. both methods find a topic that can be represented by the words Therapie (therapy), Therapeut (therapist), etc.", "labels": [], "entities": []}, {"text": "(topic 15 in).", "labels": [], "entities": []}, {"text": "In the case of LSA this topic was assigned to 888 positive and 569 negative documents.", "labels": [], "entities": []}, {"text": "In LSA this topic thus has a strongly negative connotation.", "labels": [], "entities": []}, {"text": "Using LSA-ExplWF the topic was assign to 978 positive and 373 negative documents.", "labels": [], "entities": []}, {"text": "The comment \"Ich habe mich hier ausgesprochen wohl gef\u00fchlt, als ich eine k\u00fcnstliche H\u00fcfte (TEP) erhalten hatte und nach dem Krankenhausaufenthalt drei Wochen in dieser Reha Klinik verbrachte.", "labels": [], "entities": []}, {"text": "Die Therapie wurde ganz individuell auf meine Bed\u00fcrfnisse abgestimmt.", "labels": [], "entities": []}, {"text": "\" (I felt very well here when I got an artificial hip (TEP) and spent three weeks in the rehabilitation clinic after the hospital stay.", "labels": [], "entities": []}, {"text": "The therapy was individually tailored to my needs.", "labels": [], "entities": []}, {"text": ") got topics 15 and 18 from LSA-ExplWF, while LSA assigned topics 2 and 18, probably because topic 15 has a negative bias in LSA and did not fit for this positive comment.", "labels": [], "entities": [{"text": "LSA-ExplWF", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9114032983779907}]}, {"text": "In another example a patient is massively complaining that the doctors did not take time for him, that there was only a standard treatment and he could not shower everyday.", "labels": [], "entities": []}, {"text": "Here LSA assigns topics 15 and 1, while LSA-ExplWF assigns topics 11 (taking time, answering questions) and 14 (nursing care).", "labels": [], "entities": []}, {"text": "LSA probably assigns topic 15 mainly because the text is extremely negative, while LSA-ExplWF precisely identifies the topics that frustrated the patient.", "labels": [], "entities": [{"text": "LSA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.904384434223175}]}, {"text": "gives the most prominent words for the first two topics.", "labels": [], "entities": []}, {"text": "Interestingly, the word empfehlen) (recommend) is found both for positive and negative sentiment: this word is used in stronly polarized contexts, both with positive and with negative sentiment, but it is not used frequently in neutral reviews.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fraction of negative documents per topic.", "labels": [], "entities": []}, {"text": " Table 2: Most prominent words for all topics found by LSA-ExplWF. Fixed topics are excluded. See", "labels": [], "entities": [{"text": "LSA-ExplWF", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.8676341772079468}]}]}