{"title": [{"text": "Character-Aware Decoder for Translation into Morphologically Rich Languages", "labels": [], "entities": [{"text": "Translation into Morphologically Rich Languages", "start_pos": 28, "end_pos": 75, "type": "TASK", "confidence": 0.8830017209053039}]}], "abstractContent": [{"text": "Neural machine translation (NMT) systems operate primarily on words (or sub-words), ignoring lower-level patterns of morphology.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8403767943382263}]}, {"text": "We present a character-aware decoder designed to capture such patterns when translating into morphologically rich languages.", "labels": [], "entities": []}, {"text": "We achieve character-awareness by augmenting both the softmax and embedding layers of an attention-based encoder-decoder model with convolutional neural networks that operate on the spelling of a word.", "labels": [], "entities": []}, {"text": "To investigate performance on a wide variety of morphological phenomena, we translate English into 14 typologically diverse target languages using the TED multi-target dataset.", "labels": [], "entities": [{"text": "TED multi-target dataset", "start_pos": 151, "end_pos": 175, "type": "DATASET", "confidence": 0.7195541262626648}]}, {"text": "In this low-resource setting, the character-aware decoder provides consistent improvements with BLEU score gains of up to +3.05.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9781697988510132}]}, {"text": "In addition, we analyze the relationship between the gains obtained and properties of the target language and find evidence that our model does indeed exploit morphological patterns.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditional attention-based encoder-decoder neural machine translation (NMT) models learn wordlevel embeddings, with a continuous representation for each unique word type (.", "labels": [], "entities": [{"text": "attention-based encoder-decoder neural machine translation (NMT)", "start_pos": 12, "end_pos": 76, "type": "TASK", "confidence": 0.7629493921995163}]}, {"text": "However, this results in along tail of rare words for which we do not learn good representations.", "labels": [], "entities": []}, {"text": "More recently, it has become standard prac- * Equal Contribution tice to mitigate the vocabulary size problem with Byte-Pair Encoding (BPE).", "labels": [], "entities": []}, {"text": "BPE iteratively merges consecutive characters into larger chunks based on their frequency, which results in the breaking up of less common words into \"subword units.\"", "labels": [], "entities": [{"text": "BPE", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6288480758666992}]}, {"text": "While BPE addresses the vocabulary size problem, the spellings of the subword units are still ignored.", "labels": [], "entities": [{"text": "BPE", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.4464074373245239}]}, {"text": "On the other hand, purely character-level NMT translates one character at a time and can implicitly learn about morphological patterns within words as well as generalize to unseen vocabulary.", "labels": [], "entities": []}, {"text": "Recently, show that very deep character-level models can outperform BPE, however, the smallest data size evaluated was 2 million sentences, so it is unclear if the results hold for low-resource settings and when translating into a range of different morphologically rich languages.", "labels": [], "entities": []}, {"text": "Furthermore, tuning deep character-level models is expensive, even for low-resource settings.", "labels": [], "entities": []}, {"text": "A middle-ground alternative is character-aware word-level modeling.", "labels": [], "entities": [{"text": "character-aware word-level modeling", "start_pos": 31, "end_pos": 66, "type": "TASK", "confidence": 0.6387603481610616}]}, {"text": "Here, the NMT system operates over words but uses word embeddings that are sensitive to spellings and thereby has the ability to learn morphological patterns in the language.", "labels": [], "entities": []}, {"text": "Such character-aware approaches have been applied successfully in NMT to the source-side word embedding layer, but surprisingly, similar gains have not been achieved on the target side (.", "labels": [], "entities": []}, {"text": "While source-side character-aware models only need to make the source embedding layer character-aware, on the target-side we require both the target embedding layer and the softmax layer 2 The dropout rate was found to be critical in, and each tuning run takes much longer due to longer sequence lengths.", "labels": [], "entities": []}, {"text": "Also referred to as generator, final output layer or final linear 1.", "labels": [], "entities": []}, {"text": "We propose a method for utilizing characteraware embeddings in an NMT decoder that can be used over word or subword sequences.", "labels": [], "entities": []}, {"text": "2. We explore how our method interacts with BPE over a range of merge operations (including word-level and purely characterlevel) and highlight that there is no \"typical BPE\" setting for low-resource NMT.", "labels": [], "entities": []}, {"text": "3. We evaluate our model on 14 target languages and observe consistent improvements over baselines.", "labels": [], "entities": []}, {"text": "Furthermore, we analyze to what extent the success of our method corresponds to improved handling of target language morphology.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our character aware model on 14 different languages in a low-resource setting.", "labels": [], "entities": []}, {"text": "Additionally, we sweep over several BPE merge hyperparameter settings from character-level to fully word-level for both our model and the baseline and find consistent gains in the character-aware model over the baseline.", "labels": [], "entities": []}, {"text": "These gains are stable across all BPE merge hyperparameters all the way up to word-level where they are the highest.", "labels": [], "entities": []}, {"text": "We use a collection of TED talk transcripts.", "labels": [], "entities": [{"text": "TED talk transcripts", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.6403177181879679}]}, {"text": "This dataset has languages with a variety of morphological typologies, which allows us to observe how the success of our character-aware decoder relates to morphological complexity.", "labels": [], "entities": []}, {"text": "We keep the source language fixed as English and translate into 14 different languages, since our focus is on the decoder.", "labels": [], "entities": []}, {"text": "The training sets for each vary from 74k sentences pairs for  Ukrainian to around 174k sentences pairs for Russian (provided in Appendix A), but the validation and test sets are \"multi-way parallel\", meaning the English sentences (the source side in our experiments) are the same across all 14 languages, and are about 2k sentences each.", "labels": [], "entities": []}, {"text": "We filter out training pairs where the source sentence was longer that 50 tokens (before applying BPE).", "labels": [], "entities": [{"text": "BPE", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.839828372001648}]}, {"text": "For word-level results, we used a vocabulary size of 100k (keeping the most frequent types) and replaced rare words by an <UNK> token.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiments to determine the effectiveness of com- position based embeddings and gated embeddings. We used  en-de language pair from the TED multi-target dataset. Std.  is our baseline with standard word embeddings, model C is  the composition only model and CG combines the character- aware (composed) embedding and standard embedding via a  gating function.", "labels": [], "entities": [{"text": "TED multi-target dataset", "start_pos": 147, "end_pos": 171, "type": "DATASET", "confidence": 0.8382410804430643}]}, {"text": " Table 2: Best BLEU scores swept over 6 different BPE merge setting (1.6k, 3.2k, 7.5k, 15k, 30k, 60k), and at a standard  setting of 30k. We notice a consistent improvement across languages and settings of the merge operation parameter.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9986134767532349}]}, {"text": " Table 3: BLEU scores (lowercased) comparing character- level models against CG when used on 30k BPE sequences.  We show that without sweeping BPE, CG generally outper- forms purely character-level methods, even when the purely  character-level networks are deepened as was shown to help  in Cherry et al. (2018).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994513392448425}]}, {"text": " Table 4: The Pearsons correlation between the features and  the relative gain in BLEU obtained by the CG model. See  Section 6 for details regarding features.", "labels": [], "entities": [{"text": "Pearsons correlation", "start_pos": 14, "end_pos": 34, "type": "METRIC", "confidence": 0.9745667278766632}, {"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9985083937644958}]}, {"text": " Table 6: Number of sentences in training data for each language", "labels": [], "entities": []}, {"text": " Table 7: BLEU scores (case insensitive) for a standard embedding encoder-decoder baseline (Std), and character-aware model,  composed embedding combined with standard embedding (CG) for 14 languages and various BPE merge hyperparameters. For  purely character-level we only train the standard model as CG would not have a sequence of characters to compose. For BPE  of 60k and word-level we use the softmax approximation described. We see that CG obtains the best result in all languages.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993330836296082}]}]}