{"title": [{"text": "Open Sesame: Getting Inside BERT's Linguistic Knowledge", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.8983490467071533}]}], "abstractContent": [{"text": "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear infor-mation?", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.7460691332817078}]}, {"text": "Recent work has shown that contex-tual representations like BERT perform well on tasks that require sensitivity to linguistic structure.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9813792705535889}]}, {"text": "We present here two studies which aim to provide a better understanding of the nature of BERT's representations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.5675884485244751}]}, {"text": "The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors.", "labels": [], "entities": []}, {"text": "In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.963210940361023}]}, {"text": "We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found inhuman processing of reflexive anaphora.", "labels": [], "entities": [{"text": "BERT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9207740426063538}]}], "introductionContent": [{"text": "Word embeddings have become an important cornerstone in any NLP pipeline.", "labels": [], "entities": []}, {"text": "Although such embeddings traditionally involve context-free distributed representations of words (), recent successes with contextualized representations) have led to a paradigm shift.", "labels": [], "entities": []}, {"text": "One prominent architecture is BERT (), a Transformer-based model that learns bidirectional encoder representations for words, on the basis of a masked language model and sentence adjacency training objective.", "labels": [], "entities": [{"text": "BERT", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9926233291625977}]}, {"text": "Simply using BERT's representations in place of traditional embeddings has resulted in state-of-the-art performance on a range of downstream tasks including summarization, question answering and textual entailment.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.8686979413032532}, {"text": "summarization", "start_pos": 157, "end_pos": 170, "type": "TASK", "confidence": 0.9857902526855469}, {"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.8600415885448456}, {"text": "textual entailment", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.6833536177873611}]}, {"text": "It is still, however, unclear why BERT representations perform well.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8476645350456238}]}, {"text": "A flurry of recent work () has explored how recurrent neural language models perform in cases that require sensitivity to hierarchical syntactic structure, and study how they do so, particularly in the domain of agreement.", "labels": [], "entities": []}, {"text": "In these studies, a pre-trained language model is asked to predict the next word in a sentence (a verb in the target sentence) following a sequence that may include other intervening nouns with different grammatical features (e.g., \"the bear by the trees eats...\").", "labels": [], "entities": []}, {"text": "The predicted verb should agree with the subject noun (bear) and not the attractors (trees), in spite of the latter's recency.", "labels": [], "entities": []}, {"text": "Such analyses have revealed that LSTMs exhibit state tracking and explicit notions of word order for modeling long term dependencies, although this effect is diluted when sequential and structural information in a sentence conflict.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 47, "end_pos": 61, "type": "TASK", "confidence": 0.7099375873804092}]}, {"text": "Further work by and others) argues that RNNs acquire grammatical competence in agreement that is more abstract than word collocations, although language model performance that requires sensitivity to the phenomena such as reflexive anaphora, non-local agreement and negative polarity remains low.", "labels": [], "entities": []}, {"text": "Meanwhile, studies evaluating which linguistic phenomena are encoded by contextualized representations successfully demonstrate that purely self-attentive architectures like BERT can capture hierarchy-sensitive, syntactic dependencies, and even support the extraction of dependency parses.", "labels": [], "entities": [{"text": "BERT", "start_pos": 174, "end_pos": 178, "type": "METRIC", "confidence": 0.850975513458252}, {"text": "extraction of dependency parses", "start_pos": 257, "end_pos": 288, "type": "TASK", "confidence": 0.7131329104304314}]}, {"text": "However, the way in which BERT does this has been less studied.", "labels": [], "entities": [{"text": "BERT", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.4965316951274872}]}, {"text": "In this paper, we investigate how and where the representations produced by pre-trained BERT models) express the hierarchical organization of a sentence.", "labels": [], "entities": [{"text": "BERT", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.8265208601951599}]}, {"text": "We proceed in two ways.", "labels": [], "entities": []}, {"text": "The first involves the use of diagnostic classifiers to probe the presence of hierarchical and linear properties in the representations of words.", "labels": [], "entities": []}, {"text": "However, unlike past work, we train these classifiers using a \"poverty of the stimulus\" paradigm, where the training data admit both linear and hierarchical solutions that can be distinguished by an enriched generalization set.", "labels": [], "entities": []}, {"text": "This method allows us to identify what kinds of information are represented most robustly and transparently in the BERT embeddings.", "labels": [], "entities": [{"text": "BERT", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.6343833804130554}]}, {"text": "We find that as we use embeddings from higher layers, the prevalence of linear/sequential information decreases, while the availability of on hierarchical information increases, suggesting that with each layer, BERT phases out positional information in favor of hierarchical features of increasing complexity.", "labels": [], "entities": [{"text": "BERT", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.8497764468193054}]}, {"text": "In the second set of experiments, we explore a novel approach to the study of BERT's selfattention vectors.", "labels": [], "entities": [{"text": "BERT", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.6178667545318604}]}, {"text": "Past explorations of attention mechanisms, whether in the domain of vision () or NLP (, have largely involved a range of visualization techniques or the study of the general distribution of attention.", "labels": [], "entities": []}, {"text": "Our work takes a quantitative approach to the study of attention and its encoding of syntactic dependencies.", "labels": [], "entities": []}, {"text": "Specifically, we consider the relationships between verbs and the subjects with which they agree, and reflexive anaphors and their antecedents.", "labels": [], "entities": []}, {"text": "Building on past work in psycholinguistics, we consider the influence of distractor noun phrases on the identification of these dependencies.", "labels": [], "entities": []}, {"text": "We propose a simple attention-based metric called the confusion score that captures BERT's response to syntactic distortions in an input sentence.", "labels": [], "entities": [{"text": "confusion score", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.89395871758461}, {"text": "BERT", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9869185090065002}]}, {"text": "This score provides a novel quantitative method of evaluating BERT's syntactic knowledge as encoded in its attention vectors.", "labels": [], "entities": [{"text": "BERT", "start_pos": 62, "end_pos": 66, "type": "TASK", "confidence": 0.40245434641838074}]}, {"text": "We find that BERT does indeed leverage syntactic relationships between words to preferentially attend to the \"correct\" noun phrase for the purposes of agreement and anaphora, though syntactic structure does not show the strong categorical effects we sometimes find in natural language.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.962879478931427}]}, {"text": "This result again points to a representation of syntacticallyrelevant hierarchical information in BERT, this time through attention weightings.", "labels": [], "entities": [{"text": "BERT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.937465250492096}]}, {"text": "Our analysis thus provides evidence that BERT's self-attention layers compose increasingly abstract representations of linguistic structure without explicit word order information, and that structural information is expressly favored over linear information.", "labels": [], "entities": []}, {"text": "This explains why BERT can perform well on downstream NLP tasks, which typically require complex modeling of structural relationships.", "labels": [], "entities": [{"text": "BERT", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.986998975276947}]}], "datasetContent": [{"text": "We construct synthetic datasets using contextfree grammars (shown in Appendix A.1) for both subject-verb agreement and reflexive anaphora and compute mean confusion scores across multiple sentences.", "labels": [], "entities": [{"text": "mean confusion scores", "start_pos": 150, "end_pos": 171, "type": "METRIC", "confidence": 0.8147217233975729}]}, {"text": "This allows us to control for semantic effects on the confusion score.", "labels": [], "entities": []}, {"text": "All datasets for each condition contain 10000 examples.", "labels": [], "entities": []}, {"text": "In the subject-verb agreement datasets, we vary 1) the type of subordinate clause (prepositional phrase, PP; or relative clause, RC), and 2) the number on the distractor noun phrase.", "labels": [], "entities": []}, {"text": "All conditions should be unambiguous, since only the head noun phrase can agree with the auxiliary.", "labels": [], "entities": []}, {"text": "In the reflexive anaphora datasets, we vary 1) the presence of a RC, 2) the gender match between the RC's noun phrase and the reflexive 3) the presence of an object noun phrase, and 4) the gender match between the object noun and the reflexive.", "labels": [], "entities": []}, {"text": "Conditions R1, R5, R6 are ambiguous conditions, as they include an object noun phrase that matches the reflexive in gender.", "labels": [], "entities": []}, {"text": "In other conditions, only the head noun phrase is the possible antecedent: the object mismatches in features and the noun phrase within the RC is grammatically inaccessible.", "labels": [], "entities": []}, {"text": "For each example attest time, after computing the logits we obtain the index of the classifier's most confident guess within the sentence: The average y i * across the test set is reported as the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.7039667963981628}]}, {"text": "Layerwise diagnosis One key aspect of our experiments is the training of layer-specific classifiers for all layers.", "labels": [], "entities": [{"text": "Layerwise diagnosis", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7356246709823608}]}, {"text": "This yields a layerwise diagnosis of the information content in BERT's embeddings, providing a glimpse into how BERT internally manipulates and composes linguistic information.", "labels": [], "entities": []}, {"text": "We also train classifiers on the pre-embeddings, which can be considered as the \"zero-th\" layer of BERT and hence act as useful baselines for content present in the input.", "labels": [], "entities": [{"text": "BERT", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.7524966597557068}]}], "tableCaptions": []}