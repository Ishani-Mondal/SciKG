{"title": [{"text": "A Survey on Biomedical Image Captioning", "labels": [], "entities": [{"text": "Biomedical Image Captioning", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7245901425679525}]}], "abstractContent": [{"text": "Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians.", "labels": [], "entities": [{"text": "Image captioning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7734602987766266}]}, {"text": "This article is the first survey of biomedical image caption-ing, discussing datasets, evaluation measures, and state of the art methods.", "labels": [], "entities": [{"text": "biomedical image caption-ing", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.668678214152654}]}, {"text": "Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Radiologists or other physicians may need to examine many biomedical images daily, e.g. PET/CT scans or radiology images, and write their findings as medical reports.", "labels": [], "entities": []}, {"text": "Methods assisting physicians to focus on interesting image regions () or to describe findings ( can reduce medical errors (e.g., suggesting findings to inexperienced physicians) and benefit medical departments by reducing the cost per exam (.", "labels": [], "entities": []}, {"text": "Despite the importance of biomedical image captioning, related resources are not easily accessible, hindering the emergence of new methods.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.6654881834983826}]}, {"text": "The publicly available datasets are only three and not always directly available.", "labels": [], "entities": []}, {"text": "1 Also, there is currently no assessment of simple baselines to determine the lower performance boundary and estimate the difficulty of the task.", "labels": [], "entities": []}, {"text": "By contrast, complex (typically deep learning) systems are compared to other complex systems, without establishing if they surpass baselines (; ).", "labels": [], "entities": []}, {"text": "Furthermore, current evaluation measures are adopted directly from generic image captioning, ignoring the more challenging nature of the biomedical domain (Cohen and Demner-Fushman, 2014) and thus the potential benefit from employing other measures.", "labels": [], "entities": [{"text": "generic image captioning", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6165000200271606}]}, {"text": "Addressing these limitations is crucial for the fast development of the field.", "labels": [], "entities": []}, {"text": "This paper is the first overview of biomedical image captioning methods, datasets, and evaluation measures.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6896384755770365}]}, {"text": "Section 2 describes publicly available datasets.", "labels": [], "entities": []}, {"text": "To increase accessibility and ensure consistent results across systems, we provide code to download and preprocess all the datasets.", "labels": [], "entities": []}, {"text": "Section 3 describes biomedical image captioning methods and attempts to compare their results, with the caveat that only two works use the same dataset () and can be directly compared.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6360557476679484}]}, {"text": "Section 4 describes evaluation measures that have been used and introduces two baselines.", "labels": [], "entities": []}, {"text": "The first one is based on word frequencies and provides a low performance boundary.", "labels": [], "entities": []}, {"text": "The second one is based on image retrieval and the assumption that similar images have similar diagnoses; we show that it is a strong baseline outperforming the state of the art in at least one dataset.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7781195342540741}]}, {"text": "Section 7 discusses related (mostly deep learning) biomedical image processing methods for other tasks, such as image classification and segmentation.", "labels": [], "entities": [{"text": "image classification", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8046374320983887}]}, {"text": "Section 8 highlights limitations of our work and proposes future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets for biomedical image captioning comprise medical images and associated texts.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6661498546600342}]}, {"text": "Publicly available datasets contain X-rays (IU X-RAY in), clinical photographs (PEIR GROSS in), or a mixture of X-rays and photographs (ICLEF-CAPTION in).", "labels": [], "entities": [{"text": "IU", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9110528230667114}, {"text": "PEIR GROSS", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.8546541035175323}, {"text": "ICLEF-CAPTION", "start_pos": 136, "end_pos": 149, "type": "METRIC", "confidence": 0.8478161096572876}]}, {"text": "The associated texts maybe single sentences describing the images, or longer medical reports based on the images (e.g., as in).", "labels": [], "entities": []}, {"text": "Current publicly available datasets are rather small (IU X-RAY, PEIR GROSS) or noisy (e.g., IMAGE-CLEF, which is the largest dataset, was created by automatic means that introduced a lot of noise).", "labels": [], "entities": [{"text": "PEIR GROSS", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.7087889611721039}]}, {"text": "We do not include in datasets like the one of, because their medical reports are not publicly available.", "labels": [], "entities": []}, {"text": "Furthermore, we observe that all three publicly available biomedical image captioning datasets suffer from two main shortcomings: \u2022 There is a great class imbalance, with most images having no reported findings.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6871305604775747}]}, {"text": "\u2022 The wide range of diseases leads to very scarce occurrences of disease-related terms, making it difficult for models to generalize.", "labels": [], "entities": []}, {"text": "It measures word n-gram overlap between the generated and the ground truth caption.", "labels": [], "entities": []}, {"text": "A brevity penalty is added to penalize short generated captions.", "labels": [], "entities": []}, {"text": "BLEU-1 considers unigrams (i.e., words), while BLEU-2, -3, -4 consider bigrams, trigrams, and 4-grams respectively.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9483676552772522}, {"text": "BLEU-2", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9966787099838257}]}, {"text": "The average of the four variants was used as the official measure in ICLEF-CAPTION.", "labels": [], "entities": [{"text": "ICLEF-CAPTION", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.5559509992599487}]}, {"text": "METEOR () extended BLEU-1 by employing the harmonic mean of precision and recall (F-score), biased towards recall, and by also employing stemming (Porter stemmer) and synonymy (WordNet).", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.5355591177940369}, {"text": "BLEU-1", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9963895678520203}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9796304702758789}, {"text": "recall (F-score)", "start_pos": 74, "end_pos": 90, "type": "METRIC", "confidence": 0.818514883518219}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9940067529678345}, {"text": "WordNet", "start_pos": 177, "end_pos": 184, "type": "DATASET", "confidence": 0.9508615732192993}]}, {"text": "To take into account longer subsequences, it includes a penalty of up to 50% when no common n-grams exist between the machine-generated description and the reference.", "labels": [], "entities": []}, {"text": "ROUGE-L () is the ratio of the length of the longest common subsequence between the machine-generated description and the reference human description, to the size of the reference (ROUGE-L recall); or to the generated description (ROUGE-L precision); or a combination of the two (ROUGE-L F-measure).", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9746567010879517}, {"text": "ROUGE-L recall)", "start_pos": 181, "end_pos": 196, "type": "METRIC", "confidence": 0.8474986950556437}, {"text": "ROUGE-L precision", "start_pos": 231, "end_pos": 248, "type": "METRIC", "confidence": 0.84107905626297}, {"text": "ROUGE-L F-measure)", "start_pos": 280, "end_pos": 298, "type": "METRIC", "confidence": 0.8338056206703186}]}, {"text": "We note that several ROUGE variants exist, based on different ngram lengths, stemming, stopword removal, etc., but ROUGE-L is the most commonly used variant in biomedical image captioning so far.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 160, "end_pos": 187, "type": "TASK", "confidence": 0.6265483895937601}]}, {"text": "measures the cosine similarity between n-gram TF-IDF representations of the two captions (words are also stemmed).", "labels": [], "entities": []}, {"text": "This is calculated for unigrams to 4-grams and their average is returned as the final evaluation score.", "labels": [], "entities": []}, {"text": "The intuition behind using TF-IDF is to reward frequent caption terms while penalizing common ones (e.g., stopwords).", "labels": [], "entities": []}, {"text": "However, biomedical image captioning datasets contain many scientific terms (e.g., disease names) that are common across captions (or more gener-ally document collections), which maybe mistakenly penalized.", "labels": [], "entities": [{"text": "biomedical image captioning", "start_pos": 9, "end_pos": 36, "type": "TASK", "confidence": 0.6806461811065674}]}, {"text": "We also noticed that the scores returned by the provided CIDER implementation may exceed 100%.", "labels": [], "entities": []}, {"text": "We exclude CIDER results, since these issues need to be investigated further.) extracts tuples from the two captions (machine-generated, reference), containing objects, attributes and/or relations; e.g., (patient), (has, pain), (male, patient).", "labels": [], "entities": []}, {"text": "Precision and recall are computed using WordNet synonym matching between the two sets of tuples, and the F1 score is returned.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9899118542671204}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9986355900764465}, {"text": "F1 score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.987852156162262}]}, {"text": "The creators of SPICE report improved results over both METEOR and CIDER, but it has been noted that results depend on parsing quality (.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.8532285094261169}]}, {"text": "When experimenting with the provided implementation 16 of this measure, we noticed that it failed to parse long texts to evaluate them.", "labels": [], "entities": []}, {"text": "Similarly to CIDER, we exclude SPICE from further analysis below.", "labels": [], "entities": [{"text": "CIDER", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8039069175720215}]}, {"text": "Word Mover's Distance (WMD) () computes the minimum cumulative cost required to move all word embeddings of one caption to aligned word embeddings of the other caption.", "labels": [], "entities": [{"text": "Word Mover's Distance (WMD)", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.7586094694478172}]}, {"text": "It completely ignores, however, word order, and thus readability, which is one of the main assessment dimensions in the biomedical field (.", "labels": [], "entities": []}, {"text": "Other previously discussed n-gram based measures also largely ignore word order, but at least consider local order (inside n-grams).", "labels": [], "entities": []}, {"text": "WMD scores are included in as similarity values WMS = (1 + WMD) \u22121 .  As shown in, FREQUENCY scores high when evaluated with BLEU-1 and WMS, probably because these measures are based on unigrams.", "labels": [], "entities": [{"text": "WMS", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.8320716023445129}, {"text": "FREQUENCY", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9965530633926392}, {"text": "BLEU-1", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9982878565788269}]}, {"text": "FREQUENCY, which simply concatenates the most common words of the training captions, is rewarded every time the most common words appear in the reference captions.", "labels": [], "entities": [{"text": "FREQUENCY", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9962555170059204}]}, {"text": "To our surprise, NEAREST-NEIGHBOR outperforms not only FREQUENCY, but also the state of the art in PEIR GROSS, in all evaluation measures).", "labels": [], "entities": [{"text": "NEAREST-NEIGHBOR", "start_pos": 17, "end_pos": 33, "type": "METRIC", "confidence": 0.9140251278877258}, {"text": "FREQUENCY", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.998542070388794}, {"text": "PEIR", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.6661507487297058}, {"text": "GROSS", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.7055863738059998}]}, {"text": "This could be explained by the fact that PEIR GROSS images are phototographs of medical conditions, not X-rays, and thus they maybe handled better by the RESNET-18 encoder of NEAREST-NEIGHBOR.", "labels": [], "entities": [{"text": "PEIR", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.600483775138855}, {"text": "GROSS", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.48700717091560364}, {"text": "NEAREST-NEIGHBOR", "start_pos": 175, "end_pos": 191, "type": "DATASET", "confidence": 0.8311265110969543}]}, {"text": "In future work, we intend to experiment with an encoder trained on medical images (e.g., CHEXNET).", "labels": [], "entities": []}, {"text": "In IU X-RAY, NEAREST-NEIGHBOR scores low in all measures, possibly because in this case the images are X-rays and the RESNET-18 encoder fails to handle them properly.", "labels": [], "entities": [{"text": "IU X-RAY", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.6133115291595459}, {"text": "NEAREST-NEIGHBOR", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9057530164718628}]}, {"text": "Again, by experimenting with a different encoder, trained on Xrays, this baseline might be improved.", "labels": [], "entities": []}, {"text": "In ICLEF-CAPTION, both of our baselines perform poorly, and much worse than the best system (cf.), which achieved average BLEU 26%.", "labels": [], "entities": [{"text": "ICLEF-CAPTION", "start_pos": 3, "end_pos": 16, "type": "METRIC", "confidence": 0.5889788866043091}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9738552570343018}]}, {"text": "This is partially explained by the size of this dataset (Section 2), which contains multiple different images and captions.", "labels": [], "entities": []}, {"text": "Moreover, this dataset was created automatically and includes noise and a great diversity of image types (e.g., irrelevant, generic images such as maps) and captions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Evaluation of FREQUENCY and NEAREST-NEIGHBOR on all datasets, with BLEU-1/-2/-3/-4 (B1, B2, B3,  B4), METEOR (MET), ROUGE (ROU), Word Mover's Similarity (WMS) percent scores. Best results to date per  dataset are also included (state of the art). In ICLEF-CAPTION, only the average BLEU has been reported (26.00).", "labels": [], "entities": [{"text": "FREQUENCY", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9928250312805176}, {"text": "NEAREST-NEIGHBOR", "start_pos": 38, "end_pos": 54, "type": "METRIC", "confidence": 0.648887574672699}, {"text": "BLEU-1", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9972108006477356}, {"text": "METEOR (MET)", "start_pos": 112, "end_pos": 124, "type": "METRIC", "confidence": 0.9044594317674637}, {"text": "ROUGE (ROU)", "start_pos": 126, "end_pos": 137, "type": "METRIC", "confidence": 0.9463476836681366}, {"text": "ICLEF-CAPTION", "start_pos": 260, "end_pos": 273, "type": "DATASET", "confidence": 0.5575301647186279}, {"text": "BLEU", "start_pos": 292, "end_pos": 296, "type": "METRIC", "confidence": 0.9983564019203186}]}]}