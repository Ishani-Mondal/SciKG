{"title": [{"text": "Ensembles of Neural Morphological Inflection Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate different ensemble learning techniques for neural morphological inflection using bidirectional LSTM encoder-decoder models with attention.", "labels": [], "entities": []}, {"text": "We experiment with weighted and un-weighted majority voting and bagging.", "labels": [], "entities": []}, {"text": "We find that all investigated ensemble methods lead to improved accuracy over a base-line of a single model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9987375140190125}]}, {"text": "However, contrary to expectation based on earlier work by Najafi et al.", "labels": [], "entities": []}, {"text": "(2018) and Silfverberg et al.", "labels": [], "entities": []}, {"text": "(2017), weighting does not deliver clear benefits.", "labels": [], "entities": [{"text": "weighting", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9292184710502625}]}, {"text": "Bagging was found to underper-form plain voting ensembles in general.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9512137174606323}]}], "introductionContent": [{"text": "Natural language processing (NLP) systems for languages which exhibit rich inflectional morphology often suffer from data sparsity.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7549847116072973}]}, {"text": "The root cause of this sparsity is a prohibitively high typetoken ratio which is typical for morphologically complex languages.", "labels": [], "entities": []}, {"text": "A common way to alleviate the problem is to incorporate modeling of inflectional morphology instead of building purely word-based NLP systems-by representing word forms as combinations of a lemma and morphosyntactic description, data sparsity is reduced both in analysis and generation tasks.", "labels": [], "entities": []}, {"text": "Morphology-aware language generation systems usually require a component which generates inflected word forms from lemmas and morphosyntactic descriptions.", "labels": [], "entities": [{"text": "Morphology-aware language generation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7686504125595093}]}, {"text": "Such a component is called a morphological inflection (MI) 1 model.", "labels": [], "entities": []}, {"text": "For example, given the Italian verb mangiare 'to eat' and the morphosyntactic description V;IND;FUT;1;SG, an MI system should generate the 1st person singular future indicative form manger\u00f2 as output.", "labels": [], "entities": [{"text": "IND", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9596570730209351}, {"text": "FUT", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.6874420642852783}]}, {"text": "Traditionally, rule-based methods have been applied in morphological inflection and analysis.", "labels": [], "entities": [{"text": "morphological inflection and analysis", "start_pos": 55, "end_pos": 92, "type": "TASK", "confidence": 0.8048475235700607}]}, {"text": "Recently, machine learning methods have also gained ground in this task.", "labels": [], "entities": []}, {"text": "Especially deep learning methods have delivered strong results in MI.", "labels": [], "entities": [{"text": "MI", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9923579096794128}]}, {"text": "Starting with the work by, the predominant approach has been to use a bidirectional RNN encoder-decoder system with attention.", "labels": [], "entities": []}, {"text": "While neural encoder-decoder systems have been successfully applied to the MI task and many papers have investigated simple model ensembles using unweighted majority voting, few studies have fully investigated ensembles of neural systems.", "labels": [], "entities": [{"text": "MI task", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.9357553720474243}]}, {"text": "Weighted model ensembles for MI are proposed by and but neither provides a detailed analysis of model ensembles.", "labels": [], "entities": [{"text": "MI", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9742323160171509}]}, {"text": "This paper compares the performance of different model ensembles for MI.", "labels": [], "entities": [{"text": "MI", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9694802165031433}]}, {"text": "We explore methods which use unweighted and weighted voting strategies to combine outputs of different models.", "labels": [], "entities": []}, {"text": "We also investigate different ways of training the component models in the ensemble using both random initialization of model parameters and varying the training data using bootstrap aggregation commonly known as bagging.", "labels": [], "entities": []}, {"text": "Bagging is a popular ensemble method where new training sets are created by resampling from an existing training set.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9599872827529907}]}, {"text": "Both bagging and majority voting are known to reduce the variance of the model.", "labels": [], "entities": [{"text": "variance", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9779295325279236}]}, {"text": "This makes them suitable for neural models which are known to obtain high variance.", "labels": [], "entities": []}, {"text": "Due to practicality concerns, we limit the scope of the paper to methods which can combine existing models without changes to model architecture.", "labels": [], "entities": []}, {"text": "Therefore, we do not explore merging model predictions during beam search in decoding or averaging model parameters.", "labels": [], "entities": []}, {"text": "We perform experiments on a selection often languages: Arabic, Finnish, Georgian, German, Hindi, Italian, Khaling, Navajo, Russian, and Turkish.", "labels": [], "entities": []}, {"text": "Our experiments on this morphologically and areally diverse set of languages show that model ensembles tend to deliver the best results confirming results presented in earlier work.", "labels": [], "entities": []}, {"text": "However, our findings for weighted ensembles and bagging are largely negative.", "labels": [], "entities": []}, {"text": "Contrary to expectation based on the work by and weighting did not deliver clear benefits over unweighted model ensembles.", "labels": [], "entities": []}, {"text": "Bagging, in general, does deliver improvements in model accuracy compared to a baseline of a single model but does not outperform plain majority voting.", "labels": [], "entities": [{"text": "Bagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9756017327308655}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9855208992958069}]}], "datasetContent": [{"text": "Baseline For baseline experiments, 10 inflection models were trained for each language with different random initial values for the model parameters.", "labels": [], "entities": []}, {"text": "We trained models both for the high and medium training data settings.", "labels": [], "entities": []}, {"text": "Model parameters were optimized using the Adam optimization algorithm) and we used minibatches of 64 examples during training.", "labels": [], "entities": []}, {"text": "According to preliminary experiments, the development accuracy and perplexity for each language converged around 6,000-10,000 training steps for each dataset, where one training step corresponds to updating on model parameters fora single minibatch (64 items).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.995044469833374}]}, {"text": "To ensure convergence for all languages, we therefore trained all models for 12,500 training steps.", "labels": [], "entities": []}, {"text": "We do not employ character dropout.", "labels": [], "entities": []}, {"text": "All our models are implemented using the OpenNMT neural machine translation toolkit (.", "labels": [], "entities": [{"text": "OpenNMT neural machine translation", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.7714468836784363}]}, {"text": "Ensembles The 10 baseline models of each language and training data setting were used to form voting ensembles.", "labels": [], "entities": []}, {"text": "We applied both naive majority voting and weighted majority voting.", "labels": [], "entities": []}, {"text": "For bagging, two experiments are conducted on the high, medium and low training data setting.", "labels": [], "entities": [{"text": "bagging", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9586019515991211}]}, {"text": "In the first experiment, we form 10 training sets by resampling from the original training sets.", "labels": [], "entities": []}, {"text": "In the second one, we form 100 new training sets by resampling.", "labels": [], "entities": []}, {"text": "Each of the sampled training sets has the same size as the original training set for the high, medium and low setting, respectively.", "labels": [], "entities": []}, {"text": "Subsequently, we train models on each of the newly formed training sets.", "labels": [], "entities": []}, {"text": "In addition to using different data for training, diversity between the ensemble members is ensured by different random initialization of model parameters.", "labels": [], "entities": []}, {"text": "In each experiment, both naive majority voting and weighted majority voting are applied to outputs of each model to form two ensembles for each language.", "labels": [], "entities": []}, {"text": "shows results for all experiments.", "labels": [], "entities": []}, {"text": "On the whole, ensembles delivered improvements with regard to the baseline of a single model.", "labels": [], "entities": []}, {"text": "This holds true both when comparing to the mean accuracy of the 10 individual baseline models and when comparing to the best individual baseline model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9210689663887024}]}, {"text": "In general, the best accuracies were obtained by naive and weighted majority voting ensembles.", "labels": [], "entities": []}, {"text": "For the high, medium and low settings, we obtain small improvements by weighting both majority voting and bagging ensembles.", "labels": [], "entities": []}, {"text": "However, inmost cases these improvements are not statistically significant at the 95% confidence level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracies (%) of bagging and majority voting ensembles and best baseline models, and baseline model means for", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.998773992061615}]}]}