{"title": [{"text": "Leveraging Non-Conversational Tasks for Low Resource Slot Filling: Does it help?", "labels": [], "entities": [{"text": "Low Resource Slot Filling", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6741074025630951}]}], "abstractContent": [{"text": "Slot filling is a core operation for utterance understanding in task-oriented dialogue systems.", "labels": [], "entities": [{"text": "Slot filling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9269698858261108}, {"text": "utterance understanding", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.9012241363525391}]}, {"text": "Slots are typically domain-specific, and adding new domains to a dialogue system involves data and time-intensive processes.", "labels": [], "entities": []}, {"text": "A popular technique to address the problem is transfer learning, where it is assumed the availability of a large slot filling dataset for the source domain, to be used to help slot filling on the target domain, with fewer data.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.9688321352005005}, {"text": "slot filling", "start_pos": 176, "end_pos": 188, "type": "TASK", "confidence": 0.7857745587825775}]}, {"text": "In this work, instead, we propose to leverage source tasks based on semantically related non-conversational resources (e.g., semantic sequence tagging datasets), as they are both cheaper to obtain and reusable to several slot filling domains.", "labels": [], "entities": [{"text": "semantic sequence tagging", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.6861218810081482}, {"text": "slot filling domains", "start_pos": 221, "end_pos": 241, "type": "TASK", "confidence": 0.8033257524172465}]}, {"text": "We show that using auxiliary non-conversational tasks in a multi-task learning setup consistently improves low resource slot filling performance.", "labels": [], "entities": [{"text": "low resource slot filling", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7747759521007538}]}], "introductionContent": [{"text": "Language understanding in task-oriented dialogue systems involves recognizing information (i.e., slot filling) expressed in an utterance to accomplish a particular dialogue task.", "labels": [], "entities": [{"text": "Language understanding", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6767311096191406}, {"text": "slot filling) expressed in an utterance", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.8529009478432792}]}, {"text": "For example, in a flight booking scenario, the utterance \"show me all Delta flights from Milan to New York\" contains information belonging to slots in the flight domain, namely airline name (Delta), origin (Milan), and destination (New York).", "labels": [], "entities": []}, {"text": "Slots are usually predefined and domain-specific, e.g. in a hotel domain slots can be different, such as room type, length of stay etc.", "labels": [], "entities": []}, {"text": "Although recent neural based models ( have shown remarkable performance in slot filling, they are still based on large labeled data, which means that training a separate model for each domain involves a resource intensive process.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.9113524258136749}]}, {"text": "Thus, as more domains are added to the system, methods that can generalize slot filling to new domains with limited labeled data (i.e., low-resource settings) are preferable.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 75, "end_pos": 87, "type": "TASK", "confidence": 0.7671570777893066}]}, {"text": "Existing works in low resource slot filling are mostly based on transfer learning (, whose aim is to leverage relatively large resources in a source domain (D S ) fora source task (T S ), to help a task (T T ) in a target domain, where less data are available.", "labels": [], "entities": [{"text": "low resource slot filling", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6953255236148834}]}, {"text": "Depending on how the adaptation is performed, there are two notable approaches: data-driven adaptation, and model-driven adaptation.", "labels": [], "entities": []}, {"text": "Essentially, both approaches produce a model on the target domain performing training on the same task (slot filling, in our case), i.e., assuming (T S = T T ), although from different domains, i.e. (D S = D T ).", "labels": [], "entities": [{"text": "slot filling", "start_pos": 104, "end_pos": 116, "type": "TASK", "confidence": 0.7589647769927979}]}, {"text": "All of these approaches assume that slot filling datasets for the source domain are available, and little effort has been devoted in finding and exploiting cheaper T S , which is crucial in a situation where a slot filling dataset in D S is not ready yet (cold-start).", "labels": [], "entities": [{"text": "slot filling", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.759266197681427}]}, {"text": "Accordingly, we attempt to leverage nonconversational source tasks (T S = T T ) i.e., tasks that use widely available non-conversational resources, to help slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 156, "end_pos": 168, "type": "TASK", "confidence": 0.9388884007930756}]}, {"text": "These resources are cheaper to obtain compared to domain-specific slot filling datasets, and many of them are annotated with rich linguistic knowledge, which is potentially useful for slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 66, "end_pos": 78, "type": "TASK", "confidence": 0.760177344083786}, {"text": "slot filling", "start_pos": 184, "end_pos": 196, "type": "TASK", "confidence": 0.9651897847652435}]}, {"text": "Among these resources, we mention PropBank () and FrameNet (, which consist of annotated documents with verb and frame-based semantic roles, respectively; CoNLL 2003) and OntoNotes (, which provide named entity information; and Abstract Meaning Representation (AMR) (, which provides a graph-based seman-: An example of slot filling annotation from the ATIS (Airline Travel Information System) dataset and author-annotated NER and SemTag in IOB format.", "labels": [], "entities": [{"text": "CoNLL 2003", "start_pos": 155, "end_pos": 165, "type": "DATASET", "confidence": 0.9288640320301056}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 228, "end_pos": 265, "type": "TASK", "confidence": 0.6918895741303762}, {"text": "slot filling annotation", "start_pos": 320, "end_pos": 343, "type": "TASK", "confidence": 0.8341628511746725}, {"text": "ATIS (Airline Travel Information System) dataset", "start_pos": 353, "end_pos": 401, "type": "DATASET", "confidence": 0.6744585335254669}]}, {"text": "Some ATIS slots correspond to NER or SemTag labels, such as FROM LOC and TO LOC with GPE in NER and SemTag.", "labels": [], "entities": [{"text": "FROM LOC", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9578135311603546}, {"text": "TO LOC", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.8168945610523224}, {"text": "GPE", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9470497965812683}]}, {"text": "Some slot tags can also be composed of several SemTag labels such as COST REL which is composed of TOP (superlative positive) and IST (intersective adjective).", "labels": [], "entities": [{"text": "COST REL", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.687708705663681}, {"text": "TOP", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.994390070438385}]}, {"text": "In this work, we leverage non-conversational tasks as auxiliary tasks in a multi-task learning (MTL) (Caruana, 1997) setup.", "labels": [], "entities": [{"text": "multi-task learning (MTL) (Caruana, 1997)", "start_pos": 75, "end_pos": 116, "type": "TASK", "confidence": 0.6375713586807251}]}, {"text": "Given appropriate auxiliary tasks, MTL has shown to be particularly effective in which labeled data is scarce and has been applied to various NLP tasks such as parsing, POS tagging (, neural machine translation (, and opinion role labeling.", "labels": [], "entities": [{"text": "MTL", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9517578482627869}, {"text": "parsing", "start_pos": 160, "end_pos": 167, "type": "TASK", "confidence": 0.9793994426727295}, {"text": "POS tagging", "start_pos": 169, "end_pos": 180, "type": "TASK", "confidence": 0.8965899050235748}, {"text": "neural machine translation", "start_pos": 184, "end_pos": 210, "type": "TASK", "confidence": 0.6402835547924042}, {"text": "opinion role labeling", "start_pos": 218, "end_pos": 239, "type": "TASK", "confidence": 0.632484366496404}]}, {"text": "While there are potentially many non-conversational tasks that we can use as auxiliary tasks, we focus on those that assign semantic class categories to a word, as they are similar in nature to slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 194, "end_pos": 206, "type": "TASK", "confidence": 0.7880707383155823}]}, {"text": "In particular, in this work we choose Named Entity Recognition (NER) and the recently introduced Semantic Tagging (SemTag) (), motivated by the following rationales: \u2022 Both NER and SemTag are semantically related to slot filling.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.7562869886557261}, {"text": "slot filling", "start_pos": 216, "end_pos": 228, "type": "TASK", "confidence": 0.843595951795578}]}, {"text": "As illustrated in, slot labels may correspond to either NER or SemTag labels.", "labels": [], "entities": [{"text": "NER", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.8393099308013916}]}, {"text": "In addition, SemTag complements NER as its labels subsume NER labels, and thus could be useful to address linguistic phenomena (e.g. comparative expression, intersective adjective) relevant for slot filling and that are beyond named entities.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 194, "end_pos": 206, "type": "TASK", "confidence": 0.856054961681366}]}, {"text": "\u2022 Both NER and SemTag can be re-used in many slot filling domains.", "labels": [], "entities": [{"text": "slot filling domains", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8724093039830526}]}, {"text": "Labels in both tasks are typically more general (coarse-grained) compared to labels in slot filling.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.8193524479866028}]}, {"text": "\u2022 The resources for both tasks are cheaper to obtain compared to domain-specific slot filling datasets, as there have been several initiatives in constructing large datasets for NER and SemTag, for example OntoNotes (Pradhan et al., 2013) and Parallel Meaning Bank (PMB) ( respectively.", "labels": [], "entities": []}, {"text": "This is beneficial in a cold-start situation in which no slot filling dataset is already available in D S . Although NER has been already used in slot filling models, most of these approaches) use and incorporate ground truth NER labels or output of NER systems as features to train a slot filling model, our work differs in the method of learning and leveraging such features from disjoint datasets through MTL and evaluating the performance in low-resource settings.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 146, "end_pos": 158, "type": "TASK", "confidence": 0.789002537727356}]}, {"text": "Our contributions are: (i) we propose to leverage non-conversational tasks, namely NER and SemTag, to improve low resource slot filling through MTL; to our knowledge this MTL combination has not been explored before.", "labels": [], "entities": [{"text": "low resource slot filling", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.6734011694788933}]}, {"text": "(ii) We show that MTL models with NER and SemTag strongly improve single-task slot filling models on three well known datasets.", "labels": [], "entities": [{"text": "MTL", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.945927083492279}, {"text": "single-task slot filling", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.6779444317022959}]}, {"text": "While we focus on using NER and SemTag, our study has shed light on the potential use of non-conversational tasks in general to help low resource slot filling.", "labels": [], "entities": [{"text": "low resource slot filling", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.5879678875207901}]}], "datasetContent": [{"text": "The main objective of our experiments is to validate the hypothesis that using non-conversational tasks as auxiliary tasks in a MTL setup can help low resource slot filling.", "labels": [], "entities": [{"text": "MTL setup", "start_pos": 128, "end_pos": 137, "type": "TASK", "confidence": 0.9005288779735565}, {"text": "low resource slot filling", "start_pos": 147, "end_pos": 172, "type": "TASK", "confidence": 0.7125582173466682}]}, {"text": "In our MTL configuration, the target task (T T ) is slot filling, and the auxiliary tasks (T S ) are set to NER or SemTag or both.", "labels": [], "entities": [{"text": "MTL", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9422266483306885}, {"text": "slot filling", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7195857614278793}, {"text": "NER", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.843680739402771}]}, {"text": "We compare the two MTL approaches (see \u00a72.2) with the following baselines: \u2022 Single-Task Learning (STL).", "labels": [], "entities": [{"text": "MTL", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9559016823768616}]}, {"text": "The base model is directly trained and tested on T T , without incorporating any information from T S . The base model (see \u00a72.1) is a bi-LSTM-CRF which is the core of many models for slot filling (Goo between T T and T S . Consequently, as the training data size of T S is larger than T T , the same T T data is reused until the whole T S is used in the training.", "labels": [], "entities": [{"text": "slot filling", "start_pos": 184, "end_pos": 196, "type": "TASK", "confidence": 0.7961840033531189}]}, {"text": "We evaluate the performance by computing the F1-score on the test set using the standard CoNLL-2000 evaluation 4 .", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9991180300712585}, {"text": "CoNLL-2000 evaluation 4", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.9280190070470175}]}], "tableCaptions": [{"text": " Table 2: Statistics about the datasets, reporting the  number of sentences in train/dev/test set, and the num- ber of labels.", "labels": [], "entities": []}, {"text": " Table 3: Average F1-score and standard deviation  (numbers in subscript) of the performance on the test  sets. For the T T training split, only 10% data is used.  Bold indicates the best score for each T T . N and S in  T S denote NER and SemTag, respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9662453532218933}]}, {"text": " Table 4: Performance on slots related to person (PER),  location (LOC), and organization (ORG) concepts. We  use the best MTL from Table 3 for each T T .", "labels": [], "entities": []}]}