{"title": [{"text": "Experiments on Non-native Speech Assessment and its Consistency", "labels": [], "entities": [{"text": "Non-native Speech Assessment", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6774257123470306}]}], "abstractContent": [{"text": "In this paper, we report some preliminary experiments on automated scoring of non-native English speech and the prompt specific nature of the constructed models.", "labels": [], "entities": []}, {"text": "We use ICNALE, a publicly available corpus of non-native speech, as well as a variety of non-proprietary speech and natural language processing (NLP) tools.", "labels": [], "entities": []}, {"text": "Our results show that while the best performing model achieves an accuracy of 73% fora 4-way classification task, this performance does not transfer to a cross-prompt evaluation scenario.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.999365508556366}]}, {"text": "Our feature selection experiments show that most predictive features are related to the vocabulary aspects of speaking proficiency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Advancements in NLP and speech processing have given rise to the research and development of automated speech scoring systems in the past 10-15 years.", "labels": [], "entities": [{"text": "NLP", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9257670640945435}, {"text": "speech processing", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7798428237438202}]}, {"text": "The goal of such systems is to provide efficient and consistent evaluation in oral proficiency tests.", "labels": [], "entities": []}, {"text": "Whereas early systems scoring English proficiency predominantly made use of extracting low-level features, such as pronunciation (e.g. segmental errors, phone spectral match) and fluency (e.g. speech rate, number of pauses, lengths of silences), a sustained push to fully represent and evaluate test takers communicative competence has provided major momentum to the investigations in automated scoring for spontaneous or unconstrained speech rather than scripted or constrained speech.", "labels": [], "entities": []}, {"text": "As a result, automated scoring systems expanded their inventories to include multiple dimensions of speaking This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "labels": [], "entities": []}, {"text": "Licence details: http://creativecommons.org/licenses/by/4.0/ proficiency such as prosody, vocabulary, grammar, content, and discourse, as well as exploiting complex models to makes sense of rich data in complex tasks from large-scale assessment contexts ().", "labels": [], "entities": []}, {"text": "However, the research and development of such systems has largely centralized around a few proprietary systems (e.g.,).", "labels": [], "entities": []}, {"text": "Language assessment researchers expressed concerns about the validity of inferences made from such automated systems in high-stakes testing scenarios such as college admissions in the past.", "labels": [], "entities": []}, {"text": "In this paper, we take first steps towards addressing these issues of proprietary work and validity by: a) reporting our experiments on a freely available corpus, b) looking the transferability of our approach by performing cross-prompt evaluations, c) studying the consistency of our results and d) understanding what features perform well for prediction.", "labels": [], "entities": [{"text": "validity", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9386443495750427}]}, {"text": "Specifically, we explore the following research questions: 1.", "labels": [], "entities": []}, {"text": "RQ1: Which classifier performs the best in terms of agreement with human scorers when compared using multiple performance measures?", "labels": [], "entities": [{"text": "RQ1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7311241030693054}]}, {"text": "2. RQ2: How consistent are the machine scores rendered by the best performing model?", "labels": [], "entities": [{"text": "RQ2", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.6252094507217407}]}, {"text": "3. RQ3: What features are influential in predicting human scores?", "labels": [], "entities": []}, {"text": "While the first and third questions were also studied in the past research (with proprietary datasets and software), the second question is somewhat under explored, to our knowledge.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly surveys related work on the topic.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 describe our methods, experiments and results.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}, {"text": "SpeechRater TM , developed by Educational Testing Service (ETS) can be considered as a leading strand of research into automated scoring of non-native speech (.", "labels": [], "entities": [{"text": "Educational Testing Service (ETS)", "start_pos": 30, "end_pos": 63, "type": "DATASET", "confidence": 0.7621965408325195}]}, {"text": "Since its initial deployment in 2006, a large amount of research has been conducted into the role of various features for this task (e.g.,).", "labels": [], "entities": []}, {"text": "Other recent research,a) explored the role of prosody features in automated proficiency scoring for unconstrained speech.", "labels": [], "entities": [{"text": "automated proficiency scoring", "start_pos": 66, "end_pos": 95, "type": "TASK", "confidence": 0.5819750428199768}]}, {"text": "However, much of the previous work in this direction has been on corpora that are not freely accessible, making replications or adaptions to new corpora difficult.", "labels": [], "entities": []}, {"text": "In this paper, we follow existing approaches, but with a hitherto unexplored, publicly available corpus.", "labels": [], "entities": []}, {"text": "Since such test scores typically serve high-stake purposes, the need for ensuring the validity of machine scores arises.", "labels": [], "entities": [{"text": "validity", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9914581179618835}]}, {"text": "As reviewed by, such validity enquiry can be approached by: (1) demonstrating the correspondence between human and machine scorers; (2) understanding the construct represented within the automated processes; and (3) examining the relationship between machine scores and criterion measures.", "labels": [], "entities": []}, {"text": "In this paper, we take the first steps in this direction by addressing the first aspect.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Model Performances of All Classifiers in  Training Set", "labels": [], "entities": []}, {"text": " Table 2: Psychometric Measures for Model Per- formance", "labels": [], "entities": [{"text": "Psychometric Measures", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7471133768558502}]}]}