{"title": [{"text": "Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets with Multi-Head Self-Attention", "labels": [], "entities": [{"text": "Detecting and Extracting of Adverse Drug Reaction Mentioning Tweets", "start_pos": 0, "end_pos": 67, "type": "TASK", "confidence": 0.8945985833803812}]}], "abstractContent": [{"text": "This paper describes our system for the first and second shared tasks of the fourth Social Media Mining for Health Applications (SMM4H) workshop.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) workshop", "start_pos": 84, "end_pos": 144, "type": "TASK", "confidence": 0.8000298142433167}]}, {"text": "We enhance tweet representation with a language model and distinguish the importance of different words with Multi-Head Self-Attention.", "labels": [], "entities": [{"text": "tweet representation", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.752732902765274}]}, {"text": "In addition, transfer learning is exploited to makeup for the data shortage.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.9359976947307587}]}, {"text": "Our system achieved competitive results on both tasks with an F1-score of 0.5718 for task 1 and 0.653 (overlap) / 0.357 (strict) for task 2.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9997166991233826}]}], "introductionContent": [{"text": "Automatic adverse drug reaction (ADR) detection and extraction are of great social benefits to public health, with which pharmacovigilance (Sarker and can be performed at a broader and more automatic level.", "labels": [], "entities": [{"text": "Automatic adverse drug reaction (ADR) detection and extraction", "start_pos": 0, "end_pos": 62, "type": "TASK", "confidence": 0.7433762669563293}]}, {"text": "Recent research focus their attention on online public sources such as tweets due to their availability and authenticity ().", "labels": [], "entities": [{"text": "authenticity", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.9726501703262329}]}, {"text": "The SMM4H shared task is proposed) to enhance ADR recognition.", "labels": [], "entities": [{"text": "SMM4H shared task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.870269258817037}, {"text": "ADR recognition", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.9692404568195343}]}, {"text": "Task 1 is a binary classification task between ADR mentioned tweets and drug name only tweets, followed by task 2 to extract the particular position of ADR entities.", "labels": [], "entities": []}, {"text": "Based on the work we did last year (, we extend our previous model with hierarchical tweet representation and multi-head self-attention (HTR-MSA) to a model using both hierarchical tweet representation and attention (HTA) to jointly participate both tasks.", "labels": [], "entities": []}, {"text": "Moreover, additional features and a language model are incorporated to enhance the semantic representations.", "labels": [], "entities": []}, {"text": "In task 1, transfer learning \u2020 Equal contribution.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.9165737628936768}]}, {"text": "on a smaller dataset is exploited.", "labels": [], "entities": []}, {"text": "In task 2, we add a CRF layer for the named entity recognition task.", "labels": [], "entities": [{"text": "named entity recognition task", "start_pos": 38, "end_pos": 67, "type": "TASK", "confidence": 0.7315027862787247}]}], "datasetContent": [{"text": "In our experiments,the word embedding we use is 400 dimension and Bi-LSTM network has 2\u00d7200 units.", "labels": [], "entities": []}, {"text": "The CNN network has 400 filters with window size of 3.", "labels": [], "entities": [{"text": "CNN network", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9625612199306488}]}, {"text": "There are 16 heads in the multi-head self-attention network, and the output dimension of each head is 16.", "labels": [], "entities": []}, {"text": "Adam is selected as the optimizer.", "labels": [], "entities": []}, {"text": "Transfer learning is conducted on the CADEC medical ADR dataset () first in task 1.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9005614817142487}, {"text": "CADEC medical ADR dataset", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8688239455223083}]}, {"text": "However, we do not adopt this method in task 2 due to the relative small training dataset of this task.", "labels": [], "entities": []}, {"text": "For the word classification, we train for this task a marginal CRF with probabilities as output.", "labels": [], "entities": [{"text": "word classification", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.7826095521450043}]}, {"text": "Detailed evaluation score is illustrated in table 1, which illustrated the effectiveness of our approach.", "labels": [], "entities": []}, {"text": "In task 1, our model outperforms the average score among all participants by 0.070.", "labels": [], "entities": []}, {"text": "In task 2, the improvement on relax F1 is also significant, we improve 0.115 on relax F1 and 0.040 on strict F1.", "labels": [], "entities": [{"text": "relax", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9310410022735596}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.7201536893844604}]}, {"text": "Besides, compared to the best model we submitted for task 1 last year (, which reached a 0.522 F1 score, our method with the language model and transfer learning improves the original model by 0.050.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9775916337966919}]}], "tableCaptions": []}