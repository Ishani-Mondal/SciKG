{"title": [{"text": "PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation", "labels": [], "entities": [{"text": "MEDIQA", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.6534299850463867}, {"text": "Knowledge Distillation", "start_pos": 73, "end_pos": 95, "type": "TASK", "confidence": 0.7216624915599823}]}], "abstractContent": [{"text": "This paper describes the models designated for the MEDIQA 2019 shared tasks by the team PANLP.", "labels": [], "entities": [{"text": "MEDIQA 2019 shared tasks", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.5421596989035606}, {"text": "PANLP", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9498797059059143}]}, {"text": "We take advantages of the recent advances in pre-trained bidirectional transformer language models such as BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019b).", "labels": [], "entities": [{"text": "BERT", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9964750409126282}]}, {"text": "We find that pre-trained language models can significantly outperform traditional deep learning models.", "labels": [], "entities": []}, {"text": "Transfer learning from the NLI task to the RQE task is also experimented, which proves to be useful in improving the results of fine-tuning MT-DNN large.", "labels": [], "entities": []}, {"text": "A knowledge distillation process is implemented, to distill the knowledge contained in a set of models and transfer it into an single model, whose performance turns out to be comparable with that obtained by the ensemble of that set of models.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 2, "end_pos": 24, "type": "TASK", "confidence": 0.727254256606102}]}, {"text": "Finally, for test submissions, model ensemble and a re-ranking process are implemented to boost the performances.", "labels": [], "entities": []}, {"text": "Our models participated in all three tasks and ranked the 1st place for the RQE task, and the 2nd place for the NLI task, and also the 2nd place for the QA task.", "labels": [], "entities": [{"text": "RQE task", "start_pos": 76, "end_pos": 84, "type": "TASK", "confidence": 0.4678504019975662}, {"text": "QA task", "start_pos": 153, "end_pos": 160, "type": "TASK", "confidence": 0.7878063917160034}]}], "introductionContent": [{"text": "There are three tasks in the MEDIQA 2019 shared tasks (see Ben for details of the tasks).", "labels": [], "entities": [{"text": "MEDIQA 2019 shared tasks", "start_pos": 29, "end_pos": 53, "type": "DATASET", "confidence": 0.870045930147171}]}, {"text": "The first one, NLI, consists in identifying three inference relations between two sentences: Entailment, Neutral and Contradiction.", "labels": [], "entities": []}, {"text": "The second one, RQE, requires one to identify whether one question entails the other, where the definition of entailment is that a question A entails a question B if every answer to B is also a complete or partial answer to A.", "labels": [], "entities": [{"text": "RQE", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.5165827870368958}]}, {"text": "The third task, QA, considers not only the identification of entailment for the asked question among a set of retrieved questions, but also the ranks of retrieved answers.", "labels": [], "entities": [{"text": "QA", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.7466258406639099}]}, {"text": "In this work, we demonstrate that we can achieve significant performance gains over traditional deep learning models like ESIM, by adapting pre-trained language models into the medical domain.", "labels": [], "entities": []}, {"text": "Language model pre-training has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "Some of the most famous examples are GPT-V2 (see and BERT ( by.", "labels": [], "entities": [{"text": "GPT-V2", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9032749533653259}, {"text": "BERT", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9913526177406311}]}, {"text": "These are neural network language models trained on text data using unsupervised objectives.", "labels": [], "entities": []}, {"text": "For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9482051730155945}, {"text": "masked word prediction", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.6358233988285065}, {"text": "next sentence prediction", "start_pos": 131, "end_pos": 155, "type": "TASK", "confidence": 0.5901462535063425}]}, {"text": "To apply a pre-trained model to specific NLU tasks such as tasks for MEDIQA 2019 shared tasks, we often need to fine-tune the model with additional task-specific layers using task-specific training data.", "labels": [], "entities": [{"text": "MEDIQA 2019 shared tasks", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.7654170244932175}]}, {"text": "For example, show that BERT can be fine-tuned this way to create state-of-the-art models fora range of NLU tasks, such as question answering and natural language inference.", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9740741848945618}, {"text": "question answering", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.8884268701076508}, {"text": "natural language inference", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.6012196739514669}]}, {"text": "We also tryout a transfer learning procedure, where an intermediate model obtained on the NLI task is used to be fine-tuned on the RQE task.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9210716485977173}]}, {"text": "Although this procedure cannot consistently improve the dev set performance for all the models, it is proven to be beneficial on the test set by adding variety to the model pool.", "labels": [], "entities": []}, {"text": "To further improve the performance of single models, we implement a knowledge distillation procedure on the RQE task and the NLI task.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7033237516880035}, {"text": "RQE task", "start_pos": 108, "end_pos": 116, "type": "DATASET", "confidence": 0.7768877446651459}]}, {"text": "Knowledge distillation distills or transfers the knowledge from a (set of) large, cumbersome model(s) to a lighter, easier-to-deploy single model, without significant loss in performance (.", "labels": [], "entities": [{"text": "Knowledge distillation distills", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8057424525419871}]}, {"text": "Knowledge distillation recently has attracted a lot of attentions.", "labels": [], "entities": [{"text": "Knowledge distillation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7156205624341965}]}, {"text": "We believe it is interesting and of great importance to explore this method on the applications of the medical domain.", "labels": [], "entities": []}, {"text": "For test submissions, model ensemble is used to obtain more stable and unbiased predictions.", "labels": [], "entities": []}, {"text": "We only adopt a simple ensemble model, that is, averaging the class probabilities of different models.", "labels": [], "entities": []}, {"text": "After obtaining test predictions, for the NLI and RQE task, simple re-ranking operations among pairs with the same premise are used to boost the performance metrics.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 , we demonstrate our experiments on the three tasks.", "labels": [], "entities": []}, {"text": "In Section 3, transfer learning from NLI to RQE is presented.", "labels": [], "entities": [{"text": "RQE", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.7338147163391113}]}, {"text": "Section 4 elaborates on the knowledge distillation and the corresponding experimental results.", "labels": [], "entities": [{"text": "knowledge distillation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7784208059310913}]}, {"text": "Section 5 and Section 6 present the model ensemble technique and the reranking strategies.", "labels": [], "entities": []}, {"text": "Section 7 explains our submission records in detail.", "labels": [], "entities": []}, {"text": "Section 8 concludes and discusses future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: performances of different models on the valid set of the RQE task.", "labels": [], "entities": [{"text": "RQE task", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.7095232307910919}]}, {"text": " Table 2: performances of the MT-DNN base model  with linear projection, when different number of lay- ers are freezed during fine-tuning on the RQE dataset", "labels": [], "entities": [{"text": "RQE dataset", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.9182451367378235}]}, {"text": " Table 3: performances of different models on the valid set of the NLI task.", "labels": [], "entities": []}, {"text": " Table 3. It turns out, the  BERT-based model significantly outperforms the  traditional models. MT-DNN models still perform  quite well, but the Sci-BERT with linear projec- tion achieves the highest accuracy on the dev set.  The Bio-BERT model still cannot achieve satisfy- ing results. We find that models behave quite dif- ferently on NLI compared with the RQE datasets.  First, on the NLI dataset, BERT large and the MT- DNN large, which is derived from BERT large,  perform better than their base counterparts, BERT", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.984289824962616}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.9986507296562195}, {"text": "RQE datasets", "start_pos": 361, "end_pos": 373, "type": "DATASET", "confidence": 0.9469030201435089}, {"text": "NLI dataset", "start_pos": 390, "end_pos": 401, "type": "DATASET", "confidence": 0.8982930481433868}, {"text": "BERT", "start_pos": 403, "end_pos": 407, "type": "METRIC", "confidence": 0.9953091740608215}, {"text": "BERT", "start_pos": 517, "end_pos": 521, "type": "METRIC", "confidence": 0.9679227471351624}]}, {"text": " Table 4: performances of different models on the valid set of the QA task. Here accuracy is calculated on the whole  dev set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9995015859603882}]}, {"text": " Table 7: The submission results on the RQE task. Multiplication symbol \"*\" here means multiple runs or epochs  of the same model (with different random seed). \"TL\" means the model go through transfer learning on the NLI  task. \"KD\" means the model is obtained via knowledge distillation. Without declaration, all the models here are  trained on the train and dev set.", "labels": [], "entities": []}, {"text": " Table 9: The submission results on the QA task.", "labels": [], "entities": [{"text": "QA task", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.5940156877040863}]}]}