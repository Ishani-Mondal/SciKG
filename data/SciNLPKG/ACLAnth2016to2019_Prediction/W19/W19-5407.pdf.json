{"title": [{"text": "QE BERT: Bilingual BERT using Multi-task Learning for Neural Quality Estimation", "labels": [], "entities": [{"text": "BERT", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.6638151407241821}, {"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.826747715473175}, {"text": "Neural Quality Estimation", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.6605211496353149}]}], "abstractContent": [{"text": "For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks.", "labels": [], "entities": [{"text": "translation quality estimation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.9068426291147867}, {"text": "BERT", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9813396334648132}]}, {"text": "Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level sub-tasks (i.e., source word, target word, and target gap).", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9967572093009949}, {"text": "translation quality estimation", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.9102729956309}]}, {"text": "Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.", "labels": [], "entities": [{"text": "Quality Estimation", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6246331483125687}, {"text": "WMT19", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.7674915194511414}]}], "introductionContent": [{"text": "Translation quality estimation (QE) has become an important research topic in the field of machine translation (MT), which is used to estimate quality scores and categories fora machine-translated sentence without reference translations at various levels (.", "labels": [], "entities": [{"text": "Translation quality estimation (QE)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8357801139354706}, {"text": "machine translation (MT)", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.8585202097892761}]}, {"text": "Recent Predictor-Estimator architecture-based approaches have significantly improved QE performance.", "labels": [], "entities": [{"text": "QE", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.8467934131622314}]}, {"text": "The Predictor-Estimator () is based on a modified neural encoder architecture that consists of two subsequent neural models: 1) a word prediction model, which predicts each target word given the source sentence and the left and right context of the target word, and 2) a quality estimation model, which estimates sentence-level scores and word-level labels from features produced by the predictor.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.68573959171772}]}, {"text": "The word prediction model is trained from additional large-scale parallel data and the quality estimation model is trained from small-scale QE data.", "labels": [], "entities": [{"text": "word prediction", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.8277959525585175}]}, {"text": "Recently, BERT) has led to impressive improvements on various natural language processing tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9919501543045044}]}, {"text": "BERT is a bidirectionally trained language model from large-scale \"monolingual\" data to learn the \"monolingual\" context of a word based on all of its surroundings (left and right of the word).", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.968818187713623}]}, {"text": "Both BERT that is based on the Transformer architecture ( and the word prediction model in the Predictor-Estimator that is based on the attention-based recurrent neural network (RNN) encoder-decoder architecture) have some common ground utilizing generative pretraining of sentence encoder.", "labels": [], "entities": [{"text": "BERT", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.986750066280365}, {"text": "word prediction", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7410736680030823}]}, {"text": "In this paper, we propose a \"bilingual\" BERT using multi-task learning for translation quality estimation (called the QE BERT).", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.8649696707725525}, {"text": "translation quality estimation", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.8746604720751444}, {"text": "BERT", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.5157003998756409}]}, {"text": "We describe how we have applied BERT to the QE task to make much improvements.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9969406127929688}, {"text": "QE task", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.6911398768424988}]}, {"text": "In addition, for recent QE task, which consists of one sentence-level subtask to predict HTER scores and three word-level subtasks to detect errors for each source word, target (mt) word, and target (mt) gap, we also have applied multi-task learning) to enhance the training data from other QE subtasks . The results of experiments conducted on the WMT19 QE datasets show that our proposed QE BERT using multi-task learning provides significant improvements over the baseline system.", "labels": [], "entities": [{"text": "WMT19 QE datasets", "start_pos": 349, "end_pos": 366, "type": "DATASET", "confidence": 0.9663386146227518}, {"text": "BERT", "start_pos": 393, "end_pos": 397, "type": "METRIC", "confidence": 0.9260004758834839}]}], "datasetContent": [{"text": "The proposed learning methods were evaluated on the WMT19 QE Shared Task 5 of word-level and sentence-level English-Russian and EnglishGerman.", "labels": [], "entities": [{"text": "WMT19 QE Shared Task 5", "start_pos": 52, "end_pos": 74, "type": "DATASET", "confidence": 0.8321092605590821}]}, {"text": "We used parallel data provided for the WMT19 news machine translation task 6 to pre-train QE BERT.", "labels": [], "entities": [{"text": "WMT19 news machine translation task", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.8489319920539856}, {"text": "QE", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.5725778341293335}, {"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.7034470438957214}]}, {"text": "The English-Russian parallel data set consisted of the ParaCrawl corpus, Common Crawl corpus, News Commentary corpus, and Yandex Corpus.", "labels": [], "entities": [{"text": "English-Russian parallel data set", "start_pos": 4, "end_pos": 37, "type": "DATASET", "confidence": 0.7318810075521469}, {"text": "ParaCrawl corpus", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.904592901468277}, {"text": "Common Crawl corpus", "start_pos": 73, "end_pos": 92, "type": "DATASET", "confidence": 0.8762473464012146}, {"text": "News Commentary corpus", "start_pos": 94, "end_pos": 116, "type": "DATASET", "confidence": 0.8202986121177673}, {"text": "Yandex Corpus", "start_pos": 122, "end_pos": 135, "type": "DATASET", "confidence": 0.9426232278347015}]}, {"text": "The English-German parallel data set consisted of the Europarl corpus, ParaCrawl corpus, Common Crawl corpus, News Commentary corpus, and Document-split Rapid corpus.", "labels": [], "entities": [{"text": "English-German parallel data set", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.7250529453158379}, {"text": "Europarl corpus", "start_pos": 54, "end_pos": 69, "type": "DATASET", "confidence": 0.98679319024086}, {"text": "ParaCrawl corpus", "start_pos": 71, "end_pos": 87, "type": "DATASET", "confidence": 0.8166753053665161}, {"text": "Common Crawl corpus", "start_pos": 89, "end_pos": 108, "type": "DATASET", "confidence": 0.779634972413381}, {"text": "News Commentary corpus", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.7740946213404337}]}, {"text": "In pre-training, we used the default hyperparameter setting of the released multilingual model.", "labels": [], "entities": []}, {"text": "In fine-turing, a sequence length of 512 was used to cover the length of QE data.", "labels": [], "entities": [{"text": "QE data", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.782543957233429}]}, {"text": "To make ensembles, we combined five instances having different hyperparameter weight for 'BAD' label (i.e., 1:10, 1:15, 1:20, 1:25, and 1:30).", "labels": [], "entities": [{"text": "BAD", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.8797711133956909}]}, {"text": "For word-level ensemble results, we voted the predicted labels from each instance.", "labels": [], "entities": []}, {"text": "For sentencelevel ensemble results, we averaged the predicted HTER scores from each instance.", "labels": [], "entities": [{"text": "HTER", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.720690131187439}]}, {"text": "show the experimental results obtained from the QE BERT using the different learning methods for the WMT19 word-level and sentence-level QE tasks.", "labels": [], "entities": [{"text": "QE", "start_pos": 48, "end_pos": 50, "type": "DATASET", "confidence": 0.4813336730003357}, {"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9464064836502075}, {"text": "WMT19", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9264671802520752}]}, {"text": "For both language pairs, using multi-task learning consistently improves the scores.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of the QE BERT model on the development set of the WMT19 word-level QE task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9534416794776917}, {"text": "WMT19 word-level QE task", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6629779785871506}]}, {"text": " Table 2: Results of the QE BERT model on the development set of the WMT19 sentence-level QE task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9538040161132812}, {"text": "WMT19 sentence-level QE task", "start_pos": 69, "end_pos": 97, "type": "DATASET", "confidence": 0.7065136134624481}]}, {"text": " Table 3: Results of the QE BERT model on the test set of the WMT19 word-level QE task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.942477822303772}, {"text": "WMT19 word-level QE task", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.6724272668361664}]}, {"text": " Table 4: Results of the QE BERT model on the test set of the WMT19 sentence-level QE task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9432962536811829}, {"text": "WMT19 sentence-level QE task", "start_pos": 62, "end_pos": 90, "type": "DATASET", "confidence": 0.7186760008335114}]}]}