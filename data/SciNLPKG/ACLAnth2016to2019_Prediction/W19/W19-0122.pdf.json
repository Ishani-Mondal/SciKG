{"title": [{"text": "Modeling the Acquisition of Words with Multiple Meanings", "labels": [], "entities": [{"text": "Modeling the Acquisition of Words with Multiple Meanings", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.8297572657465935}]}], "abstractContent": [{"text": "Learning vocabulary is essential to successful communication.", "labels": [], "entities": []}, {"text": "Complicating this task is the underappreciated fact that most common words are associated with multiple senses (are polysemous) (e.g., baseball cap vs. cap of a bottle), while other words are homonymous, evoking meanings that are unrelated to one another (e.g., baseball bat vs. flying bat).", "labels": [], "entities": []}, {"text": "Models of human word learning have thus far failed to represent this level of naturalistic complexity.", "labels": [], "entities": [{"text": "word learning", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7084679156541824}]}, {"text": "We extend a feature-based computational model to allow for multiple meanings, while capturing the gradient distinction between pol-ysemy and homonymy by using structured sets of features.", "labels": [], "entities": []}, {"text": "Results confirm that the present model correlates better with human data on novel word learning tasks than the existing feature-based model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Children acquire language at a remarkable rate despite many layers of complexity in their learning environment.", "labels": [], "entities": []}, {"text": "Previous computational models of human vocabulary learning have been primarily aimed at the mapping problem or the problem of \"referential indeterminacy\", namely, determining which word maps onto which object within a noisy context).", "labels": [], "entities": [{"text": "human vocabulary learning", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6521053612232208}]}, {"text": "These models explicitly make the simplifying but counter-factual assumption that each word can map to only one meaning in order to address how it is that learners determine which meaning a word refers to from among multiple potential referents in a scene.", "labels": [], "entities": []}, {"text": "The models further assume that each possible meaning competes with every other possible meaning.", "labels": [], "entities": []}, {"text": "For example, in a scene depicting \"A cat drinking milk\", the meaning of the word cat competes with the meaning of milk, bowl and every other potential meaning evoked in the scene.", "labels": [], "entities": []}, {"text": "This perspective emphasizes the richness of visual scenes, but it overlooks the complexity associated with word meanings which very commonly refer to multiple distinct senses or meanings.", "labels": [], "entities": []}, {"text": "For example, a bowl can refer to a \"dish used for feeding\" in the cat scene, but to a \"toilet bowl\" within a different context.", "labels": [], "entities": []}, {"text": "That is, the meaning of a word cannot be a winner-takesall affair in which meanings compete with one another across contexts, because people learn to assign multiple meanings to many words in their vocabularies.", "labels": [], "entities": []}, {"text": "Multiple meanings of one word can typically not be subsumed under a general definition or rule.", "labels": [], "entities": []}, {"text": "This is clearly true in the case of homonyms, which have multiple, unrelated meanings (e.g., baseball bat vs. flying bat).", "labels": [], "entities": []}, {"text": "It is also true of many polysemes, which evoke conventional senses that are related to one another yet distinct.", "labels": [], "entities": []}, {"text": "Natural language polysemy often involves extensions along multiple dimensions that are not completely predictable on the basis of a general definition or rule.", "labels": [], "entities": []}, {"text": "For example, while baseball caps and bottle caps both cover something tightly, English speakers must learn that corks and lids, which also cover things tightly, are not called caps, while mushroom caps are, even though the latter do not cover anything tightly (for discussion of rule-based polysemy see e.g.,).", "labels": [], "entities": []}, {"text": "Notably, polysemes are much more frequent than homonyms, insofar as 40% of frequent English words are polysemous, while closer to 4% of words are homonyms.", "labels": [], "entities": []}, {"text": "Even though homonyms are relatively rare, children as young as 3 years old have been found to know a number of them.", "labels": [], "entities": []}, {"text": "At least for these words, preschoolers have managed to overcome their reluctance to as-sign a second meaning to a familiar word).", "labels": [], "entities": []}, {"text": "We also know that children readily generalize the meaning of a word to include new referents that share a single dimension, such as shape () or function, and has found that 4-5 year-old children can be taught that a word extends to other referents that share the same material (.", "labels": [], "entities": []}, {"text": "While previous psycholinguistic work has primarily focused on learning words with a single meaning or words that can be generalized along a single dimension (rule-based polysemy), a recent study that we simulate below has investigated words with multiple distinct, conventional meanings (non-rule-based).", "labels": [], "entities": []}, {"text": "This work has demonstrated that it is easier to learn conventional polysemy when compared with homonymy, even when the polysemy follows complex, multidimensional extension patterns as in natural language.", "labels": [], "entities": []}, {"text": "We propose a computational model that allows words to be assigned multiple meanings that cannot be generated by a one-dimensional rule, but must instead be learned through exposure.", "labels": [], "entities": []}, {"text": "We use the results from the behavioral experiment in order to inform and test the proposed model.", "labels": [], "entities": []}, {"text": "As reported, the model not only captures the finding that people find it easier to learn polysemous words than ambiguous words, but it also closely approximates human errors.", "labels": [], "entities": []}, {"text": "This represents a first step toward addressing the complexity involved in learning more than a single meaning of a given word.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental work explicitly compared the distinction between homonymy and conventional polysemy.", "labels": [], "entities": []}, {"text": "In particular, participants learned 4 novel words, and each novel word was associated with 3 clearly distinct novel objects.", "labels": [], "entities": []}, {"text": "Randomly interspersed among the 12 labeled objects were 20 Figure 2: An example of the stimuli presented to participants in the label ID task.", "labels": [], "entities": [{"text": "label ID task", "start_pos": 128, "end_pos": 141, "type": "TASK", "confidence": 0.7853286465009054}]}, {"text": "The target object was presented along with 3 distractors, which were targets for other novel words.", "labels": [], "entities": []}, {"text": "unlabeled filler (non-target) objects accompanied by tones.", "labels": [], "entities": []}, {"text": "Novel objects were used to avoid interference from familiar words.", "labels": [], "entities": []}, {"text": "Half of the participants were randomly assigned to a Polysemy condition in which the objects were related to one another with the 3 objects sharing distinct features with one another.", "labels": [], "entities": []}, {"text": "The other half was assigned to a Homonymy condition, in which the 3 objects assigned to each word did now share any distinguishing features that could distinguish them from the filler objects in terms of a stronger featurerelation.", "labels": [], "entities": []}, {"text": "(See for an example).", "labels": [], "entities": []}, {"text": "The \"polysemous\" meanings of words were confirmed to be more similar than the \"homonymous\" meanings, as intended, using a separate norming study with anew group of participants.", "labels": [], "entities": []}, {"text": "After brief exposure, participants completed the following two tasks designed to determine whether polysemous words were easier to learn than homonymous words.", "labels": [], "entities": []}, {"text": "1. Label ID task -Participants were asked to select, from 4 available options, the one object that corresponded to a given label.", "labels": [], "entities": [{"text": "Label ID task", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.7908445994059244}]}, {"text": "The 3 foil objects had been labeled by each of the 3 other labels (see.", "labels": [], "entities": []}, {"text": "Results showed significantly higher accuracy for the polysemy condition over the homonymy condition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995782971382141}]}, {"text": "2. Sense Selection task -Participants were presented with the label of one of the 4 words, and shown 8 objects (see.", "labels": [], "entities": [{"text": "Sense Selection task", "start_pos": 3, "end_pos": 23, "type": "TASK", "confidence": 0.7361509799957275}]}, {"text": "Three of the objects corresponded to the 3 senses of the word and the 5 additional objects were fillers that had been witnessed during exposure.", "labels": [], "entities": []}, {"text": "Accuracy was lower on this task, showing only slight polysemy advantage due to task difficulty.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9924237728118896}]}, {"text": "An example of the stimuli presented to participants in the Sense Selection task.", "labels": [], "entities": [{"text": "Sense Selection task", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.8299045364061991}]}, {"text": "All 3 target objects associated with one of the novel words were presented along with 5 filler objects, which had also been witnessed during the exposure but had not been labeled.", "labels": [], "entities": []}, {"text": "The Label ID task allows a comparison of the two conditions, polysemy and homonymy, but participants may have used the memory of one or two words to perform well by a process of elimination by recognizing an object as related to a different label.", "labels": [], "entities": [{"text": "Label ID task", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8183896938959757}]}, {"text": "The Sense Selection task allows fora more thorough error analysis; importantly, the particular objects selected by humans was made available to the computational analysis.", "labels": [], "entities": [{"text": "Sense Selection task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7994426488876343}]}, {"text": "We perform an error analysis on the selection rate of each filler object.", "labels": [], "entities": []}, {"text": "The results of this task provide a crucial test of the bag-of-features meanings learned by the NFS12 model.", "labels": [], "entities": []}, {"text": "We trained the model on input that reflected the exposure in the novel word study.", "labels": [], "entities": []}, {"text": "In particular, two annotators hand-coded each object with 4 to 5 features to compose a joint a list of 40 features that jointly described all 12 labeled and 20 unlabeled (filler) objects.", "labels": [], "entities": []}, {"text": "The features included properties related to shape, size, color, texture, material, symmetry, etc.", "labels": [], "entities": []}, {"text": "We trained the NSF12 and SMF models independently: the features were used as a bag-of-features for the NFS12 model, and as structured sets of features for SMF.", "labels": [], "entities": [{"text": "NSF12", "start_pos": 15, "end_pos": 20, "type": "DATASET", "confidence": 0.940072774887085}, {"text": "SMF", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6420199275016785}, {"text": "NFS12", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.9535878896713257}, {"text": "SMF", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9616993069648743}]}, {"text": "Recall that although each input item consisted of a single word associated with observable features, the models differ in the way they learn.", "labels": [], "entities": []}, {"text": "NFS12 learns the association of a word with each feature, while SMF learns the association of a word to a subset of features.", "labels": [], "entities": [{"text": "NFS12", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8801385760307312}]}, {"text": "At the end of training, we tested the models by simulating each of the two tasks described above.", "labels": [], "entities": []}, {"text": "We first estimate the association of each word to each of the items, using the cosine distance between the learned associations and the feature rep-: Pearson correlation between results from participants on the task with NSF12 and proposed SMF models.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 150, "end_pos": 169, "type": "METRIC", "confidence": 0.9082949757575989}, {"text": "NSF12", "start_pos": 221, "end_pos": 226, "type": "DATASET", "confidence": 0.912599503993988}, {"text": "SMF", "start_pos": 240, "end_pos": 243, "type": "TASK", "confidence": 0.9541884064674377}]}, {"text": "For the NFS12 model, we calculated the cosine similarity between all the associated features.", "labels": [], "entities": [{"text": "NFS12", "start_pos": 8, "end_pos": 13, "type": "DATASET", "confidence": 0.8838805556297302}, {"text": "cosine similarity", "start_pos": 39, "end_pos": 56, "type": "METRIC", "confidence": 0.7700129151344299}]}, {"text": "For the SMF model, we calculated the maximum cosine similarity score overall the sets of features associated with the word and the feature representation of the item (i.e., we considered the sense of the word most similar to the object in question).", "labels": [], "entities": [{"text": "SMF", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9838756918907166}, {"text": "cosine similarity score", "start_pos": 45, "end_pos": 68, "type": "METRIC", "confidence": 0.8152747948964437}]}, {"text": "The likelihood of choosing an object as a target is measured by the proportional similarity of each object compared with the other objects presented in the task.", "labels": [], "entities": []}, {"text": "For each stimuli set of 4 items used in the Label ID task (see) and 8 items used for the Sense Selection task (see) , we calculate where, w is the word presented as visual stimuli attest.", "labels": [], "entities": [{"text": "Label ID task", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.7961708505948385}, {"text": "Sense Selection task", "start_pos": 89, "end_pos": 109, "type": "TASK", "confidence": 0.7758124470710754}]}, {"text": "o ranges overall the objects presented attest (4 or 8 items), and O is the full set of objects for this test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson correlation between results from  participants on the task with NSF12 and proposed  SMF models.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9093044996261597}, {"text": "NSF12", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.8904958367347717}, {"text": "SMF", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9731431603431702}]}, {"text": " Table 2: Pearson correlation between errors pro- duced by human participants with NSF12 and  SMF models on the Sense Identification task.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9425796568393707}, {"text": "Sense Identification task", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.8256703416506449}]}]}