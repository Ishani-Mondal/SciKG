{"title": [{"text": "Improving Long Distance Slot Carryover in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Improving Long Distance Slot", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8722424060106277}, {"text": "Carryover", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.4898279309272766}]}], "abstractContent": [{"text": "Tracking the state of the conversation is a central component in task-oriented spoken dialogue systems.", "labels": [], "entities": [{"text": "Tracking the state of the conversation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8663804332415262}]}, {"text": "One such approach for tracking the dialogue state is slot carryover, where a model makes a binary decision if a slot from the context is relevant to the current turn.", "labels": [], "entities": []}, {"text": "Previous work on the slot carryover task used models that made independent decisions for each slot.", "labels": [], "entities": []}, {"text": "A close analysis of the results show that this approach results in poor performance over longer context dialogues.", "labels": [], "entities": []}, {"text": "In this paper, we propose to jointly model the slots.", "labels": [], "entities": []}, {"text": "We propose two neural network architectures, one based on pointer networks that incorporate slot ordering information, and the other based on transformer networks that uses self attention mechanism to model the slot interdependen-cies.", "labels": [], "entities": []}, {"text": "Our experiments on an internal dialogue benchmark dataset and on the public DSTC2 dataset demonstrate that our proposed models are able to resolve longer distance slot references and are able to achieve competitive performance .", "labels": [], "entities": [{"text": "DSTC2 dataset", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9767726957798004}]}], "introductionContent": [{"text": "In task-oriented spoken dialogue systems, the user and the system are engaged in interactions that can span multiple turns.", "labels": [], "entities": []}, {"text": "A key challenge here is that the user can reference entities introduced in previous dialogue turns.", "labels": [], "entities": []}, {"text": "For example, if a user request for what's the weather in arlington is followed by how about tomorrow, the dialogue system has to keep track of the entity arlington being referenced.", "labels": [], "entities": []}, {"text": "In slot-based spoken dialogue systems, tracking the entities in context can be cast as slot carryover task -only the relevant slots from the dialogue context are carried over to the current turn.", "labels": [], "entities": []}, {"text": "Recent work by describes a scalable multi-domain neural network architecture to address the task in a diverse schema setting.", "labels": [], "entities": []}, {"text": "However, this approach treats every slot as indepen-, and should be carried over together due to their interdependencies (2) PLACE slot is often seen to occur along with TOWN. dent.", "labels": [], "entities": [{"text": "PLACE slot", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9381000697612762}, {"text": "TOWN. dent", "start_pos": 170, "end_pos": 180, "type": "DATASET", "confidence": 0.9098578890164694}]}, {"text": "Consequently, as shown in our experiments, this results in lower performance when the contextual slot being referenced is associated with dialogue turns that are further away from the current turn.", "labels": [], "entities": []}, {"text": "We posit that modeling slots jointly is essential for improving the accuracy overlong distances, particularly when slots are correlated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9989237189292908}]}, {"text": "We motivate this with an example conversation in.", "labels": [], "entities": []}, {"text": "In this example, the slots WEATH-ERCITY/WEATHERSTATE, need to be carried over together from dialogue history as they are correlated.", "labels": [], "entities": []}, {"text": "However, the model in has no information about this slot interdependence and may choose to carryover only one of the slots.", "labels": [], "entities": []}, {"text": "In this work, we alleviate this issue by propos-ing two novel neural network architectures -one based on pointer networks ( and another based on self-attention with transformers ( -that can learn to jointly predict jointly whether a subset of related slots should be carried over from dialogue history.", "labels": [], "entities": []}, {"text": "To validate our approach, we conduct thorough evaluations on both the publicly available DSTC2 task (), as well as our internal dialogue dataset collected from a commercial digital assistant.", "labels": [], "entities": []}, {"text": "In Section 4.3, we show that our proposed approach improve slot carryover accuracy over the baseline systems over longer dialogue contexts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9550668597221375}]}, {"text": "A detailed error analysis reveals that our proposed models are more likely to utilize \"anchor\" slots -slots tagged in the current utterance -to carryover long-distance slots from context.", "labels": [], "entities": []}, {"text": "To summarize we make the following contributions in this work: 1.", "labels": [], "entities": []}, {"text": "We improve upon the slot carryover model architecture in by introducing approaches for modeling slot interdependencies.", "labels": [], "entities": []}, {"text": "We propose two neural network models based on pointer networks and transformer networks that can make joint predictions over slots.", "labels": [], "entities": []}, {"text": "2. We provide a detailed analysis of the proposed models both on an internal benchmark and public dataset.", "labels": [], "entities": []}, {"text": "We show that contextual encoding of slots and modeling slot interdependencies is essential for improving performance of slot carryover over longer dialogue contexts.", "labels": [], "entities": []}, {"text": "Transformer architectures with self attention provide the best performance overall.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all the models, we initialize the word embeddings using fastText embeddings ().", "labels": [], "entities": []}, {"text": "The models are trained using mini-batch SGD with Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 to minimize the negative log-likelihood loss.", "labels": [], "entities": []}, {"text": "We set the dropout rate of 0.3 for our models during training.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 11, "end_pos": 23, "type": "METRIC", "confidence": 0.951891154050827}]}, {"text": "In our experiments, we use 300 dimensions for the LSTM hidden states in the pointer network encoder and decoder.", "labels": [], "entities": []}, {"text": "Our transformer decoder has 1 layer, Z = 80 heads, d k = d v = 64 for the projection size of keys and values in the attention heads.", "labels": [], "entities": []}, {"text": "We do not use positional encoding for the transformer decoder.", "labels": [], "entities": []}, {"text": "All pointer network model setups are trained for 40 epochs, our transformer models are trained for 200 epochs.", "labels": [], "entities": []}, {"text": "For evaluation on the test set, we pick the best model based on performance on dev set.", "labels": [], "entities": []}, {"text": "We use standard definitions of precision, recall, and F1 by comparing the reference slots with the model hypothesis slots.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9995586276054382}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.999222993850708}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9997397065162659}]}], "tableCaptions": [{"text": " Table 1: Internal Dataset breakdown showing the  number of carryover candidate slots at different dis- tances. 'Total' shows the total number of candidate  slots and 'Positive' shows the number of candidate slots  that are relevant for the current turn.", "labels": [], "entities": [{"text": "Internal Dataset", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.6182187795639038}]}, {"text": " Table 2: DSTC2 Dataset breakdown showing the num- ber of carryover candidate slots at different distances.  'Total' shows the total number of candidate slots and  'Positive' shows the number of candidate slots that rep- resent the user goal at the current turn.", "labels": [], "entities": [{"text": "DSTC2 Dataset", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9029074013233185}]}, {"text": " Table 3: Carryover performance (F1) of different models for slots at different distances on Internal dataset. The  rightmost column contains the aggregate scores for all slots with distance greater than or equal to 1.", "labels": [], "entities": [{"text": "Carryover performance (F1)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9012664794921875}, {"text": "Internal dataset", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.8853766620159149}]}, {"text": " Table 4: Carryover performance (F1) of different models for slots at different distances on DSTC2 dataset.", "labels": [], "entities": [{"text": "Carryover performance (F1)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.899371063709259}, {"text": "DSTC2 dataset", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.991820901632309}]}]}