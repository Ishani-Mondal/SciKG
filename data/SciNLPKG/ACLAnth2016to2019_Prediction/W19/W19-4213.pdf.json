{"title": [{"text": "CUNI-Malta system at SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context: Operation-based word formation", "labels": [], "entities": [{"text": "CUNI-Malta system at SIGMORPHON 2019", "start_pos": 0, "end_pos": 36, "type": "DATASET", "confidence": 0.8429221034049987}, {"text": "Morphological Analysis", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.7699151635169983}, {"text": "Operation-based word formation", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.6149553855260214}]}], "abstractContent": [{"text": "This paper presents the submission by the Charles University-University of Malta team to the SIGMORPHON 2019 Shared Task on Morphological Analysis and Lemmatization in context.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 Shared Task", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.6476055830717087}, {"text": "Morphological Analysis and Lemmatization", "start_pos": 124, "end_pos": 164, "type": "TASK", "confidence": 0.7687336951494217}]}, {"text": "We present a lemmatization model based on previous work on neural transducers (Makarov and Clematide, 2018b; Aharoni and Goldberg, 2016).", "labels": [], "entities": []}, {"text": "The key difference is that our model transforms the whole word form in every step, instead of consuming it character by character.", "labels": [], "entities": []}, {"text": "We propose a merging strategy inspired by Byte-Pair-Encoding that reduces the space of valid operations by merging frequent adjacent operations.", "labels": [], "entities": []}, {"text": "The resulting operations not only encode the actions to be performed but the relative position in the word token and how characters need to be transformed.", "labels": [], "entities": []}, {"text": "Our morphological tag-ger is a vanilla biLSTM tagger that operates over operation representations, encoding operations and words in a hierarchical manner.", "labels": [], "entities": []}, {"text": "Even though relative performance according to metrics is below the baseline, experiments show that our models capture important associations between interpretable operation labels and fine-grained morpho-syntax labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tasks related to morphological analysis have been traditionally formulated as string transduction problems tackled by weighted finite state transducers).", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.869690865278244}]}, {"text": "More recently, however, the problem has been tackled with neural architectures featuring sequence-tosequence architectures ( and neural transducers.", "labels": [], "entities": []}, {"text": "In this paper we describe our submission for the SIGMORPHON 2019 Shared Task related to morphological analysis and lemmatization in context (.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 Shared Task", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6125626266002655}, {"text": "morphological analysis", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.9056550562381744}]}, {"text": "We focus on an operation-based word formation process using a neural transducer which consumes more than one character at a time.", "labels": [], "entities": [{"text": "word formation process", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7629659573237101}]}, {"text": "Our main motivation for this approach stems from neural transducers that normally consume one character at a time using context-enriched representation of characters.", "labels": [], "entities": []}, {"text": "In language modelling, character-based RNNs have a difficulty capturing long dependencies between characters, especially dependencies in words which are separated by several tokens.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.6954651474952698}]}, {"text": "This can be a crucial piece of information for morphological analysis in context.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8851720094680786}]}, {"text": "This type of approach has already been extend effectively to Neural Machine Translation by, who employ simple character n-gram models and a segmentation based on the byte pair encoding (BPE) compression algorithm.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.8483973542849222}]}], "datasetContent": [{"text": "We follow a two step approach to morphological analysis by first obtaining the action sequence using the lemmatizer model, and then obtaining the feature label sequence over these action representations.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8554269373416901}]}, {"text": "All models were implemented and trained using PyTorch 1.0.0.", "labels": [], "entities": [{"text": "PyTorch 1.0.0", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.8436615765094757}]}], "tableCaptions": [{"text": " Table 3: Hyper-parameters of all models proposed.  Lem = Lemmatizer; Anlz = Analyzer", "labels": [], "entities": [{"text": "Hyper-parameters", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9361445903778076}, {"text": "Anlz", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9827980399131775}]}, {"text": " Table 4: Results on Task2 for the best and worst 5 treebanks. Scores over the development set are presented as mean  (std) values over 10 runs. Scores over test set are taken from the official results. LAcc = lemmatization accuracy;  Lev-Dist = Levenshtein distance of lemmas; MAcc = accuracy of morphosyntactic descriptions (features); M-F1  = F 1 score of morphosyntactic descriptions.", "labels": [], "entities": [{"text": "LAcc", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9622644782066345}, {"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.7936214208602905}, {"text": "MAcc", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.916462242603302}, {"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9793409705162048}, {"text": "F 1 score", "start_pos": 346, "end_pos": 355, "type": "METRIC", "confidence": 0.9785416920979818}]}]}