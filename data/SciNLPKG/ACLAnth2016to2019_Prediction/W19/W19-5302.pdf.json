{"title": [{"text": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges", "labels": [], "entities": [{"text": "WMT19 Metrics Shared Task", "start_pos": 15, "end_pos": 40, "type": "TASK", "confidence": 0.5365137457847595}, {"text": "MT Systems Pose Big Challenges", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.8375711798667907}]}], "abstractContent": [{"text": "This paper presents the results of the WMT19 Metrics Shared Task.", "labels": [], "entities": [{"text": "WMT19 Metrics Shared Task", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6499949544668198}]}, {"text": "Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics.", "labels": [], "entities": [{"text": "WMT19 News Translation Task", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.7378077954053879}]}, {"text": "13 research groups submitted 24 metrics, 10 of which are reference-less \"metrics\" and constitute submissions to the joint task with WMT19 Quality Estimation Task, \"QE as a Met-ric\".", "labels": [], "entities": [{"text": "WMT19 Quality Estimation Task", "start_pos": 132, "end_pos": 161, "type": "DATASET", "confidence": 0.7710030972957611}]}, {"text": "In addition, we computed 11 baseline metrics, with 8 commonly applied base-lines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reim-plementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9968987703323364}, {"text": "WER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.829750120639801}, {"text": "PER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.7015138268470764}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.6845051646232605}]}, {"text": "Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality.", "labels": [], "entities": [{"text": "WMT19 official manual ranking", "start_pos": 88, "end_pos": 117, "type": "DATASET", "confidence": 0.8990586549043655}]}, {"text": "This year, we use direct assessment (DA) as our only form of manual evaluation.", "labels": [], "entities": [{"text": "direct assessment (DA)", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.6009126901626587}]}], "introductionContent": [{"text": "To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8559506416320801}]}, {"text": "Manual/human evaluation can be costly and time consuming, and so an automatic evaluation metric, given that it sufficiently correlates with manual evaluation, can be useful in developmental cycles.", "labels": [], "entities": []}, {"text": "In studies involving hyperparameter tuning or architecture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7910396158695221}, {"text": "architecture search", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8018479943275452}]}, {"text": "As objective, reproducible quantities, metrics can also facilitate cross-paper comparisons.", "labels": [], "entities": []}, {"text": "The WMT Metrics Shared Task 1 annually serves as avenue to validate the use of existing metrics (including baselines such as BLEU), and to develop new ones; see through.", "labels": [], "entities": [{"text": "WMT Metrics Shared Task 1", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.6088796854019165}, {"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9959478974342346}]}, {"text": "In the setup of our Metrics Shared Task, an automatic metric compares an MT system's output translations with manual reference translations to produce: either (a) system-level score, i.e. a single overall score for the given MT system, or (b) segment-level scores for each of the output translations, or both.", "labels": [], "entities": [{"text": "MT system's output translations", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.8174452185630798}]}, {"text": "This year we teamed up with the organizers of the QE Task and hosted \"QE as a Metric\" as a joint task.", "labels": [], "entities": [{"text": "QE Task", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.6587290465831757}]}, {"text": "In the setup of the Quality Estimation Task (, no humanproduced translations are provided to estimate the quality of output translations.", "labels": [], "entities": []}, {"text": "Quality estimation (QE) methods are built to assess MT output based on the source or based on the translation itself.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.5865362048149109}, {"text": "MT", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9889534711837769}]}, {"text": "In this task, QE developers were invited to perform the same scoring as standard metrics participants, with the exception that they refrain from using a reference translation in production of their scores.", "labels": [], "entities": []}, {"text": "We then evaluate the QE submissions in exactly the same way as regular metrics are evaluated, see below.", "labels": [], "entities": []}, {"text": "From the point of view of correlation with manual judgements, there is no difference in metrics using or not using references.", "labels": [], "entities": []}, {"text": "The source, reference texts, and MT system outputs for the Metrics task come from the News Translation Task (, which we denote as Findings 2019).", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9805085062980652}, {"text": "News Translation Task", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7749545276165009}]}, {"text": "The texts were drawn from the news domain and involve translations of English (en) to/from Czech (cs), German (de), Finnish (fi), Gujarati (gu), Kazakh (kk), Lithuanian (lt), Russian (ru), and Chinese (zh), but excluding csen (15 language pairs).", "labels": [], "entities": []}, {"text": "Three other language pairs not including English were also manually evaluated as part of the News Translation Task: German\u2192Czech and German\u2194French.", "labels": [], "entities": [{"text": "News Translation", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7433739602565765}]}, {"text": "In total, metrics could participate in 18 language pairs, with 10 target languages.", "labels": [], "entities": []}, {"text": "In the following, we first give an overview of the task (Section 2) and summarize the baseline (Section 3) and submitted (Section 4) metrics.", "labels": [], "entities": []}, {"text": "The results for system-and segment-level evaluation are provided in Sections 5.1 and 5.2, respectively, followed by a joint discussion Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In system-level evaluation, the series of YiSi metrics achieve the highest correlations in several language pairs and it is not significantly outperformed by any other metrics (denoted as a \"win\" in the following) for almost all language pairs.", "labels": [], "entities": []}, {"text": "The new metric ESIM performs best on 5 language languages (18 language pairs) and obtains 11 \"wins\" out of 16 language pairs in which ESIM participated.", "labels": [], "entities": []}, {"text": "The metric EED performs better for language pairs out-of English and excluding En-glish compared to into-English language pairs, achieving 7 out of 11 \"wins\" there.", "labels": [], "entities": [{"text": "EED", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.4525184631347656}]}, {"text": "For segment-level evaluation, most language pairs are quite discerning, with only one or two metrics taking the \"winner\" position (of not being significantly surpassed by others).", "labels": [], "entities": []}, {"text": "Only French-German differs, with all metrics performing similarly except the significantly worse sentBLEU.", "labels": [], "entities": [{"text": "sentBLEU", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.8313262462615967}]}, {"text": "YiSi-1_srl stands out as the \"winner\" for all language pairs in which it participated.", "labels": [], "entities": []}, {"text": "The excluded language pairs were probably due to the lack of semantic information required by YiSi-1_srl.", "labels": [], "entities": []}, {"text": "YiSi-1 participated all language pairs and its correlations are comparable with those of YiSi-1_srl.", "labels": [], "entities": []}, {"text": "ESIM obtain 6 \"winners\" out of all 18 languages pairs.", "labels": [], "entities": [{"text": "ESIM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9217888116836548}]}, {"text": "Both YiSi and ESIM are based on neural networks (YiSi via word and phrase embeddings, as well as other types of available resources, ESIM via sentence embeddings).", "labels": [], "entities": []}, {"text": "This is a confirmation of a trend observed last year.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Absolute Pearson correlation of to-English system-level metrics with DA human assessment in  newstest2019; correlations of metrics not significantly outperformed by any other for that language pair  are highlighted in bold.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8136680722236633}]}, {"text": " Table 4: Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment  in newstest2019; correlations of metrics not significantly outperformed by any other for that language  pair are highlighted in bold.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.8256769378980001}]}, {"text": " Table 5: Absolute Pearson correlation of system-level metrics for language pairs not involving English  with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by  any other for that language pair are highlighted in bold.", "labels": [], "entities": [{"text": "Absolute Pearson correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.7937355041503906}]}, {"text": " Table 6: Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall's  Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly  outperformed by any other for that language pair are highlighted in bold.", "labels": [], "entities": [{"text": "DA", "start_pos": 158, "end_pos": 160, "type": "METRIC", "confidence": 0.9141351580619812}]}, {"text": " Table 7: Segment-level metric results for out-of-English language pairs in newstest2019: absolute  Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not  significantly outperformed by any other for that language pair are highlighted in bold.", "labels": [], "entities": [{"text": "DA", "start_pos": 162, "end_pos": 164, "type": "METRIC", "confidence": 0.9153145551681519}]}, {"text": " Table 8: Segment-level metric results for language  pairs not involving English in newstest2019: ab- solute Kendall's Tau formulation of segment-level  metric scores with DA scores; correlations of met- rics not significantly outperformed by any other for  that language pair are highlighted in bold.", "labels": [], "entities": [{"text": "ab- solute Kendall's Tau", "start_pos": 98, "end_pos": 122, "type": "METRIC", "confidence": 0.901319831609726}, {"text": "DA", "start_pos": 172, "end_pos": 174, "type": "METRIC", "confidence": 0.9395474195480347}]}]}