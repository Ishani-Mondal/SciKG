{"title": [{"text": "Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring", "labels": [], "entities": [{"text": "Unsupervised Machine Translation", "start_pos": 40, "end_pos": 72, "type": "TASK", "confidence": 0.6846866607666016}, {"text": "Language Model Rescoring", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7127788861592611}]}], "abstractContent": [{"text": "This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech.", "labels": [], "entities": [{"text": "WMT'19 news shared task from German to Czech", "start_pos": 93, "end_pos": 137, "type": "DATASET", "confidence": 0.7794843390583992}]}, {"text": "We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 14, "end_pos": 66, "type": "TASK", "confidence": 0.7123808349881854}, {"text": "word-level neural machine translation (NMT)", "start_pos": 117, "end_pos": 160, "type": "TASK", "confidence": 0.7173196758542743}]}, {"text": "We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018).", "labels": [], "entities": []}, {"text": "To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search.", "labels": [], "entities": [{"text": "consistency", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9717231392860413}]}, {"text": "Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) has achieved huge advances in the past few years (.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8953391075134277}]}, {"text": "However, the need fora large amount of manual parallel data obstructs its performance under low-resource conditions.", "labels": [], "entities": []}, {"text": "Building an effective model on low resource data or even in an unsupervised way is always an interesting and challenging research topic (.", "labels": [], "entities": []}, {"text": "Recently, unsupervised MT (Artetxe et al., 2018b,a;, which can immensely reduce the reliance on parallel corpora, has been gaining more and more interest.", "labels": [], "entities": [{"text": "MT", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9533072113990784}]}, {"text": "Training cross-lingual word embeddings) is always the *These two authors contributed equally.", "labels": [], "entities": []}, {"text": "first step of the unsupervised MT models which produce a word-level shared embedding space for both the source and target, but the lexical coverage can bean intractable problem.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9755206108093262}]}, {"text": "To tackle this issue, provided a subwordlevel solution to overcome the out-of-vocabulary (OOV) problem.", "labels": [], "entities": []}, {"text": "In this work, the systems we implement for the German-Czech language pair are built based on the previously proposed unsupervised MT systems, with some adaptations made to accommodate the morphologically rich characteristics of German and Czech (.", "labels": [], "entities": []}, {"text": "Both word-level and subword-level neural machine translation (NMT) models are applied in this task and further tuned by pseudo-parallel data generated from a phrase-based statistical machine translation (PBSMT) model, which is trained following the steps proposed in without using any parallel data.", "labels": [], "entities": [{"text": "subword-level neural machine translation (NMT)", "start_pos": 20, "end_pos": 66, "type": "TASK", "confidence": 0.8080181053706578}, {"text": "phrase-based statistical machine translation (PBSMT)", "start_pos": 158, "end_pos": 210, "type": "TASK", "confidence": 0.7182334320885795}]}, {"text": "We propose to train BPE embeddings for German and Czech separately and align those trained embeddings into a shared space with MUSE ( to reduce the combinatorial explosion of word forms for both languages.", "labels": [], "entities": [{"text": "MUSE", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.6120675802230835}]}, {"text": "To ensure the fluency and consistency of translations, an additional Czech language model is trained to select the translation candidates generated through beam search by rescoring them.", "labels": [], "entities": [{"text": "consistency", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9790757894515991}]}, {"text": "Besides the above, a series of post-processing steps are applied to improve the quality of final translations.", "labels": [], "entities": []}, {"text": "Our contribution is two-fold: \u2022 We propose a method to combine word and subword (BPE) pre-trained input representations aligned using MUSE ( as an NMT training initialization on a morphologically-rich language pair such as German and Czech.", "labels": [], "entities": []}, {"text": "\u2022 We study the effectiveness of language model rescoring to choose the best sentences and unknown word replacement (UWR) procedure to reduce the drawback of OOV words.", "labels": [], "entities": [{"text": "language model rescoring", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6413942873477936}, {"text": "unknown word replacement (UWR)", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.5821395417054495}]}, {"text": "This paper is organized as follows: in Section 2, we describe our approach to the unsupervised translation from German to Czech.", "labels": [], "entities": []}, {"text": "Section 3 reports the training details and the results for each steps of our approach.", "labels": [], "entities": []}, {"text": "More related work is provided in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. From comparing the  results, we observe that back-translation can im- prove the quality of the phrase table significantly,  but after five iterations, the phrase table has hardly  improved. The PBSMT model at the sixth itera- tion is selected as the final PBSMT model.", "labels": [], "entities": []}, {"text": " Table  3.8  + Back-translation Iter. 1  6.6  + Back-translation Iter. 2  7.3  + Back-translation Iter. 3  7.5  + Back-translation Iter. 4  7.6  + Back-translation Iter. 5  7.7  + Back-translation Iter. 6  7.7", "labels": [], "entities": []}, {"text": " Table 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our  approach. Except the result that is listed on the last line, all results are under the condition that the translations are  post-processed without patch-up.", "labels": [], "entities": []}]}