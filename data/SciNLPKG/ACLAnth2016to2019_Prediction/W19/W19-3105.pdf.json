{"title": [{"text": "On the Compression of Lexicon Transducers", "labels": [], "entities": [{"text": "Compression of Lexicon Transducers", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.7610679715871811}]}], "abstractContent": [{"text": "In finite-state language processing pipelines, a lexicon is often a key component.", "labels": [], "entities": []}, {"text": "It needs to be comprehensive to ensure accuracy, reducing out-of-vocabulary misses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9991868138313293}]}, {"text": "However, in memory-constrained environments (e.g., mobile phones), the size of the component au-tomata must be kept small.", "labels": [], "entities": []}, {"text": "Indeed, a delicate balance between comprehensiveness, speed, and memory must be struck to conform to device requirements while providing a good user experience.", "labels": [], "entities": [{"text": "speed", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9749061465263367}, {"text": "memory", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.979607343673706}]}, {"text": "In this paper, we describe a compression scheme for lexicons when represented as finite-state transducers.", "labels": [], "entities": []}, {"text": "We efficiently encode the graph of the transducer while storing transition labels separately.", "labels": [], "entities": []}, {"text": "The graph encoding scheme is based on the LOUDS (Level Order Unary Degree Sequence) tree representation , which has constant time tree traversal for queries while being information-theoretically optimal in space.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9844512939453125}]}, {"text": "We find that our encoding is near the theoretical lower bound for such graphs and substantially outperforms more traditional representations in space while remaining competitive in latency benchmarks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern finite-state language processing pipelines often consist of several finite-state transducers in composition.", "labels": [], "entities": []}, {"text": "For example, a virtual keyboard pipeline, used for decoding on mobile devices, can consist of a context dependency transducer C, a lexicon L, and an n-gram language model G (.", "labels": [], "entities": []}, {"text": "A bikey C transducer is used to encode context in gesture decoding, the lexicon transducer L maps from a character string to the corresponding word ID, and the language model G gives the a priori probability of a word sequence.", "labels": [], "entities": [{"text": "gesture decoding", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7274396419525146}]}, {"text": "A similar decomposition is often used in speech recognition decoding.", "labels": [], "entities": [{"text": "speech recognition decoding", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8501879572868347}]}, {"text": "These models are then composed as The application of this combined model to an input character string outputs the corresponding word string and probability.", "labels": [], "entities": []}, {"text": "Unfortunately, in order to be accurate, these models may need to be large.", "labels": [], "entities": []}, {"text": "This problem is aggravated when the composition is performed statically since the state space grows with the product of the input automata sizes.", "labels": [], "entities": []}, {"text": "In practice, on-the-fly composition is often used to save space ().", "labels": [], "entities": []}, {"text": "Additionally, it is of practical importance to have compact and efficient finite-state language model component representations.", "labels": [], "entities": []}, {"text": "There area variety of compression schemes available for automata).", "labels": [], "entities": []}, {"text": "These range from general compression algorithms, which do not depend on a specific underlying structure to schemes that try to heavily exploit specific structural properties of the inputs (.", "labels": [], "entities": []}, {"text": "Another important consideration is whether the automata can be decompressed just fora queried portion or need to be more fully decompressed.", "labels": [], "entities": []}, {"text": "Generic compression algorithms often have relatively good compression ratios over a wide class of machines, but they sacrifice speed and space in use since they often do not admit such selective decompression.", "labels": [], "entities": []}, {"text": "In contrast, structurally-specific compression algorithms can have an attractive balance between the compression ratio and query performance, but are limited to precise subclasses of machines.", "labels": [], "entities": []}, {"text": "In real-time production systems, the latter method often proves more desirable since a user should not have to wait long or waste space when a query is answered.", "labels": [], "entities": []}, {"text": "Among the transducers mentioned above, the context-dependency transducer C can be represented implicitly (in code) and structurallyspecific compression algorithms for the n-gram language model G have previously been developed.", "labels": [], "entities": []}, {"text": "This leads us to investigate the compression of the lexicon L.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the formal algebraic structures and notation that we will use.", "labels": [], "entities": []}, {"text": "Section 3 describes different representations for these algebraic structures.", "labels": [], "entities": []}, {"text": "In Section 4, we formally define a lexicon and explore its possible representations.", "labels": [], "entities": []}, {"text": "Section 5 develops an information-theoretic bound on the number of bits needed to encode a lexicon, Section 6 presents our encoding, and Section 7 presents experiments on the quality of that encoding.", "labels": [], "entities": []}, {"text": "Finally, we offer concluding remarks in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our lexicon encoding to the two other transducer representations in Section 3.2.", "labels": [], "entities": []}, {"text": "We measure the memory size of the resulting machines as well as their runtimes on a decoding task.", "labels": [], "entities": []}, {"text": "We prepare a set of lexicons using the most common 500k words in the Google keyboard (GBoard) Russian language model.", "labels": [], "entities": [{"text": "Google keyboard (GBoard) Russian language model", "start_pos": 69, "end_pos": 116, "type": "DATASET", "confidence": 0.8148270919919014}]}, {"text": "We extract the 50k, 100k, . .", "labels": [], "entities": []}, {"text": ", 500k most frequent words to create a total of 10 lexicons.", "labels": [], "entities": []}, {"text": "We first compare the space used by the AdjList, the CmpAdjList, and the LOUDS lexicon formats.", "labels": [], "entities": [{"text": "AdjList", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8950021266937256}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The LOUDS lexicon outperforms the other two formats in every case.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9519251585006714}]}, {"text": "On the 500k word lexicon, it is 90.8% smaller than the AdjList format and 58.8% smaller than the CmpAdjList format.", "labels": [], "entities": []}, {"text": "shows the number of bits required to encode the Russian lexicons using our encoding and the upper bound of the optimal encoding.", "labels": [], "entities": []}, {"text": "We use the parameters from along with the upper bound described in Section 5 and the number of bits for our representation from Section 6.", "labels": [], "entities": []}, {"text": "Our graph encoding nearly matches the upper bound approximation in all situations.", "labels": [], "entities": []}, {"text": "For the 500k lexicon, the difference between our encoding and the upper bound is less than 2%.", "labels": [], "entities": [{"text": "encoding", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9663234949111938}]}, {"text": "In contrast, the standard adjacency list format graph requires ten times more space across all test cases.", "labels": [], "entities": []}, {"text": "We now consider the performance of our encoding on a benchmark decoding task consisting of on-the-fly composition with an n-gram language model followed by shortest path computation, which simulates atypical pipeline in applications.", "labels": [], "entities": []}, {"text": "For the language model, we use a 244k state n-gram model trained: The size of the prefix out-tree and suffix in-tree as well as the number of leaves in each for all of the Russian lexicons.", "labels": [], "entities": []}, {"text": "Note that the number of bridge arcs is the same as the number of words.", "labels": [], "entities": []}, {"text": "shows the speed of this benchmark for each of the lexicon formats.", "labels": [], "entities": [{"text": "speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9912745356559753}]}, {"text": "At its worst, the LOUDS format was \u223c 20% slower than the CmpAdjList format.", "labels": [], "entities": [{"text": "LOUDS", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.8024072051048279}]}, {"text": "However, for the 500k word case, the difference between the LOUDS format and the CmpAdjList format was only 8.6%.", "labels": [], "entities": []}, {"text": "In these experiments, no pre-processing (transition sorting, caching, etc.) of the transducers was done so that the raw access time for each format could be measured more accurately.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The size of the prefix out-tree and suffix in-tree as well as the number of leaves in each for all of the  Russian lexicons. Note that the number of bridge arcs is the same as the number of words.", "labels": [], "entities": []}]}