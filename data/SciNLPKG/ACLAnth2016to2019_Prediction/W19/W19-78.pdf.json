{"title": [], "abstractContent": [{"text": "The data from a parallel annotated English-Czech corpus serve for testing the general issue of the variability of the mutual position of LOC and TWHEN in Czech and English (Sect.", "labels": [], "entities": []}, {"text": "4.1) and for the analysis of the relation between information structure and the given order in the two languages (Sect. 4.2).", "labels": [], "entities": []}, {"text": "The most relevant and innovative results in the investigation, namely the cases where the position of TWHEN and LOC differs in Czech and English in that the same modification is placed in Topic in the sentence in one language and in Focus in the corresponding sentence in the other are presented in Sect.", "labels": [], "entities": [{"text": "TWHEN", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.9804078936576843}, {"text": "LOC", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9681594371795654}]}], "introductionContent": [{"text": "With the rise of digital humanities, more and more ancient texts are made available.", "labels": [], "entities": []}, {"text": "Annotating them and keeping this information in treebanks helps study and describe old stages of a language.", "labels": [], "entities": []}, {"text": "Some are available for Medieval French (9 th -15 th c.), namely the MCVF 1, annotated with constituency syntax, and the SRCMF 2, annotated with dependency syntax and covering Old French (9 th -13 th c.) for now.", "labels": [], "entities": []}, {"text": "Our goal is to automatically extend the SRCMF treebank to obtain a larger resource.", "labels": [], "entities": [{"text": "SRCMF treebank", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.8715162873268127}]}, {"text": "In particular, we want to add texts of Middle French, the next stage in the evolution of French (14 th -15 th c.), as well as new texts of Old French.", "labels": [], "entities": []}, {"text": "This new resource would then contain one million words, four times more than the current SRCMF.", "labels": [], "entities": [{"text": "SRCMF", "start_pos": 89, "end_pos": 94, "type": "DATASET", "confidence": 0.9276127219200134}]}, {"text": "We want to annotate these data automatically with the highest quality, which means we need to find away to parse both Old and Middle French.", "labels": [], "entities": []}, {"text": "However, this task is difficult because we have limited resources annotated with dependency syntax in Old French, and none in Middle French ().", "labels": [], "entities": []}, {"text": "Moreover, Medieval French, like Old French, is subjected to great variation (sections 2 and 3).", "labels": [], "entities": []}, {"text": "The new texts will be annotated by both a statistical and a symbolic parser.", "labels": [], "entities": []}, {"text": "The annotation will then be merged to obtain the best possible analysis.", "labels": [], "entities": []}, {"text": "For this work, we focus on the symbolic approach.", "labels": [], "entities": []}, {"text": "Using wide coverage grammars () has shown effective (section 4), so we chose to adapt the French Metagrammar (FRMG, Villemonte de la), a symbolic system for contemporary French (section 5).", "labels": [], "entities": [{"text": "French Metagrammar (FRMG, Villemonte de la)", "start_pos": 90, "end_pos": 133, "type": "DATASET", "confidence": 0.7672688696119521}]}, {"text": "Our contribution is to make a diachronic grammar for Medieval French, currently still in process.", "labels": [], "entities": [{"text": "Medieval French", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.7633139491081238}]}], "datasetContent": [{"text": "We use the Universal Dependencies (UD v2.3) treebanks of 14 languages as a dataset ( ).", "labels": [], "entities": [{"text": "Universal Dependencies (UD v2.3) treebanks", "start_pos": 11, "end_pos": 53, "type": "DATASET", "confidence": 0.6161272057465145}]}, {"text": "The languages were selected for typological diversity: the dataset contains 8 head-initial languages and 6 head-final languages.", "labels": [], "entities": []}, {"text": "We do not include dependencies marking punctuation (labeled as 'punct' in UD scheme) and the abstract root of the tree (labeled as 'root' in UD scheme) in our analysis.", "labels": [], "entities": []}, {"text": "As we discuss below, the process of sampling random baseline trees makes it prohibitively difficult to study all languages in the UD dataset.", "labels": [], "entities": [{"text": "UD dataset", "start_pos": 130, "end_pos": 140, "type": "DATASET", "confidence": 0.8277795612812042}]}, {"text": "Therefore we study treebanks of 14 languages: German, English, Hindi, French, Arabic, Russian, Czech, Italian, Spanish, Afrikaans, Japanese, Korean, Bulgarian and Slovak.", "labels": [], "entities": []}, {"text": "We present results aggregating over dependency trees from all these languages.", "labels": [], "entities": []}, {"text": "In this section we will present the results of the annotation by evaluating parser performance on the current state of the treebank.", "labels": [], "entities": []}, {"text": "In particular, we evaluate the relevance of macrosyntactic markup for syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.841599851846695}]}, {"text": "We expect the macrosyntactic annotation to have a positive influence on dependency parsing, in particular for constructions such as coordination and dislocation, which have specific macrosyntactic markups resulting in specific dependency relations.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.8484475314617157}]}, {"text": "In order to verify this claim, we trained the Mate tagger and parser by, first on aversion of the treebank with these markups, and then on aversion of the treebank where they have been removed except for \"//\" (the segmentation into illocutionary units \u2243 sentences).", "labels": [], "entities": []}, {"text": "This type of experiment is also important to set a baseline for the development of a Naija parser, to be used for parsing the rest of the NSC corpus (which is transcribed and macrosyntactically annotated) as well as for parsing other spoken and written data without markup.", "labels": [], "entities": [{"text": "Naija parser", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.7913797199726105}, {"text": "NSC corpus", "start_pos": 138, "end_pos": 148, "type": "DATASET", "confidence": 0.8485181331634521}, {"text": "parsing other spoken and written data", "start_pos": 220, "end_pos": 257, "type": "TASK", "confidence": 0.8251790603001913}]}, {"text": "We used a sample of 52k words, with 90% training and 10% test data on the Mate parser.", "labels": [], "entities": [{"text": "Mate parser", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7569025754928589}]}, {"text": "While the POS tagging scores are as expected very similar whether macrosyntactic annotation is present or not, we obtain an LAS error reduction of 11% and a UAS error reduction of 18% through macrosyntactic annotation, see.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.6765489876270294}, {"text": "LAS error reduction", "start_pos": 124, "end_pos": 143, "type": "METRIC", "confidence": 0.9528919458389282}, {"text": "UAS error reduction", "start_pos": 157, "end_pos": 176, "type": "METRIC", "confidence": 0.8345349828402201}]}, {"text": "Parsing results with and without macro-syntactic annotation.", "labels": [], "entities": []}, {"text": "Unsurprisingly, the syntactic functions which most benefit from this type of markup are those that are targeted by the annotation, such as piles (paradigmatic relations like conj:coord, conj:dicto, compound:redup) and coordinators (cc).", "labels": [], "entities": []}, {"text": "We also observe an improvement for relations that connect a nucleus and adnuclei, such as clefts, dislocations, and peripheric modifiers.", "labels": [], "entities": []}, {"text": "The parser scores are promising, 11 in particular for spoken texts, and we hope to further improve the parser performance by the ongoing process of semi-automatic rule-based enhancement of the treebank coherence.", "labels": [], "entities": []}, {"text": "In particular, we address this problem of annotation inconsistencies by a systematic comparison of parsing results with the gold annotation and the double SUD-UD-SUD conversion, and by different error mining tools such as the relation table proposed by the grew tool available on match.grew.fr, which shows the number of dependency relation types between any pair of categories.", "labels": [], "entities": []}, {"text": "Also, the move to a neural network-based parser can be expected to result in better scores.", "labels": [], "entities": []}, {"text": "10 punct relations are excluded from the evaluation as they are exclusively used for macro-syntactic markers.", "labels": [], "entities": []}, {"text": "Although Naija was part of the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies), it is difficult to compare the results as Naija was one of the low-resource languages.", "labels": [], "entities": [{"text": "CoNLL 2018 Shared Task", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.7855292111635208}, {"text": "Multilingual Parsing from Raw Text to Universal Dependencies)", "start_pos": 55, "end_pos": 116, "type": "TASK", "confidence": 0.750661356581582}]}, {"text": "The best score for UPOS, UAS, and LAS are 67..", "labels": [], "entities": [{"text": "UPOS", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.5395641922950745}, {"text": "UAS", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6272491812705994}, {"text": "LAS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9834095239639282}]}, {"text": "Data The analyses were undertaken using the English treebanks (EWT, GUM, LinES, and ParTUT) and also Bulgarian-BTB, German-GSD, and Japanese-GSD from UD v2.3 ( . No results are given for Japanese-GSD for morphological feature tagging as it does not contain this information.", "labels": [], "entities": [{"text": "English treebanks", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.8883118629455566}, {"text": "EWT", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.5818787813186646}, {"text": "ParTUT", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8685290813446045}, {"text": "morphological feature tagging", "start_pos": 204, "end_pos": 233, "type": "TASK", "confidence": 0.6409166057904562}]}, {"text": "Network hyperparameters We used the framework as described above and hyperparameters from  which can be seen in in the Appendix B. The standard input to the system consisted of word embeddings concatenated with character embeddings.", "labels": [], "entities": []}, {"text": "All embeddings were randomly initialised.", "labels": [], "entities": []}, {"text": "Figure 3: Multi-task architecture shown with sequence-labelling dependency parsing (as described in subsection 4.1), POS tagging, and chunking as shared tasks.", "labels": [], "entities": [{"text": "sequence-labelling dependency parsing", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.639182835817337}, {"text": "POS tagging", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.8212856352329254}]}, {"text": "Network input is a concatenation of word embeddings (circles) and character-level word embeddings (triangles) obtained from a character-based LSTM layer.", "labels": [], "entities": []}, {"text": "The network is constructed of BiLSTM layers followed by a softmax layer for inference.", "labels": [], "entities": []}, {"text": "Experiment 1 We tested the impact of our chunks on POS and morphological feature tagging in a shared multi-task setting.", "labels": [], "entities": [{"text": "morphological feature tagging", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6360216240088145}]}, {"text": "This entails feeding word and character embeddings as input to the network with the output being some combination of POS tags, morphological feature tags, and chunk labels.", "labels": [], "entities": []}, {"text": "These results were compared against the baseline taggers (single-task networks and POS and morphological features shared only).", "labels": [], "entities": []}, {"text": "As a further baseline we include results for POS and morphological feature tagging using UDPipe 2.2 (.", "labels": [], "entities": [{"text": "morphological feature tagging", "start_pos": 53, "end_pos": 82, "type": "TASK", "confidence": 0.6508674720923106}]}, {"text": "Experiment 2 We used the best predictions (when using chunking) from experiment 1 as additional features fora sequence-labelling dependency parser (.", "labels": [], "entities": []}, {"text": "Therefore, network input consisted of word and character embedding and then some combination of POS tags, morphological feature tags, or chunk labels with the sole output being a dependency parser tag.", "labels": [], "entities": []}, {"text": "We used gold tags and labels as input during training, but at runtime we used predicted tags and labels.", "labels": [], "entities": []}, {"text": "For baselines we train a model with no features which is decoded with predicted POS tags using UDPipe 2.2 (as the sequencelabelling encoding we are using requires POS tags to resolve dependency heads) and also a model trained with POS tags as features but also using UDPipe 2.2 predicted POS tags at runtime.", "labels": [], "entities": []}, {"text": "In order to map the Greenbergian universals wrt certain linguistic orders onto the network, we reduced the problem to only probing the node parameters of the layer 2 'combinatorial' nodes.", "labels": [], "entities": []}, {"text": "This was done because we are interested in word order generalizations related to the verb.", "labels": [], "entities": [{"text": "word order generalizations", "start_pos": 43, "end_pos": 69, "type": "TASK", "confidence": 0.651287704706192}]}, {"text": "In particular, we looked at each of the word-order based Greenbergian universal and translated them to a particular network parameter of various combinatorial nodes in layer 2.", "labels": [], "entities": []}, {"text": "The orders SOV, SVO, VSO etc. are believed to be encoded in the parameter 'Outperc' of the layer 2 nodes.", "labels": [], "entities": [{"text": "Outperc", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.9399250149726868}]}, {"text": "'Outperc' is defined as the out-degree of the concerned node divided by the total no. of nodes in layer 2.", "labels": [], "entities": [{"text": "Outperc", "start_pos": 1, "end_pos": 8, "type": "METRIC", "confidence": 0.9622323513031006}]}, {"text": "A language is deemed to be SOV if the SOV node's 'Outperc' is high relative to other nodes in layer 2.", "labels": [], "entities": []}, {"text": "We investigate if the distribution of 'Outperc' across all language networks leads to the correct language typology clusters.", "labels": [], "entities": []}, {"text": "This experiment is intended as a supervised way of identifying language typology clusters based on Greenberg's word order universals.", "labels": [], "entities": []}, {"text": "The data available in WALS) was used to get the word order patterns related to the Greenbergian universals for various language.", "labels": [], "entities": []}, {"text": "Experiment 1 targetted a specific universal and mapped it onto the network using a prespecified node property (Outperc/Outdgree of SOV, VSO, SVO layer 2 nodes).", "labels": [], "entities": []}, {"text": "In experiment 2, we asked a more general question -which node parameter in different language networks leads to the best language typology classi-  fication based on Greenberg's universals?", "labels": [], "entities": []}, {"text": "The linguistic orders that we looked at were taken from WALS (]; these were, (a) Order of subject, verb and object, (b) Order of Adposition and Noun Phrase, (c) Order of Adjective and Noun, and (d) Position of Interrogative Phrase and Content Questions.", "labels": [], "entities": [{"text": "Position of Interrogative Phrase and Content Questions", "start_pos": 198, "end_pos": 252, "type": "TASK", "confidence": 0.6831454379217965}]}, {"text": "We investigate various parameters 8 for each node in layer 2 to see which node-parameter combinations across all the languages lead to the best language classification fora particular word order.", "labels": [], "entities": []}, {"text": "For example, consider \"Order of Adposition and Noun Phrase\".", "labels": [], "entities": []}, {"text": "In order to find which parameter of which layer 2 node can lead to the best classification of languages based on this order, we get a particular node-parameter values from all language networks, and check if this distribution leads to the correct classification of languages as given in the WALS data.", "labels": [], "entities": [{"text": "WALS data", "start_pos": 291, "end_pos": 300, "type": "DATASET", "confidence": 0.8565488159656525}]}, {"text": "The correlation between the node-parameter values and the correct language cluster (which is already known) is quantified by silhouette value.", "labels": [], "entities": [{"text": "silhouette", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9796903133392334}]}, {"text": "This silhouette value is obtained for all the (nodes \u00d7 parameters) node-parameter combinations and the highest score gives us the node-parameter that classifies the languages best based on the word order under consideration.", "labels": [], "entities": []}, {"text": "A greater silhouette value corresponds to better clustering.", "labels": [], "entities": [{"text": "silhouette", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9768148064613342}]}, {"text": "Intuitively, the silhouette value captures the cohesiveness of the data point with its cluster.", "labels": [], "entities": []}, {"text": "To summarize, experiment 2 discusses a method to induce the linguistic orders by probing all possible parameters for each verb-order nodes that are contained in layer 2.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the GNN algorithm compared to the AVG baseline in an automated way across various languages, we must unfortunately use a single target reference to compare the generated sentences.", "labels": [], "entities": [{"text": "AVG baseline", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.8884535729885101}]}, {"text": "Thus the reference for each sentence is the attested version in the source UD corpus; the generated sentences from both AVG and GNN will be measured for similarity to the attested version.", "labels": [], "entities": [{"text": "UD corpus", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.8546269536018372}]}, {"text": "The algorithm is attempting to order a set of words as closely as possible to their original surface realization in the corpus.", "labels": [], "entities": []}, {"text": "Because words may repeat in the sentence, each order is instead represented by a list of integers, and it is these lists of integers which are compared.", "labels": [], "entities": []}, {"text": "For example, assuming a target reference order of for the red horse, the generated order of red the horse would be.", "labels": [], "entities": []}, {"text": "An obvious way to quantify how similar these integer lists are is with the widely used Spearman's rank correlation coefficient, also known as Spearman's \u03c1 (rho), which non-parametrically measures the similarity of two rankings.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 87, "end_pos": 126, "type": "METRIC", "confidence": 0.6301902592182159}, {"text": "Spearman's \u03c1 (rho)", "start_pos": 142, "end_pos": 160, "type": "METRIC", "confidence": 0.733870675166448}]}, {"text": "It ranges from -1, indicating that one order is the reverse of the other, to 1, for perfect correlation.", "labels": [], "entities": []}, {"text": "The example of [2,1,3] returns a \u03c1 of 0.5, since in the second order 1 and 2 both precede 3, but 1 does not precede 2.", "labels": [], "entities": []}, {"text": "This measure tells us which approach, AVG or GNN, generates orders closest to the attested UD order, as well as a loose gauge of overall effectiveness for both the general approach as well as each algorithm.", "labels": [], "entities": []}, {"text": "Further, to address the question of projectivity, the percentage of projective dependency arcs generated by the AVG baseline, the GNN algorithm, and the attested sentences is evaluated.", "labels": [], "entities": [{"text": "AVG baseline", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.8915346264839172}]}, {"text": "In each case, projectivity is calculated as the number of instances in which a word appearing between ahead hand dependent dis not dominated by h.", "labels": [], "entities": []}, {"text": "This measure allows us to explore how dependency distance might result in known rates of projectivity in natural language.", "labels": [], "entities": []}, {"text": "shows the results of running both the AVG baseline and GNN algorithm on 14 v2.4 UD corpora representing a range of language families.", "labels": [], "entities": [{"text": "AVG baseline", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.8748928308486938}]}, {"text": "These are relatively small corpora-between 500 and 2000 training sentences-and as a consequence their small vocabularies result in embedding vector dimensions between 14 and 22 due to the use of twice the fourth root of vocabulary size ( \u00a72.4, \u00a73.1).", "labels": [], "entities": []}, {"text": "While smaller than the more usual 50-or 300-element vectors, tying dimensionality to corpus vocabulary size seemed to avoid instability in the embedding space, though perhaps not in every case.", "labels": [], "entities": []}, {"text": "Further, experiments with larger dimensions resulted in poor generalization to the testing set, possibly due to alack of correlation between embeddings seen and unseen during training.", "labels": [], "entities": []}, {"text": "Results from Spearman's \u03c1 rank correlation show that both AVG and GNN were able to positively correlate surface order with the source UD corpora.", "labels": [], "entities": [{"text": "\u03c1 rank correlation", "start_pos": 24, "end_pos": 42, "type": "METRIC", "confidence": 0.9088839888572693}, {"text": "AVG", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.883718729019165}]}, {"text": "Because Spearman's \u03c1 ranges from -1 to 1, positive values are better than chance; values above 0.5 seem rather promising.", "labels": [], "entities": []}, {"text": "A large part of surface order can apparently be predicted based on dependency distance, averaged or learned.", "labels": [], "entities": []}, {"text": "In all cases the GNN was able to approach AVG, exceeding it 10 out of 14 times.", "labels": [], "entities": [{"text": "GNN", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.904448926448822}, {"text": "AVG", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7227097749710083}]}, {"text": "For many languages, the GNN achieved its peak value before training was complete, probably indicating overfitting.", "labels": [], "entities": []}, {"text": "In the cases in which the GNN did not best AVG, the sparkline trends for Czech, Hungarian, and Latin suggest problems during training, perhaps due to overzealous learning rates or unstable embeddings, while Uyghur came very close.", "labels": [], "entities": [{"text": "AVG", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.7354077100753784}, {"text": "sparkline", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9795023798942566}]}, {"text": "We present parsing baselines for the new German UD treebank, using the state-of-the-art parser of.", "labels": [], "entities": [{"text": "German UD treebank", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.8650795817375183}]}, {"text": "The parser is a neural dependency parser that learns complex, non-linear representations directly from the input text, based on bidirectional LSTMs . It only considers local context and predicts attachments and labels in a greedy fashion.", "labels": [], "entities": []}, {"text": "The huge success of the parser is based on its use of biaffine attention.", "labels": [], "entities": []}, {"text": "In our first experiment, we train the parser on the 250 tweets in the tweeDe training set.", "labels": [], "entities": [{"text": "tweeDe training set", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.8077884912490845}]}, {"text": "We use pretrained skipgram embeddings with 100 dimensions (window size: 5, min word count: 10), trained on a large collection of German tweets, collected in a time period from 2013 to 2017.", "labels": [], "entities": []}, {"text": "The embeddings are publically available from https://www.cl.uni-heidelberg.de/research/downloads.", "labels": [], "entities": []}, {"text": "All models have been trained with default parameters.", "labels": [], "entities": []}, {"text": "(left) shows results for gold PoS and for automatically predicted PoS tags.", "labels": [], "entities": []}, {"text": "Using UD PoS tags for parsing outperforms the STTS tags by a large margin, probably due to sparsity caused by the more fine-grained STTS.", "labels": [], "entities": [{"text": "UD PoS tags", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.7308308084805807}, {"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9754664897918701}]}, {"text": "Feeding both, UD and STTS tags, to the parser can further increase results, but only slightly (less than 1%).", "labels": [], "entities": [{"text": "UD", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.6906505227088928}]}, {"text": "Most surprisingly, we obtain higher results when using automatically predicted STTS tags (as compared to using gold STTS tags).", "labels": [], "entities": []}, {"text": "This observation, however, is more pronounced for the test set and might not be representative, being an artefact of the small data size.", "labels": [], "entities": []}, {"text": "Results for training on the small tweeDe dataset only are in the range of 74% LAS (gold PoS) and 68% LAS (auto PoS).", "labels": [], "entities": [{"text": "tweeDe dataset", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8969221711158752}, {"text": "LAS (gold PoS)", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.8244947075843811}, {"text": "LAS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9751986265182495}]}, {"text": "When adding the training data from the German-GSD UD treebank, results increase to 81% LAS (gold PoS) and 76% LAS (auto PoS).", "labels": [], "entities": [{"text": "German-GSD UD treebank", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.907496710618337}, {"text": "LAS (gold PoS)", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.7829505264759063}, {"text": "LAS", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9885103106498718}, {"text": "auto PoS)", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9090534647305807}]}, {"text": "The large gap of 5% between the gold and auto PoS setting highlights the importance of high-quality PoS tags for parsing tweets. were among the first to provide syntactic analyses for Twitter microtext.", "labels": [], "entities": [{"text": "parsing tweets.", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.8925531506538391}]}, {"text": "They created a testset with over 500 sentences extracted from tweets.", "labels": [], "entities": []}, {"text": "The data was automatically parsed with a constituency parser and the trees were manually corrected by one annotator.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement (IAA) for labelled bracketing, measured on a subset of the data annotated by a second annotator, was quite high with nearly 96%.", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8808324217796326}, {"text": "labelled bracketing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7728201448917389}]}, {"text": "Parsing accuracy without any domain adaptation, however, was low: the Malt parser (), trained on the WSJ, achieved an LAS of 63.3% on the Twitter testset.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8209936618804932}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9014686346054077}, {"text": "WSJ", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.968020498752594}, {"text": "LAS", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9980055689811707}]}, {"text": "The Tweebank v1 () is another English Twitter treebank, with a size of over 900 tweets annotated with unlabelled dependencies.", "labels": [], "entities": [{"text": "English Twitter treebank", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.8271355231602987}]}, {"text": "extend the work of by enlarging the treebank to more than 3,500 tweets, refining the guidelines and adding labels to the former unlabelled trees.", "labels": [], "entities": []}, {"text": "They report an IAA of 84.3% for labelled attachments in the Tweebank v2.", "labels": [], "entities": [{"text": "IAA", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.999235987663269}, {"text": "Tweebank v2", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.941591888666153}]}, {"text": "A third English Twitter treebank was created by.", "labels": [], "entities": [{"text": "English Twitter treebank", "start_pos": 8, "end_pos": 32, "type": "DATASET", "confidence": 0.9565508961677551}]}, {"text": "Their corpus includes 250 AfricanAmerican English (AAE) tweets and 250 tweets of mainstream American English microtext.", "labels": [], "entities": []}, {"text": "The data has been annotated by two coders but no inter-annotator agreement is reported.", "labels": [], "entities": []}, {"text": "We tested the impact of our chunks on a sequence-labelling dependency parser in a multi-task framework with and without the other tasks.", "labels": [], "entities": []}, {"text": "POS tagging was treated as a secondary main task with a weight of 0.5 (as POS tags are needed to decode the sequence-labelling scheme for the dependency parser) and chunks and morphological features were considered auxiliary tasks with a weight of 0.25 when used.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6775170266628265}]}, {"text": "The input during this experiment were only word and character embeddings.", "labels": [], "entities": []}, {"text": "An example is shown in where the shared tasks are chunking, POS tagging, and dependency parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.847257137298584}, {"text": "dependency parsing", "start_pos": 77, "end_pos": 95, "type": "TASK", "confidence": 0.8134017884731293}]}, {"text": "The baseline used here is a model trained solely to predict dependency parsing tags which are then decoded using predicted POS tags from UDPipe 2.2.", "labels": [], "entities": [{"text": "dependency parsing tags", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.776930550734202}]}], "tableCaptions": [{"text": " Table 1: Mixed-effect Poisson regression results for all the crossing constraints and dependency tree  measures for 14 languages. \"Observed\" is an indicator variable with value 1 for observed trees and 0 for  random trees, the same as r i in Equation 2. A significant interaction between an independent variable  and Observed rejects the null hypothesis.", "labels": [], "entities": []}, {"text": " Table 1. Inter-annotator agreement scores", "labels": [], "entities": []}, {"text": " Table 2. Parsing results with and without macro-syntactic annotation.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.905936598777771}]}, {"text": " Table 1: Top 3 silhouette score for the clusters related to the 4 word order patterns. 81A: Order of subject,  verb and object; 85A: Order of adposition and noun phrase; 87A: Order of adjective and noun; 93A: position  of interrogative phrases in content question. Note: The results for 85A are based on manual evaluation as the  top silhouette scores failed to give the correct clusters. Closeness C = Closeness Centrality; Neighbourhood  C = Neighbourhood connectivity.", "labels": [], "entities": []}, {"text": " Table 4: Statistics on manual class disambiguation results, by type", "labels": [], "entities": [{"text": "manual class disambiguation", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.5866179764270782}]}, {"text": " Table 1: The relative position of TWHEN and LOC in English and Czech in PCEDT", "labels": [], "entities": [{"text": "TWHEN", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.9720262885093689}, {"text": "LOC", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8909090161323547}, {"text": "PCEDT", "start_pos": 73, "end_pos": 78, "type": "DATASET", "confidence": 0.7552485466003418}]}, {"text": " Table 2: The occurrence of orderings of TWHEN and LOC in Focus in E. and in Cz. in PCEDT", "labels": [], "entities": [{"text": "TWHEN", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9932398796081543}, {"text": "LOC", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9698821902275085}, {"text": "PCEDT", "start_pos": 84, "end_pos": 89, "type": "DATASET", "confidence": 0.8619294166564941}]}, {"text": " Table 3: The frequency of TWHEN and LOC (expressed by non-sentential elements) and their ordering  in Focus in Czech in PDT 2.0 according to", "labels": [], "entities": [{"text": "TWHEN", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9693326950073242}, {"text": "LOC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.972258985042572}, {"text": "Czech in PDT 2.0", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.6833680868148804}]}, {"text": " Table 4: The position of TWHEN and LOC with respect to the Predicate in English compared to Czech", "labels": [], "entities": [{"text": "TWHEN", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9945029020309448}, {"text": "LOC", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9960365891456604}, {"text": "Predicate", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.4698977470397949}]}, {"text": " Table 1: Results. Each language is listed by its corpus; number of training and testing sentences; embed- ding dimension; Spearman's \u03c1 rank correlation coefficient for AVG and GNN; and rate of projectivity  for AVG, GNN, and as attested in the UD corpus. Boldfaced numbers indicate cases in which GNN per- formed better than AVG. Sparklines show trends over 10K iterations with horizontal gray lines indicating  AVG performance and black dots showing peak performance of GNN.", "labels": [], "entities": [{"text": "embed- ding dimension", "start_pos": 100, "end_pos": 121, "type": "METRIC", "confidence": 0.8582498580217361}, {"text": "Spearman's \u03c1 rank correlation coefficient", "start_pos": 123, "end_pos": 164, "type": "METRIC", "confidence": 0.7572756359974543}, {"text": "UD corpus", "start_pos": 245, "end_pos": 254, "type": "DATASET", "confidence": 0.9209071099758148}]}, {"text": " Table 2: POS and parsing results of neural- stacking model for different languages", "labels": [], "entities": [{"text": "neural- stacking", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.6895479361216227}]}, {"text": " Table 3: Effect of embeddings on POS and Parser  results for the Trilingual + Gold-(HE + BE) model", "labels": [], "entities": [{"text": "Trilingual + Gold-(HE + BE)", "start_pos": 66, "end_pos": 93, "type": "METRIC", "confidence": 0.4460099972784519}]}, {"text": " Table 2: Parsing results for the Dozat parser on tweeDe, without (left) and with additional training data  from the German-GSD UD treebank (right).", "labels": [], "entities": [{"text": "German-GSD UD treebank", "start_pos": 117, "end_pos": 139, "type": "DATASET", "confidence": 0.9194230238596598}]}, {"text": " Table 3: Statistics for manually annotated treebanks (*Foster et al. only report # sentences, not # tweets.  We expect the no. of tweets to be slightly lower than 500). The data of Blodgett et al. includes AAE and  main-stream (MS) English tweets. The last two columns report results for the Dozat & Manning parser  (Dozat et al., 2017) (w/o domain adaptation) or the Malt parser from the literature.", "labels": [], "entities": [{"text": "AAE", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.9262394309043884}]}, {"text": " Table 1: Chunking statistics on test data for each treebank used where # rules is the number of rules in a  ruleset for a given threshold and C/sent corresponds to the number of chunks per sentence found.", "labels": [], "entities": [{"text": "Chunking", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.9612972140312195}]}, {"text": " Table 2: Multi-task tagging performance on English UD treebanks (en-ewt, en-gum, en-lines, and en- partut), Bulgarian-BTB (bg), German-GSD (de), and Japanese-GSD (ja) UD treebanks: single, single- task training; pos, with POS tagging; feats, with morphological feature tagging (except Japanese (ja)  which has no morphological features); and chunks x , with chunks with threshold x.", "labels": [], "entities": [{"text": "Multi-task tagging", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7242712378501892}, {"text": "English UD treebanks", "start_pos": 44, "end_pos": 64, "type": "DATASET", "confidence": 0.5768701235453287}, {"text": "POS tagging", "start_pos": 223, "end_pos": 234, "type": "TASK", "confidence": 0.6802299320697784}]}, {"text": " Table 3: Chunker F1 scores in multi task setting where the baseline presented is from training the chunker  for a given ruleset with threshold 75% or 95% as a single task and multi is from training with pos and  morphological feature tagging except for Japanese (ja) which has no morphological features.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.8450721502304077}]}, {"text": " Table 5: Multi-task parsing results for English (en-ewt, en-gum, en-lines, and en-partut), Bulgarian-BTB  (bg), German-GSD (de), and Japanese-GSD (ja) UD treebanks: single ud pipe , parsing as single task with  UDPipe predicted POS tags used to decode parser output; pos, with POS tagging as aux. task; feats, with  morphological feature tagging as aux. task; and chunks x , with chunking as aux. task for threshold x.", "labels": [], "entities": [{"text": "Multi-task parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7083562612533569}, {"text": "UD treebanks", "start_pos": 152, "end_pos": 164, "type": "DATASET", "confidence": 0.7242357432842255}]}, {"text": " Table 6: Hyperparameters for the evolutionary algorithm: k-best, the number of best parents chosen to  seed next generation; P mutate , the probability an individual will mutate; P mutate gene , the probability a given  gene will mutate; P crossover , the probability a pair of individuals will crossover; and decay is how much  P mutate and P crossover decrease after each generation.", "labels": [], "entities": []}, {"text": " Table 7: Hyperparameters for the neural-net chunker used during the evolutionary algorithm.", "labels": [], "entities": []}, {"text": " Table 8: Hyperparameters for the network used in all experiments.", "labels": [], "entities": [{"text": "Hyperparameters", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.947329044342041}]}]}