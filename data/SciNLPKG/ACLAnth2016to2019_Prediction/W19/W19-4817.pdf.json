{"title": [{"text": "Probing word and sentence embeddings for long-distance dependencies effects in French and English", "labels": [], "entities": []}], "abstractContent": [{"text": "The recent widespread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages.", "labels": [], "entities": []}, {"text": "Results are at present inconclusive.", "labels": [], "entities": []}, {"text": "In this paper , we extend previous work on long-distance dependencies in three ways.", "labels": [], "entities": []}, {"text": "We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties understudy.", "labels": [], "entities": []}, {"text": "We extend the work to sentence embed-dings and to new languages.", "labels": [], "entities": []}, {"text": "We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they use, learn and generate and specifically if they exhibit properties similar to those characterising human languages.", "labels": [], "entities": []}, {"text": "Results are at present rather inconclusive on whether RNNs and the representations they learn have human-like properties.", "labels": [], "entities": []}, {"text": "While many pieces of work seems to indicate that they do, some other pieces of work have mixed results, and a few appear to show that the representations of RNNs do not match those predicted by linguistic theory or human experiments.", "labels": [], "entities": []}, {"text": "For example, one line of work aims to correlate RNN-induced representations to linguistic properties, namely the fact that subject-verb number agreement is structuredependent.", "labels": [], "entities": []}, {"text": "Initial work had shown RNNs do not really learn the structure-dependency of this construction (), but followup work has shown that stronger techniques can yield more positive results (), only to be very promptly rebutted by work suggesting that the apparently positive results could be the artifact of a much simpler strategy, which takes advantage of the unnaturally simple structure of the examples and simply learns properties of the first word in the sentence (.", "labels": [], "entities": []}, {"text": "Recent work by, however, studies RNNs in more detail, looking at single neurons, and finds that individual neurons encode linguistically meaningful features very saliently and with behaviour overtime that corresponds to the expected propagation of subject-verb number agreement information.", "labels": [], "entities": []}, {"text": "Similarly, probing different aspects of longdistance dependencies, so far divergent results have been reported on these constructions.", "labels": [], "entities": []}, {"text": "While some experiments have shown that RNNs can learn the main descriptive properties of longdistance dependencies in English, for example the fact that they obey a uniqueness constraint (only one gap per filler) and also that they obey island constraints (, work attempting to replicate finer-grained human judgments for French have failed to show a correlation with human behaviour, while other work on English has found mixed results.", "labels": [], "entities": []}, {"text": "In this paper, we extend previous work on longdistance dependencies to tease apart the potential grounds for the different outcomes by making previous work more comparable.", "labels": [], "entities": []}, {"text": "There are several differences between the pieces of work on long-distance dependencies mentioned above.", "labels": [], "entities": []}, {"text": "First, the work that does not find a correspondence between the two sources of information being compared) imposes a much stricter test of correspondencetotal correlation-than the general effect reported in.", "labels": [], "entities": []}, {"text": "Secondly, the pieces of work vary in task: it is possible that word embed-dings need to be used holistically in a prediction task similar to what humans solve to show a positive correlation.", "labels": [], "entities": []}, {"text": "Finally, these pieces of work are on different languages.", "labels": [], "entities": []}, {"text": "Beside these differences in experimental setup, it is also possible that holistic representations such as word embeddings need to be transformed and translated into the right space to show correlations with human judgments.", "labels": [], "entities": []}, {"text": "Specifically, word embeddings area merger of the many levels of representation that we find inhuman languages: lexical, morphological, syntactic, semantic.", "labels": [], "entities": []}, {"text": "It has been argued that post-processing transformations can tease apart syntactic aspects of distributed representations from semantic aspects (.", "labels": [], "entities": []}, {"text": "Based on all these observation, we extend the work of, which had found no correlation, along these lines.", "labels": [], "entities": []}, {"text": "To preview, while we are able to get slightly better correlations to human judgments than those reported by, the mixed results are confirmed: word embeddings and sentence embeddings, the representations produced by RNNs, do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.", "labels": [], "entities": []}], "datasetContent": [{"text": "The psycholinguistic experiments that collected the experimental measures reflecting the acceptability or reading times of a sentence are described in and and they are the same as those discussed in.", "labels": [], "entities": []}, {"text": "The initial experiments were done in French.", "labels": [], "entities": []}, {"text": "1 shows the English version of the kind of sentences that are used as stimuli in the experiments.", "labels": [], "entities": []}, {"text": "The object relative clause experiment collected on-line reading times, manipulating the number (singular or plural) of the object of the relative clause as the intervening feature and the construction, with or without long-distance dependency.", "labels": [], "entities": []}, {"text": "A speed-up effect in number mismatch configurations (plural object) was found in object relative clauses.", "labels": [], "entities": []}, {"text": "The weak islands experiment collected off-line acceptability judgments, manipulating animacy and lexical restriction of the intervener.", "labels": [], "entities": []}, {"text": "A clear effect of animacy match for lexically restricted phrases (less acceptable) and less so for bare wh-phrases was found.", "labels": [], "entities": []}, {"text": "Recall that, in, it was found that similarity scores calculated on these experimental items using word embeddings do not correlate with experimental results.", "labels": [], "entities": []}, {"text": "This lead to the conclusion that word embeddings do not encode relevant information related to the important notion of intervener.", "labels": [], "entities": []}, {"text": "In the next two sections, we present our extensions to these results.", "labels": [], "entities": []}, {"text": "The basic intuition is that, for example, a second-order similarity is a similarity matrix of similarities.", "labels": [], "entities": []}, {"text": "Instead of changing the similarity matrix, the word embeddings themselves are transformed, calculating first, second, nth-order similarities directly.", "labels": [], "entities": []}, {"text": "These similarities are based on the tuning of a single parameter \u03b1, the power of the matrix, to increase or decrease the similarity order.", "labels": [], "entities": []}, {"text": "For example \u03b1= 0 is first-order similarity, the similarity of two given words, \u03b1= 0.5 is second-order similarity, the similarity of the context of two given words.", "labels": [], "entities": []}, {"text": "Values of \u03b1 can vary both positively and negatively.", "labels": [], "entities": []}, {"text": "Intuitively, negative values are similarities of two given words as first, second, n-order contexts of other words.", "labels": [], "entities": []}, {"text": "Artexte and colleagues argue that different-order similarities are related to different levels of linguistic representations, and certain values of the parameter move the vectors in a space where similarities are more syntactic (as in sing, singing), while other values of the parameter move the vectors in a space that is more semantic (as in sing, chant).", "labels": [], "entities": []}, {"text": "They also distinguish a notion of similarity as analogy, such as the one exhibited by words like car and automobile, and relatedness, such as in car and road.", "labels": [], "entities": []}, {"text": "Specifically, they claim that their results show that the notion of similarity represented in vectorial space can be decomposed into a more 'syntactic' notion of similarity and the notion of 'relatedness' of a more semantic flavour.", "labels": [], "entities": []}, {"text": "They confirm these claims by better performance of the transformed vectors in different tasks of analogy and relatedness that tap into different notions of similarity.", "labels": [], "entities": []}, {"text": "We apply this eigendecomposition technique to our data, serching for the appropriate values of \u03b1, to see if moving the vectors in a region of the space that corresponds better to syntactic similarity yields better correlations between word embeddings similarity scores and experimental results than found in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percent accuracy predictions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8247241973876953}]}, {"text": " Table 3: Results for object relatives.", "labels": [], "entities": [{"text": "object relatives", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7145242094993591}]}]}