{"title": [], "abstractContent": [{"text": "Recently, several methods have been proposed to explain the predictions of recurrent neu-ral networks (RNNs), in particular of LSTMs.", "labels": [], "entities": []}, {"text": "The goal of these methods is to understand the network's decisions by assigning to each input variable, e.g., a word, a relevance indicating to which extent it contributed to a particular prediction.", "labels": [], "entities": []}, {"text": "In previous works, some of these methods were not yet compared to one another, or were evaluated only qualitatively.", "labels": [], "entities": []}, {"text": "We close this gap by systematically and quantitatively comparing these methods in different settings, namely (1) a toy arithmetic task which we use as a sanity check, (2) a five-class sentiment prediction of movie reviews, and besides (3) we explore the usefulness of word relevances to build sentence-level representations.", "labels": [], "entities": [{"text": "five-class sentiment prediction of movie reviews", "start_pos": 173, "end_pos": 221, "type": "TASK", "confidence": 0.7887013653914133}]}, {"text": "Lastly, using the method that performed best in our experiments, we show how specific linguistic phenomena such as the negation in sentiment analysis reflect in terms of relevance patterns, and how the relevance visualization can help to understand the misclassification of individual samples.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 131, "end_pos": 149, "type": "TASK", "confidence": 0.8504112362861633}]}], "introductionContent": [{"text": "Recurrent neural networks such as LSTMs) area standard building block for understanding and generating text data in NLP.", "labels": [], "entities": []}, {"text": "They find usage in pure NLP applications, such as abstractive summarization (, machine translation (, textual entailment (; as well as in multimodal tasks involving NLP, such as image captioning (, visual question answering ( or lipreading (.", "labels": [], "entities": [{"text": "abstractive summarization", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7101009488105774}, {"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7402380853891373}, {"text": "image captioning", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.725175529718399}, {"text": "question answering", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.6893976777791977}]}, {"text": "As these models become more and more widespread due to their predictive performance, there is also a need to understand why they took a particular decision, i.e., when the input is a sequence of words: which words are determinant for the final decision?", "labels": [], "entities": []}, {"text": "This information is crucial to unmask \"Clever Hans\" predictors (, and to allow for transparency of the decision-making process.", "labels": [], "entities": [{"text": "unmask \"Clever Hans\" predictors", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.5225055664777756}]}, {"text": "Early works on explaining neural network predictions include;;;;;, with several works focusing on explaining the decisions of convolutional neural networks (CNNs) for image recognition.", "labels": [], "entities": [{"text": "image recognition", "start_pos": 167, "end_pos": 184, "type": "TASK", "confidence": 0.7679151296615601}]}, {"text": "More recently, this topic found a growing interest within NLP, amongst others to explain the decisions of general CNN classifiers (, and more particularly to explain the predictions of recurrent neural networks (.", "labels": [], "entities": []}, {"text": "In this work, we focus on RNN explanation methods that are solely based on a trained neural network model and a single test data point . Thus, methods that use additional information, such as training data statistics, sampling, or are optimization-based () are out of our scope.", "labels": [], "entities": [{"text": "RNN explanation", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.9404390454292297}]}, {"text": "Among the methods we consider, we note that the method of was not yet compared against;; and that the method of was validated only visually.", "labels": [], "entities": []}, {"text": "Moreover, to the best of our knowledge, no recurrent neural network explanation method was tested so far on a toy problem where the ground truth rele-vance value is known.", "labels": [], "entities": [{"text": "recurrent neural network explanation", "start_pos": 43, "end_pos": 79, "type": "TASK", "confidence": 0.6269923225045204}]}, {"text": "Therefore our contributions are the following: we evaluate and compare the aforementioned methods, using two different experimental setups, thereby we assess basic properties and differences between the explanation methods.", "labels": [], "entities": []}, {"text": "Along-the-way we purposely adapted a simple toy task, to serve as a testbed for recurrent neural networks explanations.", "labels": [], "entities": [{"text": "recurrent neural networks explanations", "start_pos": 80, "end_pos": 118, "type": "TASK", "confidence": 0.6068209111690521}]}, {"text": "Lastly, we explore how word relevances can be used to build sentence-level representations, and demonstrate how the relevance visualization can help to understand the (mis-)classification of selected samples w.r.t. semantic composition.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}