{"title": [{"text": "Incorporating Figure Captions and Descriptive Text in MeSH Term Indexing", "labels": [], "entities": [{"text": "Incorporating Figure Captions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7801342010498047}, {"text": "MeSH Term Indexing", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8044789632161459}]}], "abstractContent": [{"text": "The goal of text classification is to automatically assign categories to documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7445298433303833}]}, {"text": "Deep learning automatically learns effective features from data instead of adopting human-designed features.", "labels": [], "entities": []}, {"text": "In this paper, we focus specifically on biomedical document classification using a deep learning approach.", "labels": [], "entities": [{"text": "biomedical document classification", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7501403292020162}]}, {"text": "We present a novel multichannel TextCNN model for MeSH term indexing.", "labels": [], "entities": [{"text": "MeSH term indexing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6645648082097372}]}, {"text": "Beyond the normal use of the text from the abstract and title for model training, we also consider figure and table captions, as well as paragraphs associated with the figures and tables.", "labels": [], "entities": []}, {"text": "We demonstrate that these latter text sources are important feature sources for our method.", "labels": [], "entities": []}, {"text": "A new dataset consisting of these text segments curated from 257,590 full text articles together with the articles' MED-LINE/PubMed MeSH terms is publicly available .", "labels": [], "entities": [{"text": "MED-LINE/PubMed MeSH terms", "start_pos": 116, "end_pos": 142, "type": "DATASET", "confidence": 0.7355464577674866}]}], "introductionContent": [{"text": "Text classification is a process that assigns labels or tags to text according to its contents.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8071013391017914}]}, {"text": "It can be done manually or automatically.", "labels": [], "entities": []}, {"text": "Most text classification tasks were done by human annotators prior to the information age.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.8687012990315756}]}, {"text": "A human annotator reads and interprets the content of the text and then classifies it into certain categories.", "labels": [], "entities": []}, {"text": "Traditional text classification is time consuming and expensive, especially when dealing with a large number of documents.", "labels": [], "entities": [{"text": "text classification", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7526015639305115}]}, {"text": "Currently, there is a trend to support text classification through automatic tools as it does the same job as human annotators, but accomplishes it in more accurate and efficient ways.", "labels": [], "entities": [{"text": "text classification", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.8583530783653259}]}, {"text": "Automatic text classification is an important application and research topic in natural language processing because of the exponentially increasing number of online documents.", "labels": [], "entities": [{"text": "Automatic text classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7304845452308655}, {"text": "natural language processing", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6683420538902283}]}, {"text": "It saves time and money in general, leading to its continued and enthusiastic usage in both business and research.", "labels": [], "entities": []}, {"text": "MEDLINE 1 and PubMed 2 are databases that can access publications of life sciences and biomedical topics.", "labels": [], "entities": [{"text": "MEDLINE 1", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9299517273902893}, {"text": "PubMed", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.8939383029937744}]}, {"text": "They are maintained by the United States National Library of Medicine (NLM).", "labels": [], "entities": [{"text": "National Library of Medicine (NLM)", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.7197172130857196}]}, {"text": "The MEDLINE database includes bibliographic information for articles in various disciplines of life sciences and biomedicine, such as medicine, healthcare, biology, biochemistry and molecular evolution.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9199751615524292}]}, {"text": "The database contains more than 25 million records in over 5,200 worldwide journals.", "labels": [], "entities": []}, {"text": "More than 800,000 citations were added to MED-LINE in 2017, which is more than 2,000 updates daily . PubMed has a web server that can freely access the MEDLINE database of references and abstracts.", "labels": [], "entities": [{"text": "MED-LINE", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.8310956358909607}, {"text": "MEDLINE database", "start_pos": 152, "end_pos": 168, "type": "DATASET", "confidence": 0.8391388356685638}]}, {"text": "Some PubMed records have full text articles available on PubMed Central . Journal articles in MEDLINE are indexed according to Medical subject headings (MeSH) , which are the NLM's controlled vocabulary thesaurus.", "labels": [], "entities": [{"text": "PubMed Central", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9429703056812286}, {"text": "MEDLINE", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9141958355903625}]}, {"text": "MeSH is a hierarchically-organized terminology indexing system that categorizes biomedical documents in the NLM databases.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8988425135612488}]}, {"text": "The 2018 version of MeSH contains 28,939 headings . Among these MeSH terms, there are 29 check tags which area special group of MeSH terms describing subjects of research (human or animal; mice or rats, etc.).", "labels": [], "entities": []}, {"text": "MeSH terms are distinctive features of MEDLINE, which are great tools for indexers and searchers.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.79627525806427}, {"text": "MEDLINE", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.8100330233573914}]}, {"text": "Indexers from NLM use MeSH terms to classify documents based on the contents of journal articles in the MEDLINE database.", "labels": [], "entities": [{"text": "MEDLINE database", "start_pos": 104, "end_pos": 120, "type": "DATASET", "confidence": 0.9429451823234558}]}, {"text": "Searchers and researchers use MeSH terms to assist subject searching in MEDLINE, PubMed and other databases.", "labels": [], "entities": [{"text": "subject searching", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.6716704219579697}, {"text": "MEDLINE", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.839837372303009}]}, {"text": "Currently MeSH term indexing is performed by a large number of human annotators, who review full text documents and assign suitable MeSH terms to each article.", "labels": [], "entities": [{"text": "MeSH term indexing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.659967044989268}]}, {"text": "Human annotation is time consuming and costly.", "labels": [], "entities": []}, {"text": "Research shows that the average cost of annotation per document is around $9.40 , which translates into a huge cost for indexing a large number of documents.", "labels": [], "entities": []}, {"text": "Meanwhile, there is a large number of documents uploaded to MEDLINE and PubMed databases everyday (approximately 2,000-4,000 on a daily basis) 2 . It is challenging to annotate all new incoming documents in a relatively short time.", "labels": [], "entities": [{"text": "MEDLINE and PubMed databases", "start_pos": 60, "end_pos": 88, "type": "DATASET", "confidence": 0.7846388965845108}]}, {"text": "Therefore, a computational system that can assist the indexing of a large number of biomedical articles is highly desired.", "labels": [], "entities": [{"text": "indexing of a large number of biomedical articles", "start_pos": 54, "end_pos": 103, "type": "TASK", "confidence": 0.749146930873394}]}, {"text": "In this paper, we focus on the task of automatic MeSH indexing.", "labels": [], "entities": [{"text": "MeSH indexing", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.7439079284667969}]}, {"text": "We propose a novel deep learning based discriminative method, multichannel TextCNN, which uses convolutional neural network based feature selection to extract important information from the article to be indexed.", "labels": [], "entities": []}, {"text": "In addition to extracting information from the title and abstract of the article, our innovation integrates figure and table captions, as well as relevant paragraphs into the indexing process.", "labels": [], "entities": []}, {"text": "We summarize the most major contributions as follows: \u2022 We explore the use of multichannel deep learning architectures for the automatic MeSH indexing task.", "labels": [], "entities": [{"text": "MeSH indexing task", "start_pos": 137, "end_pos": 155, "type": "TASK", "confidence": 0.7917500336964926}]}, {"text": "\u2022 Experimental results show that incorporating figure and table information improves the performance of automatic MeSH indexing.", "labels": [], "entities": [{"text": "MeSH indexing", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.6897577494382858}]}, {"text": "\u2022 We make available a labeled full text biomedical document dataset (including title, abstract, figure and table captions, as well as paragraphs related to the figures and tables) to the research community.", "labels": [], "entities": []}], "datasetContent": [{"text": "Most existing approaches in automatic MeSH indexing are performed on datasets with abstracts and titles only.", "labels": [], "entities": [{"text": "MeSH indexing", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.8643284738063812}]}, {"text": "In this paper, we created a full text dataset which is composed of table and figure captions as well as associated paragraphs, as we believe figures and tables might provide important MeSH features for classification.", "labels": [], "entities": []}, {"text": "The two datasets that were used to build our four datasets are described below: \u2022 2015 Subject Extraction Test Collection (SETC2015): SETC2015 contains 14,828 PMC full text articles used by Demner-.", "labels": [], "entities": [{"text": "2015 Subject Extraction Test Collection (SETC2015)", "start_pos": 82, "end_pos": 132, "type": "DATASET", "confidence": 0.7263268977403641}, {"text": "SETC2015", "start_pos": 134, "end_pos": 142, "type": "DATASET", "confidence": 0.8229116797447205}]}, {"text": "We used this dataset to create the following two Small (S) datasets: -AT (S): labelled documents from SETC2015 which contain abstract and title only -Full (S): labelled documents from SETC2015 which contain abstract, title, figure and provides statistical information for the described datasets.", "labels": [], "entities": [{"text": "AT", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9791854023933411}, {"text": "SETC2015", "start_pos": 102, "end_pos": 110, "type": "DATASET", "confidence": 0.9814621210098267}, {"text": "SETC2015", "start_pos": 184, "end_pos": 192, "type": "DATASET", "confidence": 0.9802418947219849}]}, {"text": "Our labeled datasets are using 28,939 MeSH terms in total.", "labels": [], "entities": []}, {"text": "To assist in our understanding of the hierarchical evaluation, we explored the MeSH hierarchical structure and split them into 5 levels to see how many MeSH terms exist at each level (it should be noted that there is some overlap of MeSH terms between levels).", "labels": [], "entities": []}, {"text": "The number of MeSH terms in the first, the second, the third, the fourth and the fifth level, are, respectively.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.5000758767127991}]}, {"text": "There is generally no accepted standard for the evaluation of multi-label classifications.", "labels": [], "entities": []}, {"text": "Evaluation metrics adopted from multi-class classification and binary classification are used to measure multi-label classification in an effective way.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.7476661205291748}, {"text": "multi-label classification", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.7725798785686493}]}, {"text": "In automatic MeSH indexing, even if the label space is very large, only relatively few MeSH terms match each document.", "labels": [], "entities": [{"text": "MeSH indexing", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.8306022584438324}]}, {"text": "To evaluate the performance of our proposed model, we present three groups of measurements suggested by and, namely bipartition-based, rankingbased and hierarchy-based evaluation.", "labels": [], "entities": []}, {"text": "To set the stage to discuss the three metrics, we define a test set of N document-label pairs taken from the dataset, where xi is the document text and y i \u2208 {0, 1} L . The vector y i denotes the set of true labels (i.e., MeSH terms) for each document i (0 meaning the label is not in the set, 1 meaning it is in the set), N denotes the number of test examples, and L is the total number of labels.", "labels": [], "entities": []}, {"text": "Given a document xi , the set of labels predicted by the classifiers is denoted as , wher\u00ea y i \u2208 {0, 1} L , and the ranking indexes of predicted labels among the top k is denoted as r k (\u02c6 y), wher\u00ea . Bipartition evaluation is further divided into example-based and label-based metrics.", "labels": [], "entities": [{"text": "Bipartition evaluation", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.8669589459896088}]}, {"text": "Examplebased measurements calculate precision, recall, and F-score over (in our evaluation) the top 5, top 10, and top 15 ranked labels overall of the documents of the test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9996647834777832}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9995212554931641}, {"text": "F-score", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9995343685150146}]}, {"text": "The measurements include example-based precision (EBP), examplebased recall (EBR) and example-based F-score (EBF).", "labels": [], "entities": [{"text": "precision (EBP)", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.8948114663362503}, {"text": "recall (EBR)", "start_pos": 69, "end_pos": 81, "type": "METRIC", "confidence": 0.9087400585412979}, {"text": "F-score (EBF)", "start_pos": 100, "end_pos": 113, "type": "METRIC", "confidence": 0.9293186366558075}]}, {"text": "The metrics are defined as: Label-based evaluation is calculated for each label in the label set.", "labels": [], "entities": []}, {"text": "The measurements include macroand micro-average precision, macroand micro-average recall (MaR, MiR),and macroand micro-average F-score (MaF, MiF).", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.7752724289894104}, {"text": "recall (MaR, MiR)", "start_pos": 82, "end_pos": 99, "type": "METRIC", "confidence": 0.8795388340950012}, {"text": "macroand micro-average F-score (MaF, MiF)", "start_pos": 104, "end_pos": 145, "type": "METRIC", "confidence": 0.7186590731143951}]}, {"text": "The metrics are defined as: MiR + MiP where TP j , FP j and FN j as true positives, false positives, and false negatives respectively for each label l j in the set of total labels L.", "labels": [], "entities": []}, {"text": "Ranking-based evaluation, including precision at k (p@k), and normalized discounted cumulative gain (nDCG), ranks the predicted labels and aims to rank the relevant labels higher than the irrelevant ones.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9992917776107788}, {"text": "normalized discounted cumulative gain (nDCG)", "start_pos": 62, "end_pos": 106, "type": "METRIC", "confidence": 0.8267601345266614}]}, {"text": "The metrics are defined as follows: DCG@k IDCG Hierarchy-based evaluation, including hierarchical precision (HP) and hierarchical recall (HR), is used to measure a hierarchical classification that classifies elements into a hierarchy of classes.", "labels": [], "entities": [{"text": "hierarchical precision (HP)", "start_pos": 85, "end_pos": 112, "type": "METRIC", "confidence": 0.7957279086112976}, {"text": "recall (HR)", "start_pos": 130, "end_pos": 141, "type": "METRIC", "confidence": 0.9462433457374573}]}, {"text": "It measures performance based on the gold standard labels and the predicted labels augmented with their ancestors and descendants within distances 1 and 2.", "labels": [], "entities": [{"text": "gold standard labels", "start_pos": 37, "end_pos": 57, "type": "DATASET", "confidence": 0.8173830509185791}]}, {"text": "The augmented gold standard labels Y aug and predicted labels\u02c6Ylabels\u02c6 labels\u02c6Y aug are used in the hierarchical evaluation.", "labels": [], "entities": []}, {"text": "HP and HR are defined as follows: In ranking-based evaluation, for p@k, k \u2208 {1, 3, 5, 10, 15}, and k \u2208 {1, 3, 5} for nDCG@k.", "labels": [], "entities": []}, {"text": "In example-based, label-based, and hierarchybased evaluations, the calculation is done with the top 5, 10, and 15 predicted labels.", "labels": [], "entities": []}, {"text": "In hierarchical evaluation, we used distances 1 and 2 for HP and HR.", "labels": [], "entities": [{"text": "HR", "start_pos": 65, "end_pos": 67, "type": "DATASET", "confidence": 0.7575197815895081}]}, {"text": "The example-based, ranking-based, and hierarchical evaluation metrics are calculated for each document.", "labels": [], "entities": []}, {"text": "An average score overall documents in the test set is returned.", "labels": [], "entities": []}, {"text": "Likewise, the label-based evaluation is calculated for each label and averaged overall labels in the test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of Datasets: D is the total number  of documents (90% training, 10% testing); F repre- sents the number of unique tokens contained in all of  the documents; L is the number of class labels; \u00af  L is  the average number of labels per document; \u02dc  L is the  average number of documents per label", "labels": [], "entities": [{"text": "F repre- sents", "start_pos": 99, "end_pos": 113, "type": "METRIC", "confidence": 0.8938462436199188}]}, {"text": " Table 2: Results for TextCNN in p@k and nDCG@k. Boldface indicates the best result on the each dataset.", "labels": [], "entities": [{"text": "TextCNN", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.915917158126831}]}, {"text": " Table 3: Flat and Hierarchical Measures for TextCNN on Different Datasets. top k indicates", "labels": [], "entities": []}, {"text": " Table 4: Hierarchical Analysis on TextCNN -top k selected indicates the top k labels return by the classifier", "labels": [], "entities": [{"text": "Hierarchical Analysis", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.7950894236564636}]}]}