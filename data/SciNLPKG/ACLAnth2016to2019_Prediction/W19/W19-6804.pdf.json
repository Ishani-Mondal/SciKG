{"title": [{"text": "A Continuous Improvement Framework of Machine Translation for Shipibo-Konibo", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.717679038643837}, {"text": "Shipibo-Konibo", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.42076539993286133}]}], "abstractContent": [{"text": "Shipibo-Konibo is a low-resource language from Peru with prior results in statistical machine translation; however, it is challenging to enhance them mainly due to the expensiveness of building more parallel corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.6524714231491089}]}, {"text": "Thus, we aim fora continuous improvement framework of the Spanish-Shipibo-Konibo language-pair by taking advantage of more advanced strategies and crowd-sourcing.", "labels": [], "entities": []}, {"text": "Besides the introduction of anew domain for translation based on language learning flash-cards, our main contributions are the extension of the machine translation experiments for Shipibo-Konibo to neural ar-chitectures with transfer and active learning ; and the building of a conversational agent prototype to retrieve new translations through asocial media platform.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.706154927611351}]}], "introductionContent": [{"text": "The focus on low-resource Machine Translation (MT) has driven further work with different machine learning settings to take advantage of Neural MT (NMT) methods, where the amount of training data is relevant to obtain quality results (.", "labels": [], "entities": [{"text": "low-resource Machine Translation (MT)", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8258305589358012}]}, {"text": "For instance, with a Transfer Learning approach, we can learn specific components in a system from a resource-rich domain (e.g. a language-pair) and transfer the updated parameters to the real target (, usually in a resource-poor domain.", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.9425175487995148}]}, {"text": "Regarding the size of available corpora, with Active Learning meth- ods, we can rank new samples to label (e.g. sentences to translate) to improve a learning system efficiently ().", "labels": [], "entities": []}, {"text": "Besides, crowdsourcing strategies and platforms, such as Amazon Mechanical Turk, have gained attention in translation studies and MT to retrieve less expensive corpora.", "labels": [], "entities": [{"text": "translation", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.9711375832557678}, {"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9868603348731995}]}, {"text": "Given the background, Peru offers a diversityrich language context for MT research with more than 40 native languages) that are typologically different from Castilian Spanish (spa), the primary official language in the country.", "labels": [], "entities": [{"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.9954858422279358}]}, {"text": "Specifically, Shipibo-Konibo (shp) is an Amazonian language that has been addressed in Natural Language Processing (NLP) recently, including a statistical MT (SMT) study with religious and educational domain corpora (.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 155, "end_pos": 163, "type": "TASK", "confidence": 0.842967078089714}]}, {"text": "However, the language is far from being considered a rich-resource one with less than 20,000 sentences for the spa-shp language-pair.", "labels": [], "entities": []}, {"text": "Thus, it is crucial to look for different approaches that could deliver better MT systems, and also, new parallel corpus.", "labels": [], "entities": [{"text": "MT", "start_pos": 79, "end_pos": 81, "type": "TASK", "confidence": 0.9834133386611938}]}, {"text": "Therefore, this study extends previous MT studies of Shipibo-Konibo by introducing anew domain for translation based on flashcards for language learning (see \u00a74), experimenting with transfer and active learning strategies in neural architectures (see \u00a75), and proposing a conversational agent prototype in social media to retrieve new translations from native speakers (see \u00a76).", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9857136011123657}]}, {"text": "Our main goal is to mount an initial framework able to continuously improve MT for Peruvian languages, with the potential to include further NMT features.", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.9914097189903259}]}, {"text": "To complement the article, \u00a72 presents previous work on MT for Peruvian languages, \u00a73 introduces more details about the target language, and finally, \u00a77 concludes and proposes further steps.: Details of the parallel corpora for spa-shp per domain and in total: S = number of sentences; rshp-spa = average of the ratioshp-spa per sentence; T = number of tokens; |V| = vocabulary size; HLT = tokens with frequency equals to one.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9888046383857727}, {"text": "HLT", "start_pos": 384, "end_pos": 387, "type": "METRIC", "confidence": 0.9756779670715332}]}], "datasetContent": [{"text": "A previous study of spa-shp introduced two corpora: religious and educational (.", "labels": [], "entities": []}, {"text": "The former is a compilation with postprocessing steps of the Bible entries, whereas the latter contains translated sentences of bilingual educational texts from the Peruvian Government 2 . Besides those domains, we introduce anew parallel corpus that was built from a sample of sentences of the Tatoeba project, specifically, the Tabdelimited Bilingual Sentence Pairs in EnglishSpanish . A few thousands of short sentences were translated from Spanish into Shipibo-Konibo fora certified bilingual translator.", "labels": [], "entities": []}, {"text": "We named the new corpus Flashcards, as it is based on flashcards with bilingual sentences to easier memorisation in a language learning context . describes the corpus per domain and overall, including information about the number of translated sentences, an average of the ratio of tokens per sentence between the Shipibo-Konibo and Spanish translations (, the total number of tokens, the vocabulary size, and the amount of hapax legomenon tokens (HLT) or terms with frequency equals to one.", "labels": [], "entities": []}, {"text": "We observe that the Flashcards domain is proportionally bigger in vocabulary size and HLT regarding the other two, even when the amount of tokens per sentence in average is lower (T /S).", "labels": [], "entities": [{"text": "HLT", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.998432457447052}]}, {"text": "Moreover, the averaged ratio of tokens (r shp-spa ) has a particular value, as it is the only domain with more tokens per parallel sentence in the Shipibo-Konibo side than in the Spanish one.", "labels": [], "entities": []}, {"text": "The following example illustrates a related case: shp: Westiora kafe keniresa ea ike.", "labels": [], "entities": []}, {"text": "spa: S\u00f3lo quer\u00eda un caf\u00e9.", "labels": [], "entities": []}, {"text": "eng: I just wanted a coffee.", "labels": [], "entities": []}, {"text": "where there is a null subject in Spanish (ea or I), and Shipibo-Konibo merges s\u00f3lo quer\u00eda (just wanted) into keniresa and adds ike as an auxiliar.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of the parallel corpora for spa-shp per domain and in total: S = number of sentences; rshp-spa = average of the  ratioshp-spa per sentence; T = number of tokens; |V| = vocabulary size; HLT = tokens with frequency equals to one.", "labels": [], "entities": [{"text": "HLT", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9489507675170898}]}, {"text": " Table 2: BLEU scores with the NMT baseline settings at  word-(w) and subword-level using joint BPE with 5,000 (5k)  and 15,000 (15k) merge operations for the latter.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9967233538627625}, {"text": "NMT baseline", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.7934583127498627}, {"text": "BPE", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9121326208114624}]}, {"text": " Table 3: Transfer learning experiments using spa-L as a par- ent language-pair. S indicates the size of the corpus, BLEU  the score of translation in the child language-pair spa-shp,  and D is the Hamming distance between L and shp.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9993541836738586}]}, {"text": " Table 4: BLEU scores for the 40% incremental step over the  initial 50% in the Active Learning experimental setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998259961605072}]}]}