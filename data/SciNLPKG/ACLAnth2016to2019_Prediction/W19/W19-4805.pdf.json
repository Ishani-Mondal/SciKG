{"title": [{"text": "Multi-Granular Text Encoding for Self-Explaining Categorization", "labels": [], "entities": [{"text": "Multi-Granular Text Encoding", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.576041042804718}]}], "abstractContent": [{"text": "Self-explaining text categorization requires a classifier to make a prediction along with supporting evidence.", "labels": [], "entities": [{"text": "Self-explaining text categorization", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.616003413995107}]}, {"text": "A popular type of evidence is sub-sequences extracted from the input text which are sufficient for the classifier to make the prediction.", "labels": [], "entities": []}, {"text": "In this work, we define multi-granular ngrams as basic units for explanation, and organize all ngrams into a hierarchical structure, so that shorter ngrams can be reused while computing longer ngrams.", "labels": [], "entities": []}, {"text": "We leverage a tree-structured LSTM to learn a context-independent representation for each unit via parameter sharing.", "labels": [], "entities": []}, {"text": "Experiments on medical disease classification show that our model is more accurate, efficient and compact than BiL-STM and CNN baselines.", "labels": [], "entities": [{"text": "medical disease classification", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6727126638094584}]}, {"text": "More importantly, our model can extract intuitive multi-granular evidence to support its predictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Increasingly complex neural networks have achieved highly competitive results for many NLP tasks (), but they prevent human experts from understanding how and why a prediction is made.", "labels": [], "entities": []}, {"text": "Understanding how a prediction is made can be very important for certain domains, such as the medical domain.", "labels": [], "entities": []}, {"text": "Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions (.", "labels": [], "entities": []}, {"text": "For example, in order to make diagnoses based on the medical report in, the highlighted symptoms maybe extracted as evidence.", "labels": [], "entities": []}, {"text": "Two methods have been proposed on how to jointly provide highlights along with classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.9626922011375427}]}, {"text": "(1) an extraction-based method (, which first extracts evidences from the original text and then makes a prediction solely based on the extracted evidences; (2) an attentionbased method (, which leverages the self-attention mechaMedical Report: The patient was admitted to the Neurological Intensive Care Unit for close observation.", "labels": [], "entities": []}, {"text": "She was begun on heparin anticoagulated carefully secondary to the petechial bleed . She started weaning from the vent the next day.", "labels": [], "entities": []}, {"text": "She was started on Digoxin to control her rate and her Cardizem was held.", "labels": [], "entities": [{"text": "Digoxin", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.9362786412239075}]}, {"text": "She was started on antibiotics for possible aspiration pneumonia . Her chest xray showed retrocardiac effusion . She had some bleeding after nasogastric tube insertion . Diagnoses: Cerebral artery occlusion; Unspecified essential hypertension; Atrial fibrillation; Diabetes mellitus.", "labels": [], "entities": []}, {"text": "nism to show the importance of basic units (words or ngrams) through their attention weights.", "labels": [], "entities": []}, {"text": "However, previous work has several limitations., for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases.", "labels": [], "entities": []}, {"text": "For instance, useful symptoms in, such as \"bleeding after nasogastric tube insertion\", are larger than a single word.", "labels": [], "entities": [{"text": "nasogastric tube insertion", "start_pos": 58, "end_pos": 84, "type": "TASK", "confidence": 0.6495875318845113}]}, {"text": "Another issue of is that their attention model is applied on the representation vectors produced by an LSTM.", "labels": [], "entities": []}, {"text": "Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear.", "labels": [], "entities": []}, {"text": "defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting.", "labels": [], "entities": []}, {"text": "Thus the explainability of the model is limited.", "labels": [], "entities": []}, {"text": "introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases.", "labels": [], "entities": []}, {"text": "However, their method cannot tell how much a selected unit contributes to the model's decision through a weight value.", "labels": [], "entities": []}, {"text": "In this paper, we study what the meaningful units to highlight are.", "labels": [], "entities": []}, {"text": "We define multi-granular ngrams as basic units, so that all highlighted symptoms in  ngrams into a hierarchical structure, such that the shorter ngram representations can be reused to construct longer ngram representations.", "labels": [], "entities": []}, {"text": "Experiments on medical disease classification show that our model is more accurate, efficient and compact than BiLSTM and CNN baselines.", "labels": [], "entities": [{"text": "medical disease classification", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.6571292579174042}]}, {"text": "Furthermore, our model can extract intuitive multi-granular evidence to support its predictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Dataset: We experiment on a public medical text classification dataset.", "labels": [], "entities": [{"text": "medical text classification", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.6933712760607401}]}, {"text": "tokens, and one label out of five categories indicating which disease this document is about.", "labels": [], "entities": []}, {"text": "We randomly split the dataset into train/dev/test sets by 8:1:1 for each category, and end up with 11,216/1,442/1,444 instances for each set.", "labels": [], "entities": []}, {"text": "Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (, and set the hidden size as 100 for node embeddings.", "labels": [], "entities": [{"text": "840B Common Crawl corpus", "start_pos": 83, "end_pos": 107, "type": "DATASET", "confidence": 0.895484060049057}]}, {"text": "We apply dropout to every layer with a dropout ratio 0.2, and set the batch size as 50.", "labels": [], "entities": []}, {"text": "We minimize the cross-entropy of the training set with the ADAM optimizer (, and set the learning rate is to 0.001.", "labels": [], "entities": []}, {"text": "During training, the pre-trained word embeddings are not updated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Highlighted ngrams by our model, where darker colors means higher weights.", "labels": [], "entities": []}, {"text": " Table 3: Test set results.", "labels": [], "entities": []}]}