{"title": [], "abstractContent": [{"text": "Vector representations of words have seen an increasing success over the past years in a variety of NLP tasks.", "labels": [], "entities": [{"text": "Vector representations of words", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8784202337265015}]}, {"text": "While there seems to be a consensus about the usefulness of word em-beddings and how to learn them, it is still unclear which representations can capture the meaning of phrases or even whole sentences.", "labels": [], "entities": []}, {"text": "Recent work has shown that simple operations outperform more complex deep architectures.", "labels": [], "entities": []}, {"text": "In this work, we propose two novel constraints for computing noun phrase vector representations.", "labels": [], "entities": [{"text": "computing noun phrase vector representations", "start_pos": 51, "end_pos": 95, "type": "TASK", "confidence": 0.619805908203125}]}, {"text": "First, we propose that the semantic and not the syntactic contribution of each component of a noun phrase should be considered, so that the resulting composed vectors express more of the phrase meaning.", "labels": [], "entities": []}, {"text": "Second, the composition process of the two phrase vectors should apply suitable dimensions' selection in away that specific semantic features captured by the phrase's meaning become more salient.", "labels": [], "entities": []}, {"text": "Our proposed methods are compared to 11 other approaches, including popular baselines and a neural net architecture, and are evaluated across 6 tasks and 2 datasets.", "labels": [], "entities": []}, {"text": "Our results show that these constraints lead to more expressive phrase representations and can be applied to other state-of-the-art methods to improve their performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector representations of words date back to the 1990's) and have seen an increasing success over the past years (.", "labels": [], "entities": []}, {"text": "While there seems to be a consensus about the usefulness of word embeddings and how to learn them, it is still controversial how to learn representations that capture the meaning of phrases or even whole sentences (.", "labels": [], "entities": []}, {"text": "Generally, two main approaches are used to compute phrase representations: non-compositional and compositional.", "labels": [], "entities": []}, {"text": "The former treats phrases as single units and attempts to learn their representations directly from corpora, much as it is done for words).", "labels": [], "entities": []}, {"text": "These approaches ignore the components of the phrase and are not scalable to all possible phrases of a language.", "labels": [], "entities": []}, {"text": "On the other hand, the compositional approach derives a phrase or sentence representation from the embeddings of its component words in various ways, from simple addition and average operations, e.g.,;, to more complex neural net architectures, e.g.,;.", "labels": [], "entities": []}, {"text": "However, such approaches often ignore word order and other linguistic intuitions and lead to representations that cannot truly express the meaning of the sentence, as recently discussed by.", "labels": [], "entities": []}, {"text": "We concentrate on efficient phrase representations which capture meaning and can be handled as sentence components.", "labels": [], "entities": []}, {"text": "We believe that from such representations the meaning of a full sentence can be compositionally computed, much as in more traditional semantic theories, e.g. in the Fregean functional application.", "labels": [], "entities": []}, {"text": "For example, if we can compute efficient representations for all possible phrases contained in constituency parsing, say NP, VP, PP, etc., we can then derive the meaning of the whole sentence by functionally applying the constituents' representations on each other.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7500807344913483}]}, {"text": "To this end, we believe that for compositional phrases there should be compositional phrase representations, while for non-compositional ones, e.g., idioms, learning direct representations from corpora might be more effective.", "labels": [], "entities": []}, {"text": "In this paper, we focus on bigram compositional nominal phrase vectors of adjective-noun and noun-noun (compounds) combinations.", "labels": [], "entities": [{"text": "bigram compositional nominal phrase vectors", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.8099828422069549}]}, {"text": "By starting from this linguistic category, we can reliably evaluate the two constraints we propose on one of the most common con-stituent types, namely the NP phrase.", "labels": [], "entities": []}, {"text": "Specifically, in this work we propose two novel constraints for computing such phrase vectors that are linguistically informed and intuitively explainable.", "labels": [], "entities": []}, {"text": "First, we propose to focus on the semantic -and not the syntactic -contribution of each phrase component and decide whether the syntactic head or the syntactic modifier) is semantically more significant for the meaning of the phrase.", "labels": [], "entities": []}, {"text": "The phrase component with the most clear contribution to the meaning of the phrase might actually be the syntactic modifier and not the syntactic head and then this word is to be treated as the semantic head for the composition.", "labels": [], "entities": []}, {"text": "Second, we propose that for two given word embeddings that need to be composed, we should select for the composition only those dimensions of the semantic modifier embedding that are more relevant to the semantic head of the phrase.", "labels": [], "entities": []}, {"text": "In order words, we need to pick from the semantic modifier these attributes that are more relevant to the semantic head phrase.", "labels": [], "entities": []}, {"text": "For example, for the compositional phrase black magic, intuitively we want to select all dimensions of black that have to do something with magic and not others that have to do with, e.g. t-shirt.", "labels": [], "entities": []}, {"text": "In this way, we can compose the representation black magic by combining the attributes of magic with the \"magic-like\" attributes of black.", "labels": [], "entities": []}, {"text": "The contributions of this paper are three-fold: Firstly, we propose two novel constraints for composing linguistically informed and intuitively explainable noun phrase representations and show how these approaches could benefit future composition methods.", "labels": [], "entities": []}, {"text": "Secondly, we provide a thorough evaluation of our methods over 6 evaluation tasks, 2 datasets and 11 other methods.", "labels": [], "entities": []}, {"text": "Thirdly, we create an evaluation dataset of nomimal phraseunigram paraphrase pairs, which we make openly available.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we include baseline approaches of vector arithmetics, the popular matrix-vector composition approach and an own trained neural network.", "labels": [], "entities": []}, {"text": "If the injection of our first constraint into those approaches boosts their performance, the semantic contribution constraint can be considered for future composition approaches, especially those aiming at simple but linguistically informed operations.", "labels": [], "entities": []}, {"text": "On the other hand, if the composition process described in our second constraint outperforms the compared approaches, we can be confident that the dimensions' selection as proposed in the previous section is a useful intuition capturing compositionality and can be safely integrated in future composition tasks..", "labels": [], "entities": []}, {"text": "Since addition and multiplication have been shown to perform so strongly and since multiplicative models have the drawback that the presence of zeroes in either of the vectors leads to information essentially being lost, we follow and also include a fourth equation, combining the addition and multiplication operations (4).", "labels": [], "entities": []}, {"text": "For the weighting we do our own fine-tuning which is specific to the dataset we use.", "labels": [], "entities": [{"text": "weighting", "start_pos": 8, "end_pos": 17, "type": "TASK", "confidence": 0.9624999761581421}]}, {"text": "This fine-tuning also showed that for our set the distinction between weights for adjective-noun and noun-noun phrases is not beneficial, contrary to, who set the weights based on the part-of-speech.", "labels": [], "entities": []}, {"text": "After tuning, the parameters are set to \u03b1 = \u03b2 = 1.0, which in practice means that the unweighted variants perform better than their weighted counterparts.", "labels": [], "entities": []}, {"text": "We also include \"easy\" baselines involving only the syntactic head or the syntactic modifier of the phrase and check whether the proposed compositional functions are better than those variants with no composition at all.", "labels": [], "entities": []}, {"text": "(1) wei addj : rj = \u03b1mj + \u03b2hj (2) wei multj : rj = \u03b1mj \u00b7 \u03b2hj.", "labels": [], "entities": []}, {"text": "In these approaches the two constituent vectors of a phrase u and v \u2208 Rn are composed by multiplying them via two matrices A,B \u2208 Rn \u00d7 n . For and Guevara (2010), A and B are the same for every u and v and are calculated with partial least squares regression, while for the adjective-noun composition of, A is set to 0 and the weight matrix B is specifically learned for each single adjective.", "labels": [], "entities": []}, {"text": "The mathematical formulation of this approach is: r = Au + Bv.", "labels": [], "entities": []}, {"text": "Given the effectiveness of this approach (see e.g. Boleda et al.", "labels": [], "entities": []}, {"text": "The left-most column of gives a better overview of all compared methods.", "labels": [], "entities": []}, {"text": "To compare the approaches, we employed 6 evaluations tasks, aiming at testing different semantic aspects of the phrases.", "labels": [], "entities": []}, {"text": "Our goal is to see which of the 13 methods perform best in each of the tasks.", "labels": [], "entities": []}, {"text": "We include popular tasks, like synonymy detection and concept clustering (see, e.g.,), but we do not employ the human similarity judgments task.", "labels": [], "entities": [{"text": "synonymy detection", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.8680115640163422}, {"text": "concept clustering", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7276799380779266}]}, {"text": "We are not convinced that semantic similarity can be scaled in a range of 1 to 7 as we are not sure how one should decide, e.g., between a 3 and a 4.", "labels": [], "entities": []}, {"text": "Such criticisms were also discussed by.", "labels": [], "entities": []}, {"text": "Plain similarity One of the most common intrinsic evaluation tasks is the semantic similarity between an item and a target.", "labels": [], "entities": []}, {"text": "Since targets are part of our dataset, the simplest task is to calculate the cosine similarity between the composed vector of a phrase and the embedding of its target.", "labels": [], "entities": []}, {"text": "Precision This task is a modification of the analogy task of.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8940519690513611}]}, {"text": "Given a phrase vector and its neighbors in the semantic space, we check if the target word is its closest neighbor (cf.;).", "labels": [], "entities": []}, {"text": "The task is also undertaken for the next two closest neighbors of the phrase.", "labels": [], "entities": []}, {"text": "Ultimately, we measure Precision@1, Precision@2 and Precision@3, respectively, for how many items of our set had their targets as neighbors at the corresponding positions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9009904861450195}]}, {"text": "Overlapping neighbors Here we measure how many neighbors of the phrase representation are also neighbors of the target embedding.", "labels": [], "entities": []}, {"text": "Since embeddings capture the relational co-occurrences of words, it should be the case that the phrase and the target vectors share neighbors.", "labels": [], "entities": []}, {"text": "This would mean that they are closer in the semantic space than items not sharing any neighbors, even if the target word itself is not a neighbor of the phrase embedding.", "labels": [], "entities": []}, {"text": "Synonymy detection This popular task, first applied on the TOEFL examples for word embeddings, is to select out of some candidate targets, the one with the highest similarity to the given word.", "labels": [], "entities": [{"text": "Synonymy detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9100852906703949}, {"text": "TOEFL examples", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.9375656843185425}]}, {"text": "Similarly, (cf. Turney, 2012) we create a set of 7 candidate unigrams for each given phrase: its syntactic modifier, its syntactic head, its target, a synonym of its syntactic modifier and of its syntactic head 8 and two random words.", "labels": [], "entities": []}, {"text": "We compute the similarity of the phrase representation to each of those and create a ranked list of the 7 candidates.", "labels": [], "entities": []}, {"text": "Targets that are lower in the ranked list are penalized and targets that are higher up are boosted; conversely for the random words.", "labels": [], "entities": []}, {"text": "Ultimately, we obtain a score between -1 and 1, with -1 being the worst with a random word at rank 1 and the target at the last rank and 1 standing for the best case where the target is at rank 1 and the randoms last.", "labels": [], "entities": []}, {"text": "Clustering A popular task is concept categorization or clustering.", "labels": [], "entities": [{"text": "concept categorization", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.7069124728441238}]}, {"text": "Given a set of concepts, the task is to group them into categories.", "labels": [], "entities": []}, {"text": "We adjust this task to measure how many of the phrase representations are clustered together with their target embedding.", "labels": [], "entities": []}, {"text": "If the phrase vector truly expresses: Overview of all compared methods across the 6 evaluation tasks.", "labels": [], "entities": []}, {"text": "The notation +Const1 is added to the methods containing the semantic contribution constraint (Constraint One).", "labels": [], "entities": []}, {"text": "The metric given for each task is the average metric across the entire dataset.", "labels": [], "entities": []}, {"text": "Numbers in boldface mark the best performance per task.", "labels": [], "entities": []}, {"text": "Multiple numbers maybe in boldface in the same task, if there is no statistically significant difference between them.", "labels": [], "entities": []}, {"text": "meaning, the two should be clustered together.", "labels": [], "entities": []}, {"text": "We use k-means clustering with 1914 clusters (as many as the pairs of our set) and 99 iterations.", "labels": [], "entities": []}, {"text": "Positive-Negative Similarity Distribution This task is the original used by, so we only apply it on the ZZ set.", "labels": [], "entities": []}, {"text": "Here, we test if the distribution of the cosine similarities of the positive pairs is statistically different from the distribution of the similarities of the negative pairs: if it is, it means that the corresponding functions perform well because they can keep the two categories apart (see for more details).", "labels": [], "entities": []}, {"text": "As in the original experiment, the results show p-values, calculated with the Students ttest for two independent samples of different sizes: lower values characterize better models.", "labels": [], "entities": [{"text": "Students ttest", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.5750055313110352}]}], "tableCaptions": [{"text": " Table 1: Overview of all compared methods across the 6 evaluation tasks. The notation +Const1 is added to  the methods containing the semantic contribution constraint (Constraint One). The metric given for each task is  the average metric across the entire dataset. Numbers in boldface mark the best performance per task. Multiple  numbers may be in boldface in the same task, if there is no statistically significant difference between them.", "labels": [], "entities": []}]}