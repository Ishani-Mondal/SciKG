{"title": [{"text": "Evaluation of Morphological Embeddings for English and Russian Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper evaluates morphology-based em-beddings for English and Russian languages.", "labels": [], "entities": []}, {"text": "Despite the interest and introduction of several morphology-based word embedding models in the past and acclaimed performance improvements on word similarity and language modeling tasks, in our experiments, we did not observe any stable preference over two of our baseline models-SkipGram and FastText.", "labels": [], "entities": [{"text": "word similarity and language modeling tasks", "start_pos": 142, "end_pos": 185, "type": "TASK", "confidence": 0.6920861055453619}]}, {"text": "The performance exhibited by morphological embeddings is the average of the two baselines mentioned above.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the most significant shifts in the area of natural language processing is to the practical use of distributed word representations.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6485093832015991}]}, {"text": "showed that a neural model could achieve close to state-of-the-art results in Part of Speech (POS) tagging and chunking by relying almost only on word embeddings learned with a language model.", "labels": [], "entities": [{"text": "Part of Speech (POS) tagging and chunking", "start_pos": 78, "end_pos": 119, "type": "TASK", "confidence": 0.6815961731804742}]}, {"text": "In modern language processing architectures, high quality pre-trained representations of words are one of the major factors of the resulting model performance.", "labels": [], "entities": []}, {"text": "Although word embeddings became ubiquitous, there is no single benchmark on evaluating their quality, and popular intrinsic evaluation techniques are subject to criticism . Researchers very often rely on intrinsic evaluation, such as semantic similarity or analogy tasks.", "labels": [], "entities": []}, {"text": "While intrinsic evaluations are simple to understand and conduct, they do not necessarily imply the quality of embeddings for all possible tasks ( . In this paper, we turn to the evaluation of morphological embeddings for English and Russian languages.", "labels": [], "entities": []}, {"text": "Over the last decade, many approaches tried to include subword information into word representations.", "labels": [], "entities": []}, {"text": "Such approaches involve additional techniques that perform segmentation of a word into morphemes).", "labels": [], "entities": []}, {"text": "The presumption is that we can potentially increase the quality of distributional representations if we incorporate these segmentations into the language model (LM).", "labels": [], "entities": []}, {"text": "Several approaches that include morphology into word embeddings were proposed, but the evaluation often does not compare proposed embedding methodologies with the most popular embedding vectors -Word2Vec, FastText, Glove.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 195, "end_pos": 203, "type": "DATASET", "confidence": 0.9496408700942993}]}, {"text": "In this paper, we aim at answering the question of whether morphology-based embeddings can be useful, especially for languages with rich morphology (such as Russian).", "labels": [], "entities": []}, {"text": "Our contribution is the following: 1.", "labels": [], "entities": []}, {"text": "We evaluate simple SkipGram-based (SGbased) morphological embedding models with new intrinsic evaluation BATS dataset (  2.", "labels": [], "entities": [{"text": "SkipGram-based", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9311624765396118}, {"text": "BATS dataset", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.7581654787063599}]}, {"text": "We compare relative gain of using morphological embeddings against Word2Vec and FastText for English and Russian languages 3.", "labels": [], "entities": [{"text": "Word2Vec", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.974919855594635}]}, {"text": "We test morphological embeddings on several downstream tasks other than language modeling, i.e., mapping embedding spaces, POS tagging, and chunking The rest of the paper is organized as follows.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7262902110815048}, {"text": "POS tagging", "start_pos": 123, "end_pos": 134, "type": "TASK", "confidence": 0.8295621573925018}]}, {"text": "Section 2 contains an overview of existing approaches for morphological embeddings and methods of their evaluation.", "labels": [], "entities": []}, {"text": "Section 3 explains embedding models that we have tested.", "labels": [], "entities": []}, {"text": "Section 4 explains our evaluation approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "To understand the effect of using morphemes for training word embeddings, we performed intrinsic and extrinsic evaluations of SG, FastText, and MorphGram model for two languages -English and Russian.", "labels": [], "entities": []}, {"text": "Russian language, in contrast to English, is characterized by rich morphology, which makes this pair of languages a good choice for exploring the difference in the effect of morphologybased models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation between human judgments and  model scores for similarity datasets, Spearman's \u03c1.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 89, "end_pos": 101, "type": "METRIC", "confidence": 0.5743204553922018}]}, {"text": " Table 2: Accuracy of models on different analogies  tasks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.993523359298706}]}, {"text": " Table 3: Accuracy of supervised mapping from Rus- sian to English using different models, searching  among first and ten nearest neighbors.", "labels": [], "entities": []}, {"text": " Table 4: Accuracy on POS task", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975520968437195}]}, {"text": " Table 5: Accuracy on Chunk task", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9957711100578308}]}]}