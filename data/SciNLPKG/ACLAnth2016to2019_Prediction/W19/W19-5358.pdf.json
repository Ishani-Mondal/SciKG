{"title": [{"text": "YiSi -A unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources", "labels": [], "entities": [{"text": "MT quality evaluation", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.8906611800193787}]}], "abstractContent": [{"text": "We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources.", "labels": [], "entities": [{"text": "semantic machine translation quality evaluation", "start_pos": 37, "end_pos": 84, "type": "TASK", "confidence": 0.7440027534961701}]}, {"text": "Underneath the interface with different language resources settings , YiSi uses the same representation for the two sentences in assessment.", "labels": [], "entities": []}, {"text": "Besides, we show significant improvement in the correlation of YiSi-1's scores with human judgment is made by using contextual embeddings in multilingual BERT-Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity.", "labels": [], "entities": []}, {"text": "YiSi is open source and publicly available.", "labels": [], "entities": [{"text": "YiSi", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9112478494644165}]}], "introductionContent": [{"text": "A good automatic MT quality metric is one that closely reflect the usefulness of the translation, in terms of assisting human readers to understand the meaning of the input sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9824586510658264}]}, {"text": "BLEU () has long been shown not to correlate well with human judgment on translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9828706979751587}, {"text": "translation quality", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.8517109155654907}]}, {"text": "However, it is still the most commonly used metric for reporting quality of machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7380155026912689}]}, {"text": "One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9414076805114746}]}, {"text": "Semantic MT evaluation metrics, such as ME-TEOR) and MEANT, require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.8523062765598297}, {"text": "MEANT", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9291417002677917}]}, {"text": "The lower portability hinders the wide adoption of these metrics.", "labels": [], "entities": []}, {"text": "We, therefore, propose a unified framework, YiSi, for MT quality evaluation and estimation that take advantage of both metric paradigms by providing options to fallback to surface-level lexical similarity when semantic models are not available for the languages in assessment.", "labels": [], "entities": [{"text": "YiSi", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8734877109527588}, {"text": "MT quality evaluation", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.9146495262781779}]}, {"text": "YiSi were first used in WMT 2018 metrics shared task ( and performed well and consistently at segment-level across the tested language pairs in correlating with human judgment.", "labels": [], "entities": []}, {"text": "An YiSi based system successfully served in WMT2018 parallel corpus filtering task (.", "labels": [], "entities": [{"text": "WMT2018 parallel corpus filtering task", "start_pos": 44, "end_pos": 82, "type": "TASK", "confidence": 0.6793983101844787}]}, {"text": "This year, instead of using word2vec () to evaluate lexical semantic similarity in YiSi, we use BERT -Bidirectional Encoder Representation from Transformers (.", "labels": [], "entities": [{"text": "YiSi", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.9377090334892273}, {"text": "BERT", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9954146146774292}]}, {"text": "YiSi is open source and publicly available.", "labels": [], "entities": [{"text": "YiSi", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9112478494644165}]}], "datasetContent": [{"text": "YiSi-0 is the degenerate resource-free variant of YiSi for MT quality evaluation, where sentence E is the MT output and sentence F is the reference.", "labels": [], "entities": [{"text": "YiSi-0", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9122119545936584}, {"text": "MT quality evaluation", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.9042545358339945}]}, {"text": "shows the resources used in YiSi-0.", "labels": [], "entities": [{"text": "YiSi-0", "start_pos": 28, "end_pos": 34, "type": "DATASET", "confidence": 0.8823549151420593}]}, {"text": "YiSi-0 uses the longest common character substring accuracy to evaluate lexical similarity between the MT output and human reference.", "labels": [], "entities": [{"text": "YiSi-0", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.853512167930603}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9031708240509033}]}, {"text": "Since the MT output and the human reference are both in the same language, the lexical weight w(e) of word e in the translation and the lexical weight w(f ) of word fin the reference are both estimated by the inverse-document-frequency of those words in the reference document F.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9494075179100037}]}, {"text": "Thus, formally YiSi-  YiSi-1 is the monolingual variant of YiSi for MT quality evaluation, where sentence E is the MT output and sentence F is the reference.", "labels": [], "entities": [{"text": "MT quality evaluation", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.9027402997016907}]}, {"text": "shows the resources used in YiSi-1.", "labels": [], "entities": []}, {"text": "YiSi-1 requires an embedding model to evaluate lexical semantic similarity and optionally requires a semantic role labeler in the output language for evaluating structural semantic similarity.", "labels": [], "entities": []}, {"text": "The lexical semantic similarity is the cosine similarity of the embeddings from the lexical representation model.", "labels": [], "entities": []}, {"text": "Similar to YiSi-0, the lexical weight w(u) of word unit u in the MT and the reference are estimated by the inverse-document-frequency of that word in the reference document F.", "labels": [], "entities": []}, {"text": "Thus, formally YiSi-1 is defined as follow: YiSi-1 srl = YiSi(s=s 1 , \u03b2=0.1, E=MT, F =REF)  We use WMT 2018 metrics task evaluation set () for our development experiments.", "labels": [], "entities": [{"text": "REF", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9170340895652771}, {"text": "WMT 2018 metrics task evaluation set", "start_pos": 99, "end_pos": 135, "type": "DATASET", "confidence": 0.9219483633836111}]}, {"text": "The official human judgments of translation quality in WMT 2018 were collected using direct assessment.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9653645753860474}, {"text": "WMT 2018", "start_pos": 55, "end_pos": 63, "type": "TASK", "confidence": 0.5241170674562454}]}, {"text": "The direct assessment evaluation protocol in WMT2018 gave the annotators the reference and a MT output and asked them to evaluate the translation adequacy of the MT output on an absolute scale.", "labels": [], "entities": [{"text": "WMT2018", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8758783340454102}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.8134567737579346}]}, {"text": "Due to space limitations, we only report the results of YiSi, chrF, BLEU and the best correlation in each of the individual language pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 68, "end_pos": 72, "type": "METRIC", "confidence": 0.9989140033721924}]}, {"text": "Since we use exactly the same correlation analysis as the official task for each of the test sets, our reported results are directly comparable with those reported in the task's overview paper.", "labels": [], "entities": []}, {"text": "We summarize our observations in the following sections.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pearson's correlation of the metric scores with WMT 2018 aggregated human direct assessment scores at  system-level.", "labels": [], "entities": [{"text": "WMT 2018 aggregated human direct assessment scores", "start_pos": 58, "end_pos": 108, "type": "DATASET", "confidence": 0.8732923524720329}]}, {"text": " Table 2: Kendall's correlation of metric scores with the rankings at segment-level human direct assessment in  WMT 2018.", "labels": [], "entities": [{"text": "WMT 2018", "start_pos": 112, "end_pos": 120, "type": "DATASET", "confidence": 0.7965797185897827}]}]}