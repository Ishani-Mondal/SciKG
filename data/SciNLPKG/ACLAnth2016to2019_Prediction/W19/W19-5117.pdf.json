{"title": [], "abstractContent": [{"text": "This article focuses on the lemmatization of multiword expressions (MWEs).", "labels": [], "entities": [{"text": "lemmatization of multiword expressions (MWEs)", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7009302846023014}]}, {"text": "We propose a deep encoder-decoder architecture generating for every MWE word its corresponding part in the lemma, based on the internal context of the MWE.", "labels": [], "entities": []}, {"text": "The encoder relies on recurrent networks based on (1) the character sequence of the individual words to capture their morphological properties, and (2) the word sequence of the MWE to capture lexical and syntactic properties.", "labels": [], "entities": []}, {"text": "The decoder in charge of generating the corresponding part of the lemma for each word of the MWE is based on a classical character-level attention-based recurrent model.", "labels": [], "entities": []}, {"text": "Our model is evaluated for Ital-ian, French, Polish and Portuguese and shows good performances except for Polish.", "labels": [], "entities": [{"text": "Ital-ian", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.886413037776947}]}], "introductionContent": [{"text": "Lemmatization consists in finding the canonical form of an inflected form occurring in a text.", "labels": [], "entities": []}, {"text": "Usually, the lemma is the base form that can be found in a dictionary.", "labels": [], "entities": []}, {"text": "In this paper, we are interested in the lemmatization of multiword expressions (MWEs), that has received little attention in the past.", "labels": [], "entities": [{"text": "lemmatization of multiword expressions (MWEs)", "start_pos": 40, "end_pos": 85, "type": "TASK", "confidence": 0.6645448718752179}]}, {"text": "MWEs consist of combinations of several words that show some idiosyncrasy;.", "labels": [], "entities": []}, {"text": "They display the linguistic properties of a lexical unit and are present in lexicons as simple words are.", "labels": [], "entities": []}, {"text": "For instance, such a task maybe of interest for the identification of concepts and entities in morphologically-rich languages.", "labels": [], "entities": [{"text": "identification of concepts and entities in morphologically-rich languages", "start_pos": 52, "end_pos": 125, "type": "TASK", "confidence": 0.8427165448665619}]}, {"text": "The main difficulty of the task resides in the variable morphological, lexical and syntactic properties of MWEs leading to many dif-ferent lemmatization rules on top of simpleword lemmatization knowledge, as illustrated by the 27 hand-crafted rules used by the rule-based multiword lemmatizer for Polish.", "labels": [], "entities": []}, {"text": "For example, in French, the nominal MWE cartes bleues (cards.noun.fem.pl blue.noun.fem.pl), meaning credit cards, is lemmatized in carte bleue (car.noun.fem.sg blue.adj.fem.sg) where the adjective bleue (blue) agrees in person (sg) and gender (fem) with the noun carte (card).", "labels": [], "entities": []}, {"text": "A singleword lemmatization would not preserve the gender agreement in this example: the feminine adjective bleues would be lemmatized in the masculine bleu.", "labels": [], "entities": []}, {"text": "In this paper, we propose a deep encoderdecoder architecture generating for every MWE word its corresponding part in the lemma, based on the internal context of the MWE.", "labels": [], "entities": []}, {"text": "The encoder relies on recurrent networks based on (1) the character sequence of the individual words to capture their morphological properties, and (2) the word sequence of the MWE to capture lexical and syntactic properties.", "labels": [], "entities": []}, {"text": "The decoder in charge of generating the corresponding part of the lemma for each word of the MWE is based on a classical characterlevel attention-based recurrent model.", "labels": [], "entities": []}, {"text": "One research question is whether the system is able to encode the complex linguistic properties in order to generate an accurate MWE lemma.", "labels": [], "entities": []}, {"text": "As a preliminary stage, we evaluated our architecture in five suffix-based inflectional languages with a special focus on French and Polish.", "labels": [], "entities": []}, {"text": "Contrary to the lemmatization of simple words, our task is not a disambiguation task 2 , as fora given MWE form, there is one possible lemma in all cases but some very rare exceptions.", "labels": [], "entities": []}, {"text": "This means that the lemma of a known MWE is simply its associated lemma in the training data.", "labels": [], "entities": []}, {"text": "The interest of a neural system is thus limited to the case of unknown MWEs.", "labels": [], "entities": []}, {"text": "One research question is whether the system is able to generalize well on unknown MWEs.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first attempt to implement a language-independent MWE lemmatizer based entirely on neural networks.", "labels": [], "entities": []}, {"text": "Previous work used rule-based methods and/or statistical classification methods (.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.7848891615867615}]}, {"text": "The article is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe our model and our dataset.", "labels": [], "entities": []}, {"text": "Then we display and discuss experimental results, before describing related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our dataset 6 embodies sets of gold pairs (MWE form, MWE lemma) in five languages namely Brazilian Portuguese (BR), French (FR), Italian (IT), Polish (PL), Portuguese Portuguese (PT).", "labels": [], "entities": []}, {"text": "It includes both token-based and type-based data.", "labels": [], "entities": []}, {"text": "Token-based data are derived from annotated corpora and are intended to be used to evaluate our approach on areal MWE distribution.", "labels": [], "entities": [{"text": "MWE distribution", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.7979733943939209}]}, {"text": "Type-based data are derived from different morphosyntactic dictionaries and are intended to be used to evaluate the coverage and robustness of our approach.", "labels": [], "entities": []}, {"text": "They are divided in train/dev/test splits.", "labels": [], "entities": []}, {"text": "displays the dataset sources and statistics.", "labels": [], "entities": []}, {"text": "French and Polish data are by far the larger datasets and includes both token-and type-based resources.", "labels": [], "entities": [{"text": "French and Polish data", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.6245180740952492}]}, {"text": "Italian and Portuguese data are smaller and only typebased.", "labels": [], "entities": [{"text": "Italian and Portuguese data", "start_pos": 0, "end_pos": 27, "type": "DATASET", "confidence": 0.6959359496831894}]}, {"text": "They are derived from the freely available dictionaries in the Unitex plateform ().", "labels": [], "entities": [{"text": "Unitex plateform", "start_pos": 63, "end_pos": 79, "type": "DATASET", "confidence": 0.9478386044502258}]}, {"text": "We constructed our dataset by applying some automatic preprocessing to resolve tokenization and lemma discrepancies between the different sources, and to filter MWEs whose number of words is not equal to the number of words of the lemma, since our approach is based on a word-to-word process (1.6% of the Datasets and code can be found at the following url: https://git.atilf.fr/parseme-fr/ deep-lexical-analysis.", "labels": [], "entities": []}, {"text": "Note that the French Treebank data are distributed upon request because of license specificities.", "labels": [], "entities": [{"text": "French Treebank data", "start_pos": 14, "end_pos": 34, "type": "DATASET", "confidence": 0.9929427901903788}]}, {"text": "MWEs are thus taken off in French).", "labels": [], "entities": []}, {"text": "For tokenbased datasets, we used the official splits used in  and for for Polish.", "labels": [], "entities": []}, {"text": "For dictionary-based resources, we applied a random split by taking care of keeping all entries with the same lemma in the same split.", "labels": [], "entities": []}, {"text": "For every language, we constructed a unique 7 training set composed of the different train parts of the different resources used.", "labels": [], "entities": []}, {"text": "We also augmented our training sets with gold pairs (simple-word form, simple-word lemma) to account for simpleword lemmatization knowledge in the MWE lemmatization process.", "labels": [], "entities": [{"text": "MWE lemmatization", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7866010069847107}]}, {"text": "This information comes from the same sources as MWEs.", "labels": [], "entities": [{"text": "MWEs", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.7473245859146118}]}, {"text": "We manually tuned the hyperparameters of our system on the dev sections.", "labels": [], "entities": []}, {"text": "Our final results on test sections were obtained using the best hyperparameter setting for the dev sections (hidden layer size: 192, character embedding size: 32, tag embedding size: 8, learning rate: 0.005, dropout: 0.25).", "labels": [], "entities": [{"text": "learning rate", "start_pos": 186, "end_pos": 199, "type": "METRIC", "confidence": 0.9243703186511993}]}, {"text": "We used UDPipe () to predict word POS tags for all languages.", "labels": [], "entities": []}, {"text": "We also included predicted morphological features for Polish.", "labels": [], "entities": []}, {"text": "We evaluated our system by using two metrics: MWE-based accuracy and word-based accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.8757088780403137}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.8890039324760437}]}, {"text": "MWE-based accuracy, also used for tuning, accounts for the proportion of MWEs that have been correctly lemmatized.", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.5039786696434021}, {"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9237172603607178}, {"text": "tuning", "start_pos": 34, "end_pos": 40, "type": "TASK", "confidence": 0.9671033620834351}]}, {"text": "Word-based accuracy indicates the total proportion of words that have been given the correct corresponding lemma part.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9650160074234009}]}, {"text": "displays our final results on the dev and test sets of our five languages.", "labels": [], "entities": []}, {"text": "First, it shows that our system generalizes well on unknown MWEs (columns unk.).", "labels": [], "entities": []}, {"text": "For type-based data, scores on unknown MWEs are comparable or slightly better than for all MWEs.", "labels": [], "entities": []}, {"text": "For token-based data, the MWE-based accuracy loss is reasonable, ranging from almost 0 point for French verbal expressions (ST data) to 13 points for Polish MWEs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9727672934532166}]}, {"text": "Our system shows good performances on French.", "labels": [], "entities": [{"text": "French", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.9909716844558716}]}, {"text": "On similar languages (BR, IT, PT), results are lower, but rather good given the limited size of the training sets.", "labels": [], "entities": []}, {"text": "The system shows disappointing results for Polish, especially for the dictionary.", "labels": [], "entities": []}, {"text": "On the token-based dataset, results are very far from the ones obtained by the rule-based system of) which displays around 98% accuracy using 27 rules and dictionary information.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9988634586334229}]}, {"text": "Polish being a morphologically-rich language, the encoding of morphological constraints would deserve more investigations.", "labels": [], "entities": []}, {"text": "The system also shows lower scores for verbal expressions in French, which show much morphological and syntactic variation.", "labels": [], "entities": []}, {"text": "We also evaluated our system to lemmatize simple words, as it would have been convenient to have a single system processing the lemmatization on both simple words and MWEs.", "labels": [], "entities": []}, {"text": "However, it did not show satisfying results: we obtained a score of 73% on the FTB corpus, against 99% when the system is trained on simple words only.", "labels": [], "entities": [{"text": "FTB corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9859088957309723}]}], "tableCaptions": [{"text": " Table 1: Dataset sources and statistics. The column Nb of = MWEs refers to the number of MWE types (i.e.  number of different MWEs). The column Nb of MWE POS refers to the size of the set of MWE-level POS tags", "labels": [], "entities": []}, {"text": " Table 2: Final results for all and unknown MWEs.  Columns Dev(MWEs) and Test(MWEs) provide MWE- based accuracy on the dev and test sets respectively.  Column Test(words) gives word-based accuracy on the  test set.", "labels": [], "entities": [{"text": "Test(MWEs)", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.8913030922412872}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.8713311553001404}, {"text": "accuracy", "start_pos": 188, "end_pos": 196, "type": "METRIC", "confidence": 0.9076865911483765}]}, {"text": " Table 3: MWE-based accuracy on dev section for  French with different architectures and comparison  with baselines.", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.7963340282440186}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9837073683738708}]}, {"text": " Table 4: Best result for our system compared to UD- Pipe adaptation baseline for French and Polish dev sets.  The table shows MWE-based accuracy.", "labels": [], "entities": [{"text": "MWE-based", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.4431110918521881}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9089401960372925}]}, {"text": " Table 5: MWE-based accuracy on dev section accord- ing to MWE subclasses. * indicates that lemmas were  predicted by UDPipe. Otherwise they are gold. Num- bers between parentheses indicate the repartition of the  MWE subclasses in the tested dataset (in percentage).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9915279150009155}, {"text": "UDPipe", "start_pos": 118, "end_pos": 124, "type": "DATASET", "confidence": 0.7704083323478699}]}]}