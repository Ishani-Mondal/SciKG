{"title": [{"text": "Multi-Element Long Distance Dependencies: Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "In order to successfully model Long Distance Dependencies (LDDs) it is necessary to understand the full-range of the characteristics of the LDDs exhibited in a target dataset.", "labels": [], "entities": []}, {"text": "In this paper, we use Strictly k-Piecewise languages to generate datasets with various properties.", "labels": [], "entities": []}, {"text": "We then compute the characteristics of the LDDs in these datasets using mutual information and analyze the impact of factors such as (i) k, (ii) length of LDDs, (iii) vocabulary size, (iv) forbidden subsequences, and (v) dataset size.", "labels": [], "entities": []}, {"text": "This analysis reveal that the number of interacting elements in a dependency is an important characteristic of LDDs.", "labels": [], "entities": []}, {"text": "This leads us to the challenge of modelling multi-element long-distance dependencies.", "labels": [], "entities": []}, {"text": "Our results suggest that attention mechanisms in neu-ral networks may aide in modeling datasets with multi-element long-distance dependencies.", "labels": [], "entities": []}, {"text": "However, we conclude that there is a need to develop more efficient attention mechanisms to address this issue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Long Distance Dependencies (LDDs) describe an interaction between two (or more) elements in a sequence that are separated by an arbitrary number of positions.", "labels": [], "entities": [{"text": "Long Distance Dependencies (LDDs)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6788364723324776}]}, {"text": "LDDs are related to the rate of decay of statistical dependence of two points with increasing time interval or spatial distance between them.", "labels": [], "entities": []}, {"text": "For example, in English there is a requirement for subjects and verbs to agree, compare: \"The dog in that house is aggressive\" with \"The dogs in that house are aggressive\".", "labels": [], "entities": []}, {"text": "This dependence can be computed using information theoretic measure i.e. Mutual Information.", "labels": [], "entities": []}, {"text": "To date most research on LDDs has focused on the distance the dependency spans within the sequence.", "labels": [], "entities": []}, {"text": "However, as our analysis will show the complexity of LDDs not only arises from the distance but also a number of other factors, including: (i) the number of unique symbols in a dataset, (ii) the size of the dataset, (iii) the number of interacting symbols within an LDD, and (iv) the distance between the interacting symbols.", "labels": [], "entities": []}, {"text": "In this paper we use SPk languages to explore the complexity of LDDs.", "labels": [], "entities": []}, {"text": "The motivation for using the SPk language modelling task, is that the standard sequential benchmark datasets provide little to no control over the factors which directly contribute to LDD characteristics.", "labels": [], "entities": [{"text": "SPk language modelling task", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.905730739235878}]}, {"text": "By contrast, using SPk languages we can generate benchmark datasets with varying degrees of LDD complexity by modifying the grammar of the SPk language (.", "labels": [], "entities": []}, {"text": "One aspect of LDDs that has been neglected in the research on LDDs is the complexity that arises from a multi-element dependency (i.e., dependencies that involves interactions between more than 2 elements).", "labels": [], "entities": []}, {"text": "By controlling kin the SPk grammar, it is possible to generate datasets with varying degrees of multi-element dependency.", "labels": [], "entities": []}, {"text": "This multi-element dependencies pose specific challenges to neural architectures that may require these architectures to be augmented with pointer or attention mechanisms.", "labels": [], "entities": []}, {"text": "We explore whether attention mechanism can help with multielement LDDs using two models, Transformer-XL and AWD-LSTM.", "labels": [], "entities": []}, {"text": "Transformer-XL employs multihead attention mechanism along with recurrence mechanism.", "labels": [], "entities": []}, {"text": "Whereas, AWD-LSTM is a weight dropped LSTM which does not employ any attention/pointer mechanism.", "labels": [], "entities": [{"text": "AWD-LSTM", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.7208886742591858}]}, {"text": "The Transformer-XL and AWD-LSTM models are both language models.", "labels": [], "entities": []}, {"text": "A language model accepts a sequence of symbols and predicts the next symbol in the sequence.", "labels": [], "entities": []}, {"text": "The accuracy of a language model is dependent on the capacity of the model to capture the LDDs in the data on which it is evaluated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991145730018616}]}, {"text": "The standard evaluation metric for language models is perplexity.", "labels": [], "entities": []}, {"text": "Perplexity is the measurement of the confusion or uncertainty of a language model as it predicts the next symbol in a sequence, and the lower the perplexity of a model the better the performance of the model.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9514133930206299}]}], "datasetContent": [{"text": "Natural datasets present little to no control over the factors that affect LDDs.", "labels": [], "entities": []}, {"text": "This, limits our ability to understand LDDs in more detail.", "labels": [], "entities": []}, {"text": "SPk languages exhibit some types of LDDs occurring in natural datasets.", "labels": [], "entities": []}, {"text": "Moreover, by modifying the SPk grammar we can control the LDD characteristics within a dataset generated by the grammar.", "labels": [], "entities": []}, {"text": "To understand and validate the interaction between an SPk grammar and the characteristics of the data it generates, we used a number of datasets of SPk grammar and analyzed the properties of these datasets.", "labels": [], "entities": []}, {"text": "Every dataset is a collection of strings and these strings strictly follow the grammar.", "labels": [], "entities": []}, {"text": "Hence the size of the dataset (|dataset|) is the sum of the size of all the strings.", "labels": [], "entities": []}, {"text": "The datasets were generated using foma) and python ( . Below we analyze the impact of various factors on the resulting LDD characteristics.", "labels": [], "entities": []}, {"text": "Another factor to analyze is the impact of the size of the dataset (|dataset|) on LDDs of the same grammar.", "labels": [], "entities": []}, {"text": "We generate two sizes of the same SP2 grammar to study the impact of the size of the data on the LDD characteristics, where one dataset is twice the size of the other.", "labels": [], "entities": []}, {"text": "In we can observe that LDD characteristics of datasets sampled from the same grammar are less likely to be affected by the size of the dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity score of 1: Transformer-XL and  2: AWD-LSTM models of SP2, SP4, SP8 and SP16  datasets with vocabulary size V =4", "labels": [], "entities": [{"text": "Perplexity score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.93947434425354}]}, {"text": " Table 2: Perplexity score of 1: Transformer-XL and  2: AWD-LSTM models of SP2, SP4, SP6 and SP8  datasets with vocabulary size V =26", "labels": [], "entities": [{"text": "Perplexity score", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9428815543651581}]}]}