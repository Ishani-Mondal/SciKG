{"title": [{"text": "SIM: A Slot-Independent Neural Model for Dialogue State Tracking", "labels": [], "entities": [{"text": "Dialogue State Tracking", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7300368944803873}]}], "abstractContent": [{"text": "Dialogue state tracking is an important component in task-oriented dialogue systems to identify users' goals and requests as a dialogue proceeds.", "labels": [], "entities": [{"text": "Dialogue state tracking", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7848396301269531}]}, {"text": "However, as most previous models are dependent on dialogue slots, the model complexity soars when the number of slots increases.", "labels": [], "entities": []}, {"text": "In this paper, we put forward a slot-independent neural model (SIM) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots.", "labels": [], "entities": []}, {"text": "The model utilizes attention mechanisms between user utterance and system actions.", "labels": [], "entities": []}, {"text": "SIM achieves state-of-the-art results on WoZ and DSTC2 tasks, with only 20% of the model size of previous models.", "labels": [], "entities": [{"text": "WoZ", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.820641815662384}]}], "introductionContent": [{"text": "With the rapid development in deep learning, there is a recent boom of task-oriented dialogue systems in terms of both algorithms and datasets.", "labels": [], "entities": []}, {"text": "The goal of task-oriented dialogue is to fulfill a user's requests such as booking hotels via communication in natural language.", "labels": [], "entities": []}, {"text": "Due to the complexity and ambiguity of human language, previous systems have included semantic decoding) to project natural language input into pre-defined dialogue states.", "labels": [], "entities": []}, {"text": "These states are typically represented by slots and values: slots indicate the category of information and values specify the content of information.", "labels": [], "entities": []}, {"text": "For instance, the user utterance \"can you help me find the address of any hotel in the south side of the city\" can be decoded as inf orm(area, south) and request(address), meaning that the user has specified the value south for slot area and requested another slot address.", "labels": [], "entities": []}, {"text": "Numerous methods have been put forward to decode a user's utterance into slot values.", "labels": [], "entities": []}, {"text": "Some use hand-crafted features and domain-specific delexicalization methods to achieve strong performance (.", "labels": [], "entities": []}, {"text": "Mrk\u0161i\u00b4 employs CNN and pretrained embeddings to further improve the state tracking accuracy.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8925545811653137}, {"text": "state tracking", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.7730631828308105}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8402578234672546}]}, {"text": "Mrk\u0161i\u00b4 extends this work by using two additional statistical update mechanisms.", "labels": [], "entities": [{"text": "Mrk\u0161i\u00b4", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9061064124107361}]}, {"text": "uses human teaching and feedback to boost the state tracking performance.", "labels": [], "entities": [{"text": "state tracking", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7088847160339355}]}, {"text": "utilizes both global and local attention mechanism in the proposed GLAD model which obtains state-ofthe-art results on WoZ and DSTC2 datasets.", "labels": [], "entities": [{"text": "WoZ and DSTC2 datasets", "start_pos": 119, "end_pos": 141, "type": "DATASET", "confidence": 0.7439225912094116}]}, {"text": "However, most of these methods require slot-specific neural structures for accurate prediction.", "labels": [], "entities": []}, {"text": "For example, defines a parametrized local attention matrix for each slot.", "labels": [], "entities": []}, {"text": "Slot-specific mechanisms become unwieldy when the dialogue task involves many topics and slots, as is typical in a complex conversational setting like product troubleshooting.", "labels": [], "entities": []}, {"text": "Furthermore, due to the sparsity of labels, there may not be enough data to thoroughly train each slot-specific network structure.; both propose to remove the model's dependency on dialogue slots but there's no modification to the representation part, which could be crucial to textual understanding as we will show later.", "labels": [], "entities": []}, {"text": "To solve this problem, we need a state tracking model independent of dialogue slots.", "labels": [], "entities": []}, {"text": "In other words, the network should depend on the semantic similarity between slots and utterance instead of slot-specific modules.", "labels": [], "entities": []}, {"text": "To this end, we propose the Slot-Independent Model (SIM).", "labels": [], "entities": []}, {"text": "Our model complexity does not increase when the number of slots in dialogue tasks go up.", "labels": [], "entities": []}, {"text": "Thus, SIM has many fewer parameters than existing dialogue state tracking models.", "labels": [], "entities": [{"text": "SIM", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9327529072761536}, {"text": "dialogue state tracking", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.6363726357618967}]}, {"text": "To compensate for the exclusion of slot-specific parameters, we incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).", "labels": [], "entities": []}, {"text": "The refined representation, in addition to cross and self-attention mechanisms, make our model achieve even better performance than slot-specific models.", "labels": [], "entities": []}, {"text": "For instance, on Wizard-of-Oz (WOZ) 2.0 dataset ), the SIM model obtains a joint-accuracy score of 89.5%, 1.4% higher than the previously best model GLAD, with only 22% of the number of parameters.", "labels": [], "entities": [{"text": "Wizard-of-Oz (WOZ) 2.0 dataset", "start_pos": 17, "end_pos": 47, "type": "DATASET", "confidence": 0.770942876736323}]}, {"text": "On DSTC2 dataset, SIM achieves comparable performance with previous best models with only 19% of the model size.", "labels": [], "entities": [{"text": "DSTC2 dataset", "start_pos": 3, "end_pos": 16, "type": "DATASET", "confidence": 0.97437584400177}]}], "datasetContent": [{"text": "We evaluated our model on Wizard of Oz (WoZ) ) and the second Dialogue System Technology Challenges ().", "labels": [], "entities": []}, {"text": "Both tasks are for restaurant reservation and have slot-value pairs of both goal and request types.", "labels": [], "entities": [{"text": "restaurant reservation", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7131337374448776}]}, {"text": "WoZ has 4 kinds of slots (area, food, price range, request) and 94 values in total.", "labels": [], "entities": [{"text": "WoZ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9666882157325745}]}, {"text": "DSTC2 has an additional slot name and 220 values in total.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9608463048934937}]}, {"text": "WoZ has 800 dialogues in the training and development set and 400 dialogues in the test set, while DSTC2 dataset consists of 2118 dialogues in the training and development set, and 1117 dialogues in the test set.", "labels": [], "entities": [{"text": "WoZ", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9777700304985046}, {"text": "DSTC2 dataset", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9851815700531006}]}], "tableCaptions": [{"text": " Table 1: Joint goal and turn request accuracies on WoZ and DSTC2 restaurant reservation datasets.", "labels": [], "entities": [{"text": "WoZ and DSTC2 restaurant reservation datasets", "start_pos": 52, "end_pos": 97, "type": "DATASET", "confidence": 0.8169474999109904}]}, {"text": " Table 2: Model size comparison between SIM and  GLAD (Zhong et al., 2018) on WoZ and DSTC2.", "labels": [], "entities": [{"text": "WoZ", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9239569306373596}, {"text": "DSTC2", "start_pos": 86, "end_pos": 91, "type": "DATASET", "confidence": 0.5553506016731262}]}, {"text": " Table 3: Ablation study of SIM on WoZ. We pick the  model with highest joint goal score on development set  and report its performance on test set.", "labels": [], "entities": [{"text": "SIM", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9742618799209595}, {"text": "WoZ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8933640718460083}]}]}