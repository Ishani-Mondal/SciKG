{"title": [{"text": "Findings of the WMT 2019 Shared Tasks on Quality Estimation", "labels": [], "entities": [{"text": "WMT 2019 Shared Tasks on Quality Estimation", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.6657993623188564}]}], "abstractContent": [{"text": "We report the results of the WMT19 shared task on Quality Estimation, i.e. the task of predicting the quality of the output of machine translation systems given just the source text and the hypothesis translations.", "labels": [], "entities": [{"text": "WMT19 shared task", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.5494486689567566}]}, {"text": "The task includes estimation at three granularity levels: word, sentence and document.", "labels": [], "entities": []}, {"text": "A novel addition is evaluating sentence-level QE against human judgments: in other words, designing MT metrics that do not need a reference translation.", "labels": [], "entities": [{"text": "MT metrics", "start_pos": 100, "end_pos": 110, "type": "TASK", "confidence": 0.8840341866016388}]}, {"text": "This year we include three language pairs, produced solely by neural machine translation systems.", "labels": [], "entities": []}, {"text": "Participating teams from eleven institutions submitted a variety of systems to different task variants and language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "This shared task builds on its previous seven editions to further examine automatic methods for estimating the quality of machine translation (MT) output at run-time, without the use of reference translations.", "labels": [], "entities": [{"text": "estimating the quality of machine translation (MT) output", "start_pos": 96, "end_pos": 153, "type": "TASK", "confidence": 0.7297135651111603}]}, {"text": "It includes the (sub)tasks of wordlevel, sentence-level and document-level estimation.", "labels": [], "entities": [{"text": "document-level estimation", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6160382032394409}]}, {"text": "In addition to advancing the state of the art at all prediction levels, our more specific goals include to investigate the following: \u2022 The predictability of missing words in the MT output.", "labels": [], "entities": [{"text": "predictability of missing words", "start_pos": 140, "end_pos": 171, "type": "TASK", "confidence": 0.8558786958456039}, {"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.7315005660057068}]}, {"text": "As in last year, our data include this annotation.", "labels": [], "entities": []}, {"text": "\u2022 The predictability of source words that lead to errors in the MT output, also as in last year.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9695796370506287}]}, {"text": "\u2022 Quality prediction for documents based on errors annotated at word-level with added severity judgments.", "labels": [], "entities": [{"text": "Quality prediction", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.8181376755237579}]}, {"text": "This is also like in last year.", "labels": [], "entities": []}, {"text": "\u2022 The predictability of individual errors within documents, which may depend on a larger context.", "labels": [], "entities": [{"text": "predictability of individual errors within documents", "start_pos": 6, "end_pos": 58, "type": "TASK", "confidence": 0.7835663060347239}]}, {"text": "This is a novel task, building upon the existing document-level quality estimation.", "labels": [], "entities": []}, {"text": "\u2022 The reliability of quality estimation models as a proxy for metrics that depend on a reference translation.", "labels": [], "entities": [{"text": "reliability", "start_pos": 6, "end_pos": 17, "type": "METRIC", "confidence": 0.9601185917854309}]}, {"text": "\u2022 The generalization ability of quality estimation models to different MT systems instead of a single ones We present a simpler setup in comparison to last edition, which featured more language pairs, statistical MT outputs alongside neural ones, and an additional task for phrase-based QE.", "labels": [], "entities": []}, {"text": "This simplification reflects a more realistic scenario, in which NMT systems have mostly replaced SMT ones, making phrase-level predictions harder.", "labels": [], "entities": [{"text": "SMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.9500131011009216}]}, {"text": "We used both new data as well as some existing data from the previous edition of this shared task.", "labels": [], "entities": []}, {"text": "For word and sentence level, we reused the English-German dataset from last year, but also added anew English-Russian one.", "labels": [], "entities": []}, {"text": "For document level, we reused last year's English-French data for training and validation, but introduced anew test set from the same corpus.", "labels": [], "entities": []}, {"text": "For QE as a metric we ran the evaluation jointly with the WMT19 metrics task, which meant applying the QE systems to news translation submissions and evaluating them against the human judgments collected this year.", "labels": [], "entities": [{"text": "WMT19 metrics task", "start_pos": 58, "end_pos": 76, "type": "DATASET", "confidence": 0.8374993801116943}, {"text": "news translation submissions", "start_pos": 117, "end_pos": 145, "type": "TASK", "confidence": 0.7358104189236959}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Scores for the example system output shown  in", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the datasets used in Task 1. Number of sentences is always the same in source and target;  number of words refer to the source. Values shown for HTER are mean and standard deviation in parentheses.", "labels": [], "entities": [{"text": "HTER", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.5117917656898499}]}, {"text": " Table 3: Statistics of the datasets used in Task 2. The column Annotations shows the average number of annotations  per document in the dataset. The values for MQM and Annotations are the mean with standard deviation in  parentheses", "labels": [], "entities": [{"text": "Annotations", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9454672336578369}, {"text": "MQM", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.5385477542877197}]}, {"text": " Table 4: Word-level results for EN-DE.  \u2020 indicates the winning system.* indicates late submissions that were not  considered in the official ranking.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8124916553497314}]}, {"text": " Table 5: Word-level results for EN-RU.  \u2020 indicates the winning systems. * indicates late submissions that were not  considered in the official ranking.", "labels": [], "entities": [{"text": "EN-RU", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8431247472763062}]}, {"text": " Table 6: Sentence-level results for EN-DE.  \u2020 indicates the winning system. * indicates late submissions that were  not considered in the official ranking.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8105173110961914}]}, {"text": " Table 7: Sentence-level results for EN-RU.  \u2020 indicates the winning system. * indicates late submissions that were  not considered in the official ranking.", "labels": [], "entities": [{"text": "EN-RU", "start_pos": 37, "end_pos": 42, "type": "DATASET", "confidence": 0.8180201053619385}]}, {"text": " Table 8: Document-level fine grained annotation re- sults for EN-FR", "labels": [], "entities": [{"text": "EN-FR", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.6946799755096436}]}, {"text": " Table 9: Document-level MQM results for EN-FR.  \u2020  indicates the winning system. * indicates late submis- sions.", "labels": [], "entities": [{"text": "MQM", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.7994250655174255}, {"text": "EN-FR", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.7737793326377869}]}, {"text": " Table 10: Results of task 3: system-level Pearson correlations between the submitted metrics and human judgments  on all translation directions into English. LASER and LogProb are the baselines. The reference-based BLEU and  chrF metrics are provided for comparison.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.933089405298233}, {"text": "LASER", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9441646337509155}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9819239974021912}]}, {"text": " Table 11: Results of task 3: system-level Pearson correlations between the submitted metrics and human judgments  on all translation directions from English. LASER and LogProb are the baselines. The reference-based BLEU and  chrF metrics are provided for comparison.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.9345129728317261}, {"text": "LASER", "start_pos": 159, "end_pos": 164, "type": "METRIC", "confidence": 0.9180300831794739}, {"text": "BLEU", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9821546077728271}]}, {"text": " Table 12: Results of task 3: segment-level Kendall \u03c4 correlations between the submitted metrics and human  judgments on all translation directions into English. LASER and LogProb are the baselines. The reference-based  sentBLEU and chrF metrics are provided for comparison.", "labels": [], "entities": [{"text": "segment-level Kendall \u03c4 correlations", "start_pos": 30, "end_pos": 66, "type": "METRIC", "confidence": 0.7089589461684227}, {"text": "LASER", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9455529451370239}]}, {"text": " Table 13: Results of task 3: segment-level Kendall \u03c4 correlations between the submitted metrics and human  judgments on all translation directions from English. LASER and LogProb are the baselines. The reference-based  sentBLEU and chrF metrics are provided for comparison.", "labels": [], "entities": [{"text": "segment-level Kendall \u03c4 correlations", "start_pos": 30, "end_pos": 66, "type": "METRIC", "confidence": 0.7121260464191437}, {"text": "LASER", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9178418517112732}]}, {"text": " Table 14: Results of task 3: system-level Pearson cor- relations between the submitted metrics and human  judgments on all translation directions without English  involved. The LASER and LogProb baselines were  not computed for these language pairs. The reference- based BLEU and chrF metrics are provided for com- parison.", "labels": [], "entities": [{"text": "Pearson cor- relations", "start_pos": 43, "end_pos": 65, "type": "METRIC", "confidence": 0.9322238266468048}, {"text": "LASER", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.8396410942077637}, {"text": "BLEU", "start_pos": 272, "end_pos": 276, "type": "METRIC", "confidence": 0.986164927482605}]}, {"text": " Table 15: Results of task 3: segment-level Kendall \u03c4  correlations between the submitted metrics and human  judgments on all translation directions without English  involved. The LASER and LogProb baselines were  not computed for these language pairs. The reference- based sentBLEU and chrF metrics are provided for  comparison.", "labels": [], "entities": [{"text": "segment-level Kendall \u03c4  correlations", "start_pos": 30, "end_pos": 67, "type": "METRIC", "confidence": 0.7014747709035873}, {"text": "LASER", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.8851929306983948}]}]}