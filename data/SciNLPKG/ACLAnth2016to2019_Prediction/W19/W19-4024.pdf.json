{"title": [{"text": "Comparative judgments are more consistent than binary classification for labelling word complexity", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical simplification systems replace complex words with simple ones based on a model of which words are complex in context.", "labels": [], "entities": []}, {"text": "We explore how users can help train complex word identification models through labelling more efficiently and reliably.", "labels": [], "entities": [{"text": "word identification", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7462729513645172}]}, {"text": "We show that using an interface where annotators make comparative rather than binary judgments leads to more reliable and consistent labels, and explore whether comparative judgments may provide a faster way for collecting labels.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we address the use of machine learning (ML) for natural language readability assessment concerned with the identification of factors that affect a reader's understanding, reading speed and level of interest (.", "labels": [], "entities": [{"text": "natural language readability assessment", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.6582941263914108}]}, {"text": "We focus on lexical simplification, which aims to adapt text by replacing contextually complex words with more accessible meaning-equivalent alternatives: e.g. replacing ameliorate with improve in the context like \"They aimed to ameliorate \u2192 improve the situation.\"", "labels": [], "entities": []}, {"text": "Lexical simplification can be framed as a two step procedure, where the algorithm needs to first identify which words (or more specifically word senses) in context require simplification, and then replace them with simpler alternatives.", "labels": [], "entities": [{"text": "Lexical simplification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.89105024933815}]}, {"text": "The first step is commonly referred to as complex word identification (CWI).", "labels": [], "entities": [{"text": "complex word identification (CWI)", "start_pos": 42, "end_pos": 75, "type": "TASK", "confidence": 0.7543078313271204}]}, {"text": "In supervised ML, algorithms are trained using data that is labelled according to a target concept ().", "labels": [], "entities": [{"text": "ML", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9391258358955383}]}, {"text": "In the CWI task, the concept is word complexity in context, which fora human reader may combine multiple factors that a machine tries to learn from the data.", "labels": [], "entities": []}, {"text": "Labelling of large data sets is time-consuming and costly, and often carried out using crowd-sourcing platforms such as Amazon Mechanical).", "labels": [], "entities": []}, {"text": "For the CWI task, crowd-source workers have in the past been employed to identify which words within a training dataset are complex: for example, given a sentence \"They aimed to ameliorate the situation\", the annotators might identify ameliorate as complex.", "labels": [], "entities": []}, {"text": "Labelled datasets collected this way are then used to train a model that can predict previously unseen words' complexity.", "labels": [], "entities": []}, {"text": "Prior work on labelling of CWI datasets has found that annotation of word complexity is challenging, yielding relatively low levels of interannotator agreement such as \u03b1 = 0. and \u03ba = 0.398 (.", "labels": [], "entities": [{"text": "CWI datasets", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.7314253151416779}]}, {"text": "In this paper, we show that representing the concept of word complexity in a continuous manner results in higher inter-annotator agreement than using binary labels.", "labels": [], "entities": []}, {"text": "In particular, we investigate the following hypothesis: Hypothesis 1 (H1): Do comparative judgments for CWI lead to higher inter-annotator agreement and higher quality labelled data than binary judgments?", "labels": [], "entities": []}, {"text": "Furthermore, this paper poses the following questions regarding the general setting of the CWI annotation experiments: 1.", "labels": [], "entities": []}, {"text": "Does controlling for the homogeneity of the group of annotators with respect to their age, education level and native language contribute to higher agreement?", "labels": [], "entities": []}, {"text": "2. Can comparative judgments be made in a significantly shorter period of time than binary judgments for word complexity?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Standard of inter-annotator agreement in pre- vious CWI datasets", "labels": [], "entities": [{"text": "Standard", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9019544720649719}, {"text": "CWI datasets", "start_pos": 62, "end_pos": 74, "type": "DATASET", "confidence": 0.724127471446991}]}, {"text": " Table 2.  Comparative Binary  Judgment  Judgment  Kappa Coefficient 0.6775  0.3937  Alpha Coefficient 0.6821  0.4960  Avg Time (s)  28.77  38.69", "labels": [], "entities": [{"text": "Comparative Binary  Judgment  Judgment  Kappa Coefficient 0.6775  0.3937  Alpha Coefficient 0.6821  0.4960  Avg Time (s)", "start_pos": 11, "end_pos": 131, "type": "METRIC", "confidence": 0.6168237985933528}]}, {"text": " Table 2: Results of the study", "labels": [], "entities": []}]}