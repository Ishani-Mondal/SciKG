{"title": [{"text": "Transferring knowledge from discourse to arguments: A case study with scientific abstracts", "labels": [], "entities": [{"text": "Transferring knowledge from discourse", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8767862170934677}]}], "abstractContent": [{"text": "In this work we propose to leverage resources available with discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task.", "labels": [], "entities": [{"text": "identification of argumentative components and relations in scientific texts", "start_pos": 107, "end_pos": 183, "type": "TASK", "confidence": 0.7599844071600173}]}, {"text": "In particular, we implement and evaluate a transfer learning approach in which contextualized representations learned from discourse parsing tasks are used as input of argument mining models.", "labels": [], "entities": [{"text": "discourse parsing tasks", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.7978503704071045}, {"text": "argument mining", "start_pos": 168, "end_pos": 183, "type": "TASK", "confidence": 0.7245670557022095}]}, {"text": "As a pilot application , we explore the feasibility of using automatically identified argumentative components and relations to predict the acceptance of papers in computer science venues.", "labels": [], "entities": [{"text": "acceptance of papers in computer science venues", "start_pos": 140, "end_pos": 187, "type": "TASK", "confidence": 0.782226026058197}]}, {"text": "In order to conduct our experiments, we propose an annotation scheme for argumentative units and relations and use it to enrich an existing corpus with an argumentation layer.", "labels": [], "entities": []}], "introductionContent": [{"text": "The growing number of scientific publications and the shortening of the research-publication cycles () pose a challenge to authors, reviewers and editors.", "labels": [], "entities": []}, {"text": "The development of automatic systems to support the quality assessment of scientific texts can facilitate the work of editors and referees of scientific publications and, at the same time, be of value for researchers to obtain feedback that can lead to improve the communication of their results.", "labels": [], "entities": []}, {"text": "The quality assessment of scientific texts has many dimensions, and each one involves different levels of difficulties.", "labels": [], "entities": []}, {"text": "While the relevance of the problem at stake and the novelty of the solutions proposed by the authors are of great significance in terms of weighting the ultimate contributions of the work, aspects such as the argumentative structure of the text are key when analyzing its effectiveness with respect to its communication objectives (.", "labels": [], "entities": []}, {"text": "A fine-grained assessment of the contributions made in research articles requires to identify the main claims made by the authors and to determine if the evidence provided to support them is strong enough.", "labels": [], "entities": []}, {"text": "Or, in other terms, if both the structure and the contents of the arguments proposed by the authors can persuade a potential reader of the validity of their contributions.", "labels": [], "entities": []}, {"text": "In addition to being useful for facilitating the assessment of some quality aspects of a text, the automatic identification of argumentative units and their relations-a set of related tasks known as argument mining-is a relevant problem in itself in the context of knowledge mining (.", "labels": [], "entities": [{"text": "argument mining-is", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.6748829782009125}, {"text": "knowledge mining", "start_pos": 265, "end_pos": 281, "type": "TASK", "confidence": 0.7314591258764267}]}, {"text": "Being able to extract not only what is being stated by the authors of a text but also the reasons they provide to support it can be useful in multiple applications, ranging from a finegrained analysis of opinions to the generation of abstractive summaries of texts.", "labels": [], "entities": [{"text": "generation of abstractive summaries of texts", "start_pos": 220, "end_pos": 264, "type": "TASK", "confidence": 0.6925466358661652}]}, {"text": "As an example of a potential application for argument mining, ( suggest the possibility of developing an argumentative ranking component in a search engine so that it retrieves documents based on claims and evidence on a given topic extracted automatically from texts.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.8426735103130341}]}, {"text": "The tasks involved in the extraction of arguments from text-including the identification of argumentative sentences, the detection of argument component boundaries and the prediction of argument structures-are related to other text mining tasks-including sequence labeling, text segmentation, entity recognition and relation extractionwhich are in general tackled by means of supervised learning methods (.", "labels": [], "entities": [{"text": "extraction of arguments from text-including the identification of argumentative sentences", "start_pos": 26, "end_pos": 115, "type": "TASK", "confidence": 0.6498996794223786}, {"text": "text mining tasks-including sequence labeling", "start_pos": 227, "end_pos": 272, "type": "TASK", "confidence": 0.7912292361259461}, {"text": "text segmentation", "start_pos": 274, "end_pos": 291, "type": "TASK", "confidence": 0.7332341969013214}, {"text": "entity recognition", "start_pos": 293, "end_pos": 311, "type": "TASK", "confidence": 0.7885697185993195}, {"text": "relation extractionwhich", "start_pos": 316, "end_pos": 340, "type": "TASK", "confidence": 0.8260310888290405}]}, {"text": "The lack of annotated data with argumentative information, however, presents a challenge when trying to apply these well-known approaches to argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 141, "end_pos": 156, "type": "TASK", "confidence": 0.8365637362003326}]}, {"text": "This is so, in part, due to the inherent difficulty of unambiguously identifying argumentative elements in texts, which is reflected in the low levels of inter-annotator agreement reached in general for this task.", "labels": [], "entities": []}, {"text": "If this is true in several knowledge domains, it poses a more difficult problem in the case of scientific texts due to their inherent argumentative complexity.", "labels": [], "entities": []}, {"text": "We propose to address this challenge by leveraging data annotated with discourse relations, as previous works suggest potential benefits in linking discourse analysis and argument mining tasks).", "labels": [], "entities": [{"text": "argument mining tasks", "start_pos": 171, "end_pos": 192, "type": "TASK", "confidence": 0.7745051980018616}]}], "datasetContent": [{"text": "The first set of experiments, described in this section, are aimed at exploring the potential of applying a transfer learning method to improve the performance of argument mining tasks trained with a small corpus of 60 abstracts by leveraging the discourse annotations available in the full SciDTB corpus.", "labels": [], "entities": [{"text": "argument mining tasks", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.8399208386739095}, {"text": "SciDTB corpus", "start_pos": 291, "end_pos": 304, "type": "DATASET", "confidence": 0.7572591006755829}]}, {"text": "We train each of the tasks described in 4.1 separately and compare the results obtained with those obtained by an inductive transfer learning method in which we use encoders trained with the RST annotations available in the SciDTB corpus.", "labels": [], "entities": [{"text": "SciDTB corpus", "start_pos": 224, "end_pos": 237, "type": "DATASET", "confidence": 0.8983587026596069}]}, {"text": "These encoders are then used to produce contextualized representations of the input tokens that are fed to the argument mining learning processes.", "labels": [], "entities": [{"text": "argument mining learning", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.7677952249844869}]}, {"text": "The discourse parsing tasks considered to train the specialized encoders are: \u2022 DFu (discourse function): Identify the boundaries and discourse roles of the EDUs before in the text.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6980402171611786}]}, {"text": "(attribution, evaluation, progression, etc.).", "labels": [], "entities": [{"text": "progression", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9730752110481262}]}, {"text": "8 \u2022 DPa (discourse attachment): Identify the boundaries of the EDUs and the relative position of the parent units in the RST tree.", "labels": [], "entities": []}, {"text": "The discourse tasks (DFu and DPa) are trained with the 738 abstracts left in the SciDTB corpus when excluding the 60 abstracts annotated with arguments.", "labels": [], "entities": []}, {"text": "This is done in order to avoid introducing a bias that would not reflect the results obtained when no discourse annotations are available.", "labels": [], "entities": []}, {"text": "All the argument mining models (AFu, ATy, APa) are trained and evaluated in a 10-fold crossvalidation setting.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7384279817342758}, {"text": "AFu", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7387761473655701}]}, {"text": "In all cases the models are generated by means of bi-directional long short-term memory (BiL-STM) networks, as this type of architecture has proven to perform reasonably well in argument mining tasks across different classification scenarios (.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 178, "end_pos": 193, "type": "TASK", "confidence": 0.7783213555812836}]}, {"text": "In order to simplify the experiments and the interpretation of their results we use the same architecture for all tasks: two layers of 100 recurrent units, Adam optimizer, naive dropout probability of 0.25 and a conditional random fields (CRF) classifier as the last layer of the network.", "labels": [], "entities": []}, {"text": "We use, for the BiLSTMs, the implementation made available by the Ubiquitous Knowledge Processing Lab of the Technische Universit\u00e4t Darmstadt (.", "labels": [], "entities": []}, {"text": "As our intention is to compare the different approaches and not necessarily obtain the best possible models for these tasks, no hyperparameter optimization is done in these experiments and, in all of the cases, the networks are trained for 100 epochs.", "labels": [], "entities": []}, {"text": "All of the tasks are modeled as sequence labeling problems in which the tokens are tagged using the beginning-inside-outside (BIO) tagging scheme.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.6494855433702469}]}, {"text": "The tokens are encoded as the concatenation of 300-dimensional dependency-based word embeddings (DEmb) 10 ( k) () and 1024-dimensional contextualized word embeddings (ELMo) ( e) (.", "labels": [], "entities": []}, {"text": "In these experiments we use the 5.5 billiontoken version of ELMo trained with Wikipedia and monolingual news from the WMT 2008-2012 corpora.", "labels": [], "entities": [{"text": "WMT 2008-2012 corpora", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.9466000199317932}]}, {"text": "For the experiments with the RST encoders we include the 200-dimensional embeddings obtained from the concatenation of the backward and forward hidden states of the top layers of the DFu or DPa models (RSTEnc) ( f and p, respectively).", "labels": [], "entities": []}, {"text": "As a pilot application we explore the possibility of predicting the acceptance/rejection of papers in computer science conferences 15 based on the annotations generated by the best argument mining models of the experiments described in Section 4.", "labels": [], "entities": [{"text": "predicting the acceptance/rejection of papers in computer science conferences", "start_pos": 53, "end_pos": 130, "type": "TASK", "confidence": 0.8651574850082397}]}, {"text": "Quality assessment metrics that consider elements such as clarity and simplicity, lack of redundancy and comprehensiveness of scientific reporting have been developed for abstracts in other domains-in particular, in life sciences-().", "labels": [], "entities": [{"text": "clarity", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9959908127784729}, {"text": "simplicity", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9915589094161987}]}, {"text": "These instruments were used in studies that show that abstracts with higher formal quality scores-as measured by human experts-are more frequently accepted for presentations in conferences ().", "labels": [], "entities": []}, {"text": "We do not believe that these results can be directly extrapolated to the quality assessment of scientific abstracts in computer science, an area in which full manuscripts are most frequently considered for review and where abstracts have less fixed structures.", "labels": [], "entities": []}, {"text": "Furthermore, clearer links between the formal quality of scientific reporting and the overall quality of research in computer science still need to be established.", "labels": [], "entities": []}, {"text": "Considering all these limitations, we were interested in exploring whether the automatically identified argumentative structure of the abstracts could reflect some quality aspects of the full manuscripts and if this, in turn, could contribute to predict their acceptance in conferences in a specific research area in the field of computer science.", "labels": [], "entities": []}, {"text": "As training set for the acceptance prediction experiment we use 117 abstracts of manuscripts submitted to the Compact Deep Neural Network Representation with Industrial Applications (CDNNRIA) and the Interpretability and Robustness for Audio, Speech and Language (IRASL) workshops held in the context of the Thirtysecond Conference on Neural Information Processing Systems.", "labels": [], "entities": [{"text": "acceptance prediction", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.9852671027183533}, {"text": "Compact Deep Neural Network Representation", "start_pos": 110, "end_pos": 152, "type": "TASK", "confidence": 0.5975531041622162}]}, {"text": "As test set we use 30 abstracts of manuscripts submitted to the Sixth International Conference on Learning Representations).", "labels": [], "entities": [{"text": "International Conference on Learning Representations", "start_pos": 70, "end_pos": 122, "type": "TASK", "confidence": 0.4988276779651642}]}, {"text": "All of the abstracts were collected from the OpenReviews website (.", "labels": [], "entities": []}, {"text": "The distribution of accepted/rejected papers in the training and test sets is shown in  The CDNNRIA, IRASL and ICLR abstracts are used as input to the AFu, ATy and APa models described in Section 4 obtaining sequences of argumentative units, types and parent attachments.", "labels": [], "entities": [{"text": "CDNNRIA", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9525452852249146}, {"text": "IRASL", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.5674756169319153}, {"text": "ICLR", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.747927188873291}]}, {"text": "These sequences are then used as features to train and evaluate a binary classifier aimed at predicting the acceptance or rejection of the corresponding papers.", "labels": [], "entities": []}, {"text": "Considering that we are dealing with a small set of features with a reduced number of potential values for each one, we use a decision tree algorithm for our pilot classification experiment.", "labels": [], "entities": []}, {"text": "In addition to the training and evaluation speed of the algorithm we consider that the higher interpretability of the results-by examining the decision points-can also contribute to assess to what degree the different elements of the predicted argumentative structure are used in the classification.", "labels": [], "entities": []}, {"text": "We use Weka's implementation of the C4.5 algorithm (Quinlan, 1993) (J48) with default parameters with the exception of the confidence factor used for pruning the tree, which was selected evaluating the different models obtained against a random split of 20% of the test set used for validation.", "labels": [], "entities": []}, {"text": "As the training set is not perfectly balanced, we pre-process the data with Weka's ClassBalancer algorithm, which assigns weights to each instance so that each class has the same total weight.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Average F1-measures in epochs 10-100", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9466130137443542}]}, {"text": " Table 3: Accepted/rejected papers in training and test  sets", "labels": [], "entities": [{"text": "Accepted", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9961697459220886}]}, {"text": " Table 4: Example of input instances to the classifier", "labels": [], "entities": []}, {"text": " Table 5: Precision, recall and F1-measures for the ac- ceptance prediction classifiers with and without fine- grained argumentative information", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9965049028396606}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.999140739440918}, {"text": "F1-measures", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9987568855285645}]}]}