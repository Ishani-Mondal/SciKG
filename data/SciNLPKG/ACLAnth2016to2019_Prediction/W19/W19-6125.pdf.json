{"title": [{"text": "Template-free Data-to-Text Generation of Finnish Sports News", "labels": [], "entities": [{"text": "Template-free Data-to-Text Generation of Finnish Sports News", "start_pos": 0, "end_pos": 60, "type": "DATASET", "confidence": 0.5938964741570609}]}], "abstractContent": [], "introductionContent": [{"text": "Automated, or robotic, journalism aims at news generation from structured data sources, either as the final product or as a draft for subsequent postediting.", "labels": [], "entities": []}, {"text": "At present, automated journalism typically focuses on domains such as sports, finance and similar statistics-based reporting, where there is a commercial product potential due to the high volume of news, combined with the expectation of a relatively straightforward task.", "labels": [], "entities": [{"text": "automated journalism", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.7727512419223785}]}, {"text": "News generation systems-especially those deployed in practice-tend to be based on intricate template filling, aiming to give the users the full control of the generated facts, while maintaining a reasonable variability of the resulting text.", "labels": [], "entities": [{"text": "News generation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7294721007347107}]}, {"text": "This comes at the price of having to develop the templates and specify their control logic, neither of which are tasks naturally fitting journalists' work.", "labels": [], "entities": []}, {"text": "Further, this development needs to be repeated for every domain, as the templates are not easily transferred across domains.", "labels": [], "entities": []}, {"text": "Examples of the templatebased news generation systems for Finnish are Voitto 2 by the Finnish Public Service Broadcasting Company (YLE) used for sports news generation, as well as), a hybrid machine learning and template-based system used for election news.", "labels": [], "entities": [{"text": "templatebased news generation", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.6553565363089243}, {"text": "sports news generation", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7384605010350546}]}, {"text": "suggested a neural template generation, which jointly models latent templates and text generation.", "labels": [], "entities": [{"text": "neural template generation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7259221871693929}, {"text": "text generation", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7507929503917694}]}, {"text": "Such a system increases interpretability and controllability of the generation, however, recent sequence-to-sequence systems represent the state-of-the-art in data-totext generation.", "labels": [], "entities": [{"text": "data-totext generation", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.772788792848587}]}, {"text": "In this paper, we report on the development of a news generation system for the Finnish ice hockey news domain, based on sequence-tosequence methods.", "labels": [], "entities": [{"text": "news generation", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.7255788147449493}, {"text": "Finnish ice hockey news domain", "start_pos": 80, "end_pos": 110, "type": "DATASET", "confidence": 0.5258062779903412}]}, {"text": "In order to train such a system, we compile a corpus of news based on over 2000 game reports from the Finnish News Agency STT.", "labels": [], "entities": [{"text": "Finnish News Agency STT", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.8030240386724472}]}, {"text": "While developing this corpus into a form suitable for training of end-to-end systems naturally requires manual effort, we argue that compiling and refining a set of text examples is a more natural way for journalists to interact with the system, in order for them to codify their knowledge and to adapt it for new domains.", "labels": [], "entities": []}, {"text": "Our aim is to generate reports that give an overview of a game based on information inferrable from the statistics.", "labels": [], "entities": []}, {"text": "Such reports can be used either as a basis for further post-editing by a journalist imprinting own insights and background information, or even used directly as a news stream labelled as machine-generated.", "labels": [], "entities": []}, {"text": "In the following, we will introduce the news dataset and the process of its creation, introduce an end-to-end model for news generation, and eval-uate its output respective to the abovementioned objectives.", "labels": [], "entities": [{"text": "news dataset", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.7815587222576141}, {"text": "news generation", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7193082571029663}]}], "datasetContent": [{"text": "An ice hockey game is recorded into statistics in terms of different events occurring during play, such as goals and penalties.", "labels": [], "entities": []}, {"text": "In order to train a model to generate game reports, we need access to these events, as well as example news articles about the game.", "labels": [], "entities": []}, {"text": "Only recently have game statistics become available to the public through a web interface or API, whereas the information has traditionally been recorded as structured text files.", "labels": [], "entities": []}, {"text": "The news corpus from the Finnish News Agency STT 3 includes, among all other news, articles covering ice hockey games in the Finnish leagues during the years 1994-2018.", "labels": [], "entities": [{"text": "news corpus from the Finnish News Agency STT 3", "start_pos": 4, "end_pos": 50, "type": "DATASET", "confidence": 0.8211260967784457}]}, {"text": "In addition to news articles, the corpus also includes the original game statistics text files.", "labels": [], "entities": []}, {"text": "This creates an opportunity to align the game statistics with the corresponding news articles, producing a dataset of over 20 years of ice hockey data with reference news articles for the games.", "labels": [], "entities": []}, {"text": "When automatically pairing the game statistics with news articles using date and team names as a heuristic, we obtain a total of 3,454 games with existing statistics and at least one corresponding news article.", "labels": [], "entities": []}, {"text": "Utilizing real journalistic material poses a challenge in that the articles mix information that can be found directly in the game statistics (e.g., scores and names) with information inferable from the statistics (e.g., statements such as shortly after), information based on background knowledge (e.g., a team's home city or player's position), game insight and judgement based on viewing the game (e.g., expressions such as slapshot or tipping the puck describing the character of a shot), and even player interviews.", "labels": [], "entities": []}, {"text": "Therefore, directly using the limited amount of actual news articles for end-to-end system training becomes problematic.", "labels": [], "entities": []}, {"text": "In our initial experiments the generation model learns to \"hallucinate\" facts, as easily occurs when the target text is too loosely related to the conditioning input.", "labels": [], "entities": []}, {"text": "In order to ensure that the generation model is able to learn to generate accurate descriptions from game statistics, we clean the news corpus by manually align-3 A version of the corpus is available at http://urn.", "labels": [], "entities": []}, {"text": "fi/urn:nbn:fi:lb-2019041501 for academic use.", "labels": [], "entities": []}, {"text": "This observation is also supported by mentioning that their generation model occasionally \"hallucinates factual statements\" that are plausible but false.", "labels": [], "entities": []}, {"text": "ing corresponding text spans with game events detailed in the statistics.", "labels": [], "entities": []}, {"text": "For the sake of comparison, let us consider the Rotowire corpus () containing basketball game summaries and statistics, which was recently released and has become a popular data set for training data-to-text generation systems (cf., e.g.,;;).", "labels": [], "entities": [{"text": "Rotowire corpus", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9400477111339569}, {"text": "training data-to-text generation", "start_pos": 186, "end_pos": 218, "type": "TASK", "confidence": 0.6388374169667562}]}, {"text": "The Rotowire game summaries are straightforward in their style of reporting, focusing on the game at hand and tend for the most part to reference facts in the statistics.", "labels": [], "entities": [{"text": "Rotowire game summaries", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.639315535624822}]}, {"text": "By contrast, our news corpus is more heterogeneous, including both articles focusing on the particular game and articles that take a broader perspective (e.g., describing a player's career).", "labels": [], "entities": []}, {"text": "The STT news articles tend to read in the journalist's voice, putting substantial emphasis on the character of the game, often in colorful language, as well as quoting players and coaches.", "labels": [], "entities": [{"text": "STT news", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.6274160444736481}]}, {"text": "An example of the events available in the game statistics, the actual news article on the game, and how these align, is shown in.", "labels": [], "entities": []}, {"text": "Text spans highlighted with blue color are based on information available in the statistics, all other being external information.", "labels": [], "entities": []}, {"text": "It illustrates the typical portion of a raw article that is not inferrable from the data.", "labels": [], "entities": []}, {"text": "English translations are available fora comparable example in.", "labels": [], "entities": []}, {"text": "To demonstrate the performance of our generation model architecture, we report results on a known dataset with published baselines, namely the E2E NLG Challenge) on end-to-end natural language generation in spoken dialogue systems.", "labels": [], "entities": [{"text": "E2E NLG Challenge", "start_pos": 143, "end_pos": 160, "type": "DATASET", "confidence": 0.8946834802627563}, {"text": "end-to-end natural language generation", "start_pos": 165, "end_pos": 203, "type": "TASK", "confidence": 0.705132931470871}]}, {"text": "The task is to produce a natural language description of a restaurant based on a given meaning representation (MR)-an unordered set of attributes and their values.", "labels": [], "entities": []}, {"text": "The attributes included, among others, the restaurant name, area, food type and rating.", "labels": [], "entities": []}, {"text": "We represent the given MR as a sequence of tokens where each attribute value is embedded into XML-style beginning and end attribute markers, and the order of attributes is kept fixed across the whole dataset.", "labels": [], "entities": []}, {"text": "The target output is a sequence of tokens.", "labels": [], "entities": []}, {"text": "We do not apply any explicit delexicalization steps.", "labels": [], "entities": []}, {"text": "In we measure BLEU (), NIST), METEOR (Lavie and Agarwal, 2007), ROUGE-L () and CIDEr () metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers 7 . Our generation system is compared to the official shared task baseline system, TGen, as well as to the top performing participant system on each score (ST top).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9994107484817505}, {"text": "METEOR", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9941725134849548}, {"text": "ROUGE-L", "start_pos": 64, "end_pos": 71, "type": "METRIC", "confidence": 0.9976510405540466}, {"text": "CIDEr", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9347642660140991}, {"text": "E2E NLG Challenge test data", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.9549406886100769}, {"text": "TGen", "start_pos": 274, "end_pos": 278, "type": "DATASET", "confidence": 0.8314303159713745}]}, {"text": "Our system outperforms the TGen baseline on 3 out of 5 metrics (BLEU, METEOR and ROUGE-L), which is on par with the official shared task results, where not a single one participant system was able to surpass the baseline on all five metrics.", "labels": [], "entities": [{"text": "TGen baseline", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.8088036775588989}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9993323683738708}, {"text": "METEOR", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9949081540107727}, {"text": "ROUGE-L", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9934912323951721}]}, {"text": "On two metrics, BLEU and METEOR, our system outperforms the best shared task participants.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9991641044616699}, {"text": "METEOR", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9795430898666382}]}, {"text": "E2E NLG Challenge evaluation is based on having multiple references for each MR, on average each unique MR in the corpus having 8 reference descriptions.", "labels": [], "entities": [{"text": "E2E NLG Challenge evaluation", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.9383068084716797}]}, {"text": "In the evaluation, the output for each unique MR is compared against all references and the maximum score is used, naturally leading to higher scores.", "labels": [], "entities": []}, {"text": "To have more comparable numbers to our ice hockey corpus, where we have only one reference for each input event, we also include scores obtained by comparing each MR to each of its reference descriptions separately as if they were individual data points (Ours single ref.).", "labels": [], "entities": []}, {"text": "In, we provide evaluation results using the five aforementioned metrics.", "labels": [], "entities": []}, {"text": "We evaluate on event level using gold standard event selection, where each generated event description is compared to its existing reference text.", "labels": [], "entities": []}, {"text": "As the model is trained to produce a tokenized sequence, we apply a detokenizer to be able to compare against the original untokenized reference text.", "labels": [], "entities": []}, {"text": "On the test set, the model achieves a BLEU score of 19.67.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9861400723457336}]}, {"text": "To the extent that different datasets allow comparison, the best reported score on the Rotowire basketball news corpus is 16.50).", "labels": [], "entities": [{"text": "Rotowire basketball news corpus", "start_pos": 87, "end_pos": 118, "type": "DATASET", "confidence": 0.9578417241573334}]}, {"text": "Compared to our earlier E2E baseline experiment, we score lower than our closest comparable reference of 31.90 (with single references), which is understandable due to the much smaller train set size for the hockey corpus (about 13% in size).", "labels": [], "entities": [{"text": "E2E baseline experiment", "start_pos": 24, "end_pos": 47, "type": "DATASET", "confidence": 0.8407150904337565}]}, {"text": "In, we plot the learning curve with increasing sizes of training data in order to illustrate how generation performance benefits from more data.", "labels": [], "entities": []}, {"text": "The learning curve is still steadily increasing when using 100% of the training data currently available, which indicates that more data would most likely further improve the performance.", "labels": [], "entities": [{"text": "learning", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9583281874656677}]}, {"text": "As our objective is practically usable news generation, we carryout a manual evaluation of its output on 59 randomly selected games from the test set, focusing in particular on the corrections that would be necessary to obtain acceptable output.", "labels": [], "entities": [{"text": "news generation", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7566353976726532}]}, {"text": "Example corrections are shown in.", "labels": [], "entities": []}, {"text": "These full game reports are generated by first applying the selection model described in Section 3 to select the events to be included in the report, and then using the text generation model to verbalize each selected event.", "labels": [], "entities": []}, {"text": "The generated texts of the events are then detokenized and concatenated in chronological order into a full game report.", "labels": [], "entities": []}, {"text": "In the minimum edit evaluation, carried out by the annotator who created the news corpus, only factual mistakes and grammatical errors are corrected, resulting in text which may remain awkward or unfluent.  of the generated text compared to its corrected variant as a reference is 5.6% (6.2% disregarding punctuation).", "labels": [], "entities": []}, {"text": "The WER measure is defined as the number of insertions, substitutions, and deletions divided by the total length of the reference, in terms of tokens.", "labels": [], "entities": [{"text": "WER measure", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9425950348377228}]}, {"text": "The measure is the edit distance of the generated text and its corrected variant, directly reflecting the amount of effort needed to correct the generated output.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 19, "end_pos": 32, "type": "METRIC", "confidence": 0.8051562905311584}]}, {"text": "The second human evaluation aimed at judging the acceptability of the output for production use in a news agency.", "labels": [], "entities": []}, {"text": "The output is evaluated in terms of its usability fora news channel labelled as being machine-generated, i.e. not aiming at the level of a human journalist equipped with substantial background information.", "labels": [], "entities": []}, {"text": "The evaluation was carried out by two journalists from the STT agency, who split the 59 games among themselves approximately evenly.", "labels": [], "entities": [{"text": "STT agency", "start_pos": 59, "end_pos": 69, "type": "DATASET", "confidence": 0.8935968577861786}]}, {"text": "The first journalist edited the games to a form corresponding to a draft for subsequent minor post-editing by a human, simulating the use of the generated output as a product where the final customer is expected to do own post-editing before publication.", "labels": [], "entities": []}, {"text": "The second journalist directly edited the news to a state ready for direct publication in a news stream labeled as machine-generated news.", "labels": [], "entities": []}, {"text": "In addition to correcting factual errors, the journalists removed excessive repetition, improved text fluency, as well as occasionally included important facts which the system left ungenerated.", "labels": [], "entities": [{"text": "repetition", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.9814479351043701}]}, {"text": "The WER measured against the output considered ready for post-editing, is 9.9% (11.2% disregarding punctuation), only slightly worse than the evaluation with only the factual and grammatical errors corrected.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9962653517723083}]}, {"text": "The WER measured against the output considered ready for direct release, was 22.0% (24.4% disregarding punctuation).", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9968463778495789}]}, {"text": "In other words, 75-90% of the generated text can be directly used, depending on the expected post-editing effort.", "labels": [], "entities": []}, {"text": "shows two example games along with the generated reports and manual corrections made by the journalist in order to prepare it for publication.", "labels": [], "entities": []}, {"text": "Literal translations from the generated, uncorrected Finnish into English are provided for reference.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Event alignment statistics.", "labels": [], "entities": [{"text": "Event alignment", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.8472540974617004}]}, {"text": " Table 2: Comparison of the original hockey news  and the aligned sentences, including analysis of  lexical diversity.", "labels": [], "entities": []}, {"text": " Table 3: Performance of our generation model on the E2E test set compared to the shared task baseline  (TGen) and winners on each metric (*Juraska et al. (2018), **Puzikov and Gurevych (2018),  \u2020Zhang  et al. (2018),  \u2021Gong", "labels": [], "entities": [{"text": "E2E test set", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.9377508560816447}]}, {"text": " Table 4: Automatic evaluation metrics on hockey  corpus test set.", "labels": [], "entities": [{"text": "hockey  corpus test set", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8189624845981598}]}]}