{"title": [{"text": "UCAM Biomedical translation at WMT19: Transfer learning multi-domain ensembles", "labels": [], "entities": [{"text": "UCAM Biomedical translation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7388890584309896}, {"text": "WMT19", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8159154653549194}]}], "abstractContent": [{"text": "The 2019 WMT Biomedical translation task involved translating Medline abstracts.", "labels": [], "entities": [{"text": "WMT Biomedical translation task", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.8773597329854965}, {"text": "translating Medline abstracts", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.6284394164880117}]}, {"text": "We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles.", "labels": [], "entities": []}, {"text": "We further experiment with an adaptive language-model ensemble weighting scheme.", "labels": [], "entities": []}, {"text": "Our submission achieved the best submitted results on both directions of English-Spanish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) in the biomedical domain presents challenges in addition to general domain translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8320465783278147}, {"text": "general domain translation", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.7825463612874349}]}, {"text": "Firstly, available corpora are relatively small, exacerbating the effect of noisy or poorly aligned training data.", "labels": [], "entities": []}, {"text": "Secondly, individual sentences within a biomedical document may use specialist vocabulary from small domains like health or statistics, or may contain generic language.", "labels": [], "entities": []}, {"text": "Training to convergence on a single biomedical dataset may therefore not correspond to good performance on arbitrary biomedical test data.", "labels": [], "entities": []}, {"text": "Transfer learning is an approach in which a model is trained using knowledge from an existing model (.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9532177746295929}]}, {"text": "Transfer learning typically involves initial training on a large, general domain corpus, followed by fine-tuning on the domain of interest.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9656230211257935}]}, {"text": "We apply transfer learning iteratively on datasets from different domains, obtaining strong models that cover two domains for both directions of the English-German language pair, and three domains for both directions of EnglishSpanish.", "labels": [], "entities": []}, {"text": "The domain of individual documents in the 2019 Medline test dataset is unknown, and may vary sentence-to-sentence.", "labels": [], "entities": [{"text": "Medline test dataset", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.9528565208117167}]}, {"text": "Evenly-weighted ensembles of models from different domains can give good results in this case).", "labels": [], "entities": []}, {"text": "However, we suggest a better approach would take into account the likely domain, or domains, of each test sentence.", "labels": [], "entities": []}, {"text": "We therefore investigate applying Bayesian Interpolation for language-model based multi-domain ensemble weighting.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report on two language pairs: Spanish-English (es-en) and English-German (en-de).", "labels": [], "entities": []}, {"text": "lists the data used to train our biomedical domain evaluation systems.", "labels": [], "entities": []}, {"text": "For en2de and de2en we additionally reuse strong general domain models trained on the WMT19 news data, including filtered Paracrawl.", "labels": [], "entities": [{"text": "WMT19 news data", "start_pos": 86, "end_pos": 101, "type": "DATASET", "confidence": 0.9801689187685648}, {"text": "Paracrawl", "start_pos": 122, "end_pos": 131, "type": "DATASET", "confidence": 0.9490953683853149}]}, {"text": "Details of data preparation and filtering for these models are discussed in . For each language pair we use the same training data in both directions, and use a 32K-merge source-target BPE vocabulary ( ) trained on the 'base' domain training data (news for en-de, Scielo health for es-en) For the biomedical data, we preprocess the data using Moses tokenization, punctuation normalization and truecasing.", "labels": [], "entities": [{"text": "punctuation normalization", "start_pos": 363, "end_pos": 388, "type": "TASK", "confidence": 0.6366567760705948}]}, {"text": "We then use a series of simple heuristics to filter the parallel datasets: \u2022 Detected language filtering using the Python langdetect package . In addition to mislabelled sentences, this step removes many sentences which are very short or have a high proportion of punctuation or HTML tags.", "labels": [], "entities": [{"text": "Detected language filtering", "start_pos": 77, "end_pos": 104, "type": "TASK", "confidence": 0.7775895992914835}]}, {"text": "\u2022 Remove sentences containing more than 120 tokens or fewer than 3.", "labels": [], "entities": []}, {"text": "\u2022 Remove duplicate sentence pairs \u2022 Remove sentences where the ratio of source to target tokens is less than 1:3.5 or more than 3.5:1 \u2022 Remove pairs where more than 30% of either sentence is the same token.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Biomedical training and validation data used in the evaluation task. For both language pairs identical data  was used in both directions.", "labels": [], "entities": []}, {"text": " Table 2: Validation BLEU for English-Spanish models with transfer learning. We use the final three models in our  submission.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9539961218833923}]}, {"text": " Table 3: Validation and test BLEU for models used in English-Spanish language pair submissions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9929353594779968}]}, {"text": " Table 4: Validation and test BLEU for models used in English-German language pair submissions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.992845356464386}]}, {"text": " Table 5: Comparing uniform ensembles and BI with  varying smoothing factor on the WMT19 test data.", "labels": [], "entities": [{"text": "BI", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.999061644077301}, {"text": "WMT19 test data", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.9663199186325073}]}]}