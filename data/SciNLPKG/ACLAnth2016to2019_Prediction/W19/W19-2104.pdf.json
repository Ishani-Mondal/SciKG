{"title": [], "abstractContent": [{"text": "This work introduces a general method for automatically finding the locations where political events in text occurred.", "labels": [], "entities": [{"text": "automatically finding the locations where political events in text occurred", "start_pos": 42, "end_pos": 117, "type": "TASK", "confidence": 0.7093728244304657}]}, {"text": "Using a novel set of 8,000 labeled sentences, I create a method to link automatically extracted events and locations in text.", "labels": [], "entities": []}, {"text": "The model achieves human level performance on the annotation task and outperforms previous event geolocation systems.", "labels": [], "entities": []}, {"text": "It can be applied to most event extraction systems across geographic contexts.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7773970067501068}]}, {"text": "I formalize the event-location linking task, describe the neural network model, describe the potential uses of such a system in political science, and demonstrate a workflow to answer an open question on the role of conventional military offensives in causing civilian casualties in the Syrian civil war.", "labels": [], "entities": []}], "introductionContent": [{"text": "Researchers in social science, and especially in comparative politics and security studies, are increasingly turning toward micro-level data, with subnational variation at very fine resolutions becoming a major source of empirical puzzles and evidence in these fields.", "labels": [], "entities": []}, {"text": "At the same time, text data is becoming one of the most important sources of new data in social science.", "labels": [], "entities": []}, {"text": "I develop and describe a method that enables researchers to connect these two trends, automatically linking events extracted from text to the specific locations where they are reported to occur.", "labels": [], "entities": []}, {"text": "Specifically, I develop a method that, given a sentence and an event's verb in the sentence, will return the place names from the sentence where the event took place.", "labels": [], "entities": []}, {"text": "Formulated as a general task, this is an unsolved problem in both political science and computer science.", "labels": [], "entities": []}, {"text": "Drawing on a set of 8,000 hand-labeled sentences, I train a recurrent neural network that draws on a rich set of linguistic features to label a sequence of text with labels for whether the word is a location word corresponding to a specified verb.", "labels": [], "entities": []}, {"text": "Measured by token, the model produces precision and recall scores of over 0.83, compared with a rule-based model's 0.25-0.29.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9995988011360168}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.998767614364624}]}, {"text": "A software implementation and example workflow is provided.", "labels": [], "entities": []}, {"text": "I provide an example application, creating anew dataset on the locations of military offensives in Syria and contributing to an ongoing debate in conflict studies on the causes of civilian casualties in civil war.", "labels": [], "entities": []}, {"text": "The model is general enough for applied researchers to use in other contexts, including the study of protests, political mobilization, political violence, and electoral politics.", "labels": [], "entities": []}, {"text": "The new shared dataset will enable other researchers in NLP to contribute to this task and the wider research project of better extracting political relationships from text.", "labels": [], "entities": [{"text": "extracting political relationships from text", "start_pos": 128, "end_pos": 172, "type": "TASK", "confidence": 0.900127375125885}]}], "datasetContent": [{"text": "I evaluate these and several existing models on the task and the English-language dataset I introduce.", "labels": [], "entities": []}, {"text": "To evaluate the performance of each model, I assess accuracy on both a per-token and per-sentence basis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9995447993278503}]}, {"text": "For per-token accuracy, I take a common approach of calculating the precision and recall in the evaluation sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9738571643829346}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.999016523361206}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9957512617111206}]}, {"text": "Each model is evaluated on how well it cancan produce, for each token w i \u2208 X whether w i is an event location for event k.", "labels": [], "entities": []}, {"text": "This evaluation approach allows \"partial credit\" for models that that may miss or falsely include a single token and is a common approach to evaluating sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.70065904657046}]}, {"text": "I also include a second measure that more closely matches real-word accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.980290412902832}]}, {"text": "Profile () performs next worst, with an token-level F1 score of 0.37.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9204170107841492}]}, {"text": "The model is unable to to vary its location prediction by event type, meaning that it will correctly locate at best one event's location in a multi-event, multilocation sentence.", "labels": [], "entities": []}, {"text": "Profile also returns only one location per sentence, lowering its accuracy on events that occur in multiple locations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9988746047019958}]}, {"text": "Profile's intended use case is on longer pieces of text: its able for this more general task of linking arbitrary locations and events.", "labels": [], "entities": []}, {"text": "Finally, the replication code provided is not easily applicable to new datasets, only to run the initial experiments.", "labels": [], "entities": []}, {"text": "8 These numbers are performance on the PropBank dataset, not on the dataset I create.", "labels": [], "entities": [{"text": "PropBank dataset", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9817438721656799}]}, {"text": "Performance of Chung et al's model on their corpus of 48 OntoNotes documents.", "labels": [], "entities": [{"text": "OntoNotes documents", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.7876029014587402}]}, {"text": "The maximum values achieved for precision, recall, and F1 across their models are reported here.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.999725878238678}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.999592125415802}, {"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9996106028556824}]}, {"text": "Note that the results on my model report per-token precision and recall, while they report per-location precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9778099656105042}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9973925352096558}, {"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.856121838092804}, {"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9954221844673157}]}, {"text": "poor performance on this task should betaken only as an indication of its ability to geolocate events in text, not on its ability to find the primary \"focus\" (D') location of apiece of text.", "labels": [], "entities": [{"text": "betaken", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.619028627872467}]}, {"text": "PropBank is included as a point of comparison.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9688544273376465}]}, {"text": "The PropBank values are reported for the ArgM-LOC label in Palmer,.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.6034030914306641}, {"text": "ArgM-LOC label in Palmer", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.7779970467090607}]}, {"text": "The framing of the location task in PropBank is quite different than the generalized event-location linking task I introduce, as I describe above.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.9254747629165649}]}, {"text": "The reported F1 score of 0.47 can betaken as a reasonable baseline performance on an event-location linking task.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9873150587081909}, {"text": "betaken", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.6000807285308838}]}, {"text": "Chung et al's (2017) accuracy on their dataset and version of the task is the best of any prior model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9992959499359131}]}, {"text": "The LSTM model performs much better than the CNN model, even after extensive tuning for the CNN model.", "labels": [], "entities": []}, {"text": "Inspection of the CNN model's output (not included) indicates that the model seems to not learn long-distance relationships well, and failed to appropriately change probability weights when the verb of interest changed.", "labels": [], "entities": []}, {"text": "The LSTM model, in contrast, performs very well and is very sensitive to changes in the input verb: the same sentence with two different flagged verbs of interest will produce quite different results for those events' location.", "labels": [], "entities": []}, {"text": "The LSTM and CNN are comparable in training time.", "labels": [], "entities": [{"text": "LSTM", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7475386261940002}, {"text": "CNN", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.6935755014419556}]}, {"text": "Notably, the LSTM also outperforms an estimate of expected human performance on the event-location linking task.", "labels": [], "entities": []}, {"text": "While humans are able to pickup on nuance and deal with grammatical complexity that machines still cannot handle, humans are also unsuited to the tedium of labeling thousands of sentences and maybe susceptible to drift in their definitions or understanding of the task.", "labels": [], "entities": [{"text": "labeling thousands of sentences", "start_pos": 156, "end_pos": 187, "type": "TASK", "confidence": 0.8493793904781342}]}, {"text": "Not only is the automated method vastly cheaper and faster than a human process, it does so with accuracy at least as good.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9992133378982544}]}, {"text": "shows the results of an ablation process on the best performing LSTM model, revealing that some features are more important than others across several random partitions and retrainings of the model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Per-token precision, recall, and F1 scores, and  full-sentence accuracy for the word distance baseline  model, expected human performance, existing results  from the literature, and new model-based approaches.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9286901354789734}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9995438456535339}, {"text": "F1 scores", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.981353223323822}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9814743399620056}]}]}