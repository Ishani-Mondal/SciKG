{"title": [{"text": "Constrained Sequence-to-sequence Semitic Root Extraction for Enriching Word Embeddings", "labels": [], "entities": [{"text": "Sequence-to-sequence Semitic Root Extraction", "start_pos": 12, "end_pos": 56, "type": "TASK", "confidence": 0.8282495886087418}]}], "abstractContent": [{"text": "In this paper, we tackle the problem of \"root extraction\" from words in the Semitic language family.", "labels": [], "entities": [{"text": "root extraction\" from words", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.874242639541626}]}, {"text": "A challenge in applying natural language processing techniques to these languages is the data sparsity problem that arises from their rich internal morphology, where the substructure is inherently non-concatenative and morphemes are interdigitated in word formation.", "labels": [], "entities": [{"text": "word formation", "start_pos": 251, "end_pos": 265, "type": "TASK", "confidence": 0.7091268002986908}]}, {"text": "While previous automated methods have relied on human-curated rules or multi-class classification, they have not fully lever-aged the various combinations of regular, sequential concatenative morphology within the words and the internal interleaving within tem-platic stems of roots and patterns.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.7102561742067337}]}, {"text": "To address this, we propose a constrained sequence-to-sequence root extraction method.", "labels": [], "entities": [{"text": "sequence-to-sequence root extraction", "start_pos": 42, "end_pos": 78, "type": "TASK", "confidence": 0.7022875746091207}]}, {"text": "Experimental results show our constrained model outper-forms a variety of methods at root extraction.", "labels": [], "entities": [{"text": "root extraction", "start_pos": 85, "end_pos": 100, "type": "TASK", "confidence": 0.7462185919284821}]}, {"text": "Furthermore, by enriching word embeddings with resulting decompositions, we show improved results on word analogy, word similarity , and language modeling tasks.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 101, "end_pos": 113, "type": "TASK", "confidence": 0.7780784964561462}, {"text": "word similarity", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.756476879119873}, {"text": "language modeling tasks", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.79517329732577}]}], "introductionContent": [{"text": "The Semitic languages area language family commonly spoken throughout North Africa, the Horn of Africa, the Arabian peninsula, and the regions between.", "labels": [], "entities": []}, {"text": "With approximately 500 million speakers, the proliferation of large online text collections of such news articles, social media, digitized literature, and web blogs has created a wealth of data offering challenges and opportunities for semantic understanding of Semitic texts.", "labels": [], "entities": [{"text": "semantic understanding of Semitic texts", "start_pos": 236, "end_pos": 275, "type": "TASK", "confidence": 0.8707204937934876}]}, {"text": "In these languages, a majority of words are derived from a small number of mostly triliteral consonantal roots, with some quadriliteral roots and a trace number of biliteral and quintliteral roots.", "labels": [], "entities": []}, {"text": "It is estimated that two of the most prominent Semitic * *Equal contribution languages, Arabic and Hebrew, possess approximately 10,000 and 3,000 roots, respectively.", "labels": [], "entities": []}, {"text": "As such, root identification of a given Semitic word is often an important task in morphological analysis and the first step to morphological decomposition.", "labels": [], "entities": [{"text": "root identification of a given Semitic word", "start_pos": 9, "end_pos": 52, "type": "TASK", "confidence": 0.8475958023752485}, {"text": "morphological analysis", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8416539132595062}, {"text": "morphological decomposition", "start_pos": 128, "end_pos": 155, "type": "TASK", "confidence": 0.861928254365921}]}, {"text": "Morphological analysis of Semitic languages poses a unique challenge to traditional NLP techniques due to the non-contiguous morphology inherent in these languages.", "labels": [], "entities": [{"text": "Morphological analysis of Semitic languages", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.9049825429916382}]}, {"text": "This morphology is best described as the application of a pattern resulting in the interdigitation of morphemes within a single root to form derivative words.", "labels": [], "entities": []}, {"text": "This fusional morphology allows for many surface form words derived from the same single root, but with different, yet abstractly-related semantic meanings depending on constituent morphemes.", "labels": [], "entities": []}, {"text": "Because many surface words can be formed through this root and pattern word formation process, and the root's characters may not necessarily be contiguously situated within each resultant surface word, morpheme boundaries are often difficult to identify.", "labels": [], "entities": []}, {"text": "Unlike other fusional languages, the Semitic languages are unique in that the word formation process follows a highly-structured process of adding vowels and consonants to roots.", "labels": [], "entities": [{"text": "word formation", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7355418354272842}]}, {"text": "This word formation process consists of a fixed number of slots for different morphemes, which are fixed in their position and order relative to each other.", "labels": [], "entities": [{"text": "word formation", "start_pos": 5, "end_pos": 19, "type": "TASK", "confidence": 0.7718474268913269}]}, {"text": "As such, these languages contain significant sequential (albeit not necessarily contiguous) substructure.", "labels": [], "entities": []}, {"text": "In this work, we propose to leverage this sequential substructure to improve the root extraction process and morphological decomposition.", "labels": [], "entities": [{"text": "root extraction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.7193371504545212}, {"text": "morphological decomposition", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.7674878835678101}]}, {"text": "Morphological analysis is essential in working with Semitic languages as well as other highlyinflectional languages due to data sparsity.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8878555297851562}]}, {"text": "For instance, previous research has shown that many text corpora demonstrate long-tail distributions in relation to word frequency.", "labels": [], "entities": []}, {"text": "This long-tail often results in corpora with many infrequent words, with 40% \u2212 60% of words appearing just once).", "labels": [], "entities": []}, {"text": "We can verify this for Arabic in, where, on a Wikipedia monolingual Arabic corpus (described in Section 5.1), approximately 80% of words occur fewer than five times and 60% occur once.", "labels": [], "entities": [{"text": "Wikipedia monolingual Arabic corpus", "start_pos": 46, "end_pos": 81, "type": "DATASET", "confidence": 0.626152902841568}]}, {"text": "To process such long-tailed corpora, it is necessary to exploit finer-granularity, highlyshared substructures between words that can be used to infer semantic meaning.", "labels": [], "entities": []}, {"text": "In, we look at a selection of Arabic words sharing the common root --(transliteration K-T-B), which means to \"write\".", "labels": [], "entities": []}, {"text": "These words are formed by appending different prefixes, suffixes, and other templatic interleavings of morphemes within the root.", "labels": [], "entities": []}, {"text": "Despite the many surface words, the derivations share a semantic relationship based on the root, as well as other concatenative and interdigitated templatic morphemes.", "labels": [], "entities": []}, {"text": "Additionally, as seen in the example, the root word's characters are not necessarily contiguous within the word; this is due to the non-concatenative templatic process whereby morphemes are inserted between characters of the root as part of the word formation process.", "labels": [], "entities": [{"text": "word formation", "start_pos": 245, "end_pos": 259, "type": "TASK", "confidence": 0.7093939632177353}]}, {"text": "Finally, not all characters in the root are necessarily found in the final surface-form of the word as some root characters can be dropped.", "labels": [], "entities": []}, {"text": "Traditional concatenative morphological analyzers struggle to identify and extract roots precisely because root word characters are not necessarily contiguous or even present in the surface word.", "labels": [], "entities": []}, {"text": "To address these challenges, we present a supervised root extraction algorithm that, given a word, directly extracts the root with high accuracy.", "labels": [], "entities": [{"text": "root extraction", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7822719514369965}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9903115034103394}]}, {"text": "Given this root and the original word, we demonstrate how the templatic pattern-based word formation process that transforms the root to the original word can be used for further morphological decomposition.", "labels": [], "entities": [{"text": "templatic pattern-based word formation", "start_pos": 62, "end_pos": 100, "type": "TASK", "confidence": 0.6753077208995819}]}, {"text": "We demonstrate that our method outperforms unsupervised rule-based root extraction methods ( and our seq2seq classifier outperforms general multiclass classifiers).", "labels": [], "entities": []}, {"text": "As a testament to the utility of root extraction, we demonstrate how one can leverage the root information alongside a simple slot-based morphological decomposition to improve upon word embedding representations as evaluated through word similarity, word analogy, and language modeling tasks.", "labels": [], "entities": [{"text": "root extraction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7464735805988312}, {"text": "word analogy", "start_pos": 250, "end_pos": 262, "type": "TASK", "confidence": 0.7775565385818481}]}], "datasetContent": [{"text": "We introduce the datasets and methods for comparison used.", "labels": [], "entities": []}, {"text": "We then describe evaluations for root extraction and embedding quality.", "labels": [], "entities": [{"text": "root extraction", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7202628701925278}]}, {"text": "We use the following datasets and ground-truth labels for evaluation purposes: \u2022 Arabic Word & Root Pairs: 140K words along associated with 11K roots from dictionary (al Zabidi and Murthada, 1886).", "labels": [], "entities": []}, {"text": "\u2022 Hebrew Word & Root Pairs.", "labels": [], "entities": []}, {"text": "11.5K words associated with approximately 500 roots from Wiktionary 1 and human curation.", "labels": [], "entities": []}, {"text": "Wikipedia corpus with 274K articles and 62.5M tokens and 1.26M unique words.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9651727974414825}]}, {"text": "1 wiktionary.org For baseline methods to compare against our proposed constrained seq2seq (Constrain-S2S), we evaluate against three standard multiclass classification models: (1) a standard convolutional neural network, CNN-Class,), a GRU model, GRU-Class, and a bi-directional GRU model, BiGRU-Class.", "labels": [], "entities": [{"text": "BiGRU-Class", "start_pos": 290, "end_pos": 301, "type": "METRIC", "confidence": 0.8878839015960693}]}, {"text": "In addition, we compare against two unconstrained seq2seq models, encoder-decoder models using GRUs, GRU-S2S and bi-directional GRUs, BiGRU-S2S.", "labels": [], "entities": []}, {"text": "Finally, for Arabic, we evaluate against three unsupervised Arabic root-extraction algorithms from the literature: Tashaphyne, ISRI, and Khoja.", "labels": [], "entities": []}, {"text": "To evaluate on the quality of the resultant morphological decomposition, we compare against three variants of embeddings: (1) SkipGram (2) FastText (3) RootVec (Embedding enriched with solely the root) .  Given our comprehensive dataset of Arabic roots and human-curated evaluation set of Arabic word embeddings, we show the effectiveness of enriching Arabic word embeddings with their morphological decompositions via a word analogy task.", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 126, "end_pos": 134, "type": "DATASET", "confidence": 0.8669869899749756}]}, {"text": "The goal of said task is to identify the best value for D in analogies of the form \"A is to B as C is to D\".", "labels": [], "entities": []}, {"text": "After training each embedding model on the Arabic Wikipedia dataset, we use an analogy dataset () curated for methodological evaluation of Arabic word embeddings.", "labels": [], "entities": [{"text": "Arabic Wikipedia dataset", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.7446484267711639}]}, {"text": "We further differentiate the analogies into two categories: (1) morphemic analogies (e.g. plurals, tense or gender) where a derivational or inflectional morpheme is inserted, removed, or replaced while the root remains unchanged, and (2) semantic analogies where the root itself changes between the analogous pairs (e.g. bird is to fly as fish is to swim).", "labels": [], "entities": []}, {"text": "As seen in, embeddings that utilize morphemes or subword-level features perform significantly better at morphemic analogies than do SkipGram word embeddings.", "labels": [], "entities": []}, {"text": "This does not extend to semantic analogies where all methods appear to degrade with the use of morpheme and subword-level enrichment.", "labels": [], "entities": []}, {"text": "This is not surprising since, under the vector algebra that is used to compute the word analogies, the summation of the morphemes used to enrich the embeddings captures morphemic relationships but not necessarily semantic ones.", "labels": [], "entities": []}, {"text": "This can be seen in the performance gap between the morpheme-enriched embeddings and SkipGram.", "labels": [], "entities": [{"text": "SkipGram", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.964516818523407}]}, {"text": "Unlike the other methods, Templatic embeddings based on constrained roots maintains comparable performance to SkipGram on the semantic analogies while demonstrating superior performance on the morphemic analogies.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Root Extraction Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9374793171882629}]}]}