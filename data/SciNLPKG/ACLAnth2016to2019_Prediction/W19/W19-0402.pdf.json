{"title": [{"text": "A Type-coherent, Expressive Representation as an Initial Step to Language Understanding", "labels": [], "entities": [{"text": "Language Understanding", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.7139278054237366}]}], "abstractContent": [{"text": "A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7425300776958466}, {"text": "semantic parsing", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7180548310279846}]}, {"text": "Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable.", "labels": [], "entities": []}, {"text": "This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form fora semantic representation that balances these needs.", "labels": [], "entities": []}, {"text": "ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution.", "labels": [], "entities": []}, {"text": "This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannota-tor agreement of 0.88 on confident annotations.", "labels": [], "entities": []}, {"text": "We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7398091852664948}]}], "introductionContent": [{"text": "Episodic Logic (EL) is a semantic representation extending FOL, designed to closely match the expressivity and surface form of natural language and to enable deductive inference, uncertain inference, and NLog-like inference.", "labels": [], "entities": [{"text": "Episodic Logic (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5839343249797821}, {"text": "NLog-like inference", "start_pos": 204, "end_pos": 223, "type": "TASK", "confidence": 0.8027199506759644}]}, {"text": "developed a system that transforms annotated WordNet glosses into EL axioms which were competitive with state-of-the-art lexical inference systems while achieving greater expressivity.", "labels": [], "entities": []}, {"text": "While EL is representationally appropriate for language understanding, the current EL parser is too unreliable for general text: The phrase structures produced by the underlying Treebank parser leave many ambiguities in the semantic type structure, which are disambiguated incorrectly by the hand-coded compositional rules; moreover, errors in the phrase structures can further disrupt the resulting logical forms (LFs).", "labels": [], "entities": [{"text": "language understanding", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.7195232063531876}]}, {"text": "discuss the limitations of the existing parser as a starting point for logically interpreting glosses of WordNet verb entries.", "labels": [], "entities": [{"text": "interpreting glosses of WordNet verb entries", "start_pos": 81, "end_pos": 125, "type": "TASK", "confidence": 0.6349855264027914}]}, {"text": "In order to build a better EL parser, it seems natural to take advantage of recent advances in corpus-based parsing techniques.", "labels": [], "entities": [{"text": "EL parser", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.8350900709629059}]}, {"text": "This document describes a type-coherent initial LF, or unscoped logical forms (ULF), for EL which captures the predicate-argument structure in the EL semantic types and is the first critical step in fullyresolved semantic interpretation of sentences.", "labels": [], "entities": []}, {"text": "Montague's profoundly influential work demonstrates that systematic assignments of appropriate semantic types to words and phrases allows us to view language as akin to formal logic, with meanings determined compositionally from syntactic structures.", "labels": [], "entities": []}, {"text": "This view of language directly supports inferences, at least to the extent that we can resolve -or are prepared to tolerate -ambiguity, context-dependence, and indexicality, towards which semantic types are agnostic.", "labels": [], "entities": []}, {"text": "ULF takes a minimal step across the syntax-semantics interface by doing exactly this -selecting the semantic types of words within EL.", "labels": [], "entities": []}, {"text": "Thus ULFs are amenable to corpus-construction and statistical parsing using techniques similar to those used for syntax, and they enable generation of context-dependent structural inferences.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7908474504947662}]}, {"text": "The nature of these inferences is discussed in more detail in Section 3.4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran a timing study and an interannotator agreement (IA) study to quantify the efficacy of the presented annotation framework.", "labels": [], "entities": [{"text": "timing", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9352331161499023}]}, {"text": "We timed 80 annotations of the Tatoeba dataset and found the average annotation speed to be 8 min/sent with 4 min/sent among the two experts and 11 min/sent among the three trainees that participated.", "labels": [], "entities": [{"text": "Tatoeba dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.9708792567253113}]}, {"text": "AMRs reportedly took on average 10 min/sent).", "labels": [], "entities": [{"text": "AMRs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.5585777163505554}]}, {"text": "In the IA study five annotators each annotated between 18 and 23 sentences from the same set of 23 sentences, marking their certainty of the annotations as they normally would.", "labels": [], "entities": [{"text": "IA study", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.7295193672180176}, {"text": "certainty", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9968892931938171}]}, {"text": "The sentences were sampled from the four datasets listed in.", "labels": [], "entities": []}, {"text": "The mean and standard deviation of sentence length were 15.3 words and 10.8 words, respectively.", "labels": [], "entities": []}, {"text": "We computed a similarity score between two annotations using EL-smatch, a generalization of smatch ) which handles non-atomic operators.", "labels": [], "entities": []}, {"text": "The document-level ELsmatch score between all annotated sentence pairs was 0.70.", "labels": [], "entities": []}, {"text": "When we restricted the analysis to just annotations that were marked certain, the agreement rose to 0.78.", "labels": [], "entities": []}, {"text": "The complete pairwise scores are shown in.", "labels": [], "entities": []}, {"text": "Notice that annotators 1, 2, and 3 had very high agreement with each other.", "labels": [], "entities": []}, {"text": "If we restrict the agreement to just those three annotators, the full and certain-subset scores are 0.79 and 0.88, respectively.", "labels": [], "entities": []}, {"text": "Out of all the annotations, less than a third were marked as uncertain or incomplete.", "labels": [], "entities": []}, {"text": "AMR annotations reportedly have annotator vs consensus IA of 0.83 for newswire and 0.79 for web text.", "labels": [], "entities": [{"text": "IA", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.8955090641975403}]}, {"text": "This study also demonstrates that the certainty marking indeed reflects the quality of the annotation, thus performing the role we intended.", "labels": [], "entities": [{"text": "certainty marking", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.9414687752723694}]}, {"text": "Also, based on the high agreement between annotators 1, 2, and 3, we can conclude that consistent ULF annotations across multiple annotators is possible.", "labels": [], "entities": [{"text": "ULF annotations", "start_pos": 98, "end_pos": 113, "type": "TASK", "confidence": 0.897314190864563}]}, {"text": "However, the lower scores of annotators 4 and 5, even in annotations marked ascertain, indicates room for improvement in the annotation guidelines and training of some annotators.", "labels": [], "entities": []}, {"text": "We have so far collected 927 certain annotations and have 1,580 in total.", "labels": [], "entities": []}, {"text": "The full annotation breakdown is in.", "labels": [], "entities": []}, {"text": "We started with the English portion of the Tatoeba dataset (https://tatoeba.org/ eng/), a crowd-sourced translation dataset.", "labels": [], "entities": [{"text": "Tatoeba dataset", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.976870596408844}]}, {"text": "This source tends to have shorter sentences, but they are more varied in topic and form.", "labels": [], "entities": []}, {"text": "We then added text from Project Gutenberg (http://gutenberg.org), the UIUC Question Classification dataset (, and the Discourse Graphbank (.", "labels": [], "entities": [{"text": "UIUC Question Classification dataset", "start_pos": 70, "end_pos": 106, "type": "DATASET", "confidence": 0.9358488768339157}]}, {"text": "Preliminary parsing experiments on a small dataset (900 sentences) show promising results and we expect to be able to build an accurate parser with a moderately-sized dataset and representation-specific engineering.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Current sentence annotation counts", "labels": [], "entities": [{"text": "sentence annotation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7383663058280945}]}, {"text": " Table 2. Notice  that annotators 1, 2, and 3 had very high agreement with  each other. If we restrict the agreement to just those three  annotators, the full and certain-subset scores are 0.79 and  0.88, respectively. Out of all the annotations, less than a  third were marked as uncertain or incomplete. AMR an- notations reportedly have annotator vs consensus IA of 0.83 for newswire and 0.79 for web text", "labels": [], "entities": [{"text": "IA", "start_pos": 363, "end_pos": 365, "type": "METRIC", "confidence": 0.9455795884132385}]}, {"text": " Table 1. We started with the English portion of the Tatoeba dataset (https://tatoeba.org/  eng/), a crowd-sourced translation dataset. This source tends to have shorter sentences, but they are more  varied in topic and form. We then added text from Project Gutenberg (http://gutenberg.org), the  UIUC Question Classification dataset (", "labels": [], "entities": [{"text": "Tatoeba dataset", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9805580973625183}, {"text": "UIUC Question Classification dataset", "start_pos": 297, "end_pos": 333, "type": "DATASET", "confidence": 0.9530571103096008}]}]}