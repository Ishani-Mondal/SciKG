{"title": [{"text": "MSnet: A BERT-based Network for Gendered Pronoun Resolution", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9826860427856445}, {"text": "Gendered Pronoun Resolution", "start_pos": 32, "end_pos": 59, "type": "TASK", "confidence": 0.6473629474639893}]}], "abstractContent": [{"text": "The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing.", "labels": [], "entities": [{"text": "BERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.904954195022583}]}, {"text": "For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT.", "labels": [], "entities": [{"text": "gendered pronoun resolution task", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.747466117143631}, {"text": "BERT", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9606518745422363}]}, {"text": "This model is a type of mention score classi-fier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities.", "labels": [], "entities": []}, {"text": "In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set.", "labels": [], "entities": [{"text": "gendered pronoun resolution task", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.7531720697879791}]}, {"text": "Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference resolution is an essential field of natural language processing ( and has been widely used in many systems such as dialog system, relation extraction () and question answer).", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9292136430740356}, {"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6734912395477295}, {"text": "relation extraction", "start_pos": 142, "end_pos": 161, "type": "TASK", "confidence": 0.7841099202632904}]}, {"text": "Up to now, various models for coreference resolution have been proposed, and they can be generally categorized as (1) mention-pair classifier model, (2) entity-centric model, (3) ranking model (.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9699563384056091}]}, {"text": "However, some of these models implicate gender bias.", "labels": [], "entities": []}, {"text": "To address this, presented and released Gendered Ambiguous Pronouns (GAP) dataset.", "labels": [], "entities": []}, {"text": "Recent work indicated that the pre-trained language representation models benefit to the coreference resolution ( ).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.9624866545200348}]}, {"text": "In the past years, the development of deep learning methods of language representation was swift, and the newer methods were shown to have significant effects on improving other natural language processing tasks().", "labels": [], "entities": [{"text": "language representation", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.7344384342432022}]}, {"text": "The latest one is Bidirectional Encoder Representations from Transformers (BERT), which is the cornerstone of the state of the art models in many tasks.", "labels": [], "entities": [{"text": "Bidirectional Encoder Representations from Transformers (BERT)", "start_pos": 18, "end_pos": 80, "type": "TASK", "confidence": 0.6761576682329178}]}, {"text": "In this paper, I present a novel neural network model based on the pre-trained BERT for the gendered pronoun resolution task.", "labels": [], "entities": [{"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9959343671798706}, {"text": "gendered pronoun resolution task", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.72415491938591}]}, {"text": "The model is a kind of mention score classifier, and it is named as Mention Score Network (MSNet in short) and trained on the public GAP dataset.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.8048639595508575}]}, {"text": "In particular, the model adopts an attention mechanism to compute the contextual representation of the entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities.", "labels": [], "entities": []}, {"text": "Since the MSnet cannot be tuned in a general way, I employ a two-step strategy to achieve the tuning-fine, which tunes the MSnet with freezing BERT firstly and then tunes them together.", "labels": [], "entities": [{"text": "MSnet", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.9549873471260071}, {"text": "tuning-fine", "start_pos": 94, "end_pos": 105, "type": "METRIC", "confidence": 0.9923352599143982}, {"text": "BERT", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9819825887680054}]}, {"text": "Two variants of MSnet are submitted in the gendered pronoun resolution task, and their logarithmic loss of local 5-fold cross-validation of train dataset is 0.3033 and 0.3042 respectively.", "labels": [], "entities": [{"text": "MSnet", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.9357640147209167}, {"text": "gendered pronoun resolution task", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6835545375943184}]}, {"text": "Moreover, in stage 2 of the task, they acquired the score at 0.17289 and 0.18361 respectively, by averaging the predictions on the test dataset, and won the 2nd place in the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "I train the model on the Kaggle platform by using scripts kernel which using the computational environment from the docker-python . I employ pytorch as the deep learning framework, and the pytorch-pretrained-BERT package 2 to load and tune the pre-trained BERT model.", "labels": [], "entities": [{"text": "Kaggle platform", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.9152695536613464}, {"text": "BERT", "start_pos": 256, "end_pos": 260, "type": "METRIC", "confidence": 0.9669044613838196}]}, {"text": "The GAP Coreference Dataset) has 4454 records and officially split into three parts: development set (2000 records), test set (2000 records), and validation set (454 records).", "labels": [], "entities": [{"text": "GAP Coreference Dataset)", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9665647894144058}]}, {"text": "Conforming to the stage 1 of Gendered Pronoun Resolution 4 task, the official test set and validation set are combined as the training dataset in the experiments, while the official development set is used as the test set correspondingly.", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution 4 task", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.8229406237602234}]}, {"text": "I report the multi-class logarithmic loss of the 5-fold cross-validation on train and the average of their predictions on the test.", "labels": [], "entities": []}, {"text": "Also, the running time of the scripts is reported as a reference of the performance of the MSnet.", "labels": [], "entities": [{"text": "MSnet", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9700712561607361}]}], "tableCaptions": [{"text": " Table 1: Results of Feature-based Aproach.", "labels": [], "entities": []}, {"text": " Table 2: Results of Fine-tuning Aproach.", "labels": [], "entities": []}]}