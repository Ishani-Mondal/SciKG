{"title": [{"text": "Transformer-Based Capsule Network For Stock Movements Prediction", "labels": [], "entities": [{"text": "Stock Movements Prediction", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6681509514649709}]}], "abstractContent": [{"text": "Stock movements prediction is a highly challenging study for research and industry.", "labels": [], "entities": [{"text": "Stock movements prediction", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.679784099260966}]}, {"text": "Using social media for stock movements prediction is an effective but difficult task.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.7261167764663696}]}, {"text": "However, the existing prediction methods which are based on social media usually do not consider the rich semantics and relation fora certain stock.", "labels": [], "entities": []}, {"text": "It leads to difficulty in effective encoding.", "labels": [], "entities": []}, {"text": "To solve this problem, we propose a CapTE (Capsule network based on Transformer Encoder) model which uses the Transformer En-coder to extract the deep semantic features of the social media and then captures the structural relationship of the texts through a capsule network.", "labels": [], "entities": []}, {"text": "In this paper, we evaluate our method with different benchmarks, and the results demonstrate that our method improves the performance of stock movements prediction.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.6833164691925049}]}], "introductionContent": [{"text": "According to the Efficient Market Hypothesis (EMH), stock price movements are thought to be related to the news.", "labels": [], "entities": []}, {"text": "In natural language processing (NLP), public news and social media are two primary content resources for stock movements prediction.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.759665717681249}, {"text": "stock movements prediction", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.6835304101308187}]}, {"text": "Moreover, social media such as Twitter is better in timeliness than news, so the condition that text from social media like Twitter is used to predict the stock movements draws numerous attention recently.", "labels": [], "entities": []}, {"text": "On the other hand, tweets are able to reflect the investor's mentality to some extent.", "labels": [], "entities": []}, {"text": "It is useful for the prediction of stock movements.", "labels": [], "entities": [{"text": "prediction of stock movements", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.8616860508918762}]}, {"text": "Many studies focus on stock movements prediction based on social media.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 22, "end_pos": 48, "type": "TASK", "confidence": 0.7626113096872965}]}, {"text": "Xu and Cohen introduce recurrent and continuous latent variables for better treatment of stochasticity, use neural variational inference to address the intractable posterior inference, and also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies.", "labels": [], "entities": []}, {"text": "Wu et al. propose a novel Cross-model attention based on Hybrid Recurrent Neural Network (CH- * Both authors contributed equally to this paper \u2020 Contact Author RNN), which is inspired by the recent proposed DA-RNN model.", "labels": [], "entities": []}, {"text": "In order to cross the chasm of prediction ability between machine and human, a deeper level of semantic information need to be explored in the text.", "labels": [], "entities": []}, {"text": "Obviously, the above methods do not consider the rich semantic information and structural information of the social media, which lead to the model being unable to mine the deep semantics of text.", "labels": [], "entities": []}, {"text": "For example, \"Seems Travis is Still Running Uber!", "labels": [], "entities": []}, {"text": "Surprised?\", this expression of the sentence is colloquial which is very difficult to get the real mentality of the author.", "labels": [], "entities": [{"text": "Surprised", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9226189851760864}]}, {"text": "Especially, for one stock, there are more than one tweets on the same day.", "labels": [], "entities": []}, {"text": "Existing methods cannot effectively extract semantic features from these complex texts which are vital for this task.", "labels": [], "entities": []}, {"text": "Therefore, insights on the solutions to stock movements prediction can be drawn from the novel structure for capturing a deeper level of semantic information.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7022486726442972}]}, {"text": "In this paper, we propose a CapTE (Capsule network based on Transformer Encoder) model which uses Transformer encoder to solve this problem due to its multi-head attention structure.", "labels": [], "entities": []}, {"text": "The model is able to capture more important semantic information from different texts.", "labels": [], "entities": []}, {"text": "Tang et al. prove that the Transformer encoder achieves better results in semantic feature extraction than other models based on CNN and RNN.", "labels": [], "entities": [{"text": "semantic feature extraction", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.6759515603383383}]}, {"text": "From our results, we also prove that the Transformer encoder is better than other models in the task of stock movements prediction.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.6867407560348511}]}, {"text": "At the same time, there will be a lot of tweets for each stock on the same day.", "labels": [], "entities": []}, {"text": "These tweets often contain different people's views on the same stock, and the views are often different or even opposite.", "labels": [], "entities": []}, {"text": "For example, \"4 Major Stocks That Analysts Want You to Buy Now $GE\" and \"$GE technical alerts: Non-ADX 1,2,3,4 Bearish, MACD Bullish Signal Line Cross, 1,2,3 Retracement Be\".", "labels": [], "entities": [{"text": "Retracement Be", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.9566494822502136}]}, {"text": "So how to capture valuable information from these different comments and ultimately get the right judgment is a very difficult problem.", "labels": [], "entities": []}, {"text": "From the perspective of studying the relationship for different tweets contained on one day of one stock, we input the deep semantic information extracted by the Transformer encoder into a capsule network, which achieves the relationship between the semantic information for stock movements prediction.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 275, "end_pos": 301, "type": "TASK", "confidence": 0.6011267006397247}]}, {"text": "Ablation experiment proves that the capsule network effectively improves the accuracy of prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993413090705872}]}, {"text": "Finally, experimental results show that our integrated model is effective.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 For the stock movements prediction, our model captures the deep effective semantic features of tweets more effectively through the Transformer encoder, compared with neural network models such as CNNs and RNNs.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7159432371457418}]}, {"text": "\u2022 Up to date, no work introduces the Transformer to the task of stock movements prediction except us, and our model proves the Transformer improve the performance in the task of the stock movements prediction.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 64, "end_pos": 90, "type": "TASK", "confidence": 0.6104361017545065}, {"text": "stock movements prediction", "start_pos": 182, "end_pos": 208, "type": "TASK", "confidence": 0.6104973256587982}]}, {"text": "\u2022 The capsule network is also first introduced to solve the problem of stock movements prediction based on social media.", "labels": [], "entities": [{"text": "stock movements prediction", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.6452749967575073}]}, {"text": "The results show that the capsule network is effective for this task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test our model on the open dataset 1 . It ranges from January 2017 to November 2017 and contains 47 stocks which have sufficient tweets from the Standard Poor's 500 list.", "labels": [], "entities": [{"text": "open dataset 1", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.8300062417984009}, {"text": "Standard Poor's 500 list", "start_pos": 148, "end_pos": 172, "type": "DATASET", "confidence": 0.9857395648956299}]}, {"text": "The basic statistics of the dataset are shown in.", "labels": [], "entities": []}, {"text": "The experimental dataset is still available until June 2019.", "labels": [], "entities": []}, {"text": "Totally in our model and other baselines, we split the dataset with the ratio of approximately 5: 1: 1 in chronological order, which is the same as Wu et al..", "labels": [], "entities": []}, {"text": "In our experiment, the initial word embedding is obtained by word2vec.", "labels": [], "entities": []}, {"text": "The dimension of word embedding is 512.", "labels": [], "entities": []}, {"text": "We use the rise (1) and fall (0) of the stock price as the final output.", "labels": [], "entities": []}, {"text": "The internal weights in our model are initialized by sampling from the uniform distribution and tuned in the training process.", "labels": [], "entities": []}, {"text": "We adopt mini-batch in the training process, and the batch size is 128.", "labels": [], "entities": []}, {"text": "Following previous work for stock prediction  true positive, false positive, true negative and false negative, MCC is calculated as follows:  We test our model from two aspects.", "labels": [], "entities": [{"text": "MCC", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.9746392369270325}]}, {"text": "One is to predict the rise and fall based on the dataset.", "labels": [], "entities": []}, {"text": "The results are shown in  The CapTE-nC model gets a higher score by using the only Transformer than CH-RNN in both accuracy and MCC.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9998007416725159}, {"text": "MCC", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9981411695480347}]}, {"text": "It further illustrates that the Transformer captures deep semantic features compared to RNNs.", "labels": [], "entities": []}, {"text": "At the same time, the performance of CapTE-nT is higher than the scores of CH-RNN in both the accuracy and the MCC.", "labels": [], "entities": [{"text": "CapTE-nT", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8032695055007935}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9998177886009216}, {"text": "MCC", "start_pos": 111, "end_pos": 114, "type": "METRIC", "confidence": 0.980440080165863}]}, {"text": "It further demonstrates that the capsule network obtains valuable relationship information.", "labels": [], "entities": []}, {"text": "On the other hand, the score of CapTE-nC is higher than CapTE-nT which indicates that the capture of deep semantic features is more important for complex data such as tweets.", "labels": [], "entities": [{"text": "CapTE-nC", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.8835232257843018}, {"text": "CapTE-nT", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8825937509536743}]}, {"text": "Especially, the results of CapTE-nT and CapTE-nC with the only partial model are quite different from those of the complete model.", "labels": [], "entities": [{"text": "CapTE-nT", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9256685972213745}, {"text": "CapTE-nC", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9142757654190063}]}, {"text": "We believe that it is because the transformer encoder and capsule network complement each other in the extraction of deep semantic features.", "labels": [], "entities": []}, {"text": "They constitute a complete system.", "labels": [], "entities": []}, {"text": "And the function of the system performs more effectively than a single model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Basic statistics of the dataset.", "labels": [], "entities": []}, {"text": " Table 2: Performance of baselines and CapTE variations in accuracy  and MCC.", "labels": [], "entities": [{"text": "CapTE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9085586071014404}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9997740387916565}, {"text": "MCC", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9996204376220703}]}]}