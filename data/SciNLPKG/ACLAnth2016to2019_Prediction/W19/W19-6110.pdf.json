{"title": [], "abstractContent": [{"text": "This paper analyzes results on light-verb construction identification, distinguishing between known cases that could be directly learned from training data from unknown cases that require an extra level of semantic processing.", "labels": [], "entities": [{"text": "light-verb construction identification", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.8071734805901846}]}, {"text": "We propose a simple baseline that beats the best results of the PARSEME 1.1 shared task (Savary et al., 2018) for the known cases, and couple it with another simple baseline to handle the unknown cases.", "labels": [], "entities": [{"text": "PARSEME 1.1 shared task", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.43148087710142136}]}, {"text": "We additionally present two other classifiers based on a richer set of features, with results surpassing these best results by 7 percentage points.", "labels": [], "entities": []}], "introductionContent": [{"text": "Light-verb constructions (LVCs), such as the expression pay visit, area linguistic phenomenon coupling a verb and a stative or eventive noun, in which the verb itself is only needed for morphosyntactic purposes, its syntactic dependents being semantically related to the noun.", "labels": [], "entities": []}, {"text": "For instance in the sentence John paid me a visit, the subject and object of paid play the roles of the visitor and the visited.", "labels": [], "entities": []}, {"text": "The verb's semantics is either bleached or redundant with that of the noun (as in commit crime) . This mismatch between syntax and semantics has to betaken care of for semantically-oriented tasks to recover the full predicate-argument structure of the noun, since at least one of its semantic arguments of the noun is generally attached to the verb in plain syntactic treebanks.", "labels": [], "entities": []}, {"text": "1 Moreover, the fact that the verb choice is conventionalized and semantically bleached makes LVC identification an important requirement in semantic tasks such as machine translation).", "labels": [], "entities": [{"text": "LVC identification", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7792147397994995}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7580755949020386}]}, {"text": "Because of their syntactico-semantic characteristics, LVCs are generally considered difficult to circumscribe and annotate consistently).", "labels": [], "entities": []}, {"text": "Yet recently, the PARSEME 2018 shared-task has brought forth a collection of corpora containing verbal multiword expression (VMWE) annotations across 19 languages ( , including LVCs.", "labels": [], "entities": []}, {"text": "The reported inter-annotator agreement is variable across languages, but the macro-averaged chancecorrected kappa is overall 0.69, which is generally considered to denote a good agreement.", "labels": [], "entities": []}, {"text": "In the annotated corpora, the category of LVCs 2 accounted fora third of all expressions ).", "labels": [], "entities": []}, {"text": "A total of 13 systems participated in the PARSEME shared-task, predicting VMWEs occurrence in the test corpora.", "labels": [], "entities": [{"text": "PARSEME shared-task", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.5105426013469696}]}, {"text": "Results for each system varied across different systems and target languages, in which expressions that had been seen in the test corpus were predicted with variable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 166, "end_pos": 174, "type": "METRIC", "confidence": 0.9887077808380127}]}, {"text": "However, expressions that had never been seen in the test corpus were hardly ever predicted by most systems (the best F-score on unseen-in-train expressions in the closed track is below 20%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 118, "end_pos": 125, "type": "METRIC", "confidence": 0.9979525804519653}]}, {"text": "In this paper, we investigate the task of LVC identification in running text.", "labels": [], "entities": [{"text": "LVC identification", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8938368856906891}]}, {"text": "The main contributions of this paper are: (1) we propose handling the task of LVC identification differently depending on whether it was seen in the training corpus; (2) we present a simple baseline that surpasses all systems for seen LVCs; (3) we propose and evaluate different techniques for the prediction of unseen LVCs, which we then compare to the state of the art.", "labels": [], "entities": [{"text": "LVC identification", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.9029511511325836}]}, {"text": "The remainder of this paper is structured as follows: Section 2 presents the related work; Section 3 describes the methodology that will be employed; Section 4 describes the results; and finally, Section 5 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We explore hyperparameters on the 16 languages that contained a development set, and evaluate the final systems on the test set for all 19 languages (using both training and development set for training).", "labels": [], "entities": []}, {"text": "Evaluation of LVC predictions for each language uses the MWE-based F 1 score from of the PARSEME shared task ( ).", "labels": [], "entities": [{"text": "MWE-based F 1 score", "start_pos": 57, "end_pos": 76, "type": "METRIC", "confidence": 0.7879365980625153}, {"text": "PARSEME shared task", "start_pos": 89, "end_pos": 108, "type": "DATASET", "confidence": 0.7035769621531168}]}, {"text": "We modified its evaluation script so as to output scores Basic tuning of the learning rate led us to use 0.01.", "labels": [], "entities": []}, {"text": "for seen and unseen LVCs: it first labels a LVC (whether gold or predicted) as \"seen\" if there exists at least one gold LVC occurrence with the same set of lemmas in the training set, and unseen otherwise.", "labels": [], "entities": []}, {"text": "The two labels are then evaluated separately.", "labels": [], "entities": []}, {"text": "We also present a micro-average score (\u00b5Avg), in which the F 1 scores of all languages are averaged with a weight that is proportional to the number of LVCs in that languages test (or development) set.", "labels": [], "entities": [{"text": "micro-average score (\u00b5Avg)", "start_pos": 18, "end_pos": 44, "type": "METRIC", "confidence": 0.7662121295928955}, {"text": "F 1 scores", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9349285562833151}]}, {"text": "On test sets, we compare our results with SHOMA and TRAVERSAL, the two highestscoring systems in the shared-task.", "labels": [], "entities": [{"text": "SHOMA", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9115763306617737}, {"text": "TRAVERSAL", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.99472975730896}]}, {"text": "presents the fraction of LVCs in the development set that can also be seen in the training set.", "labels": [], "entities": []}, {"text": "In the lower end, German dev LVCs were only seen in train 26% of the time, mostly due to the small training set in this language.", "labels": [], "entities": []}, {"text": "In the higher end, 90% of Romanian LVCs had a counterpart in the training set, suggesting that a simple baseline focusing on seen LVCs should already yield good results for this language.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Fraction of LVC annotations that were seen in train, and LVC candidate coverage (highest  recall achievable, if all candidates are predicted as LVC) -evaluated on the development sets.", "labels": [], "entities": [{"text": "LVC candidate coverage", "start_pos": 67, "end_pos": 89, "type": "METRIC", "confidence": 0.5685156683127085}, {"text": "recall achievable", "start_pos": 100, "end_pos": 117, "type": "METRIC", "confidence": 0.9764407873153687}]}, {"text": " Table 2: F 1 scores on Majority and kNN baselines (F c  k with k = 2) , along with the best configuration  for the SVM and FFN classifiers -on the development sets.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9672112464904785}]}, {"text": " Table 3: F 1 scores (split for seen LVCs, unseen LVCs and overall) for the Majority and kNN baselines,  the best configuration of our SVM and FFN classifiers, and the highest-scoring systems in the shared-task  (SHOMA and TRAV(ERSAL)) -evaluated on the test sets.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9790602723757426}]}]}