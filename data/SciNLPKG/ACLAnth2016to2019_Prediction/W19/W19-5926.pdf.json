{"title": [{"text": "Learning Question-Guided Video Representation for Multi-Turn Video Question Answering", "labels": [], "entities": [{"text": "Learning Question-Guided Video Representation", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6059686318039894}, {"text": "Multi-Turn Video Question Answering", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.641458734869957}]}], "abstractContent": [{"text": "Understanding and conversing about dynamic scenes is one of the key capabilities of AI agents that navigate the environment and convey useful information to humans.", "labels": [], "entities": [{"text": "Understanding and conversing about dynamic scenes", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7582582086324692}]}, {"text": "Video question answering is a specific scenario of such AI-human interaction where an agent generates a natural language response to a question regarding the video of a dynamic scene.", "labels": [], "entities": [{"text": "Video question answering", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6299527883529663}]}, {"text": "Incorporating features from multiple modalities, which often provide supplementary information, is one of the challenging aspects of video question answering.", "labels": [], "entities": [{"text": "video question answering", "start_pos": 133, "end_pos": 157, "type": "TASK", "confidence": 0.6114592651526133}]}, {"text": "Furthermore , a question often concerns only a small segment of the video, hence encoding the entire video sequence using a recurrent neural network is not computationally efficient.", "labels": [], "entities": []}, {"text": "Our proposed question-guided video representation module efficiently generates the token-level video summary guided by each word in the question.", "labels": [], "entities": []}, {"text": "The learned representations are then fused with the question to generate the answer.", "labels": [], "entities": []}, {"text": "Through empirical evaluation on the Audio Visual Scene-aware Dialog (AVSD) dataset (Alamri et al., 2019a), our proposed models in single-turn and multi-turn question answering achieve state-of-the-art performance on several automatic natural language generation evaluation metrics.", "labels": [], "entities": [{"text": "Audio Visual Scene-aware Dialog (AVSD) dataset", "start_pos": 36, "end_pos": 82, "type": "DATASET", "confidence": 0.6844192296266556}, {"text": "multi-turn question answering", "start_pos": 146, "end_pos": 175, "type": "TASK", "confidence": 0.6621501942475637}]}], "introductionContent": [{"text": "Nowadays dialogue systems are becoming more and more ubiquitous in our lives.", "labels": [], "entities": []}, {"text": "It is essential for such systems to perceive the environment, gather data and convey useful information to humans in an accessible fashion.", "labels": [], "entities": []}, {"text": "Video question answering (VideoQA) systems provide a convenient way for humans to acquire visual information about the environment.", "labels": [], "entities": [{"text": "Video question answering (VideoQA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7410191943248113}]}, {"text": "If a user wants to obtain information about a dynamic scene, one can simply ask the VideoQA system a question in natural language, and the system generates a natural-language answer.", "labels": [], "entities": []}, {"text": "The task of a VideoQA dialogue system in", "labels": [], "entities": [{"text": "VideoQA dialogue", "start_pos": 14, "end_pos": 30, "type": "DATASET", "confidence": 0.798401951789856}]}], "datasetContent": [{"text": "We implement our models using the Tensor2Tensor framework ().", "labels": [], "entities": []}, {"text": "The question and dialogue context tokens are both embedded with the same randomly-initialized word embedding matrix, which is also shared with the answer decoder's output embedding.", "labels": [], "entities": []}, {"text": "The dimension of the word embedding is 256, the same dimension to which the I3D-RGB features are transformed.", "labels": [], "entities": []}, {"text": "All of our LSTM encoders and decoder have 1 hidden layer.", "labels": [], "entities": []}, {"text": "Bahdanau attention mechanism () is used in the answer decoder.", "labels": [], "entities": []}, {"text": "During training, we apply dropout rate 0.2 in the encoder and decoder cells.", "labels": [], "entities": [{"text": "dropout rate 0.2", "start_pos": 26, "end_pos": 42, "type": "METRIC", "confidence": 0.9013087948163351}]}, {"text": "We use the ADAM optimizer () with \u03b1 = 2 \u00d7 10 \u22124 , \u03b2 1 = 0.85, \u03b2 2 = 0.997, = 10 \u22126 , and clip the gradient with L2 norm threshold 2.0 ().", "labels": [], "entities": []}, {"text": "The models are trained up to 100K steps with early stopping on the validation BLEU-4 score using batch size 1024 on a single GPU.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 78, "end_pos": 90, "type": "METRIC", "confidence": 0.9589993953704834}]}, {"text": "During inference, we use beam search decoding with beam width 3.", "labels": [], "entities": []}, {"text": "We experimented with word embedding dimension {256, 512}, dropout rate {0, 0.2}, Luong and Bahdanau attention mechanisms, {1, 2} hidden layer(s) for both encoders and the decoder.", "labels": [], "entities": []}, {"text": "We found the aforementioned setting worked best for most models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data statistics of the AVSD dataset. We use  the official training set, and the public (i.e., prototype)  validation and test sets. We also present the average  length of the question token sequences and the I3D- RGB frame feature sequences to highlight the impor- tance of time efficient video encoding without using a  recurrent neural network. The sequence lengths of the  questions and I3D-RGB frame features are denoted by  K and L respectively in the model description (Sec- tion 3).", "labels": [], "entities": [{"text": "AVSD dataset", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.94356569647789}]}, {"text": " Table 3: Ablation study on the AVSD validation set.  We observe that the performance degrades when either  of both of the question-guided per-token visual feature  summarization (TokSumm) and feature gating (Gating)  techniques are removed.", "labels": [], "entities": [{"text": "AVSD validation set", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.8280539711316427}]}]}