{"title": [], "abstractContent": [{"text": "The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English\u2194Gujarati, English\u2194Chinese, German\u2192English, and English\u2192Czech.", "labels": [], "entities": [{"text": "WMT19 Shared Task on News Translation", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.6189642747243246}]}, {"text": "For all translation directions , we created or used back-translations of monolingual data in the target language as additional synthetic training data.", "labels": [], "entities": []}, {"text": "For English\u2194Gujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9505441188812256}]}, {"text": "For translation to and from Chi-nese, we investigated character-based tokeni-sation vs. sub-word segmentation of Chinese text.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9741644263267517}]}, {"text": "For German\u2192English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al.", "labels": [], "entities": []}, {"text": "For English\u2192Czech, we compared different pre-processing and tokenisation regimes.", "labels": [], "entities": [{"text": "tokenisation", "start_pos": 60, "end_pos": 72, "type": "TASK", "confidence": 0.9636317491531372}]}], "introductionContent": [{"text": "The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English-Gujarati (EN\u2194GU), English-Chinese (EN\u2194ZH), GermanEnglish (DE\u2192EN) and English-Czech (EN\u2192CS).", "labels": [], "entities": [{"text": "WMT19 Shared Task on News Translation", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.6481258869171143}]}, {"text": "All our systems are neural machine translation (NMT) systems trained in constrained data conditions with the Marian 1 toolkit . The different language pairs pose very different challenges, due to the characteristics of the languages involved and arguably more importantly, due to the amount of training data available.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.8485490381717682}, {"text": "Marian 1 toolkit", "start_pos": 109, "end_pos": 125, "type": "DATASET", "confidence": 0.9487749139467875}]}, {"text": "Pre-processing For EN\u2194ZH, we investigate character-level pre-processing for Chinese compared with subword segmentation.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7764269113540649}]}, {"text": "For EN\u2192CS, we show that it is possible in high resource settings to simplify pre-processing by removing steps.", "labels": [], "entities": []}, {"text": "1 https://marian-nmt.github.io Exploiting non-parallel resources For all language directions, we create additional, synthetic parallel training data.", "labels": [], "entities": []}, {"text": "For the high resource language pairs, we look at ways of effectively using large quantities of backtranslated data.", "labels": [], "entities": []}, {"text": "For example, for DE\u2192EN, we investigated the most effective way of combining genuine parallel data with larger quantities of synthetic parallel data and for CS\u2192EN, we filter backtranslated data by re-scoring translations using the MT model for the opposite direction.", "labels": [], "entities": []}, {"text": "The challenge for our low resource pair, EN\u2194GU, is producing sufficiently good models for backtranslation, which we achieve by training semisupervised MT models with cross-lingual language model pre-training (.", "labels": [], "entities": []}, {"text": "We use the same technique to translate additional data from a related language, Hindi.", "labels": [], "entities": []}], "datasetContent": [{"text": "Results of our models are shown in  We first trained single transformer-base models for each language direction to serve as our baselines.", "labels": [], "entities": []}, {"text": "We then re-score the EN\u2192CS training data using the CS\u2192EN model and filter out the 5% of data with the worst cross-entropy scores, which is a one-directional version of the dual conditional cross-entropy filtering, which we also used for our EN\u2192DE experiments.", "labels": [], "entities": [{"text": "EN\u2192CS training data", "start_pos": 21, "end_pos": 40, "type": "DATASET", "confidence": 0.49802091121673586}]}, {"text": "This improves the BLEU scores on the development set and newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9963487386703491}, {"text": "newstest2017", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8837636113166809}]}, {"text": "Next, we back-translate English monolingual data and train a CS\u2192EN model, which in turn is used to generate back-translations for our final systems.", "labels": [], "entities": []}, {"text": "The addition of back-translated data improves the Transformer Base model by 1.7-2.5 BLEU, which is less than the improvement from iterative backtranslations reported by.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9993247985839844}]}, {"text": "A Transformer Big model trained on the same data is ca.", "labels": [], "entities": []}, {"text": "Due to time and resource constraints we train and submit a EN\u2192CS system (this was the only language direction for English-Czech this year) consisting of just two transformer-big models trained with back-translated data.", "labels": [], "entities": []}, {"text": "Our system achieves 28.3 BLEU on newstest2019, 2.1 BLEU less then the top system, which ranks it in third position.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9942114949226379}, {"text": "newstest2019", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9621527791023254}, {"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9994988441467285}]}, {"text": "We use the transformer-base and transformer-big architectures described in Section 3.3.", "labels": [], "entities": []}, {"text": "Models are regularised with dropout between transformer layers of 0.2 and in attention of 0.1 and feed-forward layers of 0.1, label smoothing and exponential smoothing: 0.1 and 0.0001 respectively.", "labels": [], "entities": [{"text": "attention", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9824498891830444}, {"text": "label smoothing", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.6369101405143738}]}, {"text": "We optimise with Adam with a learning rate of 0.0003 and linear warm-up for first 16k updates, followed by inverted squared decay.", "labels": [], "entities": [{"text": "inverted squared decay", "start_pos": 107, "end_pos": 129, "type": "METRIC", "confidence": 0.841451625029246}]}, {"text": "For Transformer Big models we decrease the learning rate to 0.0002.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9817429482936859}]}, {"text": "We use mini-batches dynamically fitted into 48GB of GPU memory on 4 GPUs and delay gradient updates to every second iteration, which results in mini-batches of 1-1.2k sentences.", "labels": [], "entities": []}, {"text": "We use early stopping with a patience of 5 based on the wordlevel cross-entropy on the newsdev2016 data set.", "labels": [], "entities": [{"text": "patience", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9890545606613159}, {"text": "newsdev2016 data set", "start_pos": 87, "end_pos": 107, "type": "DATASET", "confidence": 0.9678773283958435}]}, {"text": "Each model is validated every 5k updates, and we use the best model checkpoint according to uncased BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9900175929069519}]}, {"text": "Decoding is performed with beam search with abeam size of 6 with length normalisation.", "labels": [], "entities": [{"text": "Decoding", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8592183589935303}, {"text": "length normalisation", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.943536102771759}]}, {"text": "Additionally, we reconstruct Czech quotation marks using regular expressions as the only post-processing step (Popel, 2018).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Final BLEU score results and system rank- ings amongst constrained systems according to auto- matic evaluation metrics.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9420202672481537}, {"text": "system rank- ings", "start_pos": 39, "end_pos": 56, "type": "METRIC", "confidence": 0.6600952669978142}]}, {"text": " Table 2: EN-GU Parallel training data used. Average  length is calculated in number of tokens per sentence.  For the parallel corpora, this is calculated for the first  language indicated (i.e. EN, GU, then EN)", "labels": [], "entities": [{"text": "Average  length", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.9277354776859283}]}, {"text": " Table 3: The influence of number of BPE merge opera- tions on HI\u2192GU BLEU score measured using BLEU  scores on the development set", "labels": [], "entities": [{"text": "BPE", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.7420525550842285}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.8278812170028687}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9975574016571045}]}, {"text": " Table 5: BLEU scores on the development and test sets  for EN\u2192GU. Our final submissions are marked in bold.  Synthetic data is the HI2GU-EN corpus plus backtrans- lated data for that translation direction and fine-tuning  is performed on 40k sentences of genuine parallel data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993447661399841}, {"text": "HI2GU-EN corpus", "start_pos": 132, "end_pos": 147, "type": "DATASET", "confidence": 0.915130615234375}]}, {"text": " Table 6: EN\u2192ZH results on the development set.", "labels": [], "entities": []}, {"text": " Table 7: ZH\u2192EN results on the development set.", "labels": [], "entities": []}, {"text": " Table 8: Training data used for German\u2192English translation.", "labels": [], "entities": [{"text": "German\u2192English translation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.5252458602190018}]}, {"text": " Table 9: Contrastive evaluation (BLEU scores) of performance on genuine German \u2192 English (fwd) translation  vs. English source restoration from text originally translated from English into German (rev).", "labels": [], "entities": [{"text": "Contrastive evaluation (BLEU scores)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.8428981999556223}, {"text": "English source restoration from text originally translated from English into German", "start_pos": 113, "end_pos": 196, "type": "TASK", "confidence": 0.8066648244857788}]}, {"text": " Table 10: Comparison of different pre-processing  pipelines for EN\u2192CS according to BLEU. Tc stands  for truecasing, Tok for tokenisation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9973670840263367}]}, {"text": " Table 11: BLEU score results for EN-CS experiments.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9738887250423431}]}]}