{"title": [{"text": "Analytic Score Prediction and Justification Identification in Automated Short Answer Scoring", "labels": [], "entities": [{"text": "Justification Identification", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7675174474716187}, {"text": "Automated Short Answer Scoring", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.5632186457514763}]}], "abstractContent": [{"text": "This paper provides an analytical assessment of student short answer responses with a view to potential benefits in pedagogical contexts.", "labels": [], "entities": []}, {"text": "We first propose and formalize two novel analytical assessment tasks: analytic score prediction and justification identification, and then provide the first dataset created for analytic short answer scoring research.", "labels": [], "entities": [{"text": "analytic score prediction", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7268107930819193}, {"text": "justification identification", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.9191673099994659}, {"text": "analytic short answer scoring research", "start_pos": 177, "end_pos": 215, "type": "TASK", "confidence": 0.7358319640159607}]}, {"text": "Subsequently, we present a neural baseline model and report our extensive empirical results to demonstrate how our dataset can be used to explore new and intriguing technical challenges in short answer scoring.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 189, "end_pos": 209, "type": "TASK", "confidence": 0.6510829925537109}]}, {"text": "The dataset is publicly available for research purposes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Short answer scoring (SAS) is the task of assessing short, written, free-text student responses to a given prompt.", "labels": [], "entities": [{"text": "Short answer scoring (SAS)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8215684195359548}, {"text": "assessing short, written, free-text student responses to a given prompt", "start_pos": 42, "end_pos": 113, "type": "TASK", "confidence": 0.5105883230765661}]}, {"text": "Typically, a prompt is a text which either elicits recall of information that was given in a reading passage, asks fora summary of a reading passage, or asks students to draw on knowledge they already have.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9836640357971191}]}, {"text": "The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric.", "labels": [], "entities": []}, {"text": "Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry ().", "labels": [], "entities": []}, {"text": "It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which * Current affiliation: Future Corporation, mizumoto.tomoya.mh7@is.naist.jp rubrics have been prepared in advance.", "labels": [], "entities": [{"text": "Future Corporation", "start_pos": 215, "end_pos": 233, "type": "DATASET", "confidence": 0.9593592286109924}]}, {"text": "In other words, rubrics are normally available in the SAS setting as they are in preset paper assignments.", "labels": [], "entities": []}, {"text": "Additionally, at least a small amount of training data is also available because responses are scored by human raters in any case.", "labels": [], "entities": []}, {"text": "This paper examines the issue of analytical assessment of short answer responses.", "labels": [], "entities": [{"text": "analytical assessment of short answer responses", "start_pos": 33, "end_pos": 80, "type": "TASK", "confidence": 0.8507063090801239}]}, {"text": "Typically, in a short answer setting, a scoring rubric comprises multiple analytic criteria, each of which stipulates different aspects of the conditions necessary fora response to receive points, and the overall score (referred to as the holistic score) of a given student response is determined by some predefined function (e.g., summation) of the score gained for each analytic criterion (analytic score).", "labels": [], "entities": []}, {"text": "Consider the example illustrated in, where a student response is assessed according to multiple analytic scoring rubrics (denoted by A, B, C, etc.).", "labels": [], "entities": []}, {"text": "The response gains two points for analytic criterion A (denoted by the red circled \"2\") and three points for B (yellow circled \"3\"), and the holistic score is given by the total of the analytic scores (+2 for A, +3 for B, and \u22121 for the mis-spelling).", "labels": [], "entities": []}, {"text": "Assessing student responses by analytic scores as well as holistic scores is essential in pedagogical contexts because (i) for teachers, analytic scores are useful fora precise assessment of student proficiency, and (ii) for students, analytic scores can be used as informative feedback indicating what has been achieved and what remains to be learned next.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, however, no prior study on automatic SAS has ever addressed this task.", "labels": [], "entities": [{"text": "automatic SAS", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.6237868070602417}]}, {"text": "Motivated by this background, we propose and formalize two analytical assessment tasks of SAS, (i) analytic score prediction and (ii) justification identification.", "labels": [], "entities": [{"text": "SAS", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9751566052436829}, {"text": "analytic score prediction", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.5659033556779226}, {"text": "justification identification", "start_pos": 134, "end_pos": 162, "type": "TASK", "confidence": 0.9671630859375}]}, {"text": "Analytic score prediction is the task of predicting the analytic score for each analytic scoring criterion, whereas justification identification is the task of identifying the justification cue for each analytic score.", "labels": [], "entities": [{"text": "Analytic score prediction", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6009517510732015}, {"text": "justification identification", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.9630986750125885}]}, {"text": "By justification cue, we refer to the segment of the response (a subsequence of words) that causes the response to be awarded points in the analytic score.", "labels": [], "entities": []}, {"text": "In, for example, the phrase Western culture is identified as a justification for criterion A, whereas the phrase Conflicts of interest is a justification for criterion B.", "labels": [], "entities": []}, {"text": "Justification cues not only explain the model's prediction but also help students learn how to improve their responses.", "labels": [], "entities": []}, {"text": "One crucial issue in addressing such analytical assessment tasks is the lack of data.", "labels": [], "entities": []}, {"text": "The datasets that are presently available for SAS research) are all accompanied by annotations of holistic scores alone.", "labels": [], "entities": [{"text": "SAS", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9882470965385437}]}, {"text": "In this study, we developed anew dataset with annotated analytic scores and justification cues as well as holistic scores.", "labels": [], "entities": []}, {"text": "The dataset contains 2,100 sample student responses for each of six distinct reading comprehension test prompts, collected from commercial achievement tests for Japanese high school students.", "labels": [], "entities": []}, {"text": "The dataset is publicly available for research purposes.", "labels": [], "entities": []}, {"text": "SAS requires content-based, prompt-specific rubrics, which means that one needs to create a labeled dataset to train a model for each given prompt.", "labels": [], "entities": [{"text": "SAS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.953059196472168}]}, {"text": "This nature of the task raises the issue of how one can reduce the required labelling costs while achieving sufficient performance.", "labels": [], "entities": []}, {"text": "This challenge is even more critical in analytical assess-ment because annotating student responses with analytic scores and justification cues tends to be much more costly than when only holistic scores are used.", "labels": [], "entities": []}, {"text": "This study explores several situations with limited amounts of analytic scores and justification cues as well as large numbers of holistic scores.", "labels": [], "entities": []}, {"text": "We show that analytical assessment performance for analytic score prediction and justification identification can be improved by compensating fora lack of data with these different types of annotations.", "labels": [], "entities": [{"text": "analytic score prediction", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7582329312960306}, {"text": "justification identification", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.9576490223407745}]}, {"text": "The contributions of this work are three-fold.", "labels": [], "entities": []}, {"text": "First, we propose and formalize two analytical assessment tasks: analytic score prediction and justification identification.", "labels": [], "entities": [{"text": "analytic score prediction", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.7019332051277161}, {"text": "justification identification", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.9594259560108185}]}, {"text": "Second, we have created the first dataset for analytic SAS and released it for research.", "labels": [], "entities": [{"text": "analytic SAS", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8025451898574829}]}, {"text": "Third, we present a neural baseline model and report some of the empirical results to demonstrate how our dataset can be used to address new amd intriguing technical challenges in SAS.", "labels": [], "entities": [{"text": "SAS", "start_pos": 180, "end_pos": 183, "type": "TASK", "confidence": 0.9700167775154114}]}], "datasetContent": [{"text": "This section provides an overview of our dataset.", "labels": [], "entities": []}, {"text": "shows the statistics of our dataset.", "labels": [], "entities": []}, {"text": "The dataset consists of six prompts and 2,100 student responses for each prompt.", "labels": [], "entities": []}, {"text": "Those prompts and their rubrics were collected from commercial achievement tests provided by a long-standing leading education company, where problems and rubrics are carefully generated by professional experts.", "labels": [], "entities": []}, {"text": "All the prompts are for reading comprehension tests and are of the type that requires recall of information that has been given (either explicitly or implicitly) in a reading passage.", "labels": [], "entities": [{"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9794161319732666}]}, {"text": "Responses (6 prompts \u00d7 2,100 responses) were originally annotated with holistic scores by professional raters employed by the education company (not by those employed for this research).", "labels": [], "entities": []}, {"text": "Before the scoring, the raters were carefully instructed about the rubrics and conducted atrial annotation on the same sample response set for calibration.", "labels": [], "entities": []}, {"text": "As argued in Sections 1 and 3.2, one crucial issue in analytic SAS is that the annotation of analytic scores and justification cues is far more expensive than holistic score annotation.", "labels": [], "entities": [{"text": "analytic SAS", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.8497450351715088}]}, {"text": "One of our primary concerns, therefore, is finding ways to reduce the required labeling costs while achieving sufficient performance.", "labels": [], "entities": []}, {"text": "To explore this issue, we consider three experimental scenarios:: Performance in QWK for analytic score prediction.", "labels": [], "entities": [{"text": "analytic score prediction", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.7187811533610026}]}, {"text": "\"SVR\" denotes the SVR baseline model described in Section 5.1.", "labels": [], "entities": []}, {"text": "\"NN base\", \"+just.", "labels": [], "entities": []}, {"text": "\", and \"+hol.", "labels": [], "entities": []}, {"text": "\" denote the models trained in the three hypothetical situations, Situations (i) to (iii), described in Section 5.2., respectively.", "labels": [], "entities": []}, {"text": "Scenario (i): Basic setting (analytic score signals only) The first scenario assumes that we only have analytic scores annotated to a small set of responses.", "labels": [], "entities": []}, {"text": "Thus we can train a model on these annotations for each task.", "labels": [], "entities": []}, {"text": "We consider this scenario as our baseline scenario.", "labels": [], "entities": []}, {"text": "We refer to the model for this scenario as \"NN base.\"", "labels": [], "entities": []}, {"text": "Scenario (ii): (i) + justification signals In addition to the analytic score annotations, the second scenario assumes that we have justification cues annotated to the same set of responses.", "labels": [], "entities": []}, {"text": "We can thus train a model on both the analytic score and justification annotations.", "labels": [], "entities": []}, {"text": "Scenario (iii): (ii) + holistic score signals In addition to the analytic scores and justification cues, the third scenario assumes that we have holistic scores annotated to a relatively large set of responses.", "labels": [], "entities": []}, {"text": "In addition to implementing supervised learning, we can train models in a weakly supervised manner using holistic scores.", "labels": [], "entities": []}, {"text": "All the reported results are the average often distinct trials with the use often different random seeds.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of our dataset.", "labels": [], "entities": []}, {"text": " Table 2: Inter-annotator agreement of analytic scores in  Kappa", "labels": [], "entities": []}, {"text": " Table 3: Performance in QWK for analytic score pre- diction. \"SVR\" denotes the SVR baseline model de- scribed in Section 5.1. \"NN base\", \"+just. \", and \"+hol.  \" denote the models trained in the three hypothetical  situations, Situations (i) to (iii), described in Section  5.2., respectively.", "labels": [], "entities": []}, {"text": " Table 4: Performance of justification identification.", "labels": [], "entities": [{"text": "justification identification", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.8793418407440186}]}, {"text": " Table 5: The performances of holistic score prediction.  n denotes the number of training instances (responses).  \"hol. (n)\" denotes the model trained with n holistic  score signals only. \"NN base (n)\" denotes the model  trained with the analytic score signals of n responses.  \"+just. (n)\" denotes the model trained with both ana- lytic scores and justification signals of n responses.", "labels": [], "entities": [{"text": "holistic score prediction", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.6417726278305054}]}]}