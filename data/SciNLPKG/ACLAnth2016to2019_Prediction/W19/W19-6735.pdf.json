{"title": [{"text": "Improving Domain Adaptation for Machine Translation with Translation Pieces", "labels": [], "entities": [{"text": "Improving Domain Adaptation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9106288552284241}, {"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7716140747070312}]}], "abstractContent": [{"text": "Neural Machine Translation has achieved impressive results in the last couple years, in particular when aided by domain adaptation methods.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8863712946573893}]}, {"text": "However, it has well known caveats, and can sometimes generate inadequate content that appears fluent, but does not convey the meaning of the original sentence.", "labels": [], "entities": []}, {"text": "In particular, for scarce in-domain data, these models tend to over-fit, performing poorly on any content that differs slightly from the domain data.", "labels": [], "entities": []}, {"text": "In this paper, we apply a recent technique based on translation pieces and show that it can work as away to improve and stabilize domain adaptation.", "labels": [], "entities": [{"text": "stabilize domain adaptation", "start_pos": 120, "end_pos": 147, "type": "TASK", "confidence": 0.6877577801545461}]}, {"text": "We present human evaluation results, with gains as high as 20 MQM points for single domains, and consistent gains in a multiple subdomain scenario of 3 MQM points for several language pairs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) is a stateof-the-art technique to do machine translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7920003334681193}, {"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.738933190703392}]}, {"text": "While NMT has proved to be efficient translating texts between multiple languages (Sennrich et al, 2016a) () in general settings, the ability of NMT technology to adapt to new domain has not attracted much focus from the research community.", "labels": [], "entities": []}, {"text": "On the other hand, domain adaptation is a fundamental element of many industrial applications in which in-domain data is often scarce.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7886945009231567}]}, {"text": "In one of the most popular scenarios, large generic data is used to train an initial NMT system, obtaining a set of parameters that are then fine tuned on the much smaller in-domain corpus).", "labels": [], "entities": []}, {"text": "The problem with this approach is that NMT often overfits to the target domain, which makes it less robust when it has to translate content which differs even slightly from the in-domain training data (Arthur et all, 2016) ().", "labels": [], "entities": []}, {"text": "Thus, the problem also holds when trying to generalize across more than one domain (, where improving in one of the domains might sacrifice quality in others.", "labels": [], "entities": []}, {"text": "In this paper, we present a possible solution to this issue, describing an application of the retrieved translation pieces (), which we show to help with cross-domain generalisation.", "labels": [], "entities": [{"text": "cross-domain generalisation", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.7562375664710999}]}, {"text": "We show that translation pieces improves the translation quality in a single domain, but also when combining multiple domains.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of in-domain lines available for the training  of models in each language pair", "labels": [], "entities": []}, {"text": " Table 2: Comparison of domain adaptation (DA) with added  translation pieces (DA + TPs) for EN-FR", "labels": [], "entities": [{"text": "Comparison of domain adaptation (DA)", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7021983478750501}, {"text": "EN-FR", "start_pos": 93, "end_pos": 98, "type": "TASK", "confidence": 0.42931434512138367}]}, {"text": " Table 3: Macro and Micro MQM of evaluated jobs for EN- FR", "labels": [], "entities": [{"text": "EN- FR", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.5495932698249817}]}, {"text": " Table 4: Comparison of domain adaptation (DA) with added  translation pieces (DA + TPs) for EN-NL", "labels": [], "entities": [{"text": "Comparison of domain adaptation (DA)", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7057324349880219}, {"text": "EN-NL", "start_pos": 93, "end_pos": 98, "type": "TASK", "confidence": 0.4948030710220337}]}, {"text": " Table 5: Macro and Micro MQM of evaluated jobs for EN- NL", "labels": [], "entities": [{"text": "EN- NL", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.5242405931154887}]}, {"text": " Table 6: Comparison of domain adaptation (DA) with added  translation pieces (DA + TPs) for EN-RO", "labels": [], "entities": [{"text": "Comparison of domain adaptation (DA)", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.7148265625749316}, {"text": "EN-RO", "start_pos": 93, "end_pos": 98, "type": "TASK", "confidence": 0.4240078330039978}]}, {"text": " Table 7: Macro and Micro MQM of evaluated jobs for EN- RO", "labels": [], "entities": [{"text": "EN- RO", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.5471959710121155}]}, {"text": " Table 8. Interestingly, the baseline  with the wider domain performs better in all auto- matic metrics, even though the human evaluation  does not corroborate that.", "labels": [], "entities": []}, {"text": " Table 8: Comparison of translation pieces on top of Domain  Adaptation (D) and subdomain adaptation (SD) for EN\u2192FR", "labels": [], "entities": []}, {"text": " Table 9: Macro and Micro MQM of evaluated jobs for a do- main and subdomain in EN-FR", "labels": [], "entities": [{"text": "EN-FR", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.7860996127128601}]}, {"text": " Table 13: Number of lines used for training of generic models", "labels": [], "entities": []}, {"text": " Table 14: Macro MQM subdomain evaluation for EN\u2192DE", "labels": [], "entities": []}, {"text": " Table 15: Micro MQM subdomain evaluation for EN\u2192DE", "labels": [], "entities": []}, {"text": " Table 16: Macro MQM subdomain evaluation for EN\u2192FR", "labels": [], "entities": [{"text": "FR", "start_pos": 49, "end_pos": 51, "type": "DATASET", "confidence": 0.28439366817474365}]}, {"text": " Table 17: Micro MQM subdomain evaluation for EN\u2192FR", "labels": [], "entities": [{"text": "FR", "start_pos": 49, "end_pos": 51, "type": "DATASET", "confidence": 0.31605881452560425}]}, {"text": " Table 18: Macro MQM subdomain evaluation for EN\u2192IT", "labels": [], "entities": []}, {"text": " Table 19: Micro MQM subdomain evaluation for EN\u2192IT  Micro MQM", "labels": [], "entities": []}, {"text": " Table 20: Macro MQM subdomain evaluation for EN\u2192ES", "labels": [], "entities": []}, {"text": " Table 21: Micro MQM subdomain evaluation for EN\u2192ES", "labels": [], "entities": []}, {"text": " Table 22: Macro MQM subdomain evaluation for EN\u2192PT", "labels": [], "entities": []}]}