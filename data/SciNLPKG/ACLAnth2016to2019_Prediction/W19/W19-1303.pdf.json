{"title": [{"text": "Online abuse detection: the value of preprocessing and neural attention models", "labels": [], "entities": [{"text": "Online abuse detection", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7700474460919698}]}], "abstractContent": [{"text": "We propose an attention-based neural network approach to detect abusive speech in online social networks.", "labels": [], "entities": []}, {"text": "Our approach enables more effective modeling of context and the semantic relationships between words.", "labels": [], "entities": []}, {"text": "We also empirically evaluate the value of text pre-processing techniques in addressing the challenge of out-of-vocabulary words in toxic content.", "labels": [], "entities": []}, {"text": "Finally , we conduct extensive experiments on the Wikipedia Talk page datasets, showing improved predictive power over the previous state-of-the-art.", "labels": [], "entities": [{"text": "Wikipedia Talk page datasets", "start_pos": 50, "end_pos": 78, "type": "DATASET", "confidence": 0.9671479612588882}]}], "introductionContent": [{"text": "Over the past few years, there has been increasing attention devoted to the problems of abusive language and hate-based activity in online social networks, with big social media platforms feeling the pressure from governments to perform some moderation of their activities.", "labels": [], "entities": []}, {"text": "The AI research community has begun to design automated methods to detect instances of hate speech in these networks, with a primary approach proposing the use of Natural Language Processing (NLP) to perform document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.7504312992095947}]}, {"text": "A major challenge to performing this task is the intentional word and phrase obfuscation done by users to avoid detection (.", "labels": [], "entities": []}, {"text": "Examples such as 'sh*t', '1d10t' and 'banmuslim' are human-readable but difficult to detect using algorithms that rely on keyword spotting.", "labels": [], "entities": [{"text": "keyword spotting", "start_pos": 122, "end_pos": 138, "type": "TASK", "confidence": 0.7145297229290009}]}, {"text": "Obfuscation makes context modeling, a challenging problem in NLP, even harder.", "labels": [], "entities": [{"text": "context modeling", "start_pos": 18, "end_pos": 34, "type": "TASK", "confidence": 0.7879863083362579}]}, {"text": "For example, in the sentences \"You feminist cnt\" and \"I cnt understand this\", 'cnt' is used as a shorthand.", "labels": [], "entities": []}, {"text": "However, without considering the context, it is difficult to tell whether 'cnt' represents 'cannot' or a derogatory remark.", "labels": [], "entities": []}, {"text": "Early work in hate speech detection used classifiers such as Support Vector Machines and Logistic Regression, with features such as word n-gram counts and the number of insult words (.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.795137862364451}]}, {"text": "With the recent success of deep learning models in solving a variety of classification problems, they have also become the state-of-the-art in detecting abusive speech.", "labels": [], "entities": [{"text": "detecting abusive speech", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.8671217163403829}]}, {"text": "In this paper, we make the following contributions towards detecting hate speech in social networks.", "labels": [], "entities": [{"text": "detecting hate speech in social networks", "start_pos": 59, "end_pos": 99, "type": "TASK", "confidence": 0.8668645024299622}]}, {"text": "1. We propose the use of attention based deep learning models, the first being the usual word attention layer and the second being a self-targeted co-attention layer that considers the semantic relationships between word pairs.", "labels": [], "entities": []}, {"text": "2. We examine the value of text pre-processing techniques to reduce the number of out-ofvocabulary (OOV) words.", "labels": [], "entities": []}, {"text": "We find that preprocessing not only helps to improve the accuracy of existing models, but also improves the proposed attention models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9987854361534119}]}, {"text": "Our solution addresses the main challenges in detecting abusive content: capturing context to identify important words when making classification decisions, which we achieve through the attention models, and out-of-vocabulary words, which we deal with through preprocessing.", "labels": [], "entities": []}, {"text": "Altogether, we improve classification accuracy over the previous state of the art on the Wikipedia Toxicity, Personal Attack, and Aggression datasets (.", "labels": [], "entities": [{"text": "classification", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9461984038352966}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9627419114112854}, {"text": "Aggression", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.8907355666160583}]}, {"text": "In the remainder of this paper, Section 2 discusses related work, Section 3 presents our pre-processing method, Section 4 discusses our deep learning models and the baseline, Section 5 presents experimental results, and Section 6 concludes the paper with directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For consistency with previous work, our experiments are based on the recently released Wikipedia datasets: Toxicity (W-Tox), Personal Attack (W-At) and Aggression (W-Ag) (.", "labels": [], "entities": [{"text": "Wikipedia datasets", "start_pos": 87, "end_pos": 105, "type": "DATASET", "confidence": 0.8512110710144043}, {"text": "Aggression (W-Ag)", "start_pos": 152, "end_pos": 169, "type": "METRIC", "confidence": 0.8874811381101608}]}, {"text": "W-Tox contains 159,686 records, while W-At and W-Ag both contain 115,864 records each.", "labels": [], "entities": [{"text": "W-Tox", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8897204995155334}]}, {"text": "These datasets were created by having annotators from the Crowdflower platform label Wikipedia Talk Page comments as toxic or not, personal attack or not, and aggressive or not, respectively.", "labels": [], "entities": [{"text": "Crowdflower platform label Wikipedia Talk Page comments", "start_pos": 58, "end_pos": 113, "type": "DATASET", "confidence": 0.8874831284795489}]}, {"text": "Each comment was judged by multiple annotators, and, in this work, we take the majority vote as the class label.", "labels": [], "entities": []}, {"text": "This gives us a binary classification problem.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.6668620109558105}]}, {"text": "Roughly 10 percent of the comments in each dataset are labelled as toxic, personal attacks or aggressive.", "labels": [], "entities": []}, {"text": "For a fair comparison to, we use a 60:40 training-testing split.", "labels": [], "entities": []}, {"text": "Following, we use 300-dimensional Glove () embedding vectors and we further tune them during training via back-propagation.", "labels": [], "entities": []}, {"text": "We create embedding vectors for OOV words with random values in the range \u00b10.25.", "labels": [], "entities": []}, {"text": "We use 175 as the length of the sequence and we use cross-entropy loss with the Adam optimizer (, with an initial learning rate of 0.001 and L2 regularization of 10 \u22126 . Each GRU cell has a hidden dimension size of 150.", "labels": [], "entities": []}, {"text": "We experimented with batch sizes of 128, 200 and 256.", "labels": [], "entities": []}, {"text": "We implemented all the models in Pytorch () and we use the sigmoid output layer in all the models.", "labels": [], "entities": [{"text": "Pytorch", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.8791657090187073}]}, {"text": "Our source code is available at https://github.com/ ddhruvkr/Online_Abuse_Detection We first evaluate the two methods of preprocessing from Section 3, std-approach and advapproach.", "labels": [], "entities": [{"text": "Online_Abuse_Detection", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.6267666697502137}]}, {"text": "We then evaluate the models from Section 4.", "labels": [], "entities": []}, {"text": "To measure the accuracy of the models, we report macro (i.e., average) F1 scores over both classes (labelled \"Overall\" below) as well as the (micro) F1 scores for just the toxic classes (defined in the standard way, as a harmonic mean of precision and recall).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.99882572889328}, {"text": "F1", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9839158654212952}, {"text": "F1", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9930799603462219}, {"text": "precision", "start_pos": 238, "end_pos": 247, "type": "METRIC", "confidence": 0.9993940591812134}, {"text": "recall", "start_pos": 252, "end_pos": 258, "type": "METRIC", "confidence": 0.9986283779144287}]}, {"text": "In some experiments, we also report precision (P) and recall (R) individually.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9625682085752487}, {"text": "recall (R)", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9600198715925217}]}, {"text": "For each method, we repeat the experiments five times   and report the average.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: OOV counts after applying standard and ad- vanced pre-processing techniques.", "labels": [], "entities": [{"text": "OOV counts", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.8232520222663879}]}, {"text": " Table 2: Overall and toxic F1 score after applying var- ious preprocessing techniques using the BiRNN base- line model.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.965503603219986}, {"text": "BiRNN", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.7746341824531555}]}, {"text": " Table 3: Overall Macro F1 scores in the three datasets.  *  denotes results taken directly from the original papers.", "labels": [], "entities": [{"text": "Macro F1", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.5913484394550323}]}, {"text": " Table 4: Micro precision, recall and F1 scores for  toxic/personal attack/aggression classes.", "labels": [], "entities": [{"text": "Micro", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9798091053962708}, {"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9012839198112488}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9995296001434326}, {"text": "F1", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9994379878044128}]}]}