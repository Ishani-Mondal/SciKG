{"title": [{"text": "Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications", "labels": [], "entities": [{"text": "Semantic Matching of Documents from Heterogeneous Collections", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.8923789943967547}]}], "abstractContent": [{"text": "We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections.", "labels": [], "entities": []}, {"text": "We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections.", "labels": [], "entities": [{"text": "Concept-Project matching task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7413830359776815}]}, {"text": "Although our method only employs standard resources without any domain-or task-specific modifications, it clearly outperforms the more complex system of the original authors.", "labels": [], "entities": []}, {"text": "In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present a simple and efficient unsupervised method for pairwise matching of documents from heterogeneous collections.", "labels": [], "entities": []}, {"text": "Following, we consider two document collections heterogeneous if their documents differ systematically with respect to vocabulary and / or level of abstraction.", "labels": [], "entities": []}, {"text": "With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous.", "labels": [], "entities": [{"text": "length", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.977665901184082}]}, {"text": "Examples include collections in which expert answers are mapped to non-expert questions (e.g. InsuranceQA by), but also so-called community QA collections (), where the lexical mismatch between Q and A documents is often less pronounced than the length difference.", "labels": [], "entities": [{"text": "InsuranceQA", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9156143665313721}]}, {"text": "Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric.", "labels": [], "entities": []}, {"text": "However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair similarity on the basis of selected pairwise word similarities.", "labels": [], "entities": []}, {"text": "This has the following advantages, which make our method a viable candidate for practical, real-world applications: efficiency, because pairwise word similarities can be efficiently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on.", "labels": [], "entities": [{"text": "transparency", "start_pos": 213, "end_pos": 225, "type": "METRIC", "confidence": 0.9913284182548523}]}, {"text": "We demonstrate our method with the Concept-Project matching task (), which is described in the next section.", "labels": [], "entities": [{"text": "Concept-Project matching task", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.7306778232256571}]}], "datasetContent": [{"text": "Setup All experiments are based on off-the-shelf word-level resources: We employ WOMBAT) for easy access to the 840B GloVe () and the GoogleNews 5 Word2Vec () embeddings.", "labels": [], "entities": [{"text": "WOMBAT", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.8457754254341125}, {"text": "840B GloVe", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.8378848731517792}]}, {"text": "These embedding resources, while slightly outdated, are still widely used.", "labels": [], "entities": []}, {"text": "However, they cannot handle out-of-vocabulary tokens due to their fixed, word-level lexicon.", "labels": [], "entities": []}, {"text": "Therefore, we also use a pretrained English fastText model 6 (;), which also includes subword information.", "labels": [], "entities": []}, {"text": "IDF weights for approx. 12 mio.", "labels": [], "entities": [{"text": "IDF", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7875109910964966}]}, {"text": "different words were obtained from the English Wikipedia dump provided by the Polyglot project (Al-Rfou et al.).", "labels": [], "entities": [{"text": "English Wikipedia dump", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.8600600957870483}, {"text": "Polyglot project", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.944739818572998}]}, {"text": "All resources are case-sensitive, i.e. they might contain different entries for words that only differ in case (cf. Section 5).", "labels": [], "entities": []}, {"text": "We run experiments in different setups, varying both the input representation (GloVe vs. Google vs. fastText embeddings, \u00b1 TF-weighting, and \u00b1 IDF-weighting) for concepts and projects, and the extent to which concept descriptions are used: For the latter, Label means only the concept label (first and second row in the example), Description means only the textual description of the concept, and Both means the concatenation of Label and Description.", "labels": [], "entities": []}, {"text": "For the projects, we always use both label and description.", "labels": [], "entities": []}, {"text": "For the project descriptions, we extract only the last column of the original file (CONTENT), and remove user comments and some boiler-plate.", "labels": [], "entities": [{"text": "CONTENT", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.977179229259491}]}, {"text": "Each instance in the resulting data set is a tuple of < c, p, label >, where c and pare bags of words, with case preserved and function words 7 removed, and label is either 0 or 1.", "labels": [], "entities": []}, {"text": "Parameter Tuning Our method is unsupervised, but we need to define a threshold parameter which controls the minimum similarity that a concept and a project description should have in order to be considered a match.", "labels": [], "entities": []}, {"text": "Also, the TOP n COS SIM AVG measure has a parameter n which controls how many ranked words are used from c and p, and how many similarity scores are averaged to create the final score.", "labels": [], "entities": [{"text": "TOP n COS SIM AVG", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7039921522140503}]}, {"text": "Parameter tuning experiments were performed on a random subset of 20% of our data set (54% positive).", "labels": [], "entities": [{"text": "Parameter tuning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6895020753145218}]}, {"text": "Note that used only 10% of their 537 instances data set as tuning data.", "labels": [], "entities": []}, {"text": "The tuning data results of the best-performing parameter values for each setup can be found in.", "labels": [], "entities": []}, {"text": "The top F scores per type of concept input (Label, Description, Both) are given in bold.", "labels": [], "entities": []}, {"text": "For AVG COS SIM and TOP n COS SIM AVG, we determined the threshold values (T) on the tuning data by doing a simple .005 step search over the range from 0.3 to 1.0.", "labels": [], "entities": []}, {"text": "For TOP n COS SIM AVG, we additionally varied the value of n in steps of 2 from 2 to 30.", "labels": [], "entities": [{"text": "TOP n COS SIM AVG", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.5465910255908966}]}, {"text": "Results The top tuning data scores for AVG COS SIM show that the Google embeddings with TF*IDF weighting yield the top F score for all three concept input types (.881 -.945).", "labels": [], "entities": [{"text": "AVG COS SIM", "start_pos": 39, "end_pos": 50, "type": "DATASET", "confidence": 0.7447677254676819}, {"text": "TF*IDF weighting", "start_pos": 88, "end_pos": 104, "type": "METRIC", "confidence": 0.756177231669426}, {"text": "F", "start_pos": 119, "end_pos": 120, "type": "METRIC", "confidence": 0.9961589574813843}]}, {"text": "Somewhat expectedly, the best overall F score (.945) is produced in the setting Both, which provides the most information.", "labels": [], "entities": [{"text": "F score", "start_pos": 38, "end_pos": 45, "type": "METRIC", "confidence": 0.9913182556629181}]}, {"text": "Actually, this is true for all four weighting schemes for both GloVe and Google, while fastText consistently yields its top F scores (.840 -.911) in the Label setting, which provides the least information.", "labels": [], "entities": [{"text": "F", "start_pos": 124, "end_pos": 125, "type": "METRIC", "confidence": 0.9934045672416687}]}, {"text": "Generally, the level of performance of the simple baseline measure AVG COS SIM on this data set is rather striking.", "labels": [], "entities": [{"text": "AVG COS SIM", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.7360022862752279}]}, {"text": "For TOP n COS SIM AVG, the tuning data results are somewhat more varied: First, there is no single best performing set of embeddings: Google yields the best F score for the Label setting (.953), while GloVe (though only barely) leads in the Description setting (.912).", "labels": [], "entities": [{"text": "TOP n COS SIM AVG", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.4886591374874115}, {"text": "F score", "start_pos": 157, "end_pos": 164, "type": "METRIC", "confidence": 0.9839126169681549}]}, {"text": "This time, it is fastText which produces the best F score in the Both setting, which is also the best overall tuning data F score for TOP n COS SIM AVG (.954).", "labels": [], "entities": [{"text": "fastText", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8531355261802673}, {"text": "F score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.985086053609848}, {"text": "F score", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.8878215551376343}, {"text": "TOP n COS SIM AVG", "start_pos": 134, "end_pos": 151, "type": "METRIC", "confidence": 0.4861061334609985}]}, {"text": "While the difference to the Google result for Label is only minimal, it is striking that the best overall score is again produced using the 'richest' setting, i.e. the one involving both TF and IDF weighting and the most informative input.", "labels": [], "entities": [{"text": "Label", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.5871832370758057}, {"text": "IDF", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.8597837090492249}]}, {"text": "We then selected the best performing parameter settings for every concept input and ran experiments on the held-out test data.", "labels": [], "entities": []}, {"text": "Since the original data split used by is unknown, we cannot exactly replicate their settings, but we also perform ten runs using randomly selected 10% of our 408 instances test data set, and report average P, R, F, and standard deviation.", "labels": [], "entities": [{"text": "F", "start_pos": 212, "end_pos": 213, "type": "METRIC", "confidence": 0.9232191443443298}, {"text": "standard", "start_pos": 219, "end_pos": 227, "type": "METRIC", "confidence": 0.9764947891235352}]}, {"text": "The results can be found in.", "labels": [], "entities": []}, {"text": "For comparison, the two top rows provide the best results of.", "labels": [], "entities": []}, {"text": "The first interesting finding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best overall result (.926).", "labels": [], "entities": [{"text": "AVG COS SIM measure", "start_pos": 42, "end_pos": 61, "type": "METRIC", "confidence": 0.7687734812498093}]}, {"text": "Note that our Both setting is probably the one most similar to the concept input used by.", "labels": [], "entities": []}, {"text": "This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by regarding the infeasibility of document-level matching for documents of different lengths.", "labels": [], "entities": []}, {"text": "The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by reach their best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain.", "labels": [], "entities": [{"text": "TOP n COS SIM AVG measure", "start_pos": 56, "end_pos": 81, "type": "METRIC", "confidence": 0.6808732052644094}]}, {"text": "Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by, it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting.", "labels": [], "entities": [{"text": "TOP n COS SIM AVG", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.5005259573459625}]}, {"text": "However, apart from raw classification performance, our method also aims at providing human-interpretable information on how a classification was done.", "labels": [], "entities": []}, {"text": "In the next section, we perform a detail analysis on a selected setup.", "labels": [], "entities": [{"text": "detail", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9795268177986145}]}], "tableCaptions": [{"text": " Table 1: Tuning Data Results AVG COS SIM. Top F per Concept Input Type in Bold.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9519643783569336}, {"text": "AVG", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.8313730359077454}, {"text": "COS", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.7133698463439941}, {"text": "SIM", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.5514447093009949}]}, {"text": " Table 2: Tuning Data Results TOP n COS SIM AVG. Top F per Concept Input Type in Bold.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9509363770484924}, {"text": "TOP", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9555874466896057}, {"text": "COS SIM AVG", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.69588170448939}]}, {"text": " Table 3: Test Data Results", "labels": [], "entities": [{"text": "Test Data", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7505942285060883}]}]}