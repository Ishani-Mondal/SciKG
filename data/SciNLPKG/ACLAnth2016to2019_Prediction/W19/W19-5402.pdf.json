{"title": [], "abstractContent": [{"text": "We present the results from the 5 th round of the WMT task on MT Automatic Post-Editing.", "labels": [], "entities": [{"text": "WMT task", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.8538495600223541}, {"text": "MT Automatic Post-Editing", "start_pos": 62, "end_pos": 87, "type": "DATASET", "confidence": 0.6013844807942709}]}, {"text": "The task consists in automatically correcting the output of a \"black-box\" machine translation system by learning from human corrections.", "labels": [], "entities": [{"text": "correcting the output of a \"black-box\" machine translation", "start_pos": 35, "end_pos": 93, "type": "TASK", "confidence": 0.5821495592594147}]}, {"text": "Keeping the same general evaluation setting of the previous four rounds, this year we focused on two language pairs (English-German and English-Russian) and on domain-specific data (In-formation Technology).", "labels": [], "entities": []}, {"text": "For both the language directions, MT outputs were produced by neural systems unknown to participants.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9922040700912476}]}, {"text": "Seven teams participated in the English-German task, with a total of 18 submitted runs.", "labels": [], "entities": []}, {"text": "The evaluation, which was performed on the same test set used for the 2018 round, shows further progress in APE technology: 4 teams achieved better results than last year's winning system , with improvements up to-0.78 TER and +1.23 BLEU points over the baseline.", "labels": [], "entities": [{"text": "APE", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.4982133209705353}, {"text": "TER", "start_pos": 219, "end_pos": 222, "type": "METRIC", "confidence": 0.9995360374450684}, {"text": "BLEU", "start_pos": 233, "end_pos": 237, "type": "METRIC", "confidence": 0.9774101376533508}]}, {"text": "Two teams participated in the English-Russian task submitting 2 runs each.", "labels": [], "entities": [{"text": "English-Russian task submitting", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.48869643608729046}]}, {"text": "On this new language direction, characterized by a higher quality of the original translations , the task proved to be particularly challenging.", "labels": [], "entities": []}, {"text": "Indeed, none of the submitted runs improved the very high results of the strong system used to produce the initial translations (16.16 TER, 76.20 BLEU).", "labels": [], "entities": [{"text": "TER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9773553609848022}, {"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9967934489250183}]}], "introductionContent": [{"text": "MT Automatic Post-Editing (APE) is the task of automatically correcting errors in a machinetranslated text.", "labels": [], "entities": [{"text": "MT Automatic Post-Editing (APE)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8172161082426707}]}, {"text": "In its 5 th round, the APE shared task organized within the WMT Conference on Machine Translation kept the same overall evaluation setting of the previous four rounds.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.789479931195577}, {"text": "WMT Conference on Machine Translation", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.7940422415733337}]}, {"text": "Specifically, the participating systems had to automatically correct the output of an unknown \"black box\" MT system by learning from human revisions of translations produced by the same system.", "labels": [], "entities": []}, {"text": "This year, the task focused on two language pairs (English-German and English-Russian) and, in continuity with the last three rounds, on data coming from the Information Technology domain.", "labels": [], "entities": []}, {"text": "While in 2018 one of the proposed subtasks was still focusing on the correction of phrase-based MT output, this year only neural MT (NMT) output has been considered.", "labels": [], "entities": [{"text": "correction of phrase-based MT output", "start_pos": 69, "end_pos": 105, "type": "TASK", "confidence": 0.6672397911548614}]}, {"text": "However, this year's campaign allows both fora fair assessment of the progress in APE technology and for tests in more challenging conditions.", "labels": [], "entities": [{"text": "APE technology", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.6648974418640137}]}, {"text": "On one side, reusing the same test English-German set used last year, the evaluation framework allows us fora direct comparison with the last year's outcomes at least on one language.", "labels": [], "entities": []}, {"text": "On the other side, dealing with a difficult language like Russian and only with highquality NMT output, also this round presented participants with an increased level of difficulty with respect to the past.", "labels": [], "entities": []}, {"text": "Seven teams participated in the EnglishGerman task, submitting 18 runs in total.", "labels": [], "entities": [{"text": "EnglishGerman task", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8334177136421204}]}, {"text": "Two teams participated in the English-Russian task, submitting 2 runs each.", "labels": [], "entities": []}, {"text": "Similar to last year, all the teams developed their systems based on neural technology, which confirms to be the state-ofthe-art approach to APE.", "labels": [], "entities": [{"text": "APE", "start_pos": 141, "end_pos": 144, "type": "TASK", "confidence": 0.9235951900482178}]}, {"text": "Only in one case, indeed, a participating team achieved its highest results (but with no improvement over the baseline) with a phrase-based APE system.", "labels": [], "entities": []}, {"text": "In most of the cases, participants experimented with the Transformer architecture (), either directly or by adapting it to the task (see Section 3).", "labels": [], "entities": []}, {"text": "Another common trait of the submitted systems is the reliance on the consolidated multi-source approach, which is able to exploit information from both the MT output to be corrected and the corresponding source sentence.", "labels": [], "entities": []}, {"text": "The third aspect common to all submissions is the exploitation of synthetic data, either those provided together with the task data ( or similar, domain-specific resources created ad-hoc by participants.", "labels": [], "entities": []}, {"text": "In the English-German task, the evaluation was performed on the same test set used in 2018, whose \"gold\" human post-edits were kept undisclosed to participants for the sake of future comparisons.", "labels": [], "entities": []}, {"text": "Evaluating on the same benchmark allowed to observe further technology improvements over the past.", "labels": [], "entities": []}, {"text": "Last year, the largest gain over the baseline.73 BLEU) was -0.38 TER (16.46) and +0.8 BLEU (75.53).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9787780046463013}, {"text": "TER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8233872056007385}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9713914394378662}]}, {"text": "This year, four teams achieved better results than last year's best submission.", "labels": [], "entities": []}, {"text": "The top-ranked system achieved 16.06 TER (-0.78 with respect to the baseline) and 75.96 BLEU (+1.23).", "labels": [], "entities": [{"text": "TER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9995438456535339}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9987809062004089}]}, {"text": "Most noticeably, the fact that the TER/BLEU differences between the top four primary submissions are not statistically significant indicates that the observed progress is not isolated.", "labels": [], "entities": [{"text": "TER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9968485236167908}, {"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8543654680252075}]}, {"text": "The newly proposed English-Russian task represents a more challenging evaluation scenario, mainly due to the higher quality of the NMT output to be corrected.", "labels": [], "entities": []}, {"text": "In this case, even the best submission (16.59 TER, 75.27 TER) was unable to beat the baseline.", "labels": [], "entities": [{"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9946870803833008}, {"text": "TER", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9805414080619812}]}, {"text": "These results confirm one of the main findings of previous rounds): improving high-quality MT output remains the biggest challenge for APE.", "labels": [], "entities": [{"text": "MT", "start_pos": 91, "end_pos": 93, "type": "TASK", "confidence": 0.9838179349899292}, {"text": "APE", "start_pos": 135, "end_pos": 138, "type": "TASK", "confidence": 0.7136490345001221}]}, {"text": "This motivates further research on precise and conservative solutions able to mimic human behaviour by performing only the minimum amount of edit operations needed.", "labels": [], "entities": []}], "datasetContent": [{"text": "System performance was evaluated both by means of automatic metrics and manually.", "labels": [], "entities": []}, {"text": "Automatic metrics were used to compute the distance between automatic and human post-edits of the machine-translated sentences present in the test sets.", "labels": [], "entities": []}, {"text": "To this aim, TER and BLEU (case-sensitive) were respectively used as primary and secondary evaluation metrics.", "labels": [], "entities": [{"text": "TER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9992175102233887}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9985073208808899}]}, {"text": "Systems were ranked based on the average TER calculated on the test set by using the TERcom 6 software: lower average TER scores correspond to higher ranks.", "labels": [], "entities": [{"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.996531069278717}, {"text": "TERcom 6 software", "start_pos": 85, "end_pos": 102, "type": "DATASET", "confidence": 0.8612663944562277}, {"text": "TER", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9932142496109009}]}, {"text": "BLEU was computed using the multi-bleu.perl package 7 available in MOSES.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9816789031028748}, {"text": "MOSES", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.8956672549247742}]}, {"text": "Manual evaluation was conducted via sourcebased direct human assessment ( as implemented by Appraise.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.8661515712738037}]}, {"text": "Details are discussed in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.812900573015213}]}, {"text": "In order to complement the automatic evaluation of APE submissions, a manual evaluation of the primary systems submitted (seven for EnglishGerman, five for English-Russian) was conducted.", "labels": [], "entities": [{"text": "APE submissions", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.6895545721054077}, {"text": "EnglishGerman", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9246132373809814}]}, {"text": "Similarly to the manual evaluation carried out for last year APE shared task, it was based on the direct assessment (DA) approach (.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.720556636651357}]}, {"text": "In this Section, we present the evaluation procedure as well as the results obtained.", "labels": [], "entities": []}, {"text": "The manual evaluation carried out this year involved 32 native German speakers with full professional proficiency in English.", "labels": [], "entities": []}, {"text": "All annotators were paid consultants, sourced by a linguistic service provider company.", "labels": [], "entities": []}, {"text": "Each evaluator had experience with the evaluation task through previous work using the same evaluation platform in order to be familiar with the user interface and its functionalities.", "labels": [], "entities": []}, {"text": "A screenshot of the evaluation interface is presented in.", "labels": [], "entities": []}, {"text": "We measure post-editing quality using sourcebased direct assessment (src-DA), as implemented in Appraise.", "labels": [], "entities": [{"text": "Appraise", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.8623201251029968}]}, {"text": "Scores are collected as x \u2208 [0, 100], focusing on adequacy (and not fluency, which previous WMT evaluation campaigns have found to be highly correlated with adequacy direct assessment results).", "labels": [], "entities": [{"text": "WMT evaluation", "start_pos": 92, "end_pos": 106, "type": "TASK", "confidence": 0.791164755821228}]}, {"text": "The original DA approach () is reference-based and, thus, needs to be adapted for use in our paraphrase assessment and translation scoring scenarios.", "labels": [], "entities": [{"text": "translation scoring", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.8973444700241089}]}, {"text": "Of course, this makes translation evaluation more difficult, as we require bilingual annotators.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.9809174239635468}]}, {"text": "Src-DA has previously been used, e.g., in.", "labels": [], "entities": []}, {"text": "Direct assessment initializes mental context for annotators by asking a priming question.", "labels": [], "entities": [{"text": "Direct assessment initializes mental context", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7316234171390533}]}, {"text": "The user interface shows two sentences: -the source (src-DA, reference otherwise); and -the candidate output.", "labels": [], "entities": []}, {"text": "Annotators read the priming question and both sentences and then assign a score x \u2208 [0, 100] to the candidate shown.", "labels": [], "entities": []}, {"text": "The interpretation of this score considers the context defined by the priming question, effectively allowing us to use the same annotation method to collect assessments wrt.", "labels": [], "entities": []}, {"text": "the different dimensions of quality as defined above.", "labels": [], "entities": []}, {"text": "Our priming questions are shown in.", "labels": [], "entities": []}, {"text": "Score convergence overtime for English-German assessments is presented in.", "labels": [], "entities": []}, {"text": "This figure tracks average system adequacy (as measured by Src-DA) overtime, as assessments come in from human annotators.", "labels": [], "entities": [{"text": "overtime", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.5647673010826111}]}, {"text": "Note that we use the so-called alternate HIT layout as named in the WMT18 findings paper, using an 88:12 split between actual assessments and those reserved for quality control.", "labels": [], "entities": [{"text": "WMT18 findings paper", "start_pos": 68, "end_pos": 88, "type": "DATASET", "confidence": 0.9615729848543803}]}, {"text": "All annotators have proven reliable, passing qualification tests.", "labels": [], "entities": []}, {"text": "The results of Src-DA for the English-German subtask are presented in.", "labels": [], "entities": []}, {"text": "Our main findings are as follows: \u2022 Human post-editing outperforms all auotmatic post-editing systems, the quality difference is significant; \u2022 UNBABEL achieves best APE performance; \u2022 USAAR DFKI comes in second; \u2022 POSTECH comes in third; \u2022 All but one APE systems outperform unedited NMT output; \u2022 Difference to the remaining APE system is not statistically significant.", "labels": [], "entities": [{"text": "APE", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.651347815990448}, {"text": "USAAR DFKI", "start_pos": 185, "end_pos": 195, "type": "DATASET", "confidence": 0.8498291969299316}]}, {"text": "Human evaluation does only result in very coarse result cluster.", "labels": [], "entities": []}, {"text": "Thus, in order to order submissions by their respective post-editing quality, as perceived by human annotators, we also present win-based results in.", "labels": [], "entities": []}, {"text": "For 2019, we did not run any human evaluation for the English-Russian subtask, due to lack of participation.", "labels": [], "entities": []}, {"text": "Instead, we focused annotation efforts on English-German.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Basic information about the APE shared task data released since 2015: languages, domain, type of MT technology,  repetition rate and initial translation quality (TER/BLEU of TGT). Grey columns refer to data covering different language pairs  and domains with respect to this year's evaluation round.", "labels": [], "entities": [{"text": "APE shared task", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7927800218264262}, {"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9488203525543213}, {"text": "repetition rate", "start_pos": 123, "end_pos": 138, "type": "METRIC", "confidence": 0.9709089398384094}, {"text": "TER/BLEU of TGT)", "start_pos": 172, "end_pos": 188, "type": "METRIC", "confidence": 0.7608015040556589}]}, {"text": " Table 4: Results for the WMT19 APE English-German subtask -average TER (\u2193), BLEU score (\u2191). The symbol \"*\" indicates  results differences between runs that are not statistically significant. The symbol \" \u2020\" indicates a difference from the MT baseline  that is not statistically significant.", "labels": [], "entities": [{"text": "WMT19 APE English-German subtask", "start_pos": 26, "end_pos": 58, "type": "DATASET", "confidence": 0.7199245244264603}, {"text": "average", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.6293816566467285}, {"text": "TER", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.5247013568878174}, {"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9875194728374481}, {"text": "MT baseline", "start_pos": 240, "end_pos": 251, "type": "DATASET", "confidence": 0.8584387004375458}]}, {"text": " Table 5: Results for the WMT19 APE English-Russian subtask -average TER (\u2193), BLEU score (\u2191).", "labels": [], "entities": [{"text": "WMT19 APE English-Russian subtask", "start_pos": 26, "end_pos": 59, "type": "DATASET", "confidence": 0.638317734003067}, {"text": "average", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.6249765157699585}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.6961992383003235}, {"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9837144017219543}]}, {"text": " Table 9: DA Human evaluation results for the English- German subtask in terms of average raw DA (Ave %) and  average standardized scores (Ave z). Dashed lines between  systems indicate clusters according to Wilcoxon signed-rank  test at p-level p \u2264 0.05.", "labels": [], "entities": [{"text": "DA", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.958747148513794}, {"text": "standardized scores (Ave z)", "start_pos": 118, "end_pos": 145, "type": "METRIC", "confidence": 0.9082249104976654}]}, {"text": " Table 10: DA Human evaluation results for the English- German subtask in terms of average raw DA (Ave %) and  average standardized scores (Ave z). Dashed lines between  systems indicate clusters according to number of wins.", "labels": [], "entities": [{"text": "DA", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9577346444129944}, {"text": "standardized scores (Ave z)", "start_pos": 119, "end_pos": 146, "type": "METRIC", "confidence": 0.9332985579967499}]}]}