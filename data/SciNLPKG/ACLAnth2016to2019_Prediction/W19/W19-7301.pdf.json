{"title": [{"text": "Neural Machine Translation of Literary Texts from English to Slovene", "labels": [], "entities": [{"text": "Neural Machine Translation of Literary Texts from English", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.8555007353425026}]}], "abstractContent": [{"text": "Neural Machine Translation has shown promising performance in literary texts.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8071037332216898}]}, {"text": "Since literary machine translation has not yet been researched for the English-to-Slovene translation direction, this paper aims to fulfill this gap by presenting a comparison among bespoke NMT models, tailored to novels, and Google Neural Machine Translation.", "labels": [], "entities": [{"text": "literary machine translation", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.6449983815352122}, {"text": "Google Neural Machine Translation", "start_pos": 226, "end_pos": 259, "type": "TASK", "confidence": 0.5897345468401909}]}, {"text": "The translation models were evaluated by the BLEU and METEOR metrics, assessment of fluency and adequacy, and measurement of the post-editing effort.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9671141505241394}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9923762679100037}, {"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8666025400161743}]}, {"text": "The findings show that all evaluated approaches resulted in an increase in translation productivity.", "labels": [], "entities": [{"text": "translation", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.9681859612464905}]}, {"text": "The translation model tailored to a specific author outperformed the model trained on a more diverse literary corpus, based on all metrics except the scores for fluency.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9627930521965027}]}, {"text": "However, the translation model by Google still outperforms all bespoke models.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9671319723129272}]}, {"text": "The evaluation reveals a very low inter-rater agreement on fluency and adequacy, based on the kappa coefficient values, and significant discrepancies between post-editors.", "labels": [], "entities": []}, {"text": "This suggests that these methods might not be reliable, which should be addressed in future studies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen the advent of Neural Machine Translation (NMT), which has shown promising performance in literary texts \u00a9 The authors.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 37, "end_pos": 69, "type": "TASK", "confidence": 0.7979402840137482}]}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND.", "labels": [], "entities": [{"text": "CCBY-ND", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9859224557876587}]}, {"text": "(. Most research on neural literary translation focused on the comparison of statistical and neural models, whereas this paper is one of the first to present a comparison exclusively among NMT models, specifically between models adapted to novels and the mixed-domain Google Neural Machine Translation (GNMT) system, exploring whether adaptation to literary text leads to better performance of NMT systems.", "labels": [], "entities": [{"text": "neural literary translation", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7218353549639384}, {"text": "mixed-domain Google Neural Machine Translation (GNMT)", "start_pos": 255, "end_pos": 308, "type": "TASK", "confidence": 0.7576291337609291}]}, {"text": "This is also the first research paper that investigates literary machine translation (MT) from English to the highly inflected and under-resourced Slovene language.", "labels": [], "entities": [{"text": "literary machine translation (MT)", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.7732781569163004}]}, {"text": "The models are evaluated both with automatic evaluation methodologies, more precisely the BLEU and the METEOR metrics, and human evaluation methods, i.e. an assessment of fluency and accuracy, a measurement of the temporal dimension of post-editing effort and error analysis.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.9970487952232361}, {"text": "METEOR metrics", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.8894275724887848}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9950975775718689}]}, {"text": "Since the neural models are evaluated by multiple evaluation methodologies, we are able to compare evaluation methods, and determine whether they are efficient.", "labels": [], "entities": []}, {"text": "Our hypotheses were that all models adapted to literary texts would yield better results than GNMT, based on automatic (hypothesis 1), as well as human evaluation, and that the model trained on out-of-domain parallel data and retrained on the novel Practice Makes Perfect (model 'Novel') would perform better than the model trained on out-of-domain parallel data and retrained on the corpus SPOOK (model 'SPOOK'), according to both automatic (hypothesis 3) and human evaluation (hypothesis 4).", "labels": [], "entities": []}], "datasetContent": [{"text": "As manual evaluation is time-consuming and expensive to perform, it is regarded to be more accurate than automatic evaluation.", "labels": [], "entities": []}, {"text": "However, research conducted by revealed low inter-annotation agreement for the assessment of fluency and adequacy, calling this method into question.", "labels": [], "entities": []}, {"text": "To determine the inter-annotator agreement, they calculated the kappa coefficient, which is the proportion of time two or more annotators assigned identical scores to the same segments.", "labels": [], "entities": [{"text": "kappa coefficient", "start_pos": 64, "end_pos": 81, "type": "METRIC", "confidence": 0.9403519630432129}]}, {"text": "According to, result from 0.0 to 0.2 means slight agreement, 0.21 to 0.4 fair, 0.41 to 0.6 moderate, 0.61 to 0.8 substantial and a higher score than 0.8 means almost perfect agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.8889121413230896}]}, {"text": "Analysis performed by revealed that the inter-annotation agreement for assessing fluency and adequacy was merely fair.", "labels": [], "entities": []}, {"text": "In this section, we give an overview of the training and test datasets used in our experiment.", "labels": [], "entities": []}, {"text": "Then, we present NMT systems and give insights into evaluation methods.", "labels": [], "entities": []}, {"text": "Firstly, all models were evaluated based on automatic evaluation methodologies.", "labels": [], "entities": []}, {"text": "Then, we conducted a more detailed human evaluation of GNMT and two bespoke models, i.e. the SPOOK and the Novel NMT models.", "labels": [], "entities": [{"text": "GNMT", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8228460550308228}]}, {"text": "For the automatic evaluation, we used the BLEU () and METEOR) metrics, which are based on the correspondence of the MT output and the reference translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9989959597587585}, {"text": "METEOR) metrics", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9550493359565735}]}, {"text": "The BLEU score was obtained with the Interactive BLEU score evaluator, 1 which is available on the Tilde platform, whereas the METEOR score was calculated by the automatic machine translation evaluation system METEOR, available on GitHub.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9717013537883759}, {"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.8368824124336243}, {"text": "Tilde platform", "start_pos": 99, "end_pos": 113, "type": "DATASET", "confidence": 0.965215653181076}, {"text": "METEOR score", "start_pos": 127, "end_pos": 139, "type": "METRIC", "confidence": 0.9571907818317413}]}, {"text": "The human evaluation consisted of error analysis of the MT output, an assessment of fluency and adequacy, and a measurement of the temporal dimension of post-editing (PE) effort.", "labels": [], "entities": [{"text": "MT output", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.8735005557537079}]}, {"text": "Twelve Master's students in translation or interpreting took part in the evaluation.", "labels": [], "entities": [{"text": "translation or interpreting", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.9136144518852234}]}, {"text": "On average, participants had at least four years of translation experience and 83% of them have already had some PE experience.", "labels": [], "entities": [{"text": "translation", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9794312119483948}, {"text": "PE", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.902787446975708}]}, {"text": "Each translated one excerpt from the novel Something about you by Julie James and post-edited the hypotheses of a similar excerpt, while assessing the fluency and adequacy of each segment.", "labels": [], "entities": []}, {"text": "The translators were divided into six groups of two: groups A and B evaluated GNMT, C and D evaluated the translations provided by the SPOOK neural model, and E and F by the Novel model.", "labels": [], "entities": []}, {"text": "In that way, all three models were evaluated by four participants each and on two excerpts.", "labels": [], "entities": []}, {"text": "Participants also provided feedback after the translation via a questionnaire.", "labels": [], "entities": []}, {"text": "Participants translated and post-edited MT outputs using the Post-Editing Tool (PET) interface (Aziz et al., 2012), a CAT tool built for research purposes.", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.884989470243454}]}, {"text": "PET measures time spent on editing each segment, tracks changes and allows adding optional assessments, which can be configured via a context file.", "labels": [], "entities": [{"text": "PET", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7365939021110535}]}, {"text": "Thus, after confirming a post-edited sentence, participants also assessed its fluency and adequacy on a pop-up assessment page before moving to the next sentence.", "labels": [], "entities": []}, {"text": "Prior to the beginning of the assigned tasks, participants were provided with guidelines in order to produce professional quality translations.", "labels": [], "entities": []}, {"text": "Moreover, they postedited automatically generated translation of a short excerpt from the novel Something about you, containing three sentences, to familiarize themselves with the PET tool and the workflow.", "labels": [], "entities": []}, {"text": "We followed TAUS guidelines for quality evaluation using adequacy and fluency approaches.", "labels": [], "entities": [{"text": "TAUS", "start_pos": 12, "end_pos": 16, "type": "DATASET", "confidence": 0.8078593015670776}]}, {"text": "Participants were asked to rate adequacy on a 4-point scale based on the extent to which the meaning, expressed in the source, is also expressed in the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 152, "end_pos": 154, "type": "TASK", "confidence": 0.7558993101119995}]}, {"text": "Score 4 means that all meaning is expressed, 3 means most meaning, 2 little meaning and 1 means that no meaning is expressed in the hypothesis provided by the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 159, "end_pos": 161, "type": "TASK", "confidence": 0.8107569813728333}]}, {"text": "The second 4-point scale indicates how fluent and grammatically wellformed the hypothetical translation is.", "labels": [], "entities": []}, {"text": "In this case, score 4 means that a translation is written in flawless Slovene, 3 means good Slovene, 2 means disfluent Slovene and 1 means that it is incomprehensible.", "labels": [], "entities": []}, {"text": "After the assessment, we measured inter-annotation agreement using the kappa coefficient.", "labels": [], "entities": []}, {"text": "In addition to the measuring of the PE effort and assessing fluency and adequacy, we also compared GNMT, the SPOOK and the Novel NMT models based on an error analysis.", "labels": [], "entities": [{"text": "PE", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9530667662620544}, {"text": "GNMT", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.7712207436561584}, {"text": "SPOOK", "start_pos": 109, "end_pos": 114, "type": "DATASET", "confidence": 0.7104107737541199}]}, {"text": "shows the results of the automatic evaluation.", "labels": [], "entities": []}, {"text": "It revealed that GNMT achieved the best METEOR and BLEU score (30 and 21.97 respectively), followed by Novel with METEOR score of 20.35 and BLEU score of 20.75, and SPOOK with METEOR score of 19.67 and BLEU score of 19.01.", "labels": [], "entities": [{"text": "GNMT", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.809643030166626}, {"text": "METEOR", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9781010150909424}, {"text": "BLEU score", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.9817984104156494}, {"text": "Novel", "start_pos": 103, "end_pos": 108, "type": "DATASET", "confidence": 0.9729803800582886}, {"text": "METEOR score", "start_pos": 114, "end_pos": 126, "type": "METRIC", "confidence": 0.9647515118122101}, {"text": "BLEU score", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9844516515731812}, {"text": "METEOR score", "start_pos": 176, "end_pos": 188, "type": "METRIC", "confidence": 0.9557231962680817}, {"text": "BLEU score", "start_pos": 202, "end_pos": 212, "type": "METRIC", "confidence": 0.9871882796287537}]}, {"text": "These findings refute the first hypothesis predicting that models tailored to literature would achieve better scores than GNMT.", "labels": [], "entities": []}, {"text": "On the other hand, the results confirmed the third hypothesis supposing that the Novel model, tailored to a specific author, would perform better than the SPOOK model, trained on a bigger but more varied literary corpus.", "labels": [], "entities": []}, {"text": "The lowest score was obtained by the Just Novel model, with two layers.", "labels": [], "entities": []}, {"text": "However, a similar model with four layers, trained on the same training set, obtained higher scores, although it produced considerably lower quality translations consisting of just six words.", "labels": [], "entities": []}, {"text": "This indicates that BLEU and METEOR scores are not always accurate.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.999256432056427}, {"text": "METEOR", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9911209940910339}]}, {"text": "The combined SPOOK + Novel model that was trained on the corpus SPOOK and on the corpus, consisting of a novel Practice Makes Perfect and its translation, performed worse than the models, trained on just one of those corpora.", "labels": [], "entities": []}, {"text": "According to the BLEU metric, it performed even worse than the model, trained solely on out-of-domain data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9964417815208435}]}, {"text": "This contradicts the common belief that the addition of more training data always leads to better results.", "labels": [], "entities": []}, {"text": "In the case of the SPOOK + Novel neural model we can also observe a discrepancy between the BLEU and METEOR metrics.", "labels": [], "entities": [{"text": "SPOOK + Novel neural model", "start_pos": 19, "end_pos": 45, "type": "DATASET", "confidence": 0.7059873938560486}, {"text": "BLEU", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.9987608194351196}, {"text": "METEOR", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9738425612449646}]}, {"text": "According to the METEOR metric, this model outperforms the baseline by 0.62 point, whereas based on the BLEU metric, it achieves 1.48 fewer points.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9514196515083313}, {"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9976359605789185}]}, {"text": "Furthermore, the biggest difference between BLEU and METEOR scores is 8.03 points in the case of GNMT, whereas in the case of another model, the difference is only 0.40 point..", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9983828067779541}, {"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9749724268913269}, {"text": "GNMT", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.7711867094039917}]}], "tableCaptions": [{"text": " Table 1. Statistics on datasets, used for training the  neural translation models", "labels": [], "entities": [{"text": "neural translation", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.74673992395401}]}, {"text": " Table 2. Results of the automatic evaluation", "labels": [], "entities": []}, {"text": " Table 3. Measurement of the temporal dimension of post-editing effort  GNMT  SPOOK  Novel  Average difference between  translation and PE time (min)  2.9  0.5  1.0", "labels": [], "entities": [{"text": "GNMT  SPOOK  Novel  Average difference", "start_pos": 72, "end_pos": 110, "type": "METRIC", "confidence": 0.7237787961959838}, {"text": "PE time", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.8349078893661499}]}, {"text": " Table 4. Average difference between translation and PE time", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9856495261192322}, {"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.8797139525413513}, {"text": "PE time", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9214727580547333}]}]}