{"title": [{"text": "The relation between dependency distance and frequency", "labels": [], "entities": []}], "abstractContent": [{"text": "This present pilot study investigates the relationship between dependency distance and frequency based on the analysis of an English dependency treebank.", "labels": [], "entities": [{"text": "English dependency treebank", "start_pos": 125, "end_pos": 152, "type": "DATASET", "confidence": 0.6916117866834005}]}, {"text": "The preliminary result shows that there is a non-linear relation between dependency distance and frequency.", "labels": [], "entities": []}, {"text": "This relation between them can be further formalized as a power law function which can be used to predict the distribution of dependency distance in a treebank.", "labels": [], "entities": []}], "introductionContent": [{"text": "As a well-discussed norm), dependency distance shows several attractive features for quantitative studies.", "labels": [], "entities": []}, {"text": "First, its definition is rather clear.", "labels": [], "entities": []}, {"text": "It is the linear distance between a word and its head.", "labels": [], "entities": []}, {"text": "1 Second, it is very easy to quantify.", "labels": [], "entities": []}, {"text": "We can simply compute dependency distance as the difference of the word ID and its head's ID in a CoNLL style treebank).", "labels": [], "entities": [{"text": "CoNLL style treebank", "start_pos": 98, "end_pos": 118, "type": "DATASET", "confidence": 0.8129420280456543}]}, {"text": "These features together with the emergence of large-scale dependency treebanks made dependency distance one of the popular topics in quantitative syntactic studies.", "labels": [], "entities": [{"text": "dependency distance", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8745692670345306}]}, {"text": "Among various interesting discussions, the most striking finding is probably the dependency distance minimization phenomena.", "labels": [], "entities": [{"text": "dependency distance minimization", "start_pos": 81, "end_pos": 113, "type": "TASK", "confidence": 0.7552603085835775}]}, {"text": "After empirically examining the dependency distance distributions of different human languages and comparing the results with different random baselines, found that there is a universal trend of minimizing the dependency distance inhuman languages.", "labels": [], "entities": []}, {"text": "conducted a similar study which widened the language range and added one more random baseline.", "labels": [], "entities": []}, {"text": "Their results are coherent with Liu's finding.", "labels": [], "entities": []}, {"text": "Both and connect this phenomenon with the short-term memory (or working memory) storage of human beings and the least effort principle.", "labels": [], "entities": [{"text": "short-term memory (or working memory) storage", "start_pos": 42, "end_pos": 87, "type": "TASK", "confidence": 0.5813652314245701}]}, {"text": "Since long dependencies, which have longer distance, occupy more shortterm memory storage, they are more difficult or inefficient to process.", "labels": [], "entities": []}, {"text": "Therefore, for lowering the processing difficulty and boosting the efficiency of communications, short dependencies are preferable according to the least effort principle.", "labels": [], "entities": []}, {"text": "Initially, the least effort principle was brought up by Zipf for explaining the observed power-law distributions of word frequencies.", "labels": [], "entities": []}, {"text": "Later on, similar power-law frequency distributions have been repeatedly observed in various linguistic units, such as letters, phoneme, word length, and etc..", "labels": [], "entities": []}, {"text": "The power law distribution, therefore, has been considered as a universal linguistic law.", "labels": [], "entities": []}, {"text": "After investigating the relationships between different word features (such as length vs frequency, frequency vs polysemy, and etc.), people found out an interesting phenomenon.", "labels": [], "entities": []}, {"text": "The relations between two highly correlated word features are usually non-linear and can be formulated as a power law function.", "labels": [], "entities": []}, {"text": "further proposed a word synergetic framework to model the interactions between different word features.", "labels": [], "entities": []}, {"text": "This model has proved quite successful also then adapted to syntax features.", "labels": [], "entities": []}, {"text": "The first studies mainly focused on the analysis of phrase structure treebanks, which naturally are limited in language types since phrase structure grammar is less suitable for describing free word order languages.", "labels": [], "entities": []}, {"text": "As the dependency treebanks are getting dominant, studies based on dependency grammar start to take lead.", "labels": [], "entities": []}, {"text": "We can find recent studies discussing the relations between sentence lengths, tree heights, tree widths, and mean dependency distances.", "labels": [], "entities": []}, {"text": "Hudson's original measures takes two adjacent words to have distance zero.", "labels": [], "entities": []}, {"text": "We prefer the alternative definition where x = y \u27fa d (x, y) = 0, i.e. a word has distance zero with itself, making the measure a metric in the mathematical sense.", "labels": [], "entities": []}, {"text": "Knowing that short dependencies are preferable by languages due to the least effort principle and that syntax features behavior similar to word features, we can easily draw our hypotheses: \u2022 The relation between dependency distance and frequency can be formulated as a non-linear function (probably also a power law function).", "labels": [], "entities": []}, {"text": "Contrary to above-mentioned studies, our study here is not focusing on mean dependency distances but the distribution of the distance of every single dependency.", "labels": [], "entities": []}, {"text": "In the dependency minimization studies or synergetic syntax studies, the observed feature is mean dependency distance per sentence.", "labels": [], "entities": [{"text": "dependency minimization", "start_pos": 7, "end_pos": 30, "type": "TASK", "confidence": 0.7886000871658325}]}, {"text": "Ina way, these observed dependency distances are treated as a dependent feature of dependency trees.", "labels": [], "entities": []}, {"text": "This is a very reasonable choice since the dependency distance is defined as the linear distance between two words in the same sentence.", "labels": [], "entities": []}, {"text": "In particular, when the studies discuss other tree-related features such as tree heights and widths, mean dependency distance is a more easily comparable feature than a group of individual dependency distances.", "labels": [], "entities": [{"text": "mean dependency distance", "start_pos": 101, "end_pos": 125, "type": "METRIC", "confidence": 0.7759539683659872}]}, {"text": "However, we believe the value of individual dependency distances is neglected.", "labels": [], "entities": []}, {"text": "Individual dependency distances ( provide more details of the fluctuation than the average which would level-up differences of dependencies in a sentence and it should be given the same attention as the mean dependency distance.", "labels": [], "entities": []}, {"text": "Therefore, our study here is trying to pickup the missing detail of previous studies by investigating the relations between individual dependency distances and their frequencies.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the data set, the Parallel Universal Dependencies (PUD) English treebank of Universal Dependencies treebanks, and introduces our computing method for dependency distance and frequency.", "labels": [], "entities": [{"text": "English treebank of Universal Dependencies treebanks", "start_pos": 76, "end_pos": 128, "type": "DATASET", "confidence": 0.872432142496109}]}, {"text": "Section 3 presents the empirical results and discussions.", "labels": [], "entities": []}, {"text": "Finally, Section 4 presents our conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: R 2 of four non-linear models.", "labels": [], "entities": []}, {"text": " Table 2: R 2 results of RT.", "labels": [], "entities": [{"text": "RT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.8451553583145142}]}, {"text": " Table 3: R 2 results of PRT.", "labels": [], "entities": [{"text": "PRT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8006234169006348}]}, {"text": " Table 4: R 2 results for syntactic dependencies.", "labels": [], "entities": []}]}