{"title": [{"text": "BERT Masked Language Modeling for Co-reference Resolution", "labels": [], "entities": [{"text": "BERT Masked Language Modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.605127353221178}, {"text": "Co-reference Resolution", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.7541306018829346}]}], "abstractContent": [{"text": "This paper explains the TALP-UPC participation for the Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for Natural Language Processing.", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution shared-task of the 1st ACL Workshop on Gender Bias for Natural Language Processing", "start_pos": 55, "end_pos": 165, "type": "TASK", "confidence": 0.737677551805973}]}, {"text": "We have implemented two models for mask language modeling using pre-trained BERT adjusted to work fora classification problem.", "labels": [], "entities": [{"text": "mask language modeling", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.7734819650650024}, {"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9957855343818665}]}, {"text": "The proposed solutions are based on the word probabilities of the original BERT model, but using common English names to replace the original test names.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.957567036151886}]}], "introductionContent": [{"text": "The Gendered Pronoun Resolution task is a natural language processing task whose objective is to build pronoun resolution systems that identify the correct name a pronoun refers to.", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution task", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7173598259687424}, {"text": "pronoun resolution", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7474050223827362}]}, {"text": "It's called a co-reference resolution task.", "labels": [], "entities": [{"text": "co-reference resolution task", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.8011330763498942}]}, {"text": "Co-reference resolution tackles the problem of different elements of a text that refer to the same thing.", "labels": [], "entities": [{"text": "Co-reference resolution", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7793359160423279}]}, {"text": "Like for example a pronoun and a noun, or multiple nouns that describe the same entity.", "labels": [], "entities": []}, {"text": "There are multiple deep learning approaches to this problem.", "labels": [], "entities": []}, {"text": "NeuralCoref 1 presents one based on giving every pair of mentions (pronoun + noun) a score to represent whether or not they refer to the same entity.", "labels": [], "entities": []}, {"text": "In our current task, this approach is not possible, because we don't have the true information of every pair of mentions, only the two names per entry.", "labels": [], "entities": []}, {"text": "The current task also has to deal with the problem of gender.", "labels": [], "entities": []}, {"text": "As the GAP researchers point out, the biggest and most common datasets for co-reference resolution have a bias towards male entities.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 75, "end_pos": 98, "type": "TASK", "confidence": 0.8222123086452484}]}, {"text": "For example the OntoNotes dataset, which is used for some of the most popular models, only has a 25% female representation.", "labels": [], "entities": [{"text": "OntoNotes dataset", "start_pos": 16, "end_pos": 33, "type": "DATASET", "confidence": 0.961603045463562}]}, {"text": "This creates a problem, because any machine learning model is only as good as its training set.", "labels": [], "entities": []}, {"text": "Biased training sets will create biased models, and this will have repercussions on any uses the model may have.", "labels": [], "entities": []}, {"text": "This task provides an interesting challenge specially by the fact that it is proposed over a gender neutral dataset.", "labels": [], "entities": []}, {"text": "In this sense, the challenge is oriented towards proposing methods that are genderneutral and to not provide bias given that the data set does not have it.", "labels": [], "entities": []}, {"text": "To face this task, we propose to make use of the recent popular BERT tool.", "labels": [], "entities": [{"text": "BERT", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9677852988243103}]}, {"text": "BERT is a model trained for masked language modeling (LM) word prediction and sentence prediction using the transformer network (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9404923915863037}, {"text": "masked language modeling (LM) word prediction", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.795668113976717}, {"text": "sentence prediction", "start_pos": 78, "end_pos": 97, "type": "TASK", "confidence": 0.7484159767627716}]}, {"text": "BERT also provides a group of pretrained models for different uses, of different languages and sizes.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5476347804069519}]}, {"text": "There are implementations for it in all sorts of tasks, including text classification, question answering, multiple choice question answering, sentence tagging, among others.", "labels": [], "entities": [{"text": "text classification", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7750535309314728}, {"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8988793790340424}, {"text": "multiple choice question answering", "start_pos": 107, "end_pos": 141, "type": "TASK", "confidence": 0.5919806435704231}, {"text": "sentence tagging", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7530986368656158}]}, {"text": "BERT is gaining popularity quickly in language tasks, but before this shared-task appeared, we had no awareness of its implementation in co-reference resolution.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.980588972568512}, {"text": "co-reference resolution", "start_pos": 137, "end_pos": 160, "type": "TASK", "confidence": 0.7686474025249481}]}, {"text": "For this task, we've used an implementation that takes advantage of the masked LM which BERT is trained for and uses it fora kind of task BERT is not specifically designed for.", "labels": [], "entities": [{"text": "BERT", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9710898399353027}]}, {"text": "In this paper, we are detailing our shared-task participation, which basically includes descriptions on the use we gave to the BERT model and on our technique of 'Name Replacement' that allowed to reduce the impact of name frequency.", "labels": [], "entities": [{"text": "BERT", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.8988449573516846}, {"text": "Name Replacement", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.8794018626213074}]}, {"text": "2 Co-reference Resolution System Description 2.1 BERT for Masked LM This model's main objective is to predict a word that has been masked in a sentence.", "labels": [], "entities": [{"text": "BERT", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9934773445129395}]}, {"text": "For this exer-cise that word is the pronoun whose referent we're trying to identify.", "labels": [], "entities": []}, {"text": "This one pronoun gets replaced by the tag, the rest of the sentence is subjected to the different name change rules described in section 2.2.", "labels": [], "entities": []}, {"text": "The text is passed through the pre-trained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9737828969955444}]}, {"text": "This model keeps all of its weights intact, the only changes made in training are to the network outside of the BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.916478157043457}]}, {"text": "The resulting sequence then passes through what is called the masked language modeling head.", "labels": [], "entities": []}, {"text": "This consists of a small neural network that returns, for every word in the sequence, an array the size of the entire vocabulary with the probability for every word.", "labels": [], "entities": []}, {"text": "The array for our masked pronoun is extracted and then from that array, we get the probabilities of three different words.", "labels": [], "entities": []}, {"text": "These three words are : the first replaced name (name 1), the second replaced name (name 2) and the word none for the case of having none.", "labels": [], "entities": []}, {"text": "This third case is the strangest one, because the word none would logically not appear in the sentence.", "labels": [], "entities": []}, {"text": "Tests were made with the original pronoun as the third option instead.", "labels": [], "entities": []}, {"text": "But the results ended up being very similar albeit slightly worse, so the word none was kept instead.", "labels": [], "entities": [{"text": "none", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.8348823189735413}]}, {"text": "These cases where there is no true answer are the hardest ones for both of the models.", "labels": [], "entities": []}, {"text": "We experimented with two models.", "labels": [], "entities": []}, {"text": "Model 1 After the probabilities for each word are extracted, the rest is treated as a classification problem.", "labels": [], "entities": []}, {"text": "An array is created with the probabilities of the 2 names and none ([name 1, name 2, none]), where each one represents the probability of a class in multi-class classification.", "labels": [], "entities": []}, {"text": "This array is passed through a softmax function to adjust it to probabilities between 0 and 1 and then the log loss is calculated.", "labels": [], "entities": []}, {"text": "A block diagram of this model can be seen in.", "labels": [], "entities": []}, {"text": "Model 2 This model repeats the steps of model 1 but for two different texts.", "labels": [], "entities": []}, {"text": "These texts are mostly the same except the replacement names name 1 and name 2 have been switched (as explained in the section 2.2).", "labels": [], "entities": []}, {"text": "It calculates the probabilities for each word for each text and then takes an average of both.", "labels": [], "entities": []}, {"text": "Then finally applies the softmax and calculates the loss with the average probability of each class across both texts.", "labels": [], "entities": []}, {"text": "A block diagram of this model can be seen in figure 2.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Dataset distribution for the datasets of stages  1 and 2.", "labels": [], "entities": []}, {"text": " Table 2: Results of the tuning for both models. Min- imum and average Loss and Accuracy across all the  tuning experiments performed.", "labels": [], "entities": [{"text": "average Loss", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.7194380164146423}, {"text": "Accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9087071418762207}]}, {"text": " Table 3: Hyperparameters for the model training", "labels": [], "entities": []}, {"text": " Table 4: Model 1 results for the testing stage 1.", "labels": [], "entities": []}, {"text": " Table 5: Model 2 results for the testing stage 1.", "labels": [], "entities": []}, {"text": " Table 6: Results for the tests with different BERT im- plementations.", "labels": [], "entities": [{"text": "BERT im- plementations", "start_pos": 47, "end_pos": 69, "type": "METRIC", "confidence": 0.9422750920057297}]}, {"text": " Table 8: Results for both models across both stages of  the competition", "labels": [], "entities": []}]}