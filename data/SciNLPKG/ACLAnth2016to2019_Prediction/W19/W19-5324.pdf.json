{"title": [], "abstractContent": [{"text": "This paper describes our submission to the WMT 2019 Chinese-English (zh-en) news translation shared task.", "labels": [], "entities": [{"text": "WMT 2019 Chinese-English (zh-en) news translation shared task", "start_pos": 43, "end_pos": 104, "type": "TASK", "confidence": 0.6511468976736069}]}, {"text": "Our systems are based on RNN architectures with pre-trained embeddings which utilize character and sub-character information.", "labels": [], "entities": []}, {"text": "We compare models with these different granularity levels using different evaluating metics.", "labels": [], "entities": []}, {"text": "We find that a finer granularity embeddings can help the model according to character level evaluation and that the pre-trained embeddings can also be beneficial for model performance marginally when the training data is limited.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine Translation (NMT) systems are mostly based on an encoder-decoder architecture with attention.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8316920101642609}]}, {"text": "Given a sentence x in source language, the model predicts a corresponding output sentence yin target language, which maximizes the conditional probability p(y|x).", "labels": [], "entities": []}, {"text": "The attentionbased Recurrent Neural Network (RNN) version of this architecture has been a very popular approach to NMT (.", "labels": [], "entities": []}, {"text": "Despite the success of these models, they still suffer from problems such as outof-vocabulary (OOV) words, i.e. words that have not been seen at training.", "labels": [], "entities": [{"text": "outof-vocabulary (OOV", "start_pos": 77, "end_pos": 98, "type": "METRIC", "confidence": 0.9095725218454996}]}, {"text": "To alleviate the OOV problem, we follow the methods used in word representation and segment words into smaller units.", "labels": [], "entities": [{"text": "OOV", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.827659010887146}, {"text": "word representation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7173415571451187}]}, {"text": "In some morphorlogically rich languages such as Chinese, a word can be divided into characters and then the characters can be further divided into smaller components called glyphs.", "labels": [], "entities": []}, {"text": "Both character and glyph might contain semantic information and therefore utilizing such information might help alleviate the OOV problem.", "labels": [], "entities": [{"text": "OOV", "start_pos": 126, "end_pos": 129, "type": "TASK", "confidence": 0.7419598698616028}]}, {"text": "Based on the RNN attention-based model, we experiment with different granularity levels on the WMT19 Chinese-English (zh-en) news translation shared task.", "labels": [], "entities": [{"text": "WMT19 Chinese-English (zh-en) news translation shared task", "start_pos": 95, "end_pos": 153, "type": "TASK", "confidence": 0.8187395003106859}]}, {"text": "This paper describes our submitted systems with embeddings pre-trained on monolingual corpora.", "labels": [], "entities": []}, {"text": "The two submitted systems use pre-trained embeddings enhanced by character and sub-character information respectively.", "labels": [], "entities": []}, {"text": "The preprocessing methods include Chinese word segmentation, tokenization, data filtering based on rules and Byte Pair Encoding (BPE).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.5949679017066956}, {"text": "tokenization", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.9671704769134521}, {"text": "data filtering", "start_pos": 75, "end_pos": 89, "type": "TASK", "confidence": 0.7404528558254242}]}, {"text": "Our baseline model is based on RNNSearch () operating on word level and we use Long Short-Term Memory (LSTM)) as encoder and decoder.", "labels": [], "entities": []}, {"text": "For character level word embeddings, we use the CharacterEnhanced Word Embedding (CWE) proposed by.", "labels": [], "entities": []}, {"text": "For the sub-character level embeddings, we use the Joint Learning Word Embedding (JWE) proposed by.", "labels": [], "entities": []}, {"text": "We use various metrics, namely BLEU (), METEOR (Denkowski and Lavie, 2011), TER () and CharacTER () for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.999022364616394}, {"text": "METEOR", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.996951699256897}, {"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9980408549308777}, {"text": "CharacTER", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.8404407501220703}]}, {"text": "When compared with our baseline model, the models with pre-trained sub-character level embeddings on monolingual corpus show better performance, achieving an increase of +0.53 BLEU score with the sub-character level embeddings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9972557425498962}]}, {"text": "We ran additional experiments on the character and subcharacter level pre-trained embeddings and found that the use of these embeddings can benefit the model when the training corpus size is limited.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: Section 2 introduces the related work including the model architecture and pre-trained embeddings used in our experiment.", "labels": [], "entities": []}, {"text": "In Section 3, data selection and preprocessing methods are described.", "labels": [], "entities": [{"text": "data selection", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7692871689796448}]}, {"text": "Section 4 introduces the model architectures and hyperparameter settings.", "labels": [], "entities": []}, {"text": "Section 5 shows the evaluation results on models with different granularity levels.", "labels": [], "entities": []}, {"text": "Section 6 shows additional experiments to better understand our models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Model performance on different granularity levels. The two models with a star are the official systems  submitted to the WMT19 zh-en news translation shared task, where the pre-trained embeddings is trained on extra  monolingual data.", "labels": [], "entities": [{"text": "WMT19 zh-en news translation shared task", "start_pos": 131, "end_pos": 171, "type": "TASK", "confidence": 0.7599248687426249}]}, {"text": " Table 3: BLEU score with different training data sizes.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9597773551940918}]}, {"text": " Table 4. The model with JWE pre- trained embeddings performs better on all corpus  sizes, having a lower perplexity, though the differ- ence is marginal. Similar result as the BLEU eval- uation shows that the pre-trained embeddings ben- efit model performance on smaller corpus sizes.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9794617295265198}]}, {"text": " Table 4: Model perplexity on test set.", "labels": [], "entities": []}, {"text": " Table 5: BLEU and CharacTER for transformer mod- els.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987164735794067}, {"text": "CharacTER", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.6607398390769958}]}]}