{"title": [{"text": "Segmentation for Domain Adaptation in Arabic", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7213724702596664}]}], "abstractContent": [{"text": "Segmentation serves as an integral part in many NLP applications including Machine Translation, Parsing, and Information Retrieval.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8430003523826599}, {"text": "Parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9392107725143433}, {"text": "Information Retrieval", "start_pos": 109, "end_pos": 130, "type": "TASK", "confidence": 0.8097788989543915}]}, {"text": "When a model trained on the standard language is applied to dialects, the accuracy drops dramatically.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995111227035522}]}, {"text": "However, there are more lexical items shared by the standard language and dialects than can be found by mere surface word matching.", "labels": [], "entities": []}, {"text": "This shared lexicon is obscured by a lot of cliticization, gemination, and character repetition.", "labels": [], "entities": [{"text": "character repetition", "start_pos": 75, "end_pos": 95, "type": "TASK", "confidence": 0.7174784541130066}]}, {"text": "In this paper, we prove that segmentation and base normalization of dialects can help in domain adaptation by reducing data sparseness.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7599799633026123}]}, {"text": "Segmentation will improve a system performance by reducing the number of OOVs, help isolate the differences and allow better utilization of the commonal-ities.", "labels": [], "entities": []}, {"text": "We show that adding a small amount of dialectal segmentation training data reduced OOVs by 5% and remarkably improves POS tagging for dialects by 7.37% f-score, even though no dialect-specific POS training data is included.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.9510477185249329}, {"text": "POS tagging", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.8926325440406799}, {"text": "f-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9931517839431763}]}], "introductionContent": [{"text": "Processing of informal and dialectal data is increasingly becoming the focus of attention for many NLP tasks particularly due to the growing popularity of the various social media platforms and messaging apps which have transformed the way people interact and communicate with each other on daily basis and accelerated the pace of change of the language used on the web.", "labels": [], "entities": []}, {"text": "Today, many people write in the language they speak, leading to the influx of informal and dialectal data with the huge challenges they pose, most prominently among them are the non-standard orthography (like repeated characters for emphasis), abbreviations, non-conventional syntactic structures, spelling variability as well as mispellings, and code-switching.", "labels": [], "entities": []}, {"text": "These phenomena have been largely ignored in mainstream language processing models which mostly relied on (and also expected) standard, monolingual, clean, and edited texts.", "labels": [], "entities": []}, {"text": "Moreover, the emergence of intelligent personal assistant systems (such as Siri, Alexa, Cortana and Google Assistant) have created a paradigm shift in how people interact with smart devices.", "labels": [], "entities": []}, {"text": "Instead of issuing key words searches and formal questions, they are now more tempted to speak casually with these systems using their everyday language, which lays a growing burden on virtual assistants to accommodate unconventional (and previously unseen) queries and requests.", "labels": [], "entities": []}, {"text": "In this paper we show how NLP applications can scale up their performance on dialectal data by integrating a basic and simple preprocessing step, i.e. segmentation.", "labels": [], "entities": []}, {"text": "The process of segmentation is important for languages where the notion of word does not straightforwardly align with the common concept of a space-delimited string.", "labels": [], "entities": []}, {"text": "Arabic is a clitic language, where syntactic units can attach to other lexemes, and segmentation means identifying and splitting these syntactic units from the main lexemes or from each others.", "labels": [], "entities": []}, {"text": "This is not a deterministic process, as we need to tell, for example, whether the letter wa is a conjunction as in wa-Khaled \"and Khaled\" or part of the internal word build-up as in wahid \"Wahid\".", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a brief account of the related research on standard and dialectal segmentation of Arabic.", "labels": [], "entities": [{"text": "dialectal segmentation of Arabic", "start_pos": 72, "end_pos": 104, "type": "TASK", "confidence": 0.7898113578557968}]}, {"text": "In Section 3 we introduce our segmentation annotation scheme, explaining the meaning of clitics and how different they are from affixes, and compare our annotation convention to other approaches.", "labels": [], "entities": []}, {"text": "Section 4 gives the details of our work on dialectal data collection, explaining the challenges facing extraction, filtration and sampling.", "labels": [], "entities": [{"text": "dialectal data collection", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.8160409132639567}]}, {"text": "Section 5 spells out our hypothesis on how segmentation can help in domain adaptation and the approach we follow to test this hypothesis.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7765142619609833}]}, {"text": "In Section 6 we describe our parsing system and the features used.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9660404920578003}]}, {"text": "In Section 7 we explain our experimental setup and discuss the results, and finally Section 8 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have a high performance MSA segmenter, and when we adapt to the dialectal domain, we want to make sure that the performance on MSA data does not suffer from significant degradation.", "labels": [], "entities": [{"text": "MSA segmenter", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.9325768947601318}]}, {"text": "Therefore we build two models, one using the MSA data alone, and the other using MSA data combined with the Egyptian (EG) dialectal segmentation training data, and we evaluate both systems on the MSA and EG test sets.", "labels": [], "entities": [{"text": "Egyptian (EG) dialectal segmentation training data", "start_pos": 108, "end_pos": 158, "type": "DATASET", "confidence": 0.5838194489479065}, {"text": "EG test sets", "start_pos": 204, "end_pos": 216, "type": "DATASET", "confidence": 0.938837468624115}]}, {"text": "shows, the model trained on MSA gives an F-1 score of 97.91% on the MSA test data and a remarkably lower score on the EG data (82.56%).", "labels": [], "entities": [{"text": "F-1", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9991076588630676}, {"text": "MSA test data", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.883554220199585}, {"text": "EG data", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.7813034653663635}]}, {"text": "For a task as basic as segmentation, this level of performance is not reliable to pass onto other downstream or upstream tasks such as IR or MT.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.9664677977561951}, {"text": "MT", "start_pos": 141, "end_pos": 143, "type": "TASK", "confidence": 0.6953731775283813}]}, {"text": "When we train our model on the combined data of MSA+EG, there is a slight reduction in the performance on the MSA test set (about 0.3% absolute), while there a huge performance boost on the EG test set (8.84% absolute).", "labels": [], "entities": [{"text": "MSA test set", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.8997416297594706}, {"text": "EG test set", "start_pos": 190, "end_pos": 201, "type": "DATASET", "confidence": 0.9020589192708334}]}, {"text": "The overall score on EG is 91.40%, which is not close to the performance on MSA data, but this is understandable given the small size of the training data, and it is still comparable to the scores reported in the literature: 91.90% by, and 92.65% by.", "labels": [], "entities": [{"text": "MSA data", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.7898972034454346}]}, {"text": "This also illustrates the need to invest in acquiring more annotated data for dialects.", "labels": [], "entities": []}, {"text": "Now we want to evaluate if this improvement on the EG segmentation will cascade up the processing pipeline and help the MSA POS tagger adapt to the dialectal domain.", "labels": [], "entities": [{"text": "EG segmentation", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7381125092506409}, {"text": "MSA POS tagger", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.5358648300170898}]}, {"text": "We run our POS tagger on three different segmentation inputs: predictions of the MSA segmenter, predictions of the MSA+EG segmenter, and gold segmentation.", "labels": [], "entities": []}, {"text": "The reason we test on the gold segmentation is to seethe headroom for improvement if we have a 'perfect' segmenter.", "labels": [], "entities": []}, {"text": "shows that the loss with MSA POS tagging from adding the new dialectal data is fractional (0.26% absolute).", "labels": [], "entities": [{"text": "MSA POS tagging", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.5928811331590017}]}, {"text": "It also shows that using the MSA segmenter predictions as input, the POS tagger achieved only 66.70% f-1 measure on the EG test set.", "labels": [], "entities": [{"text": "MSA segmenter", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.7347695231437683}, {"text": "POS tagger", "start_pos": 69, "end_pos": 79, "type": "TASK", "confidence": 0.5689410418272018}, {"text": "f-1 measure", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9662006795406342}, {"text": "EG test set", "start_pos": 120, "end_pos": 131, "type": "DATASET", "confidence": 0.9321909745534261}]}, {"text": "This has risen to 74.07% when using the MSA+EG segmenter predictions, a remarkable increase of 7.37% absolute.", "labels": [], "entities": [{"text": "MSA+EG segmenter", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.6249697059392929}]}, {"text": "Improving the EG segmenter further can give a headroom up to 81.33%, which is another increase of 7.26% absolute.", "labels": [], "entities": [{"text": "EG segmenter", "start_pos": 14, "end_pos": 26, "type": "TASK", "confidence": 0.660212516784668}, {"text": "headroom", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9272707104682922}]}, {"text": "This is a significant improvement on the system performance that has been gained economically with few resources.", "labels": [], "entities": []}, {"text": "This confirms our original hypothesis that segmentation can help with dialectal domain adaptation.", "labels": [], "entities": [{"text": "dialectal domain adaptation", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.8511049747467041}]}, {"text": "One explanation of how the segmentation helps the POS tagging is that doing the right segmentation in EG data reduces the number of OOV tokens with respect to the POS tagging model, even when the POS tagger is trained with only MSA data.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 50, "end_pos": 61, "type": "TASK", "confidence": 0.7489404082298279}]}, {"text": "To verify that, we show , in, the percentage OOV tokens for the POS tagger model when the data is segmented using the segmenter trained with MSA only, the MSA+EG segmenter or using the gold segmentation.", "labels": [], "entities": [{"text": "OOV", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.972939133644104}, {"text": "POS tagger", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.6897725760936737}]}, {"text": "MSA+EG segmeter reduced the OOV by 5% points absolute which is 25% relative reduction in OOV.", "labels": [], "entities": [{"text": "OOV", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9984751343727112}, {"text": "OOV", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.9811646938323975}]}], "tableCaptions": [{"text": " Table 3: Egyptian Segmentation Evaluation", "labels": [], "entities": [{"text": "Egyptian Segmentation Evaluation", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7782479127248129}]}, {"text": " Table 4: Egyptian POS Evaluation", "labels": [], "entities": [{"text": "Egyptian POS Evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.6379963656266531}]}]}