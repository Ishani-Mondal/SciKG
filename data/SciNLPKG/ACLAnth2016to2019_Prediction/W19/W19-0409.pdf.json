{"title": [], "abstractContent": [{"text": "Inferences regarding Jane's arrival in London from predications such as Jane is going to London or Jane has gone to London depend on tense and aspect of the predications.", "labels": [], "entities": [{"text": "Jane's arrival in London", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.5310434937477112}]}, {"text": "Tense determines the temporal location of the predication in the past, present or future of the time of utterance.", "labels": [], "entities": []}, {"text": "The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of going to London is completed and whether its consequences hold at that time or not.", "labels": [], "entities": []}, {"text": "While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern NLP models capture these semantic concepts.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel entailment dataset and analyse the ability of a range of recently proposed NLP models to perform inference on temporal predications.", "labels": [], "entities": []}, {"text": "We show that the models encode a substantial amount of morphosyntactic information relating to tense and aspect, but fail to model inferences that require reasoning with these semantic properties.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tense and aspect are two of the main contributors to the semantics of a proposition, describing the temporal location of a predication and its internal constituency, thereby considerably influencing the entailment relations it licenses.", "labels": [], "entities": []}, {"text": "For example, while arrive in LOC |= be in LOC is generally considered a valid entailment rule, the case is complicated when different tenses and aspectual auxiliaries 1 of a given verb are considered as sentences (1) and (2) illustrate.", "labels": [], "entities": []}, {"text": "(1) Jane has arrived in London.", "labels": [], "entities": [{"text": "London", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.9426383972167969}]}, {"text": "|= Jane is in London now.", "labels": [], "entities": [{"text": "London", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.9471741318702698}]}, {"text": "(2) Jane will arrive in London.", "labels": [], "entities": []}, {"text": "|= Jane is in London now.", "labels": [], "entities": [{"text": "London", "start_pos": 14, "end_pos": 20, "type": "DATASET", "confidence": 0.9471741318702698}]}, {"text": "Understanding the difference between an event that has happened and whose consequences hold at the present moment, and an event that is currently happening or will happen in the future, is crucial for answering questions such as Where is Jane? or Is Jane in London now?", "labels": [], "entities": []}, {"text": "Inferring the consequences of events is important for understanding the relation between entities in the world.", "labels": [], "entities": []}, {"text": "For example, if we read that Lady Catherine has bought Longbourn estate, the inference that the acquisition is completed, and that the resulting consequence is that Lady Catherine now owns Longbourn estate, is paramount for keeping knowledge bases up-to-date.", "labels": [], "entities": [{"text": "Longbourn estate", "start_pos": 55, "end_pos": 71, "type": "DATASET", "confidence": 0.9907260239124298}, {"text": "Longbourn estate", "start_pos": 189, "end_pos": 205, "type": "DATASET", "confidence": 0.9931926131248474}]}, {"text": "In this paper we propose a novel entailment dataset that requires models to correctly determine the internal and external temporal structure of predications when performing natural language inference.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first dataset that is primarily focused on assessing natural language inference between temporally and aspectually modified predications.", "labels": [], "entities": [{"text": "natural language inference between temporally and aspectually modified predications", "start_pos": 95, "end_pos": 178, "type": "TASK", "confidence": 0.7394865221447415}]}, {"text": "As a first evaluation on our new dataset we compare to what extent five distributional embedding models,, Anchored Packed Trees (, fastText,, and BERT (, and two bi-directional LSTM (biLSTM) encoders, pre-trained on SNLI () and DNC (, respectively, are able to perform natural language inference on temporal predications.", "labels": [], "entities": [{"text": "BERT", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9979373216629028}]}, {"text": "In our evaluation, we refrain from fine-tuning any of the models as our goal is to assess to what extent tense and aspect are captured in these models per se.", "labels": [], "entities": []}, {"text": "As a pre-requisite diagnostic task for natural language inference between temporal predications we analysed whether the models encode the morphosyntax of tense and aspect and found that they capture a considerable amount of morphosyntactic information in their respective embedding spaces.", "labels": [], "entities": [{"text": "natural language inference between temporal predications", "start_pos": 39, "end_pos": 95, "type": "TASK", "confidence": 0.7661312719186147}]}, {"text": "However, neither of the models outperforms a majority class baseline on our proposed dataset due to their reliance on contextual similarity for performing inference, suggesting that models based on distributional semantics struggle with the more latent nature of tense and aspect.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are as follows: \u2022 We assess the extent to which the models in our evaluation encode information about the agreement between an inflected verb and its aspectual auxiliary, and whether a translation operation between different tenses can be learnt from the embedding spaces.", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel entailment dataset that requires models to perform inference with temporal predications, and evaluate the five embedding models and two pre-trained biLSTM encoders.", "labels": [], "entities": []}, {"text": "\u2022 We analyse the performance of the models and show that their reliance on contextual similarity is problematic for correctly modelling natural language inference governed by tense and aspect.", "labels": [], "entities": []}], "datasetContent": [{"text": "We created two experiments to assess the extent of morphosyntactic information relating to tense and aspect that is encoded in the respective embedding spaces.", "labels": [], "entities": []}, {"text": "Subsequently we propose a novel entailment dataset and evaluate the capability of the embedding models and the pre-trained biLSTMs to perform inference on temporal predications.", "labels": [], "entities": []}, {"text": "All our resources are available from https://github.com/ tttthomasssss/iwcs2019.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Auxiliary-verb agreeement results. Results are averaged accuracies with standard deviations in brackets.", "labels": [], "entities": []}, {"text": " Table 3: TEA results. All model results are significantly worse at the p < 0.01 level w.r.t. the majority class /  tense pair baseline, using a randomised bootstrap test", "labels": [], "entities": [{"text": "TEA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8099579215049744}]}, {"text": " Table 4: Similarity scores between the example predicates. DNC and SNLI refer to the two biLSTMs pre-trained  on DNC and SNLI, respectively.", "labels": [], "entities": []}, {"text": " Table 5: Detailed statistics of TEA.", "labels": [], "entities": [{"text": "TEA", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.5741679668426514}]}, {"text": " Table 6: Accuracies on the development and test sets for the pre-trained biLSTMs on SNLI and DNC, respectively.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9955289959907532}, {"text": "SNLI", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.9113330841064453}, {"text": "DNC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.7728247046470642}]}]}