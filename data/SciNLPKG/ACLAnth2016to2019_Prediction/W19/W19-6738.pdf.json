{"title": [{"text": "When less is more in Neural Quality Estimation of Machine Translation. An industry case study", "labels": [], "entities": [{"text": "Neural Quality Estimation of Machine Translation", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.663472960392634}]}], "abstractContent": [{"text": "Quality estimation (QE) of machine translation (MT), the task of predicting the quality of an MT output without human references, is particularly suitable in dynamic translation workflows, where translations need to be assessed continuously with no specific reference provided.", "labels": [], "entities": [{"text": "Quality estimation (QE) of machine translation (MT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8344807570630853}]}, {"text": "In this paper, we investigate sentence-level neural QE and its applicability in an industry use-case.", "labels": [], "entities": [{"text": "sentence-level neural QE", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.5826583405335745}]}, {"text": "We assess six QE approaches, which we divide into two-phase and one-phase approaches, based on quality and cost.", "labels": [], "entities": []}, {"text": "Our evaluation shows that while two-phase systems perform best in terms of the predicted QE scores, their computational costs suggest that alternatives should be considered for large-scale translation production.", "labels": [], "entities": [{"text": "translation production", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.8728999495506287}]}], "introductionContent": [{"text": "Quality estimation (QE) () is the process of predicting the quality of a machine translation (MT) system without human intervention or reference translations.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7557980298995972}, {"text": "machine translation (MT)", "start_pos": 73, "end_pos": 97, "type": "TASK", "confidence": 0.7745840609073639}]}, {"text": "QE can be applied at word-, sentence-, or document-level.", "labels": [], "entities": []}, {"text": "In the case of document-and sentence-level, the task is typically to predict a score that corresponds to a target evaluation criteria or metric (e.g., BLEU (), TER (), etc.), i.e. it is a regression task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.9989216327667236}, {"text": "TER", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9739373326301575}]}, {"text": "In this work, we investigate sentence-level QE, estimating TER scores.", "labels": [], "entities": [{"text": "TER scores", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9590317904949188}]}, {"text": "QE has been the focus of multiple WMT shared tasks.", "labels": [], "entities": [{"text": "QE", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5586202144622803}, {"text": "WMT shared tasks", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.9015467961629232}]}, {"text": "In such tasks the common evaluation criteria are metrics that score the quality of the estimates, such as Pearson's r or Root Mean Square Error (RMSE).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 106, "end_pos": 117, "type": "METRIC", "confidence": 0.9533391992251078}, {"text": "Root Mean Square Error (RMSE)", "start_pos": 121, "end_pos": 150, "type": "METRIC", "confidence": 0.9174383878707886}]}, {"text": "However, in a commercial setting, it is important to set a balance between performance and efficiency.", "labels": [], "entities": []}, {"text": "Furthermore, a QE solution for industry needs to be generalizable and as language-independent as possible.", "labels": [], "entities": [{"text": "QE", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9110835790634155}]}, {"text": "Feature-based methods have ranked highly in such tasks.", "labels": [], "entities": []}, {"text": "However, neural methods have recently not only outperformed feature-based ones, from a quality perspective, but they also provide a more generalizable and language-independent solution.", "labels": [], "entities": []}, {"text": "In our work, we first assess the predictive capabilities of neural QE (NQE) systems applied on MT data from the IT software domain, i.e. UI strings, for the English\u2192German and English\u2192Spanish language pairs.", "labels": [], "entities": []}, {"text": "We then focus on the efficiency aspect.", "labels": [], "entities": []}, {"text": "We further compare the performance of QE systems from a business perspective, i.e. using industryestablished metrics.", "labels": [], "entities": []}, {"text": "Our contribution is two-fold: the analysis and comparison of NQE approaches, and the implementation of anew efficient method that scores on a par with the others.", "labels": [], "entities": []}, {"text": "The use of QE in commercial setting has been discussed in previous work), but there are, to our knowledge, no published results of tests as extensive as ours of the application of QE to commercial data.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experimented with three different systems: deepQuest, QEBrain and SiameseQE.", "labels": [], "entities": [{"text": "QEBrain", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.8317347168922424}]}, {"text": "While the first two systems have been developed over an extensive period of time, have undergone significant empirical evaluations, and have achieved high rankings in WMT QE shared tasks, the last one is developed by our team for maximum efficiency.", "labels": [], "entities": [{"text": "WMT QE shared tasks", "start_pos": 167, "end_pos": 186, "type": "TASK", "confidence": 0.5983636304736137}]}], "tableCaptions": [{"text": " Table 1: Number of sentences in the QE data sets and number  of parallel sentences of extra data used to train the feature- extraction part of the two-phase systems.", "labels": [], "entities": [{"text": "QE data sets", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9548879663149515}, {"text": "feature- extraction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.6905293464660645}]}, {"text": " Table 2: Business evaluation scores of QE systems for EN- DE (best scores marked in bold).", "labels": [], "entities": [{"text": "EN- DE", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.6325262188911438}]}, {"text": " Table 3: Business evaluation scores of QE systems for EN- ES (best scores marked in bold).", "labels": [], "entities": [{"text": "EN- ES", "start_pos": 55, "end_pos": 61, "type": "TASK", "confidence": 0.5599690675735474}]}, {"text": " Table 4: Performance and rank scores for experiments on  EN-DE.", "labels": [], "entities": []}, {"text": " Table 5: Performance and rank scores for experiments on  EN-ES.", "labels": [], "entities": []}, {"text": " Table 6: Training time in minutes for phase 1, phase 2 and total, denoted as I, II and Tot. respectively. Training time for  single-phase systems is only marked as total for readability.", "labels": [], "entities": [{"text": "Training time", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9286902546882629}]}, {"text": " Table 7: Inference time (in seconds) for the validation and  the test sets. Number of sentence pairs for the validation set  for EN-DE and EN-ES: 7525, 5136 respectively; for the test  set for EN-DE and EN-ES: 32898, 34623 respectively.", "labels": [], "entities": [{"text": "Inference time", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9727781116962433}]}]}