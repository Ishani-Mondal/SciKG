{"title": [{"text": "On Learning Word Embeddings From Linguistically Augmented Text Corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "Word embedding learning is a technique in Natural Language Processing (NLP) to map words into vector space representations, is one of the most popular research directions in modern NLP by virtue of its potential to boost the performance of many NLP downstream tasks.", "labels": [], "entities": [{"text": "Word embedding learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7041788299878439}]}, {"text": "Nevertheless, most of the underlying word embedding methods such as word2vec and GloVe fail to produce high-quality representations if the text corpus is small and sparse.", "labels": [], "entities": []}, {"text": "This paper proposes a method to generate effective word embeddings from limited data.", "labels": [], "entities": []}, {"text": "Empirically, we show that the proposed model outperforms existing works for the classical word similarity task and fora domain-specific application.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.7960662444432577}]}], "introductionContent": [{"text": "Representing words as feature vectors is a vital task in NLP.", "labels": [], "entities": [{"text": "Representing words as feature vectors", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8689174175262451}]}, {"text": "The trivial approach of representing words as distinct symbols is insufficient since it ignores semantic and syntactic similarities between words.", "labels": [], "entities": []}, {"text": "As a result, distributional semantic models (DSMs) of word meanings have been emerged, which were built on the hypothesis of \"words with similar meanings tend to appear in similar contexts\".", "labels": [], "entities": []}, {"text": "Most of the earliest DSMs for word representation learning are mainly based on clustering or factorizing () global word co-occurrence matrix.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8591920137405396}]}, {"text": "However, with the introduction of neural word embedding methods by), many studies) empirically prove that neural word embedding methods regularly and substantially outperform traditional DSMs.", "labels": [], "entities": []}, {"text": "Thus, various neural models have been proposed recently for word representation learning.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.8925180037816366}]}, {"text": "Among the neural word embedding models, word2vec) is widely used in many NLP downstream tasks due to its efficiency in training and scalability.", "labels": [], "entities": []}, {"text": "Word2vec learns word representations by maximizing the likelihood of the local context of words (defined using a window around a word).", "labels": [], "entities": []}, {"text": "Following the light of word2vec, the variants of word2vec were introduced later with different context definitions.", "labels": [], "entities": []}, {"text": "() introduced a model in which the contexts are defined by first-order dependency links between words.", "labels": [], "entities": []}, {"text": "As extensions to the)'s work, () introduced second-order and higher-order dependencybased context for word embedding learning.", "labels": [], "entities": [{"text": "word embedding learning", "start_pos": 102, "end_pos": 125, "type": "TASK", "confidence": 0.6864538590113322}]}, {"text": "Nevertheless, none of the existing neural models is capable to capture different types of contexts at once in their models.", "labels": [], "entities": []}, {"text": "However, there are previous efforts to design such a model using non-neural approaches.", "labels": [], "entities": []}, {"text": "Although the neural word embedding models have been proven useful in many NLP applications, the existing models have a few limitations.", "labels": [], "entities": []}, {"text": "First, the existing works assume that the availability of large corpora, which may not be always available.", "labels": [], "entities": []}, {"text": "Especially, the resources are limited to learn domain-specific embedding inmost of the cases.", "labels": [], "entities": []}, {"text": "Second, even though there are domain adaptation techniques ( to overcome the scarcity of domain-specific resources in learning word embedding, it also requires a large amount of data from the source domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7616968750953674}]}, {"text": "Third, the existing works are only capable to capture one particular context, despite the fact that there are multiple ways to define context) using other linguistic relations (i.e., using dependency relations, using co-reference relations).: A sentence as a network, each word is anode in the network and edges are obtained from dependency links (solid links) and linear links (dashed links).", "labels": [], "entities": []}], "datasetContent": [{"text": "For the word similarity task, the embeddings were trained on a smaller section of English Wikipedia corpus (Al-Rfou et al., 2013) 1 , which contains 1,911,951 sentences, 52,468,613 tokens and 555,688 unique words.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7813796401023865}, {"text": "English Wikipedia corpus (Al-Rfou et al., 2013) 1", "start_pos": 82, "end_pos": 131, "type": "DATASET", "confidence": 0.8151839321309869}]}, {"text": "In addition, we trained malware-domain specific embeddings using a corpus extracted from APTnotes, a repository of publicly-available papers and blogs related to malicious campaigns, activity, and software 2 . We have chosen 193 reports from the year 2010 to 2015.", "labels": [], "entities": [{"text": "APTnotes", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.8355638384819031}]}, {"text": "Since APTnotes are in PDF format, PDFMiner tool) has been used to convert PDF files into plain text format.", "labels": [], "entities": []}, {"text": "After removing the non-sentences, headers, and footers; this malware-related text corpus consists of 27,553 sentences, 108,311 tokens, and 37,857 unique words.", "labels": [], "entities": []}, {"text": "Spacy 3 is used to obtain dependency labels for both datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on word similarity/relatedness.", "labels": [], "entities": [{"text": "word similarity/relatedness", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7271778210997581}]}, {"text": " Table 2: Results on malware-domain specific sentence classification.", "labels": [], "entities": [{"text": "malware-domain specific sentence classification", "start_pos": 21, "end_pos": 68, "type": "TASK", "confidence": 0.5611902847886086}]}]}