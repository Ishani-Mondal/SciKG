{"title": [], "abstractContent": [], "introductionContent": [], "datasetContent": [{"text": "We implement the compositional semantics introduced in the previous section using ccg2lambda.", "labels": [], "entities": []}, {"text": "Since there is a non-trivial gap between the outputs of the state-of-the-art CCG parsers (e.g.,) and the syntactic structures we assume in this study, we manually annotate CCG trees for the input sentences.", "labels": [], "entities": []}, {"text": "We implement the compositional mapping from CCG trees to the semantic representations discussed so far, using the lexical entries presented in the previous section.", "labels": [], "entities": []}, {"text": "The system automatically converts an input CCG tree to the corresponding semantic representation in DTS.", "labels": [], "entities": []}, {"text": "Implementation of a proof system is also needed for checking the relationships between answers and questions formulated as entailment relation.", "labels": [], "entities": []}, {"text": "We use Ltac), a tactic language available in Coq to implement proof automation necessary for our purposes.", "labels": [], "entities": [{"text": "Coq", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9338885545730591}]}, {"text": "We built a testset consisting of 49 question-answer pairs.", "labels": [], "entities": []}, {"text": "Some examples are shown in.", "labels": [], "entities": []}, {"text": "Each problem in the testset has two sentences; the first sentence is an answer and the second sentence is a question.", "labels": [], "entities": []}, {"text": "For each pair of the testset, we annotated a gold CCG tree.", "labels": [], "entities": [{"text": "CCG tree", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.7714637219905853}]}, {"text": "Here is an example for an answer sentence Everyone runs and a question sentence Who ran ? (mention-some reading): The answer to each problem is yes (entailment), no (contradiction), or unknown (neither), following the FraCaS testset.", "labels": [], "entities": [{"text": "FraCaS testset", "start_pos": 218, "end_pos": 232, "type": "DATASET", "confidence": 0.9676679372787476}]}, {"text": "The distribution of answers is: yes/no/unknown = 36/1/12 For evaluating the basic capacity of compositional semantics and proof system, we keep the sentences included in the testset as simple as possible.", "labels": [], "entities": []}, {"text": "This makes easy to detect what phenomena a given system fails to give an appropriate semantic representation.", "labels": [], "entities": []}, {"text": "In the setting for theorem proving in Coq, we use a finite domain with three entities: John, Susan and Lucy.", "labels": [], "entities": [{"text": "theorem proving", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.8388010263442993}]}, {"text": "This can be implemented using enumeration type in Coq: (24) Inductive Entity : Type := | John | Susan | Lucy.", "labels": [], "entities": []}, {"text": "The distinction between mention-some and mentione-all readings is annotated to wh-expressions; see entries for who in The Coq script automatically generated for the CCG trees for the question-answer pair in (23) is the following: (25) (forall x:Entity,(_run x)) -> (ex Entity (fun x => (_run x))) Here the semantic representation for the answer sentence appears in the antecedent of implication \"->\" and that for the question sentence appears in the consequent.", "labels": [], "entities": []}, {"text": "We use existential type in Coq, written ex, for representing mention-some wh-question.", "labels": [], "entities": []}, {"text": "The system correctly proves (25) as a theorem.", "labels": [], "entities": []}, {"text": "In this setting, we can successfully derive the desired semantic representations and prove the entailment relations for all 49 cases.", "labels": [], "entities": []}], "tableCaptions": []}