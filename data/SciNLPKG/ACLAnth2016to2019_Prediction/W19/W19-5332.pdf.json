{"title": [], "abstractContent": [{"text": "In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English-Gujarati language pair within the translation task sub-track.", "labels": [], "entities": [{"text": "WMT 2019 news translation shared task", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.7419334103663763}]}, {"text": "Our baseline and primary submissions are built using a Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism followed by fine-tuning using in-domain data.", "labels": [], "entities": [{"text": "Recurrent neural network (RNN) based neural machine translation (NMT)", "start_pos": 55, "end_pos": 124, "type": "TASK", "confidence": 0.6147296703778781}]}, {"text": "Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, building a high quality NMT system for this language pair is a difficult task.", "labels": [], "entities": []}, {"text": "We produced synthetic data through back-translation from available monolingual data.", "labels": [], "entities": []}, {"text": "We report the automatic evaluation scores of our English-Gujarati and Gujarati-English NMT systems trained at word, byte-pair and character encoding levels where RNN at word level is considered as the baseline and used for comparison purpose.", "labels": [], "entities": []}, {"text": "Our English-Gujarati system ranked in the second position in the shared task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural Machine translation (NMT) is an approach to machine translation (MT) that uses artificial neural network to directly model the conditional probability p(y|x) of translating a source sentence (x 1 ,x 2 ,...,x n ) into a target sentence (y 1 ,y 2 ,...,y m ).", "labels": [], "entities": [{"text": "Neural Machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7900330871343613}, {"text": "machine translation (MT)", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.8704617261886597}]}, {"text": "NMT has consistently performed better than the phrase-based statistical MT (PB-SMT) approaches and has provided state-ofthe-art results in the last few years.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7954626679420471}, {"text": "phrase-based statistical MT", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.5421389043331146}]}, {"text": "However, one of the major constraints of using supervised NMT is that it is not suitable for low resource language pairs.", "labels": [], "entities": []}, {"text": "Thus, to use supervised NMT, low resource pairs need to resort to other techniques * These three authors have contributed equally.", "labels": [], "entities": []}, {"text": "to increase the size of the parallel training dataset.", "labels": [], "entities": []}, {"text": "In the WMT 2019 news translation shared task, one such resource scarce language pair is EnglishGujarati.", "labels": [], "entities": [{"text": "WMT 2019 news translation shared task", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.7830694516499838}]}, {"text": "Due to insufficient volume of parallel corpora available to train an NMT system for these language pairs, creation of more actual/synthetic parallel data for low resources languages such as Gujarati, is an important issue.", "labels": [], "entities": []}, {"text": "In this paper, we described our joint participation of Jadavpur University and Saarland University in the WMT 2019 news translation task for English-Gujarati and Gujarati-English.", "labels": [], "entities": [{"text": "WMT 2019 news translation task", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.7765966773033142}]}, {"text": "The released training data set is completely different in-domain compared to the development set and the size is not anywhere close to the sizable amount of training data which is typically required for the success of NMT systems.", "labels": [], "entities": []}, {"text": "We use additional synthetic data produced through backtranslation from the monolingual corpus.", "labels": [], "entities": []}, {"text": "This provides significant improvements in translation performance for both our English-Gujarati and Gujarati-English NMT systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.9539310336112976}]}, {"text": "Our EnglishGujarati system was ranked second in terms of BLEU () and TER) in the shared task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9993171691894531}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9952898025512695}]}], "datasetContent": [{"text": "We have explained our experimental setups in the next two sections.", "labels": [], "entities": []}, {"text": "The first section contains the setup used for our final submission and the next section describes all the other supporting experimental setups.", "labels": [], "entities": []}, {"text": "We use the OpenNMT toolkit ( for our experiments. We performed several experiments where the parallel corpus is sent to the model as space separated character format, space separated word format, and space separated Byte Pair Encoding (BPE) format).", "labels": [], "entities": []}, {"text": "For our final (i.e., primary) submission for the English-Gujarati task, the source input words were converted to BPE whereas the Gujarati words were kept as it is.", "labels": [], "entities": [{"text": "BPE", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.9323819875717163}]}, {"text": "For our Gujarati-English submission, both the source and the target were in simple word level format.", "labels": [], "entities": []}, {"text": "In this section we describe all the supporting experiments that we performed for this shared task starting from Statistical MT to NMT with both supervised and unsupervised settings.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 112, "end_pos": 126, "type": "TASK", "confidence": 0.6145040988922119}]}, {"text": "All the results and experiments discussed below are tested on the released development set (considering this as the test set).", "labels": [], "entities": []}, {"text": "These models were not tested with the released test set as they provided poor BLEU scores on the development set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9994939565658569}]}, {"text": "We used uni-directional RNN having LSTM units trained on 64,346 pre-processed sentences (cf. Section 3) with 120K training steps and learning rate of 1.0.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 133, "end_pos": 146, "type": "METRIC", "confidence": 0.9417366981506348}]}, {"text": "For English-Gujarati where input was space separated words for both sides, we achieved highest BLEU score of 4.15 after fine-tuning with 10K sentences selected from the cleaned parallel corpus whose total number of tokens(words) was exceeding 8.The BLEU score dropped to 3.56 while applying BPE on the both sides.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9871933162212372}, {"text": "BLEU score", "start_pos": 249, "end_pos": 259, "type": "METRIC", "confidence": 0.9808675646781921}, {"text": "BPE", "start_pos": 291, "end_pos": 294, "type": "METRIC", "confidence": 0.9969521760940552}]}, {"text": "For the other direction (Gujarati-English) of the language pair, we got highest BLEU scores of 5.13 and 5.09 at word level and BPE level respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9996260404586792}, {"text": "BPE level", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9747428894042969}]}, {"text": "We also tried transformer-based NMT model () which however gave extremely poor results on similar experimental settings.", "labels": [], "entities": []}, {"text": "The highest BLEU we achieved was 0.74 for Gujarati-English and 0.96 for EnglishGujarati.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9994938373565674}]}, {"text": "The transformer model was trained until 100K training steps, with 64 batch size in a single GPU and positional encoding layers size was set to 2.", "labels": [], "entities": []}, {"text": "Since the the training data size was not enough, we used backtranslation to generate additional synthetic sentence pairs from the monolingual corpus released in WMT 2019.", "labels": [], "entities": [{"text": "monolingual corpus released in WMT 2019", "start_pos": 130, "end_pos": 169, "type": "DATASET", "confidence": 0.6462998588879904}]}, {"text": "We initially used monoses (, which is based on unsupervised statistical phrase based machine translation, to translate the monolingual sentences from English to Gujarati.", "labels": [], "entities": []}, {"text": "We used 2M English sentences to train the monoses system.", "labels": [], "entities": []}, {"text": "The training process took around 6 days in our modest 64 GB server.", "labels": [], "entities": []}, {"text": "However, the results were extremely poor with a BLEU score of 0.24 for English-Gujarati and 0.01 for the opposite direction, without using preprocessed parallel corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9994446635246277}]}, {"text": "Moreover, after adding preprocessed parallel corpus, the BLEU score dropped significantly.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.9824662208557129}]}, {"text": "This motivated us to use online document translator, in our case Google translation API, for backtranslating sentence pairs from the released monolingual dataset.", "labels": [], "entities": []}, {"text": "The back-translated data was later combined with our preprocessed parallel corpus for our final model.", "labels": [], "entities": []}, {"text": "Additionally, we also tried a simple unidirectional RNN model on character level, however, this also fails to contribute in terms of improving performance.", "labels": [], "entities": []}, {"text": "We have compiled all the results in table 4.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Hyper-parameter configurations for Gujarati- English translation using unidirectional RNN (Cho  et al., 2014)), *learning-rate was initially set to 1.0.", "labels": [], "entities": [{"text": "Gujarati- English translation", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7070856988430023}]}, {"text": " Table 3: Hyper-parameter configurations for English- Gujarati translation using bi-directional RNN (Cheng  et al., 2016).", "labels": [], "entities": [{"text": "English- Gujarati translation", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.564395859837532}]}, {"text": " Table 4: Results of supporting experiments.", "labels": [], "entities": []}, {"text": " Table 6: WMT 2019 evaluation for EN-GU and GU-EN on development set released by WMT 2019.", "labels": [], "entities": [{"text": "WMT", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7417178153991699}, {"text": "WMT", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.7737642526626587}]}]}