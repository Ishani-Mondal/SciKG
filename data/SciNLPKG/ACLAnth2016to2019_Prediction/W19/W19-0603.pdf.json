{"title": [{"text": "The Lexical Gap: An Improved Measure of Automated Image Description Quality", "labels": [], "entities": [{"text": "Automated Image Description", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.6388937731583914}]}], "abstractContent": [{"text": "The challenge of automatically describing images and videos has stimulated much research in Computer Vision and Natural Language Processing.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7467159926891327}]}, {"text": "In order to test the semantic abilities of new algorithms, we need reliable and objective ways of measuring progress.", "labels": [], "entities": []}, {"text": "Using our dataset of 2K human and machine descriptions, we find that standard evaluation measures alone do not adequately measure the semantic richness of a description.", "labels": [], "entities": []}, {"text": "We introduce and test anew measure of semantic ability based on relative lexical diversity.", "labels": [], "entities": []}, {"text": "We show how our measure can work alongside existing measures to achieve state of the art correlation with human judgement of quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "Image and video processing systems are being developed fora wide variety of semantically rich tasks, such storytelling (, Visual Question Answering (VQA) (, and engaging in visual dialogue (.", "labels": [], "entities": [{"text": "Visual Question Answering (VQA)", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.7294424871603647}]}, {"text": "In this paper, we consider the task of Image Description (.", "labels": [], "entities": [{"text": "Image Description", "start_pos": 39, "end_pos": 56, "type": "TASK", "confidence": 0.8234640061855316}]}, {"text": "Closing the semantic gap between human and machine descriptions requires robust and standardised measures of performance.", "labels": [], "entities": []}, {"text": "In classical computer vision problems such as object detection, segmentation and classification, quality can be defined easily as a comparison between machine predictions and reference answers.", "labels": [], "entities": [{"text": "object detection", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.7954430282115936}, {"text": "segmentation and classification", "start_pos": 64, "end_pos": 95, "type": "TASK", "confidence": 0.6582848429679871}]}, {"text": "Standard measures of image description quality consider the alignment of candidate sentences with ground truth sentences.", "labels": [], "entities": [{"text": "image description quality", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7154571910699209}]}, {"text": "However defining a set of \"correct\" answers fora given image is restrictive, as an image may contain diverse semantic information.", "labels": [], "entities": []}, {"text": "Consequently we find semantically rich and detailed content is regarded very poorly by such measures, and the more sparse and simplistic the reference data and predictions, the higher the score.", "labels": [], "entities": []}, {"text": "We sourced 2K human and machine descriptions, which we used to show that standard automated measures of quality give an incomplete picture of semantic ability.", "labels": [], "entities": []}, {"text": "The measures produce higher scores when candidates and reference data are semantically sparse, and lower scores on richer descriptions.", "labels": [], "entities": []}, {"text": "2. We show that measuring the relative lexical diversity of a system is a better indicator of semantic ability.", "labels": [], "entities": []}, {"text": "We define two measures of relative diversity, and show that when combined with standard measures, achieve achieve state-of-the-art correlation with human judgement.", "labels": [], "entities": []}, {"text": "We hope our work will stimulate research in to more advanced measures of semantic ability, helping to close the gap between human and machine descriptions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Objective measures of performance enable the automatic evaluation of systems across large datasets, avoiding the laborious process of sourcing human judgements.", "labels": [], "entities": [{"text": "sourcing human judgements", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.8842801650365194}]}, {"text": "The measures divide into three groups: 1.", "labels": [], "entities": []}, {"text": "Machine Translation measures: Early description systems considered image description as a translation task, in which information in the visual domain, is translated to the linguistic domain.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8254395127296448}, {"text": "image description", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7351612448692322}]}, {"text": "As such machine translation measures based on n-gram alignment such as BLEU(), ROUGE(Lin and Hovy, 2003) and METEOR).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.6775956004858017}, {"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9989535808563232}, {"text": "ROUGE", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9976500868797302}, {"text": "METEOR", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9619345664978027}]}, {"text": "2. Captioning Measures: CIDEr(Vedantam et al., 2015) and SPICE(, designed specifically for the description task.", "labels": [], "entities": []}, {"text": "CIDEr addresses the problem of description diversity by rewarding candidates that match the consensus of references.", "labels": [], "entities": [{"text": "description diversity", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8239503502845764}]}, {"text": "SPICE, applies work from scene graph generation() to create semantic graph representations of candidate and ground truth.", "labels": [], "entities": [{"text": "SPICE", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6780290007591248}, {"text": "scene graph generation", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.687643826007843}]}, {"text": "3. Neural Network Evaluation: Neural networks can be trained to evaluate descriptions.", "labels": [], "entities": [{"text": "Neural Network Evaluation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.73246830701828}]}, {"text": "NNEVAL () is a network trained to predict whether a description is human or machine, using both the captioning and translation measures as linguistic features.", "labels": [], "entities": [{"text": "NNEVAL", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8153042197227478}]}, {"text": "As automated measures area substitute for human evaluation, they are compared on the basis of their ability to correlate with human judgement.", "labels": [], "entities": []}, {"text": "The poor correlation of translation measures is well known, (, and captioning measures show improved results.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9676923751831055}]}, {"text": "In this work we assess the correlation using the Composite dataset().", "labels": [], "entities": [{"text": "Composite dataset", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.8775009214878082}]}, {"text": "Human and machine captions for images in subsets of MS-COCO, Flickr8K and Flickr30K are judged by Amazon Mechanical Turk workers, and rated for correctness and completeness.", "labels": [], "entities": [{"text": "Human and machine captions", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6003995537757874}, {"text": "MS-COCO", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9632693529129028}, {"text": "Flickr8K", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.7647354006767273}, {"text": "Flickr30K", "start_pos": 74, "end_pos": 83, "type": "DATASET", "confidence": 0.7727847099304199}, {"text": "correctness", "start_pos": 144, "end_pos": 155, "type": "METRIC", "confidence": 0.9469389319419861}]}, {"text": "A desirable quality of a description is to convey semantically insightful information.", "labels": [], "entities": []}, {"text": "In this section we describe how we sourced a set of human and machine descriptions, comparing them on their semantic richness.", "labels": [], "entities": []}, {"text": "We compared standard evaluation measures on semantically sparse and rich captions.", "labels": [], "entities": []}, {"text": "We evaluated human and machine descriptions separately, using the standard evaluation measures.", "labels": [], "entities": []}, {"text": "For each image we performed 1000 evaluations, where 5 sentences were randomly selected from the set of descriptions to be the ground truth candidates, with the remaining used to calculate the metrics.", "labels": [], "entities": []}, {"text": "shows that when both ground truth and candidate description sentences are semantically sparse they perform very well.", "labels": [], "entities": []}, {"text": "However descriptions of a higher semantic complexity are penalised as a result of their more diverse and rich descriptions, with many insightful descriptions scoring zero.", "labels": [], "entities": []}, {"text": "shows examples where the SPICE metric scores rich descriptions as zero.", "labels": [], "entities": []}, {"text": "When rich descriptions were used as ground truth, the machine descriptions perform very poorly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation Measures for Rich (Human) and Sparse(Machine) Domains", "labels": [], "entities": [{"text": "Evaluation Measures", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7827605307102203}]}, {"text": " Table 2: Rich-Sparse Dataset Statistics", "labels": [], "entities": [{"text": "Rich-Sparse Dataset", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.7970143258571625}]}, {"text": " Table 3: Calculation of l d and L g for the Composite Dataset", "labels": [], "entities": [{"text": "Composite Dataset", "start_pos": 45, "end_pos": 62, "type": "DATASET", "confidence": 0.7038422524929047}]}, {"text": " Table 4: Overall Correlations for LDR and Lexical Gap", "labels": [], "entities": []}, {"text": " Table 4. Then we calculated the m gap and m ldr for each evaluation measure over the entire Composite  dataset. We calculate the correlation performance with the human evaluation scores.", "labels": [], "entities": [{"text": "Composite  dataset", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.9287732839584351}]}, {"text": " Table 5: Overall Correlations for LDR and Lexical Gap. All p-values<0.001", "labels": [], "entities": []}]}