{"title": [{"text": "Generalizing Back-Translation in Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.659693032503128}]}], "abstractContent": [{"text": "Back-translation-data augmentation by translating target monolingual data-is a crucial component in modern neural machine translation (NMT).", "labels": [], "entities": [{"text": "Back-translation-data augmentation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6670225858688354}, {"text": "neural machine translation (NMT)", "start_pos": 107, "end_pos": 139, "type": "TASK", "confidence": 0.8472379247347513}]}, {"text": "In this work, we refor-mulate back-translation in the scope of cross-entropy optimization of an NMT model, clarifying its underlying mathematical assumptions and approximations beyond its heuristic usage.", "labels": [], "entities": [{"text": "cross-entropy optimization", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6870684325695038}]}, {"text": "Our formulation covers broader synthetic data generation schemes, including sampling from a target-to-source NMT model.", "labels": [], "entities": []}, {"text": "With this formulation , we point out fundamental problems of the sampling-based approaches and propose to remedy them by (i) disabling label smoothing for the target-to-source model and (ii) sampling from a restricted search space.", "labels": [], "entities": []}, {"text": "Our statements are investigated on the WMT 2018 Ger-man \u2194 English news translation task.", "labels": [], "entities": [{"text": "WMT 2018 Ger-man \u2194 English news translation task", "start_pos": 39, "end_pos": 87, "type": "TASK", "confidence": 0.8185329511761665}]}], "introductionContent": [{"text": "Neural machine translation (NMT) () systems make use of back-translation () to leverage monolingual data during the training.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7621596058209738}]}, {"text": "Here an inverse, target-to-source, translation model generates synthetic source sentences, by translating a target monolingual corpus, which are then jointly used as bilingual data.", "labels": [], "entities": []}, {"text": "Sampling-based synthetic data generation schemes were recently shown to outperform beam search (.", "labels": [], "entities": [{"text": "Sampling-based synthetic data generation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.6706618815660477}]}, {"text": "However, the generated corpora are reported to stray away from the distribution of natural data (.", "labels": [], "entities": []}, {"text": "In this work, we focus on investigating why sampling creates better training data by re-writing the loss criterion of an NMT model to include a model-based data generator.", "labels": [], "entities": []}, {"text": "\u2020 Now at DeepL GmbH.", "labels": [], "entities": [{"text": "DeepL GmbH", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.9283770024776459}]}, {"text": "By doing so, we obtain a deeper understanding of synthetic data generation methods, identifying their desirable properties and clarifying the practical approximations.", "labels": [], "entities": []}, {"text": "In addition, current state-of-the-art NMT models suffer from probability smearing issues  and are trained using label smoothing ().", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.6835438162088394}]}, {"text": "These result in low-quality sampled sentences, which influence the synthetic corpora.", "labels": [], "entities": []}, {"text": "We investigate considering only highquality hypotheses by restricting the search space of the model via (i) ignoring words under a probability threshold during sampling and (ii) N -best list sampling.", "labels": [], "entities": []}, {"text": "We validate our claims in experiments on a controlled scenario derived from the WMT 2018 German \u2194 English translation task, which allows us to directly compare the properties of synthetic and natural corpora.", "labels": [], "entities": [{"text": "WMT 2018 German \u2194 English translation task", "start_pos": 80, "end_pos": 122, "type": "TASK", "confidence": 0.791479800428663}]}, {"text": "Further, we present the proposed sampling techniques on the original WMT German \u2194 English task.", "labels": [], "entities": [{"text": "WMT German \u2194 English task", "start_pos": 69, "end_pos": 94, "type": "DATASET", "confidence": 0.8099122285842896}]}, {"text": "The experiments show that our restricted sampling techniques work comparable or superior to other generation methods by imitating human-generated data better.", "labels": [], "entities": []}, {"text": "In terms of translation quality, these do not result in consistent improvements over the typical beam search strategy.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9587046504020691}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BLEU [%] results for the controlled scenario.   *  denotes a p-value of < 0.01 w.r.t. the reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993014335632324}]}, {"text": " Table 2: IBM-1 model entropy and perplexity (PPL)  on the training and development set for the controlled  scenario using different synthetic generation methods.", "labels": [], "entities": [{"text": "IBM-1 model entropy and perplexity (PPL)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.5716704688966274}]}, {"text": " Table 3: WMT 2018 German \u2194 English BLEU [%] val- ues comparing different synthetic data generation  methods.   *  denotes a p-value of < 0.01 w.r.t. beam search.", "labels": [], "entities": [{"text": "WMT 2018 German", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8303014834721884}, {"text": "BLEU [%] val- ues", "start_pos": 36, "end_pos": 53, "type": "METRIC", "confidence": 0.845970618724823}]}]}