{"title": [{"text": "HDT-UD: Avery large Universal Dependencies treebank for German", "labels": [], "entities": [{"text": "HDT-UD", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.754604160785675}, {"text": "Universal Dependencies treebank", "start_pos": 20, "end_pos": 51, "type": "DATASET", "confidence": 0.7288821339607239}]}], "abstractContent": [{"text": "We report on the conversion of the Hamburg Dependency Treebank (Foth et al., 2014) to Universal Dependencies.", "labels": [], "entities": [{"text": "Hamburg Dependency Treebank (Foth et al., 2014)", "start_pos": 35, "end_pos": 82, "type": "DATASET", "confidence": 0.9229787826538086}]}, {"text": "The HDT consists of more than 200.000 sentences annotated with dependency structure, making every attempt at manual conversion or manual post-processing extremely costly.", "labels": [], "entities": []}, {"text": "The conversion employs an unranked tree transducer.", "labels": [], "entities": []}, {"text": "This formalism allows to express transformation rules in a concise way, guarantees the well-formedness of the output and is predictable to the rule writers.", "labels": [], "entities": []}, {"text": "Together with the release of a converted subset of the HDT spanning 3 million tokens, we release an interactive workbench for writing and refining tree transducer rules.", "labels": [], "entities": []}, {"text": "Our conversion achieves a very high labeled accuracy with respect to a manually converted gold standard of 97.3%.", "labels": [], "entities": [{"text": "labeled", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9419559836387634}, {"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.8347586393356323}]}, {"text": "Up to now, the conversion effort took about 1000 hours of work.", "labels": [], "entities": [{"text": "conversion", "start_pos": 15, "end_pos": 25, "type": "TASK", "confidence": 0.9792319536209106}]}], "introductionContent": [{"text": "Despite the availability of several German treebanks (TIGER (), T\u00fcBa-D/Z (), HDT () and a fairly active research community, the only other larger German treebank which has been converted to Universal Dependencies is T\u00fcBa-D/Z (, consisting of 95.595 sentences (1.788k tokens).", "labels": [], "entities": [{"text": "TIGER", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.7923639416694641}]}, {"text": "Until now, the largest German treebank distributed by the UD project was German GSD, consisting of 15.590 sentences (292k tokens).", "labels": [], "entities": [{"text": "German GSD", "start_pos": 73, "end_pos": 83, "type": "DATASET", "confidence": 0.838329553604126}]}, {"text": "As that treebank's original annotation still stems from the pre-UD time, interesting syntactic constructs are often not annotated in accordance to the UDv2 guidelines . Furthermore, the German UD guidelines themselves were often not up to date, sometimes even incomplete.", "labels": [], "entities": []}, {"text": "Our work on converting the HDT to Universal Dependencies therefore also consisted in a large part of working out the best way to represent German dependency structures in the UD annotation schema; these decisions are encoded in the resulting treebank and -where applicable -were documented to be added to the general UD documentation.", "labels": [], "entities": [{"text": "UD documentation", "start_pos": 317, "end_pos": 333, "type": "DATASET", "confidence": 0.8685259521007538}]}, {"text": "The Hamburg Dependency Treebank is a native dependency treebank developed mainly for research in parsing; the annotation schema) was developed as part of the annotation effort.", "labels": [], "entities": [{"text": "Hamburg Dependency Treebank", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.9593183994293213}, {"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9639792442321777}]}, {"text": "The texts in the treebank stem from heise.de, a well-known German technical website reporting about new software and hardware, technology-related politics, earnings of tech companies, inter alia.", "labels": [], "entities": []}, {"text": "Some texts are short and formulaic, others are long editorials.", "labels": [], "entities": []}, {"text": "The HDT has an average sentence length of 18.4 and more than 200.000 sentences are manually annotated.", "labels": [], "entities": [{"text": "HDT", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.5970008969306946}]}, {"text": "The text of the HDT can be distributed for academic use, the annotations are licensed under a Creative Commons share-alike license.", "labels": [], "entities": []}, {"text": "The original treebank and its conversion to Universal Dependencies are available under https://nats.gitlab.io/hdt/.", "labels": [], "entities": []}, {"text": "We will give a rough overview of other treebanks which were converted to Universal Dependencies and of the methods which were employed, explain why our approach is different and how it works.", "labels": [], "entities": []}, {"text": "After detailing our conversion process, we discuss general issues faced when converting a treebank to a different schema as well as specific problematic structures in our case and how we dealt with them, explain how we converted morphological features and finally evaluate the results of the conversion process.", "labels": [], "entities": []}], "datasetContent": [{"text": "We manually annotated a set of 50 sentences held out from the rule generation process to evaluate the quality of the converter.", "labels": [], "entities": [{"text": "rule generation", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7338665723800659}]}, {"text": "The sentences were chosen by randomly sampling the part B of the HDT and also used as validation in.", "labels": [], "entities": [{"text": "HDT", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9383121132850647}]}, {"text": "Our manual annotations differ slightly because of changes to the current UD standards.", "labels": [], "entities": [{"text": "UD standards", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8416943550109863}]}, {"text": "Punctuation marks (84 out of the 782 tokens) were ignored during this comparison.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9767712354660034}]}, {"text": "Looking at the 698 remaining converted words, 558 matched the hand-annotated dependency relations, leaving 127 words not matching the gold standard.", "labels": [], "entities": []}, {"text": "Further analysis showed that the dependency label was actually correct in 88 of these cases but the transducer gave the dependency relation an additional (correct) subtype which was not given in the set of hand-annotated sentences.", "labels": [], "entities": []}, {"text": "Some of the hand-annotated relations turned out to be wrong when comparing them to the result from the transducer.", "labels": [], "entities": []}, {"text": "In the end, neglecting the punctuation marks, for 679 out of 698 words the automatically converted annotations matched the corrected hand-annotations, yielding a labeled accuracy of 97.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7955402731895447}]}, {"text": "We further evaluated the conversion as performed by: by critically looking through 100 randomly selected sentences and checking for annotation and conversion errors.", "labels": [], "entities": [{"text": "conversion", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9731964468955994}]}, {"text": "The resulting accuracy confirms the previous evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999626874923706}]}, {"text": "Overall, 71% of the evaluated sentences where converted without any errors and 1506 of the 1548 non-punctuation dependencies where converted correctly, again yielding an accuracy of 97.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "This accuracy is significantly higher than other reported conversion accuracies; Seddah et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9994363188743591}]}, {"text": "(2018) e. g. report a labeled conversion accuracy of 94.75% and 93.27% on their held-out sets, which is twice the amount of labeled errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.5809608101844788}]}], "tableCaptions": []}