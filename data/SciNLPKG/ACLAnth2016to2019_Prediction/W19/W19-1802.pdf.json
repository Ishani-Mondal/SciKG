{"title": [{"text": "Referring to Objects in Videos using Spatio-Temporal Identifying Descriptions", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents anew task, the grounding of spatio-temporal identifying descriptions in videos.", "labels": [], "entities": []}, {"text": "Previous work suggests potential bias in existing datasets and emphasizes the need fora new data creation schema to better model linguistic structure.", "labels": [], "entities": []}, {"text": "We introduce anew data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos.", "labels": [], "entities": [{"text": "grounding spatio-temporal identifying descriptions in videos", "start_pos": 141, "end_pos": 201, "type": "TASK", "confidence": 0.7982264856497446}]}, {"text": "We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion.", "labels": [], "entities": []}, {"text": "We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules.", "labels": [], "entities": []}, {"text": "Finally , we propose a future challenge and a need fora robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Localizing referring expressions in videos involves both static and dynamic information.", "labels": [], "entities": []}, {"text": "A referring expression) is a linguistic expression that grounds its meaning to a specific referent object in the world.", "labels": [], "entities": []}, {"text": "The input video can be very long, have unknown length, contain many objects from the same class, or contain similar actions and interactions throughout the video.", "labels": [], "entities": []}, {"text": "A successful, grounded communication between a speaker and a listener must ensure that the sentence or discourse provides enough information such that the listener can eliminate all distractors and focus only on the referent object that acts in a specific time interval.", "labels": [], "entities": []}, {"text": "That essential information varies from the diversity of events in the world.", "labels": [], "entities": []}, {"text": "However, a speaker is: The first spatio-temporal identifying description in the green box grounds to the event that a panda goes down the slide.", "labels": [], "entities": []}, {"text": "Another panda can be a context because they are interacting in the same scene.", "labels": [], "entities": []}, {"text": "The second identifying description in the blue box grounds to the event that another panda climbs up the slide.", "labels": [], "entities": []}, {"text": "likely to mention salient properties and also salient differences based on the referent in comparison to other distractors.", "labels": [], "entities": []}, {"text": "The differences can be about object category, attributes, poses, actions, changes in location, relationships and contexts in the scene.", "labels": [], "entities": []}, {"text": "Existing image referring expression datasets () do not contain referring expressions that refer to dynamic properties or movements of the referent.", "labels": [], "entities": []}, {"text": "These datasets do not require temporal understanding that would require a system to learn that \"moving to the right\" is different from \"moving to the left\" and \"getting up\" is different from \"lying down\".", "labels": [], "entities": []}, {"text": "Existing video referring expression datasets and approaches focus only on temporal localization but referent object localization.", "labels": [], "entities": []}, {"text": "In other words, they do not ground events in both space and time.", "labels": [], "entities": []}, {"text": "Emphasized by, the data collection process for referring expressions should incorporate linguistic structure such that the model can learn more than shallow correlations between pairs of a sentence and visual features.", "labels": [], "entities": []}, {"text": "That is, a particular dataset should not have a shortcut that only detecting nouns (object class) can perform well.", "labels": [], "entities": []}, {"text": "Our dataset mitigates this issue by forcing instancelevel recognition.", "labels": [], "entities": [{"text": "instancelevel recognition", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6876064091920853}]}, {"text": "We create a requirement that grounding the referring expressions must identify the target object among many distractors from the contrast set (same class distractors).", "labels": [], "entities": []}, {"text": "The contributions of this paper are (i) We propose a novel vision and language data collection scheme based on grammatical constraints for surface realization to ground video referring expressions in both space and time with lexical correlations between vision and language.", "labels": [], "entities": []}, {"text": "We collected the Spatio-Temporal Video Identifying Description Localization (STV-IDL) dataset consisting of 199 video sequences from Youtube and 7,569 identifying descriptions.", "labels": [], "entities": [{"text": "Spatio-Temporal Video Identifying Description Localization (STV-IDL)", "start_pos": 17, "end_pos": 85, "type": "TASK", "confidence": 0.7130148448050022}, {"text": "Youtube", "start_pos": 133, "end_pos": 140, "type": "DATASET", "confidence": 0.9590526223182678}]}, {"text": "(ii) We propose an interpretable system based on two-stream modular attention network that models both appearance and motion to ground referring expressions as instance-level video object detection and event localization.", "labels": [], "entities": [{"text": "instance-level video object detection", "start_pos": 160, "end_pos": 197, "type": "TASK", "confidence": 0.6425331234931946}]}, {"text": "We also perform ablation studies to get insights and identify potential challenges for the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We develop anew data collection schema that ensures rich correspondences between referring ex- pressions and referred objects in a video using constraints.", "labels": [], "entities": []}, {"text": "The spatio-temporal relations that we are interested in are about state transitions, that is, what happens before and after the action and how objects move.", "labels": [], "entities": []}, {"text": "The state transitions should be relative to other objects and background.", "labels": [], "entities": []}, {"text": "For example, a sentence 'A man in a green uniform kicking the ball then running toward the net.' is a good video referring expression.", "labels": [], "entities": []}, {"text": "This sentence is valid only in a spatial region that represents a noun phrase 'a man in a green uniform' and a time interval in which an action from the verb phrase 'hitting the ball then running toward the net' occurs.", "labels": [], "entities": []}, {"text": "Also, the action 'hitting the ball' comes before his next action 'running toward the net' which shows the action steps of 'hitting' followed by 'running' and the action 'running' has a context object 'the net.'", "labels": [], "entities": []}, {"text": "First, we ensure that all of our High Definition videos (720p) crawled from Youtube contain at least two objects similar to (), but each video will focus on just one object class to form a contrast set.", "labels": [], "entities": []}, {"text": "This constraint prevents a simple video object detector from resolving referential ambiguity using only nouns by just outputting based on class information as in.", "labels": [], "entities": []}, {"text": "Because a simple object detector randomly outputs one object from a combination of the target and the contrast set.", "labels": [], "entities": []}, {"text": "Then, the output is the same as random because the object confidence scores do not correlate with the referring expression.", "labels": [], "entities": []}, {"text": "We want more language cues to guide the system to seek additional visual contexts () to focus and output only one unambiguous object detection.", "labels": [], "entities": []}, {"text": "The dataset contains 13 categories of videos which are either multi-player sports or animals.", "labels": [], "entities": []}, {"text": "Second, inspired by), a sentence, consists of a subject and a predicate, can be viewed as a set of structured labels based on part-of-speech and each label can be meaningfully grounded in a video.", "labels": [], "entities": []}, {"text": "Besides, annotations can use grammars for lexical grounding and surface real- ization.", "labels": [], "entities": []}, {"text": "Therefore, we ensure that every referring expression in our dataset provides grammatically relevant visual grounding based on part-of-speech such that a valid sentence must contain at least a noun phrase (NP), a verb phrase (VP) and one of a prepositional phrase (PP), adverb phrase (ADVP) or conjunction phrase (CONJP).", "labels": [], "entities": []}, {"text": "We also found that the annotators may write relevant sentences without the constraints but the contents are random and may not be visually grounded either spatially or temporally or both in the video.", "labels": [], "entities": []}, {"text": "Some example sentences without the constraints are \"The guy was lucky to save the tennis ball.\" and \"The sun is blocking the ball for the back player.\".", "labels": [], "entities": []}, {"text": "We want to show how and to what extent modular attention networks ground input expressions with motion information in videos.", "labels": [], "entities": []}, {"text": "So, we perform two sets of experiments, identifying description localization and automatic video object detector and temporal event localization.", "labels": [], "entities": [{"text": "identifying description localization", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7879466811815897}]}, {"text": "Similar to (, we split the dataset into training, validation and test sets at the video level; that is, there are no overlapping video segments for every split.", "labels": [], "entities": []}, {"text": "There are 159 training, 13 validation, and 27 test videos.", "labels": [], "entities": []}, {"text": "The rough ratio is 12:1:2.", "labels": [], "entities": [{"text": "rough ratio", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.9833039939403534}]}, {"text": "Implementation details are in the supplementary material.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9654510021209717}]}], "tableCaptions": [{"text": " Table 1: STV-IDL dataset statistics.", "labels": [], "entities": [{"text": "STV-IDL dataset", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8974029421806335}]}, {"text": " Table 2: STV-IDL part-of-speech statistics. (Please see  the supplementary material for more details.)", "labels": [], "entities": [{"text": "STV-IDL part-of-speech statistics", "start_pos": 10, "end_pos": 43, "type": "DATASET", "confidence": 0.7963593204816183}]}, {"text": " Table 3: Identifying Description Localization: mAP for  each collection. (values are in percents.) The fused1  MAttNet is the proposed two-stream method and the  fused5 MAttNet is the stacked version of the proposed  two-stream method.", "labels": [], "entities": []}, {"text": " Table 4: Ablation study on fused1 MAttNet: mAP for  each module combination. (values are in percents.)", "labels": [], "entities": [{"text": "fused1 MAttNet", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.748885989189148}]}, {"text": " Table 5: Ablation study on fused5 MAttNet: mAP for  each module combination. (values are in percents.)", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9545767903327942}, {"text": "fused5 MAttNet", "start_pos": 28, "end_pos": 42, "type": "DATASET", "confidence": 0.7910458743572235}]}, {"text": " Table 6: Visual Object Detection: mAP tracklet  IoU@0.5 for each model. (values are in percents.)", "labels": [], "entities": [{"text": "Visual Object Detection", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5432516634464264}]}, {"text": " Table 7: Event Localization: mAP temporal IoU@0.5  for each model. (values are in percents.)", "labels": [], "entities": []}, {"text": " Table 8: Spatio-temporal Localization: mAP temporal  IoU@0.5 then tracklet IoU@0.5 for each model. (val- ues are in percents.)", "labels": [], "entities": []}]}