{"title": [{"text": "Fill the GAP: Exploiting BERT for Pronoun Resolution", "labels": [], "entities": [{"text": "BERT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9816477298736572}, {"text": "Pronoun Resolution", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8300395011901855}]}], "abstractContent": [{"text": "In this paper, we describe our entry in the gen-dered pronoun resolution competition which achieved fourth place without data augmentation.", "labels": [], "entities": [{"text": "gen-dered pronoun resolution competition", "start_pos": 44, "end_pos": 84, "type": "TASK", "confidence": 0.6722923442721367}]}, {"text": "Our method is an ensemble system of BERTs which resolves co-reference in an interaction space.", "labels": [], "entities": [{"text": "BERTs", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9811169505119324}]}, {"text": "We report four insights from our work: BERT's representations involve significant redundancy; modeling interaction effects similar to natural language inference models is useful for this task; there is an optimal BERT layer to extract representations for pronoun resolution; and the difference between the attention weights from the pronoun to the candidate entities was highly correlated with the correct label, with interesting implications for future work.", "labels": [], "entities": [{"text": "BERT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9415594935417175}, {"text": "BERT", "start_pos": 213, "end_pos": 217, "type": "METRIC", "confidence": 0.971167266368866}, {"text": "pronoun resolution", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.7471037805080414}]}], "introductionContent": [{"text": "The Gendered Ambiguous Pronouns (GAP) dataset addresses gender bias by providing a dataset balanced over male and female pronouns.", "labels": [], "entities": []}, {"text": "The task is made challenging by long paragraph lengths of multiple sentences with a variety of named entities.", "labels": [], "entities": []}, {"text": "The text comes from the encyclopedia genre which is more formal and contains numerous technical terms.", "labels": [], "entities": []}, {"text": "Furthermore, world knowledge is indispensable to this task.", "labels": [], "entities": []}, {"text": "An example is given in where the pronoun is highlighted in green and the entities are highlighted in blue.", "labels": [], "entities": []}, {"text": "To know that She refers to Christine rather than Elsie Tanne, a model requires knowing that \"never been mentioned again\" is a result of having died.", "labels": [], "entities": []}, {"text": "Due to the small size of the dataset, our solution was mainly based on transfer learning.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8866376876831055}]}, {"text": "Specifically, we used representations of the pronoun and entities from an optimal frozen BERT 1 https://github.com/ google-research-datasets/gap-coreference Christine sent a telegram to congratulate Elsie and Steve Tanner on their wedding day in 1967.", "labels": [], "entities": [{"text": "BERT", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9561535120010376}]}, {"text": "In 1984, Elsie Tanner informed Ken Barlow that Christine had died of liver failure after becoming an alcoholic in the late 1970s.", "labels": [], "entities": []}, {"text": "She has never been mentioned again.: An example in GAP dataset.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8866904675960541}]}, {"text": "To refer \"she\" to \"Christine\", a model has to connect \"never been mentioned again\" with \"had died of liver failure\", which requires world knowledge.", "labels": [], "entities": []}, {"text": "layer) as inputs to a novel encoder architecture, whose results were then ensembled (Maclin and Opitz, 2011) over various BERT models (base and large, cased and uncased) using shallow neural networks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9205445647239685}]}, {"text": "Our result achieved fourth place in the Kaggle shared-task competition 2 . The competition is composed of two stages.", "labels": [], "entities": [{"text": "Kaggle shared-task competition", "start_pos": 40, "end_pos": 70, "type": "TASK", "confidence": 0.5204315185546875}]}, {"text": "In the first stage, the the development set of GAP which is used for evaluation and is entirely public to help competitors search for model architectures.", "labels": [], "entities": []}, {"text": "In the second stage, a large and unpublished dataset was used to test generalization ability as well as prevent label probing.", "labels": [], "entities": [{"text": "generalization", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.9706214070320129}, {"text": "label probing", "start_pos": 112, "end_pos": 125, "type": "TASK", "confidence": 0.6881401389837265}]}, {"text": "Our model makes the following contributions to this task.", "labels": [], "entities": []}, {"text": "We propose a multi-head natural language inference (NLI) encoder which resolves co-reference though heuristic interaction and efficiently addresses the redundancy in BERT by applying dropout to inputs directly.", "labels": [], "entities": [{"text": "BERT", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.7874457836151123}]}, {"text": "With layer-bylayer exploration, we extract the task-specific features from the optimal layer of BERT for coreference resolution where we observe pronouns strongly attend to the corresponding candidate entities.", "labels": [], "entities": [{"text": "BERT", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9828312397003174}, {"text": "coreference resolution", "start_pos": 105, "end_pos": 127, "type": "TASK", "confidence": 0.9770854115486145}]}], "datasetContent": [{"text": "Our models were built with Keras.", "labels": [], "entities": []}, {"text": "The input dropout rates were 0.6 and 0.7 for BERT Base and Large, respectively.", "labels": [], "entities": [{"text": "BERT Base", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9361409842967987}]}, {"text": "For the concatenation based model, the classifier was composed of a single hidden layer with size 37 following a batch normalization layer and a dropout layer with rate 0.6.", "labels": [], "entities": []}, {"text": "For the multi-head NLI encoder, the number of heads was 6 and the dimension of the down-projected vector space was 37.", "labels": [], "entities": []}, {"text": "The interactive encoder as composed of a hidden layer with size 37 following SELU activation.", "labels": [], "entities": [{"text": "SELU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.7349783778190613}]}, {"text": "To summarize the output from each NLI encoders, we used either a concatenation or summation operation following a dropout layer with rates 0.8, 0.85 respectively.", "labels": [], "entities": [{"text": "summation", "start_pos": 82, "end_pos": 91, "type": "TASK", "confidence": 0.9534435272216797}]}, {"text": "The classifier of the multi-head NLI encoder was exactly the same as the concatenation based encoder.", "labels": [], "entities": []}, {"text": "For training, we validated our models with 7-fold cross-validation and early-stopping on crossentropy with patience 20.", "labels": [], "entities": []}, {"text": "The batch size was 32 and the optimizer was Adam () with initial learning rate 1e \u22123 for all models.", "labels": [], "entities": []}, {"text": "We regularized the output layer with 0.1 L2 penalty.", "labels": [], "entities": []}, {"text": "The overall training time was about 8 hours for stage 1.", "labels": [], "entities": []}, {"text": "For other detailed settings and hyper-parameters please refer to our public code repository.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: GAP dataset sizes and label distributions.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8585121929645538}]}, {"text": " Table 2: The performance gain of each component for  both Bert Base and Bert Large, where all denotes using  input dropout, interaction layers, projection, and hand- crafted features at the same time. Performance is log  loss from the stage 1 and stage 2 testing data.", "labels": [], "entities": []}]}