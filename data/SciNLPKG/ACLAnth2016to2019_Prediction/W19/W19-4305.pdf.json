{"title": [{"text": "Multilingual NMT with a language-independent attention bridge", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose an architecture for machine translation (MT) capable of obtaining multilingual sentence representations by incorporating an intermediate attention bridge that is shared across all languages.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8668136596679688}]}, {"text": "We train the model with language-specific encoders and decoders that are connected through an inner-attention layer on the encoder side.", "labels": [], "entities": []}, {"text": "The attention bridge exploits the semantics from each language for translation and develops into a language-agnostic meaning representation that can efficiently be used for transfer learning.", "labels": [], "entities": [{"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9709209203720093}, {"text": "transfer learning", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.9039883315563202}]}, {"text": "We present anew framework for the efficient development of multilingual neural machine translation (NMT) using this model and scheduled training.", "labels": [], "entities": [{"text": "multilingual neural machine translation (NMT)", "start_pos": 59, "end_pos": 104, "type": "TASK", "confidence": 0.764461372579847}]}, {"text": "We have tested the approach in a systematic way with a multi-parallel data set.", "labels": [], "entities": []}, {"text": "The model achieves substantial improvements over strong bilingual models and performs well for zero-shot translation, which demonstrates its ability of abstraction and transfer learning.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7352207899093628}]}], "introductionContent": [{"text": "Neural machine translation (NMT) provides an ideal setting for multilingual MT because it can efficiently share model parameters and take advantage of the various similarities found by the model in the hidden layers and word embeddings).", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7652040322621664}, {"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.7775995135307312}]}, {"text": "Furthermore, multilingual NMT has the potential of considerably improving the performance of neural translation systems for low-resource languages ( and enables zero-shot translation, i.e., translating between language pairs that were not seen during training.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7946751713752747}, {"text": "zero-shot translation", "start_pos": 161, "end_pos": 182, "type": "TASK", "confidence": 0.7227660417556763}]}, {"text": "For this study we focus on models for multilingual translation that learn language-agnostic representations, where we outline the development of a language-independent representation based on an attention bridge shared across all languages.", "labels": [], "entities": [{"text": "multilingual translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6646069884300232}]}, {"text": "For this, we apply an architecture based on shared self-attention with language-specific encoders and decoders that can easily scale to a large number of languages while addressing the task of obtaining language-independent sentence embeddings.", "labels": [], "entities": []}, {"text": "Those embeddings are created from the encoder's self-attention and connect to the language-specific decoders that attend to them, hence the name 'bridge'.", "labels": [], "entities": []}, {"text": "Additionally, we add a penalty term to avoid redundancy in the shared layer.", "labels": [], "entities": []}, {"text": "More details of the architecture are given in section 2.", "labels": [], "entities": []}, {"text": "To summarise our contributions, we i) present a multilingual translation system that efficiently tackles the task of learning language-agnostic sentence representations; ii) verify that this model enables effective transfer learning and zeroshot translation through the shared representation layer; and iii) show that multilingually trained embeddings improve the majority of downstream and sentence probing tasks demonstrating the abstractions learned from the combined translation tasks.", "labels": [], "entities": [{"text": "zeroshot translation", "start_pos": 237, "end_pos": 257, "type": "TASK", "confidence": 0.6739236116409302}]}], "datasetContent": [{"text": "We conducted four translation experiments and tested the learned sentence representations via downstream tasks.", "labels": [], "entities": []}, {"text": "We used the multi30k dataset) for training and validation in all available languages: Czech, German, French and English, and tested the trained model with the flickr 2016 test data of the same dataset and obtained BLEU scores using the sacreBLEU script 2.", "labels": [], "entities": [{"text": "flickr 2016 test data", "start_pos": 159, "end_pos": 180, "type": "DATASET", "confidence": 0.9540582895278931}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9991346001625061}]}, {"text": "We lowercased, normalized and tokenized using the Moses toolkit (, and applied a 10K-operations Byte Pair Encoding (BPE) model per language.", "labels": [], "entities": []}, {"text": "Each encoder consists of 2 stacked BiLSTMs of size d h = 512, i.e., the hidden states per direction are of size 256.", "labels": [], "entities": []}, {"text": "Each decoder includes 2 stacked unidirectional LSTMs with hidden states of size 512.", "labels": [], "entities": []}, {"text": "For the model input and output, the word embeddings have dimension d x = d y = 512.", "labels": [], "entities": []}, {"text": "We used an attention bridge layer with 10 attention heads with d w = 1024, the dimensions of W 1 and W 2 from Eq.", "labels": [], "entities": []}, {"text": "We chose k = 10 because the mean length of a preprocessed sentence in the training data is 13.2 tokens in our case.", "labels": [], "entities": []}, {"text": "Choosing a much smaller k would create a bottleneck in the flow of information, and a bigger one would make the model slower and prone to overfitting.", "labels": [], "entities": []}, {"text": "We used a Stochastic Gradient Descent (SGD) optimizer with a learning rate of 1.0 and batch size 64, and selected the best model on the development set for each experiment.", "labels": [], "entities": []}, {"text": "We implemented our model on top of an OpenNMT-py () fork, which we make available for reproducibility purposes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BLEU scores obtained in the experiments. Left: Bilingual models, our baselines. Center: Models trained  on {De,Fr,Cs}\u2194En, with zero-shot translations in italics. Right: Many-to-many model. Both zero-shot and M-2-M  translations improve significantly when including monolingual data. (Best results in green cells.)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990721940994263}]}, {"text": " Table 2: Scores obtained in the SentEval tasks. The  BASELINE column reports the best score among the  bilingual models + att bridge. Green cells indicate  the highest score. All tasks show the accuracy of the  model except for SICKR and STS-B tasks, which in- clude Pearson mean values.", "labels": [], "entities": [{"text": "BASELINE", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9985002279281616}, {"text": "accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9992375373840332}, {"text": "Pearson mean values", "start_pos": 268, "end_pos": 287, "type": "METRIC", "confidence": 0.9578350186347961}]}, {"text": " Table 3.  Overall, both types of models show performance  in the same ballpark yielding similar results. As  discussed in Lin et al. (2017), the quantitative  effect of the penalty term might not be obvious  for some tasks, while keeping the positive effect  of encouraging the attentive matrix to be focused  on different aspects of the sentence.", "labels": [], "entities": []}, {"text": " Table 3: BLEU scores obtained with the BILINGUAL +  ATT BRIDGE models in the experiments with and with- out penalty term.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994871616363525}, {"text": "BILINGUAL", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9982824325561523}, {"text": "ATT", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.7257544994354248}, {"text": "BRIDGE", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.6652013063430786}]}]}