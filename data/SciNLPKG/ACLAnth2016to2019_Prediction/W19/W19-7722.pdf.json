{"title": [{"text": "Association metrics in neural transition-based dependency parsing", "labels": [], "entities": [{"text": "neural transition-based dependency parsing", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.6073146983981133}]}], "abstractContent": [{"text": "Lexical preferences encoded as association metrics have been shown to improve performance on structural ambiguities that are still challenging for modern parsers.", "labels": [], "entities": []}, {"text": "This paper introduces a mechanism to include lexical preferences into a neural transition-based dependency parser for German.", "labels": [], "entities": []}, {"text": "We compare pointwise mutual information (PMI) and embedding-based scores.", "labels": [], "entities": []}, {"text": "Both the PMI-based model and the embedding-based model outperform the baseline significantly.", "labels": [], "entities": []}, {"text": "The best model is PMI-based and increases overall performance by 0.26 LAS points over the baseline.", "labels": [], "entities": [{"text": "LAS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.9959179759025574}]}], "introductionContent": [{"text": "Structural ambiguities that cannot be solved purely on the basis of structural preferences still pose a major challenge to syntactic parsing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8732050657272339}]}, {"text": "Prepositional phrase (PP) attachment and subject-object inversion are two examples of such ambiguities.", "labels": [], "entities": [{"text": "Prepositional phrase (PP) attachment", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7527959247430166}]}, {"text": "gives an overview of the most frequent parser errors in a German newspaper corpus of 20K sentences and 350K tokens, parsed by the De parser with 92.01 labeled attachment score.", "labels": [], "entities": [{"text": "German newspaper corpus", "start_pos": 58, "end_pos": 81, "type": "DATASET", "confidence": 0.7973854541778564}]}, {"text": "It shows that more than one third of all errors involves prepositions, subjects and accusative objects.", "labels": [], "entities": []}], "datasetContent": [{"text": "The neural transition-based dependency parser of De Kok and Hinrichs (2016) serves as the baseline for the experiments.", "labels": [], "entities": []}, {"text": "Words, part-of-speech tags and characters are represented as vectors that were trained with structured skip-gram (.", "labels": [], "entities": []}, {"text": "Topological fields are used as additional input features.", "labels": [], "entities": []}, {"text": "The parser does pseudo-projective parsing (Nivre and and was trained on the shuffled T\u00fcBa-D/Z () that contains 105K sentences and 1.9M tokens of manually labeled data from the Berliner Tageszeitung (taz).", "labels": [], "entities": [{"text": "pseudo-projective parsing", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.7047281563282013}, {"text": "Berliner Tageszeitung (taz)", "start_pos": 176, "end_pos": 203, "type": "DATASET", "confidence": 0.8716273307800293}]}, {"text": "Non-gold part-of-speech tags were trained via 10-fold jackknifing on the T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "T\u00fcBa-D/Z", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.9472018678983053}]}, {"text": "The data was split in a 7:1:2 ratio for respectively training, development and testing.", "labels": [], "entities": []}, {"text": "Association scores are retrieved for lowercased word forms to increase lexical coverage.", "labels": [], "entities": []}, {"text": "Common and proper nouns are typically capitalized in German and were therefore not lowercased.", "labels": [], "entities": []}, {"text": "Results are presented as labeled (LAS) and unlabeled attachment scores (UAS) including punctuation.", "labels": [], "entities": [{"text": "labeled (LAS) and unlabeled attachment scores (UAS)", "start_pos": 25, "end_pos": 76, "type": "METRIC", "confidence": 0.716334814375097}]}, {"text": "Accuracies for inversion and prepositions indicate performance on resolving ambiguities.", "labels": [], "entities": []}, {"text": "Inversion accuracy reports correct labeling of subjects and objects in clauses with fronted object.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.6270391345024109}]}, {"text": "Preposition accuracy comprises all correct heads and labels of prepositional phrases and objects.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.7967931032180786}]}, {"text": "The test set contains 1,887 cases of inversion (5.82 percent of all clauses) and 31,687 prepositional phrases and objects.: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with PMI-based association scores.", "labels": [], "entities": [{"text": "Parser", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9727990627288818}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9183064103126526}, {"text": "neural dependency parsing", "start_pos": 189, "end_pos": 214, "type": "TASK", "confidence": 0.6440628965695699}]}, {"text": "The NPMI model with minimum frequency 5 achieves the best overall performance.", "labels": [], "entities": []}, {"text": "Both the PMI-based and embedding-based models perform better than the baseline.", "labels": [], "entities": []}, {"text": "Overall performance will improve by more correctly solved ambiguous attachments.", "labels": [], "entities": []}, {"text": "Lexical associations between more than two tokens maybe necessary to further improve ambiguity resolution.", "labels": [], "entities": [{"text": "ambiguity resolution", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.740287721157074}]}, {"text": "For PP attachment, the compatibility between the preposition, its modifier noun and the verbal or nominal head candidate of the PP have to be modeled.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9340216517448425}]}, {"text": "De have shown that trilexical preferences help to better capture attachment preferences of the preposition.", "labels": [], "entities": []}, {"text": "It can also be beneficial to make competing attachment sites available to the parser.", "labels": [], "entities": []}, {"text": "Currently, association scores are only computed for the two attachment candidates for any given parser state.", "labels": [], "entities": []}, {"text": "With beam search, several attachment candidates can compete in different analyses.", "labels": [], "entities": [{"text": "beam search", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8709271550178528}]}, {"text": "The best candidate can then be chosen from all or then best candidates (.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Five most frequent parser errors by dependency label of the parser by De Kok and Hinrichs", "labels": [], "entities": []}, {"text": " Table 2: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with  PMI-based association scores. The NPMI model with minimum frequency 5 achieves the best overall  performance.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9786089062690735}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8753945231437683}, {"text": "neural dependency parsing", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.6351583302021027}]}, {"text": " Table 3: Parser accuracy (overall, inversion, preposition attachment) for neural dependency parsing with  embedding-based association scores. The overall best model uses projectivized, fully typed dependency  embeddings with a binary indicator.", "labels": [], "entities": [{"text": "Parser", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9804832935333252}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9008966088294983}, {"text": "neural dependency parsing", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.6009583274523417}]}, {"text": " Table 4: PMI and embedding-based scores for random and incorrectly attached dependency triples.", "labels": [], "entities": [{"text": "PMI", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6335310339927673}]}]}