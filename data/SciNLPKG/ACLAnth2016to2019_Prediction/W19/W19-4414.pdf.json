{"title": [{"text": "Learning to combine Grammatical Error Corrections", "labels": [], "entities": [{"text": "combine Grammatical Error Corrections", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7294036597013474}]}], "abstractContent": [{"text": "The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 13, "end_pos": 47, "type": "TASK", "confidence": 0.8532898724079132}, {"text": "general text editing", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.5985696117083231}]}, {"text": "We propose an automatic way to combine black-box systems.", "labels": [], "entities": []}, {"text": "Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F score directly.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9995204210281372}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9994320273399353}, {"text": "F score", "start_pos": 160, "end_pos": 167, "type": "METRIC", "confidence": 0.9903610348701477}]}, {"text": "We show consistent improvement over the best standalone system in all the configurations tested.", "labels": [], "entities": []}, {"text": "This approach also outperforms average ensembling of different RNN models with random initializations.", "labels": [], "entities": []}, {"text": "In addition, we analyze the use of BERT for GEC-reporting promising results on this end.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9968463778495789}, {"text": "GEC-reporting", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6759264469146729}]}, {"text": "We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking.", "labels": [], "entities": []}, {"text": "This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correc-tion(Bryant et al., 2019).", "labels": [], "entities": []}, {"text": "Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F 0.5 by 3.7 points over the best result reported.", "labels": [], "entities": [{"text": "BEA 2019 shared task", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.8636672347784042}, {"text": "BEA 2019 shared task", "start_pos": 145, "end_pos": 165, "type": "DATASET", "confidence": 0.8182893395423889}, {"text": "F 0.5", "start_pos": 177, "end_pos": 182, "type": "METRIC", "confidence": 0.9857107996940613}]}], "introductionContent": [{"text": "Unlike other generation tasks (e.g. Machine Translation and Text Summarization), Grammatical Error Correction (GEC) contains separable outputs, edits that could be extracted from sentences, categorized) and evaluated separately.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7908958792686462}, {"text": "Text Summarization)", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7913916110992432}, {"text": "Grammatical Error Correction (GEC)", "start_pos": 81, "end_pos": 115, "type": "TASK", "confidence": 0.6632501681645712}]}, {"text": "Throughout the years different approaches were considered, some focused on specific error types () and others adjusted systems from other tasks (.", "labels": [], "entities": []}, {"text": "While * Contributed equally the first receive high precision, the latter often have high recall and differ in what they correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9988946318626404}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9993281364440918}]}, {"text": "To benefit from both worlds, pipelines and rescoring hybrids were introduced.", "labels": [], "entities": []}, {"text": "Another suggested method for combining is average ensembling , used when several end to end neural networks are trained.", "labels": [], "entities": []}, {"text": "As single systems tend to have low recall, pipelining systems may propagate errors and may not benefit from more than one system per error.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9988303780555725}]}, {"text": "Rescoring reduces recall and may not be useful with many systems ).", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9589968323707581}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9994564652442932}]}, {"text": "We propose anew method for combining systems ( \u00a74) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box.", "labels": [], "entities": []}, {"text": "We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems ( \u00a75).", "labels": [], "entities": []}, {"text": "To develop a system we trained GEC systems and gathered outputs from black-box systems ( \u00a73).", "labels": [], "entities": [{"text": "GEC", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.8623555898666382}]}, {"text": "One of the most frequent error types is spelling errors, we compared off of the shelf spellcheckers, systems developed for this error type specifically, to anew spellchecker ( \u00a73.1), finding that our spellchecker outperforms common spellcheckers on the task of spellchecking.", "labels": [], "entities": []}, {"text": "Another system tested was modifications of BERT ( to correct errors, allowing for less reliance on parallel data and more generalizability across domains ( \u00a73.4).", "labels": [], "entities": [{"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9930399656295776}]}, {"text": "Lastly, we tested generating synthetic errors ) as away to replace data in an unsupervised scenario.", "labels": [], "entities": []}, {"text": "While finding that mimicking the error distribution and generating errors on the same domain is better, we did not eventually participate in the low-resource track.", "labels": [], "entities": []}], "datasetContent": [{"text": "As our system is based on various parts and mainly focuses on the ability to smartly combine those, we experiment with how each of the parts work separately.", "labels": [], "entities": []}, {"text": "A special focus is given to combining strong components, black-box components and single components as combining is a crucial part of the innovation in this system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Combination statistics of the most common error types over two systems -Nematus and Grammarly", "labels": [], "entities": []}, {"text": " Table 2: Comparison of Grammatical Error Perfor- mance of Spellcheckers. Jamspell achieves the best  score as previously suggested.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of spellcheckers on spelling. Our  method outperforms other methods.", "labels": [], "entities": []}, {"text": " Table 4: Nematus performance on W&I dev set by  training data. The use of more data improves the sys- tem, but only when the training from the domain is up- sampled.", "labels": [], "entities": []}, {"text": " Table 5: Size of synthetic datasets and Nematus scores  when trained on them.", "labels": [], "entities": []}, {"text": " Table 6: Change in performance when avoiding hard  errors.", "labels": [], "entities": []}, {"text": " Table 7: Performance of systems and iterative combi- nation of them. Combination improves both precision  and recall even using low performing systems.", "labels": [], "entities": [{"text": "Combination", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9661666750907898}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9994082450866699}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9992270469665527}]}, {"text": " Table 8: Combining with off the shelf systems helps.", "labels": [], "entities": []}, {"text": " Table 9: Combining fares better compared to ensemble.", "labels": [], "entities": []}, {"text": " Table 10: Test set results when combining systems  from the competition used as black boxes. The com- bination is the new state of the art.", "labels": [], "entities": []}]}