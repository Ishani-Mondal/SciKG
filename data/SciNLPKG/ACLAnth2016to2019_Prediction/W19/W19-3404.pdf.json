{"title": [{"text": "A Hybrid Model for Globally Coherent Story Generation", "labels": [], "entities": [{"text": "Globally Coherent Story Generation", "start_pos": 19, "end_pos": 53, "type": "TASK", "confidence": 0.6895918995141983}]}], "abstractContent": [{"text": "Automatically generating globally coherent stories is a challenging problem.", "labels": [], "entities": []}, {"text": "Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences.", "labels": [], "entities": [{"text": "Neural text generation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6588764588038126}]}, {"text": "Existing work that incorporates a text planning module succeeded in generating recipes and dialogues, but appears quite data-demanding.", "labels": [], "entities": []}, {"text": "We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus.", "labels": [], "entities": [{"text": "novel story generation", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6779937247435252}]}, {"text": "The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data; a neural surface realization module then generates fluent text conditioned on the text plan.", "labels": [], "entities": []}, {"text": "Human evaluation showed that our model outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic story generation is the task of automatically determining the content and utilizing proper language to craft stories.", "labels": [], "entities": [{"text": "Automatic story generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6178767085075378}]}, {"text": "One of the most important aspects of these stories is their coherence.", "labels": [], "entities": []}, {"text": "The scope of global coherence includes arranging the contents in a plausible order, staying on topic, and creating cohesion through anaphoric expressions, etc.", "labels": [], "entities": []}, {"text": "Traditionally, story generation is performed with symbolic planning systems (see, e.g.,;).", "labels": [], "entities": [{"text": "story generation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8404908180236816}]}, {"text": "These systems often follow a hierarchical pipeline: higher level modules perform text planning, determine discourse relations and contents of each sentence; lower level modules account for surface realization accordingly.", "labels": [], "entities": [{"text": "text planning", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.6974916160106659}]}, {"text": "Although capable of producing impressive, coherent stories, these systems rely heavily on manual knowledge engineering to select actions, characters, etc., properly, therefore generalizing poorly to unseen domains.", "labels": [], "entities": []}, {"text": "Early NLG systems, on the other hand, excel at generating fluent on-topic utterances (see, e.g.,).", "labels": [], "entities": []}, {"text": "These models are data-based, and therefore can be applied to new domains if data is available.", "labels": [], "entities": []}, {"text": "However, these models struggle to keep track of longer story or dialog history, i.e., they may switch topics, repeat information or say things that are not consistent with sentences generated earlier (see, e.g.,).", "labels": [], "entities": []}, {"text": "Extra modelling efforts or knowledge input is required to improve global coherence.", "labels": [], "entities": []}, {"text": "Neural NLG systems that incorporate text planning efforts could improve global coherence, and grant some controllability over the contents, i.e. making it possible to generate stories given specific input of what should happen in the story.", "labels": [], "entities": []}, {"text": "uses a convolutional seq2seq model to generate a chain of predicate-argument structures from a prompt to sketch a story, and then uses another convolutional seq2seq model to convert the chain of predicate-argument structures to text.", "labels": [], "entities": []}, {"text": "The neural checklist model () keeps track of the progress of recipe generation with the usage of ingredient words, to generate recipes from a bag of ingredients.", "labels": [], "entities": [{"text": "recipe generation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7101157903671265}]}, {"text": "If we consider the task of story generation, events (\"script\" events; scripts capture knowledge about \"standardized sequences of events about daily activities such as going to a restaurant or visiting a doctor\") are also fairly informative for the narration progress.", "labels": [], "entities": [{"text": "story generation", "start_pos": 27, "end_pos": 43, "type": "TASK", "confidence": 0.7343168258666992}]}, {"text": "Thus if one could identify events within surface texts, it is, in principle, possible to regard the events as indicators of the narration progress (just like the ingredients in a recipe) and apply the neural checklist model.", "labels": [], "entities": []}, {"text": "These requirements are fulfilled by the InScript () corpus, a small corpus that contains a total of about 1000 stories about everyday events from 10 scenarios.", "labels": [], "entities": []}, {"text": "Each story is annotated with script-relevant event types to align them with surface language.", "labels": [], "entities": []}, {"text": "However, surprisingly, when we apply the neural checklist model on InScript to generate stories, the quality of the generated stories appears poor (see the second item in fora sample generation).", "labels": [], "entities": []}, {"text": "To tackle these issues, we propose anew neural story generation model that exploits explicit, symbolic text planning.", "labels": [], "entities": []}, {"text": "The model consists of two components: the agenda generator produces an agenda, a sequence of script events that would later be fleshed out to yield a story, by a neural surface realization module; the neural surface realization module treats the events in the agenda as a latent variable that encodes the progress of story generation, and produces text conditioned on it.", "labels": [], "entities": []}, {"text": "The outcome is a system that could be trained with much less data.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first attempt to integrate a completely symbolic text planner with a neural surface realization component, to perform fully interpretable text planning.", "labels": [], "entities": [{"text": "interpretable text planning", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6737443109353384}]}, {"text": "Human evaluation shows that our system significantly outperforms various baselines in terms of fluency and global coherence.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We develop a story generation model that generates globally coherent stories about daily activities.", "labels": [], "entities": [{"text": "story generation", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7546694576740265}]}, {"text": "\u2022 We propose a novel way to combine a neural story generation model with an explicit, symbolic text planning component; furthermore, we show that the design reduces the demand on training data.", "labels": [], "entities": [{"text": "neural story generation", "start_pos": 38, "end_pos": 61, "type": "TASK", "confidence": 0.7936023672421774}]}, {"text": "\u2022 We illustrate the possibility of guiding the direction of story generation by conditioning the generation on a latent intention variable.", "labels": [], "entities": [{"text": "story generation", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.7113926559686661}]}, {"text": "In the remainder of this paper, we start with a discussion of related research, and then introduce the InScript corpus.", "labels": [], "entities": [{"text": "InScript corpus", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.7719106376171112}]}, {"text": "To follow is a detailed introduction of our model and the results from human evaluation.", "labels": [], "entities": []}, {"text": "Analysis of the results and some discussions about future directions conclude the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "2.5% of the data were randomly selected as the validation set; the rest was kept as the training set.", "labels": [], "entities": []}, {"text": "As evaluation will be up to humans instead of any test set metric (see section 5.2), no test set is necessary.", "labels": [], "entities": []}, {"text": "The model was implemented with Python 3.5.", "labels": [], "entities": []}, {"text": "The neural network part of the model was implemented with Keras 2.1.2.", "labels": [], "entities": [{"text": "Keras 2.1.2", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.8587537407875061}]}, {"text": "Optimization was performed with adam optimizer) with gradient clipping to stabilize the training (see.", "labels": [], "entities": []}, {"text": "To regularize the model, dropout) was applied to all dense connections, including the explicit dense layers and the fullyconnected layers within the GRU cells; besides, we applied early stopping, which monitors the loss function as is defined in section 4.2.", "labels": [], "entities": [{"text": "early stopping", "start_pos": 180, "end_pos": 194, "type": "METRIC", "confidence": 0.9196614325046539}]}, {"text": "Hyper-parameters are tuned with a two-stage random hyper-parameter search, which is empirically proven more effective than grid search (see).", "labels": [], "entities": []}, {"text": "On the validation set, the model yields a 0.90 accuracy and a 0.75 F 1 score on the binary classification task concerning output a (whether to shift to the next event; see section 5.3.1 for some discussion on its consequences) and a 38.9 perplexity for predicting the next word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9993253946304321}, {"text": "F 1 score", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9905136028925577}, {"text": "predicting the next word", "start_pos": 253, "end_pos": 277, "type": "TASK", "confidence": 0.8482894748449326}]}, {"text": "2 .   Automatic evaluation of text quality, especially its global coherence, is a challenging task (see, e.g. Lapata and Barzilay, 2005; Purdy et al., 2018, for some meaningful attempts though).", "labels": [], "entities": []}, {"text": "We also observed poor correlations between a few automatic metrics and the results of human evaluation (see appendix C for more details), and decided that automatic metrics are not suitable for our task.", "labels": [], "entities": []}, {"text": "Thus we performed human evaluation through crowdsourcing; it evaluates the following aspects of generated stories.", "labels": [], "entities": []}, {"text": "\u2022 Syntax The syntactical correctness of the sentences.", "labels": [], "entities": []}, {"text": "\u2022 Global Coherence The global coherence of a story with regard to the given script, e.g., GOING GROCERY SHOPPING.", "labels": [], "entities": [{"text": "GOING GROCERY SHOPPING", "start_pos": 90, "end_pos": 112, "type": "METRIC", "confidence": 0.7643261551856995}]}, {"text": "We evaluate from three aspects: Inclusion (does the story cover the most necessary steps about the topic?), Relevance (does the story stay on-topic, and rarely mention anything irrelevant to the topic?", "labels": [], "entities": [{"text": "Relevance", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9923769235610962}]}, {"text": "), and Order (does the story describe the activities relevant to the topic in a plausible order?)", "labels": [], "entities": [{"text": "Order", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9824152588844299}]}, {"text": "\u2022 Agenda Coverage The correspondence between the generated story and the agenda it was fed with.", "labels": [], "entities": []}, {"text": "The participants were asked whether each of the agenda items has been realized in the story.", "labels": [], "entities": []}, {"text": "We ask participants five questions per story: for Agenda Coverage, participants were asked to checkoff the agenda items that were mentioned in the story they saw; for the other four aspects, participants were asked to rate on a 1 to 4 scale.", "labels": [], "entities": [{"text": "Agenda Coverage", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.6491909623146057}]}, {"text": "The evaluation experiment was implemented with Lingoturk (Pusse et al., 2016); we hired participants and conducted the experiment on Prolific . See appendix B for more details on conducting the experiment of human evaluation.", "labels": [], "entities": [{"text": "Prolific", "start_pos": 133, "end_pos": 141, "type": "DATASET", "confidence": 0.9401036500930786}]}, {"text": "illustrates the results from human evaluation.", "labels": [], "entities": []}, {"text": "The GRU model, a plain language model without coherence modeling, yields the worst performance on all metrics.", "labels": [], "entities": []}, {"text": "The output wildly changes between topics and is incoherent globally; the poor coherence probably also negatively affects human judgments on syntactic correctness.", "labels": [], "entities": []}, {"text": "The neural checklist saw better performance than plain GRUs, but it failed to include the most necessary steps of the scenario.", "labels": [], "entities": []}, {"text": "It seems the model cannot correctly track the progress of the generation, which, as discussed in section 1, we suspect to be a consequence of the limited amount of training data: as its attention-based contentplanning cannot make use of the order information and has to learn it from data, the model (and probably also other attention-based models) has a substantially higher demand on training data.", "labels": [], "entities": []}, {"text": "Four stories per model variant per script (that is, 200 stories in total) were randomly selected for evaluation.", "labels": [], "entities": []}, {"text": "Each task included the assessment of five stories (one from each system); participants were compensated with 1.5GBP per task, which corresponds to a payment of approx. 7GBP per hour.", "labels": [], "entities": []}, {"text": "For each of the stories, we collected the judgments of about 10 crowd-sourcing participants (about 400 participations in total).", "labels": [], "entities": []}, {"text": "All participants were native English speakers.", "labels": [], "entities": []}, {"text": "Submissions that left at least one question unanswered or fall beyond 3 standard deviations are excluded from the statistics.", "labels": [], "entities": []}, {"text": "As a result, we received 1221 valid evaluation items in total.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results from human evaluation. Highest scores out of automatic systems are displayed in boldface.", "labels": [], "entities": []}, {"text": " Table 5: The final choices of hyper-parameters. \u03b2 is the weight applied on the output a and \u03b3 is weight applied on  the loss of the less frequent category in the binary classification.", "labels": [], "entities": []}]}