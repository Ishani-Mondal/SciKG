{"title": [{"text": "Robust Document Representations for Cross-Lingual Information Retrieval in Low-Resource Settings", "labels": [], "entities": [{"text": "Robust Document Representations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8210540811220804}, {"text": "Cross-Lingual Information Retrieval", "start_pos": 36, "end_pos": 71, "type": "TASK", "confidence": 0.7191339135169983}]}], "abstractContent": [{"text": "The goal of cross-lingual information retrieval (CLIR) is to find relevant documents written in languages different from that of the query.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 12, "end_pos": 54, "type": "TASK", "confidence": 0.7840532263120016}]}, {"text": "Robustness to translation errors is one of the main challenges for CLIR, especially in low-resource settings where there is limited training data for building machine translation (MT) systems or bilingual dictionaries.", "labels": [], "entities": [{"text": "Robustness to translation errors", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6769887059926987}, {"text": "machine translation (MT)", "start_pos": 159, "end_pos": 183, "type": "TASK", "confidence": 0.8539810180664062}]}, {"text": "If the test collection contains speech documents, additional errors from automatic speech recognition (ASR) makes translation even more difficult.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 73, "end_pos": 107, "type": "TASK", "confidence": 0.7712099154790243}, {"text": "translation", "start_pos": 114, "end_pos": 125, "type": "TASK", "confidence": 0.9630234241485596}]}, {"text": "We propose a robust document representation that combines N-best translations and a novel bag-of-phrases output from various ASR/MT systems.", "labels": [], "entities": [{"text": "ASR/MT", "start_pos": 125, "end_pos": 131, "type": "TASK", "confidence": 0.8207548459370931}]}, {"text": "We perform a comprehensive empirical analysis on three challenging collections; they consist of Somali, Swahili, and Tagalog speech/text documents to be retrieved by English queries.", "labels": [], "entities": []}, {"text": "By comparing various ASR/MT systems with different error profiles , our results demonstrate that a richer document representation can consistently overcome issues in low translation accuracy for CLIR in low-resource settings.", "labels": [], "entities": [{"text": "ASR/MT", "start_pos": 21, "end_pos": 27, "type": "TASK", "confidence": 0.7698575854301453}, {"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.8674392104148865}]}], "introductionContent": [{"text": "Cross-lingual Information Retrieval (CLIR) is a search task where the user's query is written in a language different from that of the documents in the collection.", "labels": [], "entities": [{"text": "Cross-lingual Information Retrieval (CLIR)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7702269305785497}]}, {"text": "There are some important niche applications, for example, a local news reporter searching foreign-language news-feeds to obtain different perspectives for her story, or a patent writer exploring the patents in another country to understand prior art before submitting her application, or an aid worker monitoring the social media of a disaster-affected area, looking for unmet needs and new emergencies.", "labels": [], "entities": []}, {"text": "In all these scenarios, CLIR increases the user base by enabling users who are not proficient in the foreign language to productively participate as knowledge workers.", "labels": [], "entities": []}, {"text": "Even if the user requires manual translations of the retrieved documents to complete her task, CLIR can at least provide a triage/filtering step.", "labels": [], "entities": []}, {"text": "CLIR performance depends critically on the accuracy of its underlying machine translation or bilingual dictionary component.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.998813271522522}]}, {"text": "Recent advances in MT suggest that it is now evermore possible to build CLIR for practical use.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9818837642669678}]}, {"text": "In particular, the availability of large amounts of parallel text in some language-pairs (e.g. English sentences and their aligned German translations from European Parliamentary proceedings) had led to dramatic improvements in MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 228, "end_pos": 230, "type": "TASK", "confidence": 0.9948038458824158}]}, {"text": "However, there are many language-pairs-what we term \"low-resource\" settings-where parallel text is limited and the challenge is to make CLIR robust to translation errors.", "labels": [], "entities": []}, {"text": "Missing words in translation may lead to degradation in recall, while extraneous words may lead to degradation in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9978467226028442}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9977303147315979}]}, {"text": "In this work, we focus on the document translation approach to CLIR, where all foreign documents in the collection are translated into the language of the user query prior to indexing and search.", "labels": [], "entities": [{"text": "document translation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.757514625787735}]}, {"text": "While the use of N-best translations in CLIR is not anew idea, the contribution of the paper is a comprehensive analysis of how different kinds of document representations perform under low-resource settings.", "labels": [], "entities": []}, {"text": "We compare whether in-dexing the N-best translations from MT leads to better CLIR than indexing only the 1-best (mostlikely) translation.", "labels": [], "entities": [{"text": "CLIR", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.969417154788971}]}, {"text": "We also propose a novel bagof-phrases document representation and show that it can be effectively combined with the N-best document representations.", "labels": [], "entities": []}, {"text": "The idea behind the bagof-phrases translation is the fact that less strict syntax is required in a CLIR system, which is often based on keyword search.", "labels": [], "entities": [{"text": "bagof-phrases translation", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.6622785329818726}]}, {"text": "The bag-of-phrases method relaxes the strict language grammar in the target language when producing translations, and instead, emphasizes the selection of translation words.", "labels": [], "entities": []}, {"text": "We perform comprehensive experiments on three low-resource test collections from the IARPA MATERIAL project, where the documents are in Somali, Swahili, and Tagalog and the queries are in English.", "labels": [], "entities": [{"text": "IARPA MATERIAL project", "start_pos": 85, "end_pos": 107, "type": "DATASET", "confidence": 0.7583318750063578}]}, {"text": "The inclusion of speech documents (audio files) in this collection means that automatic speech recognition (ASR) has to be run before MT, leading to further challenges in translation accuracy.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7501215785741806}, {"text": "MT", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.9100318551063538}, {"text": "translation", "start_pos": 171, "end_pos": 182, "type": "TASK", "confidence": 0.9555318355560303}, {"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.6440746188163757}]}, {"text": "Our results demonstrate that a rich document representation containing many translation hypotheses consistently improves CLIR performance in these low-resource settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our ASR system follows normal pattern for Kaldibased (Povey et al., 2011) system build.", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9145857691764832}, {"text": "Kaldibased (Povey et al., 2011) system build", "start_pos": 42, "end_pos": 86, "type": "DATASET", "confidence": 0.7436685502529145}]}, {"text": "Our recipe is publicly available at GitHub . Acoustic and language model.", "labels": [], "entities": []}, {"text": "We use GMM training to create alignments and lattice-free MMItrained neural network () with factorized TDNN (.", "labels": [], "entities": []}, {"text": "We gen-erate lattices with n-gram ARPA-style language model and re-score them with an n-best RNN language model ().", "labels": [], "entities": []}, {"text": "Source-side bitext and crawled monolingual data are used in building the n-gram LM, RNNLM rescoring, as well as extending the baseline lexicon.", "labels": [], "entities": []}, {"text": "In addition to supervised training, we ran semisupervised training of acoustic models using the extension of lattice-free MMI to semi-supervised scenarios.", "labels": [], "entities": []}, {"text": "We added unlabeled audio to the labeled audio in the training set to train the acoustic model.", "labels": [], "entities": []}, {"text": "shows the WER improvements from supervised to semisupervised setup for Somali, Swahili, and Tagalog.", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.5867029428482056}]}, {"text": "To study the effect of ASR errors on CLIR, we tried both supervised and semi-supervised ASR systems in our experiments.", "labels": [], "entities": [{"text": "ASR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9889677166938782}, {"text": "ASR", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9696125984191895}]}, {"text": "Test data come in long unsegmented files of over a minute.", "labels": [], "entities": []}, {"text": "To deal with this, we split the input into equal-size (15 second) slightly overlapping segments and stitch together the ASR outputs.", "labels": [], "entities": []}, {"text": "For consistency, we lower-case all text resources that are used in training the ASR system, which include transcripts and external resources for language modeling (source-side bitext, web crawled monolingual text).", "labels": [], "entities": [{"text": "consistency", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8713296055793762}, {"text": "ASR", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9066095352172852}]}, {"text": "As a result, the ASR output would be all lower-case.", "labels": [], "entities": [{"text": "ASR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9026910662651062}]}, {"text": "However, the machine translation system expects inputs that have been tokenized and true-cased.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7287416756153107}]}, {"text": "Thus, we post-process ASR output to normalize punctuation, tokenize, and true-case using the models and scripts that are used in MT training and decoding.", "labels": [], "entities": [{"text": "ASR", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9753374457359314}, {"text": "MT training", "start_pos": 129, "end_pos": 140, "type": "TASK", "confidence": 0.9241544306278229}]}, {"text": "This post-processing helps passing names through the MT system, and improves the IR performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9341613054275513}, {"text": "IR", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9506316781044006}]}], "tableCaptions": [{"text": " Table 1: CLIR test collection statistics.", "labels": [], "entities": [{"text": "CLIR test collection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.71774293979009}]}, {"text": " Table 3: %WER for supervised (ASR1) and semi-supervised (ASR2) systems, BLEU scores for PBMT and NMT systems.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9979912042617798}, {"text": "BLEU scores", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.9712758958339691}]}, {"text": " Table 4: MAP scores for various ASR/MT systems and document representations (N=5) on Somali, Swahili, and Tagalog test  sets.", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6204649806022644}, {"text": "ASR/MT", "start_pos": 33, "end_pos": 39, "type": "TASK", "confidence": 0.8842195272445679}]}]}