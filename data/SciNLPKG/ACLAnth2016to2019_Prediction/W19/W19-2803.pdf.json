{"title": [{"text": "Entity Decisions in Neural Language Modelling: Approaches and Problems", "labels": [], "entities": [{"text": "Neural Language Modelling", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.651858518520991}]}], "abstractContent": [{"text": "We explore different approaches to explicit entity modelling in language models (LM).", "labels": [], "entities": [{"text": "explicit entity modelling in language models (LM)", "start_pos": 35, "end_pos": 84, "type": "TASK", "confidence": 0.7149511906835768}]}, {"text": "We independently replicate two existing models in a controlled setup, introduce a simplified variant of one of the models and analyze their performance indirect comparison.", "labels": [], "entities": []}, {"text": "Our results suggest that today's models are limited as several stochastic variables make learning difficult.", "labels": [], "entities": []}, {"text": "We show that the most challenging point in the systems is the decision if the next token is an entity token.", "labels": [], "entities": []}, {"text": "The low precision and recall for this variable will lead to severe cascading errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9992260932922363}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.999607264995575}]}, {"text": "Our own simplified approach dispenses with the need for latent variables and improves the performance in the entity yes/no decision.", "labels": [], "entities": []}, {"text": "A standard well-tuned baseline RNN-LM with a larger number of hidden units outperforms all entity-enabled LMs in terms of perplexity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Reference to entities in the world is a core feature of human language, and coreference between different mentions in a text is a fundamental property of coherent communication.", "labels": [], "entities": [{"text": "coreference between different mentions in a text", "start_pos": 76, "end_pos": 124, "type": "TASK", "confidence": 0.8343588369233268}]}, {"text": "Computational approaches to reference have long been studied in the area of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.9797515869140625}]}, {"text": "Very recently, explicit models of reference have also been studied in the context of language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7437716126441956}]}, {"text": "The usual approach is to introduce latent variables modelling whether the next token is part of an entity mention, and which of the previously seen entities it refers to.", "labels": [], "entities": []}, {"text": "In this work, we present a comparative study of three language modelling approaches with explicit representations of entity coreference: Yang-LM, the entity-enabled language model (LM) of, the EntityNLM model of, and SetLM, our own extension of the latter.", "labels": [], "entities": []}, {"text": "YangLM and EntityNLM differ in the parameterization of the latent variables and the order in which decisions are made.", "labels": [], "entities": [{"text": "YangLM", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8383857607841492}]}, {"text": "SetLM is a simpler architecture with fewer loss functions.", "labels": [], "entities": []}, {"text": "It replaces the latent variable modelling the decision whether to produce an entity with two extra embeddings, one fora new entity (similar to the other models) and one for the case that the token does not belong to an entity.", "labels": [], "entities": []}, {"text": "We replicate the results of and with an independent reimplementation of their models in a comparable experimental setup and evaluate the models in terms of overall language modelling performance performance in comparison with a simple RNN-LM.", "labels": [], "entities": []}, {"text": "We also study the accuracy and precision/recall in each individual decision step and look at the convergence of variables.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9994333386421204}, {"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.999043881893158}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.980146586894989}]}, {"text": "We find that YangLM outperforms the other models in terms of perplexity, whereas SetLM achieves the best results for the entity yes/no prediction.", "labels": [], "entities": [{"text": "YangLM", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.8543343544006348}]}, {"text": "None of the entity-enabled LMs is competitive with a simple RNN-LM having a higher number of hidden units, and we do not achieve similar gains by enlarging the hidden sizes of the entity LMs.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the main metric for the language models general performance, we measure the perplexity.", "labels": [], "entities": []}, {"text": "We also evaluate the entity prediction process qualitatively by measuring precision and recall for the question if the next token is part of an entity mention and the accuracy for the choice of the entity from the set, and evaluate the length prediction in the model based on EntityNLM.", "labels": [], "entities": [{"text": "entity prediction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7466721832752228}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994551539421082}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9987506866455078}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9992604851722717}, {"text": "length", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9666714668273926}, {"text": "EntityNLM", "start_pos": 276, "end_pos": 285, "type": "DATASET", "confidence": 0.9400607943534851}]}, {"text": "For our model, we regard the choice of e noentity as the Entity Nodecision and the choice for either of the entities as the Entity Yes-decision.", "labels": [], "entities": []}, {"text": "For the accuracy of the choice of the entity from the set, we only looked at the choices in the Entity Yes-case.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9985658526420593}]}], "tableCaptions": [{"text": " Table 1: Token perplexity results", "labels": [], "entities": [{"text": "Token perplexity", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8806557059288025}]}, {"text": " Table 2: Relation Perplexity All Tokens / Entity Tokens", "labels": [], "entities": [{"text": "Relation Perplexity", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8773526251316071}]}, {"text": " Table 3: Entity Yes/No Prediction", "labels": [], "entities": [{"text": "Entity Yes/No Prediction", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.5965096056461334}]}]}