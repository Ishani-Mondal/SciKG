{"title": [{"text": "UdS Submission for the WMT 19 Automatic Post-Editing Task", "labels": [], "entities": [{"text": "UdS Submission", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7302595674991608}, {"text": "WMT 19 Automatic Post-Editing", "start_pos": 23, "end_pos": 52, "type": "DATASET", "confidence": 0.7653360217809677}]}], "abstractContent": [{"text": "In this paper, we describe our submission to the English-German APE shared task at WMT 2019.", "labels": [], "entities": [{"text": "APE shared task at WMT 2019", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.7006620764732361}]}, {"text": "We utilize and adapt an NMT architecture originally developed for exploiting context information to APE, implement this in our own transformer model and explore joint training of the APE task with a de-noising encoder.", "labels": [], "entities": [{"text": "APE", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.7173557281494141}, {"text": "APE task", "start_pos": 183, "end_pos": 191, "type": "TASK", "confidence": 0.7505410313606262}]}], "introductionContent": [{"text": "The Automatic Post-Editing (APE) task is to automatically correct errors in machine translation outputs.", "labels": [], "entities": [{"text": "machine translation outputs", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.7261354227860769}]}, {"text": "This paper describes our submission to the English-German APE shared task at WMT 2019.", "labels": [], "entities": [{"text": "APE shared task at WMT 2019", "start_pos": 58, "end_pos": 85, "type": "DATASET", "confidence": 0.6479698518911997}]}, {"text": "Based on recent research on the APE task and an architecture for the utilization of documentlevel context information in neural machine translation (), we re-implement a multi-source transformer model for the task.", "labels": [], "entities": [{"text": "APE task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.8149144351482391}, {"text": "neural machine translation", "start_pos": 121, "end_pos": 147, "type": "TASK", "confidence": 0.7539798617362976}]}, {"text": "Inspired by, we try to train a more robust model by introducing a multi-task learning approach which jointly trains APE with a de-noising encoder.", "labels": [], "entities": [{"text": "APE", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.5416825413703918}]}, {"text": "We made use of the artificial eScape data set (  provided for the task, since the multi-source transformer model contains a large number of parameters and training with large amounts of supplementary synthetic data can help regularize its parameters and make the model more general.", "labels": [], "entities": [{"text": "eScape data set", "start_pos": 30, "end_pos": 45, "type": "DATASET", "confidence": 0.7838985621929169}]}], "datasetContent": [{"text": "We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) for transformer-based neural machine translation.", "labels": [], "entities": [{"text": "transformer-based neural machine translation", "start_pos": 89, "end_pos": 133, "type": "TASK", "confidence": 0.6627637445926666}]}], "tableCaptions": [{"text": " Table 1: BLEU Scores of Data Sets", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.958362877368927}]}, {"text": " Table 2: BLEU Scores on the Development Set", "labels": [], "entities": [{"text": "BLEU Scores", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9722090661525726}, {"text": "Development Set", "start_pos": 29, "end_pos": 44, "type": "DATASET", "confidence": 0.8779790103435516}]}, {"text": " Table 3: Results on the Test Set", "labels": [], "entities": [{"text": "Test Set", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.8772667348384857}]}]}