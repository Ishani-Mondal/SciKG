{"title": [], "abstractContent": [{"text": "Automatic identification and expansion of ambiguous abbreviations are essential for biomedical natural language processing applications , such as information retrieval and question answering systems.", "labels": [], "entities": [{"text": "Automatic identification and expansion of ambiguous abbreviations", "start_pos": 0, "end_pos": 65, "type": "TASK", "confidence": 0.7224223102842059}, {"text": "biomedical natural language processing", "start_pos": 84, "end_pos": 122, "type": "TASK", "confidence": 0.6049202233552933}, {"text": "information retrieval", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.7904712557792664}, {"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.8965034484863281}]}, {"text": "In this paper, we present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model.", "labels": [], "entities": [{"text": "DEep Contextualized Biomedical Abbreviation Expansion (DECBAE)", "start_pos": 26, "end_pos": 88, "type": "METRIC", "confidence": 0.6636546812951565}]}, {"text": "DECBAE automatically collects substantial and relatively clean annotated contexts for 950 ambiguous abbreviations from PubMed abstracts using a simple heuristic.", "labels": [], "entities": []}, {"text": "Then it utilizes BioELMo (Jin et al., 2019) to extract the contextualized features of words, and feed those features to abbreviation-specific bidi-rectional LSTMs, where the hidden states of the ambiguous abbreviations are used to assign the exact definitions.", "labels": [], "entities": []}, {"text": "Our DECBAE model outperforms other baselines by large margins, achieving average accuracy of 0.961 and macro-F1 of 0.917 on the dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9783161282539368}]}, {"text": "It also surpasses human performance for expanding a sample abbreviation, and remains robust in im-balanced, low-resources and clinical settings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abbreviations are shortened forms of text-strings.", "labels": [], "entities": []}, {"text": "They are prevalent in biomedical literature such as scientific articles, clinical notes and user queries in information retrieval systems.", "labels": [], "entities": []}, {"text": "Abbreviations can be ambiguous (e.g.: ER can refer to estrogen receptor, endoplasmic reticulum, emergency room etc.), especially when they appear in short or professional texts where the definitions are not given.", "labels": [], "entities": [{"text": "Abbreviations", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9873148202896118}]}, {"text": "For instance, about 15% of PubMed queries include abbreviations, and about 14.8% of all tokens in a clinical note dataset are abbreviations ().", "labels": [], "entities": []}, {"text": "In both cases, the definitions of the abbreviations are rarely provided.", "labels": [], "entities": []}, {"text": "Thus, automatic expansion of ambiguous abbreviations to their full forms is vital in biomedical natural language processing (NLP) systems.", "labels": [], "entities": [{"text": "automatic expansion of ambiguous abbreviations", "start_pos": 6, "end_pos": 52, "type": "TASK", "confidence": 0.7868041634559632}, {"text": "biomedical natural language processing (NLP)", "start_pos": 85, "end_pos": 129, "type": "TASK", "confidence": 0.7446973238672528}]}, {"text": "In this paper, we focus on the cases where definitions of ambiguous abbreviations are not directly available in the contexts, so reasoning over the contexts is required for disambiguation.", "labels": [], "entities": []}, {"text": "Under the conditions where definitions are provided in the contexts, one can easily extract them using rulebased methods.", "labels": [], "entities": []}, {"text": "We present DEep Contextualized Biomedical Abbreviation Expansion (DECBAE) model.", "labels": [], "entities": [{"text": "DEep Contextualized Biomedical Abbreviation Expansion (DECBAE)", "start_pos": 11, "end_pos": 73, "type": "METRIC", "confidence": 0.571273896843195}]}, {"text": "DECBAE uses a simple heuristic to automatically construct large supervised disambiguation datasets for 950 abbreviations from PubMed abstracts: In scientific writing, authors define abbreviations the first time they are used, and the same abbreviations in the following sentences have the same definitions as those of the first ones.", "labels": [], "entities": [{"text": "DECBAE", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7862915992736816}]}, {"text": "We extract all the sentences containing the same abbreviations in each PubMed abstract, and use the definition given in the first sentence as the full form label of abbreviations in the following sentences.", "labels": [], "entities": [{"text": "PubMed abstract", "start_pos": 71, "end_pos": 86, "type": "DATASET", "confidence": 0.9282904863357544}]}, {"text": "We group the definitions for each abbreviation and formulate abbreviation expansion as a classification task, where input is an ambiguous abbreviation with its context, and the output is one of its possible definitions.", "labels": [], "entities": [{"text": "formulate abbreviation expansion", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8857813278834025}]}, {"text": "Recent breakthroughs of language models (LM) pre-trained on large corpora like ELMo () and BERT clearly show that unsupervised LM pre-training can vastly improve performance of downstream models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9663110375404358}]}, {"text": "To fully utilize the knowledge encoded in PubMed abstracts, DECBAE uses BioELMo (), a domain adapation verison of ELMo, to embed the words.", "labels": [], "entities": [{"text": "BioELMo", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9139715433120728}]}, {"text": "After the embedding layer, DECBAE applies abbreviation-specific bidirectional LSTM (biLSTM) classifiers to do the abbreviation expansion, where the biLSTM parameters are trained separately for each abbrevi-ation.", "labels": [], "entities": []}, {"text": "We train DECBAE from the automatically collected dataset of 950 ambiguous abbreviations.", "labels": [], "entities": [{"text": "DECBAE", "start_pos": 9, "end_pos": 15, "type": "DATASET", "confidence": 0.44456109404563904}]}, {"text": "At inference time, DECBAE feeds the BioELMo embeddings of the whole sentence and uses the corresponding abbreviation-specific biLSTM classifiers to perform disambiguation of abbreviations in the sentence.", "labels": [], "entities": []}, {"text": "We show that DECBAE outperforms other baselines by large margins and even performs better than single human expert.", "labels": [], "entities": [{"text": "DECBAE", "start_pos": 13, "end_pos": 19, "type": "TASK", "confidence": 0.7916417717933655}]}, {"text": "Although training instances of DECBAE are collected from PubMed, it covers 85% of clinically related abbreviations mentioned in a previous work (.", "labels": [], "entities": [{"text": "PubMed", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.9652462601661682}]}, {"text": "Moreover, DECBAE remains robust in low-resource and imbalanced settings.", "labels": [], "entities": [{"text": "DECBAE", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.6422131657600403}]}], "datasetContent": [{"text": "Figure 2 shows our approach of automatically collecting disambiguation dataset.", "labels": [], "entities": []}, {"text": "For each abstract, we first detect and extract the pattern of \"Definition (Abbreviation)\", e.g.: \"endoplasmic reticulum (ER)\".", "labels": [], "entities": [{"text": "Definition (Abbreviation)\"", "start_pos": 63, "end_pos": 89, "type": "METRIC", "confidence": 0.8490860313177109}]}, {"text": "Then we collect all the following sentences that contain the abbreviation, and label them with the definition.", "labels": [], "entities": []}, {"text": "This would generate a noisy label set due to the variations of writing the same definition (e.g.: emergency department and emergency departments).", "labels": [], "entities": []}, {"text": "To group the same definitions together, we use MetaMap-derived MeSH terms) as features of definitions and define the MeSH similarity between definition a and definition b as: where M a and Mb are the MeSH term sets of definition a and b, respectively.", "labels": [], "entities": [{"text": "MeSH similarity", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.5809304863214493}]}, {"text": "We group those definitions with high MeSH similarity and close edit distance by heuristic thresholds.", "labels": [], "entities": [{"text": "MeSH similarity", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.8208910822868347}]}, {"text": "However, due to the unsupervised nature of the collection process, some abbreviations are invalid or not ambiguous.", "labels": [], "entities": []}, {"text": "For this, one biomedical expert 3 filtered the abbreviations we found, based on 1) Validity: abbreviations should be biomedically meaningful; 2) Ambiguity: abbreviations should have multiple possible definitions, and prevalence of the dominant one should be < 99%.", "labels": [], "entities": [{"text": "prevalence", "start_pos": 217, "end_pos": 227, "type": "METRIC", "confidence": 0.9877413511276245}]}, {"text": "After the filtering, there are 950 valid ambiguous abbreviations.", "labels": [], "entities": []}, {"text": "Their statistics are shown in.", "labels": [], "entities": []}, {"text": "We split the instances of each abbreviation into training, development and test sets: If there is more than 10k instances, we randomly select 1k for both development and test sets.", "labels": [], "entities": []}, {"text": "Otherwise, we randomly select 10% of all instances for both development and test sets.", "labels": [], "entities": []}, {"text": "We model abbreviation expansion as a multi-label classification task, and use the following metrics to measure the performance of different models: Accuracy: Accuracy is defined as the proportion of right predictions in all predictions.", "labels": [], "entities": [{"text": "abbreviation expansion", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.9498973786830902}, {"text": "Accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9952229857444763}, {"text": "Accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9929894208908081}]}, {"text": "Most of the definition labels are imbalanced, so accuracy could be misleadingly high fora trivial majority solution in these cases, thus may not reflect the real capability of models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9992597699165344}]}, {"text": "Macro-F1: In multi-label classification, macro-F1 is calculated as an unweighted average of F1 score for each class.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7268615067005157}, {"text": "F1 score", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9824011027812958}]}, {"text": "Class-wise F1 score is defined as follows: where precision and recall are calculated for each class.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9807725548744202}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9997209906578064}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9995445609092712}]}, {"text": "Kappa Statistic: Cohen's kappa was originally introduced as a metric to measure inter-rater    agreement.", "labels": [], "entities": []}, {"text": "It can also be used to evaluate predictions of multi-label classification: where p o is the observed agreement and in the case of classification p o = accuracy, p e is the expected agreement which can be achieved by pure chance: p c and\u02c6pand\u02c6 and\u02c6p c refer to the proportion of class c in ground truth labels and predictions, respectively.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7162950932979584}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9990952014923096}]}, {"text": "Empirical results in show that Kappa statistics are often lower than accuracy and macro-F1, and thus serving as a more distinctive metric for our task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.999517560005188}]}], "tableCaptions": [{"text": " Table 1: Statistics of the automatically generated abbreviation disambiguation dataset and its subsets.", "labels": [], "entities": []}, {"text": " Table 2: Mean and standard deviation of model performance on different subsets.  \u2020 Significantly lower than the  corresponding metric of DECBAE. Significance is defined by p < 0.05 in paired t-test. All numbers are in  percentages. High deviations are expected due to the variety of abbreviations in each subset.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9855077862739563}, {"text": "DECBAE", "start_pos": 138, "end_pos": 144, "type": "DATASET", "confidence": 0.6995812654495239}]}]}