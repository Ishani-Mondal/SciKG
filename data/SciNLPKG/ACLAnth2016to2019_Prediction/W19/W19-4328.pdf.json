{"title": [{"text": "Effective Dimensionality Reduction for Word Embeddings", "labels": [], "entities": [{"text": "Effective Dimensionality Reduction", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6068669656912485}]}], "abstractContent": [{"text": "Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents.", "labels": [], "entities": []}, {"text": "Recently, there has been an emphasis on improving the pre-trained word vectors through post-processing algorithms.", "labels": [], "entities": []}, {"text": "One improvement area is reducing the dimensionality of word embed-dings.", "labels": [], "entities": []}, {"text": "Reducing the size of word embed-dings can improve their utility in memory constrained devices, benefiting several real-world applications.", "labels": [], "entities": []}, {"text": "In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions.", "labels": [], "entities": []}, {"text": "Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embed-dings.", "labels": [], "entities": []}, {"text": "To foster reproducibility, we have released the source code along with paper 1 .", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings such as Glove () and word2vec Skip-Gram () obtained from unlabeled text corpora can represent words in distributed dense realvalued low dimensional vectors which geometrically capture the semantic 'meaning' of a word.", "labels": [], "entities": []}, {"text": "These embeddings capture several linguistic regularities such as analogy relationships.", "labels": [], "entities": []}, {"text": "Such embeddings are of a pivotal role in several natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6228979031244913}]}, {"text": "Recently, there has been an emphasis on applying post-processing algorithms on the pre-trained word vectors to further improve their quality.", "labels": [], "entities": []}, {"text": "For example, algorithm in () tries 1 https://github.com/vyraun/Half-Size to inject antonymy and synonymy constraints into vector representations, while tries to refine word vectors by using relational information from semantic lexicons such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 244, "end_pos": 251, "type": "DATASET", "confidence": 0.9538198113441467}]}, {"text": "() tries to remove the biases (e.g. gender biases) present in word embeddings and ( tries to 'denoise' word embeddings by strengthening salient information and weakening noise.", "labels": [], "entities": []}, {"text": "In particular, the post-processing algorithm in ( tries to improve word embeddings by projecting the embeddings away from the most dominant directions and considerably improves their performance by making them more discriminative.", "labels": [], "entities": []}, {"text": "However, a major issue related with word embeddings is their size (), e.g., loading a word embedding matrix of 2.5 M tokens takes up to 6 GB memory (for 300-dimensional vectors, on a 64-bit system).", "labels": [], "entities": []}, {"text": "Such large memory requirements impose significant constraints on the practical use of word embeddings, especially on mobile devices where the available memory is often highly restricted.", "labels": [], "entities": []}, {"text": "In this work we combine the simple dimensionality reduction technique, PCA with the post processing technique of (, as discussed above.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6656439304351807}]}, {"text": "In Section 2, we first explain the post processing algorithm ( and then our novel algorithm and describe with an example the choices behind its design.", "labels": [], "entities": []}, {"text": "The evaluation results are presented in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we discuss the related works, followed by the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our proposed algorithm on standard word similarity benchmarks and across a range of downstream tasks.", "labels": [], "entities": []}, {"text": "For all our experiments, we used pre-trained Glove embeddings of dimensions 300, 200 and 100, trained on Wikipedia 2014 and Gigaword 5 corpus (400K vocabulary) ( and fastText embeddings of 300 dimensions trained on Wikipedia using the Skip-Gram model described in () (with 2.5M vocabu-lary) . The next subsection also presents results using word2vec embeddings trained on Google News dataset 6 .", "labels": [], "entities": [{"text": "Wikipedia 2014", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.9367087185382843}, {"text": "Gigaword 5 corpus", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.87932817141215}, {"text": "Google News dataset 6", "start_pos": 372, "end_pos": 393, "type": "DATASET", "confidence": 0.9237202703952789}]}], "tableCaptions": [{"text": " Table 1: Performance (Rho \u00d7 100) of Algo. 2 on different embedding and dimensions across multiple datasets.  Bold represent the best value in each column.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of performance (in terms of test accuracy) on several classification datasets with original  embeddings and the reduced embeddings obtained using the proposed algorithm. Bold represents the reduced  embeddings performance with is within <= 1% of the original 300D dimensional embeddings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9260443449020386}]}, {"text": " Table 4: Performance in terms of (Rho \u00d7 100) be- tween the predicted scores and the ground-truth scores  for STS tasks.", "labels": [], "entities": []}]}