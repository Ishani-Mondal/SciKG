{"title": [{"text": "Data mining Mandarin tone contour shapes", "labels": [], "entities": []}], "abstractContent": [{"text": "In spontaneous speech, Mandarin tones that belong to the same tone category may exhibit many different contour shapes.", "labels": [], "entities": []}, {"text": "We explore the use of data mining and NLP techniques for understanding the variability of tones in a large corpus of Mandarin newscast speech.", "labels": [], "entities": []}, {"text": "First, we adapt a graph-based approach to characterize the clusters (fuzzy types) of tone contour shapes observed in each tone n-gram category.", "labels": [], "entities": []}, {"text": "Second, we show correlations between these realized contour shape types and a bag of automatically extracted linguistic features.", "labels": [], "entities": []}, {"text": "We discuss the implications of the current study within the context of phonological and information theory.", "labels": [], "entities": [{"text": "information theory", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.710856705904007}]}], "introductionContent": [{"text": "One of the central phenomena of interest in lexical tone production is the deviation of their surface realizations from canonical templates of tone categories.", "labels": [], "entities": [{"text": "lexical tone production", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.636840283870697}]}, {"text": "Ina tone language, different tone categories differing in pitch movements can distinguish different lexical meanings of a syllable (e.g., in Mandarin, the syllable \"ma\" in a high level pitch contour means \"mother\", whereas the same syllable spoken in a falling pitch contour means \"to scold\").", "labels": [], "entities": []}, {"text": "Even though each tone category is defined with a general pitch contour profile (such as level, rising, falling, etc.), they typically exhibit great variability in spontaneous speech.", "labels": [], "entities": []}, {"text": "As an example, shows many different realizations of Mandarin tone 1, observed during speech production experiments in the lab.", "labels": [], "entities": []}, {"text": "Previous works in phonology, speech prosody, and tone recognition have investigated this variability by asking questions such as: (1) What factors contribute to the variability in tone production (Xu, 1997)?(2) How can we model the tone contour trajectory in synthesized speech (?", "labels": [], "entities": [{"text": "speech prosody", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7003220766782761}, {"text": "tone recognition", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.767733246088028}]}, {"text": "What features can we use to improve the accuracy of automatic tone recognition?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9973191618919373}, {"text": "automatic tone recognition", "start_pos": 52, "end_pos": 78, "type": "TASK", "confidence": 0.6315666139125824}]}, {"text": "Each of the works was driven by a particular set of theoretical or practical motivations and offered us a slice of understanding into the problem.", "labels": [], "entities": []}, {"text": "In this work, we are interested in looking at the tone variability problem from a data mining perspective: we explore the structure and distribution of tone contour shapes within a large amount of data.", "labels": [], "entities": []}, {"text": "By taking a data mining approach, we contrast our work with those works that focus on tone recognition or tone learning (either by machine or by human): we seek to extract tone patterns of empirical significance from a large data set of tones from spontaneous speech.", "labels": [], "entities": [{"text": "tone recognition", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7757225334644318}]}, {"text": "Working with the MCPST corpus (see Section 3) of Mandarin newscast speech (about 100,000 tones), we ask two questions: (1) For each tone category, what are the (coarse) types/classes of tone contour shapes we observe in this corpus?", "labels": [], "entities": [{"text": "MCPST corpus", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.9556052386760712}]}, {"text": "(2) For a particular tone category, what linguistic factors caused the same tone to be realized as these different types of shapes?", "labels": [], "entities": []}, {"text": "Inspired by works in natural language processing (NLP), we further extend these research questions in two directions.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.7634779711564382}]}, {"text": "First, we extend our investigation of tone categories into a series of n consecutive tones, or tone n-grams.", "labels": [], "entities": []}, {"text": "N -grams is a classic technique in NLP language modeling , whereas in the current context, we study tone ngrams due to the importance of context in tone variability (Xu, 1997): atone category maybe realized differently depending on their neighboring tones.", "labels": [], "entities": [{"text": "NLP language modeling", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8463128010431925}]}, {"text": "What can we learn from data mining tone contour shapes for tone unigrams, bigrams, and trigrams?", "labels": [], "entities": []}, {"text": "Second, to study prosody interface in MCPST data, we use automatic methods (NLP and other) to extract linguistic features from the text, including Named Entity Recognition (NER), Coreference resolution, Part-of-speech (POS) tagging, dependency parsing, and other phonological, morphological and contextual features.", "labels": [], "entities": [{"text": "MCPST data", "start_pos": 38, "end_pos": 48, "type": "DATASET", "confidence": 0.7437743246555328}, {"text": "Named Entity Recognition (NER)", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.7048837790886561}, {"text": "Coreference resolution", "start_pos": 179, "end_pos": 201, "type": "TASK", "confidence": 0.9122687876224518}, {"text": "Part-of-speech (POS) tagging", "start_pos": 203, "end_pos": 231, "type": "TASK", "confidence": 0.6637235045433044}, {"text": "dependency parsing", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.773786187171936}]}, {"text": "In order to find out the importance of these linguistic factors in shaping tone variability, we run the following experiment: given a particular tone (or tone n-gram) category, how well can we predict the type of tone contour shape it will take in running speech, using these linguistic features that exclude information about the pitch contour f 0 values?", "labels": [], "entities": []}, {"text": "Previous works showed that many linguistic factors (such as focus, topic, etc.) affect tone production or prosody (see Section 2) . In this work, we extend this to a more comprehensive set of linguistic features, motivated by the information theory account of tone production.", "labels": [], "entities": []}, {"text": "We hypothesize that there exists an information content inequality resulting from probability distribution of events in various linguistic domains (phonological, semantic, etc).", "labels": [], "entities": []}, {"text": "These inequalities affect speakers' speech production, resulting in gradient variants of tone contour shapes in a given tone category.", "labels": [], "entities": []}, {"text": "We investigate the relative importance of these factors in predicting the types of contour shapes any particular tone n-gram will take.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses relevant previous works.", "labels": [], "entities": []}, {"text": "Next we describe the data used in this paper in Section 3.", "labels": [], "entities": []}, {"text": "In order to characterize the types of contour shapes atone n-gram will take, we develop a method to derive clusters of tone contour shape types using network analysis (Section 4).", "labels": [], "entities": []}, {"text": "In Section 5, we discuss feature engineering and feature extraction from various linguistic domains (syntax, morphology, semantics, information structure, etc.).", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7978213429450989}, {"text": "feature extraction", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7622374296188354}]}, {"text": "Section 6 reports machine learning experiments and results on predicting tone contour shape types and the analysis on feature importance.", "labels": [], "entities": [{"text": "predicting tone contour shape types", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.8987598538398742}]}, {"text": "Finally, in Section 7 we discuss the implications of this work in the context of information theory and phonological theory of speech and tone production.", "labels": [], "entities": [{"text": "information theory", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7886506617069244}, {"text": "phonological theory of speech and tone production", "start_pos": 104, "end_pos": 153, "type": "TASK", "confidence": 0.6162900073187692}]}], "datasetContent": [{"text": "We have leveraged the intrinsic structure of the tone data to derive the tone contour shape types for Clustering coefficient (CC) measures the extent to which the nodes in a network tend to cluster together.", "labels": [], "entities": [{"text": "Clustering coefficient (CC)", "start_pos": 102, "end_pos": 129, "type": "METRIC", "confidence": 0.7985741794109344}]}, {"text": "Intuitively, it expresses how saturated the network is -how many of the possible connections are actually expressed.", "labels": [], "entities": []}, {"text": "The CC fora network of k nodes and n edges is computed as:  each tone n-gram category.", "labels": [], "entities": []}, {"text": "shows examples of learned clusters of tone contour shape types from two n-gram categories of tone unigram, bigram, and trigram, respectively.", "labels": [], "entities": []}, {"text": "Without declaring any cognitive or phonological significance of these clusters, these resulting clusters should reflect the similarities of tone contour shapes within any given tone n-gram category: those that are highly similar are grouped into the same cluster.", "labels": [], "entities": []}, {"text": "This is an intrinsic property derived from the above method, and is a necessary property sufficient for carrying out the subsequent experiment on predicting the tone contour shape types from linguistic factors.", "labels": [], "entities": []}, {"text": "Nonetheless, we propose two different ways to evaluate the validity of these clusters.", "labels": [], "entities": []}, {"text": "First, in the following experiments, we show that we are able to predict these learned tone contour shape types significantly better than randomly assigned clusters (Section 6.2,).", "labels": [], "entities": []}, {"text": "Second, we train a decision tree classifier to predict the shape type of a given tone n-gram using its f 0 vector and obtained a mean accuracy of 92% (following).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9077834486961365}]}, {"text": "This indicates how well these tone contour shape types can be predicted with complete information of its pitch trajectory, which will serve as an upper bound to our next prediction task using linguistic factors without information about pitch movements f 0 values.", "labels": [], "entities": []}, {"text": "For each of the 5 unigram, 16 bigram, and 54 trigram data sets, the extracted linguistic feature vector (f 1 , f 2 , ..., f m ) forms the input space X.", "labels": [], "entities": []}, {"text": "The contour shape types T forms the output space Y . Our goal is to learn a function h : X \u2192 Y minimizing expected loss.", "labels": [], "entities": []}, {"text": "We use SVM with linear kernel so that we can extract feature importance in subsequent analyses.", "labels": [], "entities": []}, {"text": "Each data set is randomly split into 90/10 for train and test.", "labels": [], "entities": []}, {"text": "Since the classes are balanced in Y , we evaluate the classifier performance directly with accuracy on the test sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9990200996398926}]}], "tableCaptions": []}