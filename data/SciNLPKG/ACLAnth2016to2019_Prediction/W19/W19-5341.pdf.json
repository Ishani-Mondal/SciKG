{"title": [{"text": "Baidu Neural Machine Translation Systems for WMT19", "labels": [], "entities": [{"text": "Baidu Neural Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7823591828346252}, {"text": "WMT19", "start_pos": 45, "end_pos": 50, "type": "TASK", "confidence": 0.7646520733833313}]}], "abstractContent": [{"text": "In this paper we introduce the systems Baidu submitted for the WMT19 shared task on Chinese\u2194English news translation.", "labels": [], "entities": [{"text": "WMT19 shared task", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.626610537370046}, {"text": "Chinese\u2194English news translation", "start_pos": 84, "end_pos": 116, "type": "TASK", "confidence": 0.49566231966018676}]}, {"text": "Our systems are based on the Transformer architecture with some effective improvements.", "labels": [], "entities": []}, {"text": "Data selection, back translation, data augmentation , knowledge distillation, domain adaptation , model ensemble and re-ranking are employed and proven effective in our experiments.", "labels": [], "entities": [{"text": "Data selection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7180885076522827}, {"text": "back translation", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7079674601554871}, {"text": "knowledge distillation", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.749446839094162}, {"text": "domain adaptation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.723939061164856}]}, {"text": "Our Chinese\u2192English system achieved the highest case-sensitive BLEU score among all constrained submissions, and our English\u2192Chinese system ranked the second in all submissions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9708426296710968}]}], "introductionContent": [{"text": "The Transformer model (, which exploits self-attention mechanism both in the encoder and decoder, has significantly improved the translation quality in recent years.", "labels": [], "entities": []}, {"text": "It is also adopted by most participants as the basic Neural Machine Translation (NMT) system in the previous translation campaigns (.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT)", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.7869024376074473}]}, {"text": "In this year's translation task, we focus on the improvement of single system, and propose three novel Transformer variants: \u2022 Pre-trained Transformer: We train a big Transformer language model () on monolingual corpora, and use the language model as the encoder of the Transformer model.", "labels": [], "entities": [{"text": "translation task", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.9073539674282074}]}, {"text": "\u2022 Deeper Transformer: We increase the encoder layers to better learn the representation of the source sentences.", "labels": [], "entities": []}, {"text": "Specifically, we increase the number of encoder layers from 6 to 30 for the base version, and from 6 to 15 layers for the big version.", "labels": [], "entities": []}, {"text": "\u2022 Bigger Transformer: According to the previous experiments, the performance of the Transformer model is largely dependent on the dimensions of feed forward network.", "labels": [], "entities": []}, {"text": "To further improve the performance, we increase the inner dimension of feed-forward network from 4,096 to 15,000 for big version.", "labels": [], "entities": []}, {"text": "In addition, we develop effective approaches to exploit additional monolingual data and generate augmented bilingual data.", "labels": [], "entities": []}, {"text": "To use the monolingual data, back translation () is employed on large corpora including News Corpus and Gigaword.", "labels": [], "entities": [{"text": "News Corpus", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9808457493782043}, {"text": "Gigaword", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.8630955815315247}]}, {"text": "We also use an iterative approach () to extend the back translation method by jointly training source-totarget and target-to-source NMT models.", "labels": [], "entities": [{"text": "back translation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.7200223505496979}]}, {"text": "For bilingual data augmentation, a target-to-source baseline system is used to translate the target of the bilingual corpus as the synthetic data.", "labels": [], "entities": [{"text": "bilingual data augmentation", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6613928377628326}]}, {"text": "Moreover, the sequence-level knowledge distillation) mechanism is employed to boost the performance by means of using the model decoding from right to left (Right-to-Left) and the aforementioned Transformer variants to generate synthetic data for training the NMT model ().", "labels": [], "entities": [{"text": "sequence-level knowledge distillation", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.539485385020574}]}, {"text": "The remainder of paper is structured as follows: Section 2 describes the detailed overview of our training strategy.", "labels": [], "entities": []}, {"text": "Section 3 shows the experimental settings and results.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in Section 4.", "labels": [], "entities": []}, {"text": "depicts the overall process of our submissions in this year's evaluation task, in which we train our advanced Transformer models on the bilingual corpus together with synthetic corpora, fine-tune them on the well-selected in-domain data, and generate the ensemble model for the final: Architecture of Baidu NMT system re-ranking strategy.", "labels": [], "entities": [{"text": "Baidu NMT system re-ranking", "start_pos": 301, "end_pos": 328, "type": "DATASET", "confidence": 0.8808948844671249}]}, {"text": "In this section, we will introduce each step in details.", "labels": [], "entities": []}], "datasetContent": [{"text": "All of our experiments are carried out on 32 machines with 8 NVIDIA V100 GPUs each of which have 32 GB memory.", "labels": [], "entities": []}, {"text": "For all models, we average the last 20 checkpoints to avoid overfitting.", "labels": [], "entities": []}, {"text": "We use cased BLEU scores calculated with Moses 2 mteval-v12a.pl script as evaluation metric.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9927803874015808}]}, {"text": "Following the organizers' suggestion, News dev 2018 is used as the development set and News test 2018 as the test set.", "labels": [], "entities": [{"text": "News dev 2018", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9398459196090698}, {"text": "News test 2018", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.9624481399854025}]}], "tableCaptions": [{"text": " Table 2: BLEU evaluation results on the WMT 2018 Chinese\u2192English test set (* denotes the submitted system).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977999329566956}, {"text": "WMT 2018 Chinese\u2192English test set", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.9177264060292926}]}, {"text": " Table 3: BLEU evaluation results on the WMT 2018 English\u2192Chinese test set (* denotes the submitted system).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975185394287109}, {"text": "WMT 2018 English\u2192Chinese test set", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.924041177545275}]}]}