{"title": [{"text": "Deep Learning for Identification of Adverse Effect Mentions in Twitter Data", "labels": [], "entities": [{"text": "Identification of Adverse Effect Mentions", "start_pos": 18, "end_pos": 59, "type": "TASK", "confidence": 0.7950376868247986}]}], "abstractContent": [{"text": "Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task challenges participants to accurately identify spans of text within a tweet that correspond to Adverse Effects (AEs) resulting from medication usage (Weissenbacher et al., 2019).", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) Adverse Effect Mentions Shared Task", "start_pos": 0, "end_pos": 87, "type": "TASK", "confidence": 0.6775979208094733}]}, {"text": "This task features a training data set of 2,367 tweets, in addition to a 1,000 tweet evaluation data set.", "labels": [], "entities": []}, {"text": "The solution presented here features a bidirectional Long Short-term Memory Network (bi-LSTM) for the generation of character-level embeddings.", "labels": [], "entities": []}, {"text": "It uses a second bi-LSTM trained on both character and token level embeddings to feed a Conditional Random Field (CRF) which provides the final classification.", "labels": [], "entities": [{"text": "Conditional Random Field (CRF)", "start_pos": 88, "end_pos": 118, "type": "METRIC", "confidence": 0.719435895482699}]}, {"text": "This paper further discusses the deep learning algorithms used in our solution.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We measured performance of the system based on provided gold label AEs.", "labels": [], "entities": [{"text": "AEs", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.7035134434700012}]}, {"text": "We used Precision, Recall, and F1 Score to monitor a model's performance as it trained and to check that the reported values were reflective of the model's ability to generalize to the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9990806579589844}, {"text": "Recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9672269225120544}, {"text": "F1 Score", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9855915904045105}]}, {"text": "Due to the inherently noisy nature of user generated social media text, we found that noise reduction techniques performed during the preprocessing stage had a much higher impact on model performance than hyperparameter tuning.", "labels": [], "entities": []}, {"text": "Swapping tokenizers netted performance increases in F1 Score as big as 9.73, when keeping special characters, and 8.07, when not.", "labels": [], "entities": [{"text": "F1 Score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9503247737884521}]}, {"text": "shows that best results on the test set are achieved with NLTK's Word Punct tokenizer and when special characters are kept.", "labels": [], "entities": [{"text": "NLTK", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9073444604873657}]}, {"text": "The shared task was evaluated using a total of six performance metrics including both strict and relaxed variants of Precision, Recall, and F1 Score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9970404505729675}, {"text": "Recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.8812277913093567}, {"text": "F1 Score", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9882071912288666}]}, {"text": "shows that our final system provided a 59.7 Relaxed F1 Score and a 40.7 Strict F1 Score on the evaluation set, beating shared task averages by 5.9 and 9.0, respectively.", "labels": [], "entities": [{"text": "Relaxed", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.9851784110069275}, {"text": "F1 Score", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.8797168731689453}, {"text": "Strict F1 Score", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.8525358835856119}]}, {"text": "Error analysis shows that words heavily associated with AEs, such as \"withdrawal\", are almost always accurately identified as being AEs.", "labels": [], "entities": [{"text": "AEs", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9153603911399841}]}, {"text": "Alternatively, words with neither positive nor negative connotations are frequently missed as being AEs, such as \"sleep\" in \"it could be two months before i sleep well again\".", "labels": [], "entities": [{"text": "AEs", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9746889472007751}]}, {"text": "Errors also occurred when tokens frequently associated with AEs were present but not in relation to medication usage.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9679309129714966}, {"text": "AEs", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9342604875564575}]}, {"text": "An example would be the identification of \"rejection hurts\" in \"rejection hurts, cymbalta can help\".", "labels": [], "entities": [{"text": "identification", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.9745928049087524}]}, {"text": "The model appears to give excessive weight to the specific word being used while not giving enough weight to the word's context.", "labels": [], "entities": []}, {"text": "Future work would explore the use of a larger corpus that includes more negative examples of those words, additional LSTM layers in the label prediction layer, and the use of more recent word embedding algorithms.", "labels": [], "entities": [{"text": "label prediction layer", "start_pos": 136, "end_pos": 158, "type": "TASK", "confidence": 0.786221961180369}]}], "tableCaptions": [{"text": " Table 1: System performance on test set with  different tokenizers.", "labels": [], "entities": []}]}