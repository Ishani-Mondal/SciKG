{"title": [{"text": "Analysing Representations of Memory Impairment in a Clinical Notes Classification Model", "labels": [], "entities": [{"text": "Analysing Representations of Memory Impairment", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8899739265441895}]}], "abstractContent": [{"text": "Despite recent advances in the application of deep neural networks to various kinds of medical data, extracting information from unstruc-tured textual sources remains a challenging task.", "labels": [], "entities": [{"text": "extracting information from unstruc-tured textual sources", "start_pos": 101, "end_pos": 158, "type": "TASK", "confidence": 0.8314513464768728}]}, {"text": "The challenges of training and interpreting document classification models are amplified when dealing with small and highly technical datasets, as are common in the clinical domain.", "labels": [], "entities": [{"text": "interpreting document classification models", "start_pos": 31, "end_pos": 74, "type": "TASK", "confidence": 0.9066303372383118}]}, {"text": "Using a dataset of de-identified clinical letters gathered at a memory clinic, we construct several recurrent neural network models for letter classification, and evaluate them on their ability to build meaningful representations of the documents and predict patients' diagnoses.", "labels": [], "entities": [{"text": "letter classification", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.7350995242595673}]}, {"text": "Additionally, we probe sentence embedding models in order to build a human-interpretable representation of the neural net-work's features, using a simple and intuitive technique based on perturbative approaches to sentence importance.", "labels": [], "entities": [{"text": "sentence importance", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.7450127601623535}]}, {"text": "In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses.", "labels": [], "entities": []}, {"text": "Furthermore , we identify clusters of sentences in the embedding space that correlate strongly with importance scores for each clinical diagnosis class.", "labels": [], "entities": []}], "introductionContent": [{"text": "While the majority of clinical data is made up of structured information, which can often be readily integrated into data models for research, there is a significant amount of semi-structured and unstructured data which is increasingly being targeted by machine learning practitioners for analysis.", "labels": [], "entities": []}, {"text": "As a general rule, this unstructured data is more difficult to analyse due to an absence of a standardised data model.", "labels": [], "entities": []}, {"text": "Unstructured clinical data includes a variety of media, such as video, audio, image and text-based data, with the majority of such data being made up of text and images.", "labels": [], "entities": []}, {"text": "Recently, there has been a series of breakthroughs in the application of machine learning techniques for medical imaging data in order to achieve expert-level performance on diagnosis tasks.", "labels": [], "entities": []}, {"text": "However, machine learning models using semi-structured and unstructured textual data from the clinical domain have received less attention and to date have not seen the same degree of successful application.", "labels": [], "entities": []}, {"text": "Examples of unstructured medical data featuring \"free text\" include discharge summaries, nursing reports and progress notes.", "labels": [], "entities": []}, {"text": "Historically, one of the challenges of applying natural language processing (NLP) methods to clinical data has been the often limited amount of data available, which has traditionally necessitated a reliance on manual feature engineering and relatively shallow textual features (.", "labels": [], "entities": []}, {"text": "Taking a novel dataset of labelled clinical letters compiled at a memory clinic as the target data domain, we build state-of-the-art deep learning models for the task of clinical text classification, and evaluate them on their ability to predict a clinician's diagnosis of the patient.", "labels": [], "entities": [{"text": "clinical text classification", "start_pos": 170, "end_pos": 198, "type": "TASK", "confidence": 0.637063721815745}]}, {"text": "However, deep learning models generally require very large training datasets.", "labels": [], "entities": []}, {"text": "Our approach to the problem therefore incorporates transfer learning, and we make use of embedding data from pre-trained models trained on large corpora.", "labels": [], "entities": []}, {"text": "In order to investigate the relative usefulness of word-level and sentencelevel information, we train and evaluate several models, including a ULMFiT model) and two long short-term memory (LSTM)) models: one trained on word embedding representations of the documents and one trained on sentence embedding representations ().", "labels": [], "entities": []}, {"text": "An infamous problem of deep neural networks is that they are \"black boxes\", with the details of how they represent and process information being uninterpretable to humans.", "labels": [], "entities": []}, {"text": "To shed light on how a recurrent neural network models clinical documents in order to correctly predict a patient's diagnosis, we investigate two complementary approaches to model interpretation.", "labels": [], "entities": [{"text": "predict a patient's diagnosis", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.6430382549762725}, {"text": "model interpretation", "start_pos": 174, "end_pos": 194, "type": "TASK", "confidence": 0.7326688766479492}]}, {"text": "Firstly, we develop a simple measure of sentence importance and demonstrate its effectiveness in interpreting a complex LSTM model's decision making process.", "labels": [], "entities": [{"text": "interpreting a complex LSTM model's decision making process", "start_pos": 97, "end_pos": 156, "type": "TASK", "confidence": 0.6132195658153958}]}, {"text": "Secondly, we discover clusters in the high-dimensional space of the sentence embedding model and test their correlation with feature importance scores fora given diagnosis class.", "labels": [], "entities": []}, {"text": "This analysis yields insights into a model's representation of the clinical notes, allowing us to automatically extract clusters of sentences that are most relevant to the model's predictions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We investigate the relative performance of LSTM models trained with a sequence of word embeddings, LSTM models trained with a sequence of sentence embeddings, and a state-of-the-art document classification model, ULMFiT.", "labels": [], "entities": [{"text": "document classification", "start_pos": 182, "end_pos": 205, "type": "TASK", "confidence": 0.7372563481330872}]}, {"text": "One motivation for choosing these experimental models is to investigate which models can capture long-term dependencies across a clinical document, given a relatively small amount of samples (n=106).", "labels": [], "entities": []}, {"text": "In addition to these three models, we also test a random forest baseline model, a model that randomly selects the class and a model that chooses the most common class (which is non-impaired).", "labels": [], "entities": []}, {"text": "The random forest model is trained to classify a document based on its bag-of-words representation.", "labels": [], "entities": []}, {"text": "All models are cross-validated using 5 folds of: Visualisation of sentence importance with respect to the successful classification of non-impaired fora subset of a document.", "labels": [], "entities": []}, {"text": "Sentences that were found to be important for the classification of non-impaired are coloured green while a sentence that increases the chance of a misclassification (i.e. an incorrect MCI diagnosis) is coloured red.", "labels": [], "entities": []}, {"text": "The saturation of the colours corresponds to how much a given word contributes to a sentence's InferSent embedding the dataset, ensuring that the class distribution is equal across all folds.", "labels": [], "entities": []}, {"text": "The ULMFiT model is pretrained on the Wikitext-103 dataset () and fine-tuned using default hyperparameters (fine-tuning epochs=25, fine-tuning batch size=8, fine-tuning learning rate=0.004, training epochs=50, training batch size=32, training learning rate=0.01) which have been shown to be robust across various tasks).", "labels": [], "entities": [{"text": "Wikitext-103 dataset", "start_pos": 38, "end_pos": 58, "type": "DATASET", "confidence": 0.9710915386676788}]}, {"text": "The LSTM model's hyperparameters were chosen by a grid-search.", "labels": [], "entities": []}, {"text": "Both the sentence embedding LSTM and the word embedding LSTM were made up of one hidden layer with 256 hidden units.", "labels": [], "entities": []}, {"text": "The classification results for the models for the masked dataset are presented in.", "labels": [], "entities": []}, {"text": "Each of our three models perform significantly better than chance and better than the random forest baseline model, with the LSTM model trained with sentence-embedding sequential input achieving the best performance.", "labels": [], "entities": []}, {"text": "For this amount of training data, we would expect models that are trained on shorter sequences of more semantically enriched pre-trained vectors (i.e. sentence embeddings) to perform better than much longer sequences of vectors with less dimensions (i.e. word embeddings).", "labels": [], "entities": []}, {"text": "This is because much of the work of combining word-level tokens into a contextual representation that is relevant to a statistical model of human language has already been done when training with pretrained representations extracted at the sentence-level.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, the model trained on sentence embeddings outperformed the fine-tuned ULMFiT.", "labels": [], "entities": []}, {"text": "Future work may shed light on how the amount of training samples can affect the choice of whether to use fine-tuning or pre-trained embedding representations as model input.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents and sentences in the  clinical notes dataset. D: Dementia, M: MCI, N: Non- impaired.", "labels": [], "entities": []}, {"text": " Table 2: Results (average over 5 folds) for the diagnosis classification task for the masked dataset. Precision, recall  and F1 score are macro-averaged across the classes.", "labels": [], "entities": [{"text": "diagnosis classification task", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.9350867867469788}, {"text": "Precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9989408850669861}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9976885318756104}, {"text": "F1 score", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9880950748920441}]}]}