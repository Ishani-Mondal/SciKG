{"title": [{"text": "Developing a Neural Machine Translation System for Irish", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.6252596775690714}]}], "abstractContent": [{"text": "In this paper we develop a neural machine translation (NMT) system for translating from English into Irish and vice versa.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.7892283499240875}, {"text": "translating from English into Irish", "start_pos": 71, "end_pos": 106, "type": "TASK", "confidence": 0.8259346604347229}]}, {"text": "We evaluate the performance of NMT on the resource-poor English-Irish (EN-GA) language pair, show that we can achieve good translation quality in comparison to previously reported systems, and outper-form Google Translate TM with several BLEU points on a domain-specific test set related to the legal domain.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 238, "end_pos": 242, "type": "METRIC", "confidence": 0.99873286485672}]}, {"text": "We show that back-translation of monolingual data closely related to the domain of the test set can further increase the model's performance.", "labels": [], "entities": []}, {"text": "Finally, we present a lightweight method for filtering synthetic sentence pairs obtained via back-translation using a tool for misalignment detection.", "labels": [], "entities": [{"text": "misalignment detection", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.8050382137298584}]}, {"text": "We show that our approach results in a slightly higher BLEU score while requiring less training data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9824425876140594}]}], "introductionContent": [{"text": "In recent years the performance of machine translation systems has been improving significantly thanks to the shift from statistical machine translation (SMT) to NMT.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.8081314265727997}, {"text": "statistical machine translation (SMT)", "start_pos": 121, "end_pos": 158, "type": "TASK", "confidence": 0.7795819838841757}]}, {"text": "Replacing the recurrent neural network (RNN) architecture with a Transformer architecture that relies entirely on self-attention to compute representations of its input and output has set anew state of the art in the field of machine translation ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 226, "end_pos": 245, "type": "TASK", "confidence": 0.8140209019184113}]}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND.", "labels": [], "entities": [{"text": "CCBY-ND", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9859224557876587}]}, {"text": "However, for low-resource languages, the performance of (neural) machine translation systems can still be disappointing, as pointed out for instance by.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7981469035148621}]}, {"text": "Many approaches have been suggested to improve the quality of NMT in such a low-resource setting, among which multilingual models (), unsupervised approaches () and systems relying on back-translation () have been the most successful.", "labels": [], "entities": []}, {"text": "In this paper we focus on the translation of the English-Irish language pair using NMT.", "labels": [], "entities": [{"text": "translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9796475172042847}, {"text": "NMT", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.9102156162261963}]}, {"text": "The Irish language has been categorized as a 'weak or not supported language' by the META-NET report) due to the lack of good translation resources.", "labels": [], "entities": [{"text": "META-NET report", "start_pos": 85, "end_pos": 100, "type": "DATASET", "confidence": 0.9604105055332184}]}, {"text": "Despite this relatively low availability of resources, both in terms of monolingual and bilingual content, it has been shown that an SMT system can achieve promising translation quality both in a domain-specific setting () and in a more broaddomain context.", "labels": [], "entities": [{"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9904362559318542}]}, {"text": "First steps were taken by to apply NMT methods to EN-GA MT, although the resulting NMT system performed significantly worse than SMT, scoring more than 6 BLEU lower on an in-domain test set.", "labels": [], "entities": [{"text": "EN-GA MT", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.6299229860305786}, {"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.9653710126876831}, {"text": "BLEU", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.999076247215271}]}, {"text": "1 In this work we will further explore the potential of NMT for the EN-GA language pair.", "labels": [], "entities": []}, {"text": "We add web-crawled parallel data to the publicly available resources used in previous studies and show relatively good translation quality both on a domain-specific test set and on a more generic test set.", "labels": [], "entities": []}, {"text": "Next, our experiments confirm that NMT translation quality for GA\u2192EN can be significantly improved using back-translation.", "labels": [], "entities": [{"text": "NMT translation", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7401957213878632}]}, {"text": "Due to alack of Irish monolingual data, backtranslation was less useful for EN\u2192GA NMT.", "labels": [], "entities": [{"text": "Irish monolingual data", "start_pos": 16, "end_pos": 38, "type": "DATASET", "confidence": 0.8147693276405334}]}, {"text": "Finally, a set of experiments was performed in which the synthetic parallel corpus, obtained via back-translation, was filtered with Bicleaner 2 (), a tool for misalignment detection.", "labels": [], "entities": [{"text": "Bicleaner", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.939245879650116}, {"text": "misalignment detection", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.87355837225914}]}, {"text": "We show that applying misalignment detection on a synthetic corpus before adding it to the parallel training data results in small increases in BLEU score and could be a useful strategy in terms of data selection.", "labels": [], "entities": [{"text": "misalignment detection", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7915408611297607}, {"text": "BLEU score", "start_pos": 144, "end_pos": 154, "type": "METRIC", "confidence": 0.982217937707901}]}, {"text": "Filtering of parallel data has been the subject of various studies (), but such data selection methods have only been scarcely investigated in the context of back-translation.", "labels": [], "entities": []}, {"text": "suggest several sampling strategies for synthetic data obtained via back-translation, targeting difficult to predict words.", "labels": [], "entities": []}, {"text": "More closely related to our filtering technique is the method proposed by.", "labels": [], "entities": []}, {"text": "They present a method in which a synthetic corpus was filtered by calculating the BLEU score between the target monolingual sentence and the translation of the synthetic source sentence in the target language and report small increases in translation quality in a lowresource setting.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9786504805088043}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 Parallel NMT training data (EN-GA)", "labels": [], "entities": []}, {"text": " Table 2 Data (EN|GA) for back-translation", "labels": [], "entities": []}, {"text": " Table 1. The RNN  architecture was chosen because of higher  inference speed compared to the Transformer  architecture (", "labels": [], "entities": []}, {"text": " Table 3 BLEU scores of our NMT systems for different test sets and types of training data", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9991821646690369}]}, {"text": " Table 4 BLEU scores for EN\u2192GA, given various Bicleaner thresholds for filtering synthetic data", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9991759657859802}]}, {"text": " Table 5 BLEU scores GA\u2192EN, various amounts of synthetic data and Bicleaner thresholds", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9990999698638916}, {"text": "Bicleaner thresholds", "start_pos": 66, "end_pos": 86, "type": "METRIC", "confidence": 0.942590206861496}]}]}