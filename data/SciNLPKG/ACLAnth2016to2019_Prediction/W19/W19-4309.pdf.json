{"title": [{"text": "Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel model architecture and training algorithm to learn bilingual sentence embeddings from a combination of parallel and monolingual data.", "labels": [], "entities": []}, {"text": "Our method connects autoencoding and neural machine translation to force the source and target sentence embed-dings to share the same space without the help of a pivot language or an additional transformation.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.8020580013593038}]}, {"text": "We train a multilayer perceptron on top of the sentence embeddings to extract good bilingual sentence pairs from nonparallel or noisy parallel data.", "labels": [], "entities": []}, {"text": "Our approach shows promising performance on sentence alignment recovery and the WMT 2018 parallel corpus filtering tasks with only a single model.", "labels": [], "entities": [{"text": "sentence alignment recovery", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8934778571128845}, {"text": "WMT 2018 parallel corpus filtering", "start_pos": 80, "end_pos": 114, "type": "TASK", "confidence": 0.6953411936759949}]}], "introductionContent": [{"text": "Data crawling is increasingly important in machine translation (MT), especially for neural network models.", "labels": [], "entities": [{"text": "Data crawling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7234167605638504}, {"text": "machine translation (MT)", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.8594163179397583}]}, {"text": "Without sufficient bilingual data, neural machine translation (NMT) fails to learn meaningful translation parameters.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 35, "end_pos": 67, "type": "TASK", "confidence": 0.820925255616506}]}, {"text": "Even for high-resource language pairs, it is common to augment the training data with web-crawled bilingual sentences to improve the translation performance (.", "labels": [], "entities": []}, {"text": "Using crawled data in MT typically involves two core steps: mining and filtering.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9764664173126221}]}, {"text": "Mining parallel sentences, i.e. aligning source and target sentences, is usually done with lots of heuristics and features: document/URL meta information, sentence lengths with selfinduced lexicon, word alignment statistics and linguistic tags (S . tef\u02d8 anescu et al.,;.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.6989255398511887}]}, {"text": "Filtering aligned sentence pairs also often involves heavy feature engineering).", "labels": [], "entities": []}, {"text": "Most of the participants in the WMT 2018 parallel corpus filtering task use large-scale neural MT models and language models as the features ( . Bilingual sentence embeddings can bean elegant and unified solution for parallel corpus mining and filtering.", "labels": [], "entities": [{"text": "WMT 2018 parallel corpus filtering task", "start_pos": 32, "end_pos": 71, "type": "TASK", "confidence": 0.7744078834851583}, {"text": "parallel corpus mining", "start_pos": 217, "end_pos": 239, "type": "TASK", "confidence": 0.6659885942935944}]}, {"text": "They compress the information of each sentence into a single vector, which lies in a shared space between source and target languages.", "labels": [], "entities": []}, {"text": "Scoring a source-target sentence pair is done by computing similarity between the source embedding vector and the target embedding vector.", "labels": [], "entities": [{"text": "Scoring a source-target sentence pair", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8487941026687622}]}, {"text": "It is much more efficient than scoring by decoding, e.g. with a translation model.", "labels": [], "entities": []}, {"text": "Bilingual sentence embeddings have been studied primarily for transfer learning of monolingual downstream tasks across languages (.", "labels": [], "entities": [{"text": "transfer learning of monolingual downstream tasks across languages", "start_pos": 62, "end_pos": 128, "type": "TASK", "confidence": 0.8632597848773003}]}, {"text": "However, few papers apply it to bilingual corpus mining; many of them require parallel training data with additional pivot languages or lack an investigation into similarity between the embeddings (.", "labels": [], "entities": [{"text": "bilingual corpus mining", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.6359162628650665}]}, {"text": "This work solves these issues as follows: \u2022 We propose a simple end-to-end training approach of bilingual sentence embeddings with parallel and monolingual data only of the corresponding language pair.", "labels": [], "entities": []}, {"text": "\u2022 We use a multilayer perceptron (MLP) as a trainable similarity measure to match source and target sentence embeddings.", "labels": [], "entities": []}, {"text": "\u2022 We compare various similarity measures for embeddings in terms of score distribution, geometric interpretation, and performance in downstream tasks.", "labels": [], "entities": [{"text": "geometric interpretation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.7559700608253479}]}, {"text": "\u2022 We demonstrate competitive performance in sentence alignment recovery and parallel cor-pus filtering tasks without a complex combination of translation/language models.", "labels": [], "entities": [{"text": "sentence alignment recovery", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8649006684621176}, {"text": "parallel cor-pus filtering tasks", "start_pos": 76, "end_pos": 108, "type": "TASK", "confidence": 0.731384202837944}]}, {"text": "\u2022 We analyze the effect of negative examples on training an MLP similarity, using different levels of negativity.", "labels": [], "entities": [{"text": "MLP similarity", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7153321802616119}]}], "datasetContent": [{"text": "We evaluated our bilingual sentence embedding and the MLP similarity on two tasks: sentence alignment recovery and parallel corpus filtering.", "labels": [], "entities": [{"text": "sentence alignment recovery", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.8564561605453491}, {"text": "parallel corpus filtering", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.6345136761665344}]}, {"text": "The sentence embedding was trained with WMT 2018 English-German parallel data and 100M German sentences from the News Crawl monolingual data 1 , where we use German as the autoencoded language.", "labels": [], "entities": [{"text": "WMT 2018 English-German parallel data", "start_pos": 40, "end_pos": 77, "type": "DATASET", "confidence": 0.9060274839401246}, {"text": "News Crawl monolingual data 1", "start_pos": 113, "end_pos": 142, "type": "DATASET", "confidence": 0.9455362558364868}]}, {"text": "All sentences were lowercased and limited to the length of 60.", "labels": [], "entities": []}, {"text": "We learned the byte pair encoding ( jointly for the two languages with 20k merge operations.", "labels": [], "entities": []}, {"text": "We pre-trained bilingual word embeddings on 100M sentences from the News Crawl data for each language using FASTTEXT) and MUSE (.", "labels": [], "entities": [{"text": "News Crawl data", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.9622503519058228}, {"text": "FASTTEXT", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8287479877471924}, {"text": "MUSE", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.935663104057312}]}, {"text": "Our sentence embedding model has 1-layer RNN encoder/decoder, where the word embedding and hidden layers have a size of 512.", "labels": [], "entities": []}, {"text": "The training was done with stochastic gradient descent with initial learning rate of 1.0, batch size of 120 sentences, and maximum 800k updates.", "labels": [], "entities": []}, {"text": "After 100k updates, we reduced the learning rate by a factor of 0.9 for every 50k updates.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.9742288291454315}]}, {"text": "Our MLP similarity model has 2 hidden layers of size 512 with ReLU (, trained with SCIKIT-LEARN (Pedregosa et al., 2011) with maximum 1,000 updates.", "labels": [], "entities": [{"text": "ReLU", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9904712438583374}]}, {"text": "For a positive training set, we used newstest2007-2015 from WMT (around 21k sentences).", "labels": [], "entities": [{"text": "WMT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.5773017406463623}]}, {"text": "Unless otherwise noted, we took a comparable size of negative examples from the worst-scored sentence pairs of ParaCrawl 2 English-German corpus.", "labels": [], "entities": [{"text": "ParaCrawl 2 English-German corpus", "start_pos": 111, "end_pos": 144, "type": "DATASET", "confidence": 0.8709593713283539}]}, {"text": "The scoring was done with our bilingual sentence embedding and cosine similarity.", "labels": [], "entities": []}, {"text": "Note that the negative examples are selected via cosine similarity but the similarity values are not used in the MLP training (Equation 15).", "labels": [], "entities": [{"text": "MLP", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.633632242679596}]}, {"text": "Thus it does not learn to mimic the cosine similarity function again, but has anew sorting of sentence pairs-also encoding the domain information.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sentence alignment recovery results. Our  method results use cosine similarity except the last row.", "labels": [], "entities": [{"text": "Sentence alignment recovery", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.9408121109008789}]}, {"text": " Table 2: Parallel corpus filtering results (German\u2192English).", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7111877600351969}]}, {"text": " Table 3: Sentence alignment recovery results with dif- ferent similarity measures (newstest2018).", "labels": [], "entities": [{"text": "Sentence alignment recovery", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.9033701022466024}, {"text": "newstest2018", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.9131894707679749}]}, {"text": " Table 4: Example sentence pairs in the ParaCrawl corpus (Section 4.2) with their similarity values.", "labels": [], "entities": [{"text": "ParaCrawl corpus", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8902596533298492}]}, {"text": " Table 5: Parallel corpus filtering results (10M-word  task) with different negative sets for training MLP sim- ilarity (newstest2016, i.e. the validation set).", "labels": [], "entities": [{"text": "Parallel corpus filtering", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6045229037602743}]}]}