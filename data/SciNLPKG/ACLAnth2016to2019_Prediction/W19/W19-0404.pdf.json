{"title": [{"text": "Re-Ranking Words to Improve Interpretability of Automatically Generated Topics", "labels": [], "entities": [{"text": "Improve Interpretability of Automatically Generated Topics", "start_pos": 20, "end_pos": 78, "type": "TASK", "confidence": 0.8574042419592539}]}], "abstractContent": [{"text": "Topics models, such as LDA, are widely used in Natural Language Processing.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.6220248838265737}]}, {"text": "Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models.", "labels": [], "entities": []}, {"text": "Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret.", "labels": [], "entities": []}, {"text": "This paper explores the re-ranking of topic words to generate more interpretable topic representations.", "labels": [], "entities": []}, {"text": "A range of approaches are compared and evaluated in two experiments.", "labels": [], "entities": []}, {"text": "The first uses crowdworkers to associate topics represented by different word rankings with related documents.", "labels": [], "entities": []}, {"text": "The second experiment is an automatic approach based on a document retrieval task applied on multiple domains.", "labels": [], "entities": [{"text": "document retrieval task", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.7525921861330668}]}, {"text": "Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus.", "labels": [], "entities": [{"text": "topic interpretability", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.6637254953384399}]}, {"text": "In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic modelling) is a widely used approach in Natural Language Processing ) with applications to areas such as enhancing exploratory search interfaces) and developing interpretable machine learning models.", "labels": [], "entities": [{"text": "Probabilistic topic modelling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6855762600898743}]}, {"text": "A topic model, e.g. Latent Dirichlet Allocation (LDA) () learns a low-dimensional representation of documents as a mixture of latent variables called topics.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.5646317104498545}]}, {"text": "Topics are multinomial distributions over a predefined vocabulary of words.", "labels": [], "entities": []}, {"text": "Traditionally, topics have been represented by lists of the topic's n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (.", "labels": [], "entities": []}, {"text": "Improving the interpretability of topic models is an important area of research.", "labels": [], "entities": [{"text": "interpretability of topic models", "start_pos": 14, "end_pos": 46, "type": "TASK", "confidence": 0.8305751234292984}]}, {"text": "A range of approaches have been developed including computing topic coherence), determining optimal topic cardinality (, labelling topics text and/or images () and corpus pre-processing ().", "labels": [], "entities": []}, {"text": "However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet.", "labels": [], "entities": [{"text": "topic interpretability", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6806787252426147}]}, {"text": "We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus.", "labels": [], "entities": []}, {"text": "Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible.", "labels": [], "entities": []}, {"text": "shows topics represented by the 30 most probable words.", "labels": [], "entities": []}, {"text": "Words displayed in bold font are more general (less informative, e.g. with high document frequency) while the remaining words are more likely to represent: Examples of topics represented by 30 most probable words from New York Times.", "labels": [], "entities": [{"text": "New York Times", "start_pos": 218, "end_pos": 232, "type": "DATASET", "confidence": 0.7747875849405924}]}, {"text": "Less informative words are shown in bold.", "labels": [], "entities": []}, {"text": "For example in the second topic, relevant words (e.g. investment, fund) have been assigned with lower probability compared to less informative words (e.g. percent, million).", "labels": [], "entities": []}, {"text": "As a result, these words will not appear in the top 10 words.", "labels": [], "entities": []}, {"text": "This paper compares several word ranking methods and evaluate them using two approaches.", "labels": [], "entities": [{"text": "word ranking", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.7357847690582275}]}, {"text": "The first approach is based on a crowdsourcing task in which participants are provided with a document and a list of topics then asked to identify the correct one, i.e. the topic that is most closely associated with the document.", "labels": [], "entities": []}, {"text": "Topics are represented byword lists ranked using different methods.", "labels": [], "entities": []}, {"text": "The effectiveness of the re-ranking approaches is evaluated by computing the accuracy of the participants on identifying the correct topic.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9990149736404419}]}, {"text": "The second evaluation approach is based on an information retrieval (IR) task and does not rely on human judgements.", "labels": [], "entities": [{"text": "information retrieval (IR) task", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.8158297638098398}]}, {"text": "The re-ranked words are used to form a query and retrieve a set of documents from the collection.", "labels": [], "entities": []}, {"text": "The effectiveness of the word re-ranking is then evaluated in terms of how well it can retrieve documents in the collection related to the topic.", "labels": [], "entities": []}, {"text": "Results show that re-ranking topic words improves performance in both experiments.", "labels": [], "entities": []}, {"text": "The paper makes the following contributions.", "labels": [], "entities": []}, {"text": "It highlights the problem of re-ranking topic words and demonstrates that it can improve topic interpretability.", "labels": [], "entities": [{"text": "topic interpretability", "start_pos": 89, "end_pos": 111, "type": "TASK", "confidence": 0.7006889879703522}]}, {"text": "It introduces the first systematic evaluation of topic word re-ranking methods using two approaches: one based on crowdsourcing and another based on an IR task.", "labels": [], "entities": [{"text": "topic word re-ranking", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.722238560517629}]}, {"text": "The latter evaluation is an automated approach and does not rely on human judgments.", "labels": [], "entities": []}, {"text": "Experiments demonstrate strong agreement between the results produced by these approaches which indicates that the IR-based approach could be used as an automated evaluation method in future studies.", "labels": [], "entities": []}, {"text": "The paper also compares multiple approaches to word re-ranking and concludes that the most effective ones are those which combine information about the importance of words within topics and their relative frequency across the entire corpus.", "labels": [], "entities": [{"text": "word re-ranking", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7646019756793976}]}, {"text": "Code used in the experiments described in this paper can be downloaded from https://github.com/areejokaili/topic_reranking.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first experiment compares the effectiveness of different topic representations (i.e. word re-rankings) by asking humans to choose the correct topic fora given document.", "labels": [], "entities": []}, {"text": "We hypothesize that humans would be able to find the correct topic more easily when the representation is more interpretable.", "labels": [], "entities": []}, {"text": "We randomly sampled approximately 33,000 news articles from the New York Times included in the English GigaWord corpus fifth edition.", "labels": [], "entities": [{"text": "English GigaWord corpus fifth edition", "start_pos": 95, "end_pos": 132, "type": "DATASET", "confidence": 0.89752699136734}]}, {"text": "Documents were tokenized and stopwords removed.", "labels": [], "entities": []}, {"text": "Words occurring in fewer than five or more than half of the documents were also removed to control for rare and common words.", "labels": [], "entities": []}, {"text": "The size of the resulting vocabulary is approximately 52,000 words.", "labels": [], "entities": []}, {"text": "In this second experiment, we automate the evaluation of the different topic representations obtained by re-ranking the topic words.", "labels": [], "entities": []}, {"text": "The automated evaluation is based on an IR task in which the re-ranked topic words are used to form a query and retrieve documents relevant to the topic.", "labels": [], "entities": [{"text": "IR task", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.8581946492195129}]}, {"text": "The motivation behind this approach is that the most effective re-rankings are the ones that can retrieve documents related to the topic, while ineffective re-rankings will not be able to distinguish these from other documents in the collection.", "labels": [], "entities": []}, {"text": "This evaluation method does not rely on human judgments, unlike the crowdsourcing approach presented in the previous section.", "labels": [], "entities": []}, {"text": "The evaluation approach assumes that given a document collection in which each document is mapped to a label (or labels) indicating its topic.", "labels": [], "entities": []}, {"text": "We refer to these labels as gold standard topics (to distinguish them from the automatically generated topics created by the topic model).", "labels": [], "entities": []}, {"text": "First, a set of automatically generated topics are created by running a topic model over a document collection.", "labels": [], "entities": []}, {"text": "For each gold standard topic, a set of all documents labelled with that topic is created.", "labels": [], "entities": []}, {"text": "The document-topic distribution created by the topic model is then used to identify the most probable automatically generated topic within that set of documents.", "labels": [], "entities": []}, {"text": "This is achieved by summing the documenttopic distributions and choosing the automatically generated topic with the highest value.", "labels": [], "entities": []}, {"text": "A query is then created by selecting the re-ranked top n words from that automatically generated topic and use it to retrieve a set of documents from the collection.", "labels": [], "entities": []}, {"text": "The set of retrieved documents is then compared against the set of all documents labelled with the gold standard label.", "labels": [], "entities": []}, {"text": "Evaluation was carried out using datasets representing documents from a wide range of domains: news articles, scientific literature and online reviews.", "labels": [], "entities": []}, {"text": "Each of the datasets was indexed using Apache Lucene.", "labels": [], "entities": [{"text": "Apache Lucene", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.5727955549955368}]}, {"text": "The same preprocessing steps used in Experiment 1 were applied to the datasets and the statistics of the datasets are shown in.", "labels": [], "entities": []}, {"text": "For each dataset, LDA was used to generate topics and the number of topics for each dataset was set based on optimising for coherence which yielded 35 for NYT, 45 for MEDLINE and 35 for Amazon.", "labels": [], "entities": [{"text": "NYT", "start_pos": 155, "end_pos": 158, "type": "DATASET", "confidence": 0.9489450454711914}, {"text": "MEDLINE", "start_pos": 167, "end_pos": 174, "type": "DATASET", "confidence": 0.8599801063537598}]}, {"text": "The automatically generated topic that was most closely associated with each of the gold topics (i.e. 50 NYT topics, 50 MeSH topics and 50 AMZ topics) were identified by applying the process outlined above in Section 5.1.", "labels": [], "entities": []}, {"text": "The top 5, 10 and 20 words from this topic is used to form a query which is submitted to Lucene.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 89, "end_pos": 95, "type": "DATASET", "confidence": 0.8682698607444763}]}, {"text": "The BM25 retrieval model) was used to measure the similarity between the document to a given query.", "labels": [], "entities": [{"text": "BM25", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.780393660068512}]}, {"text": "The documents retrieved by applying these queries are compared against the entire set of documents labelled with the dataset topics (i.e. NYT topic, MeSH topics, or AMZ topics) by computing Mean Average Precision (MAP) 14 which is commonly used as a single metric to summarise IR system performance.", "labels": [], "entities": [{"text": "NYT topic, MeSH topics", "start_pos": 138, "end_pos": 160, "type": "DATASET", "confidence": 0.7320101678371429}, {"text": "Mean Average Precision (MAP) 14", "start_pos": 190, "end_pos": 221, "type": "METRIC", "confidence": 0.9696781124387469}, {"text": "IR", "start_pos": 277, "end_pos": 279, "type": "TASK", "confidence": 0.8210005760192871}]}], "tableCaptions": [{"text": " Table 3: Results of experiment comparing re-ranking methods in which crowdsourcing participants were  asked to associate topic representations with documents. Topics are represented with their top 5, 10 or  20 probable words.", "labels": [], "entities": []}, {"text": " Table 5: Results of experiment in which top 5, 10 and 20 ranked words are used to form query.", "labels": [], "entities": []}]}