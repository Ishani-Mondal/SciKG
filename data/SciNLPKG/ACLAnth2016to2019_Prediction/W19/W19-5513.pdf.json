{"title": [{"text": "AIG Investments.AI at the FinSBD Task: Sentence Boundary Detection through Sequence Labelling and BERT Fine-tuning", "labels": [], "entities": [{"text": "AIG Investments.AI", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9682807922363281}, {"text": "FinSBD Task", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9090518057346344}, {"text": "Sentence Boundary Detection", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.9144755999247233}, {"text": "BERT Fine-tuning", "start_pos": 98, "end_pos": 114, "type": "METRIC", "confidence": 0.8134270310401917}]}], "abstractContent": [{"text": "This paper describes the method that Investments AI at AIG (American International Group, Inc.) submitted to the FinSBD-2019 shared task (\"Sentence Boundary Detection (SBD) in PDF Noisy Text of the Financial Domain\") to extract meaningful, well-formed sentences from noisy unstructured financial text.", "labels": [], "entities": [{"text": "FinSBD-2019", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.9040093421936035}, {"text": "Sentence Boundary Detection (SBD) in PDF Noisy Text of the Financial Domain", "start_pos": 139, "end_pos": 214, "type": "TASK", "confidence": 0.7990058830806187}]}, {"text": "We approach sentence boundary detection as a sequence labelling task to recognise the start and end token boundaries of sentence(-like) constructs.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6711561878522238}]}, {"text": "We evaluated two neural architectures, namely 1) Bidirectional Long Short-Term Memory (BiLSTM) and 2) Bidirectional Encoder Representations from Transformers (BERT).", "labels": [], "entities": [{"text": "Bidirectional Long Short-Term Memory (BiLSTM)", "start_pos": 49, "end_pos": 94, "type": "METRIC", "confidence": 0.7121074157101768}, {"text": "Bidirectional Encoder Representations from Transformers (BERT", "start_pos": 102, "end_pos": 163, "type": "TASK", "confidence": 0.7039919963904789}]}, {"text": "Our extensive experiments on the official FinSBD-2019 datasets demonstrate that a fine-tuned BERT model with customised hyper-parameters (BERT-SBD) outperforms BiL-STM models in several evaluation metrics.", "labels": [], "entities": [{"text": "FinSBD-2019 datasets", "start_pos": 42, "end_pos": 62, "type": "DATASET", "confidence": 0.9872798323631287}, {"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9929903745651245}, {"text": "BERT-SBD", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9878420829772949}]}, {"text": "Our BERT-SBD submission ranked first on the English test set in terms of MEAN F1 score in the joint sentence-begin-and-end test condition.", "labels": [], "entities": [{"text": "BERT-SBD", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9552688002586365}, {"text": "English test set", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.8118899265925089}, {"text": "MEAN F1 score", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.8679102857907613}]}], "introductionContent": [{"text": "The sentence is one of the most prominent building blocks in practical NLP and formal linguistics alike.", "labels": [], "entities": [{"text": "formal linguistics", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.6847499459981918}]}, {"text": "Many, ultimately leaky, definitions for what a sentence is (not) can be found in both communities.", "labels": [], "entities": []}, {"text": "At the level of informal commonsense, a sentence is taken to represent a \"complete thought\" . In Halliday's functional-thematic interpretation, the sentence is a basic unit of information composed of a topic (theme) and a comment; and the highest graphological unit of punctuation which conventionally begins with an upper-case letter and ends with a full stop.", "labels": [], "entities": []}, {"text": "The sentence is conventionally the structurally highest construct informal syntax (lexicogrammar), typically a clause complex or minimally at least one main clause (a predicator with an internal subject complement)].", "labels": [], "entities": []}, {"text": "Many NLP applications and data sets 2 view sentences as arbitrarily truncated text snippets which are simply 'useful' in practical terms.", "labels": [], "entities": []}, {"text": "Regardless of how the sentence is defined formally, sentence boundary detection (SBD) (cf. sentence boundary disambiguation, sentence segmentation, sentence breaking, sentence chunking) is a foundational, critically important upstream step in many NLP applications and (sub)tasks, such as part-of-speech tagging, named entity recognition, dependency parsing, and semantic role labelling, to name a few.", "labels": [], "entities": [{"text": "sentence boundary detection (SBD)", "start_pos": 52, "end_pos": 85, "type": "TASK", "confidence": 0.7912697891394297}, {"text": "sentence boundary disambiguation", "start_pos": 91, "end_pos": 123, "type": "TASK", "confidence": 0.6641623477141062}, {"text": "sentence segmentation", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7077426910400391}, {"text": "sentence breaking", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.6983907669782639}, {"text": "sentence chunking)", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.7954019804795583}, {"text": "part-of-speech tagging", "start_pos": 289, "end_pos": 311, "type": "TASK", "confidence": 0.7052016854286194}, {"text": "named entity recognition", "start_pos": 313, "end_pos": 337, "type": "TASK", "confidence": 0.6276027659575144}, {"text": "dependency parsing", "start_pos": 339, "end_pos": 357, "type": "TASK", "confidence": 0.834538996219635}, {"text": "semantic role labelling", "start_pos": 363, "end_pos": 386, "type": "TASK", "confidence": 0.642451008160909}]}, {"text": "Sentence boundary detection attempts to determine the spans (bounds, begin/from-end/to token indices) of sentences and sentence-like constructs below paragraphs, sections, or other suprasentential structures.", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.863286018371582}]}, {"text": "Because incorrect sentence spans can propagate and generate noise (and undesirable complications) for downstream tasks, SBD plays a critical role in practical NLP applications.", "labels": [], "entities": [{"text": "SBD", "start_pos": 120, "end_pos": 123, "type": "TASK", "confidence": 0.9548377990722656}]}, {"text": "Despite its importance, SBD has received much less attention in the last few decades than some of the more popular subtasks and topics in NLP.", "labels": [], "entities": [{"text": "SBD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9531099796295166}]}, {"text": "On the one hand, (superficially) high baseline performance levels can be achieved by na\u00a8\u0131vena\u00a8\u0131ve lookup methods that capture obvious, frequent sentence-final punctuation characters such as in conjunction with elementary space and case heuristics.", "labels": [], "entities": []}, {"text": "Such baselines leave little room for further optimisation on traditional test sets derived from formal news(wire) sources.", "labels": [], "entities": []}, {"text": "On the other hand, the long tail of exceptions in SBD makes the task non-trivial and challenging: a good majority of potential sentence boundary markers exhibit graphemic (and deeper semantic) ambiguity, particularly the full stop (period) which occurs in abbreviations, initials, honorifics, ordinal numbers, email addresses, ellipses, and the like.", "labels": [], "entities": [{"text": "SBD", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.9567667841911316}]}, {"text": "Beyond traditional, well-formed, -edited, and -curated news data, the snowballing of noisy web and social media data since the late 1990s has made SBD much harder: when faced with unstructured user-generated content involving tweets, extremely complex graphemic devices (e.g. new emoji, abbreviations, and acronyms), mark-up, and (up to a point) machine-readable data, traditional (and most offthe-shelf) sentence breakers that were trained on \"bare\" ASCII data in the Penn Treebank (PTB) simply run out of steam.", "labels": [], "entities": [{"text": "SBD", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9653421640396118}, {"text": "sentence breakers", "start_pos": 405, "end_pos": 422, "type": "TASK", "confidence": 0.7901926934719086}, {"text": "Penn Treebank (PTB)", "start_pos": 469, "end_pos": 488, "type": "DATASET", "confidence": 0.9734917640686035}]}, {"text": "Canonical SBD approaches optimised for the canonical news genre en- counter many complications even in other formal domains such as the biomedical or legal ones.", "labels": [], "entities": [{"text": "SBD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9377064108848572}]}, {"text": "Financial documents which are replete with extremely complex sentences provide one of the most unforgiving but also rewarding application domains for any SBD method.", "labels": [], "entities": []}, {"text": "Addressing the lack of SBD research in financial NLP, the FinBSD-2019 shared task focused on the specific challenges that come with noisy financial texts, including impure data extracted and converted automatically from machine-readable formats (such as PDFs).", "labels": [], "entities": [{"text": "FinBSD-2019", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9416694641113281}]}, {"text": "The main task was to detect the spans (begin(ning)/from vs. end(ing)/to token boundaries) of \"well-formed sentences\" in financial prospectuses -official PDF documents 3 published by investment funds to describe their products to clients.", "labels": [], "entities": []}, {"text": "Datasets for the shared task were released in two languages, English and French.", "labels": [], "entities": []}, {"text": "We participated only in the English track.", "labels": [], "entities": [{"text": "English track", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.893432229757309}]}, {"text": "Our system, which relies on state-of-the-art neural models and fine-tuning techniques, approached the FinBSD-2019 challenge as a generic sequence labelling task.", "labels": [], "entities": [{"text": "FinBSD-2019", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9171244502067566}, {"text": "generic sequence labelling task", "start_pos": 129, "end_pos": 160, "type": "TASK", "confidence": 0.6421495825052261}]}, {"text": "According to the organisers' automatic evaluation metrics, we reached the highest MEAN F1 score in the English subtask with a 1-point margin over the second-best submission.", "labels": [], "entities": [{"text": "MEAN F1 score", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.8526193300882975}, {"text": "English subtask", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.9073489606380463}]}], "datasetContent": [{"text": "We built two neural systems using the official training data, and tuned their parameters on the validation set.", "labels": [], "entities": []}, {"text": "This section summarises the data we used and the steps we took to build, fine-tune, test, and evaluate our systems.", "labels": [], "entities": []}, {"text": "Because the beginning (BS) and ending (ES) tokens 9 of sentences are evaluated separately, the official FinSBD-2019 evaluation metrics include 1) F1 scores for predicting BS and ES tokens separately as well as 2) the mean of two separate F1 scores.", "labels": [], "entities": [{"text": "FinSBD-2019", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.9045910835266113}, {"text": "F1", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9994162321090698}, {"text": "F1", "start_pos": 238, "end_pos": 240, "type": "METRIC", "confidence": 0.9951266050338745}]}, {"text": "We refer to the latter as a soft (lenient) evaluation metric.", "labels": [], "entities": []}, {"text": "During training and validation, we used standard evaluation metrics -Precision (P), Recall (R), and F1 score -to evaluate BS and ES.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.9577958732843399}, {"text": "Recall (R)", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9675574898719788}, {"text": "F1 score -", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9816850423812866}, {"text": "BS", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9071639180183411}, {"text": "ES", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.5882571935653687}]}, {"text": "To extract well-formed sentences, both beginning and ending tokens need to be predicted accurately.", "labels": [], "entities": []}, {"text": "We accordingly propose an additional harsh evaluation metric -PairSE -based on the use of P, R, and F1 in information retrieval.", "labels": [], "entities": [{"text": "PairSE", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.8922982215881348}, {"text": "F1", "start_pos": 100, "end_pos": 102, "type": "METRIC", "confidence": 0.9965422749519348}, {"text": "information retrieval", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.8102211952209473}]}, {"text": "In, we refer to our systems with different settings regarding the maximum input length.", "labels": [], "entities": []}, {"text": "BiLSTM1 stands for the BiLSTM-CRF system in which the input length is limited to 100.", "labels": [], "entities": [{"text": "BiLSTM1", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7249361872673035}]}, {"text": "The input sequence length is set to 60 for BiLSTM2, which is also denoted as AIG2 in our submissions.", "labels": [], "entities": [{"text": "BiLSTM2", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.6814746856689453}]}, {"text": "BERT1 is the BERT-SBD system where the input length is constrained to 128 while BERT2 is set to 256 in terms of the input length (denoted as AIG1 in our submissions).", "labels": [], "entities": [{"text": "BERT1", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9242711067199707}, {"text": "BERT-SBD", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9288325309753418}, {"text": "BERT2", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9953095316886902}]}, {"text": "In addition, Avg represents the MEAN of scores for the corresponding Sand E.", "labels": [], "entities": [{"text": "Avg", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.995570719242096}, {"text": "MEAN", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9991145730018616}]}], "tableCaptions": [{"text": " Table 1: FinSBD-2019: summary statistics.", "labels": [], "entities": [{"text": "FinSBD-2019", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.9499886631965637}]}, {"text": " Table 2: Parameter configurations for our submission.", "labels": [], "entities": []}, {"text": " Table 3: Experimental results on the validation set.", "labels": [], "entities": []}]}