{"title": [{"text": "Composing a Picture Book by Automatic Story Understanding and Visualization", "labels": [], "entities": [{"text": "Story Understanding", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6961951106786728}]}], "abstractContent": [{"text": "Pictures can enrich storytelling experiences.", "labels": [], "entities": []}, {"text": "We propose a framework that can automatically compose a picture book by understanding story text and visualizing it with painting elements, i.e., characters and backgrounds.", "labels": [], "entities": []}, {"text": "For story understanding, we extract key information from a story on both sentence level and paragraph level, including characters, scenes and actions.", "labels": [], "entities": [{"text": "story understanding", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8897576630115509}]}, {"text": "These concepts are organized and visualized in away that depicts the development of a story.", "labels": [], "entities": []}, {"text": "We collect a set of Chinese stories for children and apply our approach to compose pictures for stories.", "labels": [], "entities": []}, {"text": "Extensive experiments are conducted towards story event extraction for visualization to demonstrate the effectiveness of our method.", "labels": [], "entities": [{"text": "story event extraction", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7465263406435648}]}], "introductionContent": [{"text": "A story is an ordered sequence of steps, each of which can contain words, images, visualizations, video, or any combination thereof.", "labels": [], "entities": []}, {"text": "There exist vast amounts of story materials on the Internet, while few of them include visual data.", "labels": [], "entities": []}, {"text": "Among the few presented to audience, some include illustrations to make the stories more vivid; others are converted to video forms such as cartoons and films, of which the production consumes a lot of time and human efforts.", "labels": [], "entities": []}, {"text": "Although visualized stories are difficult to generate, they are more comprehensible, memorable and attractive.", "labels": [], "entities": []}, {"text": "Thus, automatic story understanding and visualization has abroad application prospect in storytelling.", "labels": [], "entities": [{"text": "automatic story understanding", "start_pos": 6, "end_pos": 35, "type": "TASK", "confidence": 0.6084643205006918}]}, {"text": "As an initial study, we aim to analyze events of a story and visualize them by combining painting elements, i.e., characters and backgrounds.", "labels": [], "entities": []}, {"text": "Story understanding has been a challenging task in Natural Language Processing area fora longtime.", "labels": [], "entities": [{"text": "Story understanding", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8687862455844879}]}, {"text": "In order to understand a story, we need to tackle the problem of event extraction in a story.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.7315833270549774}]}, {"text": "A story usually consists of several plots, where characters appear and make actions.", "labels": [], "entities": []}, {"text": "We define event keywords of a story as: scene (where), character (who, to whom) and action (what).", "labels": [], "entities": []}, {"text": "We extract events from story on both sentence level and paragraph level, so as to make use of the information in each sentence and the context of the full story.", "labels": [], "entities": []}, {"text": "As for story visualization, the most challenging problem is stage directing.", "labels": [], "entities": [{"text": "story visualization", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.8323819637298584}, {"text": "stage directing", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.7534986436367035}]}, {"text": "We need to organize the events following certain spatial distribution rules.", "labels": [], "entities": []}, {"text": "Although literary devices might be used e.g. flashbacks, the order in a story plot roughly corresponds with time (.", "labels": [], "entities": []}, {"text": "We arrange the extracted events in a screen along the story timeline.", "labels": [], "entities": []}, {"text": "Positions of elements on the screen are determined according to both current and past events.", "labels": [], "entities": []}, {"text": "Finally, with audio track added, simple animations could be generated.", "labels": [], "entities": []}, {"text": "These simple animations are like storyboards, in which each image represents a major event that correspond to a sentence or a group of consecutive sentences in the story text.", "labels": [], "entities": []}, {"text": "Regarding storytelling, we need to first know our audiences, assess their level of domain knowledge and familiarity with visualization conventions (.", "labels": [], "entities": []}, {"text": "In this paper, our target is to understand and visualize Chinese stories for children.", "labels": [], "entities": []}, {"text": "We collect children's stories from the Internet.", "labels": [], "entities": []}, {"text": "(The sources are described in Section 7.1.)", "labels": [], "entities": []}, {"text": "Then, we extract events and prepare visualization materials and style for children.", "labels": [], "entities": []}, {"text": "The framework we proposed, however, has wide extensibility, since it does not depend on domain specific knowledge.", "labels": [], "entities": []}, {"text": "It could serve as an automatic picture book composition solution to other fields and target audience.", "labels": [], "entities": [{"text": "picture book composition", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6131934523582458}]}, {"text": "1) We propose an end-to-end framework to automatically generate a sequence of pictures that represent major events in a story text.", "labels": [], "entities": []}, {"text": "2) New formulation of story event extraction from sentence level to paragraph level to align the events in a temporal order.", "labels": [], "entities": [{"text": "story event extraction", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.6296015977859497}]}, {"text": "3) We propose using a neural encoder-decoder model to extract story events and present empirical results with significant improvements over the baseline.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: In Section 2 we introduce related work.", "labels": [], "entities": []}, {"text": "Then we formulate the problem and overview our proposed solution in Section 3.", "labels": [], "entities": []}, {"text": "Details of different modules are provided in Section 4, 5 and 6.", "labels": [], "entities": []}, {"text": "We describe our data and experiments in Section 7.", "labels": [], "entities": []}, {"text": "In Section 8 we make conclusion and present our future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Collection: We collect 3,680 Chinese stories for children from the Internet 1 . The stories include 47 sentences on average.", "labels": [], "entities": [{"text": "Data Collection", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7213919460773468}]}, {"text": "We randomly sample 10, 000 sentences from the stories and split them into three parts: training set (80%), testing set   (10%), and development set (10%).", "labels": [], "entities": []}, {"text": "We hired four experienced annotators to provide story events annotations.", "labels": [], "entities": []}, {"text": "For each sentence, the annotators select event keywords and give them a category label of scene, character, or action.", "labels": [], "entities": []}, {"text": "The words rather than event keywords are regarded as \"others\".", "labels": [], "entities": []}, {"text": "We present the statistics of the collected corpus in Table 2.", "labels": [], "entities": []}, {"text": "Each sentence in the training and development set was annotated by one annotator for the sake of saving cost.", "labels": [], "entities": []}, {"text": "But each sentence in the testing sets was annotated by three annotators independently.", "labels": [], "entities": []}, {"text": "We calculate Fleiss' Kappa () to evaluate the agreement among annotators.", "labels": [], "entities": [{"text": "Fleiss' Kappa ()", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.9411331415176392}]}, {"text": "For each token in a sentence, it is annotated as y(y \u2208 scene, character, action, others) by 3 annotators.", "labels": [], "entities": []}, {"text": "The Fleiss' Kappa value is 0.580, which shows that the annotations have moderate agreement.", "labels": [], "entities": [{"text": "Fleiss' Kappa value", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9024443030357361}]}, {"text": "For story visualization, we hire two designers to design elements for storytelling.", "labels": [], "entities": [{"text": "story visualization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9076481461524963}]}, {"text": "The elements include story scenes and characters (with different actions).", "labels": [], "entities": []}, {"text": "Each frame of an animation consists of several elements.", "labels": [], "entities": []}, {"text": "This mechanism is flexible for element switch and story plot development.", "labels": [], "entities": [{"text": "element switch", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.7630739808082581}, {"text": "story plot development", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7224776248137156}]}, {"text": "We prepared 30 scenes and 600 characters, which have high frequencies in the collected stories.", "labels": [], "entities": []}, {"text": "Some example animation elements are shown in.", "labels": [], "entities": []}, {"text": "Training Details: In the neural based methods, the word embedding size is 100.", "labels": [], "entities": []}, {"text": "The LSTM model contains 100 hidden units and trains with a learning rate of 0.001 and Adam) optimizer.", "labels": [], "entities": []}, {"text": "The batch size is set to 20 and 50% dropout is used to avoid overfitting.", "labels": [], "entities": []}, {"text": "We train the model for 100 epochs although it converges quickly.", "labels": [], "entities": []}, {"text": "We report the mean scores and conduct Tukey's HSD test.", "labels": [], "entities": [{"text": "Tukey's HSD test", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.7813611775636673}]}, {"text": "For scene extraction, the F1 score differences of all method pairs are statistically significant.", "labels": [], "entities": [{"text": "scene extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7730413973331451}, {"text": "F1 score differences", "start_pos": 26, "end_pos": 46, "type": "METRIC", "confidence": 0.9791693687438965}]}, {"text": "So are that on character extraction (except the difference between BiLSTM and IDCNN).", "labels": [], "entities": [{"text": "character extraction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.8422903716564178}, {"text": "BiLSTM", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.8336959481239319}]}, {"text": "For action extraction, only the difference between BERT and Parser is significant.", "labels": [], "entities": [{"text": "action extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8508310317993164}, {"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9978679418563843}, {"text": "Parser", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8923463225364685}]}, {"text": "We compare the neural based models with a baseline based on parser.", "labels": [], "entities": []}, {"text": "We first conduct word segmentation with Jieba (Sun, 2012) and part of speech (POS) annotation using Stanford CoreNLP Toolkit ().", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7262553870677948}, {"text": "Jieba (Sun, 2012)", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.8461633622646332}, {"text": "Stanford CoreNLP Toolkit", "start_pos": 100, "end_pos": 124, "type": "DATASET", "confidence": 0.9373964667320251}]}, {"text": "Then we use dependency parser to extract events.", "labels": [], "entities": []}, {"text": "For scene extraction, we find that most scenes in the childrens' stories are common places with few specific names or actions.", "labels": [], "entities": [{"text": "scene extraction", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7956489622592926}]}, {"text": "Thus, we construct a commonplace dictionary with 778 scene tokens.", "labels": [], "entities": []}, {"text": "We keep NP, NR, NT and NN ( of POS tagging results and filter the scene tokens according to the scene dictionary.", "labels": [], "entities": []}, {"text": "Dependency parser is employed to extract characters and actions.", "labels": [], "entities": [{"text": "Dependency parser", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7598431408405304}]}, {"text": "The subjects and objects in a sentence are denoted as the current story characters.", "labels": [], "entities": []}, {"text": "The predicates (usually in terms of verbs or verb phrases) in the dependency tree are considered to contain actions of the corresponding characters.", "labels": [], "entities": []}, {"text": "The mean evaluation results over the test sets are shown in.", "labels": [], "entities": []}, {"text": "The result shows that the BiLSTM-CRF method can achieve as high as 0.973 F 1 score in scene extraction.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9870051741600037}, {"text": "scene extraction", "start_pos": 86, "end_pos": 102, "type": "TASK", "confidence": 0.7503651082515717}]}, {"text": "The BERTBiLSTM-CRF method can achieve 0.843 F 1 score in character extraction, which is high too.", "labels": [], "entities": [{"text": "BERTBiLSTM-CRF", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9899415373802185}, {"text": "F 1 score", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9829297463099161}, {"text": "character extraction", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.8767800331115723}]}, {"text": "But action extraction is the most difficult.", "labels": [], "entities": [{"text": "action extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8346899151802063}]}, {"text": "Even   the best method BERT-BiLSTM-CRF can achieve 0.499 F 1 score only, which is too low to use.", "labels": [], "entities": [{"text": "BERT-BiLSTM-CRF", "start_pos": 23, "end_pos": 38, "type": "METRIC", "confidence": 0.9896001815795898}, {"text": "F 1 score", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9768915375073751}]}, {"text": "We conduct Tukey HSD significant test overall method pairs too.", "labels": [], "entities": [{"text": "Tukey HSD", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.7303756475448608}]}, {"text": "The results indicate that the neural methods are significantly better than the baseline based on parser in scene and character extraction.", "labels": [], "entities": [{"text": "scene and character extraction", "start_pos": 107, "end_pos": 137, "type": "TASK", "confidence": 0.6113281548023224}]}, {"text": "BERT-BiLSTM-CRF also significantly beats the parser baseline inaction extraction.", "labels": [], "entities": [{"text": "BERT-BiLSTM-CRF", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.974147617816925}, {"text": "parser baseline inaction extraction", "start_pos": 45, "end_pos": 80, "type": "TASK", "confidence": 0.804616317152977}]}, {"text": "Among three neural methods, BERT brings significant improvements over the BiLSTM-CRF method in scene and character extraction.", "labels": [], "entities": [{"text": "BERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9921962022781372}, {"text": "scene and character extraction", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.5891928300261497}]}, {"text": "Only in scene extraction, BiLSTM-CRF is the best and the differences are significant.", "labels": [], "entities": [{"text": "scene extraction", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.7765275835990906}, {"text": "BiLSTM-CRF", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.8769500851631165}]}, {"text": "illustrates sample event extraction results.", "labels": [], "entities": [{"text": "sample event extraction", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.6853969792524973}]}, {"text": "We can find that most of the story events are correctly extracted while there still exist a lot of biases.", "labels": [], "entities": []}, {"text": "For example, some detected events do not actually happen in real but merely appear in the imagination or dialogues.", "labels": [], "entities": []}, {"text": "(e.g. In verb phrase \"heard a deafening help\", the action is \"heard\", not \"deafening\".)", "labels": [], "entities": []}, {"text": "Some serves as an adjective that modifies an character.", "labels": [], "entities": []}, {"text": "(e.g. In noun phrase \"mountain children\", \"mountain\" does not indicate the current scene, but the children's hometown.)", "labels": [], "entities": []}, {"text": "In this evaluation, we focus on event switch detection.", "labels": [], "entities": [{"text": "event switch detection", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7957942684491476}]}, {"text": "Take paragraph-level scene detection as an example.", "labels": [], "entities": [{"text": "paragraph-level scene detection", "start_pos": 5, "end_pos": 36, "type": "TASK", "confidence": 0.7739532589912415}]}, {"text": "The story in includes three scenes: field, river and garden, starting from the 1 st , the 8 th and the 14 th sentence respectively.", "labels": [], "entities": []}, {"text": "Paragraph-level event extraction is required to find the correct switch time and the event content.", "labels": [], "entities": [{"text": "Paragraph-level event extraction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.5949829816818237}]}, {"text": "We compare simple extension of sentence-level results and paragraph-level event integration results (denoted as base and ours in).", "labels": [], "entities": [{"text": "paragraph-level event integration", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.5578832725683848}]}, {"text": "We randomly selected 20 stories from the collected corpus and manually annotated the scene and character spans.", "labels": [], "entities": []}, {"text": "Scene keywords are mapped into 30 categories of painting scenes.", "labels": [], "entities": []}, {"text": "Sentencelevel scene results are extended in away where the first sentence including the keyword is regarded as the start of the scene span and the previous sentence of next scene is denoted as the span end.", "labels": [], "entities": []}, {"text": "For paragraph-level scene integration, scene spans are extended both in forward and backward orientation.", "labels": [], "entities": [{"text": "paragraph-level scene integration", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7430008252461752}]}, {"text": "Moreover, the dialogue contexts are ignored because the scene in a dialogue might not be the current one.", "labels": [], "entities": []}, {"text": "It might be imagination or merely action of the pastor the future.", "labels": [], "entities": []}, {"text": "Other event information is also utilized as supplement, as the characters keywords might indicate specific scenes.", "labels": [], "entities": []}, {"text": "We calculate precision, recall and F 1 value for event detection.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9998155236244202}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9997499585151672}, {"text": "F 1 value", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9849955439567566}, {"text": "event detection", "start_pos": 49, "end_pos": 64, "type": "TASK", "confidence": 0.752878874540329}]}, {"text": "A correct hit should detect both the event switch time and the right content.", "labels": [], "entities": []}, {"text": "The results are listed in.", "labels": [], "entities": []}, {"text": "As we can see, about 0.878% of scene switches are correctly detected.", "labels": [], "entities": []}, {"text": "After story scene switch information extracted, it is used in paragraph-level character detection.", "labels": [], "entities": [{"text": "paragraph-level character detection", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.7108943065007528}]}, {"text": "Character switch is defined as the appearance and disappearance of a single character.", "labels": [], "entities": [{"text": "Character switch", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.80112224817276}]}, {"text": "The first time when a character keyword is detected is denoted as the switch time of appearance.", "labels": [], "entities": []}, {"text": "Scene switch is used as an indication of disappearance of the characters in that scene.", "labels": [], "entities": []}, {"text": "Paragraph-level character detection reaches relatively higher accuracy than sentence-level character detection, with F 1 score of over 0.91.", "labels": [], "entities": [{"text": "Paragraph-level character detection", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7974734902381897}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9991905093193054}, {"text": "sentence-level character detection", "start_pos": 76, "end_pos": 110, "type": "TASK", "confidence": 0.6172242164611816}, {"text": "F 1 score", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9937215248743693}]}, {"text": "T-test results indicate that our improvements are statistically significant.", "labels": [], "entities": [{"text": "T-test", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9689054489135742}]}], "tableCaptions": [{"text": " Table 4: Sentence-level results comparison. (chara  is short for character. BiLSTM and BERT represent  BiLSTM-CRF and BERT-BiLSTM-CRF respectively.)", "labels": [], "entities": [{"text": "BERT", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9894387125968933}, {"text": "BERT-BiLSTM-CRF", "start_pos": 119, "end_pos": 134, "type": "METRIC", "confidence": 0.9750227332115173}]}, {"text": " Table 5: Case study of sentence-level event extraction results.(1:Annotation, 2:Detection)", "labels": [], "entities": [{"text": "sentence-level event extraction", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.5993515054384867}, {"text": "Annotation", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9331463575363159}]}, {"text": " Table 6: Paragraph-level event integration results.  (chara is short for character.)  \u2020 and denote our im- provements are significant in t-test with p \u2264 0.01 and  p \u2264 0.10 respectively.", "labels": [], "entities": [{"text": "Paragraph-level event integration", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.6977542837460836}]}]}