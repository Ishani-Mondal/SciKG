{"title": [{"text": "UDPipe at SIGMORPHON 2019: Contextualized Embeddings, Regularization with Morphological Categories, Corpora Merging", "labels": [], "entities": [{"text": "UDPipe at SIGMORPHON 2019", "start_pos": 0, "end_pos": 25, "type": "DATASET", "confidence": 0.8041041344404221}]}], "abstractContent": [{"text": "We present our contribution to the SIGMOR-PHON 2019 Shared Task: Crosslinguality and Context in Morphology, Task 2: contextual morphological analysis and lemmatization.", "labels": [], "entities": [{"text": "SIGMOR-PHON 2019 Shared Task", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.658960372209549}, {"text": "contextual morphological analysis", "start_pos": 116, "end_pos": 149, "type": "TASK", "confidence": 0.7142745057741801}]}, {"text": "We submitted a modification of the UDPipe 2.0, one of best-performing systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation.", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text", "start_pos": 109, "end_pos": 143, "type": "TASK", "confidence": 0.7578955948352813}]}, {"text": "As our first improvement, we use the pre-trained contextualized embeddings (BERT) as additional inputs to the network; secondly, we use individual morphological features as reg-ularization; and finally, we merge the selected corpora of the same language.", "labels": [], "entities": [{"text": "BERT", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9500828981399536}]}, {"text": "In the lemmatization task, our system exceeds all the submitted systems by a wide margin with lemmatization accuracy 95.78 (second best was 95.00, third 94.46).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.8323290348052979}]}, {"text": "In the morphological analysis, our system placed tightly second: our morphological analysis accuracy was 93.19, the winning system's 93.23.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.8831214010715485}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9510221481323242}]}], "introductionContent": [{"text": "This work describes our participant system in the SIGMORPHON 2019 Shared Task: Crosslinguality and Context in Morphology.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 Shared Task", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.5902133285999298}]}, {"text": "We contributed a system in Task 2: contextual morphological analysis and lemmatization.", "labels": [], "entities": [{"text": "contextual morphological analysis", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.7493790785471598}]}, {"text": "Given a segmented and tokenized text in a CoNLL-U format with surface forms (column 2) as in the following example: # sent-id = 1 # text = They buy and sell books.", "labels": [], "entities": []}, {"text": "the task is to infer lemmas (column 3) and morphological analysis) in the form of concatenated morphological features: # sent-id = 1 # text = They buy and sell books.", "labels": [], "entities": []}, {"text": "The SIGMORPHON 2019 data consists of 66 distinct languages in 107 corpora (.", "labels": [], "entities": [{"text": "SIGMORPHON 2019 data", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.6783838967482249}]}, {"text": "We submitted a modified UDPipe 2.0 (Straka, 2018), one of the three winning systems of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies ( and an overall winner of the The 2018 Shared Task on Extrinsic Parser Evaluation.", "labels": [], "entities": [{"text": "Multilingual Parsing from Raw Text", "start_pos": 115, "end_pos": 149, "type": "TASK", "confidence": 0.7585854887962341}]}, {"text": "Our improvements to the UDPipe 2.0 are threefold: \u2022 We use the pretrained contextualized embeddings (BERT) as additional inputs to the network (described in Section 3.3).", "labels": [], "entities": [{"text": "BERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.8980295658111572}]}, {"text": "\u2022 Apart from predicting the whole POS tag, we regularize the model by also predicting individual morphological features (Section 3.4).", "labels": [], "entities": []}, {"text": "\u2022 In some languages, we merge all the corpora of the same language (Section 3.5).", "labels": [], "entities": []}, {"text": "Our system placed first in lemmatization and closely second in morphological analysis.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7312541306018829}]}, {"text": "We give an overview of the related work in Section 2, we describe our methodology in Section 3, the results with ablation experiments are given in Section 4 and we conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The effect of pretrained word embeddings, BERT contextualized embeddings and regularization with morphological features is evaluated in.", "labels": [], "entities": [{"text": "BERT", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9118916392326355}]}, {"text": "Even the baseline model without any of the mentioned enhancements achieves relatively high performance and would place third in both lemmatization and tagging accuracy (when not considering our competition entry).", "labels": [], "entities": [{"text": "tagging", "start_pos": 151, "end_pos": 158, "type": "TASK", "confidence": 0.9531456232070923}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9594773650169373}]}, {"text": "Pretrained word embeddings improve the performance of both the lemmatizer and the tagger by a substantial margin.", "labels": [], "entities": []}, {"text": "For comparison with the embeddings we trained on CoNLL 2017 UD Shared Task plain texts, we also evaluate the embeddings provided by, which achieve only slightly lower performance than our embeddings -we presume the difference is caused mostly by different tokenization, given that the training data comes from Wikipedia and CommonCrawl in both cases.", "labels": [], "entities": [{"text": "CoNLL 2017 UD Shared Task plain texts", "start_pos": 49, "end_pos": 86, "type": "DATASET", "confidence": 0.9505771398544312}]}, {"text": "BERT contextualized embeddings further considerably improve POS tagging performance, and have minor impact on lemmatization improvement.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.932853639125824}, {"text": "POS tagging", "start_pos": 60, "end_pos": 71, "type": "TASK", "confidence": 0.8379553556442261}]}, {"text": "When used in isolation, the regularization with morphological categories provides quite considerable gain for both lemmatization and tagging, nearly comparable to the effect of adding precomputed word embeddings.", "labels": [], "entities": []}, {"text": "Combining all the enhancements together then produces a model with the highest performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 5 results in lemma accuracy, lemma Levenshtein, morphological accuracy and morphological F1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9811923503875732}, {"text": "Levenshtein", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.8034725189208984}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9509149789810181}, {"text": "F1", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9773650765419006}]}, {"text": " Table 3: Lemma accuracy, lemma Levenshtein, morphological accuracy, and morphological F1 results of ablation  experiments. For comparison, the FT only embeddings denote the pretrained embeddings of (Grave et al., 2018).", "labels": [], "entities": [{"text": "Lemma", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9882943630218506}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.8582024574279785}, {"text": "Levenshtein", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.742772102355957}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8872286081314087}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9431278109550476}, {"text": "FT", "start_pos": 144, "end_pos": 146, "type": "DATASET", "confidence": 0.5764549970626831}]}, {"text": " Table 4: Lemma accuracy, lemma Levenshtein, morphological accuracy, and morphological F1 results of model  combinations. When not specified otherwise, all models utilize pretrained word embeddings, BERT, and feature  regularization with weight w = 1.", "labels": [], "entities": [{"text": "Lemma", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9851763248443604}, {"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.7218524813652039}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7640608549118042}, {"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9191369414329529}, {"text": "BERT", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.9984412789344788}]}]}