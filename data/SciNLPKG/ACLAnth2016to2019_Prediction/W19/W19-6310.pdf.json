{"title": [{"text": "The Impact of Spelling Correction and Task Context on Short Answer Assessment for Intelligent Tutoring Systems", "labels": [], "entities": [{"text": "Short Answer Assessment", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.7770231564839681}]}], "abstractContent": [{"text": "This paper explores Short Answer Assessment (SAA) for the purpose of giving automatic meaning-oriented feedback in the context of a language tutoring system.", "labels": [], "entities": [{"text": "Short Answer Assessment (SAA)", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.6690118362506231}]}, {"text": "In order to investigate the performance of standard SAA approaches on student responses arising in real-life foreign language teaching, we experimented with two different factors: 1) the incorporation of spelling normalization in the form of a task-dependent noisy channel model spellchecker (Brill and Moore, 2000) and 2) training schemes, where we explored task-and item-based splits in addition to standard tenfold cross-validation.", "labels": [], "entities": [{"text": "SAA", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9583233594894409}, {"text": "spelling normalization", "start_pos": 204, "end_pos": 226, "type": "TASK", "confidence": 0.7762062549591064}]}, {"text": "For evaluation purposes, we compiled a data set of 3,829 student answers across different comprehension task types collected in a German school setting with the English tutoring system FeedBook (Rudzewitz et al., 2017; Ziai et al., 2018) and had an expert score the answers with respect to appropriateness (correct vs. incorrect).", "labels": [], "entities": []}, {"text": "Overall, results place the normalization-enhanced SAA system ahead of the standard version and a strong baseline derived from standard text similarity measures.", "labels": [], "entities": [{"text": "normalization-enhanced SAA", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.742395669221878}]}, {"text": "Additionally, we analyze task-specific SAA performance and outline where further research could make progress.", "labels": [], "entities": [{"text": "SAA", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9396142363548279}]}], "introductionContent": [{"text": "Short Answer Assessment (SAA) is the task of determining whether an answer to a question is corrector not with respect to meaning.", "labels": [], "entities": [{"text": "Short Answer Assessment (SAA)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7470307101806005}]}, {"text": "The task is also often called Automatic Short Answer Grading (ASAG) in cases where the outcome to determine is on an ordered scale (e.g., a numeric score).", "labels": [], "entities": [{"text": "Automatic Short Answer Grading (ASAG)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.6018230319023132}]}, {"text": "After a surge of attention (cf., e.g., including shared tasks at SemEval ( and Kaggle 1 , the field has quietened down somewhat, with a couple of recent exceptions (.", "labels": [], "entities": []}, {"text": "However, SAA cannot be considered a solved problem.", "labels": [], "entities": [{"text": "SAA", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9774358868598938}]}, {"text": "In particular, it is still unclear how well standard SAA approaches work in real-life educational contexts, for example when integrating language tutoring systems into a regular school setting.", "labels": [], "entities": [{"text": "SAA", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9711858630180359}]}, {"text": "In such systems, the goal is to give immediate feedback on the language produced by the learner, e.g., to help students complete homework exercises in the system step by step.", "labels": [], "entities": []}, {"text": "For meaning-oriented exercises, such as reading and listening comprehension, this is especially challenging, since the system needs to evaluate the meaning provided by the student response and possibly give helpful feedback on how to improve it in the direction of an acceptable answer.", "labels": [], "entities": []}, {"text": "SAA can help with the evaluation part: if an answer is deemed correct, the feedback is positive, if not, further diagnosis can be carried out.", "labels": [], "entities": [{"text": "SAA", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9318178296089172}]}, {"text": "The purpose of SAA in this context is thus to help the tutoring system decide whether the feedback to be given needs to be positive or negative.", "labels": [], "entities": [{"text": "SAA", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.7634391188621521}]}, {"text": "In this paper, we therefore report on SAA work in progress on authentic data from a language tutoring system for 7th grade English currently in use in German schools.", "labels": [], "entities": [{"text": "SAA", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.5995997786521912}]}, {"text": "We employ an alignment-based SAA system (CoMiC, shown to work well for several data sets where target answers are available (, and use it to train a classifier mimicking a trained language teacher's judgments on whether a student response is acceptable or not.", "labels": [], "entities": []}, {"text": "We investigate two main factors for SAA performance: 1) the impact of automatic spelling normalization on SAA using a noisy channel approach, and 2) the influence of using different training/test splits, namely 'unseen answers', 'unseen items' (questions), and 'unseen tasks', following.", "labels": [], "entities": [{"text": "SAA", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9880131483078003}, {"text": "spelling normalization", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.6712581217288971}, {"text": "SAA", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9418643116950989}]}, {"text": "Overall, results show that using spelling normalization yields superior performance for the SAA system we use, and that the performance gap widens when only using out-of-domain training data ('unseen tasks').", "labels": [], "entities": [{"text": "spelling normalization", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8192538321018219}]}, {"text": "We also conduct a by-task analysis of spelling and non-spelling variants of the SAA system, revealing that normalization effects are not uniform across tasks.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: Section 2 introduces the data source we use for our experiments before section 3 outlines the spelling correction approach.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.9076890647411346}]}, {"text": "Section 4 then delves into the setup and results of our experiments before section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the experiments we carried out, and the results obtained.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Data set properties by task", "labels": [], "entities": []}, {"text": " Table 2: Overall accuracy (%) and Cohen's \u03ba", "labels": [], "entities": [{"text": "Overall", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.955585777759552}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9644629955291748}]}, {"text": " Table 3: Unseen tasks accuracy (%) and \u03ba for  CoMiC with and without spelling correction", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9575780630111694}, {"text": "\u03ba", "start_pos": 40, "end_pos": 41, "type": "METRIC", "confidence": 0.9990172386169434}]}]}