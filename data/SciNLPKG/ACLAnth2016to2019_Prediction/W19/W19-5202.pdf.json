{"title": [{"text": "Improving Zero-shot Translation with Language-Independent Constraints", "labels": [], "entities": [{"text": "Improving Zero-shot Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8854082425435384}]}], "abstractContent": [{"text": "An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation.", "labels": [], "entities": [{"text": "training multilingual neural machine translation (NMT)", "start_pos": 24, "end_pos": 78, "type": "TASK", "confidence": 0.7260517366230488}, {"text": "zero-shot translation", "start_pos": 146, "end_pos": 167, "type": "TASK", "confidence": 0.7213843166828156}]}, {"text": "Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7552085518836975}]}, {"text": "In this work, we carried out an investigation on this capability of the multilingual NMT models.", "labels": [], "entities": []}, {"text": "First, we intentionally create an encoder architecture which is independent with respect to the source language.", "labels": [], "entities": []}, {"text": "Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general.", "labels": [], "entities": []}, {"text": "Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions.", "labels": [], "entities": []}, {"text": "We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset.", "labels": [], "entities": [{"text": "IWSLT 2017 multilingual dataset", "start_pos": 61, "end_pos": 92, "type": "DATASET", "confidence": 0.9303484857082367}]}, {"text": "We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9989309906959534}]}, {"text": "Additionally , we carryout further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) exploits neural networks to directly learn to transform sentences from a source language to a target language ().", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8398355642954508}]}, {"text": "Universal multilingual NMT discovered that a neural translation system can be trained on datasets containing source and target sentences in multiple languages (.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8332809805870056}]}, {"text": "Successfully trained models using this approach can be used to translate arbitrarily between any languages included in the training data.", "labels": [], "entities": []}, {"text": "In low-resource scenarios, multilingual NMT has proven to bean extremely useful regularization method since each language direction benefits from the information of the others).", "labels": [], "entities": []}, {"text": "An important research focus of multilingual NMT is zero-shot translation (ZS), or translation between languages included in multilingual data for which no directly parallel training data exists.", "labels": [], "entities": [{"text": "zero-shot translation (ZS)", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.8320153832435608}]}, {"text": "Application-wise, ZS offers a faster and more direct path between languages compared to pivot translation, which requires translation to one or many intermediate languages.", "labels": [], "entities": [{"text": "pivot translation", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7796967923641205}]}, {"text": "This can result in large latency and error propagation, common issues in non-end-to-end pipelines.From a representation learning point of view, there is evidence of NMT's ability to capture language-independent features, which have proved useful for crosslingual transfer learning ( and provide motivation for ZS translation.", "labels": [], "entities": [{"text": "crosslingual transfer learning", "start_pos": 250, "end_pos": 280, "type": "TASK", "confidence": 0.8161420822143555}, {"text": "ZS translation", "start_pos": 310, "end_pos": 324, "type": "TASK", "confidence": 0.8648038804531097}]}, {"text": "However it is still unclear if minimizing the difference in representations between languages is beneficial for zero-shot learning.", "labels": [], "entities": []}, {"text": "On the other hand, the current neural architecture and learning mechanisms of multilingual NMT is not geared towards having a common representation.", "labels": [], "entities": []}, {"text": "Different languages are likely to convey the same semantic content with sentences of different lengths, which makes the desiderata difficult to achieve.", "labels": [], "entities": []}, {"text": "Moreover, the loss function of the neural translation model does not favour having sentences encoded in the same representation space regardless of the source language.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7848992347717285}]}, {"text": "As a result, if the network capacity is large enough, it may partition itself into different sub-spaces for different language pairs.", "labels": [], "entities": []}, {"text": "Our work here focuses on the zero-shot translation aspect of universal multilingual NMT.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.5895115882158279}]}, {"text": "First, we attempt to investigate the relationship of encoder representation and ZS performance.", "labels": [], "entities": []}, {"text": "By modifying the Transformer architecture of to afford a fixed-size representation for the encoder output, we found that we can significantly improve zero-shot performance at the cost of a lower performance on the supervised language pairs.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first empirical evidence showing that the multilingual model can capture both language-independent and languagedependent features, and that the former can be prioritized during training.", "labels": [], "entities": []}, {"text": "This observation leads us to the most important contribution in this work, which is to propose several techniques to learn a joint semantic space for different languages in multilingual models without any architectural modification.", "labels": [], "entities": []}, {"text": "The key idea is to prefer a source language-independent representation in the decoder using an additional loss function.", "labels": [], "entities": []}, {"text": "As a result, the NMT architecture remains untouched and the technique is scalable to the number of languages in the training data.", "labels": [], "entities": []}, {"text": "The success of this method is shown by significant gains on zero-shot translation quality in the standard IWSLT 2017 multilingual benchmark).", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 60, "end_pos": 81, "type": "TASK", "confidence": 0.5505673587322235}, {"text": "IWSLT 2017 multilingual benchmark", "start_pos": 106, "end_pos": 139, "type": "DATASET", "confidence": 0.9139064103364944}]}, {"text": "Finally, we introduce a more challenging scenario that involves more than one bridge language between source and target languages.", "labels": [], "entities": []}, {"text": "This challenging setup confirms the consistency of our zero-shot techniques while clarifying the disadvantages of pivot-based translation.", "labels": [], "entities": [{"text": "pivot-based translation", "start_pos": 114, "end_pos": 137, "type": "TASK", "confidence": 0.6378801465034485}]}], "datasetContent": [{"text": "Our experiments use the standard IWSLT2017 benchmark in multilingual translation (, which established a standardized multilingual corpora in different languages {English, German, Dutch, Romanian and Italian}.", "labels": [], "entities": [{"text": "IWSLT2017 benchmark", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.8711746633052826}, {"text": "multilingual translation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7259581685066223}]}, {"text": "The data is around 60% true parallel, i.e. the same sentences translated in multiple languages (.", "labels": [], "entities": []}, {"text": "With the target of zero-shot translation in mind, we designed two different setups which challenge multilingual models but are also industrially practical.", "labels": [], "entities": [{"text": "zero-shot translation", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.7157363891601562}]}, {"text": "First, it is typical that English is the most commonly spoken language in the language set, leading the multilingual model to use English as the bridge language participating in all language pairs.", "labels": [], "entities": []}, {"text": "Our first setup therefore consists of English\u2190\u2192{German, Dutch, Italian, Romanian} language pairs, with 8 language pairs in total having supervision during training and the remaining 12 dedicated to the ZS setup.", "labels": [], "entities": []}, {"text": "It is notable that zero-shot (or zero-resource, if the method used generates artificial data to fill the language gap) setups which have been carried out in previous works were mostly concerned language connection with only one bridge (English).", "labels": [], "entities": []}, {"text": "However, more realistically, data between local languages or dialects (such as Indian or Vietnamese languages) may more abundant than English.", "labels": [], "entities": []}, {"text": "The connectivity in this case demands more than one language for bridging, which is simulated in our second setup by setting a \"Chain\" of languages.", "labels": [], "entities": []}, {"text": "This setup also contains 8 supervised languages and 12 for zero-shot.", "labels": [], "entities": []}, {"text": "shows the connections between languages in our setups.", "labels": [], "entities": []}, {"text": "The data is preprocessed using standard MT procedures including tokenization and truecasing and byte-pair encoding with 40K codes.", "labels": [], "entities": [{"text": "MT", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8923012614250183}]}, {"text": "For model selection, the checkpoints performing best on the validation data (dev2010 and tst2010 combined) are averaged, which is then used to translate the tst2017 test set (including all 20 lan- guage pairs).", "labels": [], "entities": [{"text": "model selection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6784543991088867}, {"text": "tst2017 test set", "start_pos": 157, "end_pos": 173, "type": "DATASET", "confidence": 0.6968095799287161}]}], "tableCaptions": [{"text": " Table 1: IWSLT 2017 STAR configuration: Baseline vs (Mean/Attention) Pooling. The top section shows 8  language pairs involved in training, while the bottom section shows the zero-shot results for 12 language pairs. We  also present results for this dataset from previous work for reference.", "labels": [], "entities": [{"text": "IWSLT 2017 STAR configuration", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.7621789127588272}]}, {"text": " Table 2: IWSLT 2017 STAR configuration result. Here we showed the Mean Pooling model that is enhanced with  MSE-Encoder, and the Attn-Pooling model with MSE-Decoder.", "labels": [], "entities": [{"text": "IWSLT 2017 STAR configuration", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.7304046601057053}, {"text": "Mean Pooling", "start_pos": 67, "end_pos": 79, "type": "TASK", "confidence": 0.6608414947986603}, {"text": "MSE-Encoder", "start_pos": 109, "end_pos": 120, "type": "DATASET", "confidence": 0.8384604454040527}, {"text": "MSE-Decoder", "start_pos": 154, "end_pos": 165, "type": "DATASET", "confidence": 0.8964257836341858}]}, {"text": " Table 3: IWSLT 2017 CHAIN configuration results (12 zero-shot directions).", "labels": [], "entities": [{"text": "IWSLT 2017 CHAIN configuration", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.6592642813920975}]}]}