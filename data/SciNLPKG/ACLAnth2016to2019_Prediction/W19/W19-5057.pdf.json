{"title": [{"text": "LasigeBioTM at MEDIQA 2019: Biomedical Question Answering using Bidirectional Transformers and Named Entity Recognition", "labels": [], "entities": [{"text": "Biomedical Question Answering", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.9125396013259888}, {"text": "Named Entity Recognition", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.6571561098098755}]}], "abstractContent": [{"text": "Biomedical Question Answering (QA) aims at providing automated answers to user questions , regarding a variety of biomedical topics.", "labels": [], "entities": [{"text": "Biomedical Question Answering (QA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7893473853667577}]}, {"text": "For example, these questions may ask for related to diseases, drugs, symptoms, or medical procedures.", "labels": [], "entities": []}, {"text": "Automated biomedical QA systems could improve the retrieval of information necessary to answer these questions.", "labels": [], "entities": []}, {"text": "The MEDIQA challenge consisted of three tasks concerning various aspects of biomedical QA.", "labels": [], "entities": [{"text": "MEDIQA", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.6127250790596008}, {"text": "biomedical QA", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.6737752258777618}]}, {"text": "This challenge aimed at advancing approaches to Natural Language Inference (NLI) and Recognizing Question Entailment (RQE), which would then result in enhanced approaches to biomedical QA.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.7597589989503225}, {"text": "Recognizing Question Entailment (RQE)", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.7227927496035894}, {"text": "biomedical QA", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.5705860704183578}]}, {"text": "Our approach explored a common Transformer-based architecture that could be applied to each task.", "labels": [], "entities": []}, {"text": "This approach shared the same pre-trained weights, but which were then fine-tuned for each task using the provided training data.", "labels": [], "entities": []}, {"text": "Furthermore, we augmented the training data with external datasets and enriched the question and answer texts using MER, a named entity recognition tool.", "labels": [], "entities": [{"text": "MER", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.7376357913017273}]}, {"text": "Our approach obtained high levels of accuracy, in particular on the NLI task, which classified pairs of text according to their relation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9992368221282959}]}, {"text": "For the QA task, we obtained higher Spearman's rank correlation values using the entities recognized by MER.", "labels": [], "entities": [{"text": "QA task", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.8736366033554077}, {"text": "Spearman's rank correlation", "start_pos": 36, "end_pos": 63, "type": "METRIC", "confidence": 0.7449833154678345}, {"text": "MER", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.7852899432182312}]}], "introductionContent": [{"text": "Question Answering (QA) is a text mining task for which several systems have been proposed).", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.895324981212616}, {"text": "text mining task", "start_pos": 29, "end_pos": 45, "type": "TASK", "confidence": 0.8222171465555826}]}, {"text": "This task is particularly challenging in the biomedical domain since this is a complex subject as answers may not be as straightforward compared to other domains.", "labels": [], "entities": []}, {"text": "However, clinical and healthcare information systems could benefit greatly from automated * alamurias@lasige.di.fc.ul.pt biomedical QA systems, which could improve the retrieval of information necessary to answer these questions.", "labels": [], "entities": []}, {"text": "To help progress on this topic, the MEDIQA challenge proposed three tasks in the biomedical domain): 1.", "labels": [], "entities": [{"text": "MEDIQA challenge", "start_pos": 36, "end_pos": 52, "type": "DATASET", "confidence": 0.7667863070964813}]}, {"text": "Natural Language Inference (NLI) -classify the relation between two sentences as either entailment, neutral or contradiction; 2.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7685827116171519}]}, {"text": "Recognizing Question Entailment (RQE) -classify if two questions are entailed with each other or not; 3.", "labels": [], "entities": [{"text": "Recognizing Question Entailment (RQE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7208050886789957}]}, {"text": "Question Answering (QA) -classify which answers are correct fora given answer and rank them.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8222029566764831}]}, {"text": "We applied the same approach to all three tasks since they all could be modelled as text classification tasks.", "labels": [], "entities": [{"text": "text classification tasks", "start_pos": 84, "end_pos": 109, "type": "TASK", "confidence": 0.8037826418876648}]}, {"text": "The objectives of the tasks were to classify pairs of text: sentence-sentence (NLI), question-question (RQE), and questionanswer (QA).", "labels": [], "entities": []}, {"text": "For the NLI task, we had three possible labels for each pair (entailment, neutral or contradiction), while the RQE task was a binary classification.", "labels": [], "entities": []}, {"text": "For the QA task, each pair should be given a reference score representing how well the question is answered, which ranged between 1 and 4.", "labels": [], "entities": [{"text": "QA task", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9095965027809143}]}, {"text": "QA is a complex task that involves various components, and can be approached in several ways.", "labels": [], "entities": [{"text": "QA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8582791686058044}]}, {"text": "While real-world scenarios require the retrieval of correct answers from larger databases, the QA task of this challenge simplified this problem by providing up to 10 answers retrieved by the medical QA system CHiQA.", "labels": [], "entities": []}, {"text": "This system also provided a ranking to each answer, however, we observed that this ranking did not follow the manual ranking inmost cases.", "labels": [], "entities": []}, {"text": "We also observed that the retrieved answers could consist of one or more sentences.", "labels": [], "entities": []}, {"text": "While in some QA scenarios, systems are required to select the text span that contains the answer, in this case it was only requested to re-rank the retrieved answers and classify which ones were correct.", "labels": [], "entities": []}, {"text": "Although specific ranking algorithms exist (), due to the nature of the task and the fact that the other two tasks involved comparison of text, we decided to train a classifier that compared each question with a potential answer, i.e., we predicted how good a text is at answering a given question.", "labels": [], "entities": []}, {"text": "Our approach uses pre-trained weights as a starting point, to fine-tune deep learning models based on the Transformer architecture for each of the challenge tasks (.", "labels": [], "entities": []}, {"text": "We used the BioBERT weights, trained on PubMed abstracts and PMC full articles, as the type of text should be more similar to the challenge data than the standard BERT models, which were trained on Wikipedia and BookCorpus.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9567574262619019}, {"text": "BERT", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.9815164804458618}, {"text": "BookCorpus", "start_pos": 212, "end_pos": 222, "type": "DATASET", "confidence": 0.930886447429657}]}, {"text": "Furthermore, we incorporated other datasets into the RQE and QA tasks, and enrich the training data with semantic information obtained using MER (Minimal Named-Entity Recognizer) (, a high computing performance named entity recognition tool.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracy obtained on the RQE task.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990053772926331}, {"text": "RQE task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.7327011525630951}]}]}