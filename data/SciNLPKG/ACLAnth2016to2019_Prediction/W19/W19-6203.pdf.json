{"title": [{"text": "To Lemmatize or Not to Lemmatize: How Word Normalisation Affects ELMo Performance in Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Normalisation Affects ELMo Performance in Word Sense Disambiguation", "start_pos": 38, "end_pos": 110, "type": "TASK", "confidence": 0.7223867177963257}]}], "abstractContent": [{"text": "In this paper, we critically evaluate the widespread assumption that deep learning NLP models do not require lemmatized input.", "labels": [], "entities": []}, {"text": "To test this, we trained versions of contextualised word embedding ELMo models on raw tokenized corpora and on the corpora with word tokens replaced by their lemmas.", "labels": [], "entities": []}, {"text": "Then, these models were evaluated on the word sense disambigua-tion task.", "labels": [], "entities": [{"text": "word sense disambigua-tion", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7179932792981466}]}, {"text": "This was done for the English and Russian languages.", "labels": [], "entities": []}, {"text": "The experiments showed that while lemmatization is indeed not necessary for English, the situation is different for Rus-sian.", "labels": [], "entities": []}, {"text": "It seems that for rich-morphology languages, using lemmatized training and testing data yields small but consistent improvements: at least for word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 143, "end_pos": 168, "type": "TASK", "confidence": 0.6541930536429087}]}, {"text": "This means that the decisions about text pre-processing before training ELMo should consider the linguistic nature of the language in question.", "labels": [], "entities": []}], "introductionContent": [{"text": "Deep contextualised representations of linguistic entities (words and/or sentences) are used in many current state-of-the-art NLP systems.", "labels": [], "entities": []}, {"text": "The most well-known examples of such models are arguably and.", "labels": [], "entities": []}, {"text": "A long-standing tradition if the field of applying deep learning to NLP tasks can be summarised as follows: as minimal pre-processing as possible.", "labels": [], "entities": []}, {"text": "It is widely believed that lemmatization or other text input normalisation is not necessary.", "labels": [], "entities": [{"text": "text input normalisation", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.6744876702626547}]}, {"text": "Advanced neural architectures based on character input (CNNs, BPE, etc) are supposed to be able to * Both authors contributed equally to the paper.", "labels": [], "entities": []}, {"text": "learn how to handle spelling and morphology variations themselves, even for languages with rich morphology: 'just add more layers!'.", "labels": [], "entities": []}, {"text": "Contextualised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing.", "labels": [], "entities": []}, {"text": "show that this is not entirely true.", "labels": [], "entities": []}, {"text": "It is known that for the previous generation of word embedding models ('static' ones like, where a word always has the same representation regardless of the context in which it occurs), lemmatization of the training and testing data improves their performance.", "labels": [], "entities": []}, {"text": "showed that this is true at least for semantic similarity and analogy tasks.", "labels": [], "entities": []}, {"text": "In this paper, we describe our experiments in finding out whether lemmatization helps modern contextualised embeddings (on the example of ELMo).", "labels": [], "entities": []}, {"text": "We compare the performance of ELMo models trained on the same corpus before and after lemmatization.", "labels": [], "entities": []}, {"text": "It is impossible to evaluate contextualised models on 'static' tasks like lexical semantic similarity or word analogies.", "labels": [], "entities": []}, {"text": "Because of this, we turned to word sense disambiguation in context (WSD) as an evaluation task.", "labels": [], "entities": [{"text": "word sense disambiguation in context (WSD)", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.8138392083346844}]}, {"text": "In brief, we use contextualised representations of ambiguous words from the top layer of an ELMo model to train word sense classifiers and find out whether using lemmas instead of tokens helps in this task (see Section 5).", "labels": [], "entities": []}, {"text": "We experiment with the English and Russian languages and show that they differ significantly in the influence of lemmatization on the WSD performance of ELMo models.", "labels": [], "entities": [{"text": "WSD", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9436386823654175}]}, {"text": "Our findings and the contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "Linguistic text pre-processing still matters in some tasks, even for contemporary deep representation learning algorithms.", "labels": [], "entities": []}, {"text": "2. For the Russian language, with its rich mor-", "labels": [], "entities": []}], "datasetContent": [{"text": "Following, we decided to avoid using any standard train-test splits for our WSD datasets.", "labels": [], "entities": [{"text": "WSD datasets", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.757012128829956}]}, {"text": "Instead, we rely on perword random splits and 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "This means that for each target word we randomly generate 5 different divisions of its context sentences list into train and test sets, and then train and test 5 different classifier models on this data.", "labels": [], "entities": []}, {"text": "The resulting performance score for each target word is the average of 5 macro-F1 scores produced by these classifiers.", "labels": [], "entities": []}, {"text": "ELMo models can be employed for the WSD task in two different ways: either by fine-tuning the model or by extracting word representations from it and then using them as features in a downstream classifier.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.9317531883716583}]}, {"text": "We decided to stick to the second (feature extraction) approach, since it is conceptually and computationally simpler.", "labels": [], "entities": [{"text": "feature extraction)", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.802999864021937}]}, {"text": "For comparison, we also report the scores of the 'averaged vectors' representations with Continuous Skipgram () embedding models trained on the English or Russian Wikipedia dumps ('SGNS' rows): before the advent of contextualised models, this was one of the most widely used ways to 'squeeze' the meaning of a sentence into a fixed-size vector.", "labels": [], "entities": []}, {"text": "Of course it does not mean that the meaning of a sentence always determines the senses all its words are used in.", "labels": [], "entities": []}, {"text": "However, averaging representations of words in contexts as a proxy to the sense of one particular word is along established tradition in WSD, starting at least from.", "labels": [], "entities": [{"text": "WSD", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9493638873100281}]}, {"text": "Also, since SGNS is a 'static' embedding model, it is of course not possible to use only target word vectors as features: they would be identical whatever the context is.", "labels": [], "entities": []}, {"text": "Simple logistic regression was used as a classification algorithm.", "labels": [], "entities": []}, {"text": "We also tested a multi-layer perceptron (MLP) classifier with 200-neurons hidden layer, which yielded essentially the same results.", "labels": [], "entities": []}, {"text": "This leads us to believe that our findings are not classifier-dependent.", "labels": [], "entities": []}, {"text": "shows the results, together with the ran-: Word sense disambiguation performance on the English data across words (ELMo target models).", "labels": [], "entities": []}, {"text": "dom and most frequent sense (MFS) baselines for each dataset.", "labels": [], "entities": []}, {"text": "First, ELMo outperforms SGNS for both languages, which comes as no surprise.", "labels": [], "entities": []}, {"text": "Second, the approach with averaging representations from all words in the sentence is not beneficial for WSD with ELMo: for English data, it clearly loses to a single target word representation, and for Russian there are no significant differences (and using a single target word is preferable from the computational point of view, since it does not require the averaging operation).", "labels": [], "entities": [{"text": "WSD", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9134374856948853}]}, {"text": "Thus, below we discuss only the single target word usage mode of ELMo.", "labels": [], "entities": []}, {"text": "But the most important part is the comparison between using tokens or lemmas in the train and test data.", "labels": [], "entities": []}, {"text": "For the 'static' SGNS embeddings, it does not significantly change the WSD scores for both languages.", "labels": [], "entities": [{"text": "WSD", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.5253832340240479}]}, {"text": "The same is true for English ELMo models, where differences are negligible and seem to be simple fluctuations.", "labels": [], "entities": []}, {"text": "However, for Russian, ELMo (target) on lemmas outperforms ELMo on tokens, with small but significant 5 improvement.", "labels": [], "entities": [{"text": "ELMo", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9930611252784729}]}, {"text": "The most plausible explanation for this is that (despite of purely character-based input of ELMo) the model does not have to learn idiosyncrasies of a particular language morphology.", "labels": [], "entities": []}, {"text": "Instead, it can use its (limited) capacity to better learn lexical semantic structures, leading to better WSD performance.", "labels": [], "entities": [{"text": "WSD", "start_pos": 106, "end_pos": 109, "type": "TASK", "confidence": 0.9596456289291382}]}, {"text": "The box plots 1 and 2 illustrate the scores dispersion across words in the test sets for English and Russian correspondingly (orange lines are medians).", "labels": [], "entities": []}, {"text": "In the next section 6 we At p value of 0.1, according to the Welch's t-test.: F1 scores for target words from RUSSE'18 with significant differences between lemma-based and token-based models analyse the results qualitatively.", "labels": [], "entities": [{"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9993466734886169}, {"text": "RUSSE'18", "start_pos": 110, "end_pos": 118, "type": "DATASET", "confidence": 0.9002792835235596}]}], "tableCaptions": [{"text": " Table 4: Averaged macro-F1 scores for WSD", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8035849332809448}]}, {"text": " Table 5: F1 scores for target words from  RUSSE'18 with significant differences between  lemma-based and token-based models", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9993683695793152}, {"text": "RUSSE'18", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8628309369087219}]}]}