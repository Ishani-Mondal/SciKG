{"title": [{"text": "ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents", "labels": [], "entities": [{"text": "ToNy", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8531528115272522}, {"text": "multilingual discourse segmentation of full documents", "start_pos": 41, "end_pos": 94, "type": "TASK", "confidence": 0.7713965574900309}]}], "abstractContent": [{"text": "Segmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9496685862541199}, {"text": "discourse parsing", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.7311376333236694}]}, {"text": "The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations.", "labels": [], "entities": []}, {"text": "Existing systems on English report F1 scores as high as 95%, but they generally assume gold sentence boundaries and are restricted to En-glish newswire texts annotated within the RST framework.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9730009734630585}]}, {"text": "This article presents a generic approach and a system, ToNy, a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes, languages and domains are represented.", "labels": [], "entities": [{"text": "DisRPT shared task", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7978711128234863}]}, {"text": "In our experiments, we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems, when separately trained on each corpus.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8033185601234436}]}, {"text": "We report performance between 81% and 96% in F1 score.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9493528008460999}]}, {"text": "We also observed that discourse segmentation models only display a moderate generalization capability, even within the same language and discourse representation scheme.", "labels": [], "entities": []}], "introductionContent": [{"text": "Discourse segmentation corresponds to the identification of Elementary Discourse Units in a document, i.e. the minimal spans of text that will be linked by discourse relations within the discourse structure, and/or the explicit markings of a discourse relations.", "labels": [], "entities": [{"text": "Discourse segmentation", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7990418374538422}]}, {"text": "The task definition differs slightly across the various existing and competing formalisms: in Rhetorical Structure Theory (RST) (, all segments are adjacent while in Segmented Discourse Representation Theory (SDRT), segments can be embedded in one another; In the Penn Discourse TreeBank (PDTB), the task is expressed as finding the arguments of a discourse connective, whether this connective is implicit or explicit.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.7505826652050018}, {"text": "Segmented Discourse Representation Theory (SDRT)", "start_pos": 166, "end_pos": 214, "type": "TASK", "confidence": 0.8076411485671997}, {"text": "Penn Discourse TreeBank (PDTB)", "start_pos": 264, "end_pos": 294, "type": "DATASET", "confidence": 0.95131187637647}]}, {"text": "Combining the existing corpora is thus a challenge, while the lack of annotated data makes it an appealing solution.", "labels": [], "entities": []}, {"text": "Even within a given framework, the criteria for identifying EDUs differ between the annotation projects: for instance, the RST-DT corpus) and the RST GUM corpus have very different segmentation guidelines.", "labels": [], "entities": [{"text": "RST-DT corpus", "start_pos": 123, "end_pos": 136, "type": "DATASET", "confidence": 0.7878357172012329}, {"text": "RST GUM corpus", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.7947109738985697}]}, {"text": "While discourse analysis mainly involves semantic and pragmatic questions, discourse segmentation is closer to the syntactic level, as is reflected in the annotation guidelines, which tend to equate segments with various kinds of clauses.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.7166521400213242}, {"text": "discourse segmentation", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.7093697041273117}]}, {"text": "Most existing work considers segmentation at the sentence level (intra-sentential segmentation), implicitly assuming that the task of sentence boundary detection can be done perfectly.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 134, "end_pos": 161, "type": "TASK", "confidence": 0.6774061520894369}]}, {"text": "This assumption is rarely questioned even though the performance of sentence boundary detection systems is far from perfect and very sensitive to noisy input.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6700056294600168}]}, {"text": "Also, it is crucial for some languages to consider document-level segmentation.", "labels": [], "entities": [{"text": "document-level segmentation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7305768728256226}]}, {"text": "Within the framework of the shared task, we investigate performance at the document-level with no gold sentence information, and compare it to the performance when assuming gold sentence boundaries.", "labels": [], "entities": []}, {"text": "We present different sequence prediction architectures with different pre-trained embeddings, and show that the best configurations using contextual embeddings () seem sufficient to reach comparable performances to existing systems, when separately trained on each corpus, while using more generic resources.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7708801627159119}]}, {"text": "1 Our best system consistently improves over the state-of-the-art models at the document level without the use of any addi-tional information apart from words, obtaining F1 scores between 80% and 94% when no gold sentence boundaries are given.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 170, "end_pos": 179, "type": "METRIC", "confidence": 0.9842381775379181}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics on the corpora.", "labels": [], "entities": []}, {"text": " Table 5: Specific results on English test data at the doc- ument level. 'Rand.-50d' and 'GloVe-50d' correspond  to the baseline model, taking a whole document as in- put. BERT models are still pipelined to a sentence- splitter, but ELMo-based models take the whole docu- ment as input. BERT-E uses English embeddings and  BERT-M uses multilingual embeddings.", "labels": [], "entities": [{"text": "BERT", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.885199785232544}, {"text": "sentence- splitter", "start_pos": 209, "end_pos": 227, "type": "TASK", "confidence": 0.7713571786880493}, {"text": "BERT-E", "start_pos": 287, "end_pos": 293, "type": "METRIC", "confidence": 0.820121705532074}, {"text": "BERT-M", "start_pos": 323, "end_pos": 329, "type": "METRIC", "confidence": 0.8412807583808899}]}]}