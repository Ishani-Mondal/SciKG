{"title": [{"text": "On the difficulty of a distributional semantics of spoken language", "labels": [], "entities": [{"text": "distributional semantics of spoken language", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7698861479759216}]}], "abstractContent": [{"text": "In the domain of unsupervised learning most work on speech has focused on discovering low-level constructs such as phoneme inventories or word-like units.", "labels": [], "entities": []}, {"text": "In contrast, for written language, where there is a large body of work on unsupervised induction of semantic representations of words, whole sentences and longer texts.", "labels": [], "entities": []}, {"text": "In this study we examine the challenges of adapting these approaches from written to spoken language.", "labels": [], "entities": []}, {"text": "We conjecture that unsupervised learning of the semantics of spoken language becomes feasible if we abstract from the surface variability.", "labels": [], "entities": [{"text": "learning of the semantics of spoken language", "start_pos": 32, "end_pos": 76, "type": "TASK", "confidence": 0.6732726863452366}]}, {"text": "We simulate this setting with a dataset of utterances spoken by a realistic but uniform synthetic voice.", "labels": [], "entities": []}, {"text": "We evaluate two simple unsupervised models which, to varying degrees of success, learn semantic representations of speech fragments.", "labels": [], "entities": []}, {"text": "Finally we present inconclusive results on human speech, and discuss the challenges inherent in learning distributional semantic representations on unrestricted natural spoken language .", "labels": [], "entities": []}], "introductionContent": [{"text": "In the realm of NLP for written language, unsupervised approaches to inducing semantic representations of words have along pedigree and a history of substantial success).", "labels": [], "entities": []}, {"text": "The core idea behind these models is to build word representations that can predict their surrounding context.", "labels": [], "entities": []}, {"text": "In search for similarly generic and versatile representations of whole sentences, various composition operators have been applied on word representations (e.g..", "labels": [], "entities": []}, {"text": "Alternatively, sentence representations are induced via the objective to predict the surrounding sentences (e.g..", "labels": [], "entities": []}, {"text": "Such representations capture aspects of the meaning of the encoded sentences, which can be used in a variety of tasks such as semantic entailment or text understanding.", "labels": [], "entities": [{"text": "semantic entailment", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7002957314252853}, {"text": "text understanding", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.7745981812477112}]}, {"text": "In the case of spoken language, unsupervised methods usually focus on discovering relatively low-level constructs such as phoneme inventories or word-like units.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that the key insight from distributional semantics that \"you shall know the word by the company it keeps\") is hopelessly confounded in the case of spoken language.", "labels": [], "entities": []}, {"text": "In text two words are considered semantically similar if they co-occur with similar neighbors.", "labels": [], "entities": []}, {"text": "However, speech segments which occur in the same utterance or situation often have many other features in addition to similar meaning, such as being uttered by the same speaker or accompanied by similar ambient noise.", "labels": [], "entities": []}, {"text": "In this study we show that if we can abstract away from speaker and background noise, we can effectively capture semantic characteristics of spoken utterances in an unsupervised way.", "labels": [], "entities": []}, {"text": "We present SegMatch, a model trained to match segments of the same utterance.", "labels": [], "entities": []}, {"text": "SegMatch utterance encodings are compared to those in Audio2Vec, which is trained to decode the context that surrounds an utterance.", "labels": [], "entities": [{"text": "SegMatch utterance encodings", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.9046282966931661}]}, {"text": "To investigate whether our representations capture semantics, we evaluate on speech and vision datasets where photographic images are paired with spoken descriptions.", "labels": [], "entities": []}, {"text": "Our experiments show that fora single synthetic voice, a simple model trained only on image captions can capture pairwise similarities that correlate with those in the visual space.", "labels": [], "entities": []}, {"text": "Furthermore we discuss the factors preventing effective learning in datasets with multiple human speakers: these include confounds between semantic and situational factors as well as artifacts in the datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the quality of the learned semantic speech representations according to the following criteria.", "labels": [], "entities": []}, {"text": "Paraphrase retrieval For the Synthetically Spoken COCO dataset as well as for the Flickr8k Audio Caption Corpus each image is described via five independent spoken captions.", "labels": [], "entities": [{"text": "Paraphrase retrieval", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6731850355863571}, {"text": "COCO dataset", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.725568488240242}, {"text": "Flickr8k Audio Caption Corpus", "start_pos": 82, "end_pos": 111, "type": "DATASET", "confidence": 0.8449657112360001}]}, {"text": "Thus captions describing the same image are effectively paraphrases of each other.", "labels": [], "entities": []}, {"text": "This structure of the data allows us to use a paraphrasing retrieval task as a measure of the semantic quality of the learned speech embeddings.", "labels": [], "entities": []}, {"text": "We encode each of the spoken utterances in the validation data, and rank the others according to the cosine similarity.", "labels": [], "entities": []}, {"text": "We then measure: (a) Median rank of the top-ranked paraphrase; and (b) recall@K: the proportion of paraphrases among K top-ranked utterances, for K 2 {1, 5, 10}.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9992737174034119}]}, {"text": "Representational similarity to image space Representational similarity analysis (RSA) is away of evaluating how pairwise similarities between objects are correlated in two object representation spaces ().", "labels": [], "entities": [{"text": "Representational similarity to image space Representational similarity analysis (RSA)", "start_pos": 0, "end_pos": 85, "type": "TASK", "confidence": 0.8458415053107522}]}, {"text": "Here we compare cosine similarities among encoded utterances versus cosine similarities among vector representations of images.", "labels": [], "entities": []}, {"text": "Specifically, we create two pairwise N \u21e5 N similarity matrices: (a) among encoded utterances from the validation data, and (b) among images corresponding to each utterance in (a).", "labels": [], "entities": []}, {"text": "Note that since there are five descriptions per image, each image is replicated five times in matrix (b).", "labels": [], "entities": []}, {"text": "We then take the upper triangulars of these matrices (excluding the diagonal) and compute Pearson's correlation coefficient between them.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 90, "end_pos": 123, "type": "METRIC", "confidence": 0.9237222075462341}]}, {"text": "The image features for this evaluation are obtained from the final fully connected layer of VGG-16) pre-trained on Imagenet () and consist of 4096 dimensions.", "labels": [], "entities": [{"text": "VGG-16", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.9736843109130859}, {"text": "Imagenet", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9089953303337097}]}], "tableCaptions": [{"text": " Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from  Chrupa\u0142a et al. (2017).", "labels": [], "entities": [{"text": "VGS", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6060023903846741}]}, {"text": " Table 2: Results on Flickr8K. The row labeled VGS is the visually supervised model from Chrupa\u0142a et al. (2017).", "labels": [], "entities": [{"text": "Flickr8K", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9169074296951294}, {"text": "VGS", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.6423379778862}]}]}