{"title": [{"text": "Maximum Likelihood Estimation of Factored Regular Deterministic Stochastic Languages", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proves that for every class C of stochastic languages defined with the co-emission product of finitely many proba-bilistic, deterministic finite-state acceptors (PDFA) and for every data sequence D of finitely many strings drawn i.i.d. from some stochastic language, the Maximum Likelihood Estimate of D with respect to C can be found efficiently by locally optimizing the parameter values.", "labels": [], "entities": []}, {"text": "We show that a consequence of the co-emission product is that each PDFA behaves like an independent factor in a joint distribution.", "labels": [], "entities": []}, {"text": "Thus, the likelihood function decomposes in a natural way.", "labels": [], "entities": []}, {"text": "We also show that the negative log likelihood function is convex.", "labels": [], "entities": [{"text": "negative log likelihood function", "start_pos": 22, "end_pos": 54, "type": "METRIC", "confidence": 0.696731373667717}]}, {"text": "These results are motivated by the study of Strictly k-Piecewise (SP k) Stochastic Languages , which form a class of stochastic languages which is both linguistically motivated and naturally understood in terms of the co-emission product of certain PDFAs.", "labels": [], "entities": []}], "introductionContent": [{"text": "Stochastic languages are probability distributions overall possible strings of finite length.", "labels": [], "entities": []}, {"text": "A class C of stochastic languages is often defined parametrically: an assignment of values to the parameters uniquely determines some stochastic language Lin C and thus the probabilities that L assigns to strings.", "labels": [], "entities": []}, {"text": "An important learning criterion fora class of stochastic languages C is whether there is an algorithm which reliably returns a MaximumLikelihood Estimate (MLE) of an observed data sample D.", "labels": [], "entities": [{"text": "MaximumLikelihood Estimate (MLE)", "start_pos": 127, "end_pos": 159, "type": "METRIC", "confidence": 0.7865350127220154}]}, {"text": "The MLE is the parameter values which maximize the probability of D with respect to C.", "labels": [], "entities": [{"text": "MLE", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9932205677032471}]}, {"text": "This paper focuses on regular deterministic stochastic languages.", "labels": [], "entities": []}, {"text": "These are stochastic languages that can be defined with a probabilistic, deterministic, finite-state acceptors (PDFA).", "labels": [], "entities": []}, {"text": "The problem of finding the MLE, however, is not only about some single stochastic language L, but also about the class of stochastic languages that L belong to.", "labels": [], "entities": []}, {"text": "It is well-understood that each PDFA M naturally defines a class of stochastic languages C M because the transitional probabilities in the PDFA provide a range of possible parameter values, as we explain in detail in section 2.", "labels": [], "entities": []}, {"text": "In this case, it is well-understood how to find the MLE of a sequence of strings drawn i.i.d. from L with respect to C M (.", "labels": [], "entities": [{"text": "MLE", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9795041680335999}]}, {"text": "This paper is concerned with finding the MLE for different classes of stochastic languages.", "labels": [], "entities": [{"text": "MLE", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7188038229942322}]}, {"text": "In particular, we consider the case where C is defined by the range of parametric values over finitely many PDFA A = {M 1 . .", "labels": [], "entities": []}, {"text": "M K }, whose co-emission product determines the probabilities each L \u2208 C assigns to strings.", "labels": [], "entities": []}, {"text": "Essentially, the coemission product of these PDFAs factor the probabilities each L \u2208 C assigns to strings.", "labels": [], "entities": []}, {"text": "Each L is a complex joint distribution, and each PDFA M j represents a 'more basic' regular stochastic language whose parameter values independently contribute to L.", "labels": [], "entities": []}, {"text": "At a high level, the problem we are considering is like those addressed with Bayesian networks and Markov random fields, where complex probability distributions decompose into simpler factors.", "labels": [], "entities": []}, {"text": "We refer to the classes C we study in this paper as factored, regular, probabilistic, and deterministic (FRPD).", "labels": [], "entities": [{"text": "FRPD", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.8158697485923767}]}, {"text": "The main result is to show how the parameters of a FRPD class C can be efficiently updated to find those parameter values which maximize the likelihood of the observed sequences (Theorem 2).", "labels": [], "entities": []}, {"text": "We also show directly that each negative log likelihood associated with each FRPD class C is convex.", "labels": [], "entities": []}, {"text": "Together these results imply that the efficient method we present for updating the parameter values will yield the MLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.6134693622589111}]}, {"text": "There are several reasons for being interested in such factored classes C.", "labels": [], "entities": []}, {"text": "Perhaps the most important from our perspective is expressed by \"The ability to exploit structure in the distribution is the basis for providing a compact representation of highdimensional . .", "labels": [], "entities": []}, {"text": "In our case, the size of the representation of the class given by A = {M 1 . .", "labels": [], "entities": []}, {"text": "M K } is simply the sum of the size of each M j . In contrast, the representation of the class given by the co-emission product is in the worst case the product of the sizes of each M j . One direct benefit of this is that the number of parameters is reduced, which makes it possible to more accurately estimate them with less data.", "labels": [], "entities": []}, {"text": "Other advantages discussed by Koller and Friedman, such as modularity, we return to in the discussion in the conclusion.", "labels": [], "entities": []}, {"text": "There are also linguistic reasons to be interested in FRPD classes.", "labels": [], "entities": [{"text": "FRPD classes", "start_pos": 54, "end_pos": 66, "type": "TASK", "confidence": 0.6340802013874054}]}, {"text": "The Strictly Piecewise (SP) class of languages encode certain types of long-distance dependencies found in natural languages.", "labels": [], "entities": []}, {"text": "For example, SP languages can express generalizations like \"at most one b per string\" and \"no b may follow an a\" ( . Generalizations with this formal character are known to occur in the phonologies of the world's languages.", "labels": [], "entities": []}, {"text": "As  explain, Strictly Piecewise languages are characterized by the intersection product of finitely many deterministic finite-state acceptors (DFA).", "labels": [], "entities": []}, {"text": "used this characterization and the co-emission product to define the class of Strictly Piecewise stochastic languages because they were interested in the learnability of long-distance dependencies in natural languages probabilistically.", "labels": [], "entities": []}, {"text": "They presented a learning algorithm fora class of SP stochastic languages and claimed (p. 894) that it returns the MLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.630926787853241}]}, {"text": "This results in this paper can be seen as providing a more generalized, more meaningful, and more rigorous proof of their basic claim.", "labels": [], "entities": []}, {"text": "Theorem 2 establishes how to update the parametric values which locally optimize the model of any FRPD class.", "labels": [], "entities": []}, {"text": "Theorem 3 shows the negative log likelihood function of any FRPD class is convex, so there is in fact only one set of optimal parametric values for any sequence of data.", "labels": [], "entities": []}, {"text": "Furthermore, we prove these results in terms of the standard definition of co-emission product, and not the variant used in . (While the results here work for both, we only prove the standard case.)", "labels": [], "entities": []}, {"text": "These general results make it possible to explore not only the learning of SP k stochastic languages, but also any finite combination of PDFAs that characterize different kinds of local and non-local dependencies which can be expressed with regular grammars.", "labels": [], "entities": []}, {"text": "We return to this issue in the discussion.", "labels": [], "entities": []}, {"text": "To our knowledge, such results for FRPD classes have not been previously discussed in the literature.", "labels": [], "entities": [{"text": "FRPD classes", "start_pos": 35, "end_pos": 47, "type": "TASK", "confidence": 0.7790825963020325}]}, {"text": "One reason for this is that much work on natural language processing uses probabilistic non-deterministic automata.", "labels": [], "entities": []}, {"text": "These describe the same class of stochastic languages as Hidden Markov Models (HMMs) (.", "labels": [], "entities": []}, {"text": "Non-determinism can make a big difference when it comes to parsing and learning.", "labels": [], "entities": []}, {"text": "Ina deterministic model M, each string w can be associated with at most one path through M, whereas in non-deterministic M, there can be infinitely many paths for w.", "labels": [], "entities": []}, {"text": "This is one reason why methods used for learning HMM are not guaranteed to return a MLE.", "labels": [], "entities": [{"text": "MLE", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9533465504646301}]}, {"text": "Since the states are 'hidden' one uses methods like Expectation Maximization, which may converge to a local optimum that is not a global optimum).", "labels": [], "entities": [{"text": "Expectation Maximization", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7284527719020844}]}, {"text": "On the other hand, we are showing that, by carefully choosing the class of stochastic languages C-which the MLE which is to be found will be 'with respect to'-we can exploit the structure we assume to be present to guarantee we find a MLE.", "labels": [], "entities": []}, {"text": "This paper takes one step in establishing the theoretical soundness of this approach.", "labels": [], "entities": []}, {"text": "Finally, one reviewer commented that these results may follow from fundamental theorems in the literature on probabilistic graphical models (.", "labels": [], "entities": []}, {"text": "Regardless of whether this is true, the correctness of the proofs here stand.", "labels": [], "entities": []}, {"text": "Also, the general results of Bayesian networks and Markov random fields say nothing about the concrete forms of the algorithm for obtaining the MLE with respect to a FRPD class C given data D, and similarly for its time complexity.", "labels": [], "entities": []}, {"text": "Malouf (2002) makes a similar point, writing \"While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates . .", "labels": [], "entities": []}, {"text": "Nonetheless, how probabilistic graphical models relate to this line of research ought to be made clear.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we review languages, stochastic languages, deterministic finite-state acceptors and probabilistic versions thereof, the intersection and co-emission products, and the statement of the learning problem.", "labels": [], "entities": []}, {"text": "Before presenting our main results, section 3 defines Strictly Piecewise (stochastic) languages, which provide a running example to illustrate the main results, which are presented in section 4.", "labels": [], "entities": []}, {"text": "The computational complexity of the updates are analyzed in section 5 and section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}