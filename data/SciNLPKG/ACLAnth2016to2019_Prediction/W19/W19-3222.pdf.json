{"title": [], "abstractContent": [{"text": "CLaC labs participated in Task 1 and 4 of SMM4H 2019.", "labels": [], "entities": [{"text": "CLaC labs", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9336569607257843}, {"text": "SMM4H 2019", "start_pos": 42, "end_pos": 52, "type": "TASK", "confidence": 0.8047927021980286}]}, {"text": "We pursed two main objectives in our submission.", "labels": [], "entities": []}, {"text": "First we tried to use some textual features in a deep net framework, and second, the potential use of more than one word embedding was tested.", "labels": [], "entities": []}, {"text": "The results seem positively affected by the proposed architec-tures.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ongoing SMM4H challenge tasks define evolving challenges defined on Twitter data.", "labels": [], "entities": [{"text": "SMM4H challenge", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.9142561554908752}]}, {"text": "The intention of epidemiologists is to detect mentions of health issues early on Twitter.", "labels": [], "entities": []}, {"text": "One of the challenges is to detect real reports of personally experienced health issues and to distinguish them from generalizations, hypotheticals, news, and institutional advice.", "labels": [], "entities": []}, {"text": "Task 1 of SMM4H 2019, \"Automatic classification of adverse effects mentions in tweets\", asks to distinguish tweets that report an adverse drug effect (AE) from those that do not.", "labels": [], "entities": [{"text": "SMM4H 2019", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8440240919589996}, {"text": "Automatic classification of adverse effects mentions in tweets", "start_pos": 23, "end_pos": 85, "type": "TASK", "confidence": 0.8194658532738686}]}, {"text": "Training data consists of 25,672 tweets with imbalanced distribution: 2,374 positive and 23,298 negative labels.", "labels": [], "entities": []}, {"text": "An example of an adverse effect mention in a tweet is: saphris gives me a mad appetite omg i hate this Task 4 is on \"Generalizable identification of personal health experience mentions\".", "labels": [], "entities": [{"text": "Generalizable identification of personal health experience mentions", "start_pos": 117, "end_pos": 184, "type": "TASK", "confidence": 0.8537190386227199}]}, {"text": "Two specialized training sets were released , \"flu vaccination\" and \"flu infection\", comprising approximately 6,200 and 1,100 tweets.", "labels": [], "entities": [{"text": "flu vaccination", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7394781112670898}, {"text": "flu infection", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.6777615547180176}]}, {"text": "Task 4 training data was balanced.", "labels": [], "entities": []}, {"text": "A sample positive tweet from this task is: I must say that flu shot packed a punch.", "labels": [], "entities": []}, {"text": "#WorstInoculationEver The CLaC submission to SMM4H 2019 had three general goals: first, to experiment with architectures that can address both tasks, second, to compare different word embeddings for their individual, but also their combined effectiveness, and third, to test whether we can augment the basic word vectors input with additional local and global knowledge from word lists and text preprocessing.", "labels": [], "entities": [{"text": "SMM4H 2019", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.562048077583313}]}, {"text": "The experiments remain inconclusive, due to an error in our submission pipeline.", "labels": [], "entities": []}], "datasetContent": [{"text": "Task 1 We submitted three configurations to Task 1: Glove with our textual features, W2V alone, and W2V with the first person pronoun feature (all used in an ensemble with BERT).", "labels": [], "entities": [{"text": "BERT", "start_pos": 172, "end_pos": 176, "type": "METRIC", "confidence": 0.9752249717712402}]}, {"text": "These were not our top performing configurations during development, rather we included W2V to bridge to Task 4 and we included two runs with different textual features and one without.", "labels": [], "entities": []}, {"text": "The performance of our system in the competition is provided in Tables 3, the competition performance of all three models is commensurate with our development results with \u00b12% in F1 measure.", "labels": [], "entities": [{"text": "F1", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.99958735704422}]}, {"text": "Moreover, the three configurations performed near identically and all three were above the competition mean.", "labels": [], "entities": []}, {"text": "It is interesting to note that the Word2Vec embeddings trained on Sentiment140 data proved as effective on this data set as Glove with the textual features, in contrast to our development experiments.", "labels": [], "entities": [{"text": "Sentiment140 data", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.7629229128360748}]}, {"text": "We interpret the fact that W2V in an ensemble with BERT lies above the competition's mean to confirm the importance of our genre selection for Word2vec training. demonstrate.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.996249258518219}]}], "tableCaptions": [{"text": " Table 1: Development results for Task 1. Submitted  configurations are indicated by *", "labels": [], "entities": []}, {"text": " Table 2: Development results for Task 4. Submitted  configurations are indicated by *", "labels": [], "entities": []}, {"text": " Table 3: CLaC competition results for Task 1", "labels": [], "entities": []}, {"text": " Table 4: CLaC competition results for Task 4", "labels": [], "entities": []}]}