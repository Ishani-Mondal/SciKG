{"title": [{"text": "Can Modern Standard Arabic Approaches be used for Arabic Dialects? Sentiment Analysis as a Case Study", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.9781849980354309}]}], "abstractContent": [{"text": "We present the Shami-Senti corpus, the first Levantine corpus for Sentiment Analysis (SA), and investigate the usage of off-the-shelf models that have been built for Modern Standard Arabic (MSA) on this corpus of Dialectal Ara-bic (DA).", "labels": [], "entities": [{"text": "Sentiment Analysis (SA)", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.8613657832145691}]}, {"text": "We apply the models on DA data, showing that their accuracy does not exceed 60%.", "labels": [], "entities": [{"text": "DA data", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.7602430582046509}, {"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.999576985836029}]}, {"text": "We then proceed to build our own models involving different feature combinations and machine learning methods for both MSA and DA and achieve an accuracy of 83% and 75% respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.999581515789032}]}], "introductionContent": [{"text": "There is a growing need for text mining and analytical tools for Social Media data, for example Sentiment Analysis (SA) tools which aim to distinguish people's views into positive and negative, objective and subjective responses, or even into neutral opinions.", "labels": [], "entities": [{"text": "text mining", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.7255862355232239}]}, {"text": "The amount of internet documents in Arabic is increasing rapidly.", "labels": [], "entities": []}, {"text": "However, texts from Social media are typically not written in Modern Standard Arabic (MSA) for which computational resources and corpora exist.", "labels": [], "entities": []}, {"text": "These systems achieve reasonable accuracy on the designated tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9990754127502441}]}, {"text": "For example,  achieve an accuracy of 95% on the news domain.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995244741439819}]}, {"text": "On the other hand, research on Dialectal Arabic (DA) in terms of SA is an open research question and presents considerable challenges (.", "labels": [], "entities": [{"text": "Dialectal Arabic (DA)", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6488657474517823}]}, {"text": "The degree to which tools trained on MSA can be used on DA is still also an open research question.", "labels": [], "entities": [{"text": "DA", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9081646800041199}]}, {"text": "This is partly because different dialects differ from MSA to varying degrees(.", "labels": [], "entities": []}, {"text": "Furthermore, the speakers of Arabic present us with clear cases of, where MSA is the official language used for education, news, politics, religion and, in general, in any type of formal setting, but dialects are used in everyday communication, as well as in informal writing.", "labels": [], "entities": []}, {"text": "In this paper, we examine whether it is possible to adapt classification models that have been trained and built on MSA data for DA from the Levantine region, or whether we should build and train specific models for the individual dialects, therefore considering them as stand-alone languages.", "labels": [], "entities": []}, {"text": "To answer this question we use Sentiment Analysis as a case study.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9490057229995728}]}, {"text": "Our contributions are the following: \u2022 We systematically evaluate how well the ML models on MSA for SA perform on DA of Levantine; \u2022 We construct and present anew sentiment corpus of Levantine DA; \u2022 We investigate the issue of domain adaptation of ML models from MSA to DA.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 227, "end_pos": 244, "type": "TASK", "confidence": 0.7037527710199356}]}, {"text": "The paper is organised as follows: in Section 2, we briefly discuss the task of SA and present related work on Arabic.", "labels": [], "entities": [{"text": "SA", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.973783016204834}]}, {"text": "In Section 3, we describe an extension of the Shami corpus of Levantine dialects () annotated for Sentiment, Shami-Senti.", "labels": [], "entities": [{"text": "Sentiment", "start_pos": 98, "end_pos": 107, "type": "TASK", "confidence": 0.9151995182037354}]}, {"text": "In Section 4, we present the experimental setting and results of adapting MSA models to the dialectal domain as well as training specific models.", "labels": [], "entities": []}, {"text": "We conclude and discuss directions for future research in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to estimate the performance of the SA models, which have built on MSA data, on DA evaluation data, we use the following two corpora in our experiments.", "labels": [], "entities": [{"text": "MSA data", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.813841313123703}]}, {"text": "\u2022 LABR  in MSA with some dialectal words.", "labels": [], "entities": [{"text": "LABR", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9660653471946716}]}, {"text": "LABR is available with different subsets: the authors split it into 2,3,4 and 5 sentiment polarities with balanced and unbalanced divisions.", "labels": [], "entities": [{"text": "LABR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7820037603378296}]}, {"text": "They depend on the user ratings to classify sentences.", "labels": [], "entities": []}, {"text": "Thus, 4 and 5 stars ratings are taken as positive, 1 and 2 star ratings are taken as negative and 3 star ratings are taken as mixed or neutral.", "labels": [], "entities": []}, {"text": "The fact that LABR is limited to one domain, book reviews, makes it difficult to use it as a general SA model.", "labels": [], "entities": []}, {"text": "\u2022 ASTD (: it is an Arabic SA corpus collected from Twitter and focuses on the Egyptian dialects.", "labels": [], "entities": [{"text": "ASTD", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.7971189618110657}]}, {"text": "It consists of about 10k tweets, which are classified as objective, subjective positive, subjective negative, and subjective mixed.", "labels": [], "entities": []}, {"text": "shows the number of instances of each polarity label in different corpora.", "labels": [], "entities": []}, {"text": "In all experiments, we use the same machine learning algorithms that have been used by the LABR baseline.", "labels": [], "entities": [{"text": "LABR baseline", "start_pos": 91, "end_pos": 104, "type": "DATASET", "confidence": 0.9173583984375}]}, {"text": "These are: 1 The choice is motivated as follows.", "labels": [], "entities": []}, {"text": "LR is strong in explaining the relationship between one dependent variable and independent variables), while PA is suitable for large-scale learning).", "labels": [], "entities": [{"text": "PA", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.9408994913101196}]}, {"text": "LinearSVC is effective in cases where the number of dimensions is greater than the number of samples (.", "labels": [], "entities": []}, {"text": "BNB is suitable for discrete data, and SGD is a linear classifier which implements regularised linear models with stochastic gradient descent (SGD) learning.", "labels": [], "entities": [{"text": "BNB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8633487224578857}]}, {"text": "It is a simple baseline classifier related to neural networks).", "labels": [], "entities": []}, {"text": "In addition, we also use some popular linear and probabilistic classifiers.", "labels": [], "entities": []}, {"text": "Hence, we use Multinomial Naive-Bayes (MNB), which is suitable for classification of discrete features.", "labels": [], "entities": []}, {"text": "The multinomial distribution normally requires integer feature counts and it works well for fractional counts like tf-idf (.", "labels": [], "entities": []}, {"text": "We further use Complement Naive-Bayes (CNB), which is particularly suited for imbalanced data sets.", "labels": [], "entities": []}, {"text": "CNB uses statistics taken from the complement of each class to compute the models weights.", "labels": [], "entities": [{"text": "CNB", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9381675124168396}]}, {"text": "1 Generally speaking, a NB classifier converges quicker than discriminative models like logistic regression, so one needless training data.", "labels": [], "entities": []}, {"text": "The last one is the Ridge Classifier (RC).", "labels": [], "entities": []}, {"text": "Its most important feature is that it does not remove irrelevant features but rather minimise their impact on the trained model ().", "labels": [], "entities": []}, {"text": "All of the algorithms are implemented using the scikit learn library in Python (Pedregosa et al., 2011) .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of terms in sentiment lexicons", "labels": [], "entities": []}, {"text": " Table 2: The number of instances per category in  Shami-Senti and other sentiment corpora used in our  experiments", "labels": [], "entities": []}, {"text": " Table 3: The number of instances per category in bal- anced LABR3", "labels": [], "entities": [{"text": "bal- anced LABR3", "start_pos": 50, "end_pos": 66, "type": "DATASET", "confidence": 0.5813516080379486}]}, {"text": " Table 4: Accuracy of the baseline on LABR3 (Tf-wg :  is the Term Frequency on Word grams)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998848557472229}, {"text": "LABR3", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.7923271656036377}, {"text": "Tf-wg", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.8849368095397949}, {"text": "Term Frequency", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.7812647521495819}]}, {"text": " Table 5: Accuracy of the baseline TF wg1+2 trained  on LABR3 and Shami-Senti and tested on Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998979389667511}, {"text": "TF wg1+2", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.8414352834224701}, {"text": "LABR3", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.9681826829910278}]}, {"text": " Table 6: Accuracy of the proposed model trained and  tested on LABR3; Model 1: unigram word level with  (2,5) character grams; In Model 2 (unigram,bigrams)  word level with (2,5) character grams", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9964740872383118}, {"text": "LABR3", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9578474760055542}]}, {"text": " Table 7: Accuracy of the proposed model trained on  LABR3 and tested on Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99817955493927}, {"text": "LABR3", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9403924345970154}, {"text": "Shami-Senti", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.6442580819129944}]}, {"text": " Table 8: Accuracy of the proposed model 3-class clas- sification trained and tested on Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967742562294006}, {"text": "Shami-Senti", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.571922779083252}]}, {"text": " Table 9: Examples annotated as neutral in LABR3 and our corrected polarity", "labels": [], "entities": [{"text": "LABR3", "start_pos": 43, "end_pos": 48, "type": "DATASET", "confidence": 0.8737520575523376}, {"text": "corrected polarity", "start_pos": 57, "end_pos": 75, "type": "METRIC", "confidence": 0.9515132308006287}]}, {"text": " Table 10: Accuracy for binary classifiers with different feature sets trained on the LABR2 dataset and tested on  LABR2 and Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.990875244140625}, {"text": "LABR2 dataset", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.9750531017780304}, {"text": "LABR2", "start_pos": 115, "end_pos": 120, "type": "DATASET", "confidence": 0.967467188835144}]}, {"text": " Table 11: Accuracy of the proposed model on binary  classification trained on ASTD and tested on ASTD  and Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9915289878845215}, {"text": "binary  classification", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.697387084364891}, {"text": "ASTD", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9610423445701599}, {"text": "ASTD", "start_pos": 98, "end_pos": 102, "type": "DATASET", "confidence": 0.9748899340629578}, {"text": "Shami-Senti", "start_pos": 108, "end_pos": 119, "type": "DATASET", "confidence": 0.48330262303352356}]}, {"text": " Table 12: Accuracy of the proposed model on binary  classification trained and tested on Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9945603013038635}, {"text": "binary  classification", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7238023430109024}]}, {"text": " Table 13: Accuracy of two classifiers using feature en- gineering on 3-class classification task", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9557682871818542}]}, {"text": " Table 14: Accuracy of deep learning models 3-class  LABR and Shami-Senti", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9944695234298706}]}]}