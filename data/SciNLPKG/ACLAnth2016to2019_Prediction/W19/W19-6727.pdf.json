{"title": [{"text": "Improving Robustness in Real-World Neural Machine Translation Engines", "labels": [], "entities": [{"text": "Improving Robustness", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8793130815029144}, {"text": "Neural Machine Translation Engines", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.7494219839572906}]}], "abstractContent": [{"text": "As a commercial provider of machine translation, we are constantly training engines fora variety of uses, languages, and content types.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7553530037403107}]}, {"text": "In each case, there can be many variables, such as the amount of training data available, and the quality requirements of the end user.", "labels": [], "entities": []}, {"text": "These variables can have an impact on the robust-ness of Neural MT engines.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.7279132008552551}]}, {"text": "On the whole, Neural MT cures many ills of other MT paradigms, but at the same time it has introduced anew set of challenges to address.", "labels": [], "entities": [{"text": "Neural MT", "start_pos": 14, "end_pos": 23, "type": "TASK", "confidence": 0.8128336071968079}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.9559048414230347}]}, {"text": "In this paper, we describe some of the specific issues with practical NMT and the approaches we take to improve model robustness in real world scenarios.", "labels": [], "entities": [{"text": "NMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9435701966285706}]}], "introductionContent": [{"text": "As a commercial provider of bespoke machine translation (MT) solutions for enterprise users, we train engines all day, everyday fora variety of different languages, and content types, with different quantities and quality of training data.", "labels": [], "entities": [{"text": "bespoke machine translation (MT)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.8244097232818604}]}, {"text": "On a case by case basis, there area lot of variables to contend with.", "labels": [], "entities": []}, {"text": "The breakthrough of Neural MT (NMT) over the past number of years, and the step change in quality it can produce, means that it is a no-brainer to adopt and make an integral part of our technology stack.", "labels": [], "entities": [{"text": "Neural MT (NMT)", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7638375341892243}]}, {"text": "However, there are still some practical gaps that need to be addressed in the core technology, in order to make it broadly production ready and flexible.", "labels": [], "entities": []}, {"text": "These are either specific issues or topics that were already resolved in Statistical MT and have been reintroduced, or new types of issues unique to neural models.", "labels": [], "entities": [{"text": "Statistical MT", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.6583981812000275}]}, {"text": "This can include, but is not limited to, the need fora more rigorous data cleaning step, alack of robustness around handling terminology and various types of mistranslations, and the ability to adapt to different domains.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7452417016029358}]}, {"text": "Sometimes we can handle these issues elegantly in the models, but certain variables such as the volume of training data available in each case, make it a little less predictable.", "labels": [], "entities": []}, {"text": "In some cases, we have to find more practical workarounds in our data preparation, and pre-and post-processing steps, in order to get engines production ready.", "labels": [], "entities": []}, {"text": "In this paper, after giving an overview of our NMT pipeline, we will focus on how we address the following issues in order to better prepare NMT engines for real-world deployment: 1) data cleaning, 2) over-generation, 3) improving robustness when translating entities, and 4) domain adaptation.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 183, "end_pos": 196, "type": "TASK", "confidence": 0.850292295217514}, {"text": "domain adaptation", "start_pos": 276, "end_pos": 293, "type": "TASK", "confidence": 0.8187397718429565}]}], "datasetContent": [{"text": "We evaluated the impact of our data cleaning pipeline on the KDE4 German-English data, obtained from the OPUS corpus . We compared the training with data processed by our pipeline and with data processed by Moses tools (tokenization, length-based filter and true-casing).", "labels": [], "entities": [{"text": "KDE4 German-English data", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.9093344211578369}, {"text": "OPUS corpus", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.9399242997169495}]}, {"text": "We used the same length parameters for the length-based filter: Evaluation scores for training on data processed by Moses tools, our pipeline without (Iconic) and with (Iconic+DNT) replacement of do-not-translate phrases by placeholders.", "labels": [], "entities": []}, {"text": "(175 words) and the same true-casing models.", "labels": [], "entities": []}, {"text": "The statistics of the data are shown in.", "labels": [], "entities": []}, {"text": "In the case of the Moses pipeline, the development set was selected at random.", "labels": [], "entities": [{"text": "Moses pipeline", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.8860019445419312}]}, {"text": "The test set was the same, but processed according to each pipeline.", "labels": [], "entities": []}, {"text": "We trained small transformer models with the Fairseq tool (, with the same parameters as those indicated in the fairseq github site for IWSLT'14 German to English.", "labels": [], "entities": [{"text": "Fairseq tool", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.8464570641517639}, {"text": "fairseq github site for IWSLT'14 German to English", "start_pos": 112, "end_pos": 162, "type": "DATASET", "confidence": 0.8250509798526764}]}, {"text": "We averaged the 5 checkpoints around the best model.", "labels": [], "entities": []}, {"text": "We repeated the training 3 times and report the average and standard deviation of the 3 runs.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 60, "end_pos": 78, "type": "METRIC", "confidence": 0.9274688065052032}]}, {"text": "Training with our pipeline improves BLEU () and TER () scores respectively by 2.3 and 3.0 points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.999278724193573}, {"text": "TER () scores", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9623560508092245}]}, {"text": "The difference is larger than standard deviation error bars, thus it is statistically significant according to this criterion.", "labels": [], "entities": [{"text": "standard deviation error bars", "start_pos": 30, "end_pos": 59, "type": "METRIC", "confidence": 0.8176970928907394}]}, {"text": "This suggests that efforts to better clean the data and to choose the validation set carefully are beneficial in terms of automated quality metrics.", "labels": [], "entities": []}, {"text": "NMT models are not perfect at controlling the output length and sometimes drop or duplicate content.", "labels": [], "entities": []}, {"text": "To evaluate this category of errors, the rest of metrics measure over-generation (repetitions) and under-generation (source text not covered).", "labels": [], "entities": []}, {"text": "OVER simply counts repetitions in the output, while UNDER counts under-generation based on the ratio of number of source and output words.", "labels": [], "entities": [{"text": "OVER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.834773600101471}, {"text": "UNDER", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9490476250648499}]}, {"text": "REP and DROP count respectively the number of repetitions and under-generation in the output based on the alignment with the source (.", "labels": [], "entities": [{"text": "REP", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9690876603126526}, {"text": "DROP", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.8833748698234558}]}, {"text": "Interestingly, the REP score is significantly lower with our pipeline.", "labels": [], "entities": [{"text": "REP score", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9699796736240387}]}, {"text": "The DROP score average is also lower although the difference lies within the standard deviation.", "labels": [], "entities": [{"text": "DROP score average", "start_pos": 4, "end_pos": 22, "type": "METRIC", "confidence": 0.9518083333969116}]}, {"text": "This suggests that the engine is more robust to under-and overgeneration with our pipeline.", "labels": [], "entities": []}, {"text": "Replacing do-not-translate phrases (DNTs) by placeholders (see section 3.2.1) yields slightly worse BLEU and TER scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9993114471435547}, {"text": "TER", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9970486760139465}]}, {"text": "However, the OVER, REP and UNDER scores are improved.", "labels": [], "entities": [{"text": "OVER", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9970911741256714}, {"text": "REP", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.996152937412262}, {"text": "UNDER", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9943419694900513}]}, {"text": "Thus using DNTs may improve robustness.", "labels": [], "entities": []}, {"text": "The worsening of BLEU and TER maybe due to the fact that we used only one type of placeholder to replace entities which appear in different contexts (for example, URLs and numbers).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9985664486885071}, {"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9956057667732239}]}, {"text": "Using different types would improve the modelling of each one.", "labels": [], "entities": []}, {"text": "Our pipeline includes a module to detect duplicated content in the translation and to delete it.", "labels": [], "entities": []}, {"text": "The detection is based on the source sentence.", "labels": [], "entities": []}, {"text": "That is, if the source text contains a repetition, it is not incorrect to have it in the translation.", "labels": [], "entities": []}, {"text": "To decide whether a repetition should be deleted or not, we adopted a conservative criterion favoring precision rather than recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9991793036460876}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.9980913996696472}]}, {"text": "We delete repeated words if they are aligned with the same source word.", "labels": [], "entities": []}, {"text": "The alignment maybe given by the attention weights or by an external alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.988366425037384}]}, {"text": "also shows the impact of using our source-based repetition deletion module (\"repdel\").", "labels": [], "entities": []}, {"text": "This module drops the average number of repetitions (OVER) of the Iconic system from 29.3 to 17.3 and the REP score from 3.3 to 3.1.", "labels": [], "entities": [{"text": "average number of repetitions (OVER)", "start_pos": 22, "end_pos": 58, "type": "METRIC", "confidence": 0.798442908695766}, {"text": "Iconic system", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.9179424941539764}, {"text": "REP score", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.953098475933075}]}, {"text": "Applied to the system with DNTs, OVER drops from 26.0 to 13.3 and the REP score from 2.9 to 2.7.", "labels": [], "entities": [{"text": "OVER", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9993793964385986}, {"text": "REP score", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9796609580516815}]}, {"text": "Thus this module is effective at removing repetitions, with no significant impact on BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.997215747833252}, {"text": "TER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9812290072441101}]}, {"text": "Our sample results here are based on the publicly available IWSLT dataset 2 . The distribution of train, dev, and test datasets is detailed in.", "labels": [], "entities": [{"text": "IWSLT dataset", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9653233885765076}]}, {"text": "We randomly select a development set from the training data.", "labels": [], "entities": []}, {"text": "The test set is created by combining dev, and test) sets of earlier IWSLT shared tasks.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.8038125038146973}]}, {"text": "We use a shared vocabulary BPE Model (Sennrich et al., 2016) for subword segmentation, with a code of 32000 merge operations.", "labels": [], "entities": [{"text": "subword segmentation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7674236297607422}]}, {"text": "We use convolutional ( encoder-decoder (15x15) architecture with the size of hidden units and word embedding of 512.", "labels": [], "entities": []}, {"text": "For the training of model parameters, we use NAG ( with cross entropy as a loss function.", "labels": [], "entities": [{"text": "NAG", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.8274376392364502}]}, {"text": "We start with a learning rate of 0.25 and reduce it by a factor of 10 if there is no change in the validation perplexity fora fixed number of epochs.", "labels": [], "entities": []}, {"text": "BLEU () and TER () scores are computed with tokenized lower-cased output and references using the \"evaluater\" binary from Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9834840893745422}, {"text": "TER () scores", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9439777731895447}]}, {"text": "The evaluation scores are detailed in.", "labels": [], "entities": []}, {"text": "The quality scores have not improved using the proposed methods, but in manual evaluation, it was found that the model trained with \"no more split\" setting preserves better the named entities.", "labels": [], "entities": []}, {"text": "This is depicted with an example in.", "labels": [], "entities": []}, {"text": "The model with \"protect unseen\" with threshold value of 4 is slightly better than baseline, but in manual evaluation, we have seen that it is not better at translating the named entities compared to the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of KDE4 data for the training, develop- ment and test corpora processed by Iconic pipeline and Moses  tools.", "labels": [], "entities": [{"text": "KDE4 data", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.8430612683296204}]}, {"text": " Table 2: Evaluation scores for training on data processed by Moses tools, our pipeline without (Iconic) and with (Iconic+DNT)  replacement of do-not-translate phrases by placeholders.", "labels": [], "entities": []}, {"text": " Table 3: Data distribution after cleaning and applying tok- enizer (source side)", "labels": [], "entities": []}, {"text": " Table 4: Evaluation scores. K: threshold for the unk-count", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9322606325149536}, {"text": "K: threshold", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.7789380550384521}]}, {"text": " Table 4.  The quality scores have not improved using the  proposed methods, but in manual evaluation, it was  found that the model trained with \"no more split\"  setting preserves better the named entities. This is  depicted with an example in", "labels": [], "entities": []}, {"text": " Table 6: Evaluation scores for dynamic domain adaptation.", "labels": [], "entities": [{"text": "dynamic domain adaptation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6574257016181946}]}]}