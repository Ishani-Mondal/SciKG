{"title": [{"text": "REflex: Flexible Framework for Relation Extraction in Multiple Domains", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.9539009034633636}]}], "abstractContent": [{"text": "Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques.", "labels": [], "entities": [{"text": "Systematic comparison", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.9029794633388519}, {"text": "relation extraction (RE)", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.9105161786079407}]}, {"text": "In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets.", "labels": [], "entities": [{"text": "RE", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.979267954826355}]}, {"text": "By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of pre-processing area large contributor performance and that omission of such information can further hinder fair comparison.", "labels": [], "entities": []}, {"text": "Other insights from our exploration allow us to provide recommendations for future research in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation Extraction (RE) has gained a lot of interest from the community with the introduction of the Semeval tasks from 2007 by () and 2010 by).", "labels": [], "entities": [{"text": "Relation Extraction (RE)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8973532557487488}]}, {"text": "The task is a subset of information extraction (IE) with the goal of finding semantic relationships between concepts in a given sentence, and is an important component of Natural Language Understanding (NLU).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8768928170204162}, {"text": "Natural Language Understanding (NLU)", "start_pos": 171, "end_pos": 207, "type": "TASK", "confidence": 0.7254342635472616}]}, {"text": "Applications include automatic knowledge base creation, question answering, as well as analysis of unstructured text data.", "labels": [], "entities": [{"text": "automatic knowledge base creation", "start_pos": 21, "end_pos": 54, "type": "TASK", "confidence": 0.5716065391898155}, {"text": "question answering", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9244896471500397}]}, {"text": "Since the introduction of RE tasks in the general and medical domains, many researchers have explored the performance of different neural network architectures on the datasets (.", "labels": [], "entities": [{"text": "RE tasks", "start_pos": 26, "end_pos": 34, "type": "TASK", "confidence": 0.9347460567951202}]}, {"text": "However, progress in RE is hampered by reproducibility issues as well as the difficulty in assessing which techniques in the literature will generalize to novel tasks, datasets and contexts.", "labels": [], "entities": [{"text": "RE", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.995030403137207}]}, {"text": "To assess the extent of these problems, we performed a manual review of 53 relevant neural RE papers 1 citing the three datasets ().", "labels": [], "entities": []}, {"text": "The procedure for finding these papers is highlighted in.", "labels": [], "entities": []}, {"text": "Reproducibility Reproducibility is important for validating previous work and building upon it ().", "labels": [], "entities": []}, {"text": "Lack of reproducibility can be attributed to many factors such as difficulty in availability of source code) and omission of sources of variability such as hyperparameter details.", "labels": [], "entities": []}, {"text": "We found that only 16 out of the 53 relevant papers had released their source code.", "labels": [], "entities": []}, {"text": "14 out of 53 papers were evaluated on multiple datasets, but the source code was publicly available for only five of those.", "labels": [], "entities": []}, {"text": "Despite this, much of this code was lacking in modularity to be easily extendable to new datasets.", "labels": [], "entities": []}, {"text": "In many cases, the process of reproducing the paper results was often unclear and lack of documentation made this more difficult.", "labels": [], "entities": []}, {"text": "Even though most papers mentioned some hyperparameter details, important details were missing such as number of epochs, batch size, random initialization seed, if any, and details about early stop if that technique was applied.", "labels": [], "entities": []}, {"text": "Ablation Studies Lack of generalizability is caused by a dearth of appropriate empirical evaluation to identify the source of modeling gains.", "labels": [], "entities": []}, {"text": "Ablation studies are important for identifying sources of improvements in results.", "labels": [], "entities": []}, {"text": "Among the 53 papers that we looked at, 20 of the 24 papers in the general domain performed ablation studies.", "labels": [], "entities": []}, {"text": "However, only 10 out of 29 papers in the medical domain performed one.", "labels": [], "entities": []}, {"text": "Among these ablation studies, key details related to pre-processing were missing, which we found critical in our experiments.", "labels": [], "entities": []}, {"text": "In the absence of such information about causes of large variability of results, fair comparison of models becomes difficult.", "labels": [], "entities": []}, {"text": "In this paper, we present an open-source unifying framework enabling the comparison of various training methodologies, pre-processing, modeling techniques, and evaluation metrics.", "labels": [], "entities": []}, {"text": "The code is available at https: //github.com/geetickachauhan/ relation-extraction.", "labels": [], "entities": []}, {"text": "The experimental goals of this framework are identification of sources of variability in results for the three datasets and provide the field with a strong baseline model to compare against for future improvements.", "labels": [], "entities": []}, {"text": "The design goals of this framework are identification of best practices for relation extraction and to be a guide for approaching new datasets.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.8921160697937012}]}, {"text": "By performing systematic comparison on three datasets, we find that 1) pre-processing choices can cause the largest variations in performance, 2) reporting scores on one test set split is problematic due to split bias.", "labels": [], "entities": []}, {"text": "We perform other analyses in section 5 and also include recommendations for future research in this field in section 7.", "labels": [], "entities": []}, {"text": "Upon testing various combinations of our approaches, we achieve results near state of the art ranges for the three datasets: 85.89% macro F1 for Semeval 2010 task 8 dataset ( i.e. semeval, 71.97% macro F1 for DDI i.e. ddi and 71.01% micro F1 for i2b2/VA 2010 relation classification dataset i.e. i2b2.", "labels": [], "entities": [{"text": "F1", "start_pos": 138, "end_pos": 140, "type": "METRIC", "confidence": 0.7991902232170105}, {"text": "Semeval 2010 task 8 dataset", "start_pos": 145, "end_pos": 172, "type": "DATASET", "confidence": 0.54222252368927}, {"text": "VA 2010 relation classification dataset", "start_pos": 251, "end_pos": 290, "type": "DATASET", "confidence": 0.8870401024818421}]}, {"text": "We refer to ddi and i2b2 as medical datasets, as they belong to the biomedical and clinical domains, respectively.", "labels": [], "entities": []}], "datasetContent": [{"text": "Rel: Dataset information, with columns Rel = number of relations, Eval = evaluation metric (all F1 scores), Agreement = Inter-annotator agreement, Det = whether detection task from section 3.4 was evaluated on.", "labels": [], "entities": [{"text": "Rel", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9458702206611633}, {"text": "Eval", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9879257678985596}, {"text": "F1 scores)", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9699950615564982}]}, {"text": "Rel column only includes relations used in official evaluation metric.", "labels": [], "entities": [{"text": "Rel", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7424022555351257}]}, {"text": "ddi was built from two separately annotated sources and therefore contains two inter-annotator agreements.", "labels": [], "entities": []}, {"text": "We summarize important information about these datasets in table 1.", "labels": [], "entities": []}, {"text": "We introduce detection and classification tasks in section 3.4, but also indicate the tasks evaluated for each dataset in table 1.", "labels": [], "entities": [{"text": "detection and classification", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.6539285580317179}]}, {"text": "Semeval 2010 semeval consists of 8000 training sentences and 2,717 test sentences for the multi-way classification of semantic relations between pairs of nominals.", "labels": [], "entities": [{"text": "Semeval 2010 semeval", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.7157030502955118}, {"text": "multi-way classification of semantic relations between pairs of nominals", "start_pos": 90, "end_pos": 162, "type": "TASK", "confidence": 0.7915448579523299}]}, {"text": "Not included in the official evaluation is an Other class which is considered noisy, with annotators choosing this class if no fit was found in the other classes.", "labels": [], "entities": []}, {"text": "It is important to note that this is a synthetically generated dataset, and detection scores were not calculated due to the noisy nature of the Other class.", "labels": [], "entities": []}, {"text": "DDI Extraction ddi consists of 1,017 texts with 18,491 pharmacological substances and 5,021 drug-drug interactions from Pubmed articles in the pharmacological literature.", "labels": [], "entities": [{"text": "DDI Extraction ddi", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.7719649275143942}]}, {"text": "None class indicating no interaction between the drug pairs is included in the evaluation metric calculation.", "labels": [], "entities": []}, {"text": "i2b2/VA 2010 relations i2b2 consists of discharge summaries from Partners Healthcare and the MIMIC II Database (Saeed et al., 2011).", "labels": [], "entities": [{"text": "VA 2010 relations i2b2", "start_pos": 5, "end_pos": 27, "type": "DATASET", "confidence": 0.8691532015800476}, {"text": "Partners Healthcare", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9717691242694855}, {"text": "MIMIC II Database", "start_pos": 93, "end_pos": 110, "type": "DATASET", "confidence": 0.8612425923347473}]}, {"text": "They released 394 training reports, 477 test reports and 877 unannotated reports.", "labels": [], "entities": []}, {"text": "After the challenge, only apart of the data was publicly released for research.", "labels": [], "entities": []}, {"text": "None relation was present in the data and not considered in the official evaluation.", "labels": [], "entities": []}, {"text": "The official challenge problems for all datasets compared models based on multi-class classification, but for the medical datasets, we were also interested in looking at the changes in model performance if we treated the task as a binary classification problem.", "labels": [], "entities": []}, {"text": "This was based on the rationale that in the drug literature, for example, pharmacologists would not want to sacrifice the ability to identify a potentially life threatening drug interaction pair, even if the type of the drug pair is not known.", "labels": [], "entities": []}, {"text": "Therefore, we report results for both multi-class and binary classification scenarios.", "labels": [], "entities": []}, {"text": "For clarity, we refer to them in the rest of the paper as classification and detection respectively.", "labels": [], "entities": [{"text": "classification and detection", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.6268510520458221}]}, {"text": "Picking the right evaluation metric fora dataset is critical, and it is important to choose a metric that has the biggest delta between different model performances for example types we care about.", "labels": [], "entities": []}, {"text": "Tables for different metric results for all datasets are provided in Appendix B. When using micro and macro statistics (precision, recall and F1), class imbalance dictates the one to pick.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9179961085319519}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9992795586585999}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9969610571861267}, {"text": "F1", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9976219534873962}]}, {"text": "Macro statistics are highly affected by imbalance, whereas micro statistics are able to recover well.", "labels": [], "entities": []}, {"text": "Despite suffering due to class imbalance, though, macro statistics maybe more appropriate than micro as they provide stronger discriminative capabilities by providing equal importance to classes of smaller sizes.", "labels": [], "entities": []}, {"text": "However, micro statistics are as discriminative as macro statistics in settings when the classes are relatively balanced.", "labels": [], "entities": []}, {"text": "We are going to talk about the classification tasks in the next two paragraphs.", "labels": [], "entities": [{"text": "classification", "start_pos": 31, "end_pos": 45, "type": "TASK", "confidence": 0.9613494873046875}]}, {"text": "Compared to semeval, ddi and i2b2 suffer from stark class imbalances.", "labels": [], "entities": []}, {"text": "semeval has a number of examples in non-Other classes ranging from 200 or 300 to 1000.", "labels": [], "entities": []}, {"text": "Other class has about 3000 examples which are not included in the official metric calculations.", "labels": [], "entities": []}, {"text": "ddi has one class with 228 examples, while the others have about 1000 examples.", "labels": [], "entities": [{"text": "ddi", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8697657585144043}]}, {"text": "The None class has 21,948 examples which is included for the official score calculations.", "labels": [], "entities": []}, {"text": "i2b2 has five classes in the 100-500 range, while the others contain about 2000 examples.", "labels": [], "entities": []}, {"text": "None is the largest class with 19,934 examples.", "labels": [], "entities": []}, {"text": "Using micro statistics is reasonable for i2b2 because the highly imbalanced class is not included in the calculations.", "labels": [], "entities": []}, {"text": "Therefore, this metric is able to be as discriminative as macro statistics.", "labels": [], "entities": []}, {"text": "For example, test set micro F1 between baseline and entity blinding techniques is 59.75 and 68.76, while that for macro F1 is 36.44 and 43.76.", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.6063519716262817}, {"text": "entity blinding", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.7266601175069809}]}, {"text": "In contrast, using micro statistics is a bad idea for ddi because the performance on the None class would drive most of the predictive results of the model.", "labels": [], "entities": []}, {"text": "For example, micro-F1 between baseline and NER blinding is 88.69 and 86.18, whereas macro-F1 is 65.53 and 57.22.", "labels": [], "entities": [{"text": "NER blinding", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.6647785902023315}]}, {"text": "semeval does not have a stark contrast between micro and macro scores due to Other class not being included in the calculation.", "labels": [], "entities": []}, {"text": "Using either metric to evaluate models is reasonable for this dataset.", "labels": [], "entities": []}, {"text": "The detection task does not suffer from such variations due to the lower class imbalance.", "labels": [], "entities": [{"text": "detection task", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8896481990814209}]}, {"text": "For example, ddi dataset micro-F1 between baseline and NER blinding model is 90.01 and 88.74, while macro-F1 is 81.74 and 79.03.", "labels": [], "entities": [{"text": "ddi dataset micro-F1", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.8062538107236227}, {"text": "NER blinding", "start_pos": 55, "end_pos": 67, "type": "TASK", "confidence": 0.3992539495229721}]}, {"text": "This further suggests that modeling differences and pre-processing differences cause more variation in performance in settings when the class imbalance is higher.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dataset information, with columns Rel =  number of relations, Eval = evaluation metric (all F1  scores), Agreement = Inter-annotator agreement, Det =  whether detection task from section 3.4 was evaluated  on. Rel column only includes relations used in offi- cial evaluation metric. ddi was built from two sep- arately annotated sources and therefore contains two  inter-annotator agreements.", "labels": [], "entities": [{"text": "Eval", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.977445662021637}, {"text": "F1  scores)", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9650330146153768}]}, {"text": " Table 4: Pre-processing techniques with CRCNN model. Row labels Original = simple tokenization and lower  casing of words, Punct = punctuation removal, Digit = digit removal and Stop = stop word removal. Test set results  at the top with cross validated results (average with standard deviation) below. All cross validated results are  statistically significant compared to Original pre-processing (p < 0.05) using a paired t-test except those marked  with a \u2022", "labels": [], "entities": []}, {"text": " Table 5: Modeling techniques with original pre-processing. Test set results at the top with cross validated results  (average with standard deviation) below. All cross validated results are statistically significant compared to CRCNN  model (p < 0.05) using a paired t-test except those marked with a \u2022. In terms of statistical significance, comparing  contextualized embeddings with each other reveals that BERT-tokens is equivalent to ELMo for i2b2, but for  semeval BERT-tokens is better than ELMo and for ddi BERT-tokens is better than ELMo only for detection.", "labels": [], "entities": [{"text": "BERT-tokens", "start_pos": 409, "end_pos": 420, "type": "METRIC", "confidence": 0.9952400922775269}, {"text": "BERT-tokens", "start_pos": 514, "end_pos": 525, "type": "METRIC", "confidence": 0.9606144428253174}]}, {"text": " Table 7: Additional experiments for i2b2. E = ELMo,  B = BERT-tokens, ent = entity blinding, piece = piece- wise pooling. All results are statistically significant  compared to BERT-tokens and ELMo models respec- tively from table 5 and piece + ent row is statistically  significant compared to piecewise pool model as well  as entity blinding model. These are all statistically sig- nificantly better than the CRCNN model from", "labels": [], "entities": [{"text": "BERT-tokens", "start_pos": 58, "end_pos": 69, "type": "METRIC", "confidence": 0.9970675110816956}, {"text": "entity blinding", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.709899052977562}, {"text": "BERT-tokens", "start_pos": 178, "end_pos": 189, "type": "METRIC", "confidence": 0.978069543838501}, {"text": "entity blinding", "start_pos": 329, "end_pos": 344, "type": "TASK", "confidence": 0.7270122319459915}]}, {"text": " Table 9: Best test set classification results for all  datasets, except ddi where detection results are men- tioned after the classification results. piece = Piece- wise pooling, ent = entity blinding, E = ELMo, B =  BERT-tokens. Result corresponds to F1 scores, macro  for semeval and ddi, but micro for i2b2.", "labels": [], "entities": [{"text": "BERT-tokens", "start_pos": 218, "end_pos": 229, "type": "METRIC", "confidence": 0.9844435453414917}, {"text": "F1 scores", "start_pos": 253, "end_pos": 262, "type": "METRIC", "confidence": 0.9709741771221161}]}]}