{"title": [{"text": "Guided Neural Language Generation for Automated Storytelling", "labels": [], "entities": [{"text": "Neural Language Generation", "start_pos": 7, "end_pos": 33, "type": "TASK", "confidence": 0.6463342607021332}]}], "abstractContent": [{"text": "Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries.", "labels": [], "entities": [{"text": "automated story plot generation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6871871277689934}]}, {"text": "Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into: (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence).", "labels": [], "entities": [{"text": "neural plot generation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.7117608288923899}]}, {"text": "However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences.", "labels": [], "entities": []}, {"text": "We present an ensemble-based model that generates natural language guided by events.", "labels": [], "entities": []}, {"text": "Our method outperforms the baseline sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "Additionally, we provide results fora full end-to-end automated story generation system, demonstrating how our model works with existing systems designed for the event-to-event problem.", "labels": [], "entities": [{"text": "story generation", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.729753315448761}]}], "introductionContent": [{"text": "Automated story plot generation is the problem of creating a sequence of main plot points fora story in a given domain.", "labels": [], "entities": [{"text": "Automated story plot generation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6516310572624207}]}, {"text": "Generated plots must remain consistent across the entire story, preserve longterm dependencies, and make use of commonsense and schematic knowledge.", "labels": [], "entities": []}, {"text": "Early work focused on symbolic planning and case-based reasoning) at the expense of manual world domain knowledge engineering.", "labels": [], "entities": [{"text": "symbolic planning", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8015628457069397}]}, {"text": "Neural-networkbased approaches to story and plot generation train a neural language model on a corpus of stories to predict the next character, word, or sentence in a sequence based on a history of tokens (.", "labels": [], "entities": [{"text": "story and plot generation", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.6378788650035858}]}, {"text": "The advantage of neural network based approaches is that there is no need for explicit domain modeling beyond providing a corpus of example stories.", "labels": [], "entities": []}, {"text": "The primary pitfall of neural language model approaches to story generation is that the space of stories that can be generated is huge, which in turn, implies that in a textual story corpora any given sentence will likely only be seen once.", "labels": [], "entities": [{"text": "story generation", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7539733350276947}]}, {"text": "propose the use of a semantic abstraction called events, demonstrating that it aids in reducing the sparsity of the dataset.", "labels": [], "entities": []}, {"text": "They define an event to be a unit of a story that creates a world state change; specifically, an event is a tuple containing a subject, verb, direct object, and some additional disambiguation tokens.", "labels": [], "entities": []}, {"text": "The event representation enables the decomposition of the plot generation task into two subproblems: event-to-event and event-to-sentence.", "labels": [], "entities": [{"text": "plot generation task", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.7695755163828532}]}, {"text": "Event-to-event is broadly the problem of generating the sequence of events that together comprise a plot.", "labels": [], "entities": []}, {"text": "A model for addressing this problem is also responsible for maintaining plot coherence and consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 91, "end_pos": 102, "type": "METRIC", "confidence": 0.9681903123855591}]}, {"text": "These events are abstractions and aren't human-readable.", "labels": [], "entities": []}, {"text": "Thus the second sub-problem, event-to-sentence, focuses on transforming these events into natural language sentences.", "labels": [], "entities": []}, {"text": "This second sub-problem can also be viewed as guided language generation, using a generated event as a guide.", "labels": [], "entities": [{"text": "guided language generation", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.6236001551151276}]}, {"text": "further propose that this latter event-to-sentence problem can bethought of as a translation task-translating from the language of events into natural language.", "labels": [], "entities": []}, {"text": "We find, however, that the sequence-to-sequence LSTM networks () that they chose to address the problem frequently ignore the input event and only generate text based on the orig-inal corpus, overwriting the plot-based decisions made during event-to-event.", "labels": [], "entities": []}, {"text": "There are two contributing factors.", "labels": [], "entities": []}, {"text": "Firstly, event-to-event models tend to produce previously-unseen events, which, when fed into the event-to-sentence model results in unpredictable behavior.", "labels": [], "entities": []}, {"text": "The mapping from an unseen event to a sentence is unknown to a basic sequence-to-sequence model.", "labels": [], "entities": []}, {"text": "Secondly, sentences are often only seen once in the entire corpus.", "labels": [], "entities": []}, {"text": "Despite being converted into events, the sparsity of the data means that each event is still likely seen a very limited number of times.", "labels": [], "entities": []}, {"text": "The contributions of the paper are thus twofold.", "labels": [], "entities": []}, {"text": "We present an ensemble-based system for eventto-sentence that allows for guided language generation and demonstrate that this outperforms a baseline sequence-to-sequence approach.", "labels": [], "entities": [{"text": "guided language generation", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7766855955123901}]}, {"text": "Additionally, we present the results of a full end-to-end story generation pipeline as originally proposed by, showing how all of the sub-systems can be integrated.", "labels": [], "entities": []}], "datasetContent": [{"text": "To aid in the performance of our story generation, we select a single genre: science fiction (scifi).", "labels": [], "entities": [{"text": "science fiction (scifi)", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.716792643070221}]}, {"text": "We scraped long-running science fiction TV show plot summaries from the fandom wiki service wikia.com.", "labels": [], "entities": [{"text": "science fiction TV show plot summaries", "start_pos": 24, "end_pos": 62, "type": "TASK", "confidence": 0.7264391481876373}]}, {"text": "This fandom wiki service contains longer and more detailed plot summaries than the dataset used in, both of which are qualities that we believe to be important for the overall story generation process.", "labels": [], "entities": [{"text": "story generation", "start_pos": 176, "end_pos": 192, "type": "TASK", "confidence": 0.7511814832687378}]}, {"text": "The corpus contains 2,276 stories in total, each an episode of a TV show.", "labels": [], "entities": []}, {"text": "The average story length is 89.23 sentences.", "labels": [], "entities": []}, {"text": "There are stories from 11 shows, with an average of 207 stories per show, from shows like Doctor Who, Futurama, and The X-Files.", "labels": [], "entities": []}, {"text": "The data was pre-processed to simplify alien names in order to aid named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6432547171910604}]}, {"text": "Then the sentences were split, partially following the \"split-and-pruned\" methodology of.", "labels": [], "entities": []}, {"text": "Sentences were split at S-bars and conjunctions separating S's, and the subject of the sentence was re-inserted in the new sentences.", "labels": [], "entities": []}, {"text": "Once the sentences were split, they were \"eventified\" as described in Section 3.", "labels": [], "entities": []}, {"text": "One benefit of having split sentences is that there is a higher chance of having a 1:1 correspondence between sentence and event, instead of a single sentence becoming multiple events.", "labels": [], "entities": []}, {"text": "After the data is fully prepared, it is split in a 8:1:1 ratio to create the training, validation, and testing sets respectively.", "labels": [], "entities": []}, {"text": "We perform two sets of experiments, one set evaluating our models on the event-to-sentence prob-lem by itself, and another set intended to evaluate the full storytelling pipeline.", "labels": [], "entities": []}, {"text": "Each of the models in the event-to-sentence ensemble are trained on the training set in the scifi corpus.", "labels": [], "entities": []}, {"text": "The exact training details for each of the models are as described above.", "labels": [], "entities": []}, {"text": "Note that we present results for the generalized sentences instead of the sentences after slot-filling, as shown in, to directly measure the output of the event-to-sentence ensemble.", "labels": [], "entities": []}, {"text": "Additionally, all of the models in the ensemble slot-fill the verb automatically-filling a VerbNet class with a verb of appropriate conjugation-except for the sentence templating model which does verb slotfilling during post-processing.", "labels": [], "entities": []}, {"text": "After the models are trained, we pick the cascading thresholds for the ensemble by running the validation set through each of the models and generating confidence scores.", "labels": [], "entities": []}, {"text": "This is done by running a grid search through a limited set of thresholds such that the overall BLEU-4 score () of the generated sentences in the validation set is maximized.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 96, "end_pos": 108, "type": "METRIC", "confidence": 0.9841984808444977}]}, {"text": "These thresholds are then frozen when running the final set of evaluations on the test set.", "labels": [], "entities": []}, {"text": "For the baseline sequence-to-sequence method, we decode our output with abeam size of 5.", "labels": [], "entities": []}, {"text": "We report perplexity and BLEU-4 scores, comparing against the gold standard from the test set.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9998053908348083}]}, {"text": "Perplexity is a measure of the predictive accuracy of a model and is calculated as: where x is a token in the text, and where Y is the vocabulary.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9500932693481445}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9064100980758667}]}, {"text": "Our BLEU-4 scores are naturally low (where higher is better) because of the creative nature of the task-good sentences may not use any of the ground-truth n-grams.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9989277720451355}]}, {"text": "The second experiment uses event sequences generated by our event-to-event system as summarized in Section 3.", "labels": [], "entities": []}, {"text": "Our event-to-event system requires goals in the form of verb classes.", "labels": [], "entities": []}, {"text": "For the science fiction data, common endings for stories are VerbNet classes like \"escape-51.1\", \"discover-84\", and \"get-13.5.1\".", "labels": [], "entities": []}, {"text": "In this paper, we will use the goal \"discover-84\".", "labels": [], "entities": []}, {"text": "We seed the event-to-event system with events extracted from the first sentences of stories found in the test set.", "labels": [], "entities": []}, {"text": "The system then generates a sequence of events until it reaches  Our results are presented in the form of three tables.", "labels": [], "entities": []}, {"text": "shows the perplexity, BLEU-4 scores, and average sentence length for event-tosentence on the testing set for each of the models, ensemble, and baseline.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995853304862976}]}, {"text": "Note that some of the models, such as the sentence templates, ignore the idea of a gold standard sentence and are thus poorly optimized towards perplexity and BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 159, "end_pos": 163, "type": "METRIC", "confidence": 0.9991077780723572}]}, {"text": "The ensemble, as predicted, still performs better than any of the individual models as it is designed to combine the models such that each of their weaknesses are minimized.", "labels": [], "entities": []}, {"text": "The average sentence lengths highlight the differences between the models, with the templates producing the shortest sentences and the finite state machine taking longer to generate sentences due to the constraints it needs to satisfy.", "labels": [], "entities": []}, {"text": "shows the confidence thresholds after tuning the ensemble.", "labels": [], "entities": []}, {"text": "The RetEdit and sentence template models need 80% confidence in their results, or the next model in the cascade is tried.", "labels": [], "entities": [{"text": "RetEdit", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7802180647850037}]}, {"text": "also shows how often each model in the ensemble is used generating sentences from the eventified testing corpus and from event-to-event.", "labels": [], "entities": []}, {"text": "RedEdit was heavily used on the test set, likely due the train and test sets having a similar distribution of data.", "labels": [], "entities": []}, {"text": "On the pipeline examples RetEdit is used much less-events generated by event-to-event maybe very different from those in the training set.", "labels": [], "entities": [{"text": "RetEdit", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.7308567762374878}]}, {"text": "A majority of the events: Event-to-sentence examples for each model.", "labels": [], "entities": []}, {"text": "\u2205 represents an empty parameter; <PRP> is a pronoun.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity and BLEU scores along with av- erage sentence lengths, thresholds, and utilization per- centages for event-to-sentence models on the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9997240900993347}, {"text": "av- erage sentence lengths", "start_pos": 48, "end_pos": 74, "type": "METRIC", "confidence": 0.8372159600257874}]}, {"text": " Table 2: Thresholds and utilization percentages for the  models on the test sets and on the events generated by  the event-to-event system.", "labels": [], "entities": [{"text": "Thresholds", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9971811771392822}]}]}