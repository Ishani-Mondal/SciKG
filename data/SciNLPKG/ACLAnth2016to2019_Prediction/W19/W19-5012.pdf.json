{"title": [{"text": "A distantly supervised dataset for automated data extraction from diagnostic studies", "labels": [], "entities": [{"text": "automated data extraction from diagnostic", "start_pos": 35, "end_pos": 76, "type": "TASK", "confidence": 0.8084558367729187}]}], "abstractContent": [{"text": "Systematic reviews are important in evidence based medicine, but are expensive to produce.", "labels": [], "entities": []}, {"text": "Automating or semi-automating the data extraction of index test, target condition, and reference standard from articles has the potential to decrease the cost of conducting systematic reviews of diagnostic test accuracy, but relevant training data is not available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9194366335868835}]}, {"text": "We create a distantly supervised dataset of approximately 90,000 sentences, and let two experts manually annotate a small subset of around 1,000 sentences for evaluation.", "labels": [], "entities": []}, {"text": "We evaluate the performance of BioBERT and logistic regression for ranking the sentences, and compare the performance for distant and direct supervision.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.5565601587295532}]}, {"text": "Our results suggest that distant supervision can work as well as, or better than direct supervision on this problem, and that distantly trained models can perform as well as, or better than human annotators.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Since our model output ranked sentences, rather than a binary classification, we evaluated all experiments in terms of average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9700074791908264}]}, {"text": "As a comparison, we also evaluated the average precision using the ranking given by the other annotator.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9749606251716614}]}, {"text": "In plain language, we tried to evaluate how useful it would have been for the expert to highlight sentences for each other.", "labels": [], "entities": []}, {"text": "The expert annotations were binary (Yes/No), rather than a ranking score, so we calculated the average precision by interpolating ties in the ranking.", "labels": [], "entities": [{"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9933423399925232}]}], "tableCaptions": [{"text": " Table 2: The number of sentences in our dataset, bro- ken into distantly annotated training and test sets, as  well as a manually annotated subset. Distant anno- tations for each data type were not available for all  studies, and the total number of labelled sentences are  therefore different for each data type.", "labels": [], "entities": []}]}