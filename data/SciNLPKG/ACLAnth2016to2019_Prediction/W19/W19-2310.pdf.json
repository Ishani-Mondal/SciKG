{"title": [{"text": "Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings", "labels": [], "entities": []}], "abstractContent": [{"text": "Despite advances in open-domain dialogue systems, automatic evaluation of such systems is still a challenging problem.", "labels": [], "entities": []}, {"text": "Traditional reference-based metrics such as BLEU are ineffective because there could be many valid responses fora given context that share no common words with reference responses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9944813251495361}]}, {"text": "A recent work proposed Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) to combine a learning-based metric, which predicts relatedness between a generated response and a given query, with reference-based metric; it showed high correlation with human judgments.", "labels": [], "entities": [{"text": "Unreferenced metric Blended Evaluation Routine (RUBER)", "start_pos": 45, "end_pos": 99, "type": "METRIC", "confidence": 0.7785212881863117}]}, {"text": "In this paper, we explore using contextualized word embed-dings to compute more accurate relatedness scores, thus better evaluation metrics.", "labels": [], "entities": []}, {"text": "Experiments show that our evaluation metrics outper-form RUBER, which is trained on static em-beddings.", "labels": [], "entities": [{"text": "RUBER", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9975278973579407}]}], "introductionContent": [{"text": "Recent advances in open-domain dialogue systems (i.e. chatbots) highlight the difficulties in automatically evaluating them.", "labels": [], "entities": []}, {"text": "This kind of evaluation inherits a characteristic challenge of NLG evaluation -given a context, there might be a diverse range of acceptable responses.", "labels": [], "entities": []}, {"text": "Metrics based on n-gram overlaps such as BLEU () and ROUGE), originally designed for evaluating machine translation and summarization, have been adopted to evaluate dialogue systems ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9986904263496399}, {"text": "ROUGE", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9900146722793579}, {"text": "machine translation", "start_pos": 96, "end_pos": 115, "type": "TASK", "confidence": 0.7250305116176605}, {"text": "summarization", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.6785494685173035}]}, {"text": "However, found a weak segment-level correlation between these metrics and human judgments", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the DailyDialog dataset 1 which contains high quality multi-turn conversations about daily life including various topics (, to train our dialogue system as well as the evaluation metrics.", "labels": [], "entities": [{"text": "DailyDialog dataset 1", "start_pos": 12, "end_pos": 33, "type": "DATASET", "confidence": 0.9537990490595499}]}, {"text": "This dataset includes almost 13k multi-turn dialogues between two parties splitted into 42,000/3,700/3,900 query-response pairs for train/test/validation sets.", "labels": [], "entities": []}, {"text": "We divided these sets into two parts, the first part for training dialogue system and the second part for training unreferneced metric.", "labels": [], "entities": []}, {"text": "In some group of language generation tasks such as machine translation and text summarization, ngrams overlapping metrics have a high correlation with human evaluation.", "labels": [], "entities": [{"text": "language generation tasks", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.80278346935908}, {"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8068457245826721}, {"text": "text summarization", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7368992269039154}]}, {"text": "BLEU and METEOR are primarily used for evaluating the quality of translated sentence based on computing n-gram precisions and harmonic mean of precision and recall, respectively).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9848241209983826}, {"text": "METEOR", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9805139899253845}, {"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.7408382892608643}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9958796501159668}]}, {"text": "ROUGE computes F-measure based on the longest common subsequence and is highly applicable for evaluating text summarization).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9447107315063477}, {"text": "text summarization", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7502370476722717}]}, {"text": "The main drawback of mentioned n-gram overlap metrics, which makes them inapplicable in dialogue system evaluation is that they don't consider the semantic similarity between sentences ().", "labels": [], "entities": []}, {"text": "These word overlapping metrics are not compatible with the nature of language generation, which allows a concept to be appeared in different sentences with no common n-grams, while they all share the same meaning.", "labels": [], "entities": [{"text": "language generation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7621110081672668}]}, {"text": "Beside the heuristic metrics, researchers recently tried to develop some trainable metrics for automatically checking the quality of generated responses.", "labels": [], "entities": []}, {"text": "trained a hierarchical neural network model called Automatic Dialogue Evaluation Model (ADEM) to predict the appropriateness score of dialogue responses.", "labels": [], "entities": []}, {"text": "For this purpose, they collected a training dataset by asking human about the informativeness score for various responses of a given context.", "labels": [], "entities": []}, {"text": "However, ADEM predicts highly correlated scores with human judgments in both sentence and system level, collecting human annotation by itself is an effortful and laborious task.", "labels": [], "entities": []}, {"text": "Kannan and Vinyals (2017) followed the GAN model's structure and trained a discriminator that tries to discriminate the model's generated response from human responses.", "labels": [], "entities": []}, {"text": "Even though they found discriminator can be useful for automatic evaluation systems, they mentioned that it cannot completely address the evaluation challenges in dialogue systems.", "labels": [], "entities": []}, {"text": "RUBER is another learnable metric, which considers both relevancy and similarity concepts for evaluation process.", "labels": [], "entities": [{"text": "RUBER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9678948521614075}]}, {"text": "Referenced metric of RUBER measures the similarity between vectors of generated and reference responses computed by pooling word embeddings, while unreferenced metric uses negative sampling to train the relevancy score of generated response to a given query.", "labels": [], "entities": [{"text": "RUBER", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.8224749565124512}]}, {"text": "Despite ADEM score, which is trained on human annotated dataset, RUBER is not limited to any human annotation.", "labels": [], "entities": [{"text": "ADEM score", "start_pos": 8, "end_pos": 18, "type": "METRIC", "confidence": 0.9584769904613495}, {"text": "RUBER", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.9574424624443054}]}, {"text": "In fact, training with negative samples makes RUBER to be more general.", "labels": [], "entities": [{"text": "RUBER", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.578217625617981}]}, {"text": "It is obvious that both referenced and unreferenced metrics are under the influence of word embeddings information.", "labels": [], "entities": []}, {"text": "In this work, we show that contextualized embeddings that include much more information about words and their context can have good effects on the accuracy of evaluation metrics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9979221224784851}]}], "tableCaptions": []}