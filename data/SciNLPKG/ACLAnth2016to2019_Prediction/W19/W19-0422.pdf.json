{"title": [{"text": "Using Wiktionary as a Resource for WSD : The Case of French Verbs", "labels": [], "entities": [{"text": "WSD : The Case of French Verbs", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.6001501253673008}]}], "abstractContent": [{"text": "As opposed to word sense induction, word sense disambiguation (WSD), whether supervised or semi-supervised, has the advantage of using interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al., 1993).", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.7234365145365397}, {"text": "word sense disambiguation (WSD)", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7679500132799149}]}, {"text": "In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9510575532913208}, {"text": "verb disambiguation", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7567988932132721}]}, {"text": "We first study the us-ability of Eurosense (Bovi et al.", "labels": [], "entities": []}, {"text": "2017), a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses.", "labels": [], "entities": [{"text": "Europarl (Kohen, 2005)", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9136243164539337}]}, {"text": "Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French.", "labels": [], "entities": []}, {"text": "While this perspective looked promising, our evaluation showed the annotated senses' quality was not sufficient for supervised WSD on French verbs.", "labels": [], "entities": [{"text": "WSD on French verbs", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7503519207239151}]}, {"text": "Instead, we propose to use Wiktionary, a col-laboratively edited, multilingual online dictionary, as anew resource for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.871632993221283}]}, {"text": "Wiktionary provides both sense inventory and manually sense tagged examples which can be used to train supervised and semi-supervised WSD systems.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9612328410148621}, {"text": "WSD", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9418433904647827}]}, {"text": "Yet, because senses' distribution differ in lexicographic examples as found in Wiktionary with respect to natural text, we then focus on studying the impact on WSD of the training data size and senses' distribution.", "labels": [], "entities": []}, {"text": "Using state-of-the art semi-supervised systems, we report experiments of wiktionary-based WSD for French verbs, evaluated on FrenchSemEval (FSE), anew dataset of French verbs manually annotated with wiktionary senses.", "labels": [], "entities": [{"text": "FrenchSemEval (FSE)", "start_pos": 125, "end_pos": 144, "type": "DATASET", "confidence": 0.8427427858114243}]}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a NLP task aiming at identifying the sense of a word occurrence from its context, given a predefined sense inventory.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7717286099990209}, {"text": "identifying the sense of a word occurrence from its context", "start_pos": 56, "end_pos": 115, "type": "TASK", "confidence": 0.7331959903240204}]}, {"text": "Although the task emerged almost 70 years ago with the first work on Automatic Machine Translation, it remains unresolved.", "labels": [], "entities": [{"text": "Automatic Machine Translation", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.6711310148239136}]}, {"text": "The recent breakthrough in neural net models allowed a better representation of the context and thus improved the quality of supervised disambiguation systems (.", "labels": [], "entities": []}, {"text": "Nevertheless, although WSD has the advantage of providing interpretable senses (as opposed to the unsupervised task of word sense induction), it also has the drawback of heavily relying on the availability and quality of sense-annotated data, even in the semi-supervised setting.", "labels": [], "entities": [{"text": "WSD", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8802080154418945}, {"text": "word sense induction", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.6860044201215109}]}, {"text": "Now, such data is available in English, essentially with SemCor (), a corpus manually sense-annotated with Wordnet senses.", "labels": [], "entities": []}, {"text": "But for most languages, sense disambiguated data are very rare or simply don't exist.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that manual semantic annotation is very costly in time and resources.", "labels": [], "entities": [{"text": "manual semantic annotation", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6239492396513621}]}, {"text": "Nevertheless, recently presented Eurosense, a multilingual automatically sense-disambiguated corpus extracted from Europarl ( and annotated with BabelNet ( senses.", "labels": [], "entities": [{"text": "Eurosense", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.9351571202278137}, {"text": "Europarl", "start_pos": 115, "end_pos": 123, "type": "DATASET", "confidence": 0.9845444560050964}]}, {"text": "In this article, we focus on supervised WSD for French verbs and investigate away to perform the task when no manually sense-annotated training data specifically designed for the task are available.We focus on verbs because they are known to be central to understanding tasks, but also known to lead to lower WSD performance (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9091166257858276}, {"text": "WSD", "start_pos": 309, "end_pos": 312, "type": "TASK", "confidence": 0.7178944945335388}]}, {"text": "In section 2 we report a study on the suitability of using Eurosense as training data for our task.", "labels": [], "entities": []}, {"text": "Because the results of our evaluation were inconclusive, we decided to explore Wiktionary, a free collaboratively edited multilingual online dictionary which provides a sense inventory and manually sense tagged examples, as resource for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 237, "end_pos": 240, "type": "TASK", "confidence": 0.8922464847564697}]}, {"text": "We give a general description of Wiktionary in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we present FrenchSemEval, anew manually sense annotated dataset for French verbs, to serve as evaluation data for WSD experiments using Wiktionary as sense inventory and training examples.", "labels": [], "entities": [{"text": "FrenchSemEval", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.8830546736717224}, {"text": "WSD", "start_pos": 128, "end_pos": 131, "type": "TASK", "confidence": 0.9678308367729187}]}, {"text": "Because senses' distribution differ in the lexicographic examples found in Wiktionary with respect to natural text, we provide in section 5 a descriptive statistical comparison of the wiktionary example corpus and SemCor.", "labels": [], "entities": []}, {"text": "The WSD first experiments are reported in section 6 and we provide an analysis of the results in section 7.", "labels": [], "entities": [{"text": "WSD", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8287551403045654}]}, {"text": "We finally conclude and give insights of future work in section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "The best-suited data for training a supervised WSD system is a corpus with sense tags for all content words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9593203663825989}]}, {"text": "Training on such a corpus benefits from basic frequency information found in the corpus.", "labels": [], "entities": []}, {"text": "This is particularly striking for WSD, as the \"most frequent sense\" baseline is known to be very high.", "labels": [], "entities": [{"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9580268859863281}]}, {"text": "In the case of French, as for the majority of languages, we lack such a corpus, and turn to the Wiktionary examples to serve as training examples fora significant portion of the lexicon.", "labels": [], "entities": []}, {"text": "Yet, because senses' distribution differ in the lexicographic examples found in Wiktionary with respect to natural text, we first provide some statistics fora running text sense-annotated corpus such as SemCor (for English) versus a lexicographic training set such as Wiktionary examples (for French).", "labels": [], "entities": []}, {"text": "The FTB contains 18500 sentences, from articles from Le Monde newspaper, and Sequoia contains 3099 sentences from Europarl, the European Medicine Agency, a regional newspaper (L'Est R\u00e9publicain) and fr-Wikipedia.", "labels": [], "entities": [{"text": "FTB", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8587093353271484}, {"text": "Le Monde newspaper", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.7971193194389343}, {"text": "Europarl", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9646188616752625}, {"text": "fr-Wikipedia", "start_pos": 199, "end_pos": 211, "type": "DATASET", "confidence": 0.9189919233322144}]}, {"text": "None of them had previous experience in annotation.: Ambiguity rates for verbs, in the English usual training set (SemCor) and usual evaluation sets, and in the French training set (Wiktionary) and evaluation set (FSE).", "labels": [], "entities": [{"text": "Ambiguity rates", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.9707055985927582}, {"text": "evaluation set (FSE)", "start_pos": 198, "end_pos": 218, "type": "METRIC", "confidence": 0.5867831885814667}]}, {"text": "AMBIG trainSI corresponds to using for the number of senses the sense inventory in the corresponding training corpus, whereas AMBIG fullSI corresponds to using the full sense inventory.", "labels": [], "entities": [{"text": "AMBIG trainSI", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.720475435256958}, {"text": "AMBIG fullSI", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.4679202437400818}]}, {"text": "We now turn to comparing the difficulty of the WSD task, when tested on English SenseEval datasets versus on FrenchSemEval.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 47, "end_pos": 55, "type": "TASK", "confidence": 0.9238275289535522}, {"text": "English SenseEval datasets", "start_pos": 72, "end_pos": 98, "type": "DATASET", "confidence": 0.8151844143867493}, {"text": "FrenchSemEval", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9664552211761475}]}, {"text": "Note that performance of WSD systems cannot be used for that purpose, given that it is not comparable across languages and datasets.", "labels": [], "entities": []}, {"text": "For a corpus consisting in a sequence of tokens t 1 . .", "labels": [], "entities": []}, {"text": "We report these metrics in.", "labels": [], "entities": []}, {"text": "When studying the difference between the \"fullSI\" versus \"trainSI\" modes, namely when using the full sense inventory versus that found in the training set, we have a different trend for the English corpora (containing natural text) and the French ones: for SemCor and the English evaluation sets, there is a drop of ambiguity in trainSI mode.", "labels": [], "entities": []}, {"text": "This illustrates the usual difficulty to cover rare senses in a corpus of natural text.", "labels": [], "entities": []}, {"text": "Note though that for the French corpora, based on the wiktionary inventory, there is almost no difference between the two modes of computation, illustrating that almost all senses have examples in wiktionary.", "labels": [], "entities": []}, {"text": "When comparing, for each language, the figures for the training corpora (SemCor and Wiktionary examples) and for the evaluation datasets, it can be noted that the average ambiguity per token is similar for training and evaluation datasets, but the average ambiguity per type is much smaller for the training corpora (3.24 for SemCor, and 1.74 for Wiktionary).", "labels": [], "entities": []}, {"text": "This is because the lexicon covered in the training corpora is much larger, and contains many more monosemic verbs.", "labels": [], "entities": []}, {"text": "As far as training corpora are concerned, it can be seen that the overall average ambiguity is higher for SemCor than for Wiktionary (e.g. in fullSI mode, 10.94 per token ambiguity for SemCor, versus 5.68 for Wiktionary).", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9838407635688782}]}, {"text": "It shows that the sense inventory for Wiktionary is slightly less ambiguous than Wordnet's (both for the senses found in SemCor, and overall).", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9523937702178955}]}, {"text": "To investigate the suitability of using Wiktionary for supervised WSD on French verbs, we evaluated state-of-the-art supervised WSD systems on FrenchSemEval, using the examples of Wiktionary's senses as training data.", "labels": [], "entities": [{"text": "FrenchSemEval", "start_pos": 143, "end_pos": 156, "type": "DATASET", "confidence": 0.9838536977767944}]}, {"text": "As for the representation of the instances we used two different models that we describe below.", "labels": [], "entities": []}, {"text": "We then applied a supervised disambiguation method to evaluate the performance of the models.", "labels": [], "entities": []}, {"text": "We first describe the models we used to obtain vector representations of the instances and the disambiguation algorithm we used for evaluation.", "labels": [], "entities": []}, {"text": "Then we propose several experiments based on FSE and finally we evaluate the models using Wiktionary as input for disambiguation.", "labels": [], "entities": [{"text": "FSE", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.5687752366065979}]}], "tableCaptions": [{"text": " Table 1: Statistics from French Wiktionary of the 04-20-2018 dump available via the tool of S\u00e9rasset", "labels": [], "entities": [{"text": "French Wiktionary of the 04-20-2018 dump", "start_pos": 26, "end_pos": 66, "type": "DATASET", "confidence": 0.9471262494723002}]}, {"text": " Table 2: Statistics for the FrenchSemEval corpus (FSE).", "labels": [], "entities": [{"text": "FrenchSemEval corpus (FSE)", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.9621573686599731}]}, {"text": " Table 3: Ambiguity rates for verbs, in the English usual training set (SemCor) and usual evaluation  sets, and in the French training set (Wiktionary) and evaluation set (FSE). AMBIG trainSI corresponds  to using for the number of senses the sense inventory in the corresponding training corpus, whereas  AMBIG fullSI corresponds to using the full sense inventory.", "labels": [], "entities": [{"text": "evaluation set (FSE)", "start_pos": 156, "end_pos": 176, "type": "METRIC", "confidence": 0.6316554307937622}, {"text": "AMBIG fullSI", "start_pos": 306, "end_pos": 318, "type": "DATASET", "confidence": 0.6106497496366501}]}, {"text": " Table 4: WSD accuracies when training on Wiktionary examples, and testing on FSE.", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9725372195243835}, {"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.7675800323486328}, {"text": "FSE", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.4928323030471802}]}, {"text": " Table 5: Training on FSE examples, with varying maximum number of examples per sense (N max ).  Top: WSD accuracies. Bottom: training / test sets statistics.", "labels": [], "entities": [{"text": "FSE", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.5783376097679138}]}]}