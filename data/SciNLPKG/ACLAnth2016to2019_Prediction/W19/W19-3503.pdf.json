{"title": [{"text": "Detecting harassment in real time as conversations develop", "labels": [], "entities": [{"text": "Detecting harassment", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8736852407455444}]}], "abstractContent": [{"text": "We developed a machine-learning-based method to detect video game players that harass teammates or opponents in chat earlier in the conversation.", "labels": [], "entities": [{"text": "detect video game players that harass teammates or opponents in chat earlier in the conversation", "start_pos": 48, "end_pos": 144, "type": "TASK", "confidence": 0.6298212170600891}]}, {"text": "This real-time technology would allow gaming companies to intervene during games, such as issue warnings or mut-ing or banning a player.", "labels": [], "entities": []}, {"text": "Ina proof-of-concept experiment on League of Legends data we compute and visualize evaluation metrics fora machine learning classifier as conversations unfold, and observe that the optimal precision and recall of detecting toxic players at each moment in the conversation depends on the confidence threshold of the classifier: the threshold should start low, and increase as the conversation unfolds.", "labels": [], "entities": [{"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9975429773330688}, {"text": "recall", "start_pos": 203, "end_pos": 209, "type": "METRIC", "confidence": 0.9963032007217407}]}, {"text": "How fast this sliding threshold should increase depends on the training set size.", "labels": [], "entities": []}], "introductionContent": [{"text": "In many online platforms that allow user interaction, verbal harassment has become commonplace.", "labels": [], "entities": []}, {"text": "For example, a survey by The Wikimedia Foundation showed that '38% of the 3,845 Wikimedia editors that were surveyed (an estimated total over 130,000) had experienced some form of harassment, and over half of those contributors felt a decrease in their motivation to contribute in the future'.", "labels": [], "entities": []}, {"text": "In this work we would like to focus on harassment in the online gaming community, where so-called toxic players are the subject of frequent media attention.", "labels": [], "entities": []}, {"text": "For some video games over 1% of the player base is estimated to be consistently toxic . Yet, for the game League of Legends, researchers found that this 1% of the player population only accounted for 5% of the toxic speech.", "labels": [], "entities": []}, {"text": "The former director of Riot Games' Player Behavior Unit attributes most toxicity to \"the average person just having a bad day\".", "labels": [], "entities": [{"text": "Riot Games' Player Behavior Unit", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.5478183150291442}]}, {"text": "As encounters with harassment area major predictor for players quitting a video game 2 , creating healthy communities is an important focus point for many video game developers . There has been an increase recently in the number of academic papers on automatically detecting harassment; see and van for overviews.", "labels": [], "entities": []}, {"text": "Many of these works focus on datasets with relatively short conversations (often <20 turns), consisting of longer utterances (often multiple full sentences).", "labels": [], "entities": []}, {"text": "As a result, most of these studies approach detecting verbal harassment as a classical text classification task, where each individual comment is considered a document on its own that should be assigned one of two or more categories.", "labels": [], "entities": [{"text": "detecting verbal harassment", "start_pos": 44, "end_pos": 71, "type": "TASK", "confidence": 0.8380647301673889}, {"text": "text classification task", "start_pos": 87, "end_pos": 111, "type": "TASK", "confidence": 0.7902717192967733}]}, {"text": "Conversations in video games, on the other hand, are different in nature: they consist of up to several hundreds of utterances, depending on the length of a match in the chosen video game, and these utterances are usually shorter, at least partly due to the restriction that the act of typing temporarily prevents players from playing.", "labels": [], "entities": []}, {"text": "For this reason, we focus lesson rating individual comments (an individual swearword or insult does not indicate harassment per se), but instead on detecting players within a match that consistently and knowingly harass teammates and/or opponents.", "labels": [], "entities": []}, {"text": "Self-policing of communities has been implemented by many game companies, among other things in the form of post-game ratings by other players.", "labels": [], "entities": []}, {"text": "Based on this information, video game developers already have a good estimate of which players behaved badly at what time, so an au-tomated system that makes this estimate retroactively would not be of much added value.", "labels": [], "entities": []}, {"text": "Instead, toxic players should be detected as the conversation develops, as early as possible, making it possible for gaming companies to intervene in one way or the other (like warning, muting or banning a player).", "labels": [], "entities": []}, {"text": "Translated to a machine learning task, this means that instances (e.g.: players) changeover time, as more information about the instances (more utterances) becomes available.", "labels": [], "entities": []}, {"text": "This leads to time as an extra dimension of interest for metrics like precision, recall and F-score: instead of presenting them as a single number, it should be represented how they change during the conversation.", "labels": [], "entities": [{"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.999293327331543}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.998803973197937}, {"text": "F-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9973124265670776}]}, {"text": "In this work we will apply this idea of detecting harassment over the course of a conversation at scale, to evaluate various (parameters of) classifiers during the course of a conversation.", "labels": [], "entities": []}, {"text": "More specifically, we will show that the optimal confidence threshold above which a player can be considered toxic increases as a conversation evolves, and that the rate of this increase interacts with the amount of training material.", "labels": [], "entities": []}], "datasetContent": [{"text": "As a dataset, we use 5000 conversations from the video game League of Legends, obtained from video game developer Riot Games, containing utterances by 48512 players.", "labels": [], "entities": []}, {"text": "Toxic players in this dataset were first identified by team mates and opponents, and later reassessed by other members of the community in a voting system called the 'Tribunal'.", "labels": [], "entities": []}, {"text": "Only cases where a so-called 'overwhelming majority' was reached were considered toxic.", "labels": [], "entities": []}, {"text": "An average conversation in our dataset consists of 186.77 utterances (standard deviation 122.01), as visualized in  Pilot experiments showed that the three main predictors for toxicity in this dataset are swear words, insults and talking about losing, all of which are present in this example ('fukin', 'u cunt', 'u jus let them kill me', respectively).", "labels": [], "entities": []}], "tableCaptions": []}