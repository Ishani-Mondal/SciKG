{"title": [{"text": "Words are Vectors, Dependencies are Matrices: Learning Word Embeddings from Dependency Graphs", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts.", "labels": [], "entities": [{"text": "Distributional Semantic Models (DSMs", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6002768695354461}]}, {"text": "Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations.", "labels": [], "entities": []}, {"text": "In this work, we propose anew dependency-based DSM.", "labels": [], "entities": []}, {"text": "The novelty of our model lies in associating an independent meaning representation, a matrix, with each dependency-label.", "labels": [], "entities": []}, {"text": "This allows it to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks.", "labels": [], "entities": []}, {"text": "In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.", "labels": [], "entities": []}], "introductionContent": [{"text": "Within computational linguistics, most research on word-meaning has been focusing on developing Distributional Semantic Models (DSMs), based on the hypothesis that a word's sense can be inferred from the contexts it appears in.", "labels": [], "entities": []}, {"text": "DSMs associate each word with a vector (a.k.a. word embedding) that encodes information about its co-occurrence with other words in the vocabulary.", "labels": [], "entities": []}, {"text": "In recent work, the most popular DSMs learn the embeddings using neural-network architectures.", "labels": [], "entities": []}, {"text": "In particular, the Skip-gram model of  has gained a lot of traction due to its efficiency and high quality representations.", "labels": [], "entities": []}, {"text": "Skip-gram embeddings are trained with an objective that forces them to be similar to the vectors of their words' contexts.", "labels": [], "entities": []}, {"text": "The latter, context-word vectors, area separate parameter of the model jointly learned along with the main target-word vectors.", "labels": [], "entities": []}, {"text": "Like most DSMs, 's model derives contexts of a word from a pre-defined window of words that surround it.", "labels": [], "entities": []}, {"text": "An alternative way of defining contexts in Skip-gram was explored by, who altered the model to accept contexts coming from a different vocabulary to that of the target-words.", "labels": [], "entities": []}, {"text": "The contexts were retrieved from targets' syntactic dependency relations and were a concatenation of the word linked to the target and the dependency-label.", "labels": [], "entities": []}, {"text": "Each context type was associated with an independent vector representation.", "labels": [], "entities": []}, {"text": "In contrast to Skip-gram, which captures relatedness 1 ,'s embeddings exhibited a more intuitive notion of similarity.", "labels": [], "entities": []}, {"text": "For example, the former regards the vector for abba, a popular Swedish pop group, as close to that for agnetha -a name of the group's member, while the latter considers it close to the vectors for other pop group names.", "labels": [], "entities": []}, {"text": "But's method of constructing contexts prevented their model from directly capturing how dependency types affect relations between target and context-words, as the labels were not associated with independent representations.", "labels": [], "entities": []}, {"text": "At the same time it intensifies the problems associated with data sparsity due to the large and fine-grained context-vocabulary.", "labels": [], "entities": []}, {"text": "In this work, we address the shortcomings of's approach by introducing the dependency-matrix model -a DSM which associates meaning with each type of dependency.", "labels": [], "entities": []}, {"text": "Instead: A graphical representation of Skip-gram displaying its parameters (left) and how P (C = 1|V 2 , V 3 ) and 1 \u2212 P (C = 1|V 2 , V 5 ) are calculated for the real and a negative context, respectively.", "labels": [], "entities": []}, {"text": "N is the embedding dimensionality. of simply appending the labels to context-words, they are promoted to a separate parameter of the model.", "labels": [], "entities": []}, {"text": "They become matrices, acting as linear maps on the context-word vectors and trained alongside the embeddings.", "labels": [], "entities": []}, {"text": "We hypothesised that this approach will lead to higher-quality representations, as it allows the model to capture important interactions between all three: labels, contexts and targets, while diminishing the data sparsity problem at the same time.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compared the performance of our model to that of Skip-gram (SG),'s model (LG) and Skip-gram for which the contexts are retrieved from the target's syntactic relations but the labels are disregarded (SGdep).", "labels": [], "entities": []}, {"text": "Our primary evaluation involved a number of standard word similarity datasets, as well as the RELPRON dataset (.", "labels": [], "entities": [{"text": "RELPRON dataset", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.7368389815092087}]}, {"text": "In addition, we tested our model's performance on the task of differentiating between similarity and relatedness relations and evaluated it qualitatively, by manually inspecting the types of captured similarities.", "labels": [], "entities": []}, {"text": "We also conduct experiments on three extrinsic tasks: dependency-parsing, chunking and part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.6894362717866898}]}, {"text": "Previous findings have shown the dependency embeddings are well suited for these tasks () so our primary objective here was to compare the performance of DM to that of LG.", "labels": [], "entities": []}, {"text": "All models were trained on the WikiWoods corpus (, which contains a 2008 Wikipedia snapshot, counting approximately 1.3M articles.", "labels": [], "entities": [{"text": "WikiWoods corpus", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9510851800441742}]}, {"text": "Throughout this work we used Universal Dependencies () with all training examples for the dependency models generated from WikiWoods parsed with the Stanford Neural Network Dependency parser).", "labels": [], "entities": []}, {"text": "Because words typically participate in only a few relations, the number of training data instances obtained from the parses was a third of the number obtained for Skip-gram.", "labels": [], "entities": [{"text": "Skip-gram", "start_pos": 163, "end_pos": 172, "type": "DATASET", "confidence": 0.9400691986083984}]}, {"text": "We tuned the embedding dimensionality for all tasks and the number of negative samples for REL-PRON and word similarity.", "labels": [], "entities": [{"text": "REL-PRON", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.995582640171051}]}, {"text": "For the extrinsic tasks we experimented with dimensions 50, 100 and 200, while for RELPRON and word similarity we experimented with setting m to 5, 10 and 15, and considered dimensions of 50, 100, 200 and 300.", "labels": [], "entities": [{"text": "RELPRON", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9825103878974915}, {"text": "word similarity", "start_pos": 95, "end_pos": 110, "type": "TASK", "confidence": 0.7248777598142624}]}, {"text": "In the case of word similarity we based the hyperparameter choice on the SimLex-999 results, as the similarity datasets do not provide standard development sets.", "labels": [], "entities": [{"text": "word similarity", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6958459615707397}]}, {"text": "In all training conditions we removed all tokens in the target and context vocabularies with frequencies less than 100.", "labels": [], "entities": []}, {"text": "For Skip-gram, we used the dynamic window of size n = 5.", "labels": [], "entities": []}, {"text": "Following the original word2vec tool 3 , we sampled the initial values of the input-embeddings from a uniform distribution over the range (-0.5, 0.5) and divided them by the embedding dimensionality.", "labels": [], "entities": []}, {"text": "We initialised the output-embeddings with zeros and dependency-matrices as identity matrices.", "labels": [], "entities": []}, {"text": "The models were trained in an online fashion using stochastic gradient updates, with the learning rate initially set to 0.025 and linearly decreased during training, based on the number of remaining training examples.", "labels": [], "entities": []}, {"text": "All of the models shared the same code-base, to ensure reliable comparison.", "labels": [], "entities": []}, {"text": "Word similarity evaluation is one of the most common methods of testing vector space semantic models.", "labels": [], "entities": [{"text": "Word similarity evaluation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7861282328764597}]}, {"text": "The similarity datasets consist of word-pairs associated with human-assigned similarity scores.", "labels": [], "entities": []}, {"text": "The task is to measure how well the model's scores, obtained using the learned embeddings, correlate with the gold-standard.", "labels": [], "entities": []}, {"text": "After the scores are computed for each pair, typically using the cosine similarity measure, the pairs are ranked by these values.", "labels": [], "entities": []}, {"text": "This ranking is then compared to the gold-standard ranking using Spearman's rank correlation coefficient.", "labels": [], "entities": [{"text": "Spearman's rank correlation coefficient", "start_pos": 65, "end_pos": 104, "type": "METRIC", "confidence": 0.575348436832428}]}, {"text": "The models performed best using 300 dimensional embeddings and 20 negative samples (apart from SG, which performed best with 15 samples).", "labels": [], "entities": []}, {"text": "As reported in, DM outperformed LG on all benchmarks and SGdep on all similarity datasets.", "labels": [], "entities": []}, {"text": "The latter demonstrates that the labels area valuable information source and our model's superiority over LG should not be attributed solely to decreasing data sparsity.", "labels": [], "entities": []}, {"text": "Despite being trained on three times less training examples than SG, DM and SGdep managed to beat SG on all datasets but MEN and WS353 (rel).", "labels": [], "entities": [{"text": "MEN", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.9502366185188293}, {"text": "WS353", "start_pos": 129, "end_pos": 134, "type": "DATASET", "confidence": 0.8333924412727356}]}, {"text": "Importantly, both of these datasets measure relatedness rather than similarity.", "labels": [], "entities": []}, {"text": "To inspect the types of similarities captured by the models we made a selection of four words from the vocabulary and analysed their closest neighbours according to each model.", "labels": [], "entities": []}, {"text": "The examples presented in confirm's findings, with SG capturing both similarity and relatedness and the dependency models demonstrating a bias towards similarity.", "labels": [], "entities": []}, {"text": "Good examples of that are the neighbours of abba or voldemort.", "labels": [], "entities": []}, {"text": "For the first SG selected words such as agnetha or lyngstadthe names of members of the Swedish pop group ABBA.", "labels": [], "entities": [{"text": "SG", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9609858393669128}]}, {"text": "The dependency models, on the other hand, associated abba with other music bands, such as A-ha, Roxette or Sizzla.", "labels": [], "entities": []}, {"text": "For voldemort, a villain from the Harry Potter series, the dependency models considered other fictional villains as most similar, while SG returned mostly names of characters from the books.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Word similarity evaluation results, the values are Spearman's correlation coefficients.", "labels": [], "entities": [{"text": "Word similarity evaluation", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.8078120549519857}]}, {"text": " Table 2: Results of MAP and MRR evaluation on the test and development sets of RELPRON.", "labels": [], "entities": [{"text": "MAP", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.5679561495780945}, {"text": "MRR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7040557861328125}, {"text": "RELPRON", "start_pos": 80, "end_pos": 87, "type": "METRIC", "confidence": 0.6022157073020935}]}, {"text": " Table 4: Results of the dependency parsing, part-of-speech tagging and chunking evaluation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8736861050128937}, {"text": "part-of-speech tagging", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7494808435440063}]}]}