{"title": [{"text": "Multi-headed Architecture Based on BERT for Grammatical Errors Correction", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9959871172904968}]}], "abstractContent": [{"text": "During last years we have seen tremendous progress in the development of NLP-related solutions and area in general.", "labels": [], "entities": []}, {"text": "It happened primarily due to emergence of pre-trained models based on the Transformer (Vaswani et al., 2017) architecture such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019).", "labels": [], "entities": [{"text": "GPT", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9406078457832336}, {"text": "BERT", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9824986457824707}]}, {"text": "Fine-tuned models containing these representations can achieve state-of-the-art results in many NLP-related tasks.", "labels": [], "entities": []}, {"text": "Given this, the use of pre-trained models in the Grammatical Error Correction (GEC) task seems reasonable.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC) task", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.6520012957709176}]}, {"text": "In this paper, we describe our approach to GEC using the BERT model for creation of encoded representation and some of our enhancements, namely, \"Heads\" are fully-connected networks which are used for finding the errors and later receive recommendation from the networks on dealing with a highlighted part of the sentence only.", "labels": [], "entities": [{"text": "GEC", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9417086839675903}, {"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9891151189804077}]}, {"text": "Among the main advantages of our solution is increasing the system productivity and lowering the time of processing while keeping the high accuracy of GEC results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9991299510002136}]}], "introductionContent": [{"text": "Modern state-of-the-art GEC models use the sequence-to-sequence (seq2seq) approach and Transformer Encoder-Decoder architecture ().", "labels": [], "entities": []}, {"text": "The core idea of seq2seq approach for GEC is the following: tokens from the source sequence are sent to the model input, and a similar sequence without errors is expected as an output.", "labels": [], "entities": [{"text": "GEC", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.8568896651268005}]}, {"text": "Transformer Decoder is auto-regressive, meaning that it predicts tokens one by one.", "labels": [], "entities": [{"text": "Transformer Decoder", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7543661594390869}]}, {"text": "Though this approach can represent the following challenges: (i) the sequence is reconstructed entirely, regardless of errors number; (ii) sentences are processed at low speed during inference; (iii) errors tend to accumulate since a failure in prediction of a single token can lead to a rupture of the entire chain in the network.", "labels": [], "entities": []}, {"text": "In this paper, we suggest an alternative approach for GEC with \"Multi-headed\" architecture that uses BERT as Encoder and specialized \"Heads\" networks enabling additional text processing based on particular error types.", "labels": [], "entities": [{"text": "GEC", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.696505069732666}, {"text": "BERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9932656288146973}]}, {"text": "In addition, particular Heads let us discover the error placement and come outwith error correction.", "labels": [], "entities": []}, {"text": "When we can create an effective dictionary for different types of errors suggested in ERRor ANnotation Toolkit (, such Heads as Punctuation, Articles and Case will be used.", "labels": [], "entities": [{"text": "ERRor ANnotation Toolkit", "start_pos": 86, "end_pos": 110, "type": "DATASET", "confidence": 0.627217580874761}]}, {"text": "Otherwise, if we cant create an effective dictionary, we are going to use a special \"highlight and decode\" technique in a bundle with Transformer Decoder to suggest a correction.", "labels": [], "entities": []}, {"text": "Also, we used Boosting Approach ( as an auxiliary step to improve the GEC within the framework of this competition.", "labels": [], "entities": [{"text": "Boosting Approach", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.5286310613155365}, {"text": "GEC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9482331275939941}]}], "datasetContent": [], "tableCaptions": []}