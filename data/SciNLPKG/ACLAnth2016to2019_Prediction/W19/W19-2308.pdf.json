{"title": [{"text": "Designing a Symbolic Intermediate Representation for Neural Surface Realization", "labels": [], "entities": [{"text": "Neural Surface Realization", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.7299954096476237}]}], "abstractContent": [{"text": "Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction.", "labels": [], "entities": [{"text": "repetition", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9904246926307678}]}, {"text": "This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs.", "labels": [], "entities": [{"text": "multi-stage neural generation", "start_pos": 84, "end_pos": 113, "type": "TASK", "confidence": 0.6483644247055054}]}, {"text": "We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 133, "end_pos": 144, "type": "DATASET", "confidence": 0.9684685468673706}]}, {"text": "Furthermore, by breaking out the surface realization step from typically end-to-end neural systems, we also provide a framework for non-neural content selection and planning systems to potentially take advantage of semi-supervised pretraining of neural surface realization models.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7633614540100098}]}], "introductionContent": [{"text": "For Natural Language Generation (NLG) systems to be useful in practice, they must generate utterances that are adequate, that is, the utterances need to include all relevant information.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8104748924573263}]}, {"text": "Furthermore the information should be expressed correctly and fluently, as if written by a human.", "labels": [], "entities": []}, {"text": "The rule and template based systems which dominate commercial NLG systems are limited in their generation capabilities and require much human effort to create but are reliably adequate and known for widespread usage in areas such as financial journalism and business intelligence.", "labels": [], "entities": []}, {"text": "By contrast, neural NLG systems need only a well collected dataset to train their models and generate fluent sounding utterances but have notable problems, such as hallucination and a general lack of adequacy (.", "labels": [], "entities": []}, {"text": "There was a marked absence of neural NLG in any of the finalist systems in either the 2017 or 2018 Alexa Prize.", "labels": [], "entities": [{"text": "Alexa Prize", "start_pos": 99, "end_pos": 110, "type": "DATASET", "confidence": 0.9444867670536041}]}, {"text": "Following prior work in the area of multi-stage neural NLG, and inspired by more traditional pipeline data-to-text generation), we present a system which splits apart the typically end-to-end data-driven neural model into separate utterance planning and surface realization models using a symbolic intermediate representation.", "labels": [], "entities": [{"text": "utterance planning and surface realization", "start_pos": 231, "end_pos": 273, "type": "TASK", "confidence": 0.673570454120636}]}, {"text": "We focus in particular on surface realization and introduce anew symbolic intermediate representation which is based on an underspecified universal dependency tree ().", "labels": [], "entities": [{"text": "surface realization", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7389630973339081}]}, {"text": "In designing our intermediate representation, we are driven by the following constraints: 1.", "labels": [], "entities": []}, {"text": "The intermediate representation must be suitable for processing with a neural system.", "labels": [], "entities": []}, {"text": "2. It must not make the surface realization task too difficult because we are interested in understanding the limitations of neural generation even under favorable conditions.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7393540740013123}]}, {"text": "3. It must be possible to parse a sentence into this representation so that a surface realization training set can be easily augmented with additional in-domain data.", "labels": [], "entities": []}, {"text": "Focusing on English and using the E2E dataset, we parse the reference sentences into our intermediate representation.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9740753471851349}]}, {"text": "We then train a surface realization model to generate from this representation, comparing the resulting strings with the reference using both automatic and manual evaluation.", "labels": [], "entities": []}, {"text": "We find that the quality of the generated text is high, achieving a BLEU score of 82.47.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9802628755569458}]}, {"text": "This increases to 83.38 when we augment the training data with sentences from the TripAdvisor corpus.", "labels": [], "entities": [{"text": "TripAdvisor corpus", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9786202907562256}]}, {"text": "A manual error analysis shows that in only a very small proportion (\u223c5%) of the output sentences, the meaning of the reference is not fully recovered.", "labels": [], "entities": []}, {"text": "This high level of adequacy is expected since the intermediate representations are generated directly from the reference sentences.", "labels": [], "entities": []}, {"text": "An analysis of a sample of the adequate sentences shows that readability is on a par with the reference sentences.", "labels": [], "entities": []}, {"text": "Having established that surface realization from our intermediate representation achieves sufficiently high performance, we then test its efficacy as part of a pipeline system.", "labels": [], "entities": []}, {"text": "On the E2E task, our system scores higher on automated results than the winner of the E2E challenge (.", "labels": [], "entities": []}, {"text": "The use of additional training data in the surface realization stage results in further gains.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7270508706569672}]}, {"text": "These encouraging results suggest that pipelines can work well in the context of neural NLG.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets Experiments were performed with the E2E dataset (.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9869101345539093}]}, {"text": "contains an example of the E2E input.", "labels": [], "entities": []}, {"text": "The E2E dataset contains a training set of 42,061 pairs of meaning representations and utterances.", "labels": [], "entities": [{"text": "E2E dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9591521620750427}]}, {"text": "Training data for the surface realization model was augmented, for some experiments, with the TripAdvisor corpus (, which was filtered for sentences with a 100% vocabulary overlap with the E2E corpus and a sentence length between 5 and 30 tokens, resulting in an additional 209,823 sentences, with an average sentence length of 10 tokens.", "labels": [], "entities": [{"text": "TripAdvisor corpus", "start_pos": 94, "end_pos": 112, "type": "DATASET", "confidence": 0.9444393813610077}, {"text": "E2E corpus", "start_pos": 189, "end_pos": 199, "type": "DATASET", "confidence": 0.8977077603340149}]}, {"text": "By comparison the E2E corpus had sentence lengths ranging between 1 and 59 tokens with an average sentence length of 13 tokens.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9655237793922424}]}, {"text": "Both corpora were sentence tokenized and parsed by the Stanford NLP universal dependency parser (.", "labels": [], "entities": []}, {"text": "The parsed sentences in CoNLL-U format were then further processed by a special deep UUD parser ().", "labels": [], "entities": []}, {"text": "Utterances from the E2E corpus were delexicalised to anonymize restaurant names in both the name and near slots of the meaning representation.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 20, "end_pos": 30, "type": "DATASET", "confidence": 0.9245701134204865}]}, {"text": "All tokens were lower cased before training.", "labels": [], "entities": []}, {"text": "Models For the neural NLG pipeline system we train two separate encoder-decoder models using the neural machine translation framework Open-NMT (.", "labels": [], "entities": []}, {"text": "We trained two separate encoder-decoder models for surface realization and content selection.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7273001968860626}, {"text": "content selection", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.7163796424865723}]}, {"text": "However both used the same hyperparameters.", "labels": [], "entities": []}, {"text": "A single layer LSTM) with RNN size 450 and word vector size 300 was used.", "labels": [], "entities": []}, {"text": "The models were trained using ADAM () with a learning rate of 0.001.", "labels": [], "entities": [{"text": "ADAM", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.8785372376441956}, {"text": "learning rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.971070796251297}]}, {"text": "The only difference between the two models was that the surface realization model was trained with a copy attention mechanism (.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7235338985919952}]}, {"text": "For the full E2E task a single planning model was trained on the E2E corpus.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.9557316303253174}]}, {"text": "However two different surface realization models were compared; one trained solely on sentences from the E2E corpus and another trained on a combined corpus of E2E and TripAdvisor sentences.", "labels": [], "entities": [{"text": "E2E corpus", "start_pos": 105, "end_pos": 115, "type": "DATASET", "confidence": 0.9619261026382446}]}, {"text": "For baselines on the full E2E task we compare with two encoder-decoder models which both use semantic rerankers on their generated utterances; TGen (Du\u0161ek and Jurcicek, 2016) the baseline system for the E2E challenge and Slug2Slug (Juraska et al., 2018) the winning system of the E2E challenge.", "labels": [], "entities": []}, {"text": "Automated Evaluation The E2E task is evaluated using an array of automated metrics 2 ; BLEU (), NIST), METEOR (Lavie and Agarwal, 2007), ROUGE), and CIDEr ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9989814162254333}, {"text": "METEOR", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9867033958435059}, {"text": "ROUGE", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.9881354570388794}]}, {"text": "The two surface realization models were evaluated on how well they were able to realize sentences from the E2E validation set using silver parsed intermediate representations.", "labels": [], "entities": [{"text": "E2E validation set", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.8666277130444845}]}, {"text": "We report BLEU-4 scores 3 for the silver parse generated texts from the surface realization models.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9989250302314758}]}, {"text": "In both the E2E and WebNLG challenge (Shimorina, 2018) it was found that automated results did not correlate with the human evaluation on the sentence level.", "labels": [], "entities": []}, {"text": "However in the Surface Realization shared task correlation between BLEU score and human evaluation was noted to be highly significant ().", "labels": [], "entities": [{"text": "Surface Realization", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8472661972045898}, {"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.981658011674881}]}, {"text": "Manual Analysis The importance of using human evaluation to get a more accurate understanding of the quality of text generated by an NLG system cannot be overstated.", "labels": [], "entities": [{"text": "Manual Analysis", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.690229132771492}]}, {"text": "We perform human evaluation on the outputs of the surface realization model with a silver parse of the original utterances as input.", "labels": [], "entities": []}, {"text": "We evaluate the outputs first in terms of meaning similarity and then readability and fluency.", "labels": [], "entities": []}, {"text": "To evaluate the surface realization model we compare generated utterances with the human references.", "labels": [], "entities": []}, {"text": "For the meaning similarity human evaluation we remove sentences with no differences, only differences involving the presence or absence of hyphens or only capitalization differences.", "labels": [], "entities": []}, {"text": "We evaluate meaning similarity between two utterances as whether they contain the same meaning.", "labels": [], "entities": []}, {"text": "We treat this a binary Yes / No decision as the generated utterances are using a silver parse and ought to be able to reconstruct a sentence that, while possibly differently structured, does express the same meaning.", "labels": [], "entities": []}, {"text": "We manually analyze failure cases where semantic similarity is not achieved to discover where the issues arise.", "labels": [], "entities": []}, {"text": "There maybe failures in the method of obtaining the intermediate representation, in the surface realization model or some other issue with the intermediate representation.", "labels": [], "entities": []}, {"text": "We then pass on only those generated utterances deemed to have the same meaning with the reference utterance into the next stage of readability evaluation.", "labels": [], "entities": []}, {"text": "To evaluate readability we perform pairwise comparisons between generated utterances and reference utterances.", "labels": [], "entities": []}, {"text": "We randomize the order during evaluation so it is not clear what the origin of a particular utterance is.", "labels": [], "entities": []}, {"text": "We define readability, sometimes called fluency, as how well a given utterances reads, \"is it fluent English or does it have grammatical errors, awkward constructions, etc.\").", "labels": [], "entities": []}, {"text": "By investigating readability of utterances with meaning similarity, we hope to see how the surface realization model performs compared with a human written utterance.", "labels": [], "entities": []}, {"text": "The surface realization model is required to at least match human level performance in order to be usable, if it does not then we need to investigate where it fails and why.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7305529862642288}]}, {"text": "We used Prodigy ( as our data annotation tool.", "labels": [], "entities": [{"text": "Prodigy", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.8897436261177063}]}], "tableCaptions": [{"text": " Table 2: Surface Realization of 8024 sentences in the  E2E validation set", "labels": [], "entities": [{"text": "Surface Realization of 8024 sentences", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.788259994983673}, {"text": "E2E validation set", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.9000329971313477}]}, {"text": " Table 3: Manual analysis of a subset of remaining sen- tences from the 8024 sentences in the E2E validation  set", "labels": [], "entities": [{"text": "E2E validation  set", "start_pos": 94, "end_pos": 113, "type": "DATASET", "confidence": 0.8948912620544434}]}, {"text": " Table 4: Automated results on end-to-end task", "labels": [], "entities": []}]}