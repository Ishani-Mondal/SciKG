{"title": [{"text": "Regression or classification? Automated Essay Scoring for Norwegian", "labels": [], "entities": [{"text": "Regression or classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6533992886543274}, {"text": "Automated Essay Scoring", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.6779167056083679}]}], "abstractContent": [{"text": "In this paper we present first results for the task of Automated Essay Scoring for Norwe-gian learner language.", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.6830573976039886}, {"text": "Norwe-gian learner language", "start_pos": 83, "end_pos": 110, "type": "DATASET", "confidence": 0.9212674895922343}]}, {"text": "We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification.", "labels": [], "entities": [{"text": "prediction of essay scoring", "start_pos": 316, "end_pos": 343, "type": "TASK", "confidence": 0.6907214298844337}, {"text": "native language identification", "start_pos": 348, "end_pos": 378, "type": "TASK", "confidence": 0.6482868095239004}]}, {"text": "We find that a GRU-based attention model trained in a single-task setting performs best at the AES task.", "labels": [], "entities": [{"text": "AES", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.7460131645202637}]}], "introductionContent": [{"text": "Automated essay scoring (AES), in the literature also referred to as Assessment of Proficiency or Automated Text Scoring (ATS), considers the task of assigning a grade to a free form text, often responding to a specific prompt.", "labels": [], "entities": [{"text": "Automated essay scoring (AES)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7305437674125036}, {"text": "Assessment of Proficiency or Automated Text Scoring (ATS)", "start_pos": 69, "end_pos": 126, "type": "TASK", "confidence": 0.6396147310733795}]}, {"text": "Automation of this assessment task has clear applications in language education, where second language learners can receive feedback as to which proficiency level they might be on, for instance in relation to the Common European Framework of Reference (CEFR) level.", "labels": [], "entities": [{"text": "Common European Framework of Reference (CEFR) level", "start_pos": 213, "end_pos": 264, "type": "DATASET", "confidence": 0.6726816958851285}]}, {"text": "This may help learners who want to take language examination to find the appropriate timing and level of testing, since an examination can be both an economical and logistical inconvenience.", "labels": [], "entities": [{"text": "timing", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9843499660491943}]}, {"text": "Automation also allows students to receive feedback quicker and more frequently.", "labels": [], "entities": []}, {"text": "Recent work on the AES task has used both non-neural, feature-rich approaches that make use of a variety of linguistic features, and neural end-to-end architectures.", "labels": [], "entities": [{"text": "AES task", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.8766986727714539}]}, {"text": "Previous work has furthermore adopted different formulations of the task, either as a regression problem ( or a classification task.", "labels": [], "entities": []}, {"text": "Most previous work however, with a few noteworty exceptions, has been focused on English learner language.", "labels": [], "entities": []}, {"text": "In this paper we present first results for automated essay scoring of Norwegian learner language.", "labels": [], "entities": []}, {"text": "We make use of the ASK corpus of learner language (), with added CEFR labels, and compare and contrast different formulations of the task, a number of different machine learning architectures and different input representations and further experiment with a multi-task setting with Native Language Identification as auxiliary task.", "labels": [], "entities": [{"text": "ASK corpus of learner language", "start_pos": 19, "end_pos": 49, "type": "DATASET", "confidence": 0.8574751138687133}, {"text": "CEFR", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9438210725784302}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "We present related work in section 2 and goon to describe the Norwegian learner corpus (ASK) in section 3.", "labels": [], "entities": [{"text": "goon", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.8766658902168274}, {"text": "Norwegian learner corpus (ASK)", "start_pos": 62, "end_pos": 92, "type": "DATASET", "confidence": 0.9038996199766794}]}, {"text": "We describe the aims of this paper in section 4.1 and describe the data preprocessing and the creation of training, development, and test datasets in section 4.2.", "labels": [], "entities": []}, {"text": "We present the results of non-neural linear models in section 5 and CNNs and Gated-RNNs on the development dataset in section 6.", "labels": [], "entities": []}, {"text": "Then, we briefly describe the results of our experiments with native language identification in section 7 and the subsequent results of multitask experiments in section 8.", "labels": [], "entities": [{"text": "native language identification", "start_pos": 62, "end_pos": 92, "type": "TASK", "confidence": 0.6584230860074362}]}, {"text": "Finally, we report the results of the best linear and neural models on the held-out test data in section 9.", "labels": [], "entities": []}, {"text": "We summarize and conclude the paper in section 10.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Distributions of first languages for each test  level in ASK. Texts in each test level for all L1. Lan- guages which are not included in our CEFR-labeled  dataset are listed in parentheses.", "labels": [], "entities": [{"text": "CEFR-labeled  dataset", "start_pos": 151, "end_pos": 172, "type": "DATASET", "confidence": 0.9906673729419708}]}, {"text": " Table 2: F 1 -scores of various classifiers. LogReg is lo- gistic regression, SVC is support vector classification,  and SVR is support vector regression.", "labels": [], "entities": [{"text": "F 1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9630050659179688}, {"text": "support vector classification", "start_pos": 86, "end_pos": 115, "type": "TASK", "confidence": 0.7186120947202047}]}, {"text": " Table 3: F 1 scores of CNN classifiers on AES. +POS:  Multi-channel input with both words and UPOS tags.  Reg: Regression model. Rank: Ordinal regression.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9628955523173014}, {"text": "AES", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8764944076538086}, {"text": "Rank", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9565615653991699}]}, {"text": " Table 4: F 1 scores of GRU classifiers on AES. BiGRU  is birectional GRU; Attn is attention model.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9918684959411621}, {"text": "AES", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.9211435317993164}, {"text": "BiGRU", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.978650689125061}, {"text": "Attn", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9493720531463623}]}, {"text": " Table 5: F 1 scores of Pre-trained, BiGRU classifier at  the NLI task consisting of 7 classes.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9826950430870056}]}, {"text": " Table 6: Descriptions of the CNN and RNN models  showing different settings.", "labels": [], "entities": [{"text": "CNN", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.8778315186500549}]}, {"text": " Table 7: Results from evaluation on the held-out test  set. SVR is support vector regression. Hyperparame- ters for RNN1 and RNN2 are found in table 6. Multi- task models use an auxiliary task weight of 0.1.", "labels": [], "entities": []}]}