{"title": [{"text": "Analyzing the Structure of Attention in a Transformer Language Model", "labels": [], "entities": [{"text": "Analyzing the Structure of Attention", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8143970489501953}]}], "abstractContent": [{"text": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model.", "labels": [], "entities": []}, {"text": "We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus.", "labels": [], "entities": []}, {"text": "We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers.", "labels": [], "entities": []}, {"text": "We also find that the deepest layers of the model capture the most distant relationships.", "labels": [], "entities": []}, {"text": "Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.", "labels": [], "entities": []}], "introductionContent": [{"text": "Contextual word representations have recently been used to achieve state-of-the-art performance across a range of language understanding tasks (.", "labels": [], "entities": []}, {"text": "These representations are obtained by optimizing a language modeling (or similar) objective on large amounts of text.", "labels": [], "entities": []}, {"text": "The underlying architecture maybe recurrent, as in ELMo (, or based on multi-head self-attention, as in OpenAI's GPT () and BERT (, which are based on the Transformer (.", "labels": [], "entities": [{"text": "OpenAI's GPT", "start_pos": 104, "end_pos": 116, "type": "DATASET", "confidence": 0.6454157034556071}, {"text": "BERT", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9905799031257629}]}, {"text": "Recently, the GPT-2 model (  outperformed other language models in a zeroshot setting, again based on self-attention.", "labels": [], "entities": []}, {"text": "An advantage of using attention is that it can help interpret the model by showing how the model attends to different parts of the input.", "labels": [], "entities": []}, {"text": "Various tools have been developed to visualize attention in NLP models, ranging from attention matrix heatmaps () to bipartite graph representations (.", "labels": [], "entities": []}, {"text": "A visualization tool designed specifically for multi-head self-attention in the Transformer) was introduced in.", "labels": [], "entities": [{"text": "Transformer", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.88910973072052}]}, {"text": "We extend the work of Jones, by visualizing attention in the Transformer at three levels of granularity: the attention-head level, the model level, and the neuron level.", "labels": [], "entities": []}, {"text": "We also adapt the original encoder-decoder implementation to the decoder-only GPT-2 model, as well as the encoder-only BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9668574333190918}]}, {"text": "In addition to visualizing attention for individual inputs to the model, we also analyze attention in aggregate over a large corpus to answer the following research questions: \u2022 Does attention align with syntactic dependency relations?", "labels": [], "entities": []}, {"text": "\u2022 Which attention heads attend to which partof-speech tags?", "labels": [], "entities": []}, {"text": "\u2022 How does attention capture long-distance relationships versus short-distance ones?", "labels": [], "entities": []}, {"text": "We apply our analysis to the GPT-2 small pretrained model.", "labels": [], "entities": [{"text": "GPT-2", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.8358322381973267}]}, {"text": "We find that attention follows dependency relations most strongly in the middle layers of the model, and that attention heads target particular parts of speech depending on layer depth.", "labels": [], "entities": []}, {"text": "We also find that attention spans the greatest distance in the deepest layers, but varies significantly between heads.", "labels": [], "entities": []}, {"text": "Finally, our method for extracting exemplar sentences yields many intuitive patterns.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focused our analysis on text from English Wikipedia, which was not included in the training When computing entropy, we exclude attention to the first (null) token (see Section 5.2.3) and renormalize the remaining weights.", "labels": [], "entities": []}, {"text": "We exclude tokens that focus over 90% of attention to the first token, to avoid a disproportionate influence from the remaining attention from these tokens.", "labels": [], "entities": []}, {"text": "We first extracted 10,000 articles, and then sampled 100,000 sentences from these articles.", "labels": [], "entities": []}, {"text": "For the qualitative analysis described later, we used the full dataset; for the quantitative analysis, we used a subset of 10,000 sentences.", "labels": [], "entities": []}], "tableCaptions": []}