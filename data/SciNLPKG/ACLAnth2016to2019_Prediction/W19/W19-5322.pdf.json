{"title": [{"text": "CUNI Submission for Low-Resource Languages in WMT News 2019", "labels": [], "entities": [{"text": "WMT News", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9649390280246735}]}], "abstractContent": [{"text": "This paper describes the CUNI submission to the WMT 2019 News Translation Shared Task for the low-resource languages: Gujarati-English and Kazakh-English.", "labels": [], "entities": [{"text": "WMT 2019 News Translation Shared Task", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7273339678843816}]}, {"text": "We participated in both language pairs in both translation directions.", "labels": [], "entities": []}, {"text": "Our system combines transfer learning from different high-resource language pair followed by training on backtranslated mono-lingual data.", "labels": [], "entities": []}, {"text": "Thanks to the simultaneous training in both directions, we can iterate the back-translation process.", "labels": [], "entities": []}, {"text": "We are using the Transformer model in a constrained submission.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, the rapid development of Neural Machine Translations (NMT) systems led to the claims, that human parity has been reached) on a high-resource language pair Chinese-English.", "labels": [], "entities": [{"text": "Neural Machine Translations (NMT)", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8277799983819326}]}, {"text": "However, NMT systems tend to be very data hungry as showed the NMT lacks behind phrase based approaches in the low-resource scenarios.", "labels": [], "entities": []}, {"text": "This lead to the rise of attention in the low-resource NMT in recent years, where the goal is to improve the performance of a language pair that have only a limited available parallel data.", "labels": [], "entities": []}, {"text": "In this paper, we describe our approach to low-resource NMT.", "labels": [], "entities": [{"text": "NMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8484974503517151}]}, {"text": "We use standard Transformerbig model ( and apply two techniques to improve the performance on the low-resource language, namely transfer learning (  and iterative backtranslation (.", "labels": [], "entities": []}, {"text": "A model trained solely on the authentic parallel data of the low-resource NMT model has poor performance, thus using it directly for the backtranslation of monolingual data lead to poor translation.", "labels": [], "entities": []}, {"text": "Hence the transfer learning is as a great tool to first improve the performance of the NMT system later used for backtranslating the monolingual data.", "labels": [], "entities": []}, {"text": "The structure of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe the transfer learning and backtranslation, followed by a description of used datasets and the NMT model architecture.", "labels": [], "entities": []}, {"text": "Next, we present our experiments, final submissions, and followup analysis of synthetic training data usage.", "labels": [], "entities": []}, {"text": "The paper is concluded in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the datasets used to train our final models.", "labels": [], "entities": []}, {"text": "All our models were trained only on the data allowed for the WMT 2019 News shared task.", "labels": [], "entities": [{"text": "WMT 2019 News shared task", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.827500295639038}]}, {"text": "Hence our submission is constrained.", "labels": [], "entities": []}, {"text": "All used training data are presented in.", "labels": [], "entities": []}, {"text": "We used all available parallel corpora allowed and accessible by WMT 2019 except for the CzechEnglish language pair, where we used only the CzEng 1.7.", "labels": [], "entities": [{"text": "WMT 2019", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9501997530460358}, {"text": "CzechEnglish language pair", "start_pos": 89, "end_pos": 115, "type": "DATASET", "confidence": 0.9509476025899252}]}, {"text": "We have not clean any of the parallel corpora except deduplication and removing pairs with the same source and target translations in Wiki Titles dataset.", "labels": [], "entities": [{"text": "Wiki Titles dataset", "start_pos": 134, "end_pos": 153, "type": "DATASET", "confidence": 0.9162169297536215}]}, {"text": "We used official WMT testsets from previous years as a development set.", "labels": [], "entities": [{"text": "WMT testsets", "start_pos": 17, "end_pos": 29, "type": "DATASET", "confidence": 0.7496053278446198}]}, {"text": "The year 2013 for Czech-English and Russian-English.", "labels": [], "entities": []}, {"text": "For the Gujarati-English, we used the official 2019 development set.", "labels": [], "entities": []}, {"text": "Lastly, for the Kazakh-English, the organizers do not provide any development set.", "labels": [], "entities": []}, {"text": "Therefore we separated the first 2000 sentence pairs from the News Commentary training set and used as our development set.", "labels": [], "entities": [{"text": "News Commentary training set", "start_pos": 62, "end_pos": 90, "type": "DATASET", "confidence": 0.9142870903015137}]}, {"text": "The monolingual data used for the backtranslation are shown in.", "labels": [], "entities": []}, {"text": "We use all available monolingual data for Gujarati and Kazakh.", "labels": [], "entities": []}, {"text": "For the English, we did not use all available English monolingual data due to the backtranslation process being time-consuming, therefore we use only the 2018 News Crawl.", "labels": [], "entities": [{"text": "2018 News Crawl", "start_pos": 154, "end_pos": 169, "type": "DATASET", "confidence": 0.7873426278432211}]}, {"text": "The available monolingual corpora are usually of high quality.", "labels": [], "entities": []}, {"text": "However, we noticed that the Common Crawl contains many sentences in a different language and also long paragraphs, that are not useful for sentence level translation.", "labels": [], "entities": [{"text": "Common Crawl", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9179172217845917}, {"text": "sentence level translation", "start_pos": 140, "end_pos": 166, "type": "TASK", "confidence": 0.6370729605356852}]}, {"text": "Therefore, we used language identification tool by on the Common Crawl corpus and dropped all sentences automatically annotated as a different language than Gujarati or Kazakh respectively.", "labels": [], "entities": [{"text": "language identification", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.706034392118454}, {"text": "Common Crawl corpus", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9259074926376343}]}, {"text": "Followed by splitting the remaining sentences that are longer than 100 words on all full stops, which led to an increase of sentences.", "labels": [], "entities": []}, {"text": "In this section, we describe our experiments starting with hyperparameter search, our training procedure, and supporting experiments.", "labels": [], "entities": [{"text": "hyperparameter search", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.6339370757341385}]}, {"text": "All reported results are calculated over the testset of WMT 2019 and evaluated with case sensitive SacreBLEU (Post, 2018) 2 if not specified otherwise.", "labels": [], "entities": [{"text": "WMT 2019", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8502698242664337}]}], "tableCaptions": [{"text": " Table 3: Testset BLEU scores of our setup. Except for the baseline, each column shows improvements obtained  after fine-tuning a single model on different datasets beginning with the score on a trained parent model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9973130822181702}]}, {"text": " Table 4: BLEU scores for training English\u2192Gujarati  from scratch on synthetic data from the second round  of backtranslation. Neither of models uses the aver- aging or beam search. Thus the final model is our  submitted model before averaging and beam search  (the model 3  ). The scores are equal to those from  http://matrix.statmt.org.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992302656173706}]}]}