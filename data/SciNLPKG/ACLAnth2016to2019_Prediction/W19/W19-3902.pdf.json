{"title": [{"text": "Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing", "labels": [], "entities": [{"text": "Grammatical Sequence Prediction", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7269591093063354}, {"text": "Neural Semantic Parsing", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.649139940738678}]}], "abstractContent": [{"text": "While sequence-to-sequence (seq2seq) models achieve state-of-the-art performance in many natural language processing tasks, they can be too slow for real-time applications.", "labels": [], "entities": []}, {"text": "One performance bottleneck is predicting the most likely next token over a large vocabulary; methods to circumvent this bottleneck area current research topic.", "labels": [], "entities": []}, {"text": "We focus specifically on using seq2seq models for semantic parsing, where we observe that grammars often exist which specify valid formal representations of utterance semantics.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 50, "end_pos": 66, "type": "TASK", "confidence": 0.7277113348245621}]}, {"text": "By developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations, we arrive at a widely applicable technique for speeding up semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 187, "end_pos": 203, "type": "TASK", "confidence": 0.7116085439920425}]}, {"text": "The technique leads to a 74% speed-up on an in-house dataset with a large vocabulary, compared to the same neural model without grammatical restrictions.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9606132507324219}]}], "introductionContent": [{"text": "Executable semantic parsing is the task of mapping an utterance to a logical form (LF) that can be executed against a data store (such as a SQL database or a knowledge graph), or interpreted by a computer program in some other way.", "labels": [], "entities": [{"text": "Executable semantic parsing is the task of mapping an utterance to a logical form (LF) that can be executed against a data store (such as a SQL database or a knowledge graph), or interpreted by a computer program in some", "start_pos": 0, "end_pos": 220, "type": "Description", "confidence": 0.8418223553233677}]}, {"text": "Various authors have tackled this task via sequenceto-sequence (seq2seq) models, which have already led to substantial advances in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7455800473690033}]}, {"text": "These models learn to directly map the input utterance into a linearised representation of the corresponding LF, predicting it token by token.", "labels": [], "entities": []}, {"text": "Seq2seq approaches have yielded state-ofthe-art accuracy on both classic (e.g., Geoquery (Zelle and Mooney, 1996) and Atis () and more recent semantic parsing datasets (e.g., WebQuestions, WikiSQL and Spider).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9976084232330322}]}, {"text": "The recent datasets are of much larger scale, which not only enables the use of more data-hungry models, such as deep neural networks, but also provides more complex challenges for semantic parsing.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 181, "end_pos": 197, "type": "TASK", "confidence": 0.7329049706459045}]}, {"text": "The material presented in this paper was motivated by a question-answering dataset for equity search in a financial data and analytics system.", "labels": [], "entities": []}, {"text": "We will refer to this dataset as \"the EQS dataset\" going forward (and we will refer to \"equity search\" as EQS for short).", "labels": [], "entities": [{"text": "EQS dataset", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.9255326688289642}]}, {"text": "The queries in the dataset pertain to equity stocks; they are usually of the form Show me companies that satisfy suchand-such criteria, or What are the top 10 companies that \u00b7 \u00b7 \u00b7?, and soon.", "labels": [], "entities": []}, {"text": "The dataset pairs such queries with logical forms that capture their semantics.", "labels": [], "entities": []}, {"text": "These logical forms are designed to be readily translatable into an executable query language in order to retrieve the corresponding answers from a data store in the back end.", "labels": [], "entities": []}, {"text": "Questions can involve a large number of diverse search criteria, such as price, earnings per share, country of domicile, membership in indices, trading in specific exchanges, etc., applied to a large set of equities for which the system offers information.", "labels": [], "entities": []}, {"text": "The large number of search criteria and entities is reflected in the LFs, leading to a problem common with newer, more complex semantic-parsing datasets: having to deal with a large LF vocabulary size.", "labels": [], "entities": []}, {"text": "In the EQS dataset the LF vocabulary has a size that exceeds 50,000.", "labels": [], "entities": [{"text": "EQS dataset", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9735007286071777}]}, {"text": "Since seq2seq models apply some operation over the whole vocabulary -usually the softmax operation -when deciding what symbol to output next, large LF vocabularies can slow them down considerably.", "labels": [], "entities": []}, {"text": "For example, we observe in our EQS experiments with seq2seq models that it takes on average between 250 and 300 milliseconds to parse a query, which is too slow for one single component in a larger, real-time question-answering pipeline.", "labels": [], "entities": []}, {"text": "This is consistent with observations made previously in the neural language modelling literature; see for example;, where the authors show that when the vocabulary size exceeds a certain threshold, the softmax calculation becomes the computational bottleneck.", "labels": [], "entities": [{"text": "neural language modelling", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.6834481755892435}]}, {"text": "Our proposal for tackling this bottleneck is based on the fact that there generally exist grammars, which we call LF grammars, specifying the concrete syntax of valid logical forms (LFs).", "labels": [], "entities": []}, {"text": "This is usually the case because LFs need to be machine-readable.", "labels": [], "entities": []}, {"text": "We further note that, fora given LF prefix, one can usually use the LF grammar to lookup the next grammatically permissible tokens (i.e., tokens that are part of a grammatically valid completion of the prefix).", "labels": [], "entities": []}, {"text": "For example, if the language of valid LFs can be expressed by a context-free grammar (CFG), as is almost always the case, then look-ups could be performed with an online version of the Earley parser.", "labels": [], "entities": []}, {"text": "If it is possible to efficiently lookup the permissible next tokens fora given prefix, then restricting the softmax operation to those permissible tokens should improve efficiency, and because only nonpermissible tokens are ruled out, this will only ever prevent the system from producing invalid LFs.", "labels": [], "entities": []}, {"text": "If the number of grammatically permissible tokens at some prediction step is substantially smaller than the LF's vocabulary size, the integration of the LF grammar may reduce prediction time for that step significantly.", "labels": [], "entities": []}, {"text": "In semantic parsing problems a grammar can naturally lead to prediction steps with few choices.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7365840375423431}]}, {"text": "To see why this might be the case, consider our LFs in, which involve atomic constraints of the form: (field operator value).", "labels": [], "entities": []}, {"text": "While there are many grammatically permissible choices for field and value, the choices for operator are rather limited.", "labels": [], "entities": []}, {"text": "LFs for many applications will contain \"structural\" elements with a limited number of choices in grammatically predictable positions, and we can use grammars to exploit this fact.", "labels": [], "entities": []}, {"text": "In order to make the computation of permissible next tokens efficient, we propose to use a finitestate automaton (FSA) approximation of the LF grammar.", "labels": [], "entities": []}, {"text": "Finite-state automata can capture local 2 Equality, less than and soon.", "labels": [], "entities": []}, {"text": "relations that are often quite predictive of the admissible tokens in a given context, and can therefore lead to considerable speed improvements for our setting, even if we use an approximate grammar.", "labels": [], "entities": []}, {"text": "Moreover, approximations can be designed in such away that a FSA accepts a superset of the actual LF language, preserving the guarantee that only ill-formed LFs will ever be ruled out.", "labels": [], "entities": [{"text": "FSA", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.6421266794204712}]}, {"text": "In this paper we therefore work with a grammar for which the next permissible tokens can be computed efficiently, and show how such a grammar can be combined with a seq2seq model in order to substantially improve the efficiency of inference.", "labels": [], "entities": []}, {"text": "While we focus on using FSAs to restrict a recurrent neural network with attention in the EQS dataset, our approach is generic and could be used to speedup any sequential prediction model with any grammar that allows for efficient computation of next-token sets.", "labels": [], "entities": [{"text": "EQS dataset", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.9724695682525635}]}, {"text": "Our experiments show that in our domain of interest we obtain a reduction in parsing time by up to 74%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9690030217170715}]}], "datasetContent": [{"text": "Our experiments are conducted on the EQS dataset.", "labels": [], "entities": [{"text": "EQS dataset", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9861814081668854}]}, {"text": "The dataset consists of queries paired with their LFs, which were obtained in a semiautomated manner.", "labels": [], "entities": []}, {"text": "The dataset contains 1981 (NL, LF) pairs as training data and 331 (NL, LF) pairs as test data.", "labels": [], "entities": []}, {"text": "The LF vocabulary size is 56209, most of which consists of enum field names and values.", "labels": [], "entities": []}, {"text": "All the LFs can be accepted by the FSA discussed in Section 3.", "labels": [], "entities": [{"text": "FSA", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.9584522843360901}]}, {"text": "The dataset is too small to effectively learn a model that can reliably predict rare fields or values.", "labels": [], "entities": []}, {"text": "However, as most of the queries involve only common fields and entities, we find in our experiments that our neural semantic parser is able to parse a large number of those queries correctly; orthogonal research is being conducted on how to handle more rare fields or entities.", "labels": [], "entities": []}, {"text": "All our experiments were conducted on a server with 40 Intel Xeon@3.00GHz CPUs and 380 GB of RAM.", "labels": [], "entities": []}, {"text": "We monitor the server state closely while conducting the experiments.", "labels": [], "entities": []}, {"text": "Our models are implemented in PyTorch (, which is able to exploit the server's multi-core architecture.", "labels": [], "entities": [{"text": "PyTorch", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.8670612573623657}]}, {"text": "The peak usage for both CPU load and memory consumption for all our models is far below the server's capacity.", "labels": [], "entities": []}, {"text": "We run all the models over the entire test dataset (331 sentences) and report the average prediction time for each sentence.", "labels": [], "entities": [{"text": "prediction time", "start_pos": 90, "end_pos": 105, "type": "METRIC", "confidence": 0.9288142025470734}]}, {"text": "For each model, we conduct 5 such runs to calculate the standard deviations of different runs.", "labels": [], "entities": []}, {"text": "The standard deviations are small in absolute and relative value.", "labels": [], "entities": []}], "tableCaptions": []}