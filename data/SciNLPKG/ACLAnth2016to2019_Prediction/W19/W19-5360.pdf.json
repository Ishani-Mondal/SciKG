{"title": [{"text": "Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7151439785957336}]}], "abstractContent": [{"text": "In this paper, we introduce our participation in the WMT 2019 Metric Shared Task.", "labels": [], "entities": [{"text": "WMT 2019 Metric Shared Task", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.5641665816307068}]}, {"text": "We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8055535852909088}]}, {"text": "We use the outputs of off-the-shelf MT systems as pseudo-references filtered by paraphrasing in addition to a single human reference (gold reference).", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9731689095497131}]}, {"text": "We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference.", "labels": [], "entities": [{"text": "BERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9810492992401123}]}, {"text": "Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (Sent-BLEU) baselines with a single reference and with unfiltered pseudo-references.", "labels": [], "entities": [{"text": "WMT 2016 and 2017 datasets", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.8951740026473999}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.921352744102478}]}], "introductionContent": [{"text": "In general, automatic evaluation of MT is based on n-gram agreement between the system output and a manually translated reference of the source sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.964267909526825}]}, {"text": "Therefore, automatic evaluation fails to evaluate a semantically correct sentence if the surface of the system output differs from that in the reference.", "labels": [], "entities": []}, {"text": "To solve this problem, many automatic evaluation methods allow the use of multiple references that potentially cover various surfaces; in particular, reported that correlation between automatic evaluation results and human evaluation increases when multiple references are used for evaluation.", "labels": [], "entities": []}, {"text": "However, owing to the time and costs involved in manually creating references, many datasets only include one reference per source sentence, which leads to improper translation evaluation, especially in the case of diverse machine translation systems.", "labels": [], "entities": [{"text": "translation evaluation", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.946979433298111}, {"text": "machine translation", "start_pos": 223, "end_pos": 242, "type": "TASK", "confidence": 0.7357080280780792}]}, {"text": "In order to obtain cheap references without any human intervention, used the outputs of off-the-shelf MT systems as pseudo-references; They showed that using multiple references consisting of gold and pseudoreferences may yield higher correlation with human evaluation than using a single gold reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.9517122507095337}]}, {"text": "However, because they did not consider the quality of the pseudo-references, this may result in using poor references.", "labels": [], "entities": []}, {"text": "Thus, in some cases the correlation becomes worse when using multiple references consisting of gold and pseudo-references relative to only using a gold reference.", "labels": [], "entities": [{"text": "correlation", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9768933653831482}]}, {"text": "To address the quality of pseudo-references, we filtered pseudo-references by checking the paraphrasability to the gold reference.", "labels": [], "entities": []}, {"text": "Our approach can be applied to various MT evaluation metrics which can be evaluated with multiple references.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.8967692852020264}]}, {"text": "The experimental results show that our method achieves higher correlation with human evaluation than the previous work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Multiple Pseudo-References 3.1 Overview shows the overview of our proposed method.", "labels": [], "entities": []}, {"text": "The procedure of our proposed method is as follows.", "labels": [], "entities": []}, {"text": "1. Prepare off-the-shelf MT systems for generating pseudo-references.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.8815869092941284}]}, {"text": "2. Translate the source sentence in the evaluation data using the abovementioned MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.8601226806640625}]}, {"text": "3. Filter the outputs of off-the-shelf MT systems by checking the paraphrasability of being a paraphrase to the single gold reference.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9845885634422302}]}, {"text": "4. Calculate the sentence evaluation score with multiple references obtained by adding filtered pseudo-references to the single gold references.", "labels": [], "entities": []}, {"text": "We calculated the SentBLEU score with system output and multiple references which consisted of a single gold reference and pseudo-references.", "labels": [], "entities": []}, {"text": "The SentBLEU is computed using the sentencebleu.cpp 5 , apart of the Moses toolkit.", "labels": [], "entities": []}, {"text": "It is a smoothed version of BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9948731064796448}]}, {"text": "We followed the tokenization method for each year's dataset.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9831582307815552}]}, {"text": "We measured Pearson correlation identically to WMT 2016 and WMT 2017 between the automatic and human evaluation scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 12, "end_pos": 31, "type": "METRIC", "confidence": 0.9424430131912231}, {"text": "WMT", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.8788251876831055}, {"text": "WMT", "start_pos": 60, "end_pos": 63, "type": "DATASET", "confidence": 0.8148715496063232}]}, {"text": "In order to compare with our method, we also performed filtering by Maximum Alignment Similarity (MAS) (, which is one of the unsupervised sentence similarity measures based on alignments between word embeddings 5 https://github.com/moses-smt/ mosesdecoder/blob/master/mert/sentence-bleu.cpp and is known to achieve good performance on Semantic Textual Similarity (STS) task. We used GloVe) as word embeddings.", "labels": [], "entities": [{"text": "Maximum Alignment Similarity (MAS)", "start_pos": 68, "end_pos": 102, "type": "METRIC", "confidence": 0.6460181872049967}, {"text": "Semantic Textual Similarity (STS) task.", "start_pos": 336, "end_pos": 375, "type": "TASK", "confidence": 0.7890154336180005}]}, {"text": "We used pseudo-references whose MAS score is higher than 0.8.", "labels": [], "entities": [{"text": "MAS score", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9802816808223724}]}, {"text": "show the segment-level Pearson correlation coefficients between automatic and human evaluation scores.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 23, "end_pos": 42, "type": "METRIC", "confidence": 0.9069838225841522}]}, {"text": "The result shows that our proposed method outperforms the baselines except in the case of the ru-en language pair in WMT 2016 and filtering by MAS does not produce any consistent result.", "labels": [], "entities": [{"text": "WMT 2016", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.8514778912067413}, {"text": "MAS", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.6233004927635193}]}, {"text": "shows an example of pseudo-references with BERT's paraphrase score for the ru-en language pair in WMT 2017.", "labels": [], "entities": [{"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.8750210404396057}, {"text": "WMT 2017", "start_pos": 98, "end_pos": 106, "type": "DATASET", "confidence": 0.9056274890899658}]}, {"text": "The pseudo-reference from Bing translation has a low paraphrase score because \"biles\" in the gold reference remains as \"bayles\" in the pseudo-reference, and \"floor exercise\" became \"freestyle exercise\" in Bing translation.", "labels": [], "entities": []}, {"text": "In the unfiltered method, the BLEU score is unreasonably high because the surface of the pseudo-reference from Bing translation is similar to the output sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.982743114233017}]}, {"text": "Filtering the pseudoreferences prevents the problem.", "labels": [], "entities": []}, {"text": "The pseudoreference from Google translation has different surfaces but carry the same meaning as in the gold reference.", "labels": [], "entities": []}, {"text": "Our filtering method correctly retains the sentence because BERT assigned high paraphrase score.", "labels": [], "entities": [{"text": "BERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9906737208366394}]}], "tableCaptions": [{"text": " Table 2: Segment-level Pearson correlation between SentBLEU and human evaluation scores in WMT 2017.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 24, "end_pos": 43, "type": "METRIC", "confidence": 0.890570729970932}, {"text": "WMT 2017", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.7701128423213959}]}, {"text": " Table 3: Numbers of sentences in each split of MRPC  and accuracy of BERT.", "labels": [], "entities": [{"text": "Numbers", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9650112986564636}, {"text": "MRPC", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.593960702419281}, {"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9997671246528625}, {"text": "BERT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.976909875869751}]}]}