{"title": [{"text": "Is Multilingual BERT Fluent in Language Generation?", "labels": [], "entities": [{"text": "Language Generation", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.6917257010936737}]}], "abstractContent": [{"text": "The multilingual BERT model is trained on 104 languages and meant to serve as a universal language model and tool for encoding sentences.", "labels": [], "entities": [{"text": "BERT", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.7208253145217896}]}, {"text": "We explore how well the model performs on several languages across several tasks: a diagnostic classification probing the embeddings fora particular syntactic property, a cloze task testing the language modelling ability to fill in gaps in a sentence, and a natural language generation task testing for the ability to produce coherent text fitting a given context.", "labels": [], "entities": [{"text": "natural language generation task", "start_pos": 258, "end_pos": 290, "type": "TASK", "confidence": 0.7370642721652985}]}, {"text": "We find that the currently available multilingual BERT model is clearly inferior to the monolingual counterparts, and cannot in many cases serve as a substitute fora well-trained monolingual model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9658470749855042}]}, {"text": "We find that the English and German models perform well at generation, whereas the multilingual model is lacking, in particular , for Nordic languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The language representation model BERT (Bidirectional Encoder Representations from Transformers) has been shown to achieve state-of-theart performance when fine-tuned on a range of downstream tasks related to language understanding (, and recently also language generation.", "labels": [], "entities": [{"text": "BERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9941380023956299}, {"text": "language understanding", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.7227379530668259}, {"text": "language generation", "start_pos": 253, "end_pos": 272, "type": "TASK", "confidence": 0.7364412099123001}]}, {"text": "In addition to downstream applications, many recent studies have explored more directly how various types of linguistic information is captured in BERT's representations.", "labels": [], "entities": []}, {"text": "However, all such studies we are aware of are conducted for English using the monolin-gual BERT model as the availability of pretrained BERT models for other languages is extremely scarce.", "labels": [], "entities": [{"text": "BERT", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9349693655967712}]}, {"text": "For the vast majority of languages, the only option is the multilingual BERT model trained jointly on 104 languages.", "labels": [], "entities": [{"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9648432731628418}]}, {"text": "In \"coffee break\" discussions, it is often mentioned that the multilingual BERT model lags behind the monolingual models in terms of quality and cannot serve as a drop-in replacement.", "labels": [], "entities": [{"text": "BERT", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.887809157371521}]}, {"text": "In this paper, we therefore set out to test the multilingual model on several tasks and several languages (primarily Nordic), to establish whether, and to what extent this is the case, as well as to establish at least an order-of-magnitude expectation of the performance of the present multilingual BERT model on these tasks and languages.", "labels": [], "entities": [{"text": "BERT", "start_pos": 299, "end_pos": 303, "type": "METRIC", "confidence": 0.864471435546875}]}, {"text": "It must be stressed that this paper deals with the particular multilingual model distributed by the BERT creators, rather than the more general question of comparison of the multilingual and monolingual training schedule.", "labels": [], "entities": [{"text": "BERT creators", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.7484618425369263}]}, {"text": "Studying those questions would necessitate training multilingual BERT models with resource requirements far beyond those at our disposal.", "labels": [], "entities": []}, {"text": "We put a particular focus on the natural language generation (NLG) task, which we hypothesize requires a deeper understanding of the language in question on the side of the model.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8152050872643789}]}, {"text": "We take English and German, for which monolingual versions of BERT are available, as reference languages, in order to compare how they perform in the mono-vs. multilingual settings.", "labels": [], "entities": [{"text": "BERT", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.8398597240447998}]}, {"text": "Furthermore, we perform experiments with the Nordic languages of Danish, Finnish, Norwegian (Bokm\u00e5l and Nynorsk) and Swedish, with in-depth evaluations on Finnish and Swedish, as well as the abovementioned two reference languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the BERT models on 6 languages, English, German, Swedish, Finnish, Danish, and Norwegian (Bokm\u00e5l and Nynorsk), and three different tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9031941294670105}, {"text": "Bokm\u00e5l", "start_pos": 102, "end_pos": 108, "type": "DATASET", "confidence": 0.9622438549995422}]}, {"text": "In addition to automatic metrics, the generated output is manually evaluated for English, German, Swedish, and Finnish, the four languages that at least one of the authors is fluent in, and therefore comfortable evaluating.", "labels": [], "entities": []}, {"text": "For English and German there are monolingual BERT models available, which we use as references to evaluate the performance of the multilingual BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.8822951316833496}]}, {"text": "We further compare performance among these languages and the four Nordic languages in order to assess its utility for such relatively lowresource languages.", "labels": [], "entities": []}, {"text": "In all evaluation tasks, we use data from the Universal Dependencies (UD) ver 2.4 treebanks ( for the languages in question.", "labels": [], "entities": [{"text": "Universal Dependencies (UD) ver 2.4 treebanks", "start_pos": 46, "end_pos": 91, "type": "DATASET", "confidence": 0.5492695085704327}]}], "tableCaptions": [{"text": " Table 1: Diagnostic classifier results. Auxiliary  classification task accuracies and majority class  baselines for all languages.", "labels": [], "entities": []}, {"text": " Table 2: Results for the cloze test in terms of sub- word predictions accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.8917434811592102}]}]}