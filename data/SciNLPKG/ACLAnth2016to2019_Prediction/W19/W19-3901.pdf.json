{"title": [], "abstractContent": [{"text": "This work attempts to explain the types of computation that neural networks can perform by relating them to automata.", "labels": [], "entities": []}, {"text": "We first define what it means fora real-time network with bounded precision to accept a language.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9958624839782715}]}, {"text": "A measure of network memory follows from this definition.", "labels": [], "entities": []}, {"text": "We then characterize the classes of languages acceptable by various recurrent networks , attention, and convolutional networks.", "labels": [], "entities": []}, {"text": "We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy.", "labels": [], "entities": []}, {"text": "Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory.", "labels": [], "entities": []}, {"text": "These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.", "labels": [], "entities": [{"text": "natural language grammar", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.6777592500050863}]}], "introductionContent": [{"text": "In recent years, neural networks have achieved tremendous success on a variety of natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 82, "end_pos": 121, "type": "TASK", "confidence": 0.7563810007912772}]}, {"text": "Neural networks employ continuous distributed representations of linguistic data, which contrast with classical discrete methods.", "labels": [], "entities": []}, {"text": "While neural methods work well, one of the downsides of the distributed representations that they utilize is interpretability.", "labels": [], "entities": []}, {"text": "It is hard to tell what kinds of computation a model is capable of, and when a model is working, it is hard to tell what it is doing.", "labels": [], "entities": []}, {"text": "This work aims to address such issues of interpretability by relating sequential neural networks to forms of computation that are more well understood.", "labels": [], "entities": []}, {"text": "In theoretical computer science, the computational capacities of many different kinds of automata formalisms are clearly established.", "labels": [], "entities": []}, {"text": "Moreover, the Chomsky hierarchy links natural * Work completed while the author was at Yale University.", "labels": [], "entities": []}, {"text": "language to such automata-theoretic languages).", "labels": [], "entities": []}, {"text": "Thus, relating neural networks to automata both yields insight into what general forms of computation such models can perform, as well as how such computation relates to natural language grammar.", "labels": [], "entities": []}, {"text": "Recent work has begun to investigate what kinds of automata-theoretic computations various types of neural networks can simulate.", "labels": [], "entities": []}, {"text": "propose a connection between long shortterm memory networks (LSTMs) and counter automata.", "labels": [], "entities": []}, {"text": "They provide a construction by which the LSTM can simulate a simplified variant of a counter automaton.", "labels": [], "entities": []}, {"text": "They also demonstrate that LSTMs can learn to increment and decrement their cell state as counters in practice., on the other hand, describe a connection between the gating mechanisms of several recurrent neural network (RNN) architectures and weighted finite-state acceptors.", "labels": [], "entities": []}, {"text": "This paper follows by analyzing the expressiveness of neural network acceptors under asymptotic conditions.", "labels": [], "entities": []}, {"text": "We formalize asymptotic language acceptance, as well as an associated notion of network memory.", "labels": [], "entities": [{"text": "asymptotic language acceptance", "start_pos": 13, "end_pos": 43, "type": "TASK", "confidence": 0.6482860048611959}]}, {"text": "We use this theory to derive computation upper bounds and automata-theoretic characterizations for several different kinds of recurrent neural networks (Section 3), as well as other architectural variants like attention (Section 4) and convolutional networks (CNNs).", "labels": [], "entities": []}, {"text": "This leads to a fairly complete automata-theoretic characterization of sequential neural networks.", "labels": [], "entities": []}, {"text": "In Section 6, we report empirical results investigating how well these asymptotic predictions describe networks with continuous activations learned by gradient descent.", "labels": [], "entities": []}, {"text": "In some cases, networks behave according to the theoretical predictions, but we also find cases where there is gap between the asymptotic characterization and ac-tual network behavior.", "labels": [], "entities": []}, {"text": "Still, discretizing neural networks using an asymptotic analysis builds intuition about how the network computes.", "labels": [], "entities": []}, {"text": "Thus, this work provides insight about the types of computations that sequential neural networks can perform through the lens of formal language theory.", "labels": [], "entities": [{"text": "formal language theory", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.6303773621718088}]}, {"text": "In so doing, we can also compare the notions of grammar expressible by neural networks to formal models that have been proposed for natural language grammar.", "labels": [], "entities": [{"text": "natural language grammar", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.7125946879386902}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Generalization performance of language models trained on a n b n c. Each model has 2 hidden units.", "labels": [], "entities": []}, {"text": " Table 2: Max validation and generalization accuracies  on string reversal over 10 trials. The top section shows  our seq2seq LSTM with and without attention. The  bottom reports the LSTM and StackNN results of Hao  et al. (2018). Each LSTM has 10 hidden units.", "labels": [], "entities": [{"text": "string reversal", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7172950953245163}]}]}