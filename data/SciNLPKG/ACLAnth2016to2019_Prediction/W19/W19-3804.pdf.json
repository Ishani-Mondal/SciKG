{"title": [{"text": "Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories", "labels": [], "entities": [{"text": "Discovering New Gender Bias Word Categories", "start_pos": 60, "end_pos": 103, "type": "TASK", "confidence": 0.7336507141590118}]}], "abstractContent": [{"text": "Prior work has shown that word embeddings capture human stereotypes, including gender bias.", "labels": [], "entities": []}, {"text": "However, there is alack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains.", "labels": [], "entities": []}, {"text": "This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking , biomedical and a gender-balanced corpus extracted from Wikipedia (GAP).", "labels": [], "entities": [{"text": "WEAT bias detection", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8220411340395609}]}, {"text": "We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings.", "labels": [], "entities": []}, {"text": "We detect some gender bias in GAP.", "labels": [], "entities": [{"text": "GAP", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9614098072052002}]}, {"text": "We also propose a simple but novel method for discovering new bias categories by clustering word embeddings.", "labels": [], "entities": []}, {"text": "We validate this method through WEAT's hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "Artificial intelligence (AI) acquired from machine learning is becoming more prominent in decisionmaking tasks in areas as diverse as industry, healthcare and education.", "labels": [], "entities": []}, {"text": "AI-informed decisions depend on AI systems' input training data which, unfortunately, can contain implicit racial, gender or ideological biases.", "labels": [], "entities": []}, {"text": "Such AI-informed decisions can thus lead to unfair treatment of certain groups.", "labels": [], "entities": []}, {"text": "For example, in Natural Language Processing (NLP), r\u00e9sum\u00e9 search engines can produce rankings that disadvantage some candidates, when these ranking algorithms take demographic features into account (directly or indirectly), while abusive online language detection systems have been observed to produce false positives on terms associated with minorities and women (.", "labels": [], "entities": []}, {"text": "Another example where bias (specifically gender bias) can be harmful is in personal pronoun coreference resolution, where systems carry the risk of relying on societal stereotypes present in the training data.", "labels": [], "entities": [{"text": "personal pronoun coreference resolution", "start_pos": 75, "end_pos": 114, "type": "TASK", "confidence": 0.7359102219343185}]}, {"text": "Whilst gender bias in the form of concepts of masculinity and femininity has been found inscribed in implicit ways in AI systems more broadly), this paper focuses on gender bias on word embeddings.", "labels": [], "entities": []}, {"text": "Word embeddings are one of the most common techniques forgiving semantic meaning to words in text and are used as input in virtually every neural NLP system.", "labels": [], "entities": []}, {"text": "It has been shown that word embeddings capture human biases (such as gender bias) present in these corpora in how they relate words to each other.", "labels": [], "entities": []}, {"text": "For the purposes of this paper, gender bias is understood as the inclination towards or prejudice against one gender.", "labels": [], "entities": []}, {"text": "Several methods have been proposed to test for the presence of gender bias in word embeddings; an example being the Word Embedding Association Test (WEAT).", "labels": [], "entities": []}, {"text": "WEAT is a statistical test that detects bias in word embeddings using cosine similarity and averaging methods, paired with hypothesis testing.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.4476470649242401}]}, {"text": "WEAT's authors applied these tests to the publicly-available GloVe embeddings trained on the English-language \"Common Crawl\" corpus () as well as the Skip-Gram (word2vec) embeddings trained on the Google News corpus (.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9416991472244263}, {"text": "Common Crawl\" corpus", "start_pos": 111, "end_pos": 131, "type": "DATASET", "confidence": 0.8856304287910461}, {"text": "Google News corpus", "start_pos": 197, "end_pos": 215, "type": "DATASET", "confidence": 0.8440635601679484}]}, {"text": "However, there is a diverse range of publicly-available word embeddings trained on corpora of different domains.", "labels": [], "entities": []}, {"text": "To address this, we applied the WEAT test on four sets of word embeddings trained on corpora from four domains: social media (Twit-ter), a Wikipedia-based gender-balanced corpus (GAP) and a biomedical corpus (PubMed) and news (Google News, in order to reproduce and validate our results against those of) (see Section 3).", "labels": [], "entities": [{"text": "WEAT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.5316113829612732}]}, {"text": "confirmed the presence of gender bias using three categories of words wellknown to be prone to exhibit gender bias: (B1) career vs. family activities, (B2) Maths vs. Arts and (B3) Science vs. Arts.", "labels": [], "entities": []}, {"text": "expanded on this work and tested additional gender bias word categories: (B4) differences on personal descriptions based on intelligence vs. appearance and on (B5) physical or emotional strength vs. weakness.", "labels": [], "entities": []}, {"text": "In this paper, we use these five categories to test for the presence of gender bias in the aforementioned domain corpora.", "labels": [], "entities": []}, {"text": "Notice that one of the tested corpora is the gender-balanced GAP corpus.", "labels": [], "entities": []}, {"text": "We specifically chose this corpus in order to test whether the automatic method used to compile it (based on sampling an equal number of male and female pronouns from Wikipedia) yielded a set that was balanced according to these five well-known gender bias word categories.", "labels": [], "entities": []}, {"text": "GAP's authors acknowledge that Wikipedia has been found to contain gender biased content ().", "labels": [], "entities": [{"text": "GAP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8869510889053345}]}, {"text": "We confirmed bias in all five categories on the Google News embeddings but far less bias on the rest of the embeddings, with the biomedical PubMed embeddings showing the least bias.", "labels": [], "entities": [{"text": "Google News embeddings", "start_pos": 48, "end_pos": 70, "type": "DATASET", "confidence": 0.9084752996762594}]}, {"text": "We did find some bias on GAP.", "labels": [], "entities": [{"text": "GAP", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8466546535491943}]}, {"text": "However, given the small size of this corpus, many test words were not present (see Section 4).", "labels": [], "entities": []}, {"text": "The six word categories studied here are word lists manually curated by Psychology researchers based on their studies (e.g.).", "labels": [], "entities": []}, {"text": "However, it is difficult to establish whether they are exhaustive as there could be other word categories presenting bias, which may well be domain-dependant.", "labels": [], "entities": []}, {"text": "In response, we developed a simple method to automatically discover new categories of gender bias words based on word clustering, and measuring statistical associations of the words in each cluster to known female and male attribute words.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.6725651770830154}]}, {"text": "Assuming that each cluster roughly represents a topic in the corpus, the set of gender bias words in each cluster/topic in the corpus corresponds to a potentially new category of gender-biased words.", "labels": [], "entities": []}, {"text": "As far as we are aware, this is the first time a method to discover new gender bias word categories is proposed.", "labels": [], "entities": []}, {"text": "We used WEAT's hypothesis testing mechanism to automatically validate the induced gender bias word categories produced by our system.", "labels": [], "entities": [{"text": "WEAT", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.7356488108634949}]}, {"text": "A visual inspection on a sample of these induced categories is consistent with the authors' intuitions of gender bias.", "labels": [], "entities": []}, {"text": "We make these induced categories available to other researchers to study.", "labels": [], "entities": []}, {"text": "An advantage of this discovery method is that it allows us to detect bias based on a corpus' own vocabulary, even if it is small, as is the casein the GAP corpus embeddings.", "labels": [], "entities": [{"text": "GAP corpus embeddings", "start_pos": 151, "end_pos": 172, "type": "DATASET", "confidence": 0.8539723753929138}]}], "datasetContent": [{"text": "We largely follow the WEAT Hypothesis testing protocol introduced by.", "labels": [], "entities": [{"text": "WEAT Hypothesis testing", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.48009316126505536}]}, {"text": "The input is a suspected gender bias word category represented by two lists, X and Y , of target words, i.e. words which are suspected to be biased to one or another gender.", "labels": [], "entities": []}, {"text": "E.g. X = {programmer, engineer, scientist}, Y = {nurse, teacher, librarian}.", "labels": [], "entities": []}, {"text": "We wish to test whether X or Y is more biased to one gender or the other, or whether there is not difference in bias between the two lists.", "labels": [], "entities": []}, {"text": "Bias is compared in relation to two reference lists of words that represent unequivocally male and female concepts.", "labels": [], "entities": [{"text": "Bias", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.9130365252494812}]}, {"text": "E.g. M = {man, male, he}, F = {woman, female, she}.", "labels": [], "entities": [{"text": "F", "start_pos": 26, "end_pos": 27, "type": "METRIC", "confidence": 0.9602667689323425}]}, {"text": "In WEAT's terminology these reference lists are called the attribute words.", "labels": [], "entities": []}, {"text": "shows the target and attribute word sets used in our experiments.", "labels": [], "entities": []}, {"text": "This p-value is the probability that Ho is true.", "labels": [], "entities": []}, {"text": "In other words, it is the probability that there is no difference between X and Y (in relation to M and F ) and therefore that the word category is not biased.", "labels": [], "entities": []}, {"text": "The higher this p-value is the less bias there is.", "labels": [], "entities": []}, {"text": "Following, in this work we consider a word category to have statistically significant gender bias if its p-value is below the 0.05 threshold.", "labels": [], "entities": []}, {"text": "Given that a full permutation test can quickly become computationally intractable, in this paper we instead use randomisation tests) with a maximum of 100,000 iterations in each test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: WEAT hypothesis test results for corpora tested for five well-known gender-biased word categories. p- values in bold indicate statistically significant gender bias (p < 0.05).", "labels": [], "entities": [{"text": "WEAT hypothesis", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.5523295700550079}]}, {"text": " Table 3: Number of out-of-vocabulary target and at- tribute words in the PubMed and GAP embeddings.  Google and Twitter embeddings contain all words.", "labels": [], "entities": [{"text": "PubMed and GAP embeddings", "start_pos": 74, "end_pos": 99, "type": "DATASET", "confidence": 0.8070418387651443}]}]}