{"title": [{"text": "Measuring Bias in Contextualized Word Representations", "labels": [], "entities": [{"text": "Measuring Bias in Contextualized Word Representations", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6341546277205149}]}], "abstractContent": [{"text": "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9712773561477661}]}, {"text": "Since they are optimized to capture the statistical properties of training data, they tend to pickup on and amplify social stereotypes present in the data as well.", "labels": [], "entities": []}, {"text": "In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution.", "labels": [], "entities": [{"text": "BERT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.8039700984954834}, {"text": "Gender Pronoun Resolution", "start_pos": 284, "end_pos": 309, "type": "TASK", "confidence": 0.7843208710352579}]}, {"text": "Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Type-level word embedding models, including word2vec and GloVe (), have been shown to exhibit social biases present in human-generated training data.", "labels": [], "entities": []}, {"text": "These embeddings are then used in a plethora of downstream applications, which perpetuate and further amplify stereotypes (.", "labels": [], "entities": []}, {"text": "To reveal and quantify corpuslevel biases is word embeddings, used the word analogy task (.", "labels": [], "entities": [{"text": "word analogy", "start_pos": 71, "end_pos": 83, "type": "TASK", "confidence": 0.7161568850278854}]}, {"text": "For example, they showed that gendered male word embeddings like he, man are associated with higher-status jobs like computer programmer and doctor, whereas gendered words like she or woman are associated with homemaker and nurse.", "labels": [], "entities": []}, {"text": "Contextual word embedding models, such as ELMo and BERT () have become increasingly common, replacing traditional type-level embeddings and attaining new state of the art results in the majority of NLP tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.992334246635437}]}, {"text": "In these models, every word has a different embedding, depending on the context and the language model state; in these settings, the analogy task used to reveal biases in uncontextualized embeddings is not applicable.", "labels": [], "entities": []}, {"text": "Recently, showed that traditional cosine-based methods for exposing bias in sentence embeddings fail to produce consistent results for embeddings generated using contextual methods.", "labels": [], "entities": []}, {"text": "We find similar inconsistent results with cosine-based methods of exposing bias; this is a motivation to the development of a novel bias test that we propose.", "labels": [], "entities": []}, {"text": "In this work, we propose anew method to quantify bias in BERT embeddings ( \u00a72).", "labels": [], "entities": [{"text": "BERT", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.653136670589447}]}, {"text": "Since BERT embeddings use a masked language modelling objective, we directly query the model to measure the bias fora particular token.", "labels": [], "entities": [{"text": "BERT", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.7174769043922424}]}, {"text": "More specifically, we create simple template sentences containing the attribute word for which we want to measure bias (e.g. programmer) and the target for bias (e.g. she for gender).", "labels": [], "entities": []}, {"text": "We then mask the attribute and target tokens sequentially, to get a relative measure of bias across target classes (e.g. male and female).", "labels": [], "entities": []}, {"text": "Contextualized word embeddings fora given token change based on its context, so such an approach allows us measure the bias for similar categories divergent by the the target attribute ( \u00a72).", "labels": [], "entities": []}, {"text": "We compare our approach with the cosine similaritybased approach ( \u00a73) and show that our measure of bias is more consistent with human biases and is sensitive to a wide range of biases in the model using various stimuli presented in.", "labels": [], "entities": []}, {"text": "Next, we investigate the effect of a specific type of bias in a specific downstream task: gender bias in BERT and its effect on the task of Gendered Pronoun Resolution (GPR).", "labels": [], "entities": [{"text": "BERT", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9470757842063904}, {"text": "Gendered Pronoun Resolution (GPR)", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.8148346940676371}]}, {"text": "We show that the bias in GPR is highly correlated with our measure of bias ( \u00a74).", "labels": [], "entities": [{"text": "GPR", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.6534245610237122}, {"text": "bias", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9565539360046387}]}, {"text": "Finally, we highlight the potential negative impacts of using BERT in downstream real world applications ( \u00a75).", "labels": [], "entities": [{"text": "BERT", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.947326123714447}]}, {"text": "The code and data used in this work are publicly available.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Effect sizes of bias measurements on WEAT Stimuli. (* indicates significant at p < 0.01)", "labels": [], "entities": [{"text": "WEAT", "start_pos": 47, "end_pos": 51, "type": "TASK", "confidence": 0.671540379524231}]}, {"text": " Table 4: Probability of pronoun referring to neither  entity in a sentence of GPR", "labels": [], "entities": [{"text": "GPR", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.5790756344795227}]}, {"text": " Table 5: Percentage of attributes associated more  strongly with the male gender", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9431506395339966}]}]}