{"title": [], "abstractContent": [{"text": "This is the system description of the Harbin Institute of Technology Shenzhen (HITSZ) team for the first and second subtasks of the fourth Social Media Mining for Health Applications (SMM4H) shared task in 2019.", "labels": [], "entities": [{"text": "Harbin Institute of Technology Shenzhen (HITSZ) team", "start_pos": 38, "end_pos": 90, "type": "DATASET", "confidence": 0.8437463508711921}, {"text": "Social Media Mining for Health Applications (SMM4H) shared task", "start_pos": 139, "end_pos": 202, "type": "TASK", "confidence": 0.758695434440266}]}, {"text": "The two subtasks are automatic classification and extraction of adverse effect mentions in tweets.", "labels": [], "entities": [{"text": "automatic classification", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.5276840180158615}, {"text": "extraction of adverse effect mentions in tweets", "start_pos": 50, "end_pos": 97, "type": "TASK", "confidence": 0.8338326215744019}]}, {"text": "The systems for the two subtasks are based on bidirectional encoder representations from transformers (BERT), and achieves promising results.", "labels": [], "entities": [{"text": "BERT", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9896904230117798}]}, {"text": "Among the systems we developed for subtask1, the best F1-score was 0.6457, for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9985847473144531}, {"text": "F1-score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9395713806152344}, {"text": "F1-score", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.923974335193634}]}, {"text": "Our system ranks first among all systems on subtask1.", "labels": [], "entities": []}], "introductionContent": [{"text": "Adverse drug reaction (ADR), namely adverse drug effect, is one of the leading causes of posttherapeutic deaths.", "labels": [], "entities": [{"text": "Adverse drug reaction (ADR)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6919564505418142}]}, {"text": "Nowadays, more and more people share information in social platform, including health information such as drugs and their ADRs.", "labels": [], "entities": []}, {"text": "Twitter, as one of the most popular social platforms, has attracted a great deal of attention from researchers in the medical domain.", "labels": [], "entities": []}, {"text": "Some methods, such as HTR_MSA ( and Neural DrugNet, have been proposed to detect tweets mentioning ADRs and medicine intake.", "labels": [], "entities": [{"text": "HTR_MSA", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.7387092709541321}]}, {"text": "In order to facilitate the use of social media for health monitoring and surveillance, the health language processing lab at University of Pennsylvania organized Social Media Mining for Health Applications (SMM4H) shared task four times.", "labels": [], "entities": [{"text": "Social Media Mining for Health Applications (SMM4H) shared task", "start_pos": 162, "end_pos": 225, "type": "TASK", "confidence": 0.6500048366459933}]}, {"text": "In 2019, the fourth SMM4H shared task was comprised of four subtasks: (1) Automatic classifications of adverse effect mentions in tweets, (2) Extraction of Adverse Effect mentions, (3) Normalization of adverse drug reaction mentions (ADR), and (4) Generalizable identification of personal health experience mentions (.", "labels": [], "entities": [{"text": "SMM4H shared task", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.9068401058514913}, {"text": "Automatic classifications of adverse effect mentions in tweets", "start_pos": 74, "end_pos": 136, "type": "TASK", "confidence": 0.7794813215732574}, {"text": "Extraction of Adverse Effect mentions", "start_pos": 142, "end_pos": 179, "type": "TASK", "confidence": 0.842606782913208}, {"text": "Normalization of adverse drug reaction mentions (ADR)", "start_pos": 185, "end_pos": 238, "type": "TASK", "confidence": 0.7352422045336829}, {"text": "Generalizable identification of personal health experience mentions", "start_pos": 248, "end_pos": 315, "type": "TASK", "confidence": 0.8638414996010917}]}, {"text": "We participated in subtask 1 and subtask2, and developed two systems based on bidirectional encoder representations from transformers (BERT) for the two subtasks respectively.", "labels": [], "entities": [{"text": "BERT", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9912910461425781}]}, {"text": "The system for subtask 1 achieved the best F1-score of 0.6457, ranking first.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995037317276001}]}, {"text": "Among the systems we developed for subtask2, the best relaxed F1-score and the best strict F1-score were 0.614 and 0.407 respectively.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9575038552284241}, {"text": "F1-score", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9635663032531738}]}], "datasetContent": [{"text": "For task1, we compared BERT and BERT+knowledge base with two classic deep learning methods, and LSTM, and also investigated the effect of different BERT models, including the BERT model publicly released by (https://github.com/google-research/bert) (denoted by BERT_noRetrained) and the BERT model retrained on a large-scale tweet unlabeled corpus based on the previous BERT model (denoted by BERT_Retrained).", "labels": [], "entities": [{"text": "BERT", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9608427882194519}, {"text": "BERT", "start_pos": 175, "end_pos": 179, "type": "METRIC", "confidence": 0.9830675721168518}, {"text": "BERT", "start_pos": 287, "end_pos": 291, "type": "METRIC", "confidence": 0.9312335848808289}, {"text": "BERT_Retrained)", "start_pos": 393, "end_pos": 408, "type": "METRIC", "confidence": 0.9215861111879349}]}, {"text": "The unlabeled corpus consisted of 1,500,000 tweets crawled from Twitter according to 150 drug names collected from the training set.", "labels": [], "entities": []}, {"text": "For task2, we only used the retrained BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9840622544288635}]}, {"text": "In our experiments, we set batch size to 32, learning rate to 5e-5 when training all models.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 45, "end_pos": 58, "type": "METRIC", "confidence": 0.9791015684604645}]}, {"text": "The epoch number was set to 8 for BERT retraining, and 20 for other models.", "labels": [], "entities": [{"text": "epoch number", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9128131568431854}, {"text": "BERT retraining", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7746517658233643}]}, {"text": "The dimension of word embeddings used in TextCNN and LSTM was set to 200.", "labels": [], "entities": [{"text": "TextCNN", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.910420298576355}]}, {"text": "We split out about 10% from the training set as a validation set for parameter optimization.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.770577996969223}]}, {"text": "The performance of all methods for the two tasks were measured by precision, recall and F1-score, which can be calculated by the official tools provided by the organizers.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9997979998588562}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9995666146278381}, {"text": "F1-score", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9996306896209717}]}, {"text": "For task2, there were two criteria for system performance evaluation: relaxed and strict. and show the performance of our systems for task 1 and task 2 on the test set, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 shows the distribution of 0 and 1 labels  over the training and test datasets, where #*  denotes the number of tweets labeled with *, and  NA denotes that the corresponding number is  currently unknown.", "labels": [], "entities": []}, {"text": " Table 2: Statistics of the training and test datasets of  task 2", "labels": [], "entities": []}, {"text": " Table 3: Results on validation and test data for Task 1", "labels": [], "entities": []}, {"text": " Table 4: Results on test data for Task 2", "labels": [], "entities": []}]}