{"title": [{"text": "Dual Monolingual Cross-Entropy-Delta Filtering of Noisy Parallel Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a purely monolingual approach to filtering for parallel data from a noisy corpus in a low-resource scenario.", "labels": [], "entities": []}, {"text": "Our work is inspired by Junczys-Dowmunt (2018), but we relax the requirements to allow for cases where no parallel data is available.", "labels": [], "entities": []}, {"text": "Our primary contribution is a dual monolingual cross-entropy delta criterion modified from Cynical data selection (Axelrod, 2017), and is competitive (within 1.8 BLEU) with the best bilingual filtering method when used to train SMT systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9972085356712341}, {"text": "SMT", "start_pos": 228, "end_pos": 231, "type": "TASK", "confidence": 0.9895106554031372}]}, {"text": "Our approach is featherweight, and runs end-to-end on a standard laptop in three hours.", "labels": [], "entities": []}], "introductionContent": [{"text": "The 2018 WMT shared task on parallel corpus filtering () required participants to select subcorpora of 10M and 100M words from an extremely noisy 1B word German-English parallel corpus from Paracrawl (.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.6679544250170389}, {"text": "parallel corpus filtering", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.5846951405207316}, {"text": "Paracrawl", "start_pos": 190, "end_pos": 199, "type": "DATASET", "confidence": 0.8709111213684082}]}, {"text": "These subcorpora were then used to train machine translation systems, and evaluated on held-out test sets.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8055986166000366}]}, {"text": "The best submission) comprised: 1.", "labels": [], "entities": []}, {"text": "a filter based on language ID 2.", "labels": [], "entities": []}, {"text": "a dual conditional cross-entropy filter to determine whether the halves of a sentence pair were of roughly equal translation probability 3.", "labels": [], "entities": []}, {"text": "a cross-entropy difference filter to prioritize in-domain sentence pairs The 2019 WMT shared task on parallel corpus filtering ( ) was set for lowresource conditions, with the goal of translating Wikipedia texts both Sinhala-to-English and Nepali-to-English ( . We participated only in the Sinhala-English track, basing our system on that of JunczysDowmunt (2018) but extensively modified for the 2019 low-resource scenario.", "labels": [], "entities": [{"text": "parallel corpus filtering", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.6450428863366445}]}, {"text": "As compared to their work, ours comprised: a minor upgrade of their first element, a relaxation of the second, a modern replacement for the third, and an additional length-based filter.", "labels": [], "entities": []}, {"text": "The resulting entirely monolingual pipeline to filter noisy parallel data proved to be competitive with the other multilingual entries when used to train downstream SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 165, "end_pos": 168, "type": "TASK", "confidence": 0.978128969669342}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Parallel Data for Sinhala-English", "labels": [], "entities": []}, {"text": " Table 3: Using SentencePiece to equalize LM perplex- ities in different languages on the dev sets.", "labels": [], "entities": []}, {"text": " Table 4: Bleu scores on test for systems trained on  subsets with 1M and 5M English words of the noisy  Paracrawl data.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9970070719718933}, {"text": "Paracrawl data", "start_pos": 105, "end_pos": 119, "type": "DATASET", "confidence": 0.95095294713974}]}]}