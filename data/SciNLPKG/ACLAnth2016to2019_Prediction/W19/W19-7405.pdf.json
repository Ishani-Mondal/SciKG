{"title": [{"text": "Multi Sense Embeddings from Topic Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Distributed word embeddings have yielded state-of-the-art performance in many NLP tasks, mainly due to their success in capturing useful semantic information.", "labels": [], "entities": []}, {"text": "These representations assign only a single vector to each word whereas a large number of words are pol-ysemous (i.e., have multiple meanings).", "labels": [], "entities": []}, {"text": "In this work, we approach this critical problem in lexical semantics, namely that of representing various senses of polysemous words in vector spaces.", "labels": [], "entities": []}, {"text": "We propose a topic modeling based skip-gram approach for learning multi-prototype word embeddings.", "labels": [], "entities": []}, {"text": "We also introduce a method to prune the embeddings determined by the probabilistic representation of the word in each topic.", "labels": [], "entities": []}, {"text": "We use our embed-dings to show that they can capture the context and word similarity strongly and outperform various state-of-the-art implementations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing words as dense, low dimensional embeddings () allow the representations to capture useful syntactic & semantic information making them useful in downstream Natural Language Processing tasks.", "labels": [], "entities": []}, {"text": "However, these embedding models ignore the lexical ambiguity among different meanings of the same word.", "labels": [], "entities": []}, {"text": "They assign only a single vector representative of all the different meanings of a word.", "labels": [], "entities": []}, {"text": "In this work, we attempt to address this problem by capturing the multiple senses of a word using the global semantics of the document in which the word appears.", "labels": [], "entities": []}, {"text": "indicated that such sense specific vectors improve the performance of applications related to semantic understanding, such as Named Entity Recognition, Part-Of-Speech tagging.", "labels": [], "entities": [{"text": "semantic understanding", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.8452810943126678}, {"text": "Named Entity Recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.6305798292160034}, {"text": "Part-Of-Speech tagging", "start_pos": 152, "end_pos": 174, "type": "TASK", "confidence": 0.735270231962204}]}, {"text": "In this work, we first train a topic model on our corpus to extract the topic distribution for each document.", "labels": [], "entities": []}, {"text": "We treat these extracted topics as a heuristic to model word senses.", "labels": [], "entities": []}, {"text": "We hypothesize that these word senses correlate quite well with the human notion of word senses, and validate it through our rigorous experiments as we demonstrate in our results section.", "labels": [], "entities": []}, {"text": "We then use this topic distribution to train sense-specific word embeddings for each sense.", "labels": [], "entities": []}, {"text": "We train these embeddings by weighing the learning procedure in proportion to the corresponding topic representation for each document.", "labels": [], "entities": []}, {"text": "However, a word need not strongly correlate with each of these extracted senses.", "labels": [], "entities": []}, {"text": "To address it, we propose a variant of this model which restricts the learning to only those embeddings where the word has a strong correlation with the topic extracted, i.e., high p(word|topic).", "labels": [], "entities": []}, {"text": "The major contributions of our work are (i) training multi-sense word embeddings based on structured skip gram using topic models as a precursor (ii) non-parametric approach which prunes the embeddings to capture variability in the number of word senses.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the English Wikipedia corpus dump (Shaoul and Westbury, 2010) for training both, topic models and embedding models.", "labels": [], "entities": [{"text": "English Wikipedia corpus dump (Shaoul and Westbury, 2010)", "start_pos": 11, "end_pos": 68, "type": "DATASET", "confidence": 0.8772000453688882}]}, {"text": "Though many previous research works have used a larger training corpus, but fora fair comparison, we only compare our results with those works which have used the same corpus.", "labels": [], "entities": []}, {"text": "We could also improve obtained results by using a larger training corpus, but this is not central point of our paper.", "labels": [], "entities": []}, {"text": "The main aim of our work is to compute sense specific embed-  dings fora word using topic models and demonstrate the strength of our model empirically.", "labels": [], "entities": []}, {"text": "The raw dataset consists of nearly 3.2 million documents and 1 billion tokens.", "labels": [], "entities": []}, {"text": "Training topic models on such a large and diverse corpus helps in obtaining clearly demarcated senses for each topic.", "labels": [], "entities": []}, {"text": "To tune the hyper parameters of our neural network model, we sample 20% of our corpus as validation data and chose those parameters that give the lowest validation loss.", "labels": [], "entities": []}, {"text": "Later, we use these parameter values for training on the entire corpus.", "labels": [], "entities": []}, {"text": "For all our experiments, we use a skip-window of size 2, 8 negative samples, embeddings of dimensionality 200, and fix the number of topics to 10.", "labels": [], "entities": []}, {"text": "A detailed analysis on how we chose the number of topics, using perplexity score, can be found later in the analysis section.", "labels": [], "entities": [{"text": "perplexity score", "start_pos": 64, "end_pos": 80, "type": "METRIC", "confidence": 0.932337075471878}]}, {"text": "We initialize the embeddings using pre-trained GloVe embeddings to ensure all our target embeddings are in the same vector space.", "labels": [], "entities": []}, {"text": "We choose the value for p thres as 1e-4 and give an analysis on how we chose the parameter value in the results section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman's correlation \u03c1 \u00d7 100 on WS-353", "labels": [], "entities": [{"text": "Spearman's correlation \u03c1", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6697291061282158}, {"text": "WS-353", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.8354212641716003}]}, {"text": " Table 2: Spearman's correlation \u03c1 \u00d7 100 on SCWS", "labels": [], "entities": [{"text": "Spearman's correlation \u03c1", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.718524806201458}, {"text": "SCWS", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8019070029258728}]}, {"text": " Table 3: Results on Word Analogy task", "labels": [], "entities": [{"text": "Word Analogy", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.7265500873327255}]}, {"text": " Table 4: Nearest neighbours of some polysemous words for Glove, and for each sense identified by our algorithm,  based on the cosine similarility. We take only those senses corresponding to topics where p(w t |j) > p thres .", "labels": [], "entities": [{"text": "Glove", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8113369345664978}]}, {"text": " Table 6: effect of number of topics on Spearman corre- lation on 50 word pairs from WordSim-353", "labels": [], "entities": [{"text": "Spearman corre- lation", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6768409013748169}, {"text": "WordSim-353", "start_pos": 85, "end_pos": 96, "type": "DATASET", "confidence": 0.9069616198539734}]}, {"text": " Table 7: Spearman's correlation \u03c1 \u00d7 100 on 50 word  pairs from WS-353 and the word senses captured for  network. The word senses are adjudged qualitatively.", "labels": [], "entities": [{"text": "Spearman's correlation \u03c1", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.5905838012695312}, {"text": "WS-353", "start_pos": 64, "end_pos": 70, "type": "DATASET", "confidence": 0.9498722553253174}]}]}