{"title": [], "abstractContent": [{"text": "Diachronic word embeddings play a key role in capturing interesting patterns about how language evolves overtime.", "labels": [], "entities": []}, {"text": "Most of the existing work focuses on studying corpora spanning across several decades, which is understandably still not a possibility when working on social media-based user-generated content.", "labels": [], "entities": []}, {"text": "In this work, we address the problem of studying semantic changes in a large Twitter corpus collected over five years, a much shorter period than what is usually the norm in di-achronic studies.", "labels": [], "entities": []}, {"text": "We devise a novel attentional model, based on Bernoulli word embeddings, that are conditioned on contextual extra-linguistic (social) features such as network, spatial and socioeconomic variables, which are associated with Twitter users, as well as topic-based features.", "labels": [], "entities": []}, {"text": "We posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem.", "labels": [], "entities": []}, {"text": "Our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain con-textual features are absent.", "labels": [], "entities": []}, {"text": "Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language changes overtime due to a wide range of linguistic, psychological, sociocultural and encyclopedic causes.", "labels": [], "entities": []}, {"text": "Studying the semantic change of a word helps us understand more about the human language and build temporally aware models, that are especially complementary to the work done in the digital humanities and his- torical linguistics.", "labels": [], "entities": []}, {"text": "Recently, diachronic word embeddings based on distributional hypothesis have been used to automatically study semantic changes in a data-driven fashion from large corpora (.", "labels": [], "entities": []}, {"text": "We refer the reader to who survey the recent methods in this field and establishes the challenges that lie ahead.", "labels": [], "entities": []}, {"text": "Currently, we find the literature on this problem to be focused on English corpora, spanning across several decades.", "labels": [], "entities": []}, {"text": "This has not only created a gap in extending the diachronic word embeddings fora wider scope of languages, but also to datasets spanning across few successive years which are common in digital humanities and social sciences.", "labels": [], "entities": []}, {"text": "In this work, we study French text from Twitter collected over just five years, which provides a challenging platform to build models that can capture semantic drifts in a noisy, subtly evolving language corpus.", "labels": [], "entities": []}, {"text": "shows an instance of the evolution of the word 'Bataclan' (a theatre in Paris that was at-tacked by terrorists on November 2015) from the French corpus.", "labels": [], "entities": []}, {"text": "It also shows that such embedding representations mostly capture the dominant sense of a word when used in synchrony and can therefore only reflect the evolution of the dominant sense when used diachronically, yet leaving open the question of whether small, subtle changes can be captured (.", "labels": [], "entities": []}, {"text": "We hypothesize that the current state-of-the-art models lack inductive biases to fit data accurately in this setting.", "labels": [], "entities": []}, {"text": "We build on the observation by that \"it's important to consider who produced the language, in what context, for what purpose, and make sure that the models are fit to the data\".", "labels": [], "entities": []}, {"text": "Hence, we propose a novel model extending on Dynamic Bernoulli word) (DBE) which exploits the inductive bias by conditioning on a number of contextualized features such as network, spatial and socio-economic variables, which are associated with Twitter users, as well as topicbased features.", "labels": [], "entities": []}, {"text": "We perform qualitative studies and show that our model can: (i) accurately capture the subtle changes caused due to cultural drifts, (ii) learn a smooth trajectory of word evolution despite exploiting various inductive biases.", "labels": [], "entities": [{"text": "word evolution", "start_pos": 167, "end_pos": 181, "type": "TASK", "confidence": 0.7483985424041748}]}, {"text": "Our quantitative studies illustrate that our model can: (i) capture better semantic properties, (ii) be less sensitive to frequency cues compared to DBE model, (iii) act as better features for 2 out of 4 tweet classification tasks.", "labels": [], "entities": [{"text": "tweet classification tasks", "start_pos": 204, "end_pos": 230, "type": "TASK", "confidence": 0.7853818535804749}]}, {"text": "Through an ablation study, we find in addition that our model can: (iv) work with a reduced set of contextualized features, (v) follow the test of law of prototypicality (.", "labels": [], "entities": []}, {"text": "In sum, we believe our model is a promising tool to study diachronic semantic changes over small time periods.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: \u2022 Our work is the first to study diachronic word embeddings for tweets from French language to the best of our knowledge.", "labels": [], "entities": []}, {"text": "Unlike previous works, we consider dataset from a narrow time horizon (five years).", "labels": [], "entities": []}, {"text": "\u2022 We propose a novel, attentional, diachronic word embedding model that derives inductive biases from several contextualized, sociodemographic, features to fit the data accurately.", "labels": [], "entities": []}, {"text": "\u2022 Our work is also the first to estimate the usefulness of the diachronic word embeddings for downstream task like tweet classification., and proposed to learn word embeddings across all time periods jointly along with their alignment in a single step.", "labels": [], "entities": [{"text": "tweet classification.", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.7343590259552002}]}, {"text": "represent word embeddings as sequential latent variables, naturally accommodating for time slices with sparse data and assuring word embeddings are grounded across time.", "labels": [], "entities": []}, {"text": "Our proposed model builds upon this work to condition on several inductive biases, using contextual extra-linguistic (social) and topicbased features, to accurately fit dataset from a narrow time horizon.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss the experimental protocol, qualitative and quantitative evaluation to understand the performance of our model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Embedding neighborhood of 'EMMANUEL' obtained by finding closest word in each time period sorted  by decreasing similarity. All named entities are italicized. Interesting words identified by the proposed model are  bolded.", "labels": [], "entities": [{"text": "EMMANUEL", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9492177963256836}]}, {"text": " Table 2: Embedding neighborhood of EQUIPEDEFRANCE 'French Team' in obtained by finding the closest word  in each time period sorted by decreasing similarity. All named entities are italicized. Interesting words identified  by the model are bolded.", "labels": [], "entities": [{"text": "EQUIPEDEFRANCE 'French Team'", "start_pos": 36, "end_pos": 64, "type": "DATASET", "confidence": 0.8059946298599243}]}, {"text": " Table 3: Quantitative results based on log likelihood, semantic similarity and tweet classification. Higher numbers  are better for all the tasks. Statistically significant differences to the best baseline for each task based on bootstrap  test are marked with an asterisk. Note that we could not perform statistical significance studies for log likelihood  experiment due to the large size of the test set and semantic similarity experiment due to the nature of clustering  evaluation.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.6865556091070175}]}, {"text": " Table 4: Ablation Results for contextual attention model based on log likelihood, semantic similarity and tweet  classification.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9958783388137817}, {"text": "tweet  classification", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6832007020711899}]}, {"text": " Table 5: Probing task accuracies. See Conneau et al. (2018) for the details of probing tasks and classifier used.", "labels": [], "entities": []}, {"text": " Table 6: Ablation results based on log likelihood, semantic similarity and tweet classification.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9956632256507874}, {"text": "tweet classification", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.6786143332719803}]}]}