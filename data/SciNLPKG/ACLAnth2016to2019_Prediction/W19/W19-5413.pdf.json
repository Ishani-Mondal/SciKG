{"title": [{"text": "Unbabel's Submission to the WMT2019 APE Shared Task: BERT-based Encoder-Decoder for Automatic Post-Editing", "labels": [], "entities": [{"text": "WMT2019 APE", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.5435140132904053}, {"text": "BERT-based", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9964752793312073}]}], "abstractContent": [{"text": "This paper describes Unbabel's submission to the WMT2019 APE Shared Task for the English-German language pair.", "labels": [], "entities": [{"text": "WMT2019 APE Shared Task", "start_pos": 49, "end_pos": 72, "type": "TASK", "confidence": 0.5666758269071579}]}, {"text": "Following the recent rise of large, powerful, pre-trained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework.", "labels": [], "entities": [{"text": "BERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9570477604866028}]}, {"text": "Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder (BED) model in which a single pretrained BERT encoder receives both the source src and machine translation mt strings.", "labels": [], "entities": [{"text": "BERT-based encoder-decoder (BED", "start_pos": 55, "end_pos": 86, "type": "METRIC", "confidence": 0.7660170793533325}]}, {"text": "Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits.", "labels": [], "entities": []}, {"text": "As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservative-ness penalty improves significantly the translations of a strong Neural Machine Translation (NMT) system by \u22120.78 and +1.23 in terms of TER and BLEU, respectively.", "labels": [], "entities": [{"text": "BED", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9666861891746521}, {"text": "Neural Machine Translation (NMT)", "start_pos": 214, "end_pos": 246, "type": "TASK", "confidence": 0.838243176539739}, {"text": "TER", "start_pos": 285, "end_pos": 288, "type": "METRIC", "confidence": 0.9989509582519531}, {"text": "BLEU", "start_pos": 293, "end_pos": 297, "type": "METRIC", "confidence": 0.9972580671310425}]}, {"text": "Finally, our submission achieves anew state-of-the-art, ex-aequo, in English-German APE of NMT.", "labels": [], "entities": [{"text": "APE", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9365159273147583}, {"text": "NMT", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.8666921854019165}]}], "introductionContent": [{"text": "Automatic Post Editing (APE) aims to improve the quality of an existing Machine Translation (MT) system by learning from human edited samples.", "labels": [], "entities": [{"text": "Automatic Post Editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7183144887288412}, {"text": "Machine Translation (MT)", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.8620529770851135}]}, {"text": "It first started by the automatic article selection for English noun phrases and continued by correcting the errors of more complex statistical MT systems ().", "labels": [], "entities": [{"text": "article selection for English noun phrases", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.7868446360031763}, {"text": "MT", "start_pos": 144, "end_pos": 146, "type": "TASK", "confidence": 0.9509316086769104}]}, {"text": "In 2018, the organizers of the WMT shared task introduced, for the first time, the automatic post-editing of neural MT systems ().", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.5805240472157797}]}, {"text": "Despite its successful application to SMT systems, it has been more challenging to automatically post edit the strong NMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9911494255065918}]}, {"text": "This mostly is due to the fact that high quality NMT systems make fewer mistakes, limiting the improvements obtained by state-of-the-art APE systems such as self-attentive transformer-based models.", "labels": [], "entities": []}, {"text": "In spite of these findings and considering the dominance of the NMT approach in both the academic and industrial applications, the WMT shared task organizers decided to move completely to the NMT paradigm this year and ignore the SMT technology.", "labels": [], "entities": [{"text": "WMT shared task", "start_pos": 131, "end_pos": 146, "type": "DATASET", "confidence": 0.7357257803281149}, {"text": "SMT", "start_pos": 230, "end_pos": 233, "type": "TASK", "confidence": 0.9897300004959106}]}, {"text": "They also provide the previous year in-domain training set (i.e. 13k of <src,mt,pe> triplets) further increasing the difficulty of the task.", "labels": [], "entities": []}, {"text": "Training state-of-the-art APE systems capable of improving high quality NMT outputs requires large amounts of training data, which is not always available, in particular for this WMT shared task.", "labels": [], "entities": [{"text": "APE", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.904044508934021}, {"text": "WMT shared task", "start_pos": 179, "end_pos": 194, "type": "TASK", "confidence": 0.8604336778322855}]}, {"text": "Augmenting the training set with artificially synthesized data is one of the popular and effective approaches for coping with this challenge.", "labels": [], "entities": []}, {"text": "It was first used to improve the quality of NMT systems ( and then it was applied to the APE task.", "labels": [], "entities": [{"text": "APE task", "start_pos": 89, "end_pos": 97, "type": "TASK", "confidence": 0.8711941540241241}]}, {"text": "This approach, however, showed limited success on automatically post editing the high quality translations of APE systems.", "labels": [], "entities": []}, {"text": "Transfer learning is another solution to deal with data sparsity in such tasks.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.96796053647995}]}, {"text": "It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains.", "labels": [], "entities": []}, {"text": "Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN (Devlin et al., 2018a;, have obtained stateof-the-art results when fine-tuned over a small set of training samples.", "labels": [], "entities": [{"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.9958781003952026}]}, {"text": "Following, in this paper we use BERT) within the encoder-decoder framework ( \u00a72.1) and formulate the task of Automatic Post Editing as generating pe which is (possibly) the modified version of mt given the original source sentence src.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9979957342147827}, {"text": "Automatic Post Editing", "start_pos": 109, "end_pos": 131, "type": "TASK", "confidence": 0.5406753122806549}]}, {"text": "As discussed in \u00a72.1, instead of using multi-encoder architecture, in this work we concatenate the src and mt with the BERT special token (i.e. and feed them to our single encoder.", "labels": [], "entities": [{"text": "BERT", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.984345018863678}]}, {"text": "We also introduce the conservativeness penalty, a simple yet effective mechanism that controls the freedom of our APE in modifying the given MT output.", "labels": [], "entities": []}, {"text": "As we show in \u00a72.2, in the cases where the automatic translations are of high quality, this factor forces the APE system to do less modifications, hence avoids the well-known problem of over-correction.", "labels": [], "entities": []}, {"text": "Finally, we augmented our original in-domain training data with a synthetic corpus which contains around 3M <src,mt,pe> triplets ( \u00a73.1).", "labels": [], "entities": []}, {"text": "As discussed in \u00a74, our system is able to improve significantly the MT outputs by \u22120.78 TER () and +1.23 BLEU (), achieving an ex-aequo firstplace in the English-German track.", "labels": [], "entities": [{"text": "MT", "start_pos": 68, "end_pos": 70, "type": "TASK", "confidence": 0.7657873630523682}, {"text": "TER", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.998501181602478}, {"text": "BLEU", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9985604882240295}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: TER scores of the baseline NMT system and our BERT encoder-decoder ape model. The columns \"w/o  c\", \"best c\", and \"worst c\" presents the scores of our system without the conservativeness penalty, with the best  and the worst conservativeness penalty settings on our dev corpus, respectively. \"logprobs\" and \"logits\" refer,  respectively, to the state where we apply the conservativeness factor (see  \u00a72.2)", "labels": [], "entities": [{"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9859644174575806}, {"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.6546687483787537}]}, {"text": " Table 2: Submission at the WMT APE shared task.", "labels": [], "entities": [{"text": "WMT APE shared task", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.6893182247877121}]}, {"text": " Table 3: APE Results as provided by the shared task or- ganizers. We only present the best score of each team.  indicates not statistically significantly different, ex  aequo.", "labels": [], "entities": [{"text": "APE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8426595330238342}]}]}