{"title": [{"text": "Equipping Educational Applications with Domain Knowledge", "labels": [], "entities": [{"text": "Equipping Educational Applications", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8494410316149393}]}], "abstractContent": [{"text": "One of the challenges of building natural language processing (NLP) applications for education is finding a large domain-specific corpus for the subject of interest (e.g., history or science).", "labels": [], "entities": []}, {"text": "To address this challenge, we propose a tool, Dexter, that extracts a subject-specific corpus from a heterogeneous corpus, such as Wikipedia, by relying on a small seed corpus and distributed document representations.", "labels": [], "entities": []}, {"text": "We empirically show the impact of the generated corpus on language modeling, estimating word embeddings, and consequently, distractor generation, resulting in a better performance than while using a general domain corpus, a heuristically constructed domain-specific corpus, and a corpus generated by a popular system: BootCaT.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.706645131111145}, {"text": "distractor generation", "start_pos": 123, "end_pos": 144, "type": "TASK", "confidence": 0.718412309885025}, {"text": "BootCaT", "start_pos": 318, "end_pos": 325, "type": "DATASET", "confidence": 0.9310044646263123}]}], "introductionContent": [{"text": "Educational applications tend to target a specific subject, in other words, a specific domain, such as the medical domain in the case of).", "labels": [], "entities": []}, {"text": "Thus, building these applications with underlying NLP algorithms, would typically require a large domain-specific corpus.", "labels": [], "entities": []}, {"text": "Example uses of these large corpora are estimating language models), estimating word embeddings (, and estimating document embeddings ().", "labels": [], "entities": []}, {"text": "These estimations are central to several downstream applications including automatic speech recognition, machine translation (, and text categorization (.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.6119641264279684}, {"text": "machine translation", "start_pos": 105, "end_pos": 124, "type": "TASK", "confidence": 0.8394780457019806}, {"text": "text categorization", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.7708043456077576}]}, {"text": "Previous findings, such as, have shown that training NLP applications on a domain different from the target domain could prove detrimental to the performance of these applications.", "labels": [], "entities": []}, {"text": "In order to help educational applications in specific disciplines such as science and history create a large, yet domain-specific corpus, we propose a domain extraction tool, Dexter , that extracts a domain-specific corpus from Wikipedia.", "labels": [], "entities": []}, {"text": "The algorithm, elaborated in Section 2, retrieves a set of documents from Wikipedia that are closest indiscipline to a user-supplied small seed corpus.", "labels": [], "entities": []}, {"text": "The size of this extracted set is a user-defined hyperparameter, and thus controls the trade-off between the specificity of the output corpus and its size.", "labels": [], "entities": []}, {"text": "We empirically determine the favorable configuration of Dexter, demonstrate its benefits towards estimating word embeddings, and consequently distractor generation, as well as language models.", "labels": [], "entities": [{"text": "distractor generation", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.7040401548147202}]}, {"text": "We also show how, on the aformentioned tasks, Dexter outperforms BootCaT, a popular toolkit to automatically create an Internet-derived corpus ().", "labels": [], "entities": []}, {"text": "Datasets used in this research are released for public use 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform three main experiments.", "labels": [], "entities": []}, {"text": "The first is an intrinsic evaluation of Dexter, guiding our design choices.", "labels": [], "entities": []}, {"text": "Second, we check the effect of the domain-specificity of the resulting word embeddings on the downstream educational task of distractor generation for science questions.", "labels": [], "entities": [{"text": "distractor generation", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.7419770658016205}]}, {"text": "Third, we evaluate the effect of Dexter on language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8212563693523407}]}, {"text": "Before we delve on the experimental details, we describe the process of labeling Wikipedia articles as science or not, and describe our competitive baseline: BootCaT.", "labels": [], "entities": [{"text": "BootCaT", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9112638235092163}]}, {"text": "Dataset Preparation: Wikipedia does not clearly partition its articles into different domains, but instead assigns a set of categories to each article.", "labels": [], "entities": [{"text": "Dataset Preparation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6055993437767029}]}, {"text": "Wikipedia also organizes its categories as a directed graph, where, if category b is a subcategory of category a, then there exists an edge (a, b).", "labels": [], "entities": []}, {"text": "Although it might seem natural to consider all descendants of a category (domain in abroad sense) to belong to that category, upon inspection we found the need for setting a The embedding of a document is the average of its words' embeddings weighted by a word's TFIDF score depth limit.", "labels": [], "entities": [{"text": "TFIDF score depth limit", "start_pos": 263, "end_pos": 286, "type": "METRIC", "confidence": 0.9682610034942627}]}, {"text": "For example, \"Stalking\" is a third descendant of \"Biology\", although \"Stalking\" is not considered a \"Biology\"-related subject.", "labels": [], "entities": []}, {"text": "Based on similar observations, we set the depth limit to two.", "labels": [], "entities": [{"text": "depth", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.965928852558136}]}, {"text": "To identify science articles, we consider a list of science root categories, and all their subcategories up to a depth of 2.", "labels": [], "entities": []}, {"text": "All articles labeled with any category in this list are considered science articles, amounting to 176,905 articles.", "labels": [], "entities": []}, {"text": "This collected science corpus will be referred to as CD , while the general Wikipedia corpus will be referred to as C.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 76, "end_pos": 92, "type": "DATASET", "confidence": 0.8991508185863495}]}, {"text": "We note that the corpus CD is created using heuristics on Wikipedia's taxonomy.", "labels": [], "entities": []}, {"text": "In order to assess the extent to which the articles in CD belong to the discipline of science, we do the following.", "labels": [], "entities": []}, {"text": "Two annotators assess the quality of CD by taking a subset of 1000 articles equally spread across depths: 0, 1, 2, and 3.", "labels": [], "entities": []}, {"text": "Each article (depth anonymized) was given an integer score between 0 and 5 to reflect how much the content of the article is related to science.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement on the scores had a Pearson's correlation coefficient of 0.77 suggesting a reasonably high agreement on the scores.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 50, "end_pos": 83, "type": "METRIC", "confidence": 0.9667007774114609}]}, {"text": "Upon comparing the average scores of articles at different depths, we found the score to be inversely proportional to the depth, with scores 4.35, 3.58, 3.21, and 2.4 for articles at depth 0, 1, 2, and 3 respectively.", "labels": [], "entities": []}, {"text": "This further justifies our depth limit of 2, below which the average score suddenly drops below 3.", "labels": [], "entities": [{"text": "depth limit", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.9467501938343048}, {"text": "average score", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.961796909570694}]}, {"text": "CD reflects the quality of such a corpus heuristically-constructed using Wikipedia's taxonomy.", "labels": [], "entities": []}, {"text": "Baseline: One popular system used by researchers to extract a domain-specific corpus is BootCaT (), which operates on the World Wide Web.", "labels": [], "entities": []}, {"text": "BootCaT generates queries to a search engine from user-supplied key phrases and parses the first n pages retrieved for each query, where n is set by the user.", "labels": [], "entities": []}, {"text": "Since Dexter requires seed articles instead of key phrases, we bridge the gap by a key phrase extraction algorithm () on the set of seed articles, by utilizing a publicly available implementation 6 with default parameters.", "labels": [], "entities": [{"text": "key phrase extraction", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.7176545262336731}]}, {"text": "We thus take C seed and extract the top-100 key phrases with less than 4 words, then feed them to BootCaT, leading to a domain-specific corpus (referred to as BootCat-KE).", "labels": [], "entities": [{"text": "BootCaT", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9279115200042725}]}, {"text": "To avoid any possible noise introduced by the keyword extraction algorithm we consider another version of the BootCaT baseline but now with a manual set of 100 key phrases describing the domain of science.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7535206377506256}, {"text": "BootCaT baseline", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9608398377895355}]}, {"text": "We take a list of science key phrases available online 7 , and then randomly select 100 phrases.", "labels": [], "entities": []}, {"text": "The corpus generated by this algorithm is referred to as BootCat-M.", "labels": [], "entities": [{"text": "BootCat-M", "start_pos": 57, "end_pos": 66, "type": "DATASET", "confidence": 0.966227650642395}]}, {"text": "Before we analyze Dexter's performance on downstream tasks and compare it to BootCaT, we study the intrinsic performance of Dexter under several design choices and conditions.", "labels": [], "entities": [{"text": "BootCaT", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9032293558120728}]}, {"text": "Our evaluation of Dexter is based on the precision of the extracted articles averaged over 5 runs.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9989672899246216}]}, {"text": "Since we would be manipulating the seed set size, and to ensure Dexter's robustness under randomness, we artificially construct our seed set by taking a random subset of CD instead of using C seed . That seed set is then used to algorithmically extract the rest of CD via Dexter.", "labels": [], "entities": []}, {"text": "Accordingly, precision is calculated as the percentage of articles extracted, which belong to CD \u2212C seed . As for recall, it is not measured since precision is sufficient as a comparison between methods assuming same number of documents extracted.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995163679122925}, {"text": "CD \u2212C seed", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.703248918056488}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9882739782333374}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9982091188430786}]}, {"text": "Document representation: We vary the document representation method while fixing the seed corpus size at 10 3 , and the distance function as Mean (c.f..", "labels": [], "entities": [{"text": "Document representation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8412544429302216}, {"text": "Mean", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.986767590045929}]}, {"text": "We observe that LSA and TFIDF are initially superior, but perform comparably to Doc2Avg and Doc2wAvg ask increases.", "labels": [], "entities": [{"text": "LSA", "start_pos": 16, "end_pos": 19, "type": "METRIC", "confidence": 0.855632483959198}, {"text": "TFIDF", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9002376794815063}, {"text": "Doc2Avg", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.938566267490387}]}, {"text": "LSA is chosen due to its low-dimensionality, and superiority for modest k values.", "labels": [], "entities": []}, {"text": "Distance function: We vary the distance function used while fixing the document representation to LSA and the seed corpus size to 1000 (c.f.).", "labels": [], "entities": []}, {"text": "We observe that the 10 th percentile distance function leads to the best precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.99895179271698}]}, {"text": "We hypothesize that this is due to the 10 th percentile being robust to noise, and requiring closeness to only one subdomain of science rather than all at once.", "labels": [], "entities": []}, {"text": "Seed corpus size: We vary the seed corpus size while fixing the document representation to LSA and the distance function as 10 th percentile (c.f.).", "labels": [], "entities": [{"text": "LSA", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.8804560899734497}]}, {"text": "We observe that a size of 100 was equally sufficient to 1000.", "labels": [], "entities": []}, {"text": "This shows that Dexter does not require an unfeasibly large seed corpus size, which would have defeated the purpose.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distractor recall@100 for word embeddings  (middle) and perplexity of language models (right)  trained on corpora of varying domain specificity.", "labels": [], "entities": [{"text": "Distractor", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9155699014663696}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.5341768264770508}]}]}