{"title": [{"text": "Customizing Neural Machine Translation for Subtitling", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.6568506260712942}, {"text": "Subtitling", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.889427125453949}]}], "abstractContent": [{"text": "In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment.", "labels": [], "entities": []}, {"text": "The neural translation model was adapted to the subti-tling content and style and extended by a simple , yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7630424201488495}]}, {"text": "The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human seg-mentation decisions.", "labels": [], "entities": [{"text": "subtitle segmentation", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7490507364273071}]}, {"text": "This model is combined with subtitle length and duration constraints established in the subtitling industry.", "labels": [], "entities": []}, {"text": "We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom).", "labels": [], "entities": []}, {"text": "It showed a notable productivity increase of up to 37% as compared to translating from scratch and significant reductions inhuman translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.", "labels": [], "entities": [{"text": "edit rate", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.765349805355072}]}], "introductionContent": [{"text": "In recent years, significant progress was observed in neural machine translation (NMT), with its quality increasing dramatically as compared to the previous generation of statistical phrase-based MT systems.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8162351548671722}, {"text": "MT", "start_pos": 196, "end_pos": 198, "type": "TASK", "confidence": 0.8206546306610107}]}, {"text": "However, user acceptance in the subtitling community has so far been rare.", "labels": [], "entities": []}, {"text": "The reason for this, in our opinion, is that the state-of-theart off-the-shelf NMT systems do not address the issues and challenges of the subtitling process in full.", "labels": [], "entities": []}, {"text": "In this paper, we present a customized NMT system for subtitling, with focus on the entertain- * equal contribution ment domain.", "labels": [], "entities": []}, {"text": "From the user perspective, we show how the quality of translation and subtitle segmentation can improve in such away that significantly reduced post-editing is required.", "labels": [], "entities": [{"text": "translation and subtitle segmentation", "start_pos": 54, "end_pos": 91, "type": "TASK", "confidence": 0.6641086712479591}]}, {"text": "We believe that such customized systems would lead to greater user acceptance in the subtitling industry and would contribute to the wider adoption of NMT technology with the subsequent benefits the latter brings in terms of productivity gain and time efficiency in subtitling workflows.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We start with the review of related research in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the details of our baseline NMT system and how it compares to NMT systems from previous research.", "labels": [], "entities": []}, {"text": "Section 4 presents the details of the changes to the MT system that were necessary to boost its performance on the subtitling tasks for entertainment domain, with a focus on Latin American Spanish as the target language.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9255864024162292}]}, {"text": "In Section 5, we present a novel algorithm for automatic subtitle segmentation that is combined with rule-based constraints which are necessary for correct subtitle representation on the screen.", "labels": [], "entities": [{"text": "subtitle segmentation", "start_pos": 57, "end_pos": 78, "type": "TASK", "confidence": 0.7256367951631546}]}, {"text": "Finally, Section 6 describes the automatic and human evaluation of the proposed system, including post-editing experiments and feedback from professional translators.", "labels": [], "entities": []}], "datasetContent": [{"text": "We computed automatic MT metrics BLEU (), TER (), and CharacTER () on the first part of each template for which we now had two independent human reference translations.", "labels": [], "entities": [{"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.9168447852134705}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.771848738193512}, {"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9972670078277588}, {"text": "CharacTER", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9889278411865234}]}, {"text": "We computed the scores three times using different evaluation modes.", "labels": [], "entities": []}, {"text": "In the mode (W), we computed the scores and error rates on the full sentences; thus, pure MT quality is evaluated, and any segmentation decisions are ignored.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9827924370765686}]}, {"text": "In the (S) mode, we compared the subtitles with each other.", "labels": [], "entities": []}, {"text": "Thus, any words and phrases wrongly placed in a different (e.g. previous or next) subtitle would count as errors.", "labels": [], "entities": []}, {"text": "Finally, in the (L) mode we additionally add a special symbol to represent a line break (in rare cases, two breaks) within a subtitle.", "labels": [], "entities": []}, {"text": "Thus, an incorrect line break is an extra token error that directly affects all error metrics.", "labels": [], "entities": []}, {"text": "To summarize, the (S) and (L) evaluation modes jointly judge the MT and segmentation quality, whereas the (W) mode only judges the MT quality.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9309908151626587}, {"text": "MT", "start_pos": 131, "end_pos": 133, "type": "TASK", "confidence": 0.9122793078422546}]}, {"text": "shows these results for the Home video.", "labels": [], "entities": [{"text": "Home video", "start_pos": 28, "end_pos": 38, "type": "DATASET", "confidence": 0.9855368435382843}]}, {"text": "We observe an improvement in BLEU from 52.3 to 53.6%, as computed with two reference translations, when comparing the baseline system with the adapted one that uses previous sentence context.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9994394183158875}]}, {"text": "This improvement becomes much larger in the (S) and (L) evaluation modes, which confirms the quality of the segmentation algorithm as compared with the baseline heuristics-only segmenta-  tion.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 108, "end_pos": 120, "type": "TASK", "confidence": 0.9586412906646729}]}, {"text": "The other error measures improve similarly with the adapted MT output and the proposed segmentation algorithm.", "labels": [], "entities": [{"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.8645842671394348}, {"text": "segmentation", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.9594297409057617}]}, {"text": "We also show the result of the adapted system, but with the baseline segmentation.", "labels": [], "entities": []}, {"text": "The result for this system is slightly better than for the baseline due to the generally better MT quality, but because of the incorrect segmentation it is very far from human references when the evaluation is performed on the level of subtitles.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9287572503089905}]}, {"text": "On part 1 of the Lucy sitcom, the improvements with the adapted system are more significant when the MT quality alone is evaluated.", "labels": [], "entities": [{"text": "Lucy sitcom", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8852295875549316}, {"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9456547498703003}]}, {"text": "This is expected, since the style of the input is further away from the general-domain (news) data that was used to train the baseline system.", "labels": [], "entities": []}, {"text": "On the other hand, the improvements with the new segmentation algorithm w.r.t baseline segmentation seem to be significant, but less pronounced, since here we are dealing with generally shorter subtitles, many of them one-liners.", "labels": [], "entities": [{"text": "segmentation algorithm w.r.t baseline segmentation", "start_pos": 49, "end_pos": 99, "type": "TASK", "confidence": 0.5825493693351745}]}, {"text": "Nevertheless, the improvement in the (L) evaluation mode, where incorrect line breaks within a subtitle are considered as errors, is as large as 8 BLEU percentage points absolute, from 21.8 to 30.4%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9992184638977051}]}, {"text": "Table 2 also shows the results for the adapted system, but when translating individual sentences without inter-sentence context (lines sent-level).", "labels": [], "entities": []}, {"text": "We observed only insignificant reduction of the pure MT quality in BLEU and TER; the CharacTER even improved.", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9826993942260742}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9977076053619385}, {"text": "TER", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9947223663330078}, {"text": "CharacTER", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.7404302358627319}]}, {"text": "The test sample was too small to make any conclusions here.", "labels": [], "entities": []}, {"text": "Nevertheless, we observed cases where the translation of some words (e.g., pronouns) was better when consecutive short sentences were translated as a single unit as described  in Section 4.3, and the improvement could only be explained by the additional context.", "labels": [], "entities": [{"text": "translation of some words (e.g., pronouns)", "start_pos": 42, "end_pos": 84, "type": "TASK", "confidence": 0.7254450453652276}]}, {"text": "We computed the HTER scores (TER against the post-edited MT output) for the parts 2 and 3 of both files.", "labels": [], "entities": [{"text": "HTER scores", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9724236726760864}, {"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.978270411491394}]}, {"text": "We also computed the subtitle error rate (SER), that we defined as the percentage of subtitles which were changed by the post-editor (not counting possible corrections of the line breaks within a subtitle).", "labels": [], "entities": [{"text": "subtitle error rate (SER)", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.8017420818408331}]}, {"text": "shows the HTER and SER results for the Home documentary.", "labels": [], "entities": [{"text": "HTER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.938412606716156}, {"text": "SER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9864085912704468}, {"text": "Home documentary", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9847661256790161}]}, {"text": "We see that the HTER is consistently better for both posteditors when the adapted MT output is used.", "labels": [], "entities": [{"text": "HTER", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9853030443191528}]}, {"text": "PE1 especially finds the adapted MT output acceptable and keeps approximately 1/3 of the subtitles completely unchanged.", "labels": [], "entities": [{"text": "PE1", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8103747367858887}, {"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9440866112709045}]}, {"text": "The second post-editor makes more corrections in general, but also for him the number of corrections made on the adapted MT output is significantly lower.", "labels": [], "entities": [{"text": "MT", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.8355251550674438}]}, {"text": "The numbers above have to betaken with a grain of salt, since there was no other way but to compare the post-editing effort on different parts of the file.", "labels": [], "entities": []}, {"text": "However, even when we compare the adapted MT output on part 2 against the post-edited baseline MT output, we obtain lower HTER and SER scores than for the baseline MT output itself.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9479940533638}, {"text": "HTER", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9864734411239624}, {"text": "SER", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9925260543823242}]}, {"text": "This again underlines the high quality of the adapted MT output with proper subtitle segmentation.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9799888134002686}]}, {"text": "Similar conclusions can be made from the HTER and SER results in for the Lucy sitcom.", "labels": [], "entities": [{"text": "HTER", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8961994647979736}, {"text": "SER", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9958694577217102}, {"text": "Lucy sitcom", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8956709802150726}]}, {"text": "Here, the number of corrections is generally higher, but the reduction of the post-editing effort when post-editing the adapted vs. baseline MT is very significant, e.g. from 73.8 to 44.0% HTER for PE1 (as measured on different parts of the file with similar translation difficulty).: Productivity and time gain by using baseline/adapted MT output as compared to translating \"from scratch\".", "labels": [], "entities": [{"text": "HTER", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9973896145820618}]}], "tableCaptions": [{"text": " Table 1: Case-sensitive MT error measures on part 1 of  the Home documentary computed in 3 different modes:", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9525521993637085}, {"text": "Home documentary", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9512515068054199}]}, {"text": " Table 2: Case-sensitive MT error measures on part 1 of  the Lucy: The Bean Queen documentary computed as  in Table 1.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9601836800575256}, {"text": "Lucy: The Bean Queen documentary", "start_pos": 61, "end_pos": 93, "type": "DATASET", "confidence": 0.8564087450504303}]}, {"text": " Table 3: Case-sensitive Human Translation Edit Rate  (HTER) and Subtitle Edit Rate (SER) on the post- edited parts 2 and 3 of the Home documentary.  *  The  comparison of the adapted NMT on section 2 is against  the human post-editing of the baseline NMT output.", "labels": [], "entities": [{"text": "Case-sensitive Human Translation Edit Rate  (HTER)", "start_pos": 10, "end_pos": 60, "type": "METRIC", "confidence": 0.7776552774012089}, {"text": "Subtitle Edit Rate (SER)", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.8860870003700256}, {"text": "Home documentary", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9608422517776489}]}, {"text": " Table 4: Case-sensitive Human Translation Edit Rate  (HTER) and Subtitle Edit Rate (SER) on the post- edited parts 2 and 3 of the Lucy: The Bean Queen  documentary.  *  The comparison of the adapted NMT on  part 2 is against the human post-editing of the baseline  NMT output.", "labels": [], "entities": [{"text": "Case-sensitive Human Translation Edit Rate  (HTER)", "start_pos": 10, "end_pos": 60, "type": "METRIC", "confidence": 0.7928359806537628}, {"text": "Subtitle Edit Rate (SER)", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.892765998840332}, {"text": "Lucy: The Bean Queen  documentary", "start_pos": 131, "end_pos": 164, "type": "DATASET", "confidence": 0.7851366897424062}]}, {"text": " Table 5: Productivity and time gain by using base- line/adapted MT output as compared to translating  \"from scratch\".", "labels": [], "entities": []}]}