{"title": [{"text": "Learning Word Embeddings without Context Vectors", "labels": [], "entities": []}], "abstractContent": [{"text": "Most word embedding algorithms such as word2vec or fastText construct two sort of vectors: for words and for contexts.", "labels": [], "entities": []}, {"text": "Naive use of vectors of only one sort leads to poor results.", "labels": [], "entities": []}, {"text": "We suggest using indefinite inner product in skip-gram negative sampling algorithm.", "labels": [], "entities": []}, {"text": "This allows us to use only one sort of vectors without loss of quality.", "labels": [], "entities": []}, {"text": "Our \"context-free\" cf algorithm performs on par with SGNS on word similarity datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector representation of words are widely used in NLP tasks.", "labels": [], "entities": [{"text": "Vector representation of words", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8185834139585495}]}, {"text": "Two approaches to word embeddings are usually contrasted: implicit (word2vec-like) and explicit (SVD-like).", "labels": [], "entities": []}, {"text": "Implicit models are usually faster and consume less memory than their explicit analogues.", "labels": [], "entities": []}, {"text": "Typically, word embedding algorithms produce two matrices both for \"words\" and \"contexts\".", "labels": [], "entities": []}, {"text": "Usually, contexts are the words themselves.", "labels": [], "entities": []}, {"text": "It is believed that word and context vectors cannot be equated to each other.", "labels": [], "entities": []}, {"text": "In practice, however, only the vectors of one sort are considered.", "labels": [], "entities": []}, {"text": "For example, typical solutions to word similarity or analogy problems use only the inner products of word vectors.", "labels": [], "entities": [{"text": "word similarity or analogy", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.8241852521896362}]}, {"text": "We present a modified skip-gram negative sampling algorithm that produces related word and context vectors.", "labels": [], "entities": []}, {"text": "One may say that some components of our word and context vectors are equal, while other components have different signs.", "labels": [], "entities": []}, {"text": "Another point of view is to say that word and context vectors are completely equal, but the inner product between them is indefinite.", "labels": [], "entities": []}, {"text": "This relation was suggested by the properties of explicit SVD embeddings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Datasets for word similarity evaluation consist of pairs of words rated by humans.", "labels": [], "entities": [{"text": "word similarity evaluation", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.7971570491790771}]}, {"text": "We use the following well-known English similarity datasets: MEN-3k (), MTurk-287), RW-STANFORD (, SimLex-999 (, SimVerb-3500 (, VERB-143 (, and WS-353 () splitted into similarity and relatedness parts).", "labels": [], "entities": [{"text": "MTurk-287", "start_pos": 72, "end_pos": 81, "type": "DATASET", "confidence": 0.8550880551338196}, {"text": "RW-STANFORD", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.7175295352935791}]}, {"text": "We use the evaluation code) for computing Spearman rank correlation \u03c1 between the similarity scores and the annotated ratings.", "labels": [], "entities": [{"text": "Spearman rank correlation \u03c1", "start_pos": 42, "end_pos": 69, "type": "METRIC", "confidence": 0.7300158813595772}]}, {"text": "shows the results for word vectors of dimension 100 with q negative components in D.", "labels": [], "entities": []}, {"text": "The best results are written in bold, and the best cf results are underlined.", "labels": [], "entities": []}, {"text": "We see that q = 0, i. e., the case of positive semidefinite approximation, has the worst performance.", "labels": [], "entities": []}, {"text": "The better perfor-  SGNS q = 0 q = 5 q = 10 q = 15 q = 20 q = 25 MEN-TR-3k . .", "labels": [], "entities": [{"text": "MEN-TR-3k", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.7451578974723816}]}, {"text": "Dataset SGNS q = 0 q = 5 q = 10 q = 15 q = 20 q = 25 MEN-TR-3k .: Word similarity for p = 100 (only \"positive\" components of p + q-dimensional cf vectors were taken).", "labels": [], "entities": [{"text": "MEN-TR-3k", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.5758872628211975}, {"text": "Word similarity", "start_pos": 66, "end_pos": 81, "type": "METRIC", "confidence": 0.8475967049598694}]}, {"text": "mance of cf is achieved at q = 10 or q = 15.", "labels": [], "entities": [{"text": "mance", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9905348420143127}]}, {"text": "This argees with empirical results of Subsection 3.2.", "labels": [], "entities": []}, {"text": "In general, cf is either on par with SGNS, or slightly loses.", "labels": [], "entities": []}, {"text": "In the second experiment we learn cf vectors of higher dimension 100 + q and projected them to 100 \"positive\" components.", "labels": [], "entities": []}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "Starting from q = 5, they are slightly better than SGNS.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Amount of negative eigenvalues in the trun- cated SVD of PPMI matrix (k = 1, i. e., no shift).", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9813269972801208}]}, {"text": " Table 2: Amount of negative eigenvalues in the trun- cated SVD of shifted PMI matrix with k = 5.", "labels": [], "entities": [{"text": "Amount", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.973821222782135}]}, {"text": " Table 3: Word similarity for d = 100.", "labels": [], "entities": [{"text": "Word similarity", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.8254570066928864}]}, {"text": " Table 4: Word similarity for p = 100 (only \"positive\" components of p + q-dimensional cf vectors were taken).", "labels": [], "entities": [{"text": "Word similarity", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7428635060787201}]}]}