{"title": [], "abstractContent": [{"text": "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation.", "labels": [], "entities": [{"text": "WMT 2019 Shared Task on Quality Estimation", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.5854404824120658}]}, {"text": "We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French.", "labels": [], "entities": []}, {"text": "Our submissions build upon the recent OpenKiwi framework: we combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models.", "labels": [], "entities": [{"text": "BERT", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.9863632321357727}]}, {"text": "We compare systems individually and propose new ensemble techniques for word and sentence-level predictions.", "labels": [], "entities": [{"text": "word and sentence-level predictions", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.5776357501745224}]}, {"text": "We also propose a simple technique for converting word labels into document-level predictions.", "labels": [], "entities": []}, {"text": "Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Quality estimation (QE) is the task of evaluating a translation system's quality without access to reference translations (.", "labels": [], "entities": [{"text": "Quality estimation (QE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8234572529792785}]}, {"text": "This paper describes the contribution of the Unbabel team to the Shared Task on Word, Sentence, and Document-Level (QE Tasks 1 and 2) at WMT 2019.", "labels": [], "entities": [{"text": "Shared Task on Word, Sentence, and Document-Level (QE Tasks 1", "start_pos": 65, "end_pos": 126, "type": "TASK", "confidence": 0.7285818136655368}, {"text": "WMT 2019", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.7768038809299469}]}, {"text": "Our system adapts OpenKiwi, 1 a recently released open-source framework for QE that implements the best QE systems from WMT 2015-18 shared tasks (, which we extend to leverage recently proposed pre-trained models https://unbabel.github.io/OpenKiwi. via transfer learning techniques.", "labels": [], "entities": [{"text": "WMT 2015-18 shared tasks", "start_pos": 120, "end_pos": 144, "type": "DATASET", "confidence": 0.873047947883606}]}, {"text": "Overall, our main contributions are as follows: \u2022 We extend OpenKiwi with a Transformer predictor-estimator model ( ).", "labels": [], "entities": []}, {"text": "\u2022 We apply transfer learning techniques, finetuning BERT) and XLM (Lample and Conneau, 2019) models in a predictor-estimator architecture.", "labels": [], "entities": [{"text": "BERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9585213661193848}]}, {"text": "\u2022 We incorporate predictions coming from the APE-BERT system described in , also used in this year's Unbabel's APE submission ().", "labels": [], "entities": [{"text": "APE-BERT system", "start_pos": 45, "end_pos": 60, "type": "DATASET", "confidence": 0.8222357630729675}, {"text": "Unbabel's APE submission", "start_pos": 101, "end_pos": 125, "type": "DATASET", "confidence": 0.8682762384414673}]}, {"text": "\u2022 We propose new ensembling techniques for combining word-level and sentence-level predictions, which outperform previously used stacking approaches).", "labels": [], "entities": []}, {"text": "\u2022 We build upon our BERT-based predictorestimator model to obtain document-level annotation and MQM predictions via a simple wordto-annotation conversion scheme.", "labels": [], "entities": [{"text": "BERT-based", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9479629993438721}]}, {"text": "Our submitted systems achieve the best results on all tracks and all language pairs by a considerable margin: on English-Russian (En-Ru), our sentence-level system achieves a Pearson score of 59.23% (+5.96% than the second best system), and on English-German (En-De), we achieve 57.18% (+2.44%).", "labels": [], "entities": [{"text": "Pearson score", "start_pos": 175, "end_pos": 188, "type": "METRIC", "confidence": 0.9849331974983215}]}], "datasetContent": [{"text": "The data resources we use to train our systems are of three types: the QE shared task corpora, additional parallel corpora, and artificial triplets (src, pe, mt) in the style of the eSCAPE corpus ().", "labels": [], "entities": []}, {"text": "\u2022 The En-De QE corpus provided by the shared task, consisting of 13,442 train triplets.", "labels": [], "entities": [{"text": "En-De QE corpus", "start_pos": 6, "end_pos": 21, "type": "DATASET", "confidence": 0.8128260572751363}]}, {"text": "\u2022 The En-Ru QE corpus provided by the shared task, consisting of 15,089 train triplets.", "labels": [], "entities": [{"text": "En-Ru QE corpus", "start_pos": 6, "end_pos": 21, "type": "DATASET", "confidence": 0.9282463192939758}]}, {"text": "\u2022 The En-De parallel dataset of 3,396,364 sentences from the IT domain provided by the shared task organizers.", "labels": [], "entities": []}, {"text": "which we extend in the style of the eSCAPE corpus to contain artificial triplets.", "labels": [], "entities": [{"text": "eSCAPE corpus", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.8596326410770416}]}, {"text": "To do this, we use OpenNMT with 5-fold jackknifing () to obtain unbiased translations of the source sentences.", "labels": [], "entities": []}, {"text": "\u2022 The En-Ru eSCAPE corpus () consisting of 7,735,361 artificial triplets.", "labels": [], "entities": [{"text": "En-Ru eSCAPE corpus", "start_pos": 6, "end_pos": 25, "type": "DATASET", "confidence": 0.8499107162157694}]}, {"text": "The data for this task consists of Amazon reviews translated from English to French using a neural MT system.", "labels": [], "entities": []}, {"text": "Translations were manually annotated for errors, with each annotation associated to a severity tag (minor, major or critical).", "labels": [], "entities": []}, {"text": "Note that each annotation may include several words, which do not have to be contiguous.", "labels": [], "entities": []}, {"text": "We refer to each contiguous block of characters in an annotation as a span, and refer to an annotation with at least two spans as a multi-span annotation.", "labels": [], "entities": []}, {"text": "illustrates this, where a single annotation is comprised of the spans bandes and parfaits.", "labels": [], "entities": []}, {"text": "Across training set and last year's development and test set, there are 36,242 annotations.", "labels": [], "entities": []}, {"text": "Out of these, 4,170 are multi-span, and 149 of the multispan annotations contain spans in different sentences.", "labels": [], "entities": []}, {"text": "The distribution of severities is 84.12% of major, 11.74% of minor and 4.14% of critical.", "labels": [], "entities": []}, {"text": "Source: resistance bands are great for home use, gym use, offices, and are ideal for travel.", "labels": [], "entities": []}, {"text": "Target: les bandes de r\u00e9sistance sont parfaits pour l'usage domestique, l'utilisation de la salle de gym, bureaux et sont id\u00e9ales pour les voyages.: Example of a multi-span annotation containing two spans: parfaits does not agree with bandes due to gender-it should be parfaites.", "labels": [], "entities": []}, {"text": "This mistake corresponds to a single annotation with severity \"minor\".", "labels": [], "entities": [{"text": "severity", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9949746131896973}]}], "tableCaptions": [{"text": " Table 2: Word and sentence-level results for En-De  and En-Ru on the validation set in terms of F 1 -MULT  and Pearson's r correlation. (*) Lines with an aster- isk use Powell's method for word level ensembling and  2 -regularized regression for sentence level. As the  weights are tuned on the dev set, their numbers can not  be directly compared to the other models", "labels": [], "entities": [{"text": "F 1 -", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.9577332735061646}, {"text": "MULT", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.5093606114387512}, {"text": "Pearson's r correlation", "start_pos": 112, "end_pos": 135, "type": "METRIC", "confidence": 0.8406946957111359}, {"text": "word level ensembling", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.5996563931306204}]}, {"text": " Table 3: Word and sentence-level results for En-De and En-Ru on the test set in terms of F 1 -MULT and Pearson's  r correlation.", "labels": [], "entities": [{"text": "F 1 -", "start_pos": 90, "end_pos": 95, "type": "METRIC", "confidence": 0.9693409403165182}, {"text": "MULT", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.5032555460929871}, {"text": "Pearson's  r correlation", "start_pos": 104, "end_pos": 128, "type": "METRIC", "confidence": 0.6931072324514389}]}, {"text": " Table 4: Results of document-level submissions, and  their performance of the dev and dev0 validation sets.", "labels": [], "entities": []}]}