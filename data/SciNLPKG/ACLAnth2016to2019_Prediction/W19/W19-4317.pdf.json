{"title": [{"text": "An Empirical study on Pre-trained Embeddings and Language Models for Bot Detection", "labels": [], "entities": [{"text": "Bot Detection", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.7283865958452225}]}], "abstractContent": [{"text": "Fine-tuning pre-trained language models has significantly advanced the state of art in a wide range of downstream NLP tasks.", "labels": [], "entities": []}, {"text": "Usually, such language models are learned from large and well-formed text corpora from e.g. en-cyclopedic resources, books or news.", "labels": [], "entities": []}, {"text": "However , a significant amount of the text to be analyzed nowadays is Web data, often from social media.", "labels": [], "entities": []}, {"text": "In this paper we consider the research question: How do standard pre-trained language models generalize and capture the peculiarities of rather short, informal and frequently automatically generated text found in social media?", "labels": [], "entities": []}, {"text": "To answer this question , we focus on bot detection in Twitter as our evaluation task and test the performance of fine-tuning approaches based on language models against popular neural architectures such as LSTM and CNN combined with pre-trained and contextualized embeddings.", "labels": [], "entities": [{"text": "bot detection", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7567503452301025}]}, {"text": "Our results also show strong performance variations among the different language model approaches , which suggest further research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, transfer learning techniques) based on language models have successfully delivered breaktrough accuracies in all kinds of downstream NLP tasks.", "labels": [], "entities": []}, {"text": "Approaches like, Open AI GPT () and BERT have in common the generation of pre-trained models learned from very large text corpora.", "labels": [], "entities": [{"text": "Open AI GPT", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.46034863591194153}, {"text": "BERT", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9951028823852539}]}, {"text": "The resulting language models are then fine-tuned for the specific domain and task, continuously advancing the state of the art across the different evaluation tasks and benchmarks commonly used by the NLP community.", "labels": [], "entities": []}, {"text": "Transfer learning approaches based on language models are therefore the NLP analogue to similar approaches in other fields of AI like Computer Vision, where the availability of large datasets like ImageNet () enabled the development of state of the art pre-trained models.", "labels": [], "entities": [{"text": "Transfer learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8843284547328949}, {"text": "Computer Vision", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.693542867898941}]}, {"text": "Before language models, common practice for transfer learning in NLP was based on pre-trained contextindependent embeddings.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.9307126402854919}]}, {"text": "These are also learned from large corpora and encode different types of syntactic and semantic relations that can be observed when operating on the vector space.", "labels": [], "entities": []}, {"text": "However, their use is limited to the input layer of neural architectures, and hence the amount of data and training effort necessary to learn a high performance task-related model is high since it is still necessary to train the whole network.", "labels": [], "entities": []}, {"text": "Pretrained language models, on the other hand, attempt to learn in the network structure the word inter-relations that can be leveraged during the fine-tuning step, usually by just learning a feed forward network for the specific task.", "labels": [], "entities": []}, {"text": "The network architecture varies depending on the approach, including transformers) based on decoders, encoders and attention mechanisms, and bi-directional long-short term memory networks.", "labels": [], "entities": []}, {"text": "Language models are usually learnt from high quality, grammatically correct and curated text corpora, such as Wikipedia (ULMFiT), BookCorpus (Open AI GPT), a combination of Wikipedia and BookCorpus (BERT) or News (ELMo).", "labels": [], "entities": [{"text": "BERT", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.47773516178131104}]}, {"text": "However, a very significant amount of the text to be analyzed nowadays is Web data, frequently from social media.", "labels": [], "entities": []}, {"text": "The question that immediately arises is therefore whether such language models also capture the nuances of the short and informal language often found in social media channels.", "labels": [], "entities": []}, {"text": "In this paper we explore this question and empirically study how pre-trained embeddings and language models perform when used to analyze text from social media.", "labels": [], "entities": []}, {"text": "To this purpose, we focus on bot detection in Twitter as evaluation task for two main reasons.", "labels": [], "entities": [{"text": "bot detection", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8467623293399811}]}, {"text": "First, the intrinsic relevance of the task for counteracting the automatic spreading of disinformation and bias on social media.", "labels": [], "entities": []}, {"text": "Second, because in this context the gap, in terms of the quality and overall characteristics of the language used, between the corpora used to learn the language models and the task-specific text to be analyzed (automatically generated in asocial media, micro-blogging context) can be particularly representative.", "labels": [], "entities": []}, {"text": "In our experiments, prior to evaluating the behavior of pre-trained language models, we test pre-trained embeddings as a baseline learned from general corpora, social media and informal vocabularies.", "labels": [], "entities": []}, {"text": "We choose two popular NLP neural architectures for our binary classification task: Long Short Term memory networks (LSTM; Hochreiter and Schmidhuber, 1997) and convolutional networks (CNN;).", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7526829640070597}]}, {"text": "We also pre-processed our Twitter dataset, observing a positive effect on our CNN and LSTM classifiers while on the other hand such effect was actually negative on some of the tested pre-trained language models.", "labels": [], "entities": [{"text": "CNN", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9196469187736511}]}, {"text": "In general, our results indicate that finetuned pre-trained language models outperform pre-trained and contextualized embeddings used in conjunction with CNN or LSTM for the task at hand.", "labels": [], "entities": []}, {"text": "This shows evidence that language models actually capture much of the peculiarities of social media and bot language or at least are flexible enough to generalize during fine-tuning in such context.", "labels": [], "entities": []}, {"text": "From the different language models we evaluated, Open AI GPT beats BERT (base) and ULMFit in the bot/no bot classification task, suggesting that a forward and unidirectional language model is more appropriated for social media messages than other language modeling architectures, which is relatively surprising.", "labels": [], "entities": [{"text": "BERT", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9981245398521423}]}, {"text": "Nevertheless, the considerable experimentation we carried out has raised a number of additional questions that will need further research.", "labels": [], "entities": []}, {"text": "During the workshop, we aim at sharing and discussing these questions with the participants.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the state of the art about the different models and embeddings used in the experiments.", "labels": [], "entities": []}, {"text": "Next, the experimental setup is presented in section 3, where the learning objective is defined as well as the dataset and the used pre-trained embeddings.", "labels": [], "entities": []}, {"text": "Section 4 and 5 present the experiments using CNN and LSTM and different combinations of pre-trained, contextualized and dynamically generated embeddings learnt during training of the bot/no bot classification model.", "labels": [], "entities": [{"text": "bot/no bot classification", "start_pos": 184, "end_pos": 209, "type": "TASK", "confidence": 0.6179460287094116}]}, {"text": "Then, section 6 describes the experiments with pre-trained language models.", "labels": [], "entities": []}, {"text": "Finally, a discussion about the results is presented in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate pre-trained language models with Twitter data we focus on the relevant problem of detecting bots in social media.", "labels": [], "entities": []}, {"text": "Bots are automatic agents that publish information fora variety of purposes such as weather and natural hazards updates, and news, but also for spreading misinformation and fake news.", "labels": [], "entities": []}, {"text": "In fact, as of 2017 it has been estimated that as 9% to 15% of twitter accounts are bots () which means that out of the 321 million active user accounts 1 the number of automatic agents range from 28 to 48 million.", "labels": [], "entities": []}, {"text": "Detecting bots can be addressed as a binary classification problem focusing only in the tweet textual content since our main target are language models, regardless of the other features that might be drawn from the social network, such user metadata, network features based on the follower and followee relations, and tweet and retweet activity.", "labels": [], "entities": []}, {"text": "To generate a dataset of tweets generated by bots or humans we rely on an existing dataset of bot and human accounts published by.", "labels": [], "entities": []}, {"text": "We create a balanced dataset containing tweets labelled as bot or human according to the account label.", "labels": [], "entities": []}, {"text": "In total our dataset comprises 500,000 tweets where 279,495 tweets were created by 1,208 human accounts, and 220,505 tweets were tweeted from 722 bot accounts.", "labels": [], "entities": []}, {"text": "In this sample, bots tend to be more prolific than humans since they average 305 tweets per account which contrasts with the human average of 231.", "labels": [], "entities": []}, {"text": "In addition, bots tend to use more URL (0.8313 URL per tweet) and hash tags (0.4745 hashtags per tweets) in their tweets than humans (0.5781 URL and 0.2887 hashtags per tweet).", "labels": [], "entities": []}, {"text": "This shows that bots aim at maximizing visibility (hashtags) and to redirect traffic to other sources (URL).", "labels": [], "entities": []}, {"text": "Finally, we found that bots display more egoistic behaviour than humans since they mention other users in their tweets (0.4371 user mentions per tweet) less frequently than humans (0.5781 user mentions per tweet).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of CNN classifiers using random  and pre-trained embeddings. Bold and italics are used  for best classifiers using pre-processing or not pre- processing respectively.", "labels": [], "entities": []}, {"text": " Table 2: Evaluation of CNN classifiers using contextu- alized embeddings.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of CNN classifiers using concate- nations of pre-trained and contextualized embeddings.  Bold and italics are used for best classifiers using pre- processing or not pre-processing respectively.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation of CNN classifiers using dynamic  embeddings and pre-trained and contextualized embed- dings using a pre-processed dataset", "labels": [], "entities": [{"text": "CNN classifiers", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.762949526309967}]}, {"text": " Table 5: Evaluation of CNN classifiers using dynamic  embeddings and concatenations of to pre-trained and  contextualized embeddings.", "labels": [], "entities": [{"text": "CNN classifiers", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.7617220282554626}]}, {"text": " Table 6: Evaluation of BiLSTM classifiers using dy- namic and pre-trained embeddings and a pre-processed  corpus.", "labels": [], "entities": [{"text": "BiLSTM classifiers", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.604751393198967}]}, {"text": " Table 7: Pre-trained language models and fine-tuning  without data pre-processing", "labels": [], "entities": []}, {"text": " Table 8: Pre-trained Language models and fine tuning  with data pre-processing", "labels": [], "entities": []}]}