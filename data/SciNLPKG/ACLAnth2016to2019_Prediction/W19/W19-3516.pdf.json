{"title": [{"text": "A Platform Agnostic Dual-Strand Hate Speech Detector", "labels": [], "entities": [{"text": "Hate Speech Detector", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.5868657231330872}]}], "abstractContent": [{"text": "Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform.", "labels": [], "entities": [{"text": "Hate speech detectors", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8000954786936442}]}, {"text": "For instance , the information stored about the text's author may differ between services, and so using such data would reduce a system's general applicability.", "labels": [], "entities": []}, {"text": "The paper thus focuses on using exclusively text-based input in the detection, in an optimised architecture combining Con-volutional Neural Networks and Long Short-Term Memory-networks.", "labels": [], "entities": []}, {"text": "The hate speech detector merges two strands with character n-grams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "An increasing number of online arenas are becoming available for users worldwide to publish their opinions, from Internet fora and blogs, to microblog services like Twitter and social media such as Facebook and MeWe, and various chat rooms.", "labels": [], "entities": [{"text": "MeWe", "start_pos": 211, "end_pos": 215, "type": "DATASET", "confidence": 0.9661899209022522}]}, {"text": "However, in all arenas that are open to user generated content, there is a risk of some people misusing this opportunity to purposefully insult others, or even to convey hateful messages.", "labels": [], "entities": []}, {"text": "This is often in breach of the given arena's terms and conditions, and sometimes, in some countries, illegal.", "labels": [], "entities": []}, {"text": "Hence, there is a need for automatic detection of these messages across a multitude of online arenas, but without depending on any information specific to a given forum, so that the systems can be used across platforms without being changed.", "labels": [], "entities": []}, {"text": "Notably, information about the text's author, such as their usage history or their social network and activities, have been shown to be useful when categorising hate speech ().", "labels": [], "entities": [{"text": "categorising hate speech", "start_pos": 148, "end_pos": 172, "type": "TASK", "confidence": 0.7864424784978231}]}, {"text": "In particular, on some occasions, an author belonging to an exposed group may use language that would normally be considered hateful towards that group, without the statement coming through as hateful.", "labels": [], "entities": []}, {"text": "In such cases, disregarding user information may lead to misclassifications.", "labels": [], "entities": []}, {"text": "However, what user metadata is stored may differ between services, and so using such information reduces the general applicability of a system.", "labels": [], "entities": []}, {"text": "The research in this paper therefore aims at avoiding any such information, using exclusively text-based input in the detection.", "labels": [], "entities": []}, {"text": "This is accomplished through a deep learning-based architecture combining Convolutional Neural Networks and Long Short-Term Memory-networks, and by utilising both character n-grams and word embeddings as input in a dualstrand methodology.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows: Section 2 describes prior work on hate speech detection.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.8455128073692322}]}, {"text": "Section 3 then introduces the data set used in the experiments and Section 4 the proposed architecture.", "labels": [], "entities": []}, {"text": "Section 5 presents experiments and results, while Section 6 discusses those results.", "labels": [], "entities": []}, {"text": "Finally, Section 7 concludes and presents ideas for future exploration.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine the optimal configurations of the system described above, experiments were carried outwith varying layer sizes of the neural networks, as well as varying number of layers used in the different components.", "labels": [], "entities": []}, {"text": "In addition, the system was tested using both bidirectional and unidirectional LSTMs.", "labels": [], "entities": []}, {"text": "In order to evaluate the effects of the variations consistently, the sizes of the word-based and the character-based components were changed separately.", "labels": [], "entities": []}, {"text": "That is, when the sizes of the characterbased component were changed, the word-based part was kept constant, and vice versa.", "labels": [], "entities": []}, {"text": "This was done so that the best configuration of each component could be found independently, reducing the number of configurations to explore.", "labels": [], "entities": []}, {"text": "As for variations in the number of layers, these were, for similar reasons, also made independently by component.", "labels": [], "entities": []}, {"text": "Furthermore, in the characterbased component, the number of convolutional and LSTM layers were changed separately.", "labels": [], "entities": []}, {"text": "In the experiments with changes to the convolution, variations in the length of the convolutional filters were also made.", "labels": [], "entities": []}, {"text": "In addition, the system was tested using only character-based input, in order to evaluate the effectiveness of the CNN-LSTM combination on the character input.", "labels": [], "entities": []}, {"text": "For comparative purposes, only using word-based input was also tested, disabling the character-based component.", "labels": [], "entities": []}, {"text": "Beyond varying the setup configurations of the network itself, the effects of using different word embeddings were explored.", "labels": [], "entities": []}, {"text": "The first experiments separately tested variations to the components, with the unmodified part forming a baseline setup.", "labels": [], "entities": []}, {"text": "In the first half of these experiments, each kind of nodes had one layer.", "labels": [], "entities": []}, {"text": "Hence, the character-based component had one convolutional layer, followed by one LSTM layer; the word-based component had one LSTM layer; and the dense, feed-forward part had one hidden layer.", "labels": [], "entities": []}, {"text": "In the baseline system, the word-based component had one layer of 150 LSTM nodes.", "labels": [], "entities": []}, {"text": "This dimensionality was chosen because it reduces the number of dimensions from the word embeddings, going down to half the size in the case of word2vec, without decimating the information carried through.", "labels": [], "entities": []}, {"text": "The convolutional layer in the character-based part had 64 filters of length 3.", "labels": [], "entities": []}, {"text": "The filter length here denotes then in the character n-grams.", "labels": [], "entities": []}, {"text": "This was set to 3 as trigrams have proven useful in prior work (.", "labels": [], "entities": []}, {"text": "64 convolution filters were used since 64 is a power of 2 approximately twice the length of the character vectors.", "labels": [], "entities": []}, {"text": "As such, it is significantly greater than the character vector size, while at the same time smaller than the size of each filter (i.e., 3 \u00d7 31).", "labels": [], "entities": []}, {"text": "The character-based component's LSTM layer had 100 nodes, a number chosen to balance the impact of the word-and character-based components on the final classifier, and since it should not be too much higher than the dimensionality of the convolution output (i.e., the number of filters used  in the convolution).", "labels": [], "entities": []}, {"text": "Hence, the final component had 250 input elements (150 from the word-based part and 100 from the character-based) and three output nodes; one for each class.", "labels": [], "entities": []}, {"text": "suggest that the number of nodes in a hidden layer should be between the numbers of input and output nodes.", "labels": [], "entities": []}, {"text": "While such rules are not entirely reliable, the hidden layer size was set to 120; near the average of the input and output sizes.", "labels": [], "entities": []}, {"text": "In addition, the bidirectional version of this baseline configuration was tested, with each direction of the LSTMs having the dimensionality described above, thus giving the input to the final component twice the number of dimensions of the unidirectional case, so the hidden layer dimensionality was doubled.", "labels": [], "entities": []}, {"text": "The system was then tested with varying configurations in the character-based component, using 100 convolutional filters along with the 100 dimensions of the character LSTM.", "labels": [], "entities": []}, {"text": "Then, the size was first cut down to 50 for both number of filters and LSTM layer size, and then increased to 512 filters and an LSTM layer size of 256.", "labels": [], "entities": []}, {"text": "Finally, experiments were performed where the dimensionality of the word-based component was changed, while the character-based part had the default configuration, running the system with word-LSTM sizes of 50, 100, 200 and 250, respectively.", "labels": [], "entities": []}, {"text": "The results are shown in, for both the uni-and bidirectional configuration versions (only unidirectional precision and recall values are displayed, since the Bi-LSTM performance did not vary substantially).", "labels": [], "entities": [{"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9505738615989685}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9823368787765503}]}, {"text": "As the table shows, the baseline setup (row 1) worked best in terms of both recall and F 1 -score.", "labels": [], "entities": [{"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9997273087501526}, {"text": "F 1 -score", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.9837794899940491}]}, {"text": "Several other configurations had better precision, such as the version where the word-based, unidirectional LSTM had a layer size of 50, but the corresponding recall values were comparatively lower than in the baseline setup.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9992296695709229}, {"text": "recall", "start_pos": 159, "end_pos": 165, "type": "METRIC", "confidence": 0.9978330731391907}]}, {"text": "In these first experiments, the coefficient restricting the impact of the L 2 -regularisation was given the commonly used value 0.001.", "labels": [], "entities": []}, {"text": "However, the experiments showed that smaller values gave better results, so later experiments used a value of 0.0002 for this coefficient.", "labels": [], "entities": []}, {"text": "In the next group of experiments, shown in Table 3, variations were made to the convolutional part of the character-based component (hence only unidirectional LSTMs were used, not bidirectional).", "labels": [], "entities": []}, {"text": "Specifically, the system performance with filter length 4 was tested; then, an extra layer of convolution was added, with combinations of length 3 and length 4 filters being used.", "labels": [], "entities": []}, {"text": "The standard number of filters in these experiments was 64, with the layers using a higher number having 128 filters.", "labels": [], "entities": []}, {"text": "Next, the same three experiments were performed with the first convolutional layer using filters of length 3, and the second layer length 4.", "labels": [], "entities": []}, {"text": "Then, the order was reversed, with the first layer filters having length 4 and the second layer length 3.", "labels": [], "entities": []}, {"text": "Finally, the experiments were run with both layers using length 4 filters.", "labels": [], "entities": []}, {"text": "Since these experiments used a smaller value for the coefficient controlling L 2 -regularisation, the first row of reports a different baseline performance than row 1 in.", "labels": [], "entities": []}, {"text": "The baseline setup still had the second highest score on recall, outperformed only by the best setup in these experiments.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9994606375694275}]}, {"text": "This configuration, with two layers of 64  convolutional filters where the first layer's filters were of length 4, and the second layer's of length 3, had a substantially better performance than the rest of the setups.", "labels": [], "entities": []}, {"text": "In addition to multilayer convolution, configurations using two-layer LSTMs were tested, with two same-sized layers in the LSTM part of the word-and character-based components, respectively.", "labels": [], "entities": []}, {"text": "First, the character-based component's LSTM was given two layers of size 100, with the rest of the system having the settings of the baseline configuration.", "labels": [], "entities": []}, {"text": "Then two 150-dimensional LSTM layers in the word-based component were used, reverting the character-based component back to the baseline.", "labels": [], "entities": []}, {"text": "Further, the system was tested with both two convolutional layers and two LSTM layers in the character-based part, trying two settings of the convolutional section, one 'baseline-like' with the two convolutional layers each having 64 filters all of length 3, and the other version being the one which performed best in the convolution experiments above, i.e., two layers of 64 convolutional filters, with the first layer's filters having length 4, and the second layer's length 3.", "labels": [], "entities": []}, {"text": "Finally, the baseline configuration was expanded to two LSTM layers in each of the system components holding LSTMs.", "labels": [], "entities": []}, {"text": "shows the results and also includes the performance of the baseline setup, for comparison.", "labels": [], "entities": []}, {"text": "Using two unidirectional LSTM layers in the character-based component of the baseline system setup and on the optimal convolution configuration (i.e., with filters of length 4 in the first convolutional layer) showed marked precision increases.", "labels": [], "entities": [{"text": "precision", "start_pos": 224, "end_pos": 233, "type": "METRIC", "confidence": 0.9982709884643555}]}, {"text": "However, recall in those cases was significantly weaker than in the baseline setup.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9997372031211853}]}, {"text": "Similar results, but with less marked precision increase,  were found when using two LSTM layers in the word-based part, as well as in the bidirectional setup versions, and the equivalent two-character LSTM.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9983252882957458}]}, {"text": "Using two convolutional layers with all filters at length 3 and using two LSTM layers in each of the system components, gave lower precision than the baseline.", "labels": [], "entities": [{"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9988443851470947}]}, {"text": "Finally, the baseline setup was used, with one convolutional layer and one LSTM layer, but with the word-based LSTM removed and the dense layer reduced to 50 nodes.", "labels": [], "entities": []}, {"text": "Then the equivalent was done using the best-performing configuration above, the system having two convolutional layers of 64 filters each, with lengths 4 and 3.", "labels": [], "entities": []}, {"text": "For comparison, the system was then tested using just the word-based input.", "labels": [], "entities": []}, {"text": "Here, too, the baseline setup was used as the starting point, meaning one LSTM layer of size 150.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Interestingly, both of the character-only systems outperformed the baseline.", "labels": [], "entities": []}, {"text": "Furthermore, the characters-only version of the baseline setup showed the highest precision of all the experiments in this research.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9988697171211243}]}, {"text": "As for the characters-only version of the configuration with two convolutional layers, the recall was higher than in the version including wordbased input, but the precision was lower.", "labels": [], "entities": [{"text": "recall", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9997822642326355}, {"text": "precision", "start_pos": 164, "end_pos": 173, "type": "METRIC", "confidence": 0.9996474981307983}]}, {"text": "Notably, it still outperformed the word-inclusive baseline setup on all measures.", "labels": [], "entities": []}, {"text": "The word-only configuration was outperformed by the character-only systems, but still performed better than the baseline using all inputs.", "labels": [], "entities": []}, {"text": "All the above experiments utilised pretrained word2vec embeddings.", "labels": [], "entities": []}, {"text": "For comparison, the baseline and optimal configurations were also evaluated using GloVe embeddings.", "labels": [], "entities": []}, {"text": "In terms of F 1 -score, both of the tested configurations improved when changing to GloVe.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9887312352657318}]}, {"text": "The baseline setup improved on all measures, though the improvement in precision was very slight.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9997245669364929}]}, {"text": "In the configuration with two convolutional layers, the precision got worse when changing to GloVe-embeddings.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9994931221008301}]}, {"text": "However, the recall of this setup using GloVe was the highest recorded throughout this research, outperforming the second best (the same configuration with the word-based component disabled) by more than 0.4%.", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9994974136352539}]}, {"text": "In addition, the precision, while lower than the equivalent word2vec-performance, was still acceptably high.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997175335884094}]}, {"text": "Hence, the resulting macro average F 1 -score was 79.24 (84.14 micro average), which is higher than any other configuration in these experiments.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.939100369811058}]}], "tableCaptions": [{"text": " Table 1: Size of the Waseem and Hovy (2016) data set", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9684329628944397}, {"text": "Waseem and Hovy (2016) data set", "start_pos": 22, "end_pos": 53, "type": "DATASET", "confidence": 0.605425514280796}]}, {"text": " Table 2: System configuration experiments.", "labels": [], "entities": []}, {"text": " Table 3: Varying the convolutional segment of the  character-based component. The setup columns show  the number of filters at each consecutive layer, along  with their corresponding filter lengths.", "labels": [], "entities": []}, {"text": " Table 4: Using multiple LSTM layers", "labels": [], "entities": []}, {"text": " Table 5: Using only character or only word input", "labels": [], "entities": []}, {"text": " Table 6: System performance comparison", "labels": [], "entities": []}]}