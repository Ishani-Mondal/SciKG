{"title": [{"text": "CUED@WMT19:EWC&LMs", "labels": [], "entities": [{"text": "WMT19", "start_pos": 5, "end_pos": 10, "type": "DATASET", "confidence": 0.8929327726364136}, {"text": "EWC&LMs", "start_pos": 11, "end_pos": 18, "type": "DATASET", "confidence": 0.9045140544573466}]}], "abstractContent": [{"text": "Two techniques provide the fabric of the Cam-bridge University Engineering Department's (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs).", "labels": [], "entities": [{"text": "Cam-bridge University Engineering Department", "start_pos": 41, "end_pos": 85, "type": "DATASET", "confidence": 0.9580761939287186}, {"text": "WMT19 evaluation", "start_pos": 108, "end_pos": 124, "type": "TASK", "confidence": 0.7567339539527893}]}, {"text": "We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC.", "labels": [], "entities": [{"text": "WMT test sets", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.8816766937573751}, {"text": "EWC", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.7317690253257751}]}, {"text": "A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains.", "labels": [], "entities": []}, {"text": "As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9674866199493408}]}], "introductionContent": [{"text": "Both fine-tuning and language modelling are techniques widely used for NMT.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7880788445472717}, {"text": "NMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9510891437530518}]}, {"text": "Fine-tuning is often used to adapt a model to anew domain, while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (.", "labels": [], "entities": [{"text": "ensembling neural machine translation (NMT)", "start_pos": 65, "end_pos": 108, "type": "TASK", "confidence": 0.7424692043236324}]}, {"text": "Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways.", "labels": [], "entities": [{"text": "WMT19 news shared task", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.9168086647987366}]}, {"text": "Our contributions are: \u2022 Elastic weight consolidation (, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original domain.", "labels": [], "entities": [{"text": "Elastic weight consolidation", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.693673570950826}, {"text": "domain adaptation", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7090286910533905}]}, {"text": "We report large gains from fine-tuning our models on former English-German WMT test sets with EWC.", "labels": [], "entities": [{"text": "English-German WMT test sets", "start_pos": 60, "end_pos": 88, "type": "DATASET", "confidence": 0.7417193949222565}, {"text": "EWC", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.918751060962677}]}, {"text": "We find that combining finetuning with checkpoint averaging (JunczysDowmunt et al., 2016b,a) yields further significant gains.", "labels": [], "entities": []}, {"text": "Fine-tuning is less effective for German-English.", "labels": [], "entities": []}, {"text": "\u2022 Inspired by the shallow fusion technique by we ensemble our neural translation models with neural language models.", "labels": [], "entities": []}, {"text": "While this technique is effective for single models, the gains are diminishing under NMT ensembles trained with large amounts of back-translated sentences.", "labels": [], "entities": []}, {"text": "\u2022 To incorporate document-level context in a light-weight fashion, we propose a modification to the Transformer () that has separate attention layers for inter-and intra-sentential context.", "labels": [], "entities": []}, {"text": "We report large perplexity reductions compared to sentence-level LMs under the new architecture.", "labels": [], "entities": []}, {"text": "Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9992415904998779}]}, {"text": "\u2022 Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 62, "end_pos": 99, "type": "TASK", "confidence": 0.8034136394659678}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9834986329078674}]}, {"text": "To combine NMT and SMT we follow and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9775133728981018}, {"text": "SMT lattices", "start_pos": 134, "end_pos": 146, "type": "TASK", "confidence": 0.8998555243015289}]}, {"text": "\u2022 While data filtering was central in last year's evaluation (), in our experiments this year we found that a very simple filtering approach based on a small number of crude heuristics can perform as well as dual conditional cross-entropy filtering (JunczysDowmunt, 2018a,b).", "labels": [], "entities": [{"text": "data filtering", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.8009039163589478}]}, {"text": "\u2022 We confirm the effectiveness of source-side noise for scaling up back-translation as proposed by.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experimental setup is essentially the same as last year . We delay SGD updates () to use larger training batch sizes than our technical infrastructure 3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor.", "labels": [], "entities": [{"text": "SGD", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9683213829994202}]}, {"text": "We use Transformer () models in two configurations (Tab. 1).", "labels": [], "entities": []}, {"text": "Preliminary experiments are carried outwith the 'Base' configuration while we use the 'Big' models for our final system.", "labels": [], "entities": []}, {"text": "We use news-test2017 as development set to tune model weights and select checkpoints and news-test2018 as test set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of ParaCrawl filtering techniques. The rest of the training data is over-sampled to roughly  match the size of the filtered ParaCrawl corpus. In the 'Dual x-ent filtering' experiments we selected the 15M best  sentences according the dual cross-entropy filtering criterion of Junczys-Dowmunt (2018a).", "labels": [], "entities": [{"text": "ParaCrawl filtering", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7810240983963013}, {"text": "ParaCrawl corpus", "start_pos": 145, "end_pos": 161, "type": "DATASET", "confidence": 0.8903943300247192}]}, {"text": " Table 3: Using different corpora for back-translation. We back-translated with a 'base' model for news-2017  and the big single Transformer model of Stahlberg et al. (2018b) for news-2016 and news-2018.", "labels": [], "entities": []}, {"text": " Table 4: Fine-tuning our models on former WMT test  sets using continued training and EWC.", "labels": [], "entities": [{"text": "WMT test  sets", "start_pos": 43, "end_pos": 57, "type": "DATASET", "confidence": 0.8559368252754211}, {"text": "EWC", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.836366593837738}]}, {"text": " Table 5: Language model perplexities of different neural language models. 'Intra-Inter' denotes our modified  Transformer architecture from Sec. 2. The standard model has 448M parameters, Intra-Inter has 549M parameters.", "labels": [], "entities": []}, {"text": " Table 6: Using different kinds of language models for translation on news-test2018. The PBSMT baseline  gets 26.7 BLEU on English-German and 27.5 BLEU on German-English.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9723864793777466}, {"text": "PBSMT baseline", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8697181940078735}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.998214602470398}, {"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9979063272476196}]}, {"text": " Table 7: English-German and German-English primary  submissions to the WMT19 shared task.", "labels": [], "entities": [{"text": "WMT19 shared task", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.4913782278696696}]}, {"text": " Table 8: Comparison of our English-German system  with the winning submissions over the past two years.", "labels": [], "entities": []}]}