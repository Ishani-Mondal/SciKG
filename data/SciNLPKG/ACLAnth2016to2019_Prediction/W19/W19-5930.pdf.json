{"title": [{"text": "Real Life Application of a Question Answering System Using BERT Language Model", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7465038597583771}, {"text": "BERT", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.9465627074241638}]}], "abstractContent": [{"text": "Real life scenarios are often left untouched by the newest advances in research.", "labels": [], "entities": []}, {"text": "They usually require the resolution of some specific task applied to a restricted domain, all the while providing small amounts of data to begin with.", "labels": [], "entities": []}, {"text": "In this study we apply one of the newest innovations in Deep Learning to a task of text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8123871088027954}]}, {"text": "The goal is to create a question answering system in Italian that provides information about a specific subject, e-invoicing and digital billing.", "labels": [], "entities": [{"text": "question answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7146954387426376}]}, {"text": "Italy recently introduced anew legislation about e-invoicing and people have some legit doubts, therefore a large share of professionals could benefit from this tool.", "labels": [], "entities": []}, {"text": "We gathered few pairs of question and answers; afterwards, we expanded the data, using it as a training corpus for BERT language model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.5060905814170837}]}, {"text": "Through a separate test corpus we evaluated the accuracy of the answer provided.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999078631401062}]}, {"text": "Values show that the automatic system alone performs surprisingly well.", "labels": [], "entities": []}, {"text": "The demo interface is hosted on Telegram, which makes the system immediately available to test.", "labels": [], "entities": [{"text": "Telegram", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9867395162582397}]}], "introductionContent": [{"text": "Pre-trained models have proven to be of great help in accomplishing many NLP tasks, such as natural language inference, text classification and question-answering.", "labels": [], "entities": [{"text": "text classification", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.8074848055839539}]}, {"text": "All of these paradigms contain a semi-supervised language model trained on large corpora of data; they are later fine-tuned to work on downstream tasks (.", "labels": [], "entities": []}, {"text": "However, real life applications can't often benefit from these advances, for many reasons: lack of data, lack of time and resources to reach a sufficient accuracy level, or the need to address some very specific domain that elude the scope of a general-purpose architecture.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9953868985176086}]}, {"text": "As a result, many concrete scenarios of applications are left untouched by the scientific progress, even though these obstacles are far from impossible to overcome.", "labels": [], "entities": []}, {"text": "The goal of this study is to build a questionanswering systems using only BERT (Bidirectional Encoder Representations from Transformers) language model (, without exploiting any rule-based refinement system or any other proprietary algorithm.", "labels": [], "entities": [{"text": "BERT", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9961787462234497}]}], "datasetContent": [], "tableCaptions": []}