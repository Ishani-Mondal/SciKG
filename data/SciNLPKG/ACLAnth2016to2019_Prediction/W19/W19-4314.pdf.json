{"title": [{"text": "Constructive Type-Logical Supertagging with Self-Attention Networks", "labels": [], "entities": [{"text": "Constructive Type-Logical Supertagging", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6897825201352438}]}], "abstractContent": [{"text": "We propose a novel application of self-attention networks towards grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8520464599132538}]}, {"text": "We present an attention-based supertag-ger fora refined type-logical grammar, trained on constructing types inductively.", "labels": [], "entities": []}, {"text": "In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the gram-mar's type system along with its denotational semantics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9439851641654968}]}, {"text": "This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential.", "labels": [], "entities": []}, {"text": "This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccom-plished.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9990611672401428}]}], "introductionContent": [{"text": "Categorial Grammars, in their various incarnations, posit a functional view on parsing: words are assigned simple or complex categories (or: types); their composition is modeled in terms of functor-argument relationships.", "labels": [], "entities": []}, {"text": "Complex categories wear their combinatorics on their sleeve, which means that most of the phrasal structure is internalized within the categories themselves; performing the categorial assignment process fora sequence of words, i.e. supertagging, amounts to almost parsing ().", "labels": [], "entities": []}, {"text": "In machine learning literature, supertagging is commonly viewed as a particular case of sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.6377942562103271}]}, {"text": "This perspective points to the immediate applicability of established, highperforming neural architectures; indeed, recurrent models have successfully been employed (e.g. within the context of Combinatory Categorial Grammars (CCG)), achieving impressive results (.", "labels": [], "entities": []}, {"text": "However, this perspective comes at a cost; the supertagger's co-domain, i.e., the different categories it may assign, is considered fixed, as defined by the set of unique categories in the training data.", "labels": [], "entities": []}, {"text": "Additionally, some categories have disproportionately low frequencies compared to the more common ones, leading to severe sparsity issues.", "labels": [], "entities": []}, {"text": "Since under-represented categories are very hard to learn, in practice models are evaluated and compared based on their accuracy over categories with occurrence counts above a certain threshold, a small subset of the full category set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.993101954460144}]}, {"text": "This practical concession has two side-effects.", "labels": [], "entities": []}, {"text": "The first pertains to the supertagger's inability to capture rare syntactic phenomena.", "labels": [], "entities": []}, {"text": "Although the percentage of sentences that may not be correctly analyzed due to the missing categories is usually relatively small, it still places an upper bound on the resulting parser's strength which is hard to ignore.", "labels": [], "entities": []}, {"text": "The second, and perhaps more far reaching, consequence is the implicit constraint it places on the grammar itself.", "labels": [], "entities": []}, {"text": "Essentially, the grammar must be sufficiently coarse while also allocating most of its probability mass on a small number of unique categories.", "labels": [], "entities": []}, {"text": "Grammars enjoying a higher level of analytical sophistication are practically unusable, since the associated supertagger would require prohibitive amounts of data to overcome their inherent sparsity.", "labels": [], "entities": []}, {"text": "We take a different view on the problem, instead treating it as sequence transduction.", "labels": [], "entities": []}, {"text": "We propose a novel supertagger based on the Transformer architecture () that is capable of constructing categories inductively, bypassing the aforementioned limitations.", "labels": [], "entities": []}, {"text": "We test our model on a highly-refined, automatically extracted type-logical grammar for written Dutch, where it achieves competitive results for high frequency categories, while acquiring the ability to treat rare and even unseen categories adequately.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all described experiments, we run the model 3 on the subset of sample sentences that are at most 20 words long.", "labels": [], "entities": []}, {"text": "We use a train/val/test split of 80/10/10 4 . We train with a batch size of 128, and pad sentences to the maximum in-batch length.", "labels": [], "entities": []}, {"text": "Training to convergence takes, on average, eight hours & 300 epochs for our training set of 45000 sentences on a GTX1080Ti.", "labels": [], "entities": [{"text": "convergence", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9601888060569763}]}, {"text": "We report averages over 5 runs.", "labels": [], "entities": []}, {"text": "Accuracy is reported on the type-level; that is, during evaluation, we predict atomic symbol sequences, then collapse subtype sequences into full types and compare the result against the ground truth.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9795692563056946}]}, {"text": "Notably, a single mistake within a type is counted as a completely wrong type.", "labels": [], "entities": []}, {"text": "The code for the model and processing scripts can be found at https: //github.com/konstantinosKokos/ Lassy-TLG-Supertagging.", "labels": [], "entities": []}, {"text": "It is worth pointing out that the training set contains only \u223c85% of the overall unique types, the remainder being present only in the validation and/or test sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Model performance at different merge scales,  with respect to training set type frequencies. M i de- notes the model at i merges, where M \u221e means the fully  merged model. For the fully merged model there is a  1 to 1 correspondence between input words and output  types, so we do away with the separation symbol.", "labels": [], "entities": []}, {"text": " Table 2: Repetition-averaged unseen type generation  and precision.", "labels": [], "entities": [{"text": "Repetition-averaged unseen type generation", "start_pos": 10, "end_pos": 52, "type": "TASK", "confidence": 0.7975805103778839}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994799494743347}]}]}