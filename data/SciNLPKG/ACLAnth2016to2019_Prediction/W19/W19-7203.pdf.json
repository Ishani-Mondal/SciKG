{"title": [{"text": "A Multi-Hop Attention for RNN based Neural Machine Translation", "labels": [], "entities": [{"text": "RNN based Neural Machine Translation", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.8056292295455932}]}], "abstractContent": [{"text": "Among recent progresses of neural machine translation models, the invention of the Transformer model is one of the most important progresses.", "labels": [], "entities": [{"text": "neural machine translation", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.7001298268636068}]}, {"text": "It is well-known that the key technologies of the Transformer include multi-head attention mechanism.", "labels": [], "entities": []}, {"text": "This paper introduces the multi-head attention mechanism into the traditional RNN-based neural machine translation model.", "labels": [], "entities": [{"text": "RNN-based neural machine translation", "start_pos": 78, "end_pos": 114, "type": "TASK", "confidence": 0.7079136073589325}]}, {"text": "Moreover, inspired by the existing multi-hop architectures such as end-to-end memory networks and convolutional sequence to sequence learning model, this paper proposes an RNN based NMT model with a multi-hop attention mechanism.", "labels": [], "entities": []}, {"text": "The proposed multi-hop attention model has two heads, where for each head, a context vector is calculated based on the states of the encoder and the decoder.", "labels": [], "entities": []}, {"text": "Then, in the second turn of the context vector calculation , those context vectors are updated depending not only on one's own context vector but also on the context vector of the other head.", "labels": [], "entities": []}, {"text": "Experimental results show that the proposed model significantly outperforms the baseline in BLEU score in Japanese-to-English/English-to-Japanese machine translation tasks with and without extended context.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9792790412902832}, {"text": "Japanese-to-English/English-to-Japanese machine translation tasks", "start_pos": 106, "end_pos": 171, "type": "TASK", "confidence": 0.6118477682272593}]}], "introductionContent": [{"text": "RNN encoder-decoder model (; Sutskever et al., made a context-aware translation model, called 2-to-2 () for OpenSubtitles 2018.", "labels": [], "entities": []}, {"text": "In the Japaneseto-English translation of the ASPEC corpus, the proposed method achieved a significantly better score than the Transformer for long sentences with more than 120 tokens.", "labels": [], "entities": [{"text": "ASPEC corpus", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.7322792112827301}]}, {"text": "In the following sections, we first show previous works on baseline RNN and multi-head RNN encoder-decoders in Section 2.", "labels": [], "entities": []}, {"text": "We then describe the proposed multi-hop method in Section 3.", "labels": [], "entities": []}, {"text": "We then show the performance for Japanese-toEnglish and English-to-Japanese translation tasks, focusing on long sentences in Section 4.", "labels": [], "entities": [{"text": "English-to-Japanese translation tasks", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.7339504957199097}]}], "datasetContent": [{"text": "In order to confirm the usefulness of the proposed method, this section describes experimental evaluation results in Japanese-to-English/English-toJapanese machine translation tasks with and without extended context.", "labels": [], "entities": [{"text": "Japanese-to-English/English-toJapanese machine translation tasks", "start_pos": 117, "end_pos": 181, "type": "TASK", "confidence": 0.613289957245191}]}, {"text": "we used BLEU () as the evaluation measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9989970326423645}]}, {"text": "The baseline is the bidirectional sequence-tosequence model () using Long Short-Term Memory (LSTM) which is a kind of RNN.", "labels": [], "entities": []}, {"text": "We used fairseq (Gehring et al., 2017) for implementation.", "labels": [], "entities": [{"text": "fairseq", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.7576337456703186}]}, {"text": "As training, we used Nesterov's Accelerated Gradient ( as optimizer with a learning rate of 0.005.", "labels": [], "entities": [{"text": "Accelerated Gradient", "start_pos": 32, "end_pos": 52, "type": "METRIC", "confidence": 0.9359011650085449}]}, {"text": "The embedding size was 512, the hidden size was 1024, and the encoder and the decoder are of one layer each.", "labels": [], "entities": []}, {"text": "For comparison, we also conducted evaluation with the Transformer, where the number of heads was set to 4 according to the default setting 4 of fairseq, and its learning rate was set to 0.0001 following the result of investigating the value at which its loss converged.", "labels": [], "entities": []}, {"text": "For all the models, the number of epochs in training was 20.", "labels": [], "entities": []}, {"text": "The number of tokens per batch was 2,000 and two GPUs were used in parallel 5 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU per sentence length (ASPEC ja\u2192en)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993346333503723}]}, {"text": " Table 4: BLEU per sentence length (ASPEC en\u2192ja)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993044137954712}]}]}