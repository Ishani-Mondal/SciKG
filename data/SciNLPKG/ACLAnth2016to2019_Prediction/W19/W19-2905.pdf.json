{"title": [{"text": "Modeling Hierarchical Syntactic Structures in Morphological Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "Sentences are represented as hierarchical syntactic structures, which have been successfully modeled in sentence processing.", "labels": [], "entities": []}, {"text": "In contrast, despite the theoretical agreement on hierarchical syntactic structures within words, words have been argued to be computationally less complex than sentences and implemented by finite-state models as linear strings of morphemes , and even the psychological reality of morphemes has been denied.", "labels": [], "entities": []}, {"text": "In this paper, extending the computational models employed in sentence processing to morphological processing , we performed a computational simulation experiment where, given incremental surprisal as a linking hypothesis, five computational models with different representa-tional assumptions were evaluated against human reaction times in visual lexical decision experiments available from the English Lexicon Project (ELP), a \"shared task\" in the morphological processing literature.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7380122244358063}, {"text": "English Lexicon Project (ELP)", "start_pos": 396, "end_pos": 425, "type": "DATASET", "confidence": 0.7391066749890646}]}, {"text": "The simulation experiment demonstrated that (i) \"amor-phous\" models without morpheme units un-derperformed relative to \"morphous\" models , (ii) a computational model with hierarchical syntactic structures, Probabilistic Context-Free Grammar (PCFG), most accurately explained human reaction times, and (iii) this performance was achieved on top of surface frequency effects.", "labels": [], "entities": []}, {"text": "These results strongly suggest that morphological processing tracks morphemes incrementally from left to right and parses them into hierarchical syntactic structures , contrary to \"amorphous\" and finite-state models of morphological processing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentences are represented as hierarchical structures, not linear strings of words).", "labels": [], "entities": []}, {"text": "The hierarchical representations of sentences have been successfully modeled in sentence processing).", "labels": [], "entities": []}, {"text": "In contrast, despite the theoretical agreement on hierarchical syntactic structures within words, especially derivational morphology, among various linguistic theories, words have been argued to be computationally less complex than sentences and implemented by finite-state models as linear strings of morphemes (, and even the psychological reality of morphemes has been denied by connectionist models (.", "labels": [], "entities": []}, {"text": "Consequently, the hierarchical representations of words have not been sufficiently considered in morphological processing, with a few exceptions.", "labels": [], "entities": []}, {"text": "In this paper, extending the computational models employed in sentence processing to morphological processing, we perform a computational simulation experiment where, given cumulative surprisal as a linking hypothesis, several computational models with different representational assumptions are evaluated against human reaction times (RTs) in visual lexical decision experiments available from the English Lexicon Project (ELP;), a \"shared task\" in the morphological processing literature, with special focus on derivational morphology.", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.7299791872501373}]}, {"text": "The goal of this paper is to investigate whether morphological processing tracks morphemes and parses them into hierarchical syntactic structures.", "labels": [], "entities": []}, {"text": "Specifically, we employ five computational models with different representational assump-tions from sentence processing: two \"amorphous\" models, Letter Markov Model and Syllable Markov Model, with transition probabilities among letters and syllables, respectively, without reference to morpheme units and three \"morphous\" models, Markov Model, Hidden Markov Model, and Probabilistic Context-Free Grammars (PCFG), with conditional probabilities among morphemes, part-of-speech (POS) tags, and nonterminal nodes of hierarchical structures, respectively.", "labels": [], "entities": []}, {"text": "Importantly, in the sentence processing literature, Markov Models and PCFGs have been exclusively compared, but these computational models differ not only in the presence of hierarchical structures but also POS tags.", "labels": [], "entities": []}, {"text": "Thus, we included HMM as an important \"midpoint\" model with POS tags but no hierarchical structures (cf.).", "labels": [], "entities": []}, {"text": "The prediction is that, if morphological processing tracks hierarchical syntactic structures, PCFG should outperform the alternative non-hierarchical models.", "labels": [], "entities": []}, {"text": "Moreover, if morphological processing tracks morphemes, the \"morphous\" models should outperform the \"amorphous\" models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two evaluation metrics are derived from surprisal: linguistic accuracy and psychological accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9155102372169495}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.7882007956504822}]}, {"text": "The linguistic accuracy of model M, LA(M ), is defined as Equation: where I(m) is the surprisal of morpheme m defined as Equation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9918988943099976}, {"text": "Equation", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9912726283073425}, {"text": "Equation", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.866166889667511}]}, {"text": "That is, the linguistic accuracy is the negative average surprisal over morphemes of morphologically complex words in the testing data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9772775173187256}]}, {"text": "Note also that the linguistic accuracy is just the negative of the NLP evaluation metric cross-entropy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9854443669319153}]}, {"text": "The linguistic accuracy maybe cognitively interpreted as offline grammaticality judgment: the higher the linguistic accuracy is, the more grammatical the model \"judges\" the testing data never seen before.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9122327566146851}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.5108025074005127}]}, {"text": "Note that the linguistic accuracy is completely independent of human be-havior (i.e. human RTs), in contrast with the psychological accuracy introduced below.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9794562458992004}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.7793384194374084}]}, {"text": "The psychological accuracy of model M, P A(M ), is defined as Equation: where \u2206D is the delta deviance defined as -2 times log-likelihood and B is the baseline model without cumulative surprisal included.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.994617760181427}, {"text": "Equation", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9987406134605408}]}, {"text": "That is, the psychological accuracy is the decrease in delta deviance between the baseline model and the target model fitted to the testing data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9717335104942322}]}, {"text": "The psychological accuracy maybe cognitively interpreted as online morphological processing: the higher psychological accuracy is, the less costly the model \"processes\" the testing data never seen before.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.873393714427948}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.7866851091384888}]}, {"text": "For example, suppose that the grammatical sentence Colorless green ideas sleep furiously empirically turned out to be less costly.", "labels": [], "entities": []}, {"text": "The most \"human-like\" model must assign the high probability, hence the less surprisal, to this sentence.", "labels": [], "entities": []}, {"text": "Interestingly, Frank and Bod (2011) and Fossum and Levy (2012) inductively observed that linguistic and psychological accuracies are positively correlated (cf., suggesting that the relationship between representation and processing is transparent).", "labels": [], "entities": []}], "tableCaptions": []}