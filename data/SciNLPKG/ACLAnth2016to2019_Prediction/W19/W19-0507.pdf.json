{"title": [{"text": "Generating a Novel Dataset of Multimodal Referring Expressions", "labels": [], "entities": []}], "abstractContent": [{"text": "Referring expressions and definite descriptions of objects in space exploit information about both object characteristics and locations.", "labels": [], "entities": []}, {"text": "Linguistic referencing strategies can rely on increasingly high-level abstractions to distinguish an object in a given location from similar ones elsewhere, yet the description of the intended location may still be unnatural or difficult to interpret.", "labels": [], "entities": [{"text": "Linguistic referencing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7485316097736359}]}, {"text": "Modalities like gesture may communicate spatial information like locations in a more concise manner.", "labels": [], "entities": []}, {"text": "When communicating with each other, humans mix language and gesture to reference entities, changing modalities as needed.", "labels": [], "entities": []}, {"text": "Recent progress in AI and human-computer interaction has created systems where a human can interact with a computer multimodally, but computers often lack the capacity to intelligently mix modalities when generating referring expressions.", "labels": [], "entities": []}, {"text": "We present a novel dataset of referring expressions combining natural language and gesture, describe its creation and evaluation, and its uses to train models for generating and interpreting multimodal referring expressions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Psychological studies suggest that gesture serves as abridge between understanding actions situated in the world and linguistic descriptions, such as symbolic references to entity classes and attributes).", "labels": [], "entities": []}, {"text": "Many researchers (e.g.,;), view gesture as a common mode of reference vis-` a-vis common ground.", "labels": [], "entities": []}, {"text": "Gesture is well-suited to directly grounding spatial information; pointing can bind to a location or be coerced to object(s) in that location ().", "labels": [], "entities": []}, {"text": "Demonstrative or attributive language (e.g., size, shape, qualitative relations), can specify entities by binding those characteristics to information received via gesture.", "labels": [], "entities": []}, {"text": "Thus, language affords abstract strategies to distinguish an object in a given location from similar ones elsewhere (e.g., the chair closest to that door-with pointing, or the green block at the right side of the table).", "labels": [], "entities": []}, {"text": "As an environment becomes more complex, so does the language used to give directions or single out specific items in it ().", "labels": [], "entities": []}, {"text": "An object indicated by deixis is usually also the topic of discussion (), but deixis maybe ambiguous depending on distance from agent to target object, or other objects close to the target object, while language can supplement it for more useful definite descriptions).", "labels": [], "entities": []}, {"text": "Co-temporal/overlapping speech and gesture (or an \"ensemble\") often involves deixis to ground the location, and language to specify further information (.", "labels": [], "entities": []}, {"text": "As a task's natural language requirements grow more complex, subjects rely on other modalities to carry semantic load, particularly as the need for immediate interpretation grows (.", "labels": [], "entities": []}, {"text": "Studies in this area have along history in computational linguistics/semantics (e.g., Claassen (1992); Krahmer and van der), human-robot interaction (e.g.,;), and computational and human discourse studies (e.g.,;;).", "labels": [], "entities": [{"text": "computational linguistics/semantics", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.7709663882851601}]}, {"text": "Following these, we seek to build models for generating, recognizing, and classifying referring expressions that are both natural and useful to the human interlocutors of computational dialogue systems.", "labels": [], "entities": []}, {"text": "Here, we present a novel dataset of Embodied Multimodal Referring Expressions (EMRE), blending gesture and natural language (English text-to-speech), used by an avatar in a human-computer interaction (HCI) scenario.", "labels": [], "entities": []}, {"text": "We describe raw data generation, annotation and evaluation, preliminary analysis, and expected uses in training machine learning models for generating referring expressions in real-time that are appropriate, salient, and natural in context.", "labels": [], "entities": []}, {"text": "As our goal is to train models which a system can use to generate and interpret naturalistic multimodal referring expressions during interaction with a human, we gathered data using such a system-specifically VoxSim, a semantically-driven visual event simulator based on the VoxML semantic modeling language , that facilitates data gathering using Monte-Carlo parameter setting to simulate motion predicates in 3D space (.", "labels": [], "entities": []}, {"text": "We created a variant on the Human-Avatar-Blocks World (HAB) system (, in which VoxSim visualizes the actions taken by an avatar in the 3D world as she interprets gestural and spoken input from a human interlocutor.", "labels": [], "entities": []}, {"text": "1 A shortcoming of the HAB system is the asymmetry between the language that the system's avatar is capable of recognizing and interpreting, and the English utterances it can generate (.", "labels": [], "entities": []}, {"text": "Specifically, the avatar can 1) produce complete sentences of structures that it cannot entirely parse and 2) properly interpret spatial terms and relations between objects, but cannot fluently use them to refer to objects or the relations between them.", "labels": [], "entities": []}, {"text": "Improvements to the first asymmetry are underdevelopment separately, and here we present data for creating a robust model of referring techniques in all available modalities, to help rectify the second asymmetry, for more fluent interaction in this and other HCI systems.).", "labels": [], "entities": []}, {"text": "E 2 defines the target of deixis as the intersection of the vector extended in e 1 with a location, and reifies that point as a variable w.", "labels": [], "entities": []}, {"text": "A 4 , shows the compound binding of w to the indicated region and objects within that region.", "labels": [], "entities": []}, {"text": "The gesture semantics in VoxSim are largely based on the formalisms of.", "labels": [], "entities": []}, {"text": "Multimodal information in a multimodal system cannot be assumed to follow the same format as unimodal information.", "labels": [], "entities": []}, {"text": "Language in an ensemble cannot be assumed to be identical to language used alone.", "labels": [], "entities": []}, {"text": "A reference to an object maybe grounded in gesture, natural language, or both, subject to constraints that vary per modality.", "labels": [], "entities": []}, {"text": "We therefore generated a dataset that can be examined for where these constraints occur, and under which circumstances human evaluators, as proxies for interlocutors with the avatar in a live interaction, prefer one referring modality to another, and with what descriptive detail.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}