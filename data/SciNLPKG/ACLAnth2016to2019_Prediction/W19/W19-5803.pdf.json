{"title": [{"text": "Bridging the Gap: Improve Part-of-speech Tagging for Chinese Social Media Texts with Foreign Words", "labels": [], "entities": [{"text": "Improve Part-of-speech Tagging", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.7628099719683329}]}], "abstractContent": [{"text": "Multilingual speakers often switch between languages and generate enormous quantities of cross-language data.", "labels": [], "entities": []}, {"text": "This phenomenon is more frequent observed in social media texts, where a large body of user generated data is produced everyday.", "labels": [], "entities": []}, {"text": "Such mix-lingual and informal texts lead to a challenge for part-of-speech (POS) tagging, which is one fundamental task in natural language processing.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6563241720199585}]}, {"text": "In this paper, we propose a language-agnostic POS tagger for social media texts, which is able to learn from heterogeneous data with different genre and language type.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.7202168107032776}]}, {"text": "Particularly, in order to comprehensively evaluate POS tagging performance, we propose anew tagging scheme including exclusive tags for special symbols in social media texts, and a human-annotated dataset of Chinese-English mixed social media texts is also developed.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.8556621968746185}]}, {"text": "Experiments on both synthetic and real datasets show the validity and effectiveness of our model on social media texts where it outperforms state-of-the-art language-specific taggers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Part-of-speech tagging is the basic step of identifying a token's functional role within a sentence and is the fundamental step in many NLP pipeline applications.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7706882655620575}, {"text": "identifying a token's functional role within a sentence", "start_pos": 44, "end_pos": 99, "type": "TASK", "confidence": 0.7842629684342278}]}, {"text": "It is well known that the performance of complex NLP systems is negatively affected if one of the preliminary stages is less than perfect.", "labels": [], "entities": []}, {"text": "For example, some tagging errors may change the semantic interpretation of an entire sentence, typically due to assigning an entirely incorrect POS category to a word, for example a Plural Noun (NNS) incorrectly tagged as a Present Tense Verb (VBZ).", "labels": [], "entities": []}, {"text": "This alteration in the semantics has a deleterious effect on all the subsequent steps in the NLP pipeline, e.g., Syntactic Parsing, Dependency Parsing, etc.", "labels": [], "entities": [{"text": "Syntactic Parsing", "start_pos": 113, "end_pos": 130, "type": "TASK", "confidence": 0.8192268311977386}, {"text": "Dependency Parsing", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7609628140926361}]}, {"text": "Compared with formal texts, like newswire articles, the POS tagging performance in the social media texts is still far from satisfactory ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.7330234348773956}]}, {"text": "Most state-of-the-art POS tagging approaches are based on supervised methods, in which a large amount of annotated data is needed to train models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9213084876537323}]}, {"text": "However, many datasets constructed for the POS tagging task are from carefully-edited newswire articles, such as PTB ( and CTB, which are greatly different from social media texts.", "labels": [], "entities": [{"text": "POS tagging task", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.9377636909484863}, {"text": "PTB", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.9077914953231812}]}, {"text": "The difference in domains between training data and testing data may heavily impact the performance of approaches based on supervised methods.", "labels": [], "entities": []}, {"text": "Hence, most state-of-the-art POS taggers cannot achieve the same performance as reported on newswire domain when applied on social media texts (.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8074553608894348}]}, {"text": "However, enormous quantities of user generated content on social media are giving increasing attention as well as valuable sources fora variety of applications, such as recommendation, disease prediction ().", "labels": [], "entities": [{"text": "disease prediction", "start_pos": 185, "end_pos": 203, "type": "TASK", "confidence": 0.6960222870111465}]}, {"text": "Yet, in such NLP tasks, one challenge is that texts from social media platforms (e.g., Twitter 1 , Weibo 2 ) usually contain many informal inputs, such as acronym (as soon as possible \u2192 asap), shorthand (technology \u2192 tech ), out-ofvocabulary words ( meeeeee \u2192 me), etc.", "labels": [], "entities": []}, {"text": "Another challenge is that many mix-lingual cases exist in microblogs, which occurs frequently in such informal texts.", "labels": [], "entities": []}, {"text": "For example, according to), in Weibo 7 , the mixed usage of Chinese and English is one of the most popular phenomena with informal language.", "labels": [], "entities": [{"text": "Weibo 7", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8653655648231506}]}, {"text": "To illustrate  such phenomenon, one example of microblogs extracted from real Weibo texts is shown in, all of which are written in Chinese with a few English words.", "labels": [], "entities": []}, {"text": "In this paper, we focus on the task of annotating Chinese-English social media texts from Weibo, and implement automatic part-of-speech (POS) tagging of these texts.", "labels": [], "entities": []}, {"text": "To this end, we propose an approach to learning a POS tagger that can be applied in truly cross-language social media texts.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 50, "end_pos": 60, "type": "TASK", "confidence": 0.6597466468811035}]}, {"text": "We discuss techniques that allow us to learn a tagger given only the amount of labeled data that contains standard monolingual languages, specifically.", "labels": [], "entities": []}, {"text": "Here, we improve the tagging performance on Weibo texts, which involves Chinese and English, by using the semantic information from different sources of labeled data.", "labels": [], "entities": [{"text": "tagging", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.959739089012146}]}, {"text": "Experimental results on both synthetic and real Weibo texts confirm the effectiveness of our method.", "labels": [], "entities": [{"text": "Weibo texts", "start_pos": 48, "end_pos": 59, "type": "DATASET", "confidence": 0.8874375224113464}]}, {"text": "Our contributions can be concluded as follows: \u2022 We explore to utilize multiple sources of annotated corpora to improve performance on tagging cross-lingual Weibo texts.", "labels": [], "entities": []}, {"text": "To this end, we extends the bi-directional long short term network with adversarial training.", "labels": [], "entities": []}, {"text": "\u2022 For the first time, we develop a cross-lingual microblog corpus and give a quantitative evaluation for POS tagging in such microblog corpus.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.8669995963573456}]}, {"text": "\u2022 Experimental results show that our model is better than existing state-of-the-art language specific taggers.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section explains our experiments on the evaluation of our proposed model on POS tagging cross-lingual Weibo texts.", "labels": [], "entities": [{"text": "POS tagging cross-lingual Weibo texts", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.8475632190704345}]}, {"text": "First, we describe how we collect and annotate Weibo texts.", "labels": [], "entities": []}, {"text": "A synthetic method to generate language mixed Weibo texts is also illustrated.", "labels": [], "entities": []}, {"text": "Both of datasets are only used for testing.", "labels": [], "entities": []}, {"text": "Next, we explore the utility of crosslingual embeddings generated by the aforementioned two methods: unsupervised training and embedding projection.", "labels": [], "entities": [{"text": "embedding projection", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.8475724160671234}]}, {"text": "Then, we evaluate the proposed model on both the synthetic and manuallyannotated datasets.", "labels": [], "entities": []}, {"text": "Our training data mainly consists of three sources: PTB (Marcus et al., 1993), ARK (Gimpel et al., 2011), and CTB).", "labels": [], "entities": [{"text": "PTB", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.49163076281547546}, {"text": "ARK", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.7636120319366455}, {"text": "CTB", "start_pos": 110, "end_pos": 113, "type": "DATASET", "confidence": 0.7523220777511597}]}, {"text": "Different tagsets are all mapped into the universal tagset described in Section 3.4.", "labels": [], "entities": []}, {"text": "Notice that since ARK is collected from Twitter, also a kind of social media data, so we keep Tweet-specific tags and map them to our defined 6 Weibo-specific tags, for the reason that some marks also appear in Weibo texts, such as At-methion, URLs, and soon.", "labels": [], "entities": []}, {"text": "We use three language-specific Chinese POS taggers (ST, Jieba, NLPIR) as our baseline models.", "labels": [], "entities": []}, {"text": "Besides, two models with and without adversarial training, denoted as BiLSTM \u2212 and BiLSTM + , are implemented to study the utility of the adversarial training in our task.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9068933129310608}, {"text": "BiLSTM", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9187459349632263}]}, {"text": "The hyper-parameters used for our model are as follows: \u2022 BiLSTM \u2212 : The hidden size is set to 150 and other hyper-parameters are tuned on a development set consisting of 10% randomly selected sentences from the training data.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.985840916633606}]}, {"text": "RMSprop () is used as optimizer.: Experimental results (F1 scores) on synthetic and manually-annoated testing datasets, denoted as S-weibo and R-weibo, respectively.", "labels": [], "entities": [{"text": "F1 scores)", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9661573966344198}]}, {"text": "In S-weibo, we only replace three types of English words using rules, so there is no OOV and other English words in it.", "labels": [], "entities": [{"text": "OOV", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.868224561214447}]}, {"text": "Besides, the Chinese POS tagger Jieba tags all foreign words as \"eng\", so its F1 scores are all considered to be 0 with regard to English words.: Experimental results by different cross-lingual embeddings (Uns-emb, Lprj-emb, Nprj-emb are crossembeddings generated by unsupervised training, linear embedding projection and non-linear embedding projection, respectively).", "labels": [], "entities": [{"text": "F1", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.9973612427711487}]}], "tableCaptions": [{"text": " Table 4: Statistics of synthetic and manually-annotated  datasets, denoted as S-weibo and M-weibo, respec- tively. (# of Chinese, English, Other denotes the num- ber of words, respectively.)", "labels": [], "entities": []}, {"text": " Table 5: Experimental results (F1 scores) on synthetic and manually-annoated testing datasets, denoted as S-weibo  and R-weibo, respectively. In S-weibo, we only replace three types of English words using rules, so there is no  OOV and other English words in it. Besides, the Chinese POS tagger Jieba tags all foreign words as \"eng\", so its  F1 scores are all considered to be 0 with regard to English words.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9768529236316681}, {"text": "F1", "start_pos": 343, "end_pos": 345, "type": "METRIC", "confidence": 0.9978761672973633}]}]}