{"title": [{"text": "Lexicon information in neural sentiment analysis: a multi-task learning approach", "labels": [], "entities": [{"text": "Lexicon information in neural sentiment analysis", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.7459258437156677}]}], "abstractContent": [{"text": "This paper explores the use of multi-task learning (MTL) for incorporating external knowledge in neural models.", "labels": [], "entities": [{"text": "multi-task learning (MTL)", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.6737022519111633}]}, {"text": "Specifically, we show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons.", "labels": [], "entities": [{"text": "MTL", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.822189211845398}, {"text": "BiLSTM sentiment classifier", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6349684695402781}]}, {"text": "Our MTL setup is shown to improve model performance (compared to a single-task setup) on both English and Norwegian sentence-level sentiment datasets.", "labels": [], "entities": []}, {"text": "The paper also introduces anew sentiment lexicon for Norwegian.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (, explicit negation annotated data, or labels representing inter-annotator agreement.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.9529677331447601}]}, {"text": "One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner.", "labels": [], "entities": []}, {"text": "Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in the model.", "labels": [], "entities": []}, {"text": "Despite achieving strong results, neural models are known to be difficult to interpret, as well as highly dependent on the training data.", "labels": [], "entities": []}, {"text": "Resources like sentiment lexicons, on the other hand, have the benefit of being completely transparent, as well as being easy to adapt or update.", "labels": [], "entities": []}, {"text": "Additionally, lexicons are often less sensitive to domain and frequency effects and can provide high coverage and precision even for rare words.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.994820237159729}]}, {"text": "We hypothesize that these two views of sentiment are complimentary and that even competitive neural models can benefit from incorporating lexicon information.", "labels": [], "entities": []}, {"text": "In the current work, we demonstrate that multitask learning) is a viable framework to incorporate lexicon information in a sentence-level sentiment classifier.", "labels": [], "entities": []}, {"text": "Our proposed multi-task model shares the lower layers in a multi-layer neural network, while allowing the higher layers to adapt to either the main or auxiliary task.", "labels": [], "entities": []}, {"text": "Specifically, the shared lower layers area feed-forward network which uses a sentiment lexicon auxiliary task to learn to predict token-level sentiment.", "labels": [], "entities": []}, {"text": "The higher layers use these learned representations as input fora BiLSTM sentiment model, which is trained on the main task of sentence-level classification.", "labels": [], "entities": [{"text": "BiLSTM sentiment", "start_pos": 66, "end_pos": 82, "type": "TASK", "confidence": 0.4502902925014496}, {"text": "sentence-level classification", "start_pos": 127, "end_pos": 156, "type": "TASK", "confidence": 0.7068834155797958}]}, {"text": "The intuition is that the representations learned from the auxiliary task give the model an advantage on the main task.", "labels": [], "entities": []}, {"text": "Compared to previous methods, our model has two advantages: 1) it requires only a single sentiment lexicon, and 2) the lexicon prediction model is able to generalize to words that are not found in the lexicon, increasing the overall performance.", "labels": [], "entities": []}, {"text": "Experimental results are reported for both English and Norwegian, with the code 1 made available.", "labels": [], "entities": []}, {"text": "While we rely on an existing sentiment lexicon for English, we introduce and make available anew lexicon for Norwegian.", "labels": [], "entities": []}, {"text": "In the following, we first consider relevant related work ( \u00a7 2), and describe the sentiment lexicons ( \u00a7 3) and datasets ( \u00a7 4) that we use for our experiments.", "labels": [], "entities": []}, {"text": "In \u00a7 5 we detail our proposed multitask model, while \u00a7 6 presents the experimental results and error analysis.", "labels": [], "entities": []}, {"text": "Finally, we summarize and point to future directions in \u00a7 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following we present the datasets used to train and evaluate our sentence-level classifiers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overview of the Norwegian sentiment  lexicon, showing counts for the manually in- spected translations, the full-forms of the ex- panded version, and finally the lemmas found after  expansion.", "labels": [], "entities": [{"text": "Norwegian sentiment  lexicon", "start_pos": 26, "end_pos": 54, "type": "DATASET", "confidence": 0.7852571209271749}]}, {"text": " Table 2: Corpus counts for the Norwegian dataset.", "labels": [], "entities": [{"text": "Norwegian dataset", "start_pos": 32, "end_pos": 49, "type": "DATASET", "confidence": 0.9663777947425842}]}, {"text": " Table 3: Macro F 1 of models on the SST and  NoReC eval sentence-level datasets. Neural models  report mean and standard deviation of the scores  over five runs.", "labels": [], "entities": [{"text": "NoReC eval sentence-level datasets", "start_pos": 46, "end_pos": 80, "type": "DATASET", "confidence": 0.8377064764499664}]}, {"text": " Table 4: Examples where MTL performs better and worse than STL. A red box indicates negative  polarity (blue box indicates positive) according to the sentiment lexicon used to in the auxiliary training  task.", "labels": [], "entities": [{"text": "MTL", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.891887903213501}]}, {"text": " Table 6: Macro F 1 of models on the SST sentence- level datasets. We compare the MTL model on  SST using different lexicons.", "labels": [], "entities": [{"text": "SST sentence- level datasets", "start_pos": 37, "end_pos": 65, "type": "DATASET", "confidence": 0.7997351050376892}]}]}