{"title": [{"text": "CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context in Morphology", "labels": [], "entities": [{"text": "CMU-01", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9084697365760803}]}], "abstractContent": [{"text": "This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019 task 2 of Morphological Analysis and Lemma-tization in Context.", "labels": [], "entities": [{"text": "CMU-01 team", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.9404548406600952}, {"text": "SIGMORPHON 2019 task", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.6924245158831278}, {"text": "Morphological Analysis", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.8933997750282288}]}, {"text": "This task requires us to produce the lemma and morpho-syntactic description of each token in a sequence, for 107 treebanks.", "labels": [], "entities": []}, {"text": "We approach this task with a hierarchical neural conditional random field (CRF) model which predicts each coarse-grained feature (eg. POS, Case, etc.) independently.", "labels": [], "entities": []}, {"text": "However, most treebanks are under-resourced, thus making it challenging to train deep neu-ral models for them.", "labels": [], "entities": []}, {"text": "Hence, we propose a multilingual transfer training regime where we transfer from multiple related languages that share similar typology.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological analysis is the task of predicting morpho-syntactic properties along with the lemma of each token in a sequence, with several downstream applications including machine translation (, named entity recognition ( and semantic role labeling (.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9194655418395996}, {"text": "machine translation", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.827829897403717}, {"text": "entity recognition", "start_pos": 203, "end_pos": 221, "type": "TASK", "confidence": 0.6940316259860992}, {"text": "semantic role labeling", "start_pos": 228, "end_pos": 250, "type": "TASK", "confidence": 0.6676942805449168}]}, {"text": "Advances in deep learning have enabled significant progress for the task of morphological tagging) and lemmatization () under large amounts of annotated data.", "labels": [], "entities": [{"text": "morphological tagging", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.628708690404892}]}, {"text": "However, most languages are underresourced and often exhibit diverse linguistic phenomena, thus making it challenging to generalize existing state-of-the-art models for all languages.", "labels": [], "entities": []}, {"text": "In order to tackle the issue of data scarcity, recent approaches have coupled deep learning with cross-lingual transfer learning (; Kondratyuk, The code is available at https://github.com/ Aditi138/MorphologicalAnalysis/. 2019) and have shown promising results.", "labels": [], "entities": []}, {"text": "Previous works (e.g., combine the set of morphological properties into a single monolithic tag and employ multi-sequence classification.", "labels": [], "entities": [{"text": "multi-sequence classification", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.72258660197258}]}, {"text": "This runs the risk of data sparsity and exploding output space for morphologically rich languages.", "labels": [], "entities": []}, {"text": "instead predict each coarse-grained feature, such as part-ofspeech (POS) or Case, separately by modeling dependencies between these features and also between the labels across the sequence using a factorial conditional random field (CRF).", "labels": [], "entities": []}, {"text": "However, this results in a large number of factors leading to a slower training time (over 24h).", "labels": [], "entities": []}, {"text": "To address the issues of both data sparsity and having a tractable computation time, we propose a hierarchical neural model which predicts each coarse-grained feature independently, but without modeling the pairwise interactions within them.", "labels": [], "entities": []}, {"text": "This results in a time-efficient computation (5-6h) and substantially outperforms the baselines.", "labels": [], "entities": []}, {"text": "To more explicitly incorporate syntactic knowledge, we embed POS information in an encoder which is shared with all feature decoders.", "labels": [], "entities": []}, {"text": "To address the issue of data scarcity, we present two multilingual transfer approaches where we train on a group of typologically related languages and find that language-groups with shallower time-depths (i.e., period of time during which languages diverged to become independent) tend to benefit the most from transfer.", "labels": [], "entities": []}, {"text": "We focus on the task of contextual morphological analysis and use the provided baseline model for the task of lemmatization (.", "labels": [], "entities": [{"text": "contextual morphological analysis", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.7031020720799764}]}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We present a hierarchical neural model for contextual morphological analysis with a shared encoder and independent decoders for each coarse-grained feature.", "labels": [], "entities": [{"text": "contextual morphological analysis", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6975220044453939}]}, {"text": "This provides us with the flexibility to produce any combination of features.", "labels": [], "entities": []}, {"text": "2. We analyze the dependencies among different morphological features to inform model choices, and find that adding POS information to the encoder significantly improves prediction accuracy by reducing errors across features, particularly Gender errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9328234791755676}]}, {"text": "3. We evaluate our proposed approach on 107 treebanks and achieve +14.76 (accuracy) average improvement over the shared task baseline) for morphological analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9993240833282471}, {"text": "morphological analysis", "start_pos": 139, "end_pos": 161, "type": "TASK", "confidence": 0.8256460130214691}]}], "datasetContent": [{"text": "We conduct the following experiments: We compare our multi-lingual transfer approach with the baselines Baselines: Cotterell and Heigold (2017) formulate this task as a sequence prediction problem with the output space being the set of all possible tagsets seen in the training data.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 169, "end_pos": 188, "type": "TASK", "confidence": 0.7446286082267761}]}, {"text": "Specifically, they construct a neural network based multi-class classifier where each tagset {N;PL;NOM;FEM} forms a class.", "labels": [], "entities": [{"text": "FEM", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9222591519355774}]}, {"text": "Since the output space is only restricted to the tagsets seen in the training data, this method cannot generalize to unseen tagsets.", "labels": [], "entities": []}, {"text": "Furthermore, for morphologically rich languages such as Russian or Turkish, the output space of the tagset is huge leading to sparse training data.", "labels": [], "entities": []}, {"text": "Since morphosyntactic properties are often correlated, they model these inter-dependencies using a factorial CRF and define two inter-dependencies: 1) a pairwise dependency, which models correlations between the morpho-syntactic properties within a token, and 2) a transition dependency, which models label correlations across all tokens in a sequence.", "labels": [], "entities": []}, {"text": "Although this formulation provides the flexibility to produce any combination of tagsets, this model is computationally expensive to train since the factors model dependencies between all labels of all coarse-grained features, leading to >20k factors.", "labels": [], "entities": []}, {"text": "Data processing: We use the train/dev/test split provided in the shared task.", "labels": [], "entities": [{"text": "Data processing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7523430585861206}]}, {"text": "Since we model feature-wise prediction for each coarse-grained feature, our model requires the provided data to be annotated for coarse-grained features.", "labels": [], "entities": []}, {"text": "Therefore, we construct a feature-label dictionary based on the UM documentation 4 to map the individual fine-grained traits, which are in the UM schema, to their respective coarse-grained categories.", "labels": [], "entities": [{"text": "UM documentation 4", "start_pos": 64, "end_pos": 82, "type": "DATASET", "confidence": 0.9306985139846802}]}, {"text": "This transforms the tagset {N;PL;NOM;FEM} as {POS=N;Number=PL;Case=NOM;Gender=FEM}.", "labels": [], "entities": [{"text": "FEM", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9591050744056702}, {"text": "FEM", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.8416728973388672}]}, {"text": "We note that usually a token has a subset of the coarse-grained categories, therefore we extend the morphological tagset for each token by adding the remaining features observed in the training set and assigning them a special value \" \" which denotes null.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Example of manually constructed typology  features for the Indo-Aryan cluster.", "labels": [], "entities": []}, {"text": " Table 2: Comparing our model for bilingual transfer with previous baselines.", "labels": [], "entities": [{"text": "bilingual transfer", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7923629879951477}]}, {"text": " Table 3: Multi-lingual comparison results for Marathi  (mr-ufal), Sanskrit (sa-ufal) and Belarusian (be-hse) on  the validation set.", "labels": [], "entities": []}, {"text": " Table 4: Ablation results for Marathi (mr-ufal),  Ukrainian (uk-iu) and Hindi (hi-hdtb) with training size  of 373, 5441, 13381 respectively on the validation set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9977994561195374}]}]}