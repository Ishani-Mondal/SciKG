{"title": [{"text": "Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation", "labels": [], "entities": [{"text": "Sequence-to-Sequence Recurrent Neural Network Machine Translation", "start_pos": 40, "end_pos": 105, "type": "TASK", "confidence": 0.7989076177279154}]}], "abstractContent": [{"text": "Reduction of training time is an important issue in many tasks like patent translation involving neural networks.", "labels": [], "entities": [{"text": "patent translation", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8145860433578491}]}, {"text": "Data parallel-ism and model parallelism are two common approaches for reducing training time using multiple graphics processing units (GPUs) on one machine.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hybrid data-model parallel approach for sequence-to-sequence (Seq2Seq) recurrent neural network (RNN) machine translation.", "labels": [], "entities": [{"text": "Seq2Seq) recurrent neural network (RNN) machine translation", "start_pos": 90, "end_pos": 149, "type": "TASK", "confidence": 0.5530318886041641}]}, {"text": "We apply a model parallel approach to the RNN en-coder-decoder part of the Seq2Seq model and a data parallel approach to the attention -softmax part of the model.", "labels": [], "entities": []}, {"text": "We achieved a speed-up of 4.13 to 4.20 times when using 4 GPUs compared with the training speed when using 1 GPU without affecting machine translation accuracy as measured in terms of BLEU scores.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.6222744137048721}, {"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.8268207311630249}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9986999034881592}]}], "introductionContent": [{"text": "Neural machine translation (NMT) has been widely used owing to its high accuracy.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7952476044495901}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9976087808609009}]}, {"text": "A downside of NMT is it requires along training time.", "labels": [], "entities": [{"text": "NMT", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8411619663238525}]}, {"text": "For instance, training a Seq2Seq RNN machine translation (MT) with attention () could takeover 10 days using 10 million sentence pairs.", "labels": [], "entities": [{"text": "Seq2Seq RNN machine translation (MT)", "start_pos": 25, "end_pos": 61, "type": "TASK", "confidence": 0.6263028106519154}]}, {"text": "A natural solution to this is to use multiple GPUs.", "labels": [], "entities": []}, {"text": "There are currently two common approaches for reducing the training time of NMT models.", "labels": [], "entities": []}, {"text": "One approach is by using data parallel approach, while the other approach is through the use of the model parallel approach.", "labels": [], "entities": []}, {"text": "The data parallel approach is common in many neural network (NN) frameworks.", "labels": [], "entities": [{"text": "data parallel", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.7099363654851913}]}, {"text": "For instance, OpenNMT-lua ( , an NMT toolkit, uses multiple GPUs in training NN models using the data parallel approach.", "labels": [], "entities": []}, {"text": "In this approach, the same model is distributed to different GPUs as replicas, and each replica is updated using different data.", "labels": [], "entities": []}, {"text": "Afterward, the gradients obtained from each replica are accumulated, and parameters are updated.", "labels": [], "entities": []}, {"text": "The model parallel approach has been used for training a Seq2Seq RNN MT with attention (.", "labels": [], "entities": [{"text": "Seq2Seq RNN MT", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.42570311824480694}]}, {"text": "In this approach, the model is distributed across multiple GPUs, that is, each GPU has only apart of the model.", "labels": [], "entities": []}, {"text": "Subsequently, the same data are processed by all GPUs so that each GPU estimates the parameters it is responsible for.", "labels": [], "entities": []}, {"text": "In this paper, we propose a hybrid data-model parallel approach for Seq2Seq RNN MT with attention.", "labels": [], "entities": [{"text": "Seq2Seq RNN MT", "start_pos": 68, "end_pos": 82, "type": "TASK", "confidence": 0.5254895587762197}]}, {"text": "We apply a model parallel approach to the RNN encoder-decoder part of the Seq2Seq model and a data parallel approach to the attention-softmax part of the model.", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows: In Section 2, we describe related work.", "labels": [], "entities": []}, {"text": "In Section 3, first, we discuss the baseline model with/without data/model parallelism.", "labels": [], "entities": []}, {"text": "Afterward, we present the proposed hybrid data-model parallel approach.", "labels": [], "entities": []}, {"text": "In Section 4, we present a comparison of these parallel approaches and demonstrate the scalability of the proposed hybrid parallel approach.", "labels": [], "entities": []}, {"text": "Section 5 presents the conclusion of the work.", "labels": [], "entities": []}, {"text": "necessary to use multiple GPUs when training NN models within a short turnaround time.", "labels": [], "entities": []}, {"text": "There are two common approaches for using multiple GPUs in training.", "labels": [], "entities": []}, {"text": "One is data parallelism, involving sending different data to different GPUs with the replicas of the same model.", "labels": [], "entities": [{"text": "data parallelism", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7371644675731659}]}, {"text": "The other is model parallelism, involving sending the same data to different GPUs having different parts of the model.", "labels": [], "entities": [{"text": "model parallelism", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.6799910366535187}]}], "datasetContent": [{"text": "We evaluate training speed, convergence speed, and translation accuracy to compare the performance of the proposed approach as shown in (hereafter referred to as HybridNMT) with the baseline model shown in with/without data/model parallelism.", "labels": [], "entities": [{"text": "translation", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.8589193820953369}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9058058261871338}]}, {"text": "We also augment the proposed approach in with input-feeding (hereafter referred to as HybridNMTIF).", "labels": [], "entities": []}, {"text": "HybridNMTIF lacks the parallelism in the decoder side but has inputfeeding.", "labels": [], "entities": []}, {"text": "Consequently, comparing HybridNMT with HybridNMTIF clarifies the advantages of the proposed hybrid parallelism.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Datasets of WMT14 and WMT17.", "labels": [], "entities": [{"text": "WMT14", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.9191931486129761}, {"text": "WMT17", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9062392115592957}]}, {"text": " Table 3. Results of training speed and scaling fac- tors.", "labels": [], "entities": []}, {"text": " Table 3.  The perplexities obtained with model parallelism  became similar to those of our hybrid parallelism  after long runs. Finally, the convergence speed of  HybridNMTIF was between those of HybridNMT  and the baseline model with model parallelism.  This indicates that the proposed hybrid data- model parallel approach is faster than model par- allelism, and removing input-feeding leads to  faster convergence.", "labels": [], "entities": []}, {"text": " Table 4. BLEU scores obtained using different hyperparameters for WMT14 and WMT17 development  data. The upper half shows the results obtained by OpenNMT-lua whereas the lower half is for the  proposed HybridNMT. \"b\" stands for the beam size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.998978853225708}, {"text": "WMT14", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9259319305419922}, {"text": "WMT17 development  data", "start_pos": 77, "end_pos": 100, "type": "DATASET", "confidence": 0.8846928874651591}]}, {"text": " Table 5. BLEU scores published regarding  Seq2Seq RNN MT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990742206573486}, {"text": "Seq2Seq RNN MT", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.5756403009096781}]}]}