{"title": [{"text": "Testing the Generalization Power of Neural Network Models Across NLI Benchmarks", "labels": [], "entities": [{"text": "NLI Benchmarks", "start_pos": 65, "end_pos": 79, "type": "DATASET", "confidence": 0.8244430422782898}]}], "abstractContent": [{"text": "Neural network models have been very successful in natural language inference, with the best models reaching 90% accuracy in some benchmarks.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 51, "end_pos": 77, "type": "TASK", "confidence": 0.6502156853675842}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993220567703247}]}, {"text": "However, the success of these models turns out to be largely benchmark specific.", "labels": [], "entities": []}, {"text": "We show that models trained on a natural language inference dataset drawn from one benchmark fail to perform well in others, even if the notion of inference assumed in these benchmarks is the same or similar.", "labels": [], "entities": []}, {"text": "We train six high performing neural network models on different datasets and show that each one of these has problems of generalizing when we replace the original test set with a test set taken from another corpus designed for the same task.", "labels": [], "entities": []}, {"text": "In light of these results, we argue that most of the current neural network models are notable to generalize well in the task of natural language inference.", "labels": [], "entities": [{"text": "natural language inference", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.6305676996707916}]}, {"text": "We find that using large pre-trained language models helps with transfer learning when the datasets are similar enough.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.9690452516078949}]}, {"text": "Our results also highlight that the current NLI datasets do not cover the different nuances of inference extensively enough.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.7857742607593536}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) has attracted considerable interest in the NLP community and, recently, a large number of neural network-based systems have been proposed to deal with the task.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7825480898221334}]}, {"text": "One can attempt a rough categorization of these systems into: a) sentence encoding systems, and b) other neural network systems.", "labels": [], "entities": [{"text": "sentence encoding", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.7223736345767975}]}, {"text": "Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.4%, which is our baseline with BERT (, and 86.7%) respectively.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9225406646728516}, {"text": "MultiNLI datasets", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9177651405334473}, {"text": "BERT", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9989230036735535}]}, {"text": "However, a big question with respect to these systems is their ability to generalize outside the specific datasets they are trained and tested on.", "labels": [], "entities": []}, {"text": "Recently, have shown that state-of-the-art NLI systems break considerably easily when, instead of tested on the original SNLI test set, they are tested on a test set which is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise.", "labels": [], "entities": [{"text": "SNLI test set", "start_pos": 121, "end_pos": 134, "type": "DATASET", "confidence": 0.8901944557825724}]}, {"text": "The results show a very significant drop inaccuracy for three of the four systems.", "labels": [], "entities": []}, {"text": "The system that was more difficult to break and had the least loss inaccuracy was the system by which utilizes external knowledge taken from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 141, "end_pos": 148, "type": "DATASET", "confidence": 0.9676370620727539}]}, {"text": "In this paper we show that NLI systems that have been very successful in specific NLI benchmarks, fail to generalize when trained on a specific NLI dataset and then these trained models are tested across test sets taken from different NLI benchmarks.", "labels": [], "entities": []}, {"text": "The results we get are inline with, showing that the generalization capability of the individual NLI systems is very limited, but, what is more, they further show the only system that was less prone to breaking in, breaks too in the experiments we have conducted.", "labels": [], "entities": []}, {"text": "We train six different state-of-the-art models on three different NLI datasets and test these trained models on an NLI test set taken from another dataset designed for the same NLI task, namely for the task to identify for sentence pairs in the dataset if one sentence entails the other one, if they are in contradiction with each other or if they are neutral with respect to inferential relationship.", "labels": [], "entities": [{"text": "NLI test set", "start_pos": 115, "end_pos": 127, "type": "DATASET", "confidence": 0.7420741319656372}]}, {"text": "One would expect that if a model learns to correctly identify inferential relationships in one dataset, then it would also be able to do so in another dataset designed for the same task.", "labels": [], "entities": []}, {"text": "Furthermore, two of the datasets, SNLI and, have been constructed using the same crowdsourcing approach and annotation instructions, leading to datasets with the same or at least very similar definition of entailment.", "labels": [], "entities": []}, {"text": "It is therefore reasonable to expect that transfer learning between these datasets is possible.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9140062630176544}]}, {"text": "As SICK () dataset has been machineconstructed, a bigger difference in performance is expected.", "labels": [], "entities": []}, {"text": "In this paper we show that, contrary to our expectations, most models fail to generalize across the different datasets.", "labels": [], "entities": []}, {"text": "However, our experiments also show that BERT performs much better than the other models in experiments between SNLI and MultiNLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9987416863441467}, {"text": "SNLI", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.7729827761650085}, {"text": "MultiNLI", "start_pos": 120, "end_pos": 128, "type": "DATASET", "confidence": 0.9210602641105652}]}, {"text": "Nevertheless, even BERT fails when testing on SICK.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9984702467918396}]}, {"text": "In addition to the negative results, our experiments further highlight the power of pre-trained language models, like BERT, in NLI.", "labels": [], "entities": [{"text": "BERT", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9946330785751343}]}, {"text": "The negative results of this paper are significant for the NLP research community as well as to NLP practice as we would like our best models to not only to be able to perform well in a specific benchmark dataset, but rather capture the more general phenomenon this dataset is designed for.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is that it shows that most of the best performing neural network models for NLI fail in this regard.", "labels": [], "entities": []}, {"text": "The second, and equally important, contribution is that our results highlight that the current NLI datasets do not capture the nuances of NLI extensively enough.", "labels": [], "entities": [{"text": "NLI datasets", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8111881017684937}]}], "datasetContent": [{"text": "In this section we describe the datasets and model architectures included in the experiments.", "labels": [], "entities": []}, {"text": "Accuracy drops the most when a model is tested on SICK.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9966249465942383}, {"text": "SICK", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.7681921720504761}]}, {"text": "The difference in this case is between 19.0-29.0 points when trained on MultiNLI, between 31.6-33.7 points when trained on SNLI and between 31.1-33.0 when trained on SNLI + MultiNLI.", "labels": [], "entities": [{"text": "MultiNLI", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9553149342536926}, {"text": "SNLI", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.934625506401062}, {"text": "SNLI + MultiNLI", "start_pos": 166, "end_pos": 181, "type": "DATASET", "confidence": 0.796340823173523}]}, {"text": "This was expected, as the method of constructing the sentence pairs was different, and hence there is too much difference in the kind of sentence pairs included in the training and test sets for transfer learning to work.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.9113817512989044}]}], "tableCaptions": [{"text": " Table 1: Dataset combinations used in the experiments. The rows in bold are baseline experiments, where the test  data comes from the same benchmark as the training and development data.", "labels": [], "entities": []}]}