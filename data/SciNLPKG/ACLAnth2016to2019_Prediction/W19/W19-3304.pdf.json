{"title": [{"text": "Parsing Meaning Representations: is Easier Always Better?", "labels": [], "entities": [{"text": "Parsing Meaning Representations", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9098278681437174}]}], "abstractContent": [{"text": "The parsing accuracy varies a great deal for different meaning representations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9608917236328125}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9305599331855774}]}, {"text": "In this paper, we compare the parsing performances between Abstract Meaning Representation (AMR) and Minimal Recursion Semantics (MRS), and provide an in-depth analysis of what factors contributed to the discrepancy in their parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9719465374946594}, {"text": "Abstract Meaning Representation (AMR)", "start_pos": 59, "end_pos": 96, "type": "TASK", "confidence": 0.7300368448098501}, {"text": "Minimal Recursion Semantics (MRS)", "start_pos": 101, "end_pos": 134, "type": "TASK", "confidence": 0.6946294158697128}, {"text": "parsing", "start_pos": 225, "end_pos": 232, "type": "TASK", "confidence": 0.9503828883171082}, {"text": "accuracy", "start_pos": 233, "end_pos": 241, "type": "METRIC", "confidence": 0.5724117755889893}]}, {"text": "By crystaliz-ing the trade-off between representation ex-pressiveness and ease of automatic parsing, we hope our results can help inform the design of the next-generation meaning representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Meaning representation (MR) parsing is the task of parsing natural language sentences into a formal representation that encodes the meaning of a sentence.", "labels": [], "entities": [{"text": "Meaning representation (MR) parsing", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8603263795375824}, {"text": "parsing natural language sentences", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.8622808903455734}]}, {"text": "As a matter of convention in the field of natural language processing, meaning representation parsing is distinguished from semantic parsing, a form of domain-dependent parsing that analyzes text into executable code for some specific applications.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.7122655312220255}, {"text": "meaning representation parsing", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.818652868270874}, {"text": "semantic parsing", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.7532999217510223}]}, {"text": "Earlier work in semantic parsing focused on parsing natural language sentences into semantic queries that can be executed against a knowledge base to answer factual questions.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.7943368852138519}, {"text": "parsing natural language sentences into semantic queries", "start_pos": 44, "end_pos": 100, "type": "TASK", "confidence": 0.8666995338031224}]}, {"text": "More recently, this line of work has been extended to parsing natural language text into computer programs ( and parsing tabular information in texts.", "labels": [], "entities": [{"text": "parsing natural language text", "start_pos": 54, "end_pos": 83, "type": "TASK", "confidence": 0.8915498703718185}, {"text": "parsing tabular information in texts", "start_pos": 113, "end_pos": 149, "type": "TASK", "confidence": 0.851185965538025}]}, {"text": "Here we focus on the parsing of natural language sentences into domain-independent MRs that are not geared towards anyone particular application, but could be potentially useful fora wide range of applications.", "labels": [], "entities": [{"text": "parsing of natural language sentences into domain-independent MRs", "start_pos": 21, "end_pos": 86, "type": "TASK", "confidence": 0.838870957493782}]}, {"text": "* Work done during the internship at Brandeis University.", "labels": [], "entities": []}, {"text": "The challenge for developing a general-purpose meaning representation is that there is not a universally accepted standard and as a result, existing MRs vary a great deal with respect to which aspects of the linguistic meaning of a sentence are included and how they are represented.", "labels": [], "entities": []}, {"text": "For example, existing MRs differ in whether and how they represent named entities, word sense, coreference, and semantic roles, among other meaning components.", "labels": [], "entities": []}, {"text": "These design decisions have consequences for the automatic parsing of these MRs.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.7434383630752563}, {"text": "MRs", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.86772620677948}]}, {"text": "Among two of the meaning representations for which largescale manual annotated data exist, the state-ofthe-art parsing accuracy for AMR is generally in the high 60s and low 70s, while state-of-the-art parsing accuracy for (variations of) MRS is in the high 80s and low 90s).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.8657182455062866}, {"text": "AMR", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.5168685913085938}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.7485095858573914}]}, {"text": "Little has been done thus far to investigate the underlying causes for this rather large discrepancy.", "labels": [], "entities": []}, {"text": "For purposes of developing the next generation MRs, it is important to know i) which aspects of the MR pose the most challenge to automatic parsing and ii) whether these challenges are \"necessary evils\" because the information encoded in the MR is important to downstream applications and has to be included, or they can be simplified without hurting the utility of the MR.", "labels": [], "entities": []}, {"text": "To answer these questions, we compare the parsing results between AMR and MRS, two meaning representations for which large-scale manually annotated data sets exist.", "labels": [], "entities": [{"text": "AMR", "start_pos": 66, "end_pos": 69, "type": "DATASET", "confidence": 0.6764243245124817}, {"text": "MRS", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.5959677696228027}]}, {"text": "We use the same parser trained on data sets annotated with the two MRs to ensure that the difference in parsing performance is not due to the difference in parsing algorithms, and we also use the same evaluation metric to ensure that the parsing accuracy is evaluated the same way.", "labels": [], "entities": []}, {"text": "The evaluation tool we use is SMATCH, and the parser we use is CAMR (, a transition-based parser originally developed for AMR that we adapt to MRS.", "labels": [], "entities": [{"text": "SMATCH", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.657328188419342}, {"text": "MRS", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.8325318694114685}]}, {"text": "To make CAMR as well as SMATCH work on MRS data, we rewrote the native MRS data in PENMAN notation.", "labels": [], "entities": [{"text": "MRS", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8794925808906555}]}, {"text": "Ideally, the parser needs to be trained on the same source text annotated with these two MRs to isolate the contributions of the MR from other factors, but this is not currently possible, so we fallback on the next best thing, and use data sets annotated with AMR and MRS that are similar in size.", "labels": [], "entities": [{"text": "AMR", "start_pos": 260, "end_pos": 263, "type": "DATASET", "confidence": 0.809685468673706}]}, {"text": "Our experimental results show that the SMATCH score for MRS parsing is almost 20% higher than that for AMR.", "labels": [], "entities": [{"text": "SMATCH score", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9770853817462921}, {"text": "MRS parsing", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9454897940158844}]}, {"text": "A detailed comparative analysis of the parsing results reveals that the main contributing factors into the lower parsing accuracy for AMR are the following: \u2022 AMR concepts show a higher level of abstraction from surface forms, meaning that AMR concepts bear less resemblance to the word tokens in the original sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8198209404945374}, {"text": "AMR", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.9021397829055786}]}, {"text": "\u2022 AMR does a much more fine-grained classification for the named entities than MRS, which contributes to errors in concept identification.", "labels": [], "entities": [{"text": "AMR", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.4864218831062317}, {"text": "concept identification", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.7053634822368622}]}, {"text": "\u2022 Semantic relations are defined differently in AMR and MRS.", "labels": [], "entities": [{"text": "AMR", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.7481801509857178}, {"text": "MRS", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.5647315979003906}]}, {"text": "While in AMR a semantic role represents a semantic relation between a verbal or nominal predicate and its argument, in MRS the predicate can also be a preposition, adjectives, or adverbs.", "labels": [], "entities": [{"text": "AMR", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9447776675224304}, {"text": "MRS", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.8578370213508606}]}, {"text": "Another difference is that while in AMR, the semantic roles for the core arguments of a predicate are interpretable with respect to an external lexicon, the semantic roles in MRS reflect the level of obliqueness and are linked to an external lexicon.", "labels": [], "entities": [{"text": "AMR", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9057804942131042}]}, {"text": "We hope that by clearly identifying aspects of the MR that contributed to the challenges in automatic meaning representation parsing, we can help researchers make more informed decisions on 1 These do not necessarily account for all the factors that might contribute to the discrepancy in performance between the two meaning representations.", "labels": [], "entities": [{"text": "meaning representation parsing", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.6580294668674469}]}, {"text": "As one reviewer points out, the lack of manual alignment between word tokens in a sentence and the concepts in its AMR graph may also have contributed to challenge in parsing AMRs.", "labels": [], "entities": [{"text": "parsing AMRs", "start_pos": 167, "end_pos": 179, "type": "TASK", "confidence": 0.8122375905513763}]}, {"text": "Annotation consistency in the data set may also be a contributing factor.", "labels": [], "entities": [{"text": "Annotation consistency", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.7854093909263611}]}, {"text": "There are no obvious way to quantify these factors and we leave these to future research.", "labels": [], "entities": []}, {"text": "the trade-off between representation expressiveness and ease of automatic parsing when developing the next-generation MRs.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: Section 2 briefly describes the key elements of MRS and AMR; Section 3 reports our experiment setup and main parsing results for the two MRs; Section 4 provides a detailed analysis of the impacts of different aspects of the MR on automatic parsing.", "labels": [], "entities": [{"text": "MRS", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.67794269323349}, {"text": "automatic parsing", "start_pos": 277, "end_pos": 294, "type": "TASK", "confidence": 0.5072693973779678}]}, {"text": "Section 5 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics and parsing results for MRS and AMR on the test set", "labels": [], "entities": [{"text": "MRS", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.511086642742157}, {"text": "AMR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.49062374234199524}]}, {"text": " Table 2: Node identification and WSD results on MRS  in terms of noun (n), verb (v), quantifier (q), preposi- tion (p), adjective (a), conjunction (c), and others (x),  and on AMR in terms of predicate (pred). Both are  measured on the test set in terms of accuracy based on  SMATCH.", "labels": [], "entities": [{"text": "Node identification", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8966597020626068}, {"text": "WSD", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8456664681434631}, {"text": "MRS", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9472413063049316}, {"text": "AMR", "start_pos": 177, "end_pos": 180, "type": "METRIC", "confidence": 0.4454633295536041}, {"text": "accuracy", "start_pos": 258, "end_pos": 266, "type": "METRIC", "confidence": 0.9993984699249268}, {"text": "SMATCH", "start_pos": 277, "end_pos": 283, "type": "DATASET", "confidence": 0.8042562007904053}]}, {"text": " Table 3: Individual percentage and score for different types of AMR's predicates", "labels": [], "entities": [{"text": "Individual percentage", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.899684727191925}, {"text": "AMR's predicates", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8408419688542684}]}, {"text": " Table 5: Results on entity recognition on the test set", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.8442360460758209}]}, {"text": " Table 6: Results on SRL. MRS's argument number be- gins at 1 so we just move all the argument to begin at 0  to make them comparable.", "labels": [], "entities": [{"text": "SRL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8537274599075317}, {"text": "MRS", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.715354323387146}]}]}