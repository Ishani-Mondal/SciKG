{"title": [{"text": "Sieg at MEDIQA 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment", "labels": [], "entities": [{"text": "Biomedical Inference and Entailment", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.7429858669638634}]}], "abstractContent": [{"text": "This paper presents a multi-task learning approach to natural language inference (NLI) and question entailment (RQE) in the biomed-ical domain.", "labels": [], "entities": [{"text": "question entailment (RQE)", "start_pos": 91, "end_pos": 116, "type": "TASK", "confidence": 0.7661466777324677}]}, {"text": "Recognizing textual inference relations and question similarity can address the issue of answering new consumer health questions by mapping them to Frequently Asked Questions on reputed websites like the NIH 1.", "labels": [], "entities": [{"text": "NIH 1", "start_pos": 204, "end_pos": 209, "type": "DATASET", "confidence": 0.9355698525905609}]}, {"text": "We show that leveraging information from parallel tasks across domains along with medical knowledge integration allows our model to learn better biomedical feature representations.", "labels": [], "entities": [{"text": "medical knowledge integration", "start_pos": 82, "end_pos": 111, "type": "TASK", "confidence": 0.6962409615516663}]}, {"text": "Our final models for the NLI and RQE tasks achieve the 4 th and 2 nd rank on the shared-task leaderboard respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "The MEDIQA challenge (  aims to improve textual inference and entailment in the medical domain to build better domainspecific Information Retrieval and Question Answering systems.", "labels": [], "entities": [{"text": "textual inference", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6972148418426514}, {"text": "domainspecific Information Retrieval", "start_pos": 111, "end_pos": 147, "type": "TASK", "confidence": 0.6158672769864401}, {"text": "Question Answering", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.7466884255409241}]}, {"text": "There are three subtasks (NLI, RQE, QA), out of which we focus on -1.", "labels": [], "entities": []}, {"text": "Natural Language Inference (NLI): Identifying the three types of inference relations (Entailment, Neutral and Contradiction) between two sentences.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7536851565043131}]}], "datasetContent": [{"text": "Our hypothesis is that these parallel datasets will help our multi-task neural model capture salient biomedical features to help our main NLI and RQE tasks.", "labels": [], "entities": []}, {"text": "The accuracy values obtained on the shared task's leaderboard are listed in for the NLI and the RQE task respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993215799331665}, {"text": "RQE task", "start_pos": 96, "end_pos": 104, "type": "TASK", "confidence": 0.4794047176837921}]}, {"text": "For the NLI task,, we see that an ensemble of the MT-DNN base model along with MT-DNN initialized with SciBERT and BioBERT keeping PubMed RCT and MultiNLI as the parallel datasets achieved a better accuracy than using only the NLI dataset with an MT-DNN base model, BioBERT ensemble.", "labels": [], "entities": [{"text": "BioBERT", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.9790614247322083}, {"text": "PubMed RCT", "start_pos": 131, "end_pos": 141, "type": "DATASET", "confidence": 0.9388534724712372}, {"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9986459612846375}]}, {"text": "To account for missing drug information and the lack of biomedical context around abbreviations in the input data, we preprocess our dataset by expanding medical abbreviations (5.2.5) and including DrugBank (5.2.4) information.", "labels": [], "entities": [{"text": "DrugBank (5.2.4) information", "start_pos": 198, "end_pos": 226, "type": "DATASET", "confidence": 0.8631886124610901}]}, {"text": "We see that taking a four-way ensemble of the MT-DNN base model, MT-DNN initialized with BioBERT, SciBERT and InferSent along with a three-pronged domain knowledge inclusion with MultiNLI and PubMed RCT as the parallel datasets gave us the best result of 91.1% on the leaderboard.", "labels": [], "entities": [{"text": "PubMed RCT", "start_pos": 192, "end_pos": 202, "type": "DATASET", "confidence": 0.9002963602542877}]}, {"text": "Our hypothesis behind this model ensemble was that since BioBERT and SciBERT are trained on different datasets, they will capture different features and hence taking an ensemble of these two models along with InferSent based on majority confidence scores will help us achieve a better accuracy than a single model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9969581365585327}]}, {"text": "Our Infersent re-implementation results are shown in.", "labels": [], "entities": []}, {"text": "To demonstrate the usefulness of parallel datasets for the RQE task and for easy comparison with the results on the leaderboard, we measure the test accuracy for different dataset combinations using the test dataset labels released by the task organizers post completion of the shared task.", "labels": [], "entities": [{"text": "RQE task", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.853060245513916}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.8699114322662354}]}, {"text": "These results are shown in   7 Error Analysis", "labels": [], "entities": [{"text": "Error", "start_pos": 31, "end_pos": 36, "type": "METRIC", "confidence": 0.9063701629638672}]}], "tableCaptions": [{"text": " Table 1: Baseline accuracy values for NLI dev set", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9122587442398071}]}, {"text": " Table 2: Baseline precision, recall and F1 values for  RQE", "labels": [], "entities": [{"text": "Baseline", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9283818006515503}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.8830124139785767}, {"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9996609687805176}, {"text": "F1", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9992495179176331}, {"text": "RQE", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.41737961769104004}]}, {"text": " Table 6: Error types observed during the qualitative analysis for the NLI Task", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9821313619613647}]}]}