{"title": [{"text": "ArbEngVec : Arabic-English Cross-Lingual Word Embedding Model", "labels": [], "entities": [{"text": "ArbEngVec", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7575814723968506}]}], "abstractContent": [{"text": "Word Embeddings (WE) are getting increasingly popular and widely applied in many Natural Language Processing (NLP) applications due to their effectiveness in capturing semantic properties of words; Machine Translation (MT), Information Retrieval (IR) and Information Extraction (IE) are among such areas.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 198, "end_pos": 222, "type": "TASK", "confidence": 0.8679612874984741}, {"text": "Information Retrieval (IR)", "start_pos": 224, "end_pos": 250, "type": "TASK", "confidence": 0.8274487733840943}, {"text": "Information Extraction (IE)", "start_pos": 255, "end_pos": 282, "type": "TASK", "confidence": 0.8481692016124726}]}, {"text": "In this paper, we propose an open source ArbEngVec which provides several Arabic-English cross-lingual word embedding models.", "labels": [], "entities": []}, {"text": "To train our bilingual models, we use a large dataset with more than 93 million pairs of Arabic-English parallel sentences.", "labels": [], "entities": []}, {"text": "In addition , we perform both extrinsic and intrinsic evaluations for the different word embedding model variants.", "labels": [], "entities": []}, {"text": "The extrinsic evaluation assesses the performance of models on the cross-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task.", "labels": [], "entities": [{"text": "cross-language Semantic Textual Similarity (STS)", "start_pos": 67, "end_pos": 115, "type": "TASK", "confidence": 0.6726466928209577}, {"text": "Word Translation (WT) task", "start_pos": 164, "end_pos": 190, "type": "TASK", "confidence": 0.8377522726853689}]}], "introductionContent": [{"text": "Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words.", "labels": [], "entities": [{"text": "Distributed word representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7050995826721191}]}, {"text": "Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models.", "labels": [], "entities": []}, {"text": "For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval), sentiment analysis () text classification (, semantic textual similarity and plagiarism detection ( . Cross-Lingual Word Embeddings (CLWE) is a more challenging task because the knowledge is transferred between two or more different languages (.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.7658345401287079}, {"text": "sentiment analysis", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.912268191576004}, {"text": "text classification", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.734536811709404}]}, {"text": "Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (), cross-language information retrieval, crosslanguage semantic similarity) and plagiarism detection across multiple languages).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8080268502235413}, {"text": "cross-language information retrieval", "start_pos": 104, "end_pos": 140, "type": "TASK", "confidence": 0.7512122591336569}, {"text": "plagiarism detection", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7706185579299927}]}, {"text": "Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest.", "labels": [], "entities": []}, {"text": "In this paper, we propose six Arabic-English cross-lingual word embedding models . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences.", "labels": [], "entities": []}, {"text": "The rest of this paper is organised as follows: in section 2 we provide a quick overview of work related to the cross-lingual word embedding models.", "labels": [], "entities": []}, {"text": "We describe our dataset collection and the preprocessing process in Section 3.", "labels": [], "entities": [{"text": "dataset collection", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.6440814882516861}]}, {"text": "Section 4 presents our proposed cross-lingual models.", "labels": [], "entities": []}, {"text": "Section 5 presents the evaluation results.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper with our main findings and points to possible directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Usually multilingual models go against two aspects of evaluation methodology: maintain monolingual aspect and provide the other cross-lingual.", "labels": [], "entities": []}, {"text": "Clearly for us, after creding on the shuffle we lost the former willingly to stick around the latter.", "labels": [], "entities": []}, {"text": "Preserving the model's monolingual behaviour requires keeping words in a semantic meaningful order, which is exactly what happens with our first parallel (non-shuffling) model with completely skewed cross-lingual aspect.", "labels": [], "entities": []}, {"text": "To clarify that, we have evaluated our models through Semantic Textual Similarity as extrinsic, and Word Translation as intrinsic.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6542579631010691}, {"text": "Word Translation", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.7469168305397034}]}, {"text": "In this step, we basically focused on word translation following) evaluation procedure, so we generated a 1000 tuples starting with choosing random 1000 words from the model vocabulary.", "labels": [], "entities": [{"text": "word translation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7602455317974091}]}, {"text": "Then, we find their k-closest (k most similar) cross-lingual words based on the cosine similarity in our six ArbEngVec models.", "labels": [], "entities": [{"text": "ArbEngVec", "start_pos": 109, "end_pos": 118, "type": "DATASET", "confidence": 0.8287747502326965}]}, {"text": "In fact, we have used five different values of k to generate the 1-closest, 2-closest, 3-closest, 5-closest and 10-closest words.", "labels": [], "entities": []}, {"text": "https://colab.research.google.com/ For example, shows the 5-closest words of and weapons in our random Skip-Gram model.", "labels": [], "entities": []}, {"text": "Afterwards, we calculate the accuracy of each range, which has been calculated by giving a value 1 to each word couple that represents a translation, we make sure that the word provided by our model is a translation with comparing it to Google Translate API's bag of words, if this comparison comes negative we compare manually, if also manual comparison comes negative we give negative score 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996042847633362}]}, {"text": "Eventually we count the average of the 1000 scores.", "labels": [], "entities": []}, {"text": "Results of the six studied models are provided in.", "labels": [], "entities": []}, {"text": "Parallel results were so dim bilingually as shows, but monolingual aspect was preserved especially in CBOW variant.", "labels": [], "entities": []}, {"text": "This fact is illustrated in: A sample of 5-closest words of and weapons in our Parallel CBOW model  Extrinsic evaluating means surveilling the model performance under real-world Natural Language Processing tasks use.", "labels": [], "entities": []}, {"text": "Our choice fell on Semantic Sentences Similarity (STS) task.", "labels": [], "entities": [{"text": "Semantic Sentences Similarity (STS) task", "start_pos": 19, "end_pos": 59, "type": "TASK", "confidence": 0.8332374862262181}]}, {"text": "To estimate the semantic similarity between the ArabicEnglish sentences, we have used the WE-based approach proposed by jointly with our ArbEngVec models.", "labels": [], "entities": [{"text": "WE-based", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.7159902453422546}, {"text": "ArbEngVec", "start_pos": 137, "end_pos": 146, "type": "DATASET", "confidence": 0.8718249201774597}]}, {"text": "In fact, we have had STS2017-Eval 10 datasets drawn from the shared taskSemEval-2017 Task1: STS Crosslingual Arabic-English ().", "labels": [], "entities": [{"text": "STS2017-Eval 10 datasets", "start_pos": 21, "end_pos": 45, "type": "DATASET", "confidence": 0.8129705389340719}, {"text": "STS Crosslingual Arabic-English", "start_pos": 92, "end_pos": 123, "type": "DATASET", "confidence": 0.8337898453076681}]}, {"text": "The sentence pairs of STS2017-Eval have been manually labelled by five annotators, and the similarity score is the average of the annotators judgments.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 91, "end_pos": 107, "type": "METRIC", "confidence": 0.9774526953697205}]}, {"text": "Afterwards, in order to evaluate the performance of each model, we calculate Pearson correlation between our assigned semantic similarity scores and human judgement.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 77, "end_pos": 96, "type": "METRIC", "confidence": 0.9757058024406433}]}, {"text": "reports the results of the six studied models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Some statistics about the used dataset (Tiedemann, 2012)", "labels": [], "entities": []}, {"text": " Table 3: Intrinsic evaluation results of ArbEngVec models", "labels": [], "entities": [{"text": "ArbEngVec", "start_pos": 42, "end_pos": 51, "type": "DATASET", "confidence": 0.7784451842308044}]}]}