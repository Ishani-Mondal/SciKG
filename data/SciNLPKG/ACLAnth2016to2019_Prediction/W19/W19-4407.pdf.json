{"title": [{"text": "A Benchmark Corpus of English Misspellings and a Minimally-supervised Model for Spelling Correction", "labels": [], "entities": [{"text": "Spelling Correction", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8714337944984436}]}], "abstractContent": [{"text": "Spelling correction has attracted a lot of attention in the NLP community.", "labels": [], "entities": [{"text": "Spelling correction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.972605973482132}]}, {"text": "However, models have been usually evaluated on artificially-created or proprietary corpora.", "labels": [], "entities": []}, {"text": "A publicly-available corpus of authentic misspellings, annotated in context, is still lacking.", "labels": [], "entities": []}, {"text": "To address this, we present and release an annotated data set of 6,121 spelling errors in context, based on a corpus of essays written by English language learners.", "labels": [], "entities": []}, {"text": "We also develop a minimally-supervised context-aware approach to spelling correction.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.9474914371967316}]}, {"text": "It achieves strong results on our data: 88.12% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.999323844909668}]}, {"text": "This approach can also train with a minimal amount of annotated data (performance reduced by less than 1%).", "labels": [], "entities": []}, {"text": "Furthermore, this approach allows easy porta-bility to new domains.", "labels": [], "entities": []}, {"text": "We evaluate our model on data from a medical domain and demonstrate that it rivals the performance of a model trained and tuned on in-domain data.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses automatic correction of spelling errors where the misspelled string is not a valid word in the language.", "labels": [], "entities": [{"text": "automatic correction of spelling errors", "start_pos": 21, "end_pos": 60, "type": "TASK", "confidence": 0.8280050873756408}]}, {"text": "Correcting non-word spelling errors has along history in the natural language processing research.", "labels": [], "entities": [{"text": "Correcting non-word spelling errors", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7922765612602234}, {"text": "natural language processing", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.6353707313537598}]}, {"text": "Earlier approaches were evaluated on spelling errors from proprietary corpora of native English texts or artificially generated errors in well-formed texts.", "labels": [], "entities": []}, {"text": "While spell checkers today are essential and ubiquitous, dealing with data in a variety of \"noisy\" domains poses particular challenges to traditional spell checkers.", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.8614380955696106}, {"text": "spell checkers", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.7084263563156128}]}, {"text": "Thus, spelling research has shifted focus primarily to correcting spelling errors in social media data, biomedical texts, and texts written by non-native English writers.", "labels": [], "entities": [{"text": "correcting spelling errors in social media", "start_pos": 55, "end_pos": 97, "type": "TASK", "confidence": 0.8686944246292114}]}, {"text": "Non-native English speakers account for the majority of people writing in English today, and spelling errors are some of the most frequent error types for these writers).", "labels": [], "entities": []}, {"text": "In some grammatical error correction approaches researchers apply a spellchecker prior to running a grammar-oriented correction model.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 8, "end_pos": 36, "type": "TASK", "confidence": 0.6513926188151041}]}, {"text": "In addition to writingassistance feedback, spelling correction for nonnative writers is also utilized in computer-aided language learning applications and in automatic scoring systems.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8723061382770538}]}, {"text": "Spelling correction in learner texts is particularly challenging.", "labels": [], "entities": [{"text": "Spelling correction in learner texts", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8934682965278625}]}, {"text": "Non-native writers have higher spelling error rates than native writers (.", "labels": [], "entities": [{"text": "spelling error rates", "start_pos": 31, "end_pos": 51, "type": "METRIC", "confidence": 0.8314718008041382}]}, {"text": "The types of misspellings produced by these writers typically differ from errors produced by native speakers.", "labels": [], "entities": []}, {"text": "While the majority of spelling errors produced by native speakers involve singlecharacter edits, multi-character edits area lot more common among non-native writers.", "labels": [], "entities": []}, {"text": "Finally, learner data is more likely to contain other errors or non-standard usage in context, which may further complicate error correction.", "labels": [], "entities": []}, {"text": "Several recent works have specifically addressed spelling correction in learner texts.", "labels": [], "entities": [{"text": "spelling correction in learner texts", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8270832180976868}]}, {"text": "However, they evaluated either on small data sets) or on proprietary corpora.", "labels": [], "entities": []}, {"text": "Despite several decades of research on spelling, there is still no publicly available largescale corpus, explicitly and exhaustively annotated for spelling errors.", "labels": [], "entities": []}, {"text": "Without such data, it is difficult to compare and track research progress in the field.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: \u2022 We present a corpus of learner essays, TOEFL-Spell, annotated for spelling errors.", "labels": [], "entities": [{"text": "TOEFL-Spell", "start_pos": 87, "end_pos": 98, "type": "METRIC", "confidence": 0.7878612279891968}]}, {"text": "This corpus can be used as a benchmark corpus to develop state-of-the-art models for spelling correction (Section 3).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.9531404078006744}]}, {"text": "\u2022 We develop a minimally-supervised approach to spelling correction that combines contextual and non-contextual information (Section 4).", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.9657871723175049}]}, {"text": "We show that inclusion of word embeddings provides information complementary to other contextual features.", "labels": [], "entities": []}, {"text": "\u2022 The proposed model is shown to be robust, evaluated on TOEFL-Spell and on an out-ofdomain data set of clinical notes.", "labels": [], "entities": [{"text": "TOEFL-Spell", "start_pos": 57, "end_pos": 68, "type": "METRIC", "confidence": 0.6764450073242188}]}, {"text": "The performance of our model on the clinical data set rivals that of the model trained on a corpus of clinical notes (Section 5).", "labels": [], "entities": [{"text": "clinical data set", "start_pos": 36, "end_pos": 53, "type": "DATASET", "confidence": 0.7886571884155273}]}, {"text": "\u2022 Evaluation of the contribution of contextual features shows that contextual information provides an error reduction of about 45%, improving the correction accuracy by 10 points on TOEFL-Spell and by 7 points on the clinical data set.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 102, "end_pos": 117, "type": "METRIC", "confidence": 0.975169837474823}, {"text": "correction", "start_pos": 146, "end_pos": 156, "type": "METRIC", "confidence": 0.9810865521430969}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.8066748976707458}, {"text": "TOEFL-Spell", "start_pos": 182, "end_pos": 193, "type": "METRIC", "confidence": 0.505132257938385}, {"text": "clinical data set", "start_pos": 217, "end_pos": 234, "type": "DATASET", "confidence": 0.7816466887791952}]}, {"text": "\u2022 Error analysis of the system on TOEFL-Spell and on the clinical data is presented in Section 6.", "labels": [], "entities": [{"text": "Error", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9862576127052307}, {"text": "TOEFL-Spell", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.5719901323318481}]}], "datasetContent": [{"text": "We address the following research questions: \u2022 How does the model compare to a baseline system?", "labels": [], "entities": []}, {"text": "\u2022 What is the contribution of individual features, especially those that provide contextual information?", "labels": [], "entities": []}, {"text": "\u2022 How much training data is needed to learn a robust model?", "labels": [], "entities": []}, {"text": "\u2022 How does the model behave on out-of-domain data?", "labels": [], "entities": []}, {"text": "First, we present results on error detection.", "labels": [], "entities": [{"text": "error detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7071287482976913}]}, {"text": "The system detected all 6,121 misspellings and flagged 43 additional words (false positives).", "labels": [], "entities": []}, {"text": "Thus, the detection recall is 100%, precision is 99.3% and F1 score is 99.65%.", "labels": [], "entities": [{"text": "detection", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.5305918455123901}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7439473867416382}, {"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.999830961227417}, {"text": "F1 score", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9784110486507416}]}, {"text": "This result applies to all experiments with the TOEFL-Spell data set.", "labels": [], "entities": [{"text": "TOEFL-Spell data set", "start_pos": 48, "end_pos": 68, "type": "DATASET", "confidence": 0.9329023957252502}]}, {"text": "The candidate generation performance is over 99%, i.e. for over 99% of the errors a valid correction is generated in the list of candidates.", "labels": [], "entities": []}, {"text": "Note that in the candidate generation stage, an average of 213 candidate corrections is generated for each misspelling in the TOEFL-Spell corpus.", "labels": [], "entities": [{"text": "TOEFL-Spell corpus", "start_pos": 126, "end_pos": 144, "type": "DATASET", "confidence": 0.9484674036502838}]}, {"text": "We now evaluate the performance of the candidate ranking component, checking whether the top-ranked candidate is indeed the gold correction.", "labels": [], "entities": []}, {"text": "The baseline system implements all the features, except word embeddings, and uses weights from.", "labels": [], "entities": []}, {"text": "For the new approach we add the feature computed with word-embeddings.", "labels": [], "entities": []}, {"text": "Feature weights are learned automatically, using linear classifiers -Logistic Regression and Averaged Perceptron.", "labels": [], "entities": []}, {"text": "We address the first research question above, using the TOEFL-Spell corpus in a five-fold crossvalidation.", "labels": [], "entities": [{"text": "TOEFL-Spell corpus", "start_pos": 56, "end_pos": 74, "type": "DATASET", "confidence": 0.8949695825576782}]}, {"text": "Each of the classifiers outperforms the baseline, and the differences are statistically significant (by twoproportions z-Test).", "labels": [], "entities": []}, {"text": "The difference between Perceptron and Logistic Regression is not significant.", "labels": [], "entities": [{"text": "Logistic Regression", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6729649305343628}]}, {"text": "The Perceptron algorithm is the best model, with over 2 points of absolute improvement, which is an error reduction of 15%.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 100, "end_pos": 115, "type": "METRIC", "confidence": 0.977753072977066}]}, {"text": "Contribution of contextual and non-contextual features.", "labels": [], "entities": []}, {"text": "To assess the contribution of individual information sources, we perform feature ablation, by removing one feature at a time.", "labels": [], "entities": [{"text": "feature ablation", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.6500662267208099}]}, {"text": "The top part of the table shows feature ablation for non-contextual features.", "labels": [], "entities": []}, {"text": "The most useful is the orthographic similarity: its removal results in a drop of almost 10 points.", "labels": [], "entities": [{"text": "similarity", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.8497653007507324}]}, {"text": "Among the contextual features, n-gram support and word2vec prove to be the most useful.", "labels": [], "entities": []}, {"text": "Notably, n-gram features and word2vec supply complementary information, and removing each one of those results in a drop in performance.", "labels": [], "entities": []}, {"text": "Interestingly, the dejavu and dejavuSM features provide almost no improvement; this result contradicts the finding by.", "labels": [], "entities": []}, {"text": "Eliminating all contextual features lowers the performance by more than 10 points, to 77.93%.", "labels": [], "entities": []}, {"text": "This demonstrates that contex-  We evaluate the model on a data set from a very different content domain -clinical medical records.", "labels": [], "entities": []}, {"text": "The genre of clinical free text poses an interesting challenge to the spelling correction task, since it is notoriously noisy (.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.9749736785888672}]}, {"text": "Clinical corpora typically contain higher spelling error rates of 7% to 10%, while in native English text error rates usually range between 0.1% and 0.4% (.", "labels": [], "entities": [{"text": "spelling error rates", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.8943178256352743}]}, {"text": "Clinical text contains domain-specific terminology and language conventions.", "labels": [], "entities": []}, {"text": "Clinical data, in addition to highly domain-specific vocabulary, can also be characterized by a large amount of noise, e.g. the use of non-standard phrases and abbreviations and is thus particularly challenging ().", "labels": [], "entities": []}, {"text": "These properties can render traditional spell checkers less effective.", "labels": [], "entities": [{"text": "spell checkers", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.7527201175689697}]}, {"text": "We use a data set of clinical notes extracted from the large MIMIC-III medical corpus.", "labels": [], "entities": [{"text": "MIMIC-III medical corpus", "start_pos": 61, "end_pos": 85, "type": "DATASET", "confidence": 0.8642444809277853}]}, {"text": "The data set contains 873 manually annotated misspellings ().", "labels": [], "entities": []}, {"text": "The distribution of errors in this data set in terms of the edit distance is very similar to that in TOEFL-Spell (see).", "labels": [], "entities": [{"text": "TOEFL-Spell", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.862520694732666}]}, {"text": "In particular, 83% of errors have edit distance of 1 to the correction, while another 15% have an edit distance of 2.", "labels": [], "entities": [{"text": "edit distance", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9828373491764069}, {"text": "correction", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9786586761474609}, {"text": "edit distance", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.972109317779541}]}, {"text": "The state-of-the-art results on this data set are reported by.", "labels": [], "entities": []}, {"text": "Their model is tuned on artificially generated spelling errors and trained on word and character embeddings from MIMIC-III (note that MIMIC-III is the superset of the annotated clinical data set).", "labels": [], "entities": [{"text": "MIMIC-III", "start_pos": 113, "end_pos": 122, "type": "DATASET", "confidence": 0.8546877503395081}, {"text": "annotated clinical data set", "start_pos": 167, "end_pos": 194, "type": "DATASET", "confidence": 0.7384899258613586}]}, {"text": "Their model outperforms off-the-shelf spelling correction tools (Aspell) and the noisy channel model.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.9241867065429688}]}, {"text": "Similarly to (), we accommodate to the medical domain by enhancing the dictionary with a comprehensive medical lexicon (the", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of errors by edit distance to cor- rect form, in TOEFL-Spell.", "labels": [], "entities": [{"text": "TOEFL-Spell", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8732592463493347}]}, {"text": " Table 3: Error correction results for the baseline model  and two linear classifiers on the TOEFL-Spell data set.  Classifiers outperform the baseline (p<0.002).", "labels": [], "entities": [{"text": "Error correction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8657814860343933}, {"text": "TOEFL-Spell data set", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.9807713627815247}]}, {"text": " Table 4: Feature ablation performance (error correc- tion accuracy %) on TOEFL-Spell. All models are  trained with the Perceptron algorithm in 5-fold cross- validation. Values marked by * differ significantly from  the value for All features, with p < 0.003.", "labels": [], "entities": [{"text": "error correc- tion accuracy", "start_pos": 40, "end_pos": 67, "type": "METRIC", "confidence": 0.852962076663971}, {"text": "TOEFL-Spell", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.8919899463653564}]}, {"text": " Table 5: Error correction performance (accuracy %) of  the Perceptron classifier trained on different amounts  of data, on TOEFL-Spell in 5-fold cross-validation.", "labels": [], "entities": [{"text": "Error correction", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8837247788906097}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9984626770019531}, {"text": "TOEFL-Spell", "start_pos": 124, "end_pos": 135, "type": "METRIC", "confidence": 0.48786798119544983}]}, {"text": " Table 6: Clinical corpus: Performance (accuracy %)  of the state-of-the-art system that uses in-domain data,  and of the models proposed in this work.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9993452429771423}]}, {"text": " Table 7: Feature ablation performance (accuracy %) on  the clinical data set. All models are trained with the  Perceptron algorithm on TOEFL-Spell data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9979648590087891}, {"text": "clinical data set", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8244198560714722}, {"text": "TOEFL-Spell data", "start_pos": 136, "end_pos": 152, "type": "DATASET", "confidence": 0.9099445641040802}]}]}