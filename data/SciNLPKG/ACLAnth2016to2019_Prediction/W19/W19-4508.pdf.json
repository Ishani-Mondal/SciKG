{"title": [{"text": "Lexicon Guided Attentive Neural Network Model for Argument Mining", "labels": [], "entities": []}], "abstractContent": [{"text": "Identification of argumentative components is an important stage of argument mining.", "labels": [], "entities": [{"text": "Identification of argumentative components", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8708288818597794}, {"text": "argument mining", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8496099412441254}]}, {"text": "Lexicon information is reported as one of the most frequently used features in the argument mining research.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7790289223194122}]}, {"text": "In this paper, we propose a methodology to integrate lexicon information into a neural network model by attention mechanism.", "labels": [], "entities": []}, {"text": "We conduct experiments on the UKP dataset, which is collected from heterogeneous sources and contains several text types, e.g., microblog, Wikipedia, and news.", "labels": [], "entities": [{"text": "UKP dataset", "start_pos": 30, "end_pos": 41, "type": "DATASET", "confidence": 0.9935402572154999}]}, {"text": "We explore lexicons from various application scenarios such as sentiment analysis and emotion detection.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9735259711742401}, {"text": "emotion detection", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7940414547920227}]}, {"text": "We also compare the experimental results of leveraging different lexicons.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argument Mining (AM) is an emerging research area that has drawn more and more attention since around 2010.", "labels": [], "entities": [{"text": "Argument Mining (AM)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9065729260444642}]}, {"text": "Recently, Project Debater from IBM has shown such an AI machine supported by argument mining techniques can do well at arguing.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.727192610502243}]}, {"text": "The task of AM can be divided into a few stages: (1) Extracting argumentative components from large texts, i.e., boundary detection or segmentation; (2) Classifying the extracted components into classes.", "labels": [], "entities": [{"text": "AM", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9826112389564514}, {"text": "boundary detection", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7146531492471695}]}, {"text": "In general, an argumentative component can be categorized into \"Claim\", which usually contains conclusions and stance toward the given topic, or \"Premise\", which contains reasoning or evidence used to support or attack a claim; (3) Predicting the relations between the identified argumentative components, i.e., supporting and attacking.", "labels": [], "entities": [{"text": "Premise", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9871065616607666}]}, {"text": "Some works also consider more complex relations such as recursively support/attack the relations themselves rather than merely build relations between components (.", "labels": [], "entities": []}, {"text": "Argument detection and classification can improve legal reasoning (, policy formulation (, and persuasive writing.", "labels": [], "entities": [{"text": "Argument detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.713552251458168}, {"text": "legal reasoning", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8461293578147888}, {"text": "policy formulation", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.8280028402805328}]}, {"text": "In this paper, we focus on mining argumentative components from a large collection of documents and further classifying them into roles of support/opposition.", "labels": [], "entities": []}, {"text": "Our model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks ().", "labels": [], "entities": []}, {"text": "With the help of the attention mechanism (, RNN can further attend on the key information.", "labels": [], "entities": []}, {"text": "We propose a novel attention mechanism that is guided by argumentative lexicon information.", "labels": [], "entities": []}, {"text": "Lexicon information is reported as one kind of the most frequently used features in argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7809723317623138}]}, {"text": "Previous works on AM have tried to integrate lexical features into the learning models ().", "labels": [], "entities": []}, {"text": "These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity.", "labels": [], "entities": []}, {"text": "That is, it may fail to be used for other domains.", "labels": [], "entities": []}, {"text": "In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (.", "labels": [], "entities": [{"text": "AM", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9543392658233643}, {"text": "sentiment analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.9655858278274536}, {"text": "opinion mining", "start_pos": 127, "end_pos": 141, "type": "TASK", "confidence": 0.8499851524829865}, {"text": "emotion detection", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7751233577728271}]}, {"text": "As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons.", "labels": [], "entities": []}, {"text": "The contribution of this work is two-fold: (1) We propose an attention mechanism to leverage lexicon information.", "labels": [], "entities": []}, {"text": "(2) In the face of the scarcity of argument lexicon, we explore several different types of lexicons to verify whether outside resources are useful for AM tasks.", "labels": [], "entities": [{"text": "AM tasks", "start_pos": 151, "end_pos": 159, "type": "TASK", "confidence": 0.917664110660553}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes related works about AM.", "labels": [], "entities": [{"text": "AM", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.7308414578437805}]}, {"text": "The dataset and linguistic resources used for experiments are shown in Section 3.", "labels": [], "entities": []}, {"text": "We introduce our model in Section 4 and show the experimental results in Section 5.", "labels": [], "entities": []}, {"text": "We also look into the errors made by our best model in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 makes a discussion on experimental results and concludes this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because most of the lengths of input sentences are less than 60 and most of the lengths of additional sentences A are less than 20, we truncate them into lengths of 60 and 20 respectively.", "labels": [], "entities": []}, {"text": "The dataset has 25,492 sentences in total.", "labels": [], "entities": []}, {"text": "We conduct 5-fold cross validation for evaluating our model.", "labels": [], "entities": []}, {"text": "To evaluate our approaches, we report the average macro F 1 as ternary setting, precision and recall of predicting supporting arguments (P arg+ , R arg+ ), and precision and recall of predicting opposing arguments (P arg\u2212 , R arg\u2212 ).", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9994456171989441}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9907653331756592}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9992756247520447}, {"text": "recall", "start_pos": 174, "end_pos": 180, "type": "METRIC", "confidence": 0.9950248599052429}]}, {"text": "We run paired t-test for each proposed model in comparison with the baseline model, and mark the models having statistical significance (i.e. p-value < 0.05) with a wildcard.", "labels": [], "entities": []}, {"text": "As the result shown in, we can observe that the proposed models benefit from the information from the adopted lexicons, improving the performance of argumentative components identification.", "labels": [], "entities": [{"text": "argumentative components identification", "start_pos": 149, "end_pos": 188, "type": "TASK", "confidence": 0.639548530181249}]}, {"text": "The best model, which uses WordNet to expand topic T , outperforms the baseline model by 4.5 percentage in F 1 . The proposed model with the lowest F 1 score (i.e. ClaimLex) still outperforms the baseline by 3.4 percentage.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9273711442947388}, {"text": "F 1", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.96857950091362}, {"text": "F 1 score", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9580022891362509}]}, {"text": "Furthermore, the best performance reported by on the same dataset is 0.4285 in macro F 1 , which is the result of only incorporating topic information into their models.", "labels": [], "entities": [{"text": "macro F 1", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.6698663036028544}]}, {"text": "This shows the impact of the lexicon information.", "labels": [], "entities": []}, {"text": "However, we can also observe that the result of integrating claim lexicon () is out of our expectation though it is a resource for argument mining.", "labels": [], "entities": [{"text": "argument mining", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7752355337142944}]}, {"text": "Possible reasons are figured out as follows.", "labels": [], "entities": []}, {"text": "Firstly, the lexicon is built based on a strong assumption, i.e., the present of the term \"that\" indicates a high probability of the occurrence of argumentative components.", "labels": [], "entities": []}, {"text": "Secondly, the lexicon has only 586 words, indicating a very small coverage with the whole vocabulary.", "labels": [], "entities": []}, {"text": "Thirdly, the lexicon built from the sentences across 100 different topics contains a number of domainspecific words such as \"LGBTQ\" and \"militarily\".", "labels": [], "entities": []}, {"text": "Highlighting of these domain-specific words may cause noise when the topic is unrelated to them.: The examples that our best model fails to correctly predict.", "labels": [], "entities": []}, {"text": "The sentences predicted/annotated as nonargumentative ones are abbreviated to non-arg.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of the baseline model and the proposed model with different lexicon resources. The highest  score of each column is highlighted in bold font.", "labels": [], "entities": []}]}