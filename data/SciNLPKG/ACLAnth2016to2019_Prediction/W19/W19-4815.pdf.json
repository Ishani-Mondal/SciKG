{"title": [{"text": "Learning the Dyck Language with Attention-based Seq2Seq Models", "labels": [], "entities": []}], "abstractContent": [{"text": "The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs).", "labels": [], "entities": []}, {"text": "Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion.", "labels": [], "entities": []}, {"text": "In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses.", "labels": [], "entities": []}, {"text": "Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language , which could compensate for the limited encoding ability of RNNs.", "labels": [], "entities": []}, {"text": "Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task.", "labels": [], "entities": [{"text": "closing bracket tagging task", "start_pos": 161, "end_pos": 189, "type": "TASK", "confidence": 0.7264120429754257}]}, {"text": "Moreover, this also suggests that this commonly used task is not sufficient to test a model's understanding of CFGs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The generalized Dyck language has been a testbed for several research on the ability of Recurrent Neural Networks (RNNs), in particular the Long Short-term Memory model (LSTM), to learn contextfree grammars.", "labels": [], "entities": []}, {"text": "It consists of strings with balanced pairs of brackets of different types, e.g., \"[ < > ] [ ] < [ ] >\".", "labels": [], "entities": []}, {"text": "Recognizing the generalized Dyck language is considered to be more difficult than an b n as tested in, since it cannot be simply solved by counting.", "labels": [], "entities": []}, {"text": "Rather, the model has to remember the sequence of different (unclosed) brackets.", "labels": [], "entities": []}, {"text": "Among the recent studies, analyze the generalizability of LSTMs to learn the generalized Dyck language with two pairs of brackets, and conclude that the model cannot learn the underlying grammar rules.", "labels": [], "entities": []}, {"text": "In contrast, concludes that the LSTM can model the language quite well.", "labels": [], "entities": []}, {"text": "experiment with several variants of RNNs, and find that the LSTM works reasonably well on the given task, but fails to generalize to cases with deeper recursion.", "labels": [], "entities": []}, {"text": "All the aforementioned work explores the ability to \"understand\" context-free languages with some tagging task similar to language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.7240386009216309}]}, {"text": "In these tasks, the RNN encodes a sequence, which is the prefix of a valid sequence in the language, and predicts the next possible token either for the last token or for every token.", "labels": [], "entities": []}, {"text": "These probing tasks have one thing in common, namely they predict only one token, which does not necessarily close the whole sequence, and thus are not sufficient to prove that the model learns the whole sequence.", "labels": [], "entities": []}, {"text": "In this work, we show that a seq2seq model with attention mechanism not only solves the tagging task, but also generalizes well over unseen depths.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 88, "end_pos": 100, "type": "TASK", "confidence": 0.9046206772327423}]}, {"text": "While it appears to have \"understood\" the Dyck language, under closer inspection, it fails to complete deeper sequences and only closes the first several brackets correctly, which happens to be the evaluation metric of the tagging task.", "labels": [], "entities": [{"text": "tagging task", "start_pos": 223, "end_pos": 235, "type": "TASK", "confidence": 0.9129182994365692}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average accuracy of the models in differ- ent evaluations in the in-domain setting. In the three  columns, target measures the accuracy of the main tar- get, all-tag measures the average accuracy of predict- ing the next bracket for all tokens, completion mea- sures the exact match of closing all the brackets. All  results are averaged from 10 runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9859945774078369}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9976431727409363}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9825373291969299}, {"text": "completion mea- sures", "start_pos": 255, "end_pos": 276, "type": "METRIC", "confidence": 0.9088966250419617}]}, {"text": " Table 1. All the models are  evaluated on the main target of the same test set  (the first column), and the tagger models are addi- tionally evaluated on predicting the target for ev- ery token in the sequence (the second column).", "labels": [], "entities": []}]}