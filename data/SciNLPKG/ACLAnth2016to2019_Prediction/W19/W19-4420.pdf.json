{"title": [{"text": "Noisy Channel for Low Resource Grammatical Error Correction", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our contribution to the low-resource track of the BEA 2019 shared task on Grammatical Error Correction (GEC).", "labels": [], "entities": [{"text": "BEA 2019 shared task on Grammatical Error Correction (GEC)", "start_pos": 71, "end_pos": 129, "type": "TASK", "confidence": 0.6948248689824884}]}, {"text": "Our approach to GEC builds on the theory of the noisy channel by combining a channel model and language model.", "labels": [], "entities": [{"text": "GEC", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.6730705499649048}]}, {"text": "We generate confusion sets from the Wikipedia edit history and use the frequencies of edits to estimate the channel model.", "labels": [], "entities": [{"text": "Wikipedia edit history", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.8749348719914755}]}, {"text": "Additionally, we use two pre-trained language models: 1) Google's BERT model, which we fine-tune for specific error types and 2) OpenAI's GPT-2 model, utilizing that it can operate with previous sentences as context.", "labels": [], "entities": [{"text": "BERT", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9908425807952881}]}, {"text": "Furthermore, we search for the optimal combinations of corrections using beam search.", "labels": [], "entities": []}], "introductionContent": [{"text": "Grammatical Error Correction Grammatical Error Correction (GEC) is the task of automatically correcting grammatical errors in written text.", "labels": [], "entities": [{"text": "Grammatical Error Correction Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.6453493105040656}, {"text": "automatically correcting grammatical errors in written text", "start_pos": 79, "end_pos": 138, "type": "TASK", "confidence": 0.738885509116309}]}, {"text": "The task is relevant for users producing text through text interfaces, both as assistance during the writing process and for proofreading already written work.", "labels": [], "entities": []}, {"text": "In recent years, GEC has received increasing attention in the research community with several shared tasks on the topic, such as CoNLL 13-14 (), HOO (, and AESW, and most recently the BEA 2019 shared task on GEC (, which this work is a contribution to.", "labels": [], "entities": [{"text": "GEC", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.69392329454422}, {"text": "CoNLL 13-14", "start_pos": 129, "end_pos": 140, "type": "DATASET", "confidence": 0.7763046026229858}, {"text": "HOO", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.8747488856315613}, {"text": "AESW", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9073163866996765}, {"text": "BEA 2019 shared task", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.7161443531513214}, {"text": "GEC", "start_pos": 208, "end_pos": 211, "type": "DATASET", "confidence": 0.4477445185184479}]}, {"text": "Supervised GEC Current state-of-the-art approaches to GEC use a supervised machine translation setup, that relies on large amounts of annotated learner data.", "labels": [], "entities": [{"text": "GEC", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.90829998254776}, {"text": "GEC", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.7808015942573547}, {"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7416968941688538}]}, {"text": "This means that systems do not generalize well to non-learner domains and that these approaches do notwork well for low-resource languages.", "labels": [], "entities": []}, {"text": "As most existing datasets are not freely available for commercial use, the supervised approach also limits industrial uses.", "labels": [], "entities": []}, {"text": "Unsupervised GEC In order to combat these problems, in recent years several approaches to GEC have used the concept of language modeling, which allows for training GEC systems without supervised data, and have so far given promising results.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7390072643756866}]}, {"text": "uses a 5-gram language model while uses a bidirectional LSTM-based language model.", "labels": [], "entities": []}, {"text": "fine-tunes LSTM-based language models for specific error types.", "labels": [], "entities": []}, {"text": "Using a language modeling approach means that we can create models that are trained unsupervised by only being based on high quality native text corpora.", "labels": [], "entities": []}, {"text": "This means that our systems will only require a small amount of labeled data for tuning purposes.", "labels": [], "entities": []}, {"text": "We can therefore build GEC systems for any language given enough native text.", "labels": [], "entities": []}, {"text": "The Noisy Channel The core idea that these language modeling approaches are using for GEC is that low probability sequences are more likely to contain grammatical errors than high probability sequences.", "labels": [], "entities": [{"text": "GEC", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.9438636302947998}]}, {"text": "However this formulation does not take into account the writer's likelihood of making particular errors.", "labels": [], "entities": []}, {"text": "For example, \"then\" \u2192 \"than\" is much more common than \"then\" \u2192 \"the\" due to an underlying similarity in phonetics.", "labels": [], "entities": []}, {"text": "In order to take this into account we utilize the concept of the noisy channel model, which allows for modeling the users likelihood of making particular errors, instead of only relying on which sequences of words are more probable.", "labels": [], "entities": []}, {"text": "Contributions In the following, we present our low-resource approach to GEC, which ranked as the 6th best performing system in the low-resource track of the BEA 2019 shared task.", "labels": [], "entities": [{"text": "GEC", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.9730607867240906}, {"text": "BEA 2019 shared task", "start_pos": 157, "end_pos": 177, "type": "DATASET", "confidence": 0.8424274921417236}]}, {"text": "We utilize confusion sets and edit statistics gathered from the Wikipedia edit history, as well as unsupervised language models in a noisy channel setting.", "labels": [], "entities": [{"text": "Wikipedia edit history", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.8237294952074686}]}, {"text": "Our contributions are 1) formalizing GEC in the noisy channel framework, 2) generating confusion sets from the Wikipedia edit history, 3) estimating a channel model based on frequencies of edits from the confusion sets, 4) combining existing pre-trained language models, with each their own strength, 5) specializing models for specific grammatical error types, and 6) using beam search to find the optimal combination of corrections.", "labels": [], "entities": [{"text": "GEC", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.761601984500885}, {"text": "Wikipedia edit history", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.794989824295044}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Span-level correction results of our system. We  do not show results for the error types we do not pre- dict.", "labels": [], "entities": []}, {"text": " Table 2: Span-level correction results of the ablated  models.", "labels": [], "entities": []}]}