{"title": [{"text": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the first complete attempt at concurrently training conversational agents that communicate only via self-generated language.", "labels": [], "entities": []}, {"text": "Using DSTC2 as seed data, we trained natural language understanding (NLU) and generation (NLG) networks for each agent and let the agents interact online.", "labels": [], "entities": [{"text": "natural language understanding (NLU) and generation (NLG)", "start_pos": 37, "end_pos": 94, "type": "TASK", "confidence": 0.7745466286485846}]}, {"text": "We model the interaction as a stochastic collaborative game where each agent (player) has a role (\"assis-tant\", \"tourist\", \"eater\", etc.) and their own objectives, and can only interact via natural language they generate.", "labels": [], "entities": []}, {"text": "Each agent, therefore, needs to learn to operate optimally in an environment with multiple sources of uncertainty (its own NLU and NLG, the other agent's NLU, Policy, and NLG).", "labels": [], "entities": []}, {"text": "In our evaluation, we show that the stochastic-game agents out-perform deep learning based supervised base-lines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning for conversational agents has seen great advances (e.g., especially when adopting deep learning models).", "labels": [], "entities": [{"text": "Machine learning", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7573646306991577}]}, {"text": "Most of these works, however, suffer from the lack of data availability as it is very challenging to design sample-efficient learning algorithms for problems as complex as training agents capable of meaningful conversations.", "labels": [], "entities": []}, {"text": "Among other simplifications, this results in treating the interaction as a singleagent learning problem, i.e. assuming that from the conversational agent's perspective the world maybe complex but is stationary.", "labels": [], "entities": []}, {"text": "In this work, we model conversational interaction as a stochastic game (e.g.) and train two conversational agents, each with a different role, which learn by interacting with each other via natural language.", "labels": [], "entities": []}, {"text": "We first train Language Understanding (NLU) and Generation (NLG) neural networks for each agent and then use multi-agent reinforcement learning, namely the Win or Lose Fast Policy Hill Climbing (WoLF-PHC) algorithm), to learn optimal dialogue policies in the presence of high levels of uncertainty that originate from each agent's statistical NLU and NLG, and the other agent's erratic behaviour (as the other agent is learning at the same time).", "labels": [], "entities": []}, {"text": "While not completely alleviating the need for seed data needed to train the NLU and NLG components, the multi-agent setup has the effect of augmenting them, allowing us to generate dialogues and behaviours not present in the original data.", "labels": [], "entities": []}, {"text": "Employing a user simulator is an established method for dialogue policy learning ( and end-to-end dialogue training.", "labels": [], "entities": [{"text": "dialogue policy learning", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8971644441286722}]}, {"text": "Training two conversational agents concurrently has been proposed by; training them via natural language communication was partially realized by, as they train agents that receive text input but generate dialogue acts.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge, this is the first study that allows fully-trained agents to communicate only in natural language, and does not allow any allseeing critic / discriminator.", "labels": [], "entities": []}, {"text": "Inspired by, each agent learns in a decentralized setting, only observing the other agent's language output and a reward signal.", "labels": [], "entities": []}, {"text": "This allows new, untrained agents to directly interact with trained agents and learn without the need for adjusting parameters that can affect the already trained agents.", "labels": [], "entities": []}, {"text": "The architecture of each agent is mirrored as shown in, so the effort of adding agents with new roles is minimal.", "labels": [], "entities": []}, {"text": "As seed data, we use data from DSTC2 (, which concerns dialogues between humans asking for restaurant information and a machine providing such information.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9192238450050354}]}, {"text": "Our contributions are: 1) we propose a method for training fully textto-text conversational agents from mutually generated data; and 2) we show how agents trained by multi-agent reinforcement learning and minimal seed human-machine data can produce high quality dialogues as compared to single-agent policy models in an empirical evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Plato Research Dialogue System 1 was used to implement, train, and evaluate the agents.", "labels": [], "entities": [{"text": "Plato Research Dialogue System 1", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.9399257063865661}]}, {"text": "To assess the quality of the dialogues our agents are capable of, we compare dialogue success rates, average cumulative rewards, and average dialogue turns along two dimensions: a) access to ground truth labels during training or not; b) stationary or non-stationary environment during training.", "labels": [], "entities": []}, {"text": "We therefore train four kinds of conversational agents for each role (eight in total) as shown in.", "labels": [], "entities": []}, {"text": "Due to the nature of our setup, algorithms designed for stationary environments (e.g. DQN) are not considered.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A failed dialogue between two conversational agents during training. Uncertainty originating from NLU  and NLG components on top of the erratic behaviour of each agent's policy (as they learn concurrently) can have  a big impact on the quality of the learned dialogue policies.", "labels": [], "entities": [{"text": "NLU", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.9352126717567444}]}, {"text": " Table 2: F1 scores for each agent's NLU model.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9732909798622131}]}, {"text": " Table 4: Average dialogue success, reward, and number of turns on the agents evaluated, over 3 training/evaluation  cycles with goals sampled from the test set of DSTC2. Regardless of training condition, all agents were evaluated  in the language to language setting. All differences between SuperDAct -WoLF-DAct, and Supervised -WoLF- PHC are significant with p < 0.02.", "labels": [], "entities": [{"text": "DSTC2", "start_pos": 164, "end_pos": 169, "type": "DATASET", "confidence": 0.9628430008888245}]}]}