{"title": [], "abstractContent": [{"text": "Automatic post-editing (APE) can be reduced to a machine translation (MT) task, where the source is the output of a specific MT system and the target is its post-edited variant.", "labels": [], "entities": [{"text": "Automatic post-editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6154236972332001}, {"text": "machine translation (MT) task", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8498839934666952}]}, {"text": "However , this approach does not consider context information that can be found in the original source of the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9724963903427124}]}, {"text": "Thus a better approach is to employ multi-source MT, where two input sequences are considered-the original source and the MT output.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8977015614509583}]}, {"text": "Extra context information can be introduced in the form of extra tokens that identify certain global properties of a group of segments, added as a prefix or a suffix to each segment.", "labels": [], "entities": []}, {"text": "Successfully applied in domain adaptation of MT as well as on APE, this technique deserves further attention.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7443177402019501}, {"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.8234766721725464}, {"text": "APE", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.6582612991333008}]}, {"text": "In this work we investigate multi-source neural APE (or NPE) systems with training data which has been augmented with two types of extra context tokens.", "labels": [], "entities": [{"text": "multi-source neural APE (or NPE)", "start_pos": 28, "end_pos": 60, "type": "TASK", "confidence": 0.7367742998259408}]}, {"text": "We experiment with authentic and synthetic data provided by WMT 2019 and submit our results to the APE shared task.", "labels": [], "entities": [{"text": "WMT 2019", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9117259383201599}, {"text": "APE shared task", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.6458485722541809}]}, {"text": "We also experiment with using statistical machine translation (SMT) methods for APE.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 30, "end_pos": 67, "type": "TASK", "confidence": 0.7703247964382172}, {"text": "APE", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.8622872233390808}]}, {"text": "While our systems score bellow the baseline, we consider this work a step towards understanding the added value of extra context in the case of APE.", "labels": [], "entities": [{"text": "APE", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.7018769979476929}]}], "introductionContent": [{"text": "Automatic post-editing (APE) aims at improving text that was previously translated by Machine Translation (MT).", "labels": [], "entities": [{"text": "Automatic post-editing (APE)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5856666624546051}, {"text": "Machine Translation (MT)", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.8457852840423584}]}, {"text": "An APE system is typically trained on triplets composed of: a segment in the source language, a translation hypothesis of that segment by an MT system, and the edited version of that hypothesis, created by a human translator.", "labels": [], "entities": []}, {"text": "Currently, neural machine translation (NMT) systems are the state-of-the-art in MT, achieving quality beyond that of phrase-based statistical MT (SMT) ().", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8192511002222697}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9927214980125427}, {"text": "phrase-based statistical MT (SMT)", "start_pos": 117, "end_pos": 150, "type": "TASK", "confidence": 0.7169944196939468}]}, {"text": "NMT output is more fluent but may contain issues related to accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9973268508911133}]}, {"text": "However, automatic post-editing of NMT output has proved to be a challenging task ( . In terms of post-editing technology, neural methods as well represent the current state-ofthe-art (do.", "labels": [], "entities": []}, {"text": "And while neural post-editing (NPE) has shown substantial improvements when applied on PBSMT output, it has not been as effective in improving output from NMT systems.", "labels": [], "entities": []}, {"text": "One of the reasons is that NMT and NPE typically use similar approaches, which can make the latter redundant, as it can be assimilated by the former, e.g., in some cases, by increasing the number of layers of the network.", "labels": [], "entities": []}, {"text": "One alternative is to explore features of the data not available while training MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9784504771232605}]}, {"text": "In this paper, we explore the effect of adding tokens that identify partitions in the training data which maybe relevant to guide the behaviour of the NPE system.", "labels": [], "entities": []}, {"text": "Examples of such tokens are related to basic source and/or target sentence length or to more sophisticated analyses of the text.", "labels": [], "entities": []}, {"text": "In this work, we explore two features: sentence length and topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our models using BLEU () and TER ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9990468621253967}, {"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9980592131614685}]}, {"text": "For the former, we used the multi-bleu implementation provided alongside Moses; and for the latter we used the script provided by the WMT organisation.", "labels": [], "entities": [{"text": "WMT organisation", "start_pos": 134, "end_pos": 150, "type": "DATASET", "confidence": 0.9423630833625793}]}, {"text": "We computed BLEU and TER using the human PE side of the data as reference, and the NPE output as hypothesis, e.g. TER(npe,pe).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9984387755393982}, {"text": "TER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9915379285812378}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9849362969398499}]}, {"text": "We also computed BLEU and TER scores for the original data, i.e. in this case the reference again is the human PE but the hypothesis is the NMT part of the training data: TER(nmt,pe).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9988981485366821}, {"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9964474439620972}, {"text": "TER", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9949398040771484}]}, {"text": "We present our results on the development set in for EN-DE and EN-RU, respectively.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.8841091394424438}, {"text": "EN-RU", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.8407324552536011}]}, {"text": "We denote the scores for the original (baseline) MT output with MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.881589412689209}, {"text": "MT", "start_pos": 64, "end_pos": 66, "type": "DATASET", "confidence": 0.5519023537635803}]}, {"text": "Scores are scaled between 0 and 100.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of SRC-NMT-PE triplets distributed  over three data sets used in our experiments.", "labels": [], "entities": []}, {"text": " Table 2: Vocabulary sizes (after applying BPE on the train data set).", "labels": [], "entities": [{"text": "BPE", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9935651421546936}, {"text": "train data set", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.8273408611615499}]}, {"text": " Table 3: BLEU and TER scores for the EN-DE NPE  and SMT models (dev set). Rows in bold indicate sub- mitted system results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993744492530823}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9979637861251831}, {"text": "EN-DE NPE", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.7333762645721436}, {"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.5645480155944824}]}, {"text": " Table 4: BLEU and TER scores for the EN-RU NPE  and SMT models (dev set). Rows in bold indicate sub- mitted system results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992221593856812}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9934178590774536}, {"text": "EN-RU NPE", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.7810759246349335}, {"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.6773607730865479}]}, {"text": " Table 6: BLEU and TER scores for submitted and base- line systems for the EN-RU language pair.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991903901100159}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9943835735321045}]}]}