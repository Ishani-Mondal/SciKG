{"title": [{"text": "WriterForcing: Generating more interesting story endings", "labels": [], "entities": [{"text": "WriterForcing", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8290057182312012}]}], "abstractContent": [{"text": "We study the problem of generating interesting endings for stories.", "labels": [], "entities": []}, {"text": "Neural genera-tive models have shown promising results for various text generation problems.", "labels": [], "entities": [{"text": "text generation", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.7559622228145599}]}, {"text": "Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence fora given input sequence.", "labels": [], "entities": []}, {"text": "However , in the context of a story, multiple endings are possible.", "labels": [], "entities": []}, {"text": "Seq2Seq models tend to ignore the context and generate generic and dull responses.", "labels": [], "entities": []}, {"text": "Very few works have studied generating diverse and interesting story endings fora given story context.", "labels": [], "entities": []}, {"text": "In this paper, we propose models which generate more diverse and interesting outputs by 1) training models to focus attention on important keyphrases of the story, and 2) promoting generation of non-generic words.", "labels": [], "entities": []}, {"text": "We show that the combination of the two leads to more diverse and interesting endings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Story ending generation is the task of generating an ending sentence of a story given a story context.", "labels": [], "entities": [{"text": "Story ending generation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7236903111139933}]}, {"text": "A story context is a sequence of sentences connecting characters and events.", "labels": [], "entities": []}, {"text": "This task is challenging as it requires modelling the characters, events and objects in the context, and then generating a coherent and sensible ending based on them.", "labels": [], "entities": []}, {"text": "Generalizing the semantics of the events and entities and their relationships across stories is a non-trivial task.", "labels": [], "entities": [{"text": "Generalizing the semantics of the events and entities", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8599229604005814}]}, {"text": "Even harder challenge is to generate stories which are nontrivial and interesting.", "labels": [], "entities": []}, {"text": "In this work, we focus on the story ending generation task, where given a story context -a sequence of sentences from a story, the model has to generate the last sentence of the story.", "labels": [], "entities": [{"text": "story ending generation", "start_pos": 30, "end_pos": 53, "type": "TASK", "confidence": 0.7902306119600931}]}], "datasetContent": [{"text": "We used the ROCStories () corpus to generate our story endings.", "labels": [], "entities": [{"text": "ROCStories () corpus", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.7342681884765625}]}, {"text": "Each story in the dataset comprises of five sentences.", "labels": [], "entities": []}, {"text": "The input is the first four sentences of the story and output is the last sentence of the story.", "labels": [], "entities": []}, {"text": "The number of stories which were used to train and test the model are shown in.", "labels": [], "entities": []}, {"text": "All our models use the same hyper-parameters.", "labels": [], "entities": []}, {"text": "We used a two layer encoder-decoder architecture with 512 GRU hidden units.", "labels": [], "entities": []}, {"text": "We train our models using Adam optimizer with a learning rate of 0.001.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9614914357662201}]}, {"text": "For the Keyphrase Attention Loss model we assign the weight of 0.9 to Keyphrase loss and 0.1 to reconstruction loss.", "labels": [], "entities": [{"text": "Keyphrase Attention Loss", "start_pos": 8, "end_pos": 32, "type": "TASK", "confidence": 0.724393626054128}, {"text": "reconstruction loss", "start_pos": 96, "end_pos": 115, "type": "METRIC", "confidence": 0.9636363983154297}]}, {"text": "We use the best win percent from our Story-Cloze metric (described in the next section) for model selection.", "labels": [], "entities": []}, {"text": "For ITF loss we use the hyperparameters mention in the original paper.", "labels": [], "entities": []}, {"text": "In this section, we briefly describe the various metrics which were used to test our models.", "labels": [], "entities": []}, {"text": "We did not use perplexity or BLEU as evaluation metric, as neither of them is likely to bean effective evaluation metric in our setting.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9989533424377441}]}, {"text": "This is since both these metrics measure performance based on a single reference story ending present in the test dataset, however there can be multiple valid possible story endings fora story.", "labels": [], "entities": []}, {"text": "Therefore, we DIST (Distinct): Distinct-1,2,3 calculates numbers of distinct unigrams, bigrams and trigrams in the generated responses divided by the total numbers of unigrams, bigrams and trigrams.", "labels": [], "entities": [{"text": "DIST", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.8563768863677979}]}, {"text": "We denote the metrics as DIST-1,2,3 in the result tables.", "labels": [], "entities": [{"text": "DIST-1,2,3", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.7910119295120239}]}, {"text": "Higher Distinct scores indicate higher diversity in generated outputs.", "labels": [], "entities": [{"text": "Distinct", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.9973353743553162}]}, {"text": "Story-Cloze: Since it is difficult to do human evaluation on all the stories, we use the StoryCloze task to create a metric in order to pick our best model and also to evaluate the efficacy of our model against Seq2Seq and its variants.", "labels": [], "entities": []}, {"text": "This new proposed metric measures the semantic relevance of the generated ending with respect to the context.", "labels": [], "entities": []}, {"text": "In the Story-Cloze task, given two endings to a story the task is to pick the correct ending.", "labels": [], "entities": []}, {"text": "We can use this task to identify the better of two endings.", "labels": [], "entities": []}, {"text": "In order to do so, we fine-tune BERT) to identify the true ending between two story candidates.", "labels": [], "entities": [{"text": "BERT", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9990062117576599}]}, {"text": "The dataset for this task was obtained using the Story-Cloze task.", "labels": [], "entities": []}, {"text": "Positive examples to BERT are obtained from the StoryCloze dataset while the negative examples are obtained by randomly sampling from other story endings to get false ending for the story.", "labels": [], "entities": [{"text": "BERT", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9970267415046692}, {"text": "StoryCloze dataset", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.9918398559093475}]}, {"text": "We fine tune BERT in the two sentence setting by providing the context as the first sentence and the final sentence as the second.", "labels": [], "entities": [{"text": "BERT", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9975211024284363}]}, {"text": "We pick the ending with a greater probability (from BERT's output head) of being a true ending as the winner.", "labels": [], "entities": [{"text": "BERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9289674758911133}]}, {"text": "With this approach we were able to get a Story-Cloze test accuracy of 72%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.8707602024078369}]}, {"text": "We now use this pre-trained model to compare the IE + GA model with our models.", "labels": [], "entities": [{"text": "IE + GA model", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.5188021063804626}]}, {"text": "We select the winner based on the probability given by the pre-trained Bert model.", "labels": [], "entities": []}, {"text": "Since automatic metrics are notable to capture all qualitative aspects of the models, we performed a human evaluation study to compare our models.", "labels": [], "entities": []}, {"text": "We first randomly selected 50 story contexts from the test set, and show them to three annotators.", "labels": [], "entities": []}, {"text": "The annotators seethe story context, and the story endings generated by our best model and the baseline IE+GA model in a random order.", "labels": [], "entities": [{"text": "IE+GA", "start_pos": 104, "end_pos": 109, "type": "METRIC", "confidence": 0.7939042051633199}]}, {"text": "They are asked to select a better ending among the two based on three criteria -1) Relevance -Story ending should be appropriate and reasonable according to the story context.", "labels": [], "entities": [{"text": "Relevance", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9779443144798279}]}, {"text": "2) Interestingness -More interesting story ending should be preferred 3) Fluency -Endings should be natural english and free of errors.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9883157014846802}]}, {"text": "We found that both models were preferred 50% of the time, that is, both model was picked for 25 stories each.", "labels": [], "entities": []}, {"text": "From a manual analysis of human evaluation, we found that our model was selected over the baseline in many cases for generating interesting endings, but was also equivalently penalized for losing the relevance in some of the story endings.", "labels": [], "entities": []}, {"text": "We discuss this aspect in more detail in section 5.7.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Model comparison based on automatic met- rics DIST-1, DIST-2 and DIST-3.", "labels": [], "entities": [{"text": "DIST-3", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.7772340774536133}]}, {"text": " Table 4: We measure the performance of the mod- els using an automated Story-Cloze classifier which  compares the outputs of model with the outputs of  IE model.", "labels": [], "entities": []}, {"text": " Table 5: Results for automatic metrics with varying  number of keyphrases. Diversity is measured using  DIST-1, DIST-2 and DIST-3 metrics. Story-Cloze loss  measures relevance in comparison to IE model.", "labels": [], "entities": [{"text": "relevance", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9563999772071838}]}]}