{"title": [{"text": "Anonymized BERT: An Augmentation Approach to the Gendered Pronoun Resolution Challenge", "labels": [], "entities": [{"text": "BERT", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9164894819259644}, {"text": "Gendered Pronoun Resolution", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6914495825767517}]}], "abstractContent": [{"text": "We present our 7th place solution 1 to the Gen-dered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks.", "labels": [], "entities": [{"text": "Gen-dered Pronoun Resolution challenge", "start_pos": 43, "end_pos": 81, "type": "TASK", "confidence": 0.7455405592918396}, {"text": "BERT", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9962463974952698}]}, {"text": "Our method anonymizes the referent by replacing candidate names with a set of common place-holder names.", "labels": [], "entities": []}, {"text": "Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names.", "labels": [], "entities": []}, {"text": "Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names.", "labels": [], "entities": []}, {"text": "The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04.", "labels": [], "entities": []}, {"text": "Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place.", "labels": [], "entities": []}], "introductionContent": [{"text": "Gender bias has been an important topic in natural language processing in recent years.", "labels": [], "entities": [{"text": "Gender bias", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.743011862039566}, {"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6447152892748514}]}, {"text": "GAP (Gendered Ambiguous Pronouns) dataset is a gender balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled from English Wikipedia, built and released by to challenge the community for gender unbiased pronoun resolution systems.", "labels": [], "entities": [{"text": "GAP (Gendered Ambiguous Pronouns) dataset", "start_pos": 0, "end_pos": 41, "type": "DATASET", "confidence": 0.6635344198771885}, {"text": "gender unbiased pronoun resolution", "start_pos": 201, "end_pos": 235, "type": "TASK", "confidence": 0.7449061125516891}]}, {"text": "In the Gendered Pronoun Resolution challenge which is based on GAP dataset, we designed a unique augmentation strategy for token-level contextual embedding models and applied it to feature based BERT () approach fora 7th place finish.", "labels": [], "entities": [{"text": "Gendered Pronoun Resolution", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.7472804387410482}, {"text": "GAP dataset", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.9105992615222931}, {"text": "BERT", "start_pos": 195, "end_pos": 199, "type": "METRIC", "confidence": 0.9853375554084778}]}, {"text": "BERT is a large bidirectional transformer trained with masked language The code is available at https://github.com/boliu61/gendered-pronoun-resolution model, which is fine-tuned to state-of-the-art results on a variety of NLP benchmark tasks.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9616925120353699}]}, {"text": "Four version of BERT model weights were released in October 2018, following a family of NLP transfer learning models in the same year,, and OpenAI GPT (.", "labels": [], "entities": [{"text": "BERT", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9765787124633789}, {"text": "OpenAI GPT", "start_pos": 140, "end_pos": 150, "type": "DATASET", "confidence": 0.873805820941925}]}, {"text": "Although augmentation has been shown to be very effective in deep learning (), most NLP augmentation methods are on document or sentence level, such as synonym replacement (, data noising ( and back-translation ().", "labels": [], "entities": [{"text": "synonym replacement", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8302121162414551}, {"text": "data noising", "start_pos": 175, "end_pos": 187, "type": "TASK", "confidence": 0.6865952759981155}]}, {"text": "For token level tasks like pronoun resolution, only the name and pronoun embeddings are in the model input.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8459762334823608}]}, {"text": "Even though altering whole document also affect these embeddings, direct change to the names has much bigger impact to the model.", "labels": [], "entities": []}, {"text": "The main idea of our augmentation is to replace each name in the name-pronoun pair by a set of common placeholder names, in order to (1) diversify the idiosyncratic information embedded in individual names and leave only the contextual information and (2) remove any gender or region related bias in names.", "labels": [], "entities": []}, {"text": "In other words, to anonymize the names and make BERT extract name-independent features purely about context.", "labels": [], "entities": [{"text": "BERT", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.6249173283576965}]}, {"text": "With the same set of common first names from the training corpus as the placeholders, the model can recognize candidate names more easily and embed contextual information more compactly into single tokens.", "labels": [], "entities": []}, {"text": "This technique could also be used in other token level tasks to anonymize people or entity names.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the official GAP dataset to build the system.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 21, "end_pos": 32, "type": "DATASET", "confidence": 0.8743022382259369}]}, {"text": "There are 2000 data in both test and development sets and 454 in validation set.", "labels": [], "entities": []}, {"text": "We used all of test and development plus 400 random rows in validation set (4400 in total) to train the system and left 54 as a sanity check to test the inference pipeline.", "labels": [], "entities": []}, {"text": "The gender is nearly equally distributed in the training data with 2195 male and 2205 female examples.", "labels": [], "entities": []}, {"text": "There are 12359 samples in stage 2 test data, but only 760 were revealed to have been labeled and used for scoring.", "labels": [], "entities": []}, {"text": "Effectively, there are 760 stage 2 test data-all the others were presumably added to prevent cheating.", "labels": [], "entities": [{"text": "cheating", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9615347385406494}]}, {"text": "The gender distribution is again almost equal with 383 female and 377 male examples.", "labels": [], "entities": []}, {"text": "The meta information for both End2end and Pure Bert model is shown in.", "labels": [], "entities": [{"text": "End2end", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9205365777015686}, {"text": "Pure Bert model", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.7891409993171692}]}, {"text": "For each model, we trained two versions, one based on BERT Large Uncased, the other based on BERT Large Cased.", "labels": [], "entities": [{"text": "BERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8954325914382935}, {"text": "BERT", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.7334738969802856}]}, {"text": "For the competition, we used layer -4 (fourth to last hidden layer) embeddings for the End2end model and a concatenation of layers -3 and -4 for the Pure BERT model.", "labels": [], "entities": [{"text": "BERT", "start_pos": 154, "end_pos": 158, "type": "METRIC", "confidence": 0.8733333349227905}]}, {"text": "As will be shown in the results section, we re-trained the models after the competition with layers -5 and -6 and achieved better results.", "labels": [], "entities": []}, {"text": "Pre-processing: As reported in the competition discussion forum, there are some clear label mistakes in GAP dataset.", "labels": [], "entities": [{"text": "Pre-processing", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9116309881210327}, {"text": "GAP dataset", "start_pos": 104, "end_pos": 115, "type": "DATASET", "confidence": 0.7742493152618408}]}, {"text": "We identified 159 mislabels (74 development, 68 test, 17 validation) to the best of our ability by going through all the examples with a log loss of 1 or larger.", "labels": [], "entities": []}, {"text": "We trained the system using corrected labels but report all results evaluated with original labels.", "labels": [], "entities": []}, {"text": "Post-processing: The problem with using clean labels to train and dirty labels to evaluate is that, loss will be huge for very confident predictions if the label is wrong (i.e. when the predicted probability for the wrong-label class is very small).", "labels": [], "entities": []}, {"text": "We solved this problem by clipping predicted probabilities smaller than a threshold 0.005, which was tuned with cross validation.", "labels": [], "entities": []}, {"text": "The idea is similar to label smoothing () and confidence penalty ( All the training was done in Google Colab with a single GPU.", "labels": [], "entities": [{"text": "label smoothing", "start_pos": 23, "end_pos": 38, "type": "TASK", "confidence": 0.7520309090614319}, {"text": "confidence penalty", "start_pos": 46, "end_pos": 64, "type": "METRIC", "confidence": 0.8868534564971924}, {"text": "Google Colab", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.8014191687107086}]}, {"text": "We used 5-fold cross validation for stage 1 results, and 5-fold average for stage 2 test results.", "labels": [], "entities": []}, {"text": "End2end model was trained 5 times using different seeds with each seed taking about 30 minutes; Pure BERT model was trained only once which took about 50 minutes.", "labels": [], "entities": [{"text": "End2end", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8899239897727966}, {"text": "BERT", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9883549213409424}]}, {"text": "Each team is allowed two submissions for this shared task.", "labels": [], "entities": []}, {"text": "Above described is our submission A.", "labels": [], "entities": []}, {"text": "Submission B is the same except that (1) it was trained on GAP test and validation sets only (2454 training samples instead of 4400), and (2) it didn't use the linguistic features.", "labels": [], "entities": [{"text": "GAP test and validation sets", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.8404541730880737}]}, {"text": "Submission B has worse results than A in both stage 1 and stage 2 as expected.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Meta information of two models.", "labels": [], "entities": []}, {"text": " Table 2: Stage 1 results improvements in End2end model due to augmentation", "labels": [], "entities": []}, {"text": " Table 3: Log loss scores of single models and the ensemble for both stages, competition version, with Overall,  Feminine, Masculine and Bias (M/F). Stage 2 results were evaluated after competition ended using the solution  provided by Kaggle, except the final score 0.1947 (in bold), which placed 7th in the competition.", "labels": [], "entities": [{"text": "Masculine", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.955450713634491}, {"text": "Bias (M/F)", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.7971085608005524}, {"text": "Kaggle", "start_pos": 236, "end_pos": 242, "type": "DATASET", "confidence": 0.8982402086257935}]}]}