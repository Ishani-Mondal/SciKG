{"title": [{"text": "Towards Effective Rebuttal: Listening Comprehension using Corpus-Wide Claim Mining", "labels": [], "entities": [{"text": "Listening Comprehension", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8933701515197754}]}], "abstractContent": [{"text": "Engaging in a live debate requires, among other things, the ability to effectively rebut arguments claimed by your opponent.", "labels": [], "entities": []}, {"text": "In particular , this requires identifying these arguments.", "labels": [], "entities": []}, {"text": "Here, we suggest doing so by automatically mining claims from a corpus of news articles containing billions of sentences, and searching for them in a given speech.", "labels": [], "entities": []}, {"text": "This raises the question of whether such claims indeed correspond to those made in spoken speeches.", "labels": [], "entities": []}, {"text": "To this end, we collected a large dataset of 400 speeches in English discussing 200 controversial topics, mined claims for each topic, and asked annotators to identify the mined claims mentioned in each speech.", "labels": [], "entities": []}, {"text": "Results show that in the vast majority of speeches debaters indeed make use of such claims.", "labels": [], "entities": []}, {"text": "In addition, we present several baselines for the automatic detection of mined claims in speeches, forming the basis for future work.", "labels": [], "entities": [{"text": "automatic detection of mined claims", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.8188554525375367}]}, {"text": "All collected data is freely available for research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Project Debater 1 is a system designed to engage in a full live debate with expert human debaters.", "labels": [], "entities": []}, {"text": "One of the major challenges in such a debate is listening to a several-minute long speech delivered by your opponent, identifying the main arguments, and rebutting them with effective persuasive counter arguments.", "labels": [], "entities": []}, {"text": "This work focuses on the former, namely, automatically identifying arguments mentioned in opponent speeches.", "labels": [], "entities": [{"text": "automatically identifying arguments mentioned in opponent speeches", "start_pos": 41, "end_pos": 107, "type": "TASK", "confidence": 0.8187742829322815}]}, {"text": "One of the fundamental capabilities developed in Debater is the automatic mining of claims () -general, concise statements that directly support or contest a given topic -from a large text corpus.", "labels": [], "entities": []}, {"text": "It allows Debater to present high-quality content supporting its side within its * * These authors equally contributed to this work.", "labels": [], "entities": []}, {"text": "1 www.research.ibm.com/ artificial-intelligence/project-debater generated speeches.", "labels": [], "entities": []}, {"text": "Our approach utilizes this capability fora different purpose: claims mined from the opposing side are searched for in a given opponent speech.", "labels": [], "entities": []}, {"text": "The implicit assumption in this approach is that mined claims would be often said by human opponents.", "labels": [], "entities": []}, {"text": "This is far from trivial, since mined content from a large text corpus is not guaranteed to provide enough coverage over arguments made by individual human debaters.", "labels": [], "entities": []}, {"text": "To assess this, we collected a large and varied dataset of recorded speeches discussing controversial topics, along with an annotation specifying which mined claims are mentioned in each speech.", "labels": [], "entities": []}, {"text": "Annotation results show our approach obtains good coverage, thus making the task of claim matching -automatically identifying given claims in speeches -interesting in the context of mined claims.", "labels": [], "entities": [{"text": "claim matching -automatically identifying given claims in speeches", "start_pos": 84, "end_pos": 150, "type": "TASK", "confidence": 0.6695681909720103}]}, {"text": "Using the collected data, several claim matching baselines are examined, forming the basis for future work in this direction.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: (i) a recorded dataset of 400 speeches discussing 200 controversial topics, along with mined claims for each topic; (ii) an annotation specifying the claims mentioned in each speech; (iii) baselines for matching mined claims to speeches.", "labels": [], "entities": []}, {"text": "All collected data is freely available for further research 2 .", "labels": [], "entities": []}], "datasetContent": [{"text": "Annotation confirmed our hypothesis that claims mined from a corpus are indeed mentioned, or are at least alluded to, in spontaneous speeches on controversial topics.", "labels": [], "entities": []}, {"text": "On average, of the 12.2 claims mined for each speech, about a third were annotated as mentioned.", "labels": [], "entities": []}, {"text": "We now present several baselines for identifying those mentioned claims, using the collected data.", "labels": [], "entities": []}, {"text": "Speech pre-processing An input audio speech is automatically transcribed into text using IBM Watson ASR 8 . The text is then segmented to sentences as in.", "labels": [], "entities": [{"text": "IBM Watson ASR 8", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.7522274255752563}]}, {"text": "Next, given a claim, semantically similar sentences are identified.", "labels": [], "entities": []}, {"text": "Each sentence is represented using a 200-dimensional vector constructed by: removing stopwords; representing remaining words using word2vec (w2v) () word embeddings learned over Wikipedia; computing a weighted average of those word embeddings using tf-idf weights (idf values are counted when considering each Wikipedia sentence as a document).", "labels": [], "entities": []}, {"text": "The claim is represented similarly, and its semantic similarity to a given sentence is computed using the cosine similarity between their vector representations.", "labels": [], "entities": []}, {"text": "All sentences with low similarity to the claim are ignored (using a fixed threshold).", "labels": [], "entities": []}, {"text": "Remaining sentences are scored by the harmonic mean (HM) of three additional semantic similarity measures, and the top-K ranked sentence are selected (we experiment with K \u2208 {1, 3, 5}).", "labels": [], "entities": [{"text": "harmonic mean (HM)", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.8905644059181214}]}, {"text": "These features are: -Concept Coverage: The fraction of Wikipedia concepts identified in the claim, found within the sentence.", "labels": [], "entities": []}, {"text": "-Parse Pairs: The parse trees of the claim and the sentence are obtained using Stanford parser).", "labels": [], "entities": []}, {"text": "Then, pairwise edge similarity is defined to be the harmonic mean of the cosine similarities computed between the two parent word embeddings and the two child word embeddings.", "labels": [], "entities": []}, {"text": "Each edge in the claim parse tree is scored using its maximal similarity to an edge from the sentence parse tree.", "labels": [], "entities": []}, {"text": "Averaging these scores yields the final feature score.", "labels": [], "entities": []}, {"text": "-Explicit Semantic Analysis (: Cosine similarity computed between vector representations of the claim and sentence over the Wikipedia concepts space.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis", "start_pos": 1, "end_pos": 27, "type": "TASK", "confidence": 0.55725626150767}]}, {"text": "Methods Following sentence selection, three methods are considered for scoring a speech and a claim: HM: Averaging the selected sentences HM scores.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7197171151638031}, {"text": "HM", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.9810066819190979}]}, {"text": "NN: Using a Siamese Network (, containing K instances of the same subnetwork: Each pair of a selected sentence and the claim is embedded with a BiLSTM, followed by an attention layer, a fully connected layer, and finally a softmax layer which yields a score for the pair.", "labels": [], "entities": [{"text": "BiLSTM", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.9040341377258301}]}, {"text": "The network outputs the maximum score of these K sub-networks.", "labels": [], "entities": []}, {"text": "LR: calculating 23 similarity measures between each selected sentence and the claim.", "labels": [], "entities": [{"text": "LR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7654070854187012}]}, {"text": "For each measure, the average over the K selected sentences is taken.", "labels": [], "entities": []}, {"text": "These averages are used as features for training a logistic regression classifier.", "labels": [], "entities": []}, {"text": "Following is a brief description of the different groups of similarity measures we used.", "labels": [], "entities": []}, {"text": "-w2v-based similarities (5 features): Computing pairwise word similarities using the cosine similarity of the corresponding word embeddings, and applying several aggregation options.", "labels": [], "entities": []}, {"text": "-Parse tree similarities (6 features): Computing the parse tree of the claim and the sentence, and calculating similarities between different elements of those trees, similarly to the Parse Pairs feature described above.", "labels": [], "entities": []}, {"text": "-Part of speech (POS) similarities (5 features): Identifying tokens with a specific POS tag in the texts, and computing either the fraction of such tokens from one text which appear in the other, or otherwise aggregating w2v-based cosine similarities between these tokens in several ways.", "labels": [], "entities": []}, {"text": "-Wikipedia concepts similarities (2 features): The fraction of Wikipedia concepts from the claim which are present in the sentence, and vice versa.", "labels": [], "entities": []}, {"text": "-Lexical similarities (5 features): n-grams are extracted from the two texts in various settings (e.g. with or without lemmatization, or using different values of n).", "labels": [], "entities": []}, {"text": "Then, each n-gram from the claim is scored by its maximal similarity to sentence n-grams (using a w2v-based similarity, with tf/idf weights).", "labels": [], "entities": []}, {"text": "The feature values is the average of these scores.", "labels": [], "entities": []}, {"text": "Training and test sets The data was randomly split into a train and test sets, equal in size.", "labels": [], "entities": []}, {"text": "Each contains 100 motions and 200 speeches.", "labels": [], "entities": []}, {"text": "The number of labeled speech-claim pairs is 2,456 in train and 2,426 in test.", "labels": [], "entities": []}, {"text": "Model selection as well as hyper-parameters tuning, such as the selection of K, are performed on train (using cross validation for LR and NN).", "labels": [], "entities": [{"text": "Model selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7017010599374771}]}, {"text": "Results The AUC score of both LR and NN on train, for various values of K, was no higher than 0.57.", "labels": [], "entities": [{"text": "AUC score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9793485403060913}]}, {"text": "In contrast, all HM configurations achieved AUC higher than 0.62.", "labels": [], "entities": [{"text": "AUC", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9993890523910522}]}, {"text": "We therefore focus on this method, though it is interesting, in future work, to improve the supervised methods or understand why they work somewhat poorly.", "labels": [], "entities": []}, {"text": "shows precision-recall curves for HM and the different values of K on test.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 6, "end_pos": 22, "type": "METRIC", "confidence": 0.9988447427749634}, {"text": "HM", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.988976776599884}, {"text": "K", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9704803824424744}]}, {"text": "The different plots are comparable, yet there is a slight advantage to K = 1 for applications valuing precision over recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9970417618751526}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9853594303131104}]}], "tableCaptions": []}