{"title": [{"text": "The utility of discourse parsing features for predicting argumentation structure", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7646449506282806}, {"text": "predicting argumentation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.9141789078712463}]}], "abstractContent": [{"text": "Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far.", "labels": [], "entities": [{"text": "argumentation mining from text", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.8956760764122009}, {"text": "discourse parsing", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.748401015996933}]}, {"text": "One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the 'argumentative microtexts' (Peldszus and Stede, 2016a).", "labels": [], "entities": []}, {"text": "While results on perus-ing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken.", "labels": [], "entities": [{"text": "predicting argumentation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.9127505421638489}, {"text": "automatic discourse parsing", "start_pos": 142, "end_pos": 169, "type": "TASK", "confidence": 0.6117799778779348}]}, {"text": "In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing.", "labels": [], "entities": [{"text": "argumentation parsing", "start_pos": 212, "end_pos": 233, "type": "TASK", "confidence": 0.8565296828746796}]}, {"text": "After reproducing the state-of-the-art Evidence Graph model from Afantenos et al.", "labels": [], "entities": []}, {"text": "(2018) for the microtexts, we find that PDTB features can indeed improve its performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The argumentative structure of texts, as captured, for instance, by schemata from or, is represented by tree structures that suggest a certain similarity to accounts of discourse structure, such as in Rhetorical Structure Theory ( or Segmented Discourse Representation Theory.", "labels": [], "entities": [{"text": "Segmented Discourse Representation Theory", "start_pos": 234, "end_pos": 275, "type": "TASK", "confidence": 0.7421141266822815}]}, {"text": "These approaches aim at accounting for the coherence of texts, which is clearly related -though not identical -to the structure of complex arguments.", "labels": [], "entities": []}, {"text": "This is not anew observation), but we are not aware of many empiricallygrounded studies of the correspondences between the two realms.", "labels": [], "entities": []}, {"text": "A corpus that facilitates such experiments is the 'argumentative microtext corpus', as it offers annotation not only for argumentation but also for discourse structure in terms of RST and SDRT.", "labels": [], "entities": []}, {"text": "While there is evidence that RST trees can \"in principle\" be helpful for parsing the argumentation (based on the gold annotations; see Peldszus and Stede, 2016b), we are not aware of experiments which try to verify such effects with automatic parsers.", "labels": [], "entities": [{"text": "parsing the argumentation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.8674513896306356}]}, {"text": "Our work aims to bridge this gap.", "labels": [], "entities": []}, {"text": "We use common parsers for RST and for Shallow Discourse Parsing (specifically the Penn Discourse Treebank, henceforth PDTB), run them on the microtexts, and first compare the RST output to the gold annotations, in order to assess the prospects of the idea.", "labels": [], "entities": [{"text": "RST", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9624520540237427}, {"text": "Penn Discourse Treebank, henceforth PDTB", "start_pos": 82, "end_pos": 122, "type": "DATASET", "confidence": 0.8933540880680084}]}, {"text": "Having selected the most promising parsers, we then compute a set of features from their output and add them to a state-ofthe-art implementation of argumentation parsing on the microtexts ().", "labels": [], "entities": [{"text": "argumentation parsing", "start_pos": 148, "end_pos": 169, "type": "TASK", "confidence": 0.76484215259552}]}, {"text": "The results indicate that the parsed PDTB features do in fact improve the accuracy of the argumentation annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9992656111717224}]}, {"text": "Section 2 discusses related work, and Section 3 describes the corpus and the discourse parsers we used.", "labels": [], "entities": []}, {"text": "Initial analyses of parser results are given in Section 4, and the experiments on predicting argumentation structure are reported in Section 5.", "labels": [], "entities": [{"text": "predicting argumentation structure", "start_pos": 82, "end_pos": 116, "type": "TASK", "confidence": 0.8777673244476318}]}, {"text": "The paper closes with some conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, we address the task of predicting the ARG structure with the help of discourse parser output.", "labels": [], "entities": []}, {"text": "We extended the system of and started from the feature set used by Peldszus and Stede (2016b); our own new features, listed in, will be referred to as 'RST+' and 'PDTB' respectively.", "labels": [], "entities": []}, {"text": "The task now is to assess their contribution in comparison to the 'Default' and 'RST' features from Peldszus and Stede (2016b) and to the best performing lexical, syntactic, semantic and discourse features used by.", "labels": [], "entities": []}, {"text": "In, which shows our results, the latter are labelled as '2018'.", "labels": [], "entities": []}, {"text": "We experimented with different combinations of the features on two different settings of the model: the simple relation set (support and attack); and the more fine-grained full relation set (support, example, join, link, undercut and rebuttal).", "labels": [], "entities": []}, {"text": "We used the same train-test splits as in, which involved 10 iterations of 5-fold cross validation.", "labels": [], "entities": []}, {"text": "The results for the full relation set were marginally better than those for the simple relations, aside from the fu classi- The numbers refer to the segments.", "labels": [], "entities": []}, {"text": "RST tree created using RSTTool fier whose highest score, 0.750, was achieved with the combination of all features for the simple relations.", "labels": [], "entities": [{"text": "RST tree", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8229907155036926}]}, {"text": "Even though the statistical analysis of the PDTB output at first did not seem promising, the PDTB features did improve all classifiers' performances.", "labels": [], "entities": []}, {"text": "The model's performance was best for the majority of classifiers with the features employed by in collaboration with our features for both settings.", "labels": [], "entities": []}, {"text": "In particular, our model achieved promising improvements on the attachment and function classifiers.", "labels": [], "entities": []}, {"text": "For illustration, shows the various analyses for one text from the corpus.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Co-occurrence matrix of the RST (rows) and  ARG (columns) relations of the matching edges in the  converted annotations", "labels": [], "entities": [{"text": "ARG", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9663337469100952}]}, {"text": " Table 2: Co-occurrence matrix of the PDTB (rows) and  ARG (columns) relations of the matching edges in the  converted annotations", "labels": [], "entities": [{"text": "ARG", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9826107621192932}]}, {"text": " Table 3: RST parser evaluation, with the categories  used by Joty et al. (2015) and others.", "labels": [], "entities": [{"text": "RST parser evaluation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8906515638033549}]}, {"text": " Table 5: Results for the full relation set with complex setting: macro-averaged F1 score, variance in parentheses,  maximum is in bold for each classifier", "labels": [], "entities": [{"text": "F1 score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9631295800209045}]}]}