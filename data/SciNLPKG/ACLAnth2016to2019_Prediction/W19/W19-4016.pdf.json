{"title": [], "abstractContent": [{"text": "The vast amount of research introducing new corpora and techniques for (semi-)automatically annotating corpora shows the important role that datasets play in today's research, especially in the machine learning community.", "labels": [], "entities": []}, {"text": "This rapid development raises concerns about the quality of the datasets created and consequently of the models trained, as recently discussed with respect to the Natural Language Inference (NLI) task.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI) task", "start_pos": 163, "end_pos": 200, "type": "TASK", "confidence": 0.6968981964247567}]}, {"text": "In this work we conduct an annotation experiment based on a small subset of the SICK corpus.", "labels": [], "entities": [{"text": "SICK corpus", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.8330127000808716}]}, {"text": "The experiment reveals several problems in the annotation guidelines, and various challenges of the NLI task itself.", "labels": [], "entities": []}, {"text": "Our quantitative evaluation of the experiment allows us to assign our empirical observations to specific linguistic phenomena and leads us to recommendations for future annotation tasks, for NLI and possibly for other tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the era of big data and deep learning there is an increasing need for large annotated corpora that can be used as training and evaluation data for (semi-)supervised methods.", "labels": [], "entities": []}, {"text": "This can be seen by the vast amount of work introducing new datasets and techniques for (semi-)automatically annotating corpora.", "labels": [], "entities": []}, {"text": "Different NLP tasks require different kinds of datasets and annotations and provide us with different challenges.", "labels": [], "entities": []}, {"text": "One task that has lately gained much attention in the community is the task of Natural Language Inference (NLI).", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.8024862507979075}]}, {"text": "NLI, also known as Recognizing Textual Entailment (RTE) ( ), is the task of defining the semantic relation between a premise text p and a conclusion text c. p can a) entail, b) contradict or c) be neutral to c.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.7604783177375793}]}, {"text": "The premise p is taken to entail conclusion c when a human reading p would infer that c is most probably true ( ).", "labels": [], "entities": []}, {"text": "This notion of \"human reading\" assumes human commonsense and common background knowledge.", "labels": [], "entities": []}, {"text": "This means that a successful automatic NLI system is a suitable evaluation measure for real natural language understanding, as discussed by and others.", "labels": [], "entities": [{"text": "real natural language understanding", "start_pos": 87, "end_pos": 122, "type": "TASK", "confidence": 0.6731819584965706}]}, {"text": "It is also a necessary step towards reasoning as more recently discussed by and who say that solving NLI perfectly means achieving human level understanding of language.", "labels": [], "entities": []}, {"text": "Thus, there is an increasing effort to design high-performing NLI systems, which in turn leads to the creation of massive learning corpora.", "labels": [], "entities": []}, {"text": "Early datasets, like or the seven RTE challenges ( ;, contained a few hundred handannotated pairs.", "labels": [], "entities": [{"text": "RTE challenges", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.6103610098361969}]}, {"text": "More recent sets have exploded from some thousand pairs (e.g.,) to some hundred thousand examples: SciTail (), SNLI, Multi-NLI (.", "labels": [], "entities": []}, {"text": "The latter two have been vastly used to train learning algorithms and achieve high performance.", "labels": [], "entities": []}, {"text": "However, it was recently shown that this high performance can drop significantly by slightly modifying the training process (.", "labels": [], "entities": []}, {"text": "It was also shown that such training sets contain annotation artifacts that bias the learning ().", "labels": [], "entities": []}, {"text": "Other recent work () discussed problematic annotations of the SICK corpus () and attempted to improve the annotations.", "labels": [], "entities": [{"text": "SICK corpus", "start_pos": 62, "end_pos": 73, "type": "DATASET", "confidence": 0.7530329525470734}]}, {"text": "All this work leads to the conclusion that corpus construction, including the annotation process, is much more important than what is often assumed and that bad corpora can falsely deliver promising results.", "labels": [], "entities": [{"text": "corpus construction", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8140082061290741}]}, {"text": "In this paper we take a closer look at the work by and attempt to build on the two conclusions that arise from their work.", "labels": [], "entities": []}, {"text": "The first conclusion is that the guidelines for the NLI annotation task need be improved, as it seems clear that human annotators often have opposing perspectives when annotating for inference.", "labels": [], "entities": []}, {"text": "This can result in faulty and illogical annotations.", "labels": [], "entities": []}, {"text": "The second conclusion concerns the annotation procedure: having an inference label is not enough; knowing why a human subject decides that an inference is an entailment or a contradiction is useful information that we should also be collecting, if we want to make sure that the corpus created adheres to the guidelines given.", "labels": [], "entities": []}, {"text": "Specifically, in this work we discuss an experiment, realized at the University of Colorado Boulder (CU), which attempts to address both these issues: provide uncontroversial, clear guidelines and give the annotators the chance to justify their decisions.", "labels": [], "entities": []}, {"text": "Our goal is to evaluate the guidelines based on the resulting agreement rates and gain insights into the NLI annotation task by collecting the annotators' comments on the annotations.", "labels": [], "entities": []}, {"text": "Thus, in the current work we make three contributions: Firstly, we discover which linguistic phenomena are hard for humans to annotate and show that these do not always coincide with what is assumed to be difficult for automatic systems.", "labels": [], "entities": []}, {"text": "Then, we propose aspects of NLI and of the annotation task itself that should betaken into account when designing future NLI corpora and annotation guidelines.", "labels": [], "entities": []}, {"text": "Thirdly, we show that it is essential to include a justification method in similar annotation tasks as a suitable way of checking the guidelines and improving the training and evaluation processes of automatic systems towards explainable AI.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiment was undertaken with the help of 12 Computer Science and Linguistics graduate students in a Computational Linguistics seminar.", "labels": [], "entities": []}, {"text": "These annotators were not under the pressure of making hasty judgements for money and had a much smaller number of pairs to work with than an average 'Mechanical Turker'.", "labels": [], "entities": []}, {"text": "The goal was to provide the students with clear, uncontroversial guidelines and ask them to annotate a small part of SICK.", "labels": [], "entities": []}, {"text": "They were also asked to justify their decisions, in order for us to see whether the given guidelines solved some of the problems discussed in relevant literature (e.g.;;) and whether we could gain additional insights from the students' justifications.", "labels": [], "entities": []}, {"text": "Apart from the inference relation and the justification, the students were also asked to give a score from 0-10 for what we would like to call \"computational feasibility\", i.e. their estimation of the likelihood of an automatic system getting the inference right.", "labels": [], "entities": []}, {"text": "The guidelines The guidelines for the CU experiment gave a detailed definition of NLI/RTE by using common literature definitions.", "labels": [], "entities": [{"text": "CU experiment", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.8579613566398621}, {"text": "NLI/RTE", "start_pos": 82, "end_pos": 89, "type": "TASK", "confidence": 0.5154308378696442}]}, {"text": "The annotators were asked to imagine sentence A as a caption of a picture, describing whatever is on that picture -following the creators of SNLI and MultiNLI to deal with coreference issues.", "labels": [], "entities": []}, {"text": "For each judgment, they were instructed to consider only the inference relation from A to B and not vice versa.", "labels": [], "entities": []}, {"text": "They were also instructed to assume that sentence A represents everything they know about the world of the picture; A represents the truth based on which they have to judge sentence B.", "labels": [], "entities": []}, {"text": "If A is talking about a man in red pants walking and B is also talking about a man in red pants running, they were told to assume that both sentences are talking about the same man and event.", "labels": [], "entities": []}, {"text": "The guidelines also provided detailed examples of each inference relation, along with the kinds of justifications expected.", "labels": [], "entities": []}, {"text": "Finally, special remarks were made for corner cases or cases that had already been shown in to cause confusion.", "labels": [], "entities": []}, {"text": "For example, they were told to ignore differences in determiners 1 and to use common-sense for matters that might seem subjective, e.g. a huge stick contradicts a small stick, even if a huge stick fora child might be a normal size stick for an adult, etc.", "labels": [], "entities": []}, {"text": "The annotation process For the current experiment, a total of 224 pairs was randomly chosen from SICK.", "labels": [], "entities": [{"text": "SICK", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8554050326347351}]}, {"text": "The pairs were annotated for their inference relation in both directions, resulting in a total of 448 judgments.", "labels": [], "entities": []}, {"text": "Each direction was annotated separately by 3 annotators.", "labels": [], "entities": []}, {"text": "The annotators had to provide an inference label (E, C, N for entailment, contradiction, neutrality, or, if they could not decide at all, DN for \"don't know\"), a justification for their choice and the \"computational feasibility\" score discussed above.", "labels": [], "entities": []}, {"text": "They could also note whether something was ungrammatical or nonsensical or if they had additional comments.", "labels": [], "entities": []}, {"text": "A set of 24 pairs (48 judgements) was given to all annotators at the beginning of the process for calibration.", "labels": [], "entities": []}, {"text": "The annotators were instructed to use the same four labels described above (E, C, N, DN).", "labels": [], "entities": []}, {"text": "In this set the three inference relations were almost equally represented: 16 entailments, 14 contradictions and 18 neutrals.", "labels": [], "entities": []}, {"text": "For the set there was 75.8% overall inter-annotator agreement (IAA) with Cohen's \u03ba at 0.68 (\"allowing tentative conclusions\" according to).", "labels": [], "entities": [{"text": "inter-annotator agreement (IAA)", "start_pos": 36, "end_pos": 67, "type": "METRIC", "confidence": 0.8883144140243531}]}, {"text": "3 More concretely, there was 80% IAA for contradiction, 93% for entailment and 63% for neutrals.", "labels": [], "entities": [{"text": "IAA", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9992114305496216}]}, {"text": "These agreement rates gave the preliminary impression that the guidelines were satisfactory.", "labels": [], "entities": []}, {"text": "\u03ba 64.25, comparable to the calibration set.", "labels": [], "entities": []}, {"text": "\u03ba is a standard metric in any similar task and here the high Kappa means that our guidelines work well enough to propose them for future tasks and allow us to make the annotated set available for further purposes.", "labels": [], "entities": []}, {"text": "However, we decided to look deeper into the annotated data and examine whether this metric is indeed sufficient to ensure reliable annotations.", "labels": [], "entities": []}, {"text": "After all, the goal of this work is to examine the annotation process in detail, especially observing the usefulness and need for the justifications we asked from the annotators.", "labels": [], "entities": []}, {"text": "This goal was reinforced by our further finding that the annotations provided by our annotators were different from the original SICK annotations in 17% of the annotated cases!", "labels": [], "entities": []}, {"text": "Assuming that our annotators are more reliable due to their training and better \"working\" conditions, this finding raises questions about the quality of the original SICK corpus, as already discussed by.", "labels": [], "entities": [{"text": "SICK corpus", "start_pos": 166, "end_pos": 177, "type": "DATASET", "confidence": 0.7089598476886749}]}, {"text": "Detailed analysis of the data revealed different kinds of justifications.", "labels": [], "entities": []}, {"text": "Firstly, there were the expected, less-informative justifications of the kind \"no relation\" or \"sentences mean the same thing\".", "labels": [], "entities": []}, {"text": "Though allowed, such justifications do not offer a lot of insight into the annotation.", "labels": [], "entities": []}, {"text": "Secondly, there were justifications describing the relation between the sentences and thus explaining the decision.", "labels": [], "entities": []}, {"text": "For example, for the pair A = A person is brushing a cat.", "labels": [], "entities": []}, {"text": "B = Nobody is brushing a cat, we got the justifications: \"cat cannot be both brushed and not brushed\", \"cannot both brush and not brush a cat\" and \"someone != no one\".", "labels": [], "entities": []}, {"text": "Such justifications were the expected ones and what we hoped for when integrating the justification annotation.", "labels": [], "entities": []}, {"text": "Thirdly, the justifications and the annotations themselves indicated that there was much confusion about when a pair should be a contradiction or neutral.", "labels": [], "entities": []}, {"text": "Annotators considered as contradiction pairs in which sentence B had nothing to do with A.", "labels": [], "entities": []}, {"text": "In an attempt to find some relation between the sentences and without paying attention to the fact that contradictions can be defined only when entities/events are coreferent, the annotators found many contradictions.", "labels": [], "entities": []}, {"text": "For example, the pair A = Two sumo ringers are fighting.", "labels": [], "entities": []}, {"text": "B = A man is riding a water toy in the water was labeled as contradiction, with the justification \"the subjects and activities are completely different\".", "labels": [], "entities": []}, {"text": "However, in what we considered clear guidelines, we had stated that \"A represents everything you know about the world of the picture, A represents the truth based on which you have to judge sentence B\" and that therefore in such an example, sentence B cannot be judged given A, hence the pair should be neutral.", "labels": [], "entities": []}, {"text": "This observation is very interesting because it seems to concern other NLI corpora as well, e.g. in SNLI we find pairs like A = A young boy in afield of flowers carrying a ball.", "labels": [], "entities": [{"text": "SNLI", "start_pos": 100, "end_pos": 104, "type": "TASK", "confidence": 0.5815681219100952}]}, {"text": "B = dog in pool also marked as contradiction, although it is clear that there is no coreference and thus it should be neutral.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9046710133552551}]}, {"text": "Conversely, we found many cases where there was an obvious coreference and contradictory events/entities but the annotators attempted to think of scenarios where both things could still co-occur.", "labels": [], "entities": []}, {"text": "The pair, A = A girl is getting a tattoo removed from her hand.", "labels": [], "entities": [{"text": "A", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9466682076454163}]}, {"text": "B = A girl is getting a tattoo on her hand, was correctly judged by two annotators as contradiction because \"getting a tattoo contradicts tattoo removal\" but the third one thought of it as neutral because \"could begetting both at the same time\".", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.884838342666626}]}, {"text": "Another more important observation was that the same pair had different agreement rates depending on its direction.", "labels": [], "entities": [{"text": "agreement rates", "start_pos": 72, "end_pos": 87, "type": "METRIC", "confidence": 0.9294711053371429}]}, {"text": "Recall that the pairs were given in both directions but separately from each other.", "labels": [], "entities": []}, {"text": "An example is the calibration pair A = A light brown dog is sprinting in the water.", "labels": [], "entities": []}, {"text": "B = A light brown dog is running in the water.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.95237135887146}]}, {"text": "This direction of the pair (A \u2192 B) was unanimously annotated as entailment by 12 annotators.", "labels": [], "entities": []}, {"text": "However, the opposite direction B \u2192 A got an agreement of 25% entailment and 75% neutrality.", "labels": [], "entities": [{"text": "neutrality", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9898297190666199}]}, {"text": "Here, some annotators gave justifications like \"running and sprinting are kind of the same for everyday situations\" while others, following dictionaries more carefully, assumed that while sprinting is a kind of running, running does not entail sprinting.", "labels": [], "entities": []}, {"text": "Only one direction of the pair is thus uncontroversial.", "labels": [], "entities": []}, {"text": "This raises questions of whether one direction is indeed harder than the other and whether such directionality effects should be considered in the design and evaluation of NLI annotation tasks.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this has so far not been taken into account for such datasets.", "labels": [], "entities": []}, {"text": "This observation is closely related to another: pairs involving what we would call \"loose definitions/loose human inference\" are also more prone to disagreements.", "labels": [], "entities": []}, {"text": "Looking at the calibration pair A = A white dog is standing on a hill covered by grass.", "labels": [], "entities": []}, {"text": "B = A dog is standing on the side of a mountain, the annotators have to decide whether hill covered by grass is the same as mountain and since definitions tend to be loose and subjective, such pairs get bad IAA (25% E, 33% C, 41% N).", "labels": [], "entities": [{"text": "IAA", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.9995204210281372}, {"text": "N", "start_pos": 230, "end_pos": 231, "type": "METRIC", "confidence": 0.960622251033783}]}, {"text": "Interestingly, the opposite direction gets a slightly better agreement (17% C, 83% N), which again brings up the issue of directionality described above.Another good example is A = A man is talking on the phone.", "labels": [], "entities": [{"text": "agreement", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.972532331943512}]}, {"text": "B = A man is making a phone call.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9550551176071167}]}, {"text": "Here, one annotator marked it as neutral as \"talking on the phone does not entail that the man initiated the call\", another marked it as contradiction because \"making a phone call is an action that precludes talking on the phone\", while the third one considered it an entailment because \"talking on implies phone call\".", "labels": [], "entities": []}, {"text": "For tasks like NLI and for certain domains, we might need this kind of looseness that would allow the pair to bean entailment even though \"talking on the phone\" does not logically entail \"making a phone call\" (assuming that \"making a phone call\" contains the concept of in fact initiating the call, \"talking on the phone\" does not entail \"initiating the call\" and thus it also does not logically entail \"making a phone call\" (modus tollens)).", "labels": [], "entities": []}, {"text": "But then, how do we define such corner cases?", "labels": [], "entities": []}, {"text": "Could the annotation guidelines ever exactly define the concept of commonsense, so that such cases are treated uniformly?", "labels": [], "entities": []}, {"text": "Another preliminary observation was the correlation of high \"computational feasibility\" scores (CF scores) with highly unambiguous pairs.", "labels": [], "entities": [{"text": "computational feasibility\" scores (CF scores)", "start_pos": 61, "end_pos": 106, "type": "METRIC", "confidence": 0.6263202428817749}]}, {"text": "The CF score was introduced in the annotation to check whether the annotators thought it was likely for an NLI system to get the inference label right.", "labels": [], "entities": [{"text": "CF score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9650604426860809}]}, {"text": "Since the score relied more on the annotators' intuition and lesson objective annotation guidelines, we observed that the given answers varied widely with poor agreement.", "labels": [], "entities": []}, {"text": "However, general observations can be made: high scores (above 8) were mainly given to pairs with direct, clear-cut negations like A = Nobody is holding a hedgehog.", "labels": [], "entities": []}, {"text": "B = Someone is holding a hedgehog. or to entailments with only differences in determiners, such as A = The person is peeling an onion.", "labels": [], "entities": []}, {"text": "B = A person is peeling an onion. or to entailment pairs with only one-worddifference, e.g. A = A child in orange is playing outdoors with a snowball.", "labels": [], "entities": []}, {"text": "B = A kid in orange is playing outside with a snowball, where child = kid is an easy lexical entailment.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9426483511924744}]}, {"text": "These observations are not surprising:  discuss such cases that can be easily solved solely based on WordNet and heuristics.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9652335047721863}]}, {"text": "The previous observations lead us to two important conclusions: for one, the justifications the annotators provided were crucial to make us understand what was being annotated and what aspects of the guidelines were still unclear.", "labels": [], "entities": []}, {"text": "Thus, if we are interested in annotated data that enables us to confirm the quality of the annotation task, similar justification fields are needed.", "labels": [], "entities": []}, {"text": "Furthermore, the guidelines need to address aspects that can be controversial, e.g. they need to state explicitly and a priori that contradictions can occur if and only if coreference can be established.", "labels": [], "entities": []}, {"text": "Such improvements will be discussed further in Section 6.1.", "labels": [], "entities": []}, {"text": "The second conclusion is even more crucial: what if the previous observations are not merely random but can indeed be classified in phenomena and observed in other NLI data?", "labels": [], "entities": []}, {"text": "While we know that many linguistic phenomena impose challenges for automatically detecting the inference relation between a pair of sentences, it is unclear which phenomena are also difficult fora human to annotate.", "labels": [], "entities": []}, {"text": "For example, the passive/active voice distinction is a phenomenon that always receives attention when dealing with inference relations.", "labels": [], "entities": []}, {"text": "However, this kind of phenomenon seems very easy for humans.", "labels": [], "entities": []}, {"text": "On the other hand, dealing with loose definitions or coreference seems difficult even for humans.", "labels": [], "entities": []}, {"text": "Since such phenomena repeatedly appeared in the justifications of the annotators, we decided to verify if the sentences that had lower agreement actually showed exactly these phenomena.", "labels": [], "entities": []}, {"text": "We conjecture that these phenomena are measurable quantities that need to be considered in all future annotation tasks.", "labels": [], "entities": []}, {"text": "If so, there should be a measurable correlation among the phenomena and the low IAA, so that these phenomena lead to statistically worse agreements.", "labels": [], "entities": [{"text": "IAA", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9807377457618713}]}, {"text": "To investigate these questions, we conducted a second experiment based on the CU experiment: based on our observations of Section 4 and the previous literature on SICK, we defined six distinct categories according to which we ourselves meta-annotated all 224 pairs.", "labels": [], "entities": [{"text": "CU experiment", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9488645195960999}]}, {"text": "Although this meta-annotation took place after making our preliminary observations on the data, the validity of this annotation is not influenced in any significant way: our preliminary observations were only that; observations and no real analysis of the data, also not an informal one.", "labels": [], "entities": [{"text": "validity", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9863013029098511}]}, {"text": "It was exactly this question that we seek to answer by this second experiment: can these abstract observations be quantified and analyzed in a formal way?", "labels": [], "entities": []}, {"text": "Specific Annotation Precisely, we metaannotated the pairs for coreference, directionality, loose definitions, atomicity, negation and quantification phenomena.", "labels": [], "entities": []}, {"text": "For the feature coreference, we marked whether a pair contains events or entities that are hard to assume coreferent (we annotated True for hard coreference and False for easy coreference).", "labels": [], "entities": [{"text": "False", "start_pos": 161, "end_pos": 166, "type": "METRIC", "confidence": 0.9829838275909424}]}, {"text": "Coreference difficulty could lead to the first phenomenon described above; not being able to decide whether something is coreferent and thus contradictory, or neutral.", "labels": [], "entities": []}, {"text": "In the category directionality, we marked for each pair direction whether this direction was harder, easier or equally difficult to annotate as the opposite direction.", "labels": [], "entities": []}, {"text": "In the loose definition category, we checked whether the pair contains concepts that are \"loose\", subjective or vague to define (annotated as True) or not (annotated as False).", "labels": [], "entities": [{"text": "False", "start_pos": 169, "end_pos": 174, "type": "METRIC", "confidence": 0.9130095839500427}]}, {"text": "The next category was inspired by the previous work of on SICK: atomicity concerns the question of whether a sentence contains only one predicate-argument structure or more.", "labels": [], "entities": []}, {"text": "This relates to the observation by that marking the inference relation, and especially making events and entities coreferent, is easier to do when the pair only contains atomic sentences, i.e. sentences with one main verb.", "labels": [], "entities": []}, {"text": "In non-atomic sentences, all parts of the sentence should be able to be made coreferent with the other sentence, something that often proves a challenge, especially if the other sentence is atomic.", "labels": [], "entities": []}, {"text": "An example is the pair A = The singer is playing the guitar at an acoustic concert fora woman.", "labels": [], "entities": []}, {"text": "B = A person is playing a guitar and singing.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9023676514625549}]}, {"text": "A is atomic but B is not (playing and singing), so that the question arises whether the person singing can be coreferent with the singer.", "labels": [], "entities": []}, {"text": "We annotate each sentence of each pair with True or False, depending on whether they are atomic or not.", "labels": [], "entities": [{"text": "False", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.9384181499481201}]}, {"text": "Negation also contains the labels True or False: here we mark if each sentence of the pair contains a negation of any kind (verbal, pronominal, etc.).", "labels": [], "entities": []}, {"text": "We do a similar task for quantifiers: we mark whether each sentence contains a quantifier or not.", "labels": [], "entities": []}, {"text": "We added these last two categories to quantitatively test our impression that negation and quantifiers also cause more annotation problems, just as coreference, loose definitions, etc.: Overview of the average IAA (%) and CF score (1-10) for each condition of our experiment.", "labels": [], "entities": [{"text": "IAA", "start_pos": 210, "end_pos": 213, "type": "METRIC", "confidence": 0.9768064618110657}, {"text": "CF score (1-10)", "start_pos": 222, "end_pos": 237, "type": "METRIC", "confidence": 0.9300311088562012}]}, {"text": "Results The overall goal of these metaannotations was to check if the presence of these phenomena correlates with low IAA and low CF scores.", "labels": [], "entities": [{"text": "IAA", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.943440854549408}, {"text": "CF scores", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9737944006919861}]}, {"text": "In other words, we wanted to test whether the IAA and CF scores are statistically worse in pairs with such phenomena.", "labels": [], "entities": [{"text": "IAA", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.695841372013092}, {"text": "CF", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.7302634119987488}]}, {"text": "To this end, we calculated the IAA and the CF score for each pair and each of the six meta-annotations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9904642105102539}, {"text": "CF score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9852644801139832}]}, {"text": "We then computed the average IAA and CF score of the pairs in each condition of our meta-annotations.", "labels": [], "entities": [{"text": "IAA", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9873145222663879}, {"text": "CF score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9792370200157166}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We should note that we could conduct this kind of study only on the re-annotated SICK pairs of our CU experiment (Section 3) and not on the original SICK annotations because for those the exact IAAs are not available but only the final majority label.", "labels": [], "entities": []}, {"text": "Thus, it would not be possible to quantify our findings over those annotations.", "labels": [], "entities": []}, {"text": "To test for the involved effects, we analyzed the IAA results using generalized additive mixed models (GAMMs) with the ocat-linking function for ordered categorical data.", "labels": [], "entities": [{"text": "IAA", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.4801175892353058}]}, {"text": "Calculated by averaging the scores of the 3 annotators.", "labels": [], "entities": []}, {"text": "We chose this kind of modelling due to the nature of our dependent variable IAA.", "labels": [], "entities": [{"text": "IAA", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.8198623657226562}]}, {"text": "The six metaannotation categories were added as fixed factors with interactions and the pairs were entered as random smoothers.", "labels": [], "entities": []}, {"text": "The fixed factors coreference, loose definitions, atomicity of A, atomicity of B, negation of A, negation of B and quantification of A and quantification of B were binary (True or False for each of them as described in 5) (cf., top) and the effect directionality was a 3-level variable (\"easier\", \"harder\" and \"equal\") (cf. Table 1, bottom).", "labels": [], "entities": []}, {"text": "Interaction, main effects and random smoothers were removed if they were not significant at \u03b1 = 0.05 and the model was refitted.", "labels": [], "entities": [{"text": "Interaction", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9463691711425781}]}, {"text": "Concerning the inter-annotator agreement, the results showed main effects of coreference, directionality, loose definitions and negation.", "labels": [], "entities": []}, {"text": "For the coreference setting, there was statistically lower agreement in pairs with coreference marked as hard than in pairs with easy coreference, with p < 0.04.", "labels": [], "entities": [{"text": "agreement", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9733919501304626}]}, {"text": "Directionality also showed a correlation with the agreement rates, with pairs in the \"harder\" direction having statistically lower IAA (p < 0.001) than pairs in the \"easier\" and \"same\" direction and pairs in the \"same\" direction having statistically lower agreements than pairs in the \"easier\" direction (p < 0.001).", "labels": [], "entities": [{"text": "IAA", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.9987847208976746}]}, {"text": "A similar observation can be made for the loose definitions effect: pairs not containing loose definitions showed a statistically better agreement than pairs with such definitions (p < 0.02).", "labels": [], "entities": []}, {"text": "The three factors presented so far confirmed our preliminary observations that these phenomena are not random but are quantitatively depicted in the data.", "labels": [], "entities": []}, {"text": "As far as negation is concerned, the results were counter-intuitive at first glance: pairs with negation in one of the sentences A or B had statistically higher IAA rates (p < 0.001) than pairs with no negation at all.", "labels": [], "entities": [{"text": "negation", "start_pos": 10, "end_pos": 18, "type": "TASK", "confidence": 0.961717963218689}, {"text": "IAA rates", "start_pos": 161, "end_pos": 170, "type": "METRIC", "confidence": 0.9847508668899536}]}, {"text": "However, after a closer look, this is not so puzzling: the pairs of our dataset containing negation are the kind of clear-cut, textbook types of negation with one sentence negating exactly what the other sentence is stating by the use of \"not\", \"no\" or \"nobody\", as A = Nobody is holding a hedgehog.", "labels": [], "entities": []}, {"text": "B = Someone is holding a hedgehog..", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9552013874053955}]}, {"text": "Thus, this statistical result shows that it might in fact be easier to decide for such straight-forward pairs with clearcut negation than for pairs that have no negation but contain hard coreference or loose definitions or generally some complex context.", "labels": [], "entities": []}, {"text": "There was no main effect of quantification, i.e. there is no statistical difference between the agreement of annotators in pairs with and without quantifiers.", "labels": [], "entities": []}, {"text": "This is probably expected given the very small number of quantifiers found in our data.", "labels": [], "entities": []}, {"text": "Otherwise, it could indicate that quantifiers are not so hard for humans as they are assumed to be for machines.", "labels": [], "entities": []}, {"text": "Last but not least, the effect of atomicity offers grounds for discussion: for one, annotating atomicity is not as clear cut as one could expect, e.g. there is the open question whether sentences with participles should count as atomic or not.", "labels": [], "entities": []}, {"text": "In the example A = A white dog is standing on a hill covered by grass.", "labels": [], "entities": []}, {"text": "B = A white dog is standing on a grassy hillside, it is not clear whether the participle covered should count as an additional predicate-argument structure.", "labels": [], "entities": []}, {"text": "We decided to annotate such sentences as atomic (we considered non-atomic only sentences containing more than one main clause verbs).", "labels": [], "entities": []}, {"text": "For another, we expected pairs with atomic sentences to be significantly easier to annotate for the inference relations compared to non-atomic sentences.", "labels": [], "entities": []}, {"text": "This turns out not to be the casein our dataset: the atomicity of the sentences does not impact the agreement rates; the slightly higher agreement when A or B are non-atomic (condition False) is not statistically significant ( p > 0.08).", "labels": [], "entities": [{"text": "False", "start_pos": 185, "end_pos": 190, "type": "METRIC", "confidence": 0.9371799826622009}]}, {"text": "It is necessary to test this factor with more and more diverse data to see if the significance changes.", "labels": [], "entities": [{"text": "significance", "start_pos": 82, "end_pos": 94, "type": "METRIC", "confidence": 0.9755589365959167}]}, {"text": "No significant interactions could be established for this model.", "labels": [], "entities": []}, {"text": "To test for the involved effects in the CF scores results, we analyzed our results with a logistic mixed-effects regression model with CF score as dependent variable and the six meta-annotation categories as fixed factors (main effects and interactions) and the pairs as random effects, using the R-packages lme4 () and lmerTest (.", "labels": [], "entities": []}, {"text": "Then, the random and fixed effects were backward fitted, using the step()-function in lmerTest with the default \u03b1 cut-off levels (0.1 for random effects and 0.05 for fixed effects).", "labels": [], "entities": []}, {"text": "The best fitted model showed main effects of coreference and negation.", "labels": [], "entities": [{"text": "coreference", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9656245112419128}, {"text": "negation", "start_pos": 61, "end_pos": 69, "type": "TASK", "confidence": 0.9632030725479126}]}, {"text": "Pairs involving hard coreference have statistically lower CF scores, i.e. they are considered harder for an automatic system to label.", "labels": [], "entities": [{"text": "CF scores", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9762494564056396}]}, {"text": "This correlation also shows that coreference is indeed an intuitively detectable factor of inference pairs that annotators \"caught\" by giving such pairs lower CF scores.", "labels": [], "entities": []}, {"text": "Pairs with negation in A or B sentence have statistically higher CF scores, i.e. they are considered easier for an automatic system to label.", "labels": [], "entities": [{"text": "CF scores", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9872081577777863}]}, {"text": "Both these findings are consistent with our preliminary observations.", "labels": [], "entities": []}, {"text": "As we observed in Section 4, high CF scores seem to correlate with pairs that are highly unambiguous.", "labels": [], "entities": [{"text": "CF scores", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.768233060836792}]}, {"text": "In our case, these are pairs with the kind of clear-cut, textbook negations like A = A woman is slicing a tomato.", "labels": [], "entities": []}, {"text": "B = There is no woman slicing a tomato. or pairs containing easy entailments, e.g that a kid is a child or that a small boy is a boy.", "labels": [], "entities": [{"text": "B", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9732548594474792}]}, {"text": "The fact that the CF scores are statistically higher when there is negation or when the coreference is clear, i.e. there is an easy entailment of the previous kind, confirms this observation.", "labels": [], "entities": [{"text": "CF scores", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9007475078105927}]}, {"text": "Nevertheless, as we noted for the inter-annotator agreement above, negation seems to bean easy case due to its nature in this dataset; it is expected that in more complex data, negation will play a different role.", "labels": [], "entities": [{"text": "negation", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.9746253490447998}]}, {"text": "No significant interactions could be established for this model.", "labels": [], "entities": []}, {"text": "Note that the small differences in the average CF scores shown in result from the actual average scores used by the annotators for each pair ranging from a minimum of 3.54 to a maximum of 8.65.", "labels": [], "entities": [{"text": "CF", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.8961268663406372}]}, {"text": "Ina small side experiment we also tested how the CF scores correlate with what is really hard for automatic systems.", "labels": [], "entities": [{"text": "CF", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.8871990442276001}]}, {"text": "We chose the best performing system from the SemEval 2014 task) on SICK by and extracted from their test data those pairs that were also included in our subcorpus.", "labels": [], "entities": [{"text": "SemEval 2014 task", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.584989051024119}, {"text": "SICK", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.7068924903869629}]}, {"text": "These 92 pairs were split into two groups: those where the label given by the automatic system was the same as the label given by our annotators and those where it was different, i.e. the system got it wrong.", "labels": [], "entities": []}, {"text": "For each of those groups we calculated the average CF score.", "labels": [], "entities": [{"text": "CF score", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9709194004535675}]}, {"text": "Both groups have an average CF between 6.2 and 6.8, which means that for our subcorpus and this NLI system there is no strong correlation between what our annotators considered hard for machines and what is indeed hard.", "labels": [], "entities": [{"text": "CF", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9974308609962463}]}], "tableCaptions": [{"text": " Table 1: Overview of the average IAA (%) and CF  score (1-10) for each condition of our experiment.", "labels": [], "entities": [{"text": "IAA", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9922139048576355}, {"text": "CF  score (1-10)", "start_pos": 46, "end_pos": 62, "type": "METRIC", "confidence": 0.909833025932312}]}]}