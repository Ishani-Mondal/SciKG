{"title": [{"text": "Unsupervised Dialogue Spectrum Generation for Log Dialogue Ranking", "labels": [], "entities": [{"text": "Log Dialogue", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.8695342540740967}]}], "abstractContent": [{"text": "Although the data-driven approaches of some recent bot building platforms make it possible fora wide range of users to easily create dialogue systems, those platforms don't offer tools for quickly identifying which log dialogues contain problems.", "labels": [], "entities": []}, {"text": "This is important since corrections to log dialogues provide a means to improve performance after deployment.", "labels": [], "entities": []}, {"text": "A log dialogue ranker, which ranks problematic dialogues higher, is an essential tool due to the sheer volume of log dialogues that could be generated.", "labels": [], "entities": []}, {"text": "However, training a ranker typically requires labelling a substantial amount of data, which is not feasible for most users.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel un-supervised approach for dialogue ranking using GANs and release a corpus of labelled dialogues for evaluation and comparison with supervised methods.", "labels": [], "entities": [{"text": "dialogue ranking", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.8833747208118439}]}, {"text": "The evaluation result shows that our method compares favorably to supervised methods without any labelled data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Task-oriented dialogue systems provide a natural interface to accomplish various daily-life tasks such as restaurant finding and flight booking.", "labels": [], "entities": [{"text": "restaurant finding", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7456861883401871}, {"text": "flight booking", "start_pos": 129, "end_pos": 143, "type": "TASK", "confidence": 0.7854803502559662}]}, {"text": "Data-driven approaches offered by common bot building platforms (e.g. Google Dialogflow, Amazon Alexa Skills Kit, Microsoft Bot Framework) make it possible fora wide range of users to easily create dialogue systems with a limited amount of data in their domain of interest.", "labels": [], "entities": []}, {"text": "Typically, the development process of a dialogue system based on data-driven approaches ( goes around an operational loop in: (1) The cycle begins with a developer creating a training dataset with seed dialogues.", "labels": [], "entities": []}, {"text": "(2) A dialogue system is trained and deployed.", "labels": [], "entities": []}, {"text": "(3) Real users interact with the system and generate log dialogues.", "labels": [], "entities": []}, {"text": "(4) The developer reviews the logs to identify which log dialogues contain problems.", "labels": [], "entities": []}, {"text": "The developer updates the training dataset to fix the problems.", "labels": [], "entities": []}, {"text": "(6) The cycle repeats from step 2).", "labels": [], "entities": []}, {"text": "Of all steps, (4) is the most significant in slowing down the loop, because of the sheer volume of log dialogues that can be generated and the need to manually inspect each.", "labels": [], "entities": []}, {"text": "Thus, it is essential to support tools that help developers quickly identify problematic log dialogues.", "labels": [], "entities": []}, {"text": "To achieve this goal, we propose a neural dialog ranker whose goal is to place problematic dialogues higher in the rank.", "labels": [], "entities": []}, {"text": "However, training a ranker typically requires labelling a substantial amount of data, which is not feasible for most developers.", "labels": [], "entities": []}, {"text": "Furthermore, one might have to repeat this process whenever a significant change is made to the system's behavior.", "labels": [], "entities": []}, {"text": "This motivates us to explore a set of unsupervised approaches to reduce the prohibitive cost.", "labels": [], "entities": []}, {"text": "The core idea of these methods is that we learn a generative model to produce problematic dialogue examples as positive examples and train a ranker with seed dialogues used as negative examples.", "labels": [], "entities": []}, {"text": "Specifically, we propose a novel dialogue generator using Generative Adversarial Networks (GANs) and train the generator with a curriculum learning scheme.", "labels": [], "entities": []}, {"text": "Another possible avenue is to leverage off-the-shelf dialogue quality classifiers which are trained on open-domain corpora such as dialogue breakdown detection challenge systems in DSTC6 ().", "labels": [], "entities": [{"text": "dialogue breakdown detection challenge", "start_pos": 131, "end_pos": 169, "type": "TASK", "confidence": 0.7988210320472717}, {"text": "DSTC6", "start_pos": 181, "end_pos": 186, "type": "DATASET", "confidence": 0.9301989674568176}]}, {"text": "In the experiment on the labelled dialogue corpus that we collected via Amazon Mechanical Turk, we show that our approach outperformes the off-the-shelf model by a significant margin thanks to the capability of generating domain-relevant problematic dialogues.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 72, "end_pos": 94, "type": "DATASET", "confidence": 0.9659899274508158}]}, {"text": "The evaluation result also shows that our unsupervised method compares favorably to supervised methods without any labelled data.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we formalize the dialogue ranking task and describe our neural ranking model.", "labels": [], "entities": [{"text": "dialogue ranking task", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.83839879433314}]}, {"text": "In Section 3, we introduce a set of unsupervised methods for problematic dialogue example generation.", "labels": [], "entities": [{"text": "dialogue example generation", "start_pos": 73, "end_pos": 100, "type": "TASK", "confidence": 0.637156625588735}]}, {"text": "Section 4 describes the datasets we used for this study.", "labels": [], "entities": []}, {"text": "Section 5 explains our experiments.", "labels": [], "entities": []}, {"text": "In Section 6, we discuss our experimental results.", "labels": [], "entities": []}, {"text": "Section 7 provides a survey of related work.", "labels": [], "entities": []}, {"text": "We finish with conclusions and future work in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we build a log dialogue ranker for the restaurant inquiry bot offered by the PyDial platform.", "labels": [], "entities": [{"text": "PyDial platform", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.9140155911445618}]}, {"text": "The task for the bot is to search for restaurants based on user's requirements in a multi-turn natural language communication.", "labels": [], "entities": []}, {"text": "Three main corpora are introduced: (1) log dialogues with labels, (2) seed dialogues for the restaurant domain, (3) a large corpus of dialogues collected from multiple domains ().", "labels": [], "entities": []}, {"text": "We describe a set of experimental settings in Section 5.1 and evaluation metrics in Section 5.2.", "labels": [], "entities": []}, {"text": "We present three experiments to demonstrate the efficacy of our approaches.", "labels": [], "entities": []}, {"text": "\u2022 StepGAN vs. other unsupervised approaches: To compare the StepGAN against other unsupervised approaches introduced in Section 3: -MultiDomain: We train the user simulator on the multi-domain corpus.", "labels": [], "entities": []}, {"text": "-FineTune: We fine-tune the user simulator on the seed dialogues.", "labels": [], "entities": []}, {"text": "-StepFineTune: We fine-tune the user simulator in a stepwise fashion.", "labels": [], "entities": []}, {"text": "We train dialogue rankers for each approach on a balanced dataset, consisting of 1000 problematic dialogues generated by each approach and 1000 normal dialogues oversampled from the seed dialogues to balance the class distribution.", "labels": [], "entities": []}, {"text": "We evaluate two additional models: One problematic example in the labelled corpus SYS: hello, welcome to the restaurant finding system.", "labels": [], "entities": [{"text": "restaurant finding", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7191754579544067}]}, {"text": "how may i help you?", "labels": [], "entities": []}, {"text": "USER: hello i would really like to eat some cheap indian tonight SYS: what part of town do you have in mind?", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.78795325756073}]}, {"text": "USER: i would really like to eat in the centre of town SYS: i am sorry but there is no place, in the expensive price range, serving corsica food and in the area of centre.", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9174923896789551}]}, {"text": "USER: i would really like so indian thats cheap in the centre of town SYS: the gandhi is a nice place, it is in the cheap price range, it is in the centre of town and it serves indian food.", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8594496250152588}, {"text": "SYS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.8546206951141357}]}, {"text": "USER: thats great, have you got a contact number?", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.6901098489761353}]}, {"text": "SYS: what part of town do you have in mind?", "labels": [], "entities": []}, {"text": "USER: the centre of town SYS: efes restaurant is a nice place, it is in the centre of town, it is in the moderate price range and it serves turkish food.", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9283283352851868}]}, {"text": "USER: i want to eat at the gandhi have you got the address?", "labels": [], "entities": []}, {"text": "SYS: the gandhi is a nice place their address is 72 regent street city centre.", "labels": [], "entities": []}, {"text": "USER: thank you, have you got a phone number for the gandhi?", "labels": [], "entities": []}, {"text": "SYS: the gandhi is a nice place their phone number is 0 1223 353942.", "labels": [], "entities": []}, {"text": "USER: thanks SYS: chiquito bar is a nice place, it serves mexican food in the expensive price range and it is in the south part of town.", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9362053871154785}]}, {"text": "USER: quit   \u2022 StepGAN vs. supervised approach: One advantage for the unsupervised approach is to generate a larger training set without human efforts.", "labels": [], "entities": [{"text": "USER", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.46825262904167175}]}, {"text": "To compare the best performance of the StepGAN model against the supervised model, we train StepGAN models on three balanced datasets with 2000, 4000 and 6000 dialogues each and compare them to a supervised dialogue ranker trained on a balanced dataset of 2000 labelled dialogues randomly sampled from the training set described in Section 4.", "labels": [], "entities": []}, {"text": "\u2022 Semi-supervised learning attempts: On top of the labelled data, we can employ unsupervised approaches for data augmentation.", "labels": [], "entities": []}, {"text": "For the evaluation of such a semi-supervised setting, we compare the performance of supervised models with 500 and 2000 labelled examples and that of their counterparts which leverage additional 6000 examples generated by StepGAN.", "labels": [], "entities": [{"text": "StepGAN", "start_pos": 222, "end_pos": 229, "type": "DATASET", "confidence": 0.9249781966209412}]}, {"text": "Note that, all dialogue rankers are tested on the 400-instance balanced test set described in.", "labels": [], "entities": []}, {"text": "We train 10 models on randomly sampled training sets and report average performance.", "labels": [], "entities": []}, {"text": "We use ranking metrics for evaluation: \u2022 P@K -Precision at k, corresponds to the number of problematic dialogues in the top k ranked options.", "labels": [], "entities": [{"text": "Precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9780846834182739}]}, {"text": "\u2022 R@K -Recall at k, corresponds to the number of problematic dialogues in the top k ranked options against the number of all problematic dialogues in the test set (i.e. 200).", "labels": [], "entities": [{"text": "R", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.975468099117279}, {"text": "Recall", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.6762660145759583}]}, {"text": "Note that we modified the standard of Recall at k to get monotonic increase with respect to k.", "labels": [], "entities": [{"text": "Recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.8602342009544373}]}], "tableCaptions": [{"text": " Table 2: The number of dialogues and average dialogue  length in the log dialogue corpus.", "labels": [], "entities": []}, {"text": " Table 3: Log dialogues are randomly sampled and split  into training, validation and test sets.", "labels": [], "entities": [{"text": "Log dialogues", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.8995920121669769}]}, {"text": " Table 4: Evaluation results. DB, PM, MD, FT, SF  and SG stand for the DBDC3, PlainMultiDomain,  MultiDomain, FineTune, StepFineTune and  StepGAN approach, respectively. The Sup denotes the  supervised approach trained on the balanced labelled  dialogues.", "labels": [], "entities": [{"text": "DB", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9117292165756226}, {"text": "FT", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9491628408432007}]}]}