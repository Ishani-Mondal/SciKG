{"title": [{"text": "Toward Dialogue Modeling: A Semantic Annotation Scheme for Questions and Answers", "labels": [], "entities": [{"text": "Dialogue Modeling", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7374236583709717}]}], "abstractContent": [{"text": "The present study proposes an annotation scheme for classifying the content and discourse contribution of question-answer pairs.", "labels": [], "entities": []}, {"text": "We propose detailed guidelines for using the scheme and apply them to dialogues in En-glish, Spanish, and Dutch.", "labels": [], "entities": []}, {"text": "Finally, we report on initial machine learning experiments for automatic annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Question-answer pair (QAP) labeling is the problem of characterizing the content and discourse contribution of questions and answers using a small but maximally informative tagset that can be consistently applied by both human annotators and NLP systems.", "labels": [], "entities": [{"text": "Question-answer pair (QAP) labeling", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6465419183174769}, {"text": "characterizing the content and discourse contribution of questions and answers", "start_pos": 54, "end_pos": 132, "type": "TASK", "confidence": 0.730288428068161}]}, {"text": "QAP labeling has many potential use cases, for example as a preprocessing step for dialogue modeling systems or for chatbots.", "labels": [], "entities": [{"text": "QAP labeling", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7741339206695557}, {"text": "dialogue modeling", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.8636623024940491}]}, {"text": "The problem is not new: in the NLP literature, different aspects of QAP tagging have been addressed in the context of question answering systems (), question generation systems (e.g., and dialogue act classification (e.g.).", "labels": [], "entities": [{"text": "QAP tagging", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.9473900496959686}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.8764148652553558}, {"text": "question generation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.8948761522769928}, {"text": "dialogue act classification", "start_pos": 188, "end_pos": 215, "type": "TASK", "confidence": 0.6447465022404989}]}, {"text": "However, we see several gaps in the literature: existing approaches to QAP classification often do not cover the full range of questions and answers found inhuman dialogues and are limited in the types of semantic information that they cover.", "labels": [], "entities": [{"text": "QAP classification", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9869734346866608}]}, {"text": "To address these issues, we propose anew annotation scheme that was developed based on corpora of natural conversations in several languages (English, Spanish, and Dutch) and provides several layers of annotations for QAPs.", "labels": [], "entities": []}, {"text": "Notably, where applicable, we annotate the semantic role of the questioned constituent in questions and their corresponding answer (e.g. 'Does she live in Paris or London?'", "labels": [], "entities": []}, {"text": "\u21d2 LOCATION), which we believe is an informative, yet easy definable way of globally characterizing the content of a QAP.", "labels": [], "entities": [{"text": "LOCATION", "start_pos": 2, "end_pos": 10, "type": "METRIC", "confidence": 0.9871270656585693}, {"text": "globally characterizing the content of a QAP", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.7636576550347465}]}, {"text": "Our paper has two main contributions: the annotation scheme itself (section 3) and two ways of applying it to real data.", "labels": [], "entities": []}, {"text": "We developed detailed and explicit guidelines for human annotators, and tested these on corpus data (section 4.1).", "labels": [], "entities": []}, {"text": "Additionally, we started experimenting with machine learning approaches for automating part of the annotation process (section 4.2).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we discuss our experiments with applying the scheme manually (section 4.1) and using machine learning techniques (section 4.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Cohen's Kappa score (\u03ba) and observed agree- ment (A o ) for gold standard dialogue.", "labels": [], "entities": [{"text": "Kappa score (\u03ba)", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.840188455581665}, {"text": "observed agree- ment (A o )", "start_pos": 38, "end_pos": 65, "type": "METRIC", "confidence": 0.9319193065166473}]}, {"text": " Table 5: Extracted features for the classification task", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix of decision tree prediction.  Testing data set, 184 questions.", "labels": [], "entities": [{"text": "decision tree prediction", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.7964794238408407}]}]}