{"title": [{"text": "How to Parse Low-Resource Languages: Cross-Lingual Parsing, Target Language Annotation, or Both?", "labels": [], "entities": [{"text": "Parse Low-Resource Languages", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.8580012122790018}]}], "abstractContent": [{"text": "To develop a parser fora language with no syntactically annotated data, we either have to develop a (small) treebank for the target language or rely on cross-lingual learning or projection, or possibly use some combination of these methods.", "labels": [], "entities": []}, {"text": "In this paper, we compare the usefulness of cross-lingual model transfer and target language annotation for three different languages, with varying support from closely related high-resource languages.", "labels": [], "entities": [{"text": "cross-lingual model transfer", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.6508133312066396}]}, {"text": "The results show that annotating even a very small amount of data in the target language is superior to any cross-lingual setup and that accuracy can be further improved by adding training data from related languages in a multilingual model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9989186525344849}]}], "introductionContent": [{"text": "Despite significant advances in natural language processing over several decades, even basic technologies like part-of-speech tagging and syntactic parsing are still available only fora tiny fraction of the languages of the world.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7130649238824844}, {"text": "syntactic parsing", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.7127267569303513}]}, {"text": "This observation has led to an increasing interest in techniques for supporting low-resource languages, typically by making use of data from high-resource languages together with methods for cross-lingual learning or transfer.", "labels": [], "entities": []}, {"text": "These techniques include annotation projection (), model transfer (), treebank translation, and multilingual parsing models (.", "labels": [], "entities": [{"text": "annotation projection", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.715342253446579}, {"text": "model transfer", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.7237274497747421}, {"text": "treebank translation", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.731178343296051}]}, {"text": "Despite the undeniable progress in this line of research, the question always looms large whether it is not more effective to simply annotate a small amount of training data in the target language of interest.", "labels": [], "entities": []}, {"text": "Daniel Zeman, one of the inventors of delexicalized transfer parsing, maintains that you can get over 50% accuracy for many languages with just 100 annotated sentences, citing as evidence the results of Ramasamy (2014) for some Indian languages.", "labels": [], "entities": [{"text": "delexicalized transfer parsing", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6702285408973694}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9988780617713928}]}, {"text": "Further support comes from the study of, who compares cross-lingual parsing to target language annotation in the context of building a treebank for Galician.", "labels": [], "entities": [{"text": "cross-lingual parsing", "start_pos": 54, "end_pos": 75, "type": "TASK", "confidence": 0.689815491437912}]}, {"text": "In this paper, we approach this question by comparing three ways of training dependency parsers for low-resource languages: monolingual models trained on small amounts of target language data; crosslingual models trained only on data from related support languages; and multilingual models trained on both support and target language data.", "labels": [], "entities": []}, {"text": "We perform experiments on three target languages with varying support from related high-resource languages: Faroese (supported by Danish, Norwegian, and Swedish), Upper Sorbian (supported by Czech, Polish, and Slovak), and North Saami (supported by Estonian, Finnish, and Hungarian).", "labels": [], "entities": []}, {"text": "Our results show that monolingual models consistently outperform crosslingual models even with very limited amounts of training data.", "labels": [], "entities": []}, {"text": "In addition, there is always a multilingual model that outperforms the best monolingual model.", "labels": [], "entities": []}, {"text": "Taken together, these results suggest that the most effective strategy for low-resource parser development may well be to annotate as much data as you can afford in the target language and then add training data from related languages if available.", "labels": [], "entities": [{"text": "low-resource parser development", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.6304139892260233}]}], "datasetContent": [{"text": "Within each cluster, we train cross-lingual models on data from every combination of one, two or three support languages (7 models), multilingual models on the same data sets plus target language data (7 models), and a monolingual model only on target language data, fora total of 15 models.", "labels": [], "entities": []}, {"text": "For the support languages, we only use the dedicated training sets from Universal Dependencies v2.3 (Nivre et al., 2018).", "labels": [], "entities": []}, {"text": "We do not standardize training set sizes, since the parser has been shown to be robust to size differences when training multi-treebank models (Stymne et al., 2018), but we limit the size of the Czech training set (which is about four times bigger than any other) to 300k tokens.", "labels": [], "entities": [{"text": "Czech training set", "start_pos": 195, "end_pos": 213, "type": "DATASET", "confidence": 0.8195119500160217}]}, {"text": "For the target languages, we need a training set for the mono-and multilingual models, a development set to tune hyper-parameters, and a test set for the final evaluation.", "labels": [], "entities": []}, {"text": "For Faroese and Upper Sorbian, there is only about 10k tokens of data, which we subdivide into 50% training, 25% development, and 25% test.", "labels": [], "entities": []}, {"text": "For North Saami, there is more data, so we leave the dedicated test set of 10k tokens intact and extract a development set of 2.5k tokens from the training set, leaving 14.3k words for training.", "labels": [], "entities": []}, {"text": "The development sets for target languages are used for model selection as follows: \u2022 For support languages with two treebanks of roughly equal size, we run preliminary experiments with cross-lingual models to decide whether to use both or only one.", "labels": [], "entities": [{"text": "model selection", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.7142263352870941}]}, {"text": "The resulting selection can be seen in.", "labels": [], "entities": []}, {"text": "\u2022 To improve compatibility of character-based representations across (support and target) languages, we try mapping characters that exist only in a target language to characters that exist in one or more support language.", "labels": [], "entities": []}, {"text": "This is helpful only for Faroese, where we map {\u00cd\u00da\u00fd\u00fa\u00f0\u00ed} to {IUyudi}.", "labels": [], "entities": []}, {"text": "\u2022 For cross-lingual models, the parser does not learn a language embedding for the target language, so we select a support language to use as proxy during parsing based on LAS on the target language development set.", "labels": [], "entities": [{"text": "LAS", "start_pos": 172, "end_pos": 175, "type": "METRIC", "confidence": 0.926639199256897}]}, {"text": "\u2022 All models are trained for 30 epochs, and the best epoch is selected according to LAS on the target language development set.", "labels": [], "entities": [{"text": "LAS", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9969538450241089}]}, {"text": "Finally, the development sets are also used in learning curve experiments (see Section 3).", "labels": [], "entities": []}, {"text": "reports results on the test sets for all cross-lingual, multilingual and monolingual models on our three target languages, both labeled attachment score (LAS) and unlabeled attachment score (UAS).", "labels": [], "entities": [{"text": "attachment score (LAS)", "start_pos": 136, "end_pos": 158, "type": "METRIC", "confidence": 0.8956675291061401}, {"text": "unlabeled attachment score (UAS)", "start_pos": 163, "end_pos": 195, "type": "METRIC", "confidence": 0.7647981941699982}]}, {"text": "The first thing to note is that the monolingual models, trained only on about 5k tokens for Faroese and Upper Sorbian and 14k tokens for North Saami, consistently outperform all cross-lingual models by a wide margin.", "labels": [], "entities": []}, {"text": "The difference is especially large for the Uralic cluster, where the target language is in a different branch of the language family from all support languages, and where the best cross-lingual model does not even reach 10% for LAS (25% for UAS).", "labels": [], "entities": []}, {"text": "But even for the Scandinavian and West Slavic clusters, where languages are more closely related and the best cross-lingual models get LAS over 40% and UAS over 50%, the monolingual model gives a higher LAS score by at least 15% absolute (12% absolute for UAS).", "labels": [], "entities": [{"text": "LAS", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9948544502258301}, {"text": "UAS", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.8239557147026062}, {"text": "LAS score", "start_pos": 203, "end_pos": 212, "type": "METRIC", "confidence": 0.9602974653244019}]}, {"text": "This indicates that annotating a relatively small amount of training data in the target language is generally superior to using cross-lingual model transfer.", "labels": [], "entities": []}, {"text": "The second main trend is that, despite the poor results for cross-lingual models, the best multilingual models consistently outperform the monolingual models.", "labels": [], "entities": []}, {"text": "For the Scandinavian and West Slavic clusters, all multilingual models outperform the monolingual model and the best model improves by as much as 5.9/6.9 LAS and 5.3/6.5 UAS.", "labels": [], "entities": [{"text": "LAS", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9935222268104553}, {"text": "UAS", "start_pos": 170, "end_pos": 173, "type": "METRIC", "confidence": 0.9074788093566895}]}, {"text": "But even for the Uralic cluster, where data from the related support languages seem completely useless in the cross-lingual scenario, using the same data in a multilingual model improves on the monolingual model in 3 out of 7 cases for UAS (2 out of 7 for LAS).", "labels": [], "entities": [{"text": "UAS", "start_pos": 236, "end_pos": 239, "type": "DATASET", "confidence": 0.6856307983398438}]}, {"text": "The relative improvement for the best multilingual model is smaller than in the other two clusters, but it should be kept in mind that the target language training set is almost three times bigger for North Saami than for Faroese and Upper Sorbian.", "labels": [], "entities": []}, {"text": "These results suggest that, even if target language annotation is more effective than cross-lingual transfer, adding data from related support languages can nevertheless lead to further improvements.", "labels": [], "entities": [{"text": "cross-lingual transfer", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7065044492483139}]}, {"text": "To understand why multilingual models work so much better than cross-lingual models, it is important to note that the former learn word, character and language embeddings for the target language and that these embeddings are learned together for all languages.", "labels": [], "entities": []}, {"text": "The cross-lingual models have no target language specific representations and have to rely on a proxy language embedding and the existence of cognates for matching word and character representations.", "labels": [], "entities": []}, {"text": "This works especially poorly for the Uralic cluster, where the distance from the target to the support languages is much larger.", "labels": [], "entities": [{"text": "Uralic cluster", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8186465501785278}]}, {"text": "So how much target language data do we need to outperform a cross-lingual model?", "labels": [], "entities": []}, {"text": "To answer this question, we run learning curve experiments for the monolingual and best multilingual models, using the development sets for evaluation, and gradually increasing the amount of target language training data from 0 to 50, 100, 500, 1k, 3k, 5k and 10k tokens).", "labels": [], "entities": []}, {"text": "For the Scandinavian and Uralic clusters, we only need 1k tokens for the monolingual model to surpass the cross-lingual model with respect to LAS.", "labels": [], "entities": []}, {"text": "For the West Slavic cluster, results are slightly erratic for the smallest training sets, but 3k tokens definitely suffice to reach the accuracy of the best cross-lingual model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9993802309036255}]}, {"text": "In all three cases, this is less than 200 sentences, 2 so the results seem to support Daniel Zeman's claim that something like 100 sentences can be sufficient to train a decent parser, although in our study it is only Faroese that reaches a (labeled) accuracy of 50% with only 100 sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 251, "end_pos": 259, "type": "METRIC", "confidence": 0.9585576057434082}]}], "tableCaptions": [{"text": " Table 2: Test set accuracy for target languages (UAS, LAS). \u2212Target = cross-lingual models trained  without target language data. +Target = models trained on target language data; monolingual (first row)  and multilingual.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9830779433250427}]}]}