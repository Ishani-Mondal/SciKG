{"title": [{"text": "NTT's Machine Translation Systems for WMT19 Robustness Task", "labels": [], "entities": [{"text": "NTT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8550248742103577}, {"text": "Machine Translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.703390821814537}, {"text": "WMT19 Robustness", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.6051206588745117}]}], "abstractContent": [{"text": "This paper describes NTT's submission to the WMT19 robustness task.", "labels": [], "entities": [{"text": "NTT", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.6110615730285645}, {"text": "WMT19 robustness task", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7928581833839417}]}, {"text": "This task mainly fo-cuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news.", "labels": [], "entities": [{"text": "translating noisy text (e.g., posts on Twitter)", "start_pos": 29, "end_pos": 76, "type": "TASK", "confidence": 0.8480915427207947}]}, {"text": "Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7045610994100571}]}, {"text": "Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 212, "end_pos": 220, "type": "METRIC", "confidence": 0.8202059268951416}]}], "introductionContent": [{"text": "This paper describes NTT's submission to the WMT 2019 robustness task (.", "labels": [], "entities": [{"text": "NTT", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.7118621468544006}, {"text": "WMT 2019 robustness task", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6493806093931198}]}, {"text": "This year, we participated in English-to-Japanese (EnJa) and Japanese-to-English (Ja-En) translation tasks with a constrained setting, i.e., we used only the parallel and monolingual corpora provided by the organizers.", "labels": [], "entities": [{"text": "Japanese-to-English (Ja-En) translation tasks", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.646340012550354}]}, {"text": "The task focuses on the robustness of Machine Translation (MT) to noisy text that can be found on social media (e.g., Reddit, Twitter).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.8620323896408081}]}, {"text": "The task is more challenging than atypical machine translation task like the news translation tasks) due to the characteristics of noisy text and the lack of a publicly available parallel corpus.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8122831583023071}, {"text": "news translation", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.7015464156866074}]}, {"text": "shows example comments from Reddit, a discussion website.", "labels": [], "entities": []}, {"text": "Text on social media usually contains various noise such as (1) abbreviations, (2) grammatical errors, (3) misspellings, (4) emojis, and (5) emoticons.", "labels": [], "entities": []}, {"text": "In addition, most provided parallel corpora are not related to our target domain, \u21e4 Equal contribution.", "labels": [], "entities": []}, {"text": "(1) I'll let you know bro, thx (2) She had a ton of rings.", "labels": [], "entities": [{"text": "I", "start_pos": 4, "end_pos": 5, "type": "METRIC", "confidence": 0.9718566536903381}]}, {"text": "(3) oh my god it's beatiful (4) Thank you so much for all your advice!!", "labels": [], "entities": []}, {"text": "(5) (\\ \u21e4\u00b48`\u21e4\u21e4\u00b48\u21e4\u00b48`\u21e4 ) so cute and the amount of in-domain parallel corpus is still limited as compared with parallel corpora used in the typical MT tasks (.", "labels": [], "entities": [{"text": "MT tasks", "start_pos": 146, "end_pos": 154, "type": "TASK", "confidence": 0.9402563571929932}]}, {"text": "To tackle this non-standard text translation with a low-resource setting, we mainly use the following techniques.", "labels": [], "entities": [{"text": "text translation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7289852797985077}]}, {"text": "First, we incorporated a placeholder mechanism () to correctly copy special tokens such as emojis and emoticons that frequently appears in social media.", "labels": [], "entities": []}, {"text": "Second, to cope with the problem of the low-resource corpus and to effectively use the monolingual corpus, we created a synthetic corpus from a target-side monolingual corpus with a target-to-source translation model.", "labels": [], "entities": []}, {"text": "Lastly, we fine-tuned our translation model with the synthetic and in-domain parallel corpora for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.6935446709394455}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we present a detailed overview of our systems.", "labels": [], "entities": []}, {"text": "Section 3 shows experimental settings and main results, and Section 4 provides an analysis of our systems.", "labels": [], "entities": []}, {"text": "Finally, Section 5 draws a brief conclusion of our work for the WMT19 robustness task.", "labels": [], "entities": [{"text": "WMT19 robustness task", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.7192877531051636}]}], "datasetContent": [{"text": "We used the Transformer model with six blocks.", "labels": [], "entities": []}, {"text": "Our model hyper-parameters are based on transformer_base settings, where the word embedding dimensions, hidden state dimensions, feedforward dimensions and number of heads are 512, 512, 2048, and 8, respectively.", "labels": [], "entities": []}, {"text": "The model shares https://unicode.org/emoji/charts 8 https://github.com/taishi-i/nagisa the parameter of the encoder/decoder word embedding layers and the decoder output layer by three-way-weight-tying.", "labels": [], "entities": []}, {"text": "Each layer is connected with a dropout probability of 0.3 ().", "labels": [], "entities": []}, {"text": "For an optimizer, we used Adam () with a learning rate of 0.001, \ud97b\udf59 1 = 0.9, \ud97b\udf59 2 = 0.98.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9460321068763733}]}, {"text": "We use a root-square decay learning rate schedule with a linear warmup of 4000 steps).", "labels": [], "entities": []}, {"text": "We applied mixed precision training that makes use of GPUs more efficiently for faster training (.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9797070026397705}]}, {"text": "Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update.", "labels": [], "entities": []}, {"text": "We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models . Training took about 1.5 days to converge with four NVIDIA V100 GPUs.", "labels": [], "entities": []}, {"text": "We compute case-sensitive BLEU scores () for evaluating translation quality . All our implementations are based on the fairseq 11 toolkit.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.9752001166343689}, {"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.96734219789505}, {"text": "fairseq 11 toolkit", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.8987038731575012}]}, {"text": "After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data.", "labels": [], "entities": []}, {"text": "During fine-tuning, we used almost the same settings as the initial training setup except we changed the model save interval to every three iterations and continued the learning rate decay schedule.", "labels": [], "entities": []}, {"text": "For fine-tuning, we trained the model for 50 iterations, which took less than 10 minutes with four GPUs.", "labels": [], "entities": []}, {"text": "When decoding, we used abeam search with the size of six and a length normalization technique with \u21b5 = 2.0 and \ud97b\udf59 = 0.0 (.", "labels": [], "entities": []}, {"text": "For the submission, we used an ensemble of three (En-Ja) or four (Ja-En) independently trained models 12 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The number of training sentences and words  on the English side contained in the provided parallel  corpora.", "labels": [], "entities": []}, {"text": " Table 4: Case-sensitive BLEU scores of provided blind test sets. The numbers in the brackets show the improve- ments from the baseline model.", "labels": [], "entities": [{"text": "Case-sensitive", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.903572678565979}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8878619074821472}, {"text": "improve- ments", "start_pos": 103, "end_pos": 117, "type": "METRIC", "confidence": 0.7951747179031372}]}]}