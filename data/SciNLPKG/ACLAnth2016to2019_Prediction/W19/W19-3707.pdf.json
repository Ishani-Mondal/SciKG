{"title": [{"text": "Data Set for Stance and Sentiment Analysis from User Comments on Croatian News", "labels": [], "entities": [{"text": "Stance and Sentiment Analysis", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.9076739251613617}, {"text": "Croatian News", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.8364942073822021}]}], "abstractContent": [{"text": "Nowadays it is becoming more important than ever to find new ways of extracting useful information from the evergrowing amount of user-generated data available online.", "labels": [], "entities": []}, {"text": "In this paper, we describe the creation of a data set that contains news articles and corresponding comments from Croatian news outlet 24 sata.", "labels": [], "entities": [{"text": "Croatian news outlet 24 sata", "start_pos": 114, "end_pos": 142, "type": "DATASET", "confidence": 0.8200926780700684}]}, {"text": "Our annotation scheme is specifically tailored for the task of detecting stances and sentiment from user comments as well as assessing if commentator claims are verifiable.", "labels": [], "entities": [{"text": "detecting stances and sentiment from user comments", "start_pos": 63, "end_pos": 113, "type": "TASK", "confidence": 0.7987884453364781}]}, {"text": "Through this data, we hope to get a better understanding of the publics viewpoint on various events.", "labels": [], "entities": []}, {"text": "In addition, we also explore the potential of applying supervised machine learning models to automate annotation of more data.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the world of unceasing connectedness there is a constant surge of user-generated data online.", "labels": [], "entities": []}, {"text": "On news outlets a multitude of opinions and reactions are present.", "labels": [], "entities": []}, {"text": "Such amounts of data are too large to analyze manually.", "labels": [], "entities": []}, {"text": "On the other hand, automated analysis of this data is difficult due to its inherently unstructured nature.", "labels": [], "entities": []}, {"text": "Models that could automatically and efficiently extract structured information from large amounts of data would save time, energy and yield valuable information.", "labels": [], "entities": []}, {"text": "We propose a structured annotation scheme that labels claim verifiability, stance, and sentiment on news outlets.", "labels": [], "entities": []}, {"text": "Information about stance, can provide an overview of public opinions and information about currently favorable political movements.", "labels": [], "entities": []}, {"text": "Furthermore, claim verifiability can help the fight against fake news, as automated verifiability detection could bring forth claims that are not verifiable and that could potentially be just a rumor or simply made up.", "labels": [], "entities": []}, {"text": "Moreover, the data set could be analyzed in search of interactions between the labels.", "labels": [], "entities": []}, {"text": "The contribution of this paper is twofold.", "labels": [], "entities": []}, {"text": "First, we create a data set of user comments on news in Croatian annotated with claim verifiability, stance, and sentiment.", "labels": [], "entities": []}, {"text": "Second, we perform preliminary experiments with several machine learning models on this data set.", "labels": [], "entities": []}, {"text": "We present a general overview of the entire data set creation process with caveats and experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each label, we split the data into a train, dev, and test portions.", "labels": [], "entities": []}, {"text": "The splits are disjunctive with respect to the articles, meaning that comments cor-   responding to the same article are all in the same split.", "labels": [], "entities": []}, {"text": "Furthermore, as the data set is highly imbalanced, we perform the splits in a stratified manner, ensuring the ratio of positive and negative examples is roughly equal for train, dev, and test.", "labels": [], "entities": []}, {"text": "Through this, we have ensured that all of our splits contain positive examples.", "labels": [], "entities": []}, {"text": "However, an imbalance that can hurt model performance was still present in the train data.", "labels": [], "entities": []}, {"text": "To alleviate this issue we artificially balanced the train set by oversampling positive examples until the number of positive and negative examples was equal.", "labels": [], "entities": []}, {"text": "This was done for all labels as positive examples were always the minority.", "labels": [], "entities": []}, {"text": "For different labels, we had different splits.", "labels": [], "entities": []}, {"text": "However, for each label, the same (artificially balanced) train, dev, and test sets were used for all models.", "labels": [], "entities": []}, {"text": "In we can seethe split through the labels.", "labels": [], "entities": []}, {"text": "For train we have counted in artificially examples thus the sum through columns isn't the same.", "labels": [], "entities": []}, {"text": "We train all models on the train set, optimize hyperparameters on the dev set and report results on the test set.", "labels": [], "entities": []}, {"text": "Some preliminary results are given in as F1 score for each label along with statistical significance tests (we used a permutation test on test set predictions).", "labels": [], "entities": [{"text": "F1 score", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9813309013843536}]}, {"text": "Performance on most labels is rather low, indicating the task is highly complex.", "labels": [], "entities": []}, {"text": "In most cases, adding features to the baseline model improved performance.", "labels": [], "entities": []}, {"text": "For labels Verifiable, Non-Verifiable the differences are statistically significant.", "labels": [], "entities": []}, {"text": "On the other hand, on the Negative label the SVM baseline is the overall best model.", "labels": [], "entities": [{"text": "Negative label", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8294952809810638}, {"text": "SVM baseline", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.6958103477954865}]}, {"text": "The deep learning approaches were not expected to be very good, as the data set is small, but they do provide some respectable results, mostly for the classes from the Claim group.", "labels": [], "entities": [{"text": "Claim group", "start_pos": 168, "end_pos": 179, "type": "DATASET", "confidence": 0.9019011855125427}]}], "tableCaptions": [{"text": " Table 2: Results of classifiers across all labels. The best result for each label is given in bold. Entries in italic  represent results that are statistically significantly better than the SVM baseline from the first row.", "labels": [], "entities": []}]}