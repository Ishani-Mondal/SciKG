{"title": [{"text": "GEval: Tool for Debugging NLP Datasets and Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a simple but general and effective method to debug the output of machine learning (ML) supervised models, including neural networks.", "labels": [], "entities": []}, {"text": "The algorithm looks for features that lower the evaluation metric in such away that it cannot be ascribed to chance (as measured by their p-values).", "labels": [], "entities": []}, {"text": "Using this method-implemented as GEval tool-you can find: (1) anomalies in test sets, (2) issues in preprocessing, (3) problems in the ML model itself.", "labels": [], "entities": []}, {"text": "It can give you an insight into what can be improved in the datasets and/or the model.", "labels": [], "entities": []}, {"text": "The same method can be used to compare ML models or different versions of the same model.", "labels": [], "entities": []}, {"text": "We present the tool, the theory behind it and use cases for text-based models of various types.", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently, given the burden of big data and possibilities to build a wide variety of deep learning models, the need to understand datasets, intrinsic parameters and model behavior is growing.", "labels": [], "entities": []}, {"text": "These problems are part of the interpretability trend in the state-of-the-art research, the good example being publications at NeurIPS 2018 conference and its Interpretability and Robustness in Audio, Speech, and Language Workshop.", "labels": [], "entities": []}, {"text": "The problem of interpretability is also crucial in terms of using ML models in business cases and applications.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.9746023416519165}, {"text": "ML", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.965837299823761}]}, {"text": "Every day, data scientists analyze large amounts of data, build models and sometimes they just do not understand: why the models work in a certain way.", "labels": [], "entities": []}, {"text": "Thus, we need fast and efficient tools to look into models in their various aspects, e.g. by analyzing train and test data, the way in which models influence their results, and how their internal features interact with each other.", "labels": [], "entities": []}, {"text": "Consequently, the aim of our research and paper is to 1 https://irasl.gitlab.io/ present a tool to help data scientists understand the model and find issues in order to improve the process.", "labels": [], "entities": []}, {"text": "The tool will be show-cased on a number of NLP challenges.", "labels": [], "entities": []}, {"text": "There area few extended reviews on interpretability techniques and their types available at (.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.9616273641586304}]}, {"text": "The authors also introduce purposes of interpretability research: justify models, control them and their changes in time (model debugging), improve their robustness and efficiency (model validation), discover weak features (new knowledge discovery).", "labels": [], "entities": []}, {"text": "The explanations can be given as: (1) other models easier to understand (e.g. linear regression), (2) sets of rules, (3) lists of strong and weak input features or even (4) textual summaries accessible for humans.", "labels": [], "entities": []}, {"text": "The interpretability techniques are categorized into global or local methods.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.9652929306030273}]}, {"text": "\"Global\" stands for techniques that can explain/interpret a model as a whole, whereas \"local\" stands for methods and models that can be interpreted around any chosen neighborhood.", "labels": [], "entities": []}, {"text": "Another dimensions of the interpretability categorization are: (1) intrinsic interpretable methods, i.e. models that approximate the more difficult ones and are also easy to understand for humans or (2) post-hoc explanations that are derived after training models.", "labels": [], "entities": []}, {"text": "Hence, explanations can be model-specific or model-agnostic, i.e. needing (or not) the knowledge about the model itself.", "labels": [], "entities": []}, {"text": "As far as model-agnostic (black-box) methods are concerned, one of the breakthroughs in the domain was the LIME method (Local Interpretable Model-Agnostic Explanations).", "labels": [], "entities": [{"text": "LIME", "start_pos": 107, "end_pos": 111, "type": "METRIC", "confidence": 0.9746164679527283}]}, {"text": "LIME requires access to a model and it changes the analyzed dataset many times (doing perturbations) by removing some features from input samples and measuring changes in the model output.", "labels": [], "entities": []}, {"text": "The idea has two main drawbacks.", "labels": [], "entities": []}, {"text": "The first is that it requires access to the model to know the model output for perturbed samples.", "labels": [], "entities": []}, {"text": "The other disadvantage is that it takes a very longtime to process big datasets, which makes the method unfeasible in case of really large datasets, e.g. several millions of text documents.", "labels": [], "entities": []}, {"text": "Other interpretability methods concern the internal model structure in a white-box manner, e.g. L2X, which instruments a deep learning model with an extra unit (layer) and the analyzed model is trained with this unit jointly.", "labels": [], "entities": []}, {"text": "We introduce an automatic, easy to use and a model-agnostic method that does not require access to models.", "labels": [], "entities": []}, {"text": "The only requirement is access to the dataset, i.e. input sample data points, model results and gold standard labels.", "labels": [], "entities": []}, {"text": "The method (and a command-line tool), called GEval, 2 is based on statistical hypothesis testing and measuring the significance of each feature.", "labels": [], "entities": []}, {"text": "GEval finds global features that \"influence\" the model evaluation score in a bad way and worsen its results.", "labels": [], "entities": [{"text": "GEval", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8480083346366882}]}, {"text": "Moreover, we present the work of GEval using examples from various text-based model types, i.e. named entity recognition, classification and translation.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6411904990673065}]}, {"text": "In the following sections we introduce the idea to use p-value and hypotheses testing to debug ML models (Section 2), describe the algorithm behind GEval (Section 3), and then show some use cases for state-of-the-art ML models (Section 4).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Example of words from Twitter classification  (see Section 4) task with their statistical properties", "labels": [], "entities": [{"text": "Twitter classification", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.6949387937784195}]}, {"text": " Table 2: GEval feature listing for classification for sen- timent analysis on Twitter dataset. We used output  from model ULMFiT with 0.86 total accuracy on the  chosen validation set. \"Acc\" stands for the average ac- curacy for tweets with a given feature. Labels for pos- itive sentiment are \"1\", i.e. in feature names \"exp:1\" ,  and for negative sentiment -\"0\".", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 79, "end_pos": 94, "type": "DATASET", "confidence": 0.762170821428299}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9916837215423584}, {"text": "Acc", "start_pos": 187, "end_pos": 190, "type": "METRIC", "confidence": 0.9902305006980896}]}, {"text": " Table 3: Comparison of machine translation models:  LIUM and UEDin -features worsening UEDin (WMT- 17 new task test data).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7527566254138947}, {"text": "LIUM", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.7814157009124756}, {"text": "WMT- 17 new task test data", "start_pos": 95, "end_pos": 121, "type": "DATASET", "confidence": 0.8283600722040448}]}]}