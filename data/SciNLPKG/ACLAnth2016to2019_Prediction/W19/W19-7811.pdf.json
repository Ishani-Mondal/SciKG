{"title": [{"text": "tweeDe -A Universal Dependencies treebank for German tweets", "labels": [], "entities": [{"text": "Universal Dependencies treebank", "start_pos": 10, "end_pos": 41, "type": "DATASET", "confidence": 0.5974190632502238}]}], "abstractContent": [{"text": "We introduce the first German treebank for Twitter microtext, annotated within the framework of Universal Dependencies.", "labels": [], "entities": []}, {"text": "The new treebank includes over 12,000 tokens from over 500 tweets, independently annotated by two human coders.", "labels": [], "entities": []}, {"text": "In the paper, we describe the data selection and annotation process and present baseline parsing results for the new testsuite.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent years have seen an increasing interest in developing robust NLP applications for data from different language varieties and domains.", "labels": [], "entities": []}, {"text": "The Universal Dependencies (UD) project has inspired the creation of many new datasets for dependency parsing in a multilingual setting.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8514236211776733}]}, {"text": "Treebanks have been created for low-resourced languages such as Bambara, Erzya, or Kurmanji as well as for many new domains, genres and language varieties for which no annotated data was yet available.", "labels": [], "entities": []}, {"text": "A casein point are web genres, spoken discourse, literary prose, historical data or data from social media.", "labels": [], "entities": []}, {"text": "We contribute to the creation of new resources for different language varieties and introduce tweeDe, anew German UD Twitter treebank.", "labels": [], "entities": [{"text": "German UD Twitter treebank", "start_pos": 107, "end_pos": 133, "type": "DATASET", "confidence": 0.8947414308786392}]}, {"text": "TweeDe has a size of over 12,000 tokens, annotated with PoS, morphological features and syntactic dependencies.", "labels": [], "entities": [{"text": "TweeDe", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9019328951835632}]}, {"text": "TweeDe is different from existing German UD treebanks as its content focusses on private communication.", "labels": [], "entities": [{"text": "TweeDe", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8923560380935669}, {"text": "UD treebanks", "start_pos": 41, "end_pos": 53, "type": "DATASET", "confidence": 0.7752197980880737}]}, {"text": "Private tweets share many properties of spoken language.", "labels": [], "entities": []}, {"text": "They are often highly informal and not carefully edited, often lack punctuation and can include ungrammatical structures.", "labels": [], "entities": []}, {"text": "In addition, the data often includes spelling errors and a creative use of language that results in a high number of unknown words.", "labels": [], "entities": []}, {"text": "These properties make user-generated microtext a challenging test case for parser evaluation.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.9142688512802124}]}, {"text": "In the paper, we describe the creation of tweeDe, including data selection, preprocessing and the annotation process.", "labels": [], "entities": [{"text": "data selection", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.6971769481897354}]}, {"text": "We report inter-annotator agreement for the syntactic annotations ( \u00a72) and discuss some of the decisions that we have made during annotation ( \u00a73).", "labels": [], "entities": []}, {"text": "We compare tweeDe to other treebanks in \u00a74.", "labels": [], "entities": []}, {"text": "In \u00a75 we present baseline parsing results for the new treebank.", "labels": [], "entities": []}, {"text": "Finally, we put our work into context ( \u00a76) and outline avenues for future work ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "We present parsing baselines for the new German UD treebank, using the state-of-the-art parser of.", "labels": [], "entities": [{"text": "German UD treebank", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.8650795817375183}]}, {"text": "The parser is a neural dependency parser that learns complex, non-linear representations directly from the input text, based on bidirectional LSTMs.", "labels": [], "entities": []}, {"text": "It only considers local context and predicts attachments and labels in a greedy fashion.", "labels": [], "entities": [{"text": "predicts attachments and labels", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.8171431869268417}]}, {"text": "The huge success of the parser is based on its use of biaffine attention.", "labels": [], "entities": []}, {"text": "In our first experiment, we train the parser on the 250 tweets in the tweeDe training set.", "labels": [], "entities": [{"text": "tweeDe training set", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.8077884912490845}]}, {"text": "We use pretrained skipgram embeddings with 100 dimensions (window size: 5, min word count: 10), trained on a large collection of German tweets, collected in a time period from 2013 to 2017.", "labels": [], "entities": []}, {"text": "The embeddings are publically available from https://www.cl.uni-heidelberg.de/research/downloads.", "labels": [], "entities": []}, {"text": "All models have been trained with default parameters.", "labels": [], "entities": []}, {"text": "(left) shows results for gold PoS and for automatically predicted PoS tags.", "labels": [], "entities": []}, {"text": "Using UD PoS tags for parsing outperforms the STTS tags by a large margin, probably due to sparsity caused by the more fine-grained STTS.", "labels": [], "entities": [{"text": "UD PoS tags", "start_pos": 6, "end_pos": 17, "type": "DATASET", "confidence": 0.7308308084805807}, {"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9754664897918701}]}, {"text": "Feeding both, UD and STTS tags, to the parser can further increase results, but only slightly (less than 1%).", "labels": [], "entities": [{"text": "UD", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.6906505227088928}]}, {"text": "Most surprisingly, we obtain higher results when using automatically predicted STTS tags (as compared to using gold STTS tags).", "labels": [], "entities": []}, {"text": "This observation, however, is more pronounced for the test set and might not be representative, being an artefact of the small data size.", "labels": [], "entities": []}, {"text": "Results for training on the small tweeDe dataset only are in the range of 74% LAS (gold PoS) and 68% LAS (auto PoS).", "labels": [], "entities": [{"text": "tweeDe dataset", "start_pos": 34, "end_pos": 48, "type": "DATASET", "confidence": 0.8969221711158752}, {"text": "LAS (gold PoS)", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.8244947075843811}, {"text": "LAS", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.9751986265182495}]}, {"text": "When adding the training data from the German-GSD UD treebank, results increase to 81% LAS (gold PoS) and 76% LAS (auto PoS).", "labels": [], "entities": [{"text": "German-GSD UD treebank", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.907496710618337}, {"text": "LAS (gold PoS)", "start_pos": 87, "end_pos": 101, "type": "METRIC", "confidence": 0.7829505264759063}, {"text": "LAS", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9885103106498718}, {"text": "auto PoS)", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9090534647305807}]}, {"text": "The large gap of 5% between the gold and auto PoS setting highlights the importance of high-quality PoS tags for parsing tweets.   were among the first to provide syntactic analyses for Twitter microtext.", "labels": [], "entities": [{"text": "parsing tweets.", "start_pos": 113, "end_pos": 128, "type": "TASK", "confidence": 0.8925531506538391}]}, {"text": "They created a testset with over 500 sentences extracted from tweets.", "labels": [], "entities": []}, {"text": "The data was automatically parsed with a constituency parser and the trees were manually corrected by one annotator.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement (IAA) for labelled bracketing, measured on a subset of the data annotated by a second annotator, was quite high with nearly 96%.", "labels": [], "entities": [{"text": "Inter-annotator agreement (IAA)", "start_pos": 0, "end_pos": 31, "type": "METRIC", "confidence": 0.8808324217796326}, {"text": "labelled bracketing", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7728201448917389}]}, {"text": "Parsing accuracy without any domain adaptation, however, was low: the Malt parser (), trained on the WSJ, achieved an LAS of 63.3% on the Twitter testset.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8209924697875977}, {"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9014685153961182}, {"text": "WSJ", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.968020498752594}, {"text": "LAS", "start_pos": 118, "end_pos": 121, "type": "METRIC", "confidence": 0.9980055689811707}]}, {"text": "The Tweebank v1 () is another English Twitter treebank, with a size of over 900 tweets annotated with unlabelled dependencies.", "labels": [], "entities": [{"text": "English Twitter treebank", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.8271355231602987}]}, {"text": "extend the work of by enlarging the treebank to more than 3,500 tweets, refining the guidelines and adding labels to the former unlabelled trees.", "labels": [], "entities": []}, {"text": "They report an IAA of 84.3% for labelled attachments in the Tweebank v2.", "labels": [], "entities": [{"text": "IAA", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.999235987663269}, {"text": "Tweebank v2", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.941591888666153}]}, {"text": "A third English Twitter treebank was created by.", "labels": [], "entities": [{"text": "English Twitter treebank", "start_pos": 8, "end_pos": 32, "type": "DATASET", "confidence": 0.9565508961677551}]}, {"text": "Their corpus includes 250 AfricanAmerican English (AAE) tweets and 250 tweets of mainstream American English microtext.", "labels": [], "entities": []}, {"text": "The data has been annotated by two coders but no inter-annotator agreement is reported.", "labels": [], "entities": []}, {"text": "The Italian Twitter treebank of is the largest existing Twitter treebank and includes more than 6,700 trees.", "labels": [], "entities": [{"text": "Italian Twitter treebank", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9380982518196106}]}, {"text": "The authors report an IAA of 0.92 \u03ba for syntactic annotation.", "labels": [], "entities": [{"text": "IAA", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9994077682495117}, {"text": "syntactic annotation", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.7915516793727875}]}, {"text": "The results fora dependency parser) trained on a combination of the Italian UD treebank and the new dataset are also quite high, with a labelled attachment score of 81.5%.", "labels": [], "entities": [{"text": "Italian UD treebank", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.8530420462290446}, {"text": "labelled attachment score", "start_pos": 136, "end_pos": 161, "type": "METRIC", "confidence": 0.8542120258013407}]}, {"text": "The high agreement and parsing scores suggest that the dataset is somewhat easier and more well-behaved than the Tweebank for baseline results for the different Twitter treebanks).", "labels": [], "entities": [{"text": "agreement", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9948301911354065}, {"text": "Tweebank", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.6425745487213135}]}, {"text": "For Arabic, a treebank with Twitter microtext has been created fully automatically, based on predictions of a rule-based and a data-driven parser.", "labels": [], "entities": []}, {"text": "Efforts have been made to map the annotations to the UD scheme, but, to the best of our knowledge, the data is not yet available.", "labels": [], "entities": [{"text": "UD scheme", "start_pos": 53, "end_pos": 62, "type": "DATASET", "confidence": 0.8034258782863617}]}, {"text": "With over 12,000 tokens, our new German Twitter treebank is comparable in size to TWEEBANK V1 () even though the number of tweets in our dataset is smaller.", "labels": [], "entities": [{"text": "German Twitter treebank", "start_pos": 33, "end_pos": 56, "type": "DATASET", "confidence": 0.9213582475980123}]}, {"text": "This is due to the fact that our data were collected after Twitter raised the maximum length for tweets from 140 to 280 characters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics for the tweeDe testsuite (OOV: number of out-of-vocabulary words with regard  to the training set; lower: OOV for lower-cased word forms).", "labels": [], "entities": [{"text": "OOV", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9933643341064453}, {"text": "OOV", "start_pos": 134, "end_pos": 137, "type": "METRIC", "confidence": 0.9918093085289001}]}, {"text": " Table 2: Parsing results for the Dozat parser on tweeDe, without (left) and with additional training data  from the German-GSD UD treebank (right).", "labels": [], "entities": [{"text": "German-GSD UD treebank", "start_pos": 117, "end_pos": 139, "type": "DATASET", "confidence": 0.9194230238596598}]}, {"text": " Table 3: Statistics for manually annotated treebanks (*Foster et al. only report # sentences, not # tweets.  We expect the no. of tweets to be slightly lower than 500). The data of Blodgett et al. includes AAE and  main-stream (MS) English tweets. The last two columns report results for the Dozat & Manning parser  (Dozat et al., 2017) (w/o domain adaptation) or the Malt parser from the literature.", "labels": [], "entities": [{"text": "AAE", "start_pos": 207, "end_pos": 210, "type": "METRIC", "confidence": 0.9262394309043884}]}]}