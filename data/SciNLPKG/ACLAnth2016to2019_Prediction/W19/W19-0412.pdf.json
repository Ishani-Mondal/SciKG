{"title": [{"text": "Aligning Open IE Relations and KB Relations using a Siamese Network Based on Word Embedding", "labels": [], "entities": [{"text": "Aligning Open IE Relations", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8667171597480774}]}], "abstractContent": [{"text": "Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7954239462103162}]}, {"text": "Given a triple, the relation expresses the type of semantic relation between the entities.", "labels": [], "entities": []}, {"text": "Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning.", "labels": [], "entities": []}, {"text": "As away to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB.", "labels": [], "entities": []}, {"text": "Our approach is to use a Siamese network that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation.", "labels": [], "entities": []}, {"text": "In order to make the approach practical, we automatically generate a training dataset using a distant supervision approach instead of relying on a hand-labeled dataset.", "labels": [], "entities": []}, {"text": "Our experiment shows that the proposed method can capture the relational semantics better than the recent approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Open Information Extraction (Open IE) aims at extracting key information from a large amount of text into a structured format, commonly in the form of triples, (subject entity, relation, object entity), where the relation denotes the type of a semantic relation between the entities.", "labels": [], "entities": [{"text": "Open Information Extraction (Open IE)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.7938286534377507}]}, {"text": "As opposed to the traditional Information Extraction that generates triples over a predefined relation set, Open IE can extract all possible relations without having to be restricted to a predefined set of relations.", "labels": [], "entities": []}, {"text": "However, a relation from an Open IE system is merely a sequence of words coming from the sentence containing the entities, resulting in ambiguous and semantically redundant relations.", "labels": [], "entities": []}, {"text": "For example, Open IE may extract \"died in\" and \"location of death\" as two distinct relations although they should be treated as semantically equal and expressed (or canonicalized) with a single relation type.", "labels": [], "entities": []}, {"text": "In order to address this problem, some methods have been proposed to canonicalize Open IE relations (.", "labels": [], "entities": []}, {"text": "Given that they rely on a clustering method, however, they tend to suffer from over-generalization.", "labels": [], "entities": []}, {"text": "For example, the latest canonicalization method called CESI ( would put \"is brother of,\" \"is son of,\" \"is main villain of,\" and \"was professor of \" into the same relation cluster.", "labels": [], "entities": [{"text": "CESI", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8912447094917297}]}, {"text": "While these relation phrases have a common pattern (to be + noun + of) and expresses that the subject entity has a certain role, the overarching relational category is too general to be useful.", "labels": [], "entities": []}, {"text": "Besides Open IE, Knowledge Base (KB) systems such as DBpedia, Freebase, and Wikidata, also store general facts in a triple format.", "labels": [], "entities": [{"text": "DBpedia", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9394248127937317}, {"text": "Freebase", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.878757655620575}]}, {"text": "Different from Open IE, the relations in a KB are already classified into distinct semantic categories.", "labels": [], "entities": []}, {"text": "Although KB relations are better defined semantically than Open IE relations, they are limited in terms of quantity and coverage.", "labels": [], "entities": []}, {"text": "attempted to mitigate the weaknesses of the two approaches by aligning the relations of Open IE triples to those in DBpedia, thereby adding semantics to Open IE triples.", "labels": [], "entities": []}, {"text": "While useful, their approach is primarily based on the frequency of triples without explicitly taking into account the relational semantics.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew model using a Siamese network for aligning relations from Open IE to those from KB (i.e. relation alignment task) for the purpose of providing more semantics to Open IE relations, which are to be used for question answering as in TriviaQA ().", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.7637396256128947}, {"text": "question answering", "start_pos": 236, "end_pos": 254, "type": "TASK", "confidence": 0.8564798533916473}]}, {"text": "The Siamese network, a form of a neural network, takes two sequences of word embeddings representing an Open IE relation and a KB relation and compares them.", "labels": [], "entities": []}, {"text": "The network is trained to learn the semantic similarities between an Open IE relational phrase and a KB relation type name that are considered identical in their meanings.", "labels": [], "entities": []}, {"text": "By utilizing word embeddings as the input of the network and encode relational descriptions, we can incorporate their semantics information without an extra process of extracting linguistic features from the training data.", "labels": [], "entities": []}, {"text": "In order to mitigate the problem of manually constructing training data, i.e. pairs of an Open IE relational phrase and a KB relation type name, we propose a distant supervision method that does not require manual annotations.", "labels": [], "entities": []}, {"text": "Our contributions in this paper are: \u2022 We propose a novel method of applying a Siamese network for the relation alignment task.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.8982973694801331}]}, {"text": "To the best of our knowledge, our model is the first attempt that incorporates the semantic information of the textual descriptions of relations, specifically for the relation alignment task.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 167, "end_pos": 190, "type": "TASK", "confidence": 0.835360050201416}]}, {"text": "\u2022 We propose to automatically generate a training dataset using a distant supervision approach so that we avoid manual creation of training data, which can be prohibitive, thereby making the proposed approach practical.", "labels": [], "entities": []}, {"text": "\u2022 We experimentally confirm that our model better captures relational semantics than the clustering and the statistical rule-based approaches with a significant margin.", "labels": [], "entities": []}, {"text": "We also analyze different variations of the Siamese network to provide insights about the relation alignment task.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.8706685702006022}]}], "datasetContent": [{"text": "The distant supervision method for the task of relation extraction was first introduced by.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.9208284020423889}]}, {"text": "It assumes that any sentence containing an entity pair participating in a triple of a known KB is likely to contain a relevant expression of the relation of the triple.", "labels": [], "entities": []}, {"text": "As a result, it becomes possible to construct positive training instances for the relation in the triple by taking the expressions between the occurrences of the two entities.", "labels": [], "entities": []}, {"text": "The collection of textual expressions can be used as revealing the target relation.", "labels": [], "entities": []}, {"text": "By adopting this approach, we can obtain the sentences containing the target relation in KB and use them to extract Open IE triples with the relation.", "labels": [], "entities": []}, {"text": "Once the Open IE triples are generated, we apply some rules to annotate them as positive or negative automatically so that we obtain training data for the KB relations used in collection the Open IE triples.", "labels": [], "entities": []}, {"text": "The training data generation steps are as follows: 1.", "labels": [], "entities": [{"text": "training data generation", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6282732685407003}]}, {"text": "Select the top 200 most frequent relations 3 in the KB and collect the KB triples containing one of the relations.", "labels": [], "entities": []}, {"text": "We utilize Wikidata) as our KB.", "labels": [], "entities": []}, {"text": "2. Crawl the sentences for each triple using the distant supervision method.", "labels": [], "entities": []}, {"text": "In other words, we pick the sentences containing the two entities of the triple.", "labels": [], "entities": []}, {"text": "In order to reduce ambiguities associated with the occurrences of the entities, we retrieve sentences from the Wikipedia page of each entity.", "labels": [], "entities": []}, {"text": "3. Apply Open IE to each sentence to extract Open IE triples.", "labels": [], "entities": []}, {"text": "In this paper, we use the existing Stanford Open IE system (Angeli et al., 2015).", "labels": [], "entities": [{"text": "Stanford Open IE system", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.7889747321605682}]}, {"text": "4. Align Open IE and KB triples.", "labels": [], "entities": [{"text": "Align Open IE", "start_pos": 3, "end_pos": 16, "type": "TASK", "confidence": 0.46965230504671734}]}, {"text": "The triples sharing the same entity pair are labeled as semantically same or positive (0).", "labels": [], "entities": []}, {"text": "But if one of the entities is different, it labeled as different or negative (1).", "labels": [], "entities": []}, {"text": "5. From the previous step, we will get a small amount of positive examples but a high number of negative examples.", "labels": [], "entities": []}, {"text": "To handle the data imbalance problem, we add more positive examples by swapping a pair of alignments when the other sides of the two alignments share the same relation but with different entities.", "labels": [], "entities": []}, {"text": "The existing approaches that serve as the baselines are: \u2022 CESI (: For this model, we adjust the clustering result so that it can be compared with our model for the evaluation tasks to be described below.", "labels": [], "entities": [{"text": "CESI", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8414498567581177}]}, {"text": "If two relations are in the same cluster, then they are labeled as semantically same; otherwise different.", "labels": [], "entities": []}, {"text": "\u2022: This model uses a statistical rule-based approach for aligning relations.", "labels": [], "entities": []}, {"text": "It calculates a confidence score of every possible Open IE relations mapping to a KB relations based on occurrence statistics of the particular mapping.", "labels": [], "entities": []}, {"text": "If the mapping has a higher confidence than the threshold determined by linear regression, it is labeled as semantically same; otherwise different.", "labels": [], "entities": []}, {"text": "Because the code has not been shared by the authors, we implemented their method on our own.", "labels": [], "entities": []}, {"text": "Besides the above baselines, we also apply our alignment rule (denotes as rule-based in) used in the dataset generation process (see Section 4) for predicting the label, i.e., the triples sharing the same entity pair are labeled as semantically same; otherwise different.", "labels": [], "entities": []}, {"text": "Note that this case is used as a reference point in explaining the performance of the proposed method and the other baselines.", "labels": [], "entities": []}, {"text": "It also can be used to measure the quality of the distant supervision dataset.", "labels": [], "entities": [{"text": "distant supervision dataset", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.6104765236377716}]}, {"text": "Since there is no standard evaluation suit available for the relation alignment task, we provide three evaluations to reveal different aspects of the proposed model and compensate for the limitations of each.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8935685555140177}]}, {"text": "variations of the proposed model using automatically generated test data of a large quantity.", "labels": [], "entities": []}, {"text": "It is internal because we only compare different variations of the proposed model, not against other methods.", "labels": [], "entities": []}, {"text": "We split our automatically generated dataset into training, validation, and testing datasets (see for details).", "labels": [], "entities": []}, {"text": "\u2022 CNN no def: This version uses CNN as the encoder and relational phrases and relation names (no definitions) as the input representation for both Open IE and KB relations.", "labels": [], "entities": [{"text": "CNN", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.9109650254249573}]}, {"text": "\u2022 CNN def: This is the same as CNN no def except that relation definitions are added.", "labels": [], "entities": [{"text": "CNN", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.8760126829147339}]}, {"text": "\u2022 CNN def ent: This version is the same as CNN def except that entity information is added.", "labels": [], "entities": [{"text": "CNN", "start_pos": 2, "end_pos": 5, "type": "DATASET", "confidence": 0.9039226174354553}, {"text": "CNN", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.908171534538269}]}, {"text": "\u2022 PCNN no def ent: This version uses PCNN as the encoder and relational phrases and relation names for Open IE and KB relations as the input, respectively, as well entity information.", "labels": [], "entities": []}, {"text": "\u2022 PCNN def ent: This is the same as PCNN no def ent except relation definitions are added.", "labels": [], "entities": []}, {"text": "Since we use a large number of sentences and triples extracted thereof, this evaluation allows us to test different variations for all the relations exhaustively.", "labels": [], "entities": []}, {"text": "The relative performance differences among the five versions are summarized in.", "labels": [], "entities": []}, {"text": "In addition to the ordering of the five variations, the difference between CNN and PCNN encoders is most notable.", "labels": [], "entities": []}, {"text": "Detailed analyses are as follows.", "labels": [], "entities": []}, {"text": "Relational Phrase vs. Relation Definition.", "labels": [], "entities": []}, {"text": "While our intuition was that the additional information obtainable from the relation definitions would help compensate for the lack of semantics in a short relation phrases and names, it turns out that the overall gain shown in is not as significant as our expectation.", "labels": [], "entities": []}, {"text": "A further analysis shows, however, that definitions help reduce incorrect predictions as in.", "labels": [], "entities": []}, {"text": "Out of 137 errors (82 false positives and 55 false negatives) made by CNN no def, 40 were predicted correctly by including definitions (CNN def), resulting in 29.2% improvement.", "labels": [], "entities": [{"text": "CNN no def", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.91325843334198}, {"text": "CNN def", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.908384770154953}]}, {"text": "On the other hand, out of 263 correct prediction in CNN no def (203 true positives and 60 true negatives), 52 were predicted incorrectly in CNN def, resulting in 19.77% drop.", "labels": [], "entities": [{"text": "CNN no def", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9247799317042033}, {"text": "CNN def", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9583750665187836}]}, {"text": "This suggests that adding definition has potential to enrich the semantics; more sophisticated approaches are left for future research.", "labels": [], "entities": []}, {"text": "We observe that the performance of CNN def ent is significantly higher than that of the CNN def model.", "labels": [], "entities": []}, {"text": "From this result, we can conclude that adding entity information contributes to predicting the similarity between two relations.", "labels": [], "entities": []}, {"text": "It suggests that entity information provides the context with which relation phrases and names can be aligned more accurately.", "labels": [], "entities": []}, {"text": "It is consistent with the result in that also shows the importance of including entities in relation classification.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 92, "end_pos": 115, "type": "TASK", "confidence": 0.8060873746871948}]}, {"text": "Compared to the performance CNN def ent, PCNN def ent is clearly better, strongly suggesting that for the relation alignment task, the PCNN encoder is better than CNN, regardless of whether relation definitions are used.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8763289451599121}]}, {"text": "A rational explanation for this result is that we lose important information when we apply max-pooling to the entire input representation including entities and relational phrases in CNN.", "labels": [], "entities": []}, {"text": "Note that in PCNN, piecewise max-pooling allows the model to extract major features from three different segments of the representation (i.e. subject entity, relation, and object entity).", "labels": [], "entities": []}, {"text": "Therefore, this result confirms that the piecewise max-pooling helps in preserving more meaningful features resulting from the convolutional layer for the relation alignment task.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 155, "end_pos": 178, "type": "TASK", "confidence": 0.8621694644292196}]}, {"text": "For more reliable evaluation of the proposed model, we compared it against the baselines using the 400 gold standards labeled by human.", "labels": [], "entities": []}, {"text": "The summary result for predicting the semantically same and different clearly shows the proposed model outperforms the baselines, CESI and Dutta, in predicting the semantically same pairs.", "labels": [], "entities": [{"text": "CESI", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.850672721862793}]}, {"text": "Although CESI has the highest precision score, it has the lowest recall among all models variations due to the bias of predicting most of the data as semantically different.", "labels": [], "entities": [{"text": "CESI", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.80703204870224}, {"text": "precision score", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9813838005065918}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9989792108535767}]}, {"text": "While it is possible to apply a different threshold in forming clusters for different precision and recall pairs, the low F1 value precludes its moderate performance for relation alignment.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.984458327293396}, {"text": "recall", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9541602730751038}, {"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9982069730758667}, {"text": "relation alignment", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.8928340077400208}]}, {"text": "Compared to CESI models, Dutta's model shows better performance, especially in recall and F1 scores for predicting semantically same pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9995321035385132}, {"text": "F1", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9989100694656372}, {"text": "predicting semantically same pairs", "start_pos": 104, "end_pos": 138, "type": "TASK", "confidence": 0.8593861758708954}]}, {"text": "Based on this result, it is obvious that using a simple probabilistic rule-based approach is better than using the clustering approach for the relation alignment task.", "labels": [], "entities": [{"text": "relation alignment task", "start_pos": 143, "end_pos": 166, "type": "TASK", "confidence": 0.8775793313980103}]}, {"text": "However, it is much worse than our model variations, with a high number of false negatives, resulting in the low recall and F1 scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.999681830406189}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9990022778511047}]}, {"text": "The scores of the rule-based model in is provided as a reference point of our proposed models.", "labels": [], "entities": []}, {"text": "Since the rule-based model predicts the label using the alignment rules in our distant supervision dataset generation, a pair (one from Open IE and the other from KB) sharing the same entities in respective triples is judged to be semantically same by this model.", "labels": [], "entities": [{"text": "distant supervision dataset generation", "start_pos": 79, "end_pos": 117, "type": "DATASET", "confidence": 0.7499781548976898}, {"text": "Open IE", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.8609516620635986}]}, {"text": "That is, all the 400 pairs are predicted to be semantically same.", "labels": [], "entities": []}, {"text": "From the alignment task perspective, it gives 100% recall for the semantically same case (all of the 258 \"same\" pairs are predicted correctly) and 0% recall for semantically different case.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9991772770881653}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9988325238227844}]}, {"text": "Since all the 142 \"different\" pairs are predicted as \"same\", the overall accuracy score is 64.5%.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.9809407293796539}]}, {"text": "It shows that the training process is still needed since we cannot only rely on the alignment rules in our dataset generation.", "labels": [], "entities": []}, {"text": "Moreover, since the rule-based model predictions are made with the distant supervision rule, we can also infer the quality of our distant supervision dataset based on the scores of the model (64.5% alignments are correctly labeled by distant supervision).", "labels": [], "entities": []}, {"text": "Note that the low F1 score for predicting semantically different pairs in the proposed model is attributed to the high number of false negatives in the dataset.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9902842044830322}, {"text": "predicting semantically different pairs", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.8402119129896164}]}, {"text": "However, it still has the highest overall accuracy score compared to the two baselines.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.9842199087142944}]}], "tableCaptions": [{"text": " Table 1: Statistics of our dataset generated by the distant supervision method.", "labels": [], "entities": []}, {"text": " Table 2: Manual evaluation result.", "labels": [], "entities": []}, {"text": " Table 3: Alignment results of some Open IE and KB relations.", "labels": [], "entities": []}]}