{"title": [{"text": "Selecting Informative Context Sentence by Forced Back-Translation", "labels": [], "entities": [{"text": "Selecting Informative Context Sentence", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8207616358995438}]}], "abstractContent": [{"text": "As one of the contributions of this paper, this paper first explores the upper bound of context-based neural machine translation and attempt to utilize previously unused context information.", "labels": [], "entities": [{"text": "context-based neural machine translation", "start_pos": 88, "end_pos": 128, "type": "TASK", "confidence": 0.5920010805130005}]}, {"text": "We found that, if we could appropriately select the most informative context sentence fora given input source sentence, we could boost translation accuracy as much as approximately 10 BLEU points.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9411529302597046}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9993157386779785}]}, {"text": "This paper next explores a criterion to select the most informative context sentences that give the highest BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.989597499370575}]}, {"text": "Applying the proposed criterion, context sentences that yield the highest forced back-translation probability when back-translating into the source sentence are selected.", "labels": [], "entities": []}, {"text": "Experimental results with Japanese and English parallel sentences from the OpenSubtitles2018 corpus demonstrate that, when the context length of five preceding and five subsequent sentences are examined, the proposed approach achieved significant improvements of 0.74 (Japanese to English) and 1.14 (En-glish to Japanese) BLEU scores compared to the baseline 2-to-2 model, where the oracle translation achieved upper bounds improvements of 5.88 (Japanese to English) and 9.10 (English to Japanese) BLEU scores.", "labels": [], "entities": [{"text": "OpenSubtitles2018 corpus", "start_pos": 75, "end_pos": 99, "type": "DATASET", "confidence": 0.8330989181995392}, {"text": "BLEU", "start_pos": 322, "end_pos": 326, "type": "METRIC", "confidence": 0.9961267113685608}, {"text": "BLEU", "start_pos": 498, "end_pos": 502, "type": "METRIC", "confidence": 0.9973213076591492}]}], "introductionContent": [{"text": "Recently, neural machine translation (NMT) models ( c \ud97b\udf59 2019 The authors.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8023514548937479}]}, {"text": "This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CC-BY-ND.) have made remarkable progress.", "labels": [], "entities": []}, {"text": "Most NMT models are designed to translate a single sentence and do not accept input greater than one sentence, i.e., input sentences that include additional context information.", "labels": [], "entities": []}, {"text": "However, recently, several approaches that attempt to translate inputs with more than one sentence have been proposed (.", "labels": [], "entities": []}, {"text": "These approaches to context-based NMT models can be roughly categorized according to the width of the context considered in those models.", "labels": [], "entities": []}, {"text": "A typical approach is to consider the sentence immediately preceding the source sentence to be translated as the context (.", "labels": [], "entities": []}, {"text": "Context-based NMT models can be further categorized according to whether the source and context sentences are encoded using a single ( or multiple encoders.", "labels": [], "entities": []}, {"text": "Another approach considers a much wider context than the immediately preceding sentence, e.g., three preceding sentences (, preceding sentences within the document (, and all preceding and subsequent sentences within the document.", "labels": [], "entities": []}, {"text": "Such approaches to context-based NMT models possibly outperform existing models that only accept a single sentence to be translated.", "labels": [], "entities": []}, {"text": "Note that we refer to the model that only accepts a single sentence as a \"1-to-1\" model.", "labels": [], "entities": []}, {"text": "Among these existing models, the 2+2 or 2-to-2 model () uses the sentence immediately preceding the source sentence to be translated as BLEU Oracle BLEU Ja-En En-Ja Ja-En En-Ja 1-to-1 (baseline)  an extended context.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 136, "end_pos": 140, "type": "METRIC", "confidence": 0.9961326122283936}, {"text": "BLEU", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.7058045864105225}]}, {"text": "Here the context sentence is concatenated to the source sentence using the \ud97b\udf59CONCAT\ud97b\udf59 token.", "labels": [], "entities": [{"text": "\ud97b\udf59CONCAT\ud97b\udf59 token", "start_pos": 75, "end_pos": 89, "type": "DATASET", "confidence": 0.662069596350193}]}, {"text": "The 2-to-2 model is easy to implement into existing 1-to-1 models: however, it only considers the immediately preceding sentence as context.", "labels": [], "entities": []}, {"text": "Thus, it is necessary to consider much wider contexts such as the second through fifth preceding sentences and the first through fifth subsequent sentences.", "labels": [], "entities": []}, {"text": "We conducted an empirical study that revealed that, in some cases, among the first through fifth sentences preceding and subsequent to the source sentence, the most informative sentence, i.e, the sentence that returns the highest BLEU score, may not be the sentence immediately preceding the source sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 230, "end_pos": 240, "type": "METRIC", "confidence": 0.9893413484096527}]}, {"text": "We measured oracle BLEU scores by selecting context sentences that give the maximum sentence-BLEU scores among the five preceding and subsequent sentences, as shown in and.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9902266263961792}]}, {"text": "Then, we found that, if we could select the most informative context sentence fora given input source sentence, we can improve translation accuracy by as much as approximately 10 BLEU points, as indicated by the oracle BLEU scores in and.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9438132047653198}, {"text": "BLEU", "start_pos": 179, "end_pos": 183, "type": "METRIC", "confidence": 0.9986494183540344}, {"text": "BLEU", "start_pos": 219, "end_pos": 223, "type": "METRIC", "confidence": 0.9885431528091431}]}, {"text": "More specifically, compared to the baseline 2-to-2 model, the oracle translation achieved upper bound improvements of 5.88 (Japanese to English) and 9.10 (English to Japanese) BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9977957010269165}]}, {"text": "Considering this result, within the framework of the 2-to-2 context based NMT model, this study explored how to select the most informative context sentences that give the highest BLEU score among the first five preceding and subsequent sentences . Here, we used the Transformer model ( as the base 1-to-1 model.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 180, "end_pos": 190, "type": "METRIC", "confidence": 0.986999899148941}]}, {"text": "To select the translation with the highest BLEU score among the 11 translations (i.e., those translated by the 1-to-1 and 10 2-to-2 models), we propose an approach that selects the translation that yields the highest forced back-translation probability when back-translating into the source sentence.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9799613654613495}]}, {"text": "The evaluation results shown in demonstrate that the proposed approach achieves significant BLEU score improvements over the baseline 2-to-2 and 1-to-1 models.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9789973795413971}]}, {"text": "More specifically, over the baseline 2-to-2 model, the proposed approach achieved significant improvements of 0.74 (Japanese to English) and 1.14 (English to Japanese) BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9971281886100769}]}, {"text": "proposed the 2-to-2 model, which uses the sentence immediately preceding the source sentence to be translated as the extended context.", "labels": [], "entities": []}, {"text": "We extend the 2-to-2 model by considering the first five preceding and first five subsequent sentences.", "labels": [], "entities": []}, {"text": "In our extended 2-to-2 context-based NMT model, the immediately preceding sentence, the second through fifth preceding sentences, and the first through fifth subse-  (b) En-Ja: Oracle BLEU and BLEU scores of baseline 2-to-2 (y 22 0 (x\u22121, x0)) and 1-to-1 (y 11 0 (x0)) models quent sentences are considered candidates for concatenation to the source sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9378622770309448}, {"text": "BLEU", "start_pos": 193, "end_pos": 197, "type": "METRIC", "confidence": 0.9953843951225281}]}, {"text": "Then, among the first through fifth preceding and first through fifth subsequent sentences, we select the most informative context sentences in the 2-to-2 context-based NMT model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The dataset used for the oracle translation statistics and the BLEU evaluation comprised 2,083,576 English and Japanese parallel sentence pairs from Opensubtitles 2018 (Lison et al., 2018).", "labels": [], "entities": [{"text": "oracle translation", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7871499359607697}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9226009249687195}]}, {"text": "Note that we followed to create the extended context dataset.", "labels": [], "entities": []}, {"text": "Here, 90% of the dataset (1,876,624 sentence pairs) was used for training, 5% (104,379 sentence pairs) for development, and 5% (102,573 sentence pairs) for oracle statistics and evaluation.", "labels": [], "entities": []}, {"text": "Here, of these 102,573 sentence pairs, only 10,000 pairs were actually used for oracle statistics and evaluation 6 7 . Throughout the paper, we approximate that all the 2-to-2 models are trained with the immediately preceding sentence as the context.", "labels": [], "entities": []}, {"text": "For both English to Japanese and Japanese to English directions, shows the BLEU scores obtained by selecting the translation candidate that maximizes the forced back-translation and the back-translation sentence-BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9995214939117432}]}, {"text": "For the proposed method, we compare the following translation candidate cases: 8 (i) between y 11 0 (x 0 ) and y 22 0 (x \u22121 , x 0 ), (ii) among y 11 0 (x 0 ) and y 0 (x i , x 0 ) (i = \u22121, . .", "labels": [], "entities": []}, {"text": ", \u22125), (iii) among y 11 0 (x 0 ) and y 22 0 (x i , x 0 ) (i = +1, . .", "labels": [], "entities": []}, {"text": ", +5), (iv) among y 11 0 (x 0 ) and y 22 0 (x i , x 0 ) (i = \u00b11, . .", "labels": [], "entities": []}, {"text": "Compared to the BLEU scores of y 11 0 (x 0 ) and y 22 0 (x \u22121 , x 0 ), all BLEU scores obtained by the proposed method demonstrate significant improvement (p < 0.01), except for the Japanese to English translation obtained by maximizing the back-translation sentence-BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.998145341873169}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9969133138656616}]}, {"text": "By comparing the BLEU scores of y 11 0 (x 0 ), y 22 0 (x \u22121 , x 0 ), the oracle among them, and the selection between them by maximizing the forced back-translation, the selection between y 11 0 (x 0 ) and y 22 0 (x \u22121 , x 0 ) by maximizing forced back-translation achieves BLEU scores that are comparable to the oracle BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9969449639320374}, {"text": "BLEU", "start_pos": 274, "end_pos": 278, "type": "METRIC", "confidence": 0.9994955062866211}, {"text": "BLEU", "start_pos": 320, "end_pos": 324, "type": "METRIC", "confidence": 0.9600411057472229}]}, {"text": "Thus, we conclude that the proposed method contributes to selecting better translation between those candidates.", "labels": [], "entities": []}, {"text": "However, the proposed method cannot select informative context sentences among y 0 (x i , x 0 ) (i = \u22125, . .", "labels": [], "entities": []}, {"text": ", \u22122, +1, . .", "labels": [], "entities": []}, {"text": ", +5), because the results obtained by adding y 0 (x i , x 0 ) (i = \u22125, . .", "labels": [], "entities": []}, {"text": ", \u22122, +1, . .", "labels": [], "entities": []}, {"text": ", +5), to translation candidates y 11 0 (x 0 ) and y 22 0 (x \u22121 , x 0 ) yields little or no gain in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9777329862117767}]}, {"text": "Note that this does not coincide with improving the oracle BLEU score by approximately four points for Japanese to English and six points for English to Japanese with the overall 11 translation candidates.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.976169615983963}]}, {"text": "Thus, it can be concluded that further study is required to appropriately select the informative context sentences among the 11 candidates such that the BLEU score becomes much closer to the oracle BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 153, "end_pos": 163, "type": "METRIC", "confidence": 0.9794099926948547}, {"text": "BLEU", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.986068606376648}]}, {"text": "Another important comparison with a baseline is also shown as \"selection from 20-best of 2-to-2 (baseline) by 2-to-2 back-translation\" in Table 1.", "labels": [], "entities": []}, {"text": "With this baseline, it is intended to examine whether the five preceding and subsequent sentences introduced in the proposed method are sufficiently informative compared to other well studied translation candidates such as n-best translations.", "labels": [], "entities": []}, {"text": "Specifically, the baseline 2-to-2 model with the immediately preceding sentence as the context is employed to generate 20-best translations, and then, out of those generated 20-best translations, the one with the maximum forced back-translation into the source sentence is selected 9 . As shown in, this baseline performed worse than the proposed approach.", "labels": [], "entities": []}, {"text": "From this result, it is obvious that the proposed approach of introducing five preceding and subsequent sentences as the context is Ja-En En-Ja category of phenomena  much more informative than 20-best translations with just the preceding sentence as the context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation results (maximizing forced back-translation probability / maximizing back-translation sentence-BLEU)  ( *  *  represents significant difference (p < 0.01) against baseline 2-to-2 model)", "labels": [], "entities": []}, {"text": " Table 3: Distribution of oracle translation phenomena  (through manual analysis of 50 examples) (proposed method  succeeds / fails in identifying those oracle translations)", "labels": [], "entities": [{"text": "Distribution of oracle translation", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.7207933515310287}]}, {"text": " Table 3.  For both Japanese to English and English to  Japanese, nearly 30\u223c40% are categorized as \"syn- onymous expression\", where the proposed ap- proach of maximizing forced back-translation suc- cessfully selects the oracle translation that includes  the synonymous expression rather than exactly the  same expression (as in the reference translation).  Due to this synonymous expression, the sentence", "labels": [], "entities": []}, {"text": " Table 4: Distribution of phenomena where baseline 2-to-2  y 22  0 (x\u22121, x0) wins v.s. 1-to-1 y 11  0 (x0) wins (through man- ual analysis of 50 examples)", "labels": [], "entities": []}, {"text": " Table 5: Example improvements over baseline 2-to-2 y 22  0 (x\u22121, x0) (Ja-En) (b) untranslated by baseline", "labels": [], "entities": []}]}