{"title": [{"text": "Generating Discourse Inferences from Unscoped Episodic Logical Formulas", "labels": [], "entities": [{"text": "Generating Discourse Inferences from Unscoped Episodic Logical Formulas", "start_pos": 0, "end_pos": 71, "type": "TASK", "confidence": 0.8130901977419853}]}], "abstractContent": [{"text": "Unscoped episodic logical form (ULF) is a semantic representation capturing the predicate-argument structure of English within the episodic logic formalism in relation to the syntactic structure, while leaving scope, word sense, and anaphora unresolved.", "labels": [], "entities": [{"text": "Unscoped episodic logical form (ULF)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6167528969900948}]}, {"text": "We describe how ULF can be used to generate natural language inferences that are grounded in the semantic and syntactic structure through a small set of rules defined over interpretable predicates and transformations on ULFs.", "labels": [], "entities": []}, {"text": "The semantic restrictions placed by ULF semantic types enables us to ensure that the inferred structures are semantically coherent while the nearness to syntax enables accurate mapping to English.", "labels": [], "entities": []}, {"text": "We demonstrate these inferences on four classes of conversationally-oriented inferences in a mixed genre dataset with 68.5% precision from human judgments.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9984679818153381}]}], "introductionContent": [{"text": "ULF was recently introduced as a semantic representation that captures the core semantic structure within an expressive logical formalism while staying close enough to the surface language to annotate a dataset that can be used to train a parser.", "labels": [], "entities": []}, {"text": "focused on the descriptive power of ULF and its relation to its fully resolved counterpart, Episodic Logic (EL), but the combination of semantic and syntactic information encoded in ULFs should position it to enable certain structurally-driven inferences.", "labels": [], "entities": []}, {"text": "In fact, mention some of these inferential classes that they expect ULF will support, but give no description of how to achieve this, nor a demonstration of it in practice.", "labels": [], "entities": [{"text": "ULF", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7317298650741577}]}, {"text": "ULF, being a pre-canonicalized semantic form, makes available many possible structures for similar semantic meanings, which leads to a challenge in formulating generalizable inferences.", "labels": [], "entities": [{"text": "ULF", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7672343850135803}]}, {"text": "This precanonicalized nature of ULF, though structurally relatively intricate, has some advantages over fully canonicalized representations for use in natural language tasks.", "labels": [], "entities": []}, {"text": "One is that it allows direct translation of intuitions about warranted textual inferences into inference rules (much as in Natural Logic).", "labels": [], "entities": [{"text": "translation of intuitions about warranted textual inferences", "start_pos": 29, "end_pos": 89, "type": "TASK", "confidence": 0.8208298512867519}]}, {"text": "As well, the ability to accurately generate the English sentences corresponding to a ULF formula and choose how and when to modify the surface form allows a more natural interface with the end task.", "labels": [], "entities": []}, {"text": "This feature allows us to evaluate inferences generated by ULF directly over English text rather than using an artificially structured interface, such as classification.", "labels": [], "entities": []}, {"text": "We present a method of generating inferences from ULFs from a small set of interpretable inference rules by first defining general semantic predicates over ULF clauses and tree transformations that correspond to natural semantic operations in ULF.", "labels": [], "entities": []}, {"text": "We then evaluate these on four of the five inferential classes presented by over a multi-genre dataset.", "labels": [], "entities": []}, {"text": "The ULF structure allows us to incorporate a paraphraselike rewrite module and then perform direct string comparisons of English generated from ULFs to human generated inferences.", "labels": [], "entities": []}, {"text": "Human evaluations show that 68.5% of these generated inferences are acceptable and an error analysis of the system shows that many of the errors can be corrected with some refinement to the inference rules and the ULF-to-English generation system.", "labels": [], "entities": [{"text": "ULF-to-English generation", "start_pos": 214, "end_pos": 239, "type": "TASK", "confidence": 0.75614333152771}]}], "datasetContent": [{"text": "We chose a variety of text sources for constructing this dataset to reduce genre-effects and provide good coverage of all the phenomena we are investigating.", "labels": [], "entities": []}, {"text": "Some of these datasets include annotations, which we use only to identify sentence and token boundaries.", "labels": [], "entities": []}, {"text": "We developed the inference rules based on a set of 40 sentences randomly sampled from the annotated dataset.", "labels": [], "entities": []}, {"text": "The correctness of these inferences is evaluated both through an automatic evaluation over the whole dataset and a human evaluation of a sample of the inferences.", "labels": [], "entities": []}, {"text": "Both evaluations are done directly over English sentences by automatically translating the ULF inferences to English sentences.", "labels": [], "entities": []}, {"text": "The automatic evaluation also involves a ULF rewriting module to handle semantically equivalent inference variants.", "labels": [], "entities": []}, {"text": "All of these components are fine-tuned on the 40 sentence devset.", "labels": [], "entities": []}, {"text": "In all of the experiments we start with human ULF annotations as a reliable ULF parser is not yet available.", "labels": [], "entities": []}, {"text": "Figure 4: A diagram of the automatic ULF inference evaluation pipeline.", "labels": [], "entities": [{"text": "ULF inference evaluation pipeline", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.8962829113006592}]}, {"text": "A diagram of the automatic evaluation pipeline is presented in.", "labels": [], "entities": []}, {"text": "The pipeline fora given source sentence and ULF proceeds as follows: 1.", "labels": [], "entities": []}, {"text": "Use the inference rules (Section 3) to generate a set of raw inferences from the source ULF.", "labels": [], "entities": []}, {"text": "2. Generate a complete set of possible realizations of the inferred ULFs by rewriting the raw inferences into possible structural variations (Section 5.2).", "labels": [], "entities": []}, {"text": "3. Translate inferred ULFs into English to get a set of inferred sentences (Section 5.1).", "labels": [], "entities": []}, {"text": "4. For each human inference elicited from the current source sentence, find the system-inferred sentence that has the smallest edit distance.", "labels": [], "entities": []}, {"text": "5. Report recall over human inferences with a max edit distance threshold.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9704927206039429}, {"text": "max edit distance threshold", "start_pos": 46, "end_pos": 73, "type": "METRIC", "confidence": 0.8513398766517639}]}, {"text": "We use an edit distance threshold of 3.", "labels": [], "entities": [{"text": "edit distance threshold", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.9164709448814392}]}, {"text": "This allows minor English generation errors such as verb conjugations and pluralizations, but does not allow simple negation insertion/deletion (a difference of a space-separated \"not\" token).", "labels": [], "entities": [{"text": "negation insertion/deletion", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.8775548487901688}]}, {"text": "lists the results of this evaluation.", "labels": [], "entities": []}, {"text": "The numerical values are fairly low, but this maybe expected given the evaluation procedure.", "labels": [], "entities": []}, {"text": "A trivial baseline such as most frequent devset inference or copying the source sentence would lead to a score of 0 or very close to 0 as these are very unlikely to be within a 3-character edit from the inferences in the dataset.", "labels": [], "entities": []}, {"text": "The human inference evaluation was performed over 127 raw ULF inferences.", "labels": [], "entities": []}, {"text": "This was built out of 100 randomly sampled inferences with the addition of every counterfactual and clause-taking inference as they are not as common.", "labels": [], "entities": []}, {"text": "Each inference was then translated to English, then presented alongside the source sentence to 3 to 4 independent human judges.", "labels": [], "entities": []}, {"text": "The judges evaluated correctness of the discourse inference and the grammaticality of the output sentence.", "labels": [], "entities": []}, {"text": "presents the results of this.", "labels": [], "entities": []}, {"text": "87 of the 127 inferences were marked as correct by a majority of judges and only 21 were marked as incorrect by a majority of judges, for the remaining 19 inferences judges either disagreed completely or a majority judged it as context-dependent.", "labels": [], "entities": []}, {"text": "99 of the 127 inferences were judged grammatical by a majority of judges, which demonstrates the efficacy of the ULF-to-: Results of majority human evaluation of system generated inferences.", "labels": [], "entities": []}, {"text": "Evaluation on 127 inferences with from the test set by 3 or 4 people per inference.", "labels": [], "entities": []}, {"text": "*Correctness is evaluated on whether the sentence is a reasonable inference in conversation, allowing for some awkwardness in phrasing.", "labels": [], "entities": [{"text": "Correctness", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.9977669715881348}]}, {"text": "Context, means the correctness is highly context-dependent.", "labels": [], "entities": []}, {"text": "The inference type labels in the header row are the same as in except for the addition of breaking down questions to q-pre for question presuppositions and q-act for question act inferences.", "labels": [], "entities": []}, {"text": "The system seems to struggle most with counterfactual and clausetaking inferences.", "labels": [], "entities": []}, {"text": "In order to verify that the rewriting rules in fact preserve the semantic meanings, we gathered a sample of 100 system-inferred sentences that were closest to a gold inference (step 4 in Section 5.3).", "labels": [], "entities": []}, {"text": "Each inferred sentence is judged as whether it is a valid rewrite of one or more of the raw inferences.", "labels": [], "entities": []}, {"text": "A valid rewrite does not introduce new semantic information.", "labels": [], "entities": []}, {"text": "91 out of the 100 were judged as valid by a majority of three human judges.", "labels": [], "entities": []}, {"text": "As such, the rewriting system is not abusively overgenerating sentences that are semantically different and match to gold inferences, increasing the recall score.", "labels": [], "entities": [{"text": "recall score", "start_pos": 149, "end_pos": 161, "type": "METRIC", "confidence": 0.9884418249130249}]}], "tableCaptions": [{"text": " Table 1: Results of automatic inference evaluation de- scribed in Section 5.3. cf stands for counterfactual in- ferences, cls for clause-taking, req for request, q for  question, oth for other.", "labels": [], "entities": [{"text": "Section 5.3", "start_pos": 67, "end_pos": 78, "type": "DATASET", "confidence": 0.9094040989875793}]}, {"text": " Table 2: Results of majority human evaluation of sys- tem generated inferences. Evaluation on 127 inferences  with from the test set by 3 or 4 people per inference.  *Correctness is evaluated on whether the sentence is  a reasonable inference in conversation, allowing for  some awkwardness in phrasing. Context, means the  correctness is highly context-dependent. The inference  type labels in the header row are the same as in", "labels": [], "entities": []}]}