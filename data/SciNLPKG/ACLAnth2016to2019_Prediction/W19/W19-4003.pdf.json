{"title": [{"text": "Crowdsourcing Discourse Relation Annotations by a Two-Step Connective Insertion Task", "labels": [], "entities": [{"text": "Crowdsourcing Discourse Relation Annotations", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.7818704545497894}]}], "abstractContent": [{"text": "The perspective of being able to crowd-source coherence relations bears the promise of acquiring annotations for new texts quickly, which could then increase the size and variety of discourse-annotated corpora.", "labels": [], "entities": []}, {"text": "It would also open the avenue to answering new research questions: Collecting annotations from a larger number of individuals per instance would allow to investigate the distribution of inferred relations, and to study individual differences in coherence relation interpretation.", "labels": [], "entities": [{"text": "coherence relation interpretation", "start_pos": 245, "end_pos": 278, "type": "TASK", "confidence": 0.6466859678427378}]}, {"text": "However, annotating coherence relations with untrained workers is not trivial.", "labels": [], "entities": []}, {"text": "We here propose a novel two-step annotation procedure, which extends an earlier method by Scholman and Demberg (2017a).", "labels": [], "entities": []}, {"text": "In our approach, coherence relation labels are inferred from connec-tives that workers insert into the text.", "labels": [], "entities": []}, {"text": "We show that the proposed method leads to replicable coherence annotations, and analyse the agreement between the obtained relation labels and annotations from PDTB and RST-DT on the same texts.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.9408873319625854}]}], "introductionContent": [{"text": "Implicit coherence relations are connections between text segments that are not overtly marked.", "labels": [], "entities": []}, {"text": "Annotating implicit coherence relations using crowd-sourcing is methodologically challenging, because assigning coherence relation labels as used in popular discourse frameworks like the Penn Discourse Treebank style (PDTB, or the Rhetorical Structure Theory) requires linguistic knowledge and substantial training.", "labels": [], "entities": [{"text": "Penn Discourse Treebank style (PDTB", "start_pos": 187, "end_pos": 222, "type": "DATASET", "confidence": 0.9190410872300466}]}, {"text": "It is thus not possible to obtain high quality annotations of coherence relation labels from untrained crowd workers (.", "labels": [], "entities": []}, {"text": "A more promising method for obtaining discourse annotations through crowd-sourcing is to ask workers to insert discourse connectives).", "labels": [], "entities": []}, {"text": "However, this method so far has only been used in settings where it was sufficient to give workers a small set of connectives to choose from, and not in broad-coverage coherence relation annotation.", "labels": [], "entities": []}, {"text": "For example, focused on identifying cases where several coherence relations may hold between two segments.", "labels": [], "entities": []}, {"text": "They provided participants with relations that were already marked with a discourse adverbial, and asked them to additionally insert a conjunction out of a list of six highly frequent connectives (and, but, so, because, before, or).", "labels": [], "entities": []}, {"text": "Highly frequent connectives are often ambiguous, for instance, the insertion of but does not allow us to infer whether the relation is a contrast or a concession relation.", "labels": [], "entities": []}, {"text": "When we want to do finegrained relation annotation, providing only general connectives is thus not sufficient.", "labels": [], "entities": []}, {"text": "addressed this problem by restricting the types of relations that could occur in their experiment.", "labels": [], "entities": []}, {"text": "They selected six types of coherence relations from the overlapping part of the PDTB2.0 and RST-DT corpora, and re-annotated them using crowd-sourced annotators.", "labels": [], "entities": [{"text": "PDTB2.0 and RST-DT corpora", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.7243594378232956}]}, {"text": "Workers in this study could choose from a list of connectives which distinguish unambiguously between the six relation types of interest.", "labels": [], "entities": []}, {"text": "For example, instead of the connective but, they provided a choice between nevertheless and by contrast.", "labels": [], "entities": []}, {"text": "However, for annotating text more generally, we need to provide connectives that can capture all types of relations, and on top of that make sure that the insertions can help us to disambiguate between coherence relations.", "labels": [], "entities": []}, {"text": "This poses the problem that the list of connectives that participants should choose from would become unwieldily large -it's unlikely that participants would be very capable of choosing one connective to insert from a list of 50 connectives.", "labels": [], "entities": []}, {"text": "In this work, we therefore propose anew annotation procedure which builds on the method of.", "labels": [], "entities": []}, {"text": "Our contributions in this paper consist of: \u2022 a novel two-step procedure for eliciting discourse connective insertions from na\u00a8\u0131vena\u00a8\u0131ve workers; \u2022 a demonstration that the generalized method is comparable in reliability of annotations to the original more restricted crowd-sourcing method proposed by); \u2022 a \"connective bank\" consisting of 800 entries including traditional connectives as well as variations of connectives and alternative lexicalizations; \u2022 an analysis comparing the obtained coherence labels to labels from professionally annotated discourse treebanks.", "labels": [], "entities": []}, {"text": "Our analysis shows that crowd-sourcing captures a mixture of characteristics from PDTB 3.0 and RST-DT annotations.", "labels": [], "entities": []}, {"text": "The data collected in this study, including the crowdsourced annotations of 447 implicit discourse relations and a connective bank of 800 connective phrases, is freely available for the community.", "labels": [], "entities": []}], "datasetContent": [{"text": "The objective of this experiment is to confirm the proposed task design and compare it with the forced-choice design proposed by Scholman and Demberg (2017a).", "labels": [], "entities": []}, {"text": "The items used in Experiment 1 were chosen such that RST-DT annotations for the same text spans were comparable to the PDTB annotations (for CONTRAST, CONCESSION, CAUSE AND CON-JUNCTION).", "labels": [], "entities": []}, {"text": "This means that the items were not entirely representative of a real-life annotation setting (i.e., the relations might have been easier to annotate).", "labels": [], "entities": []}, {"text": "We therefore conducted another experiment using items that were selected without this constraint.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Agreement of the majority crowd-sourced and  RST-DT labels with the PDTB3 labels and the label  distribution of the random set.", "labels": [], "entities": [{"text": "PDTB3 labels", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9444008469581604}]}]}