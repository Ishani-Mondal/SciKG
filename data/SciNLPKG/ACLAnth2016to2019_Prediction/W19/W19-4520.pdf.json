{"title": [{"text": "Towards Assessing Argumentation Annotation -A First Step", "labels": [], "entities": [{"text": "Assessing Argumentation Annotation", "start_pos": 8, "end_pos": 42, "type": "TASK", "confidence": 0.8567466139793396}]}], "abstractContent": [{"text": "This paper presents a first attempt at using Walton's argumentation schemes for annotating arguments in Swedish political text and assessing the feasibility of using this particular set of schemes with two linguistically trained annotators.", "labels": [], "entities": []}, {"text": "The texts are not pre-annotated with argumentation structure beforehand.", "labels": [], "entities": []}, {"text": "The results show that the annotators differ both in number of annotated arguments and selection of the conclusion and premises which makeup the arguments.", "labels": [], "entities": []}, {"text": "They also differ in their labeling of the schemes, but grouping the schemes increases their agreement.", "labels": [], "entities": []}, {"text": "The outcome from this will be used to develop guidelines for future annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Argumentation mining -the automatic recognition and classification of arguments and their components in text -is a useful technology fora number of practical text-processing applications, both commercial and academic, and in the latter case not least as a component of research tools in the digital humanities and social sciences.", "labels": [], "entities": [{"text": "Argumentation mining -the automatic recognition and classification of arguments and their components in text", "start_pos": 0, "end_pos": 108, "type": "TASK", "confidence": 0.7708959857622782}]}, {"text": "Many different annotation schemes for argument analysis have been proposed in the literature (, and a central concern in the context of argumentation mining is to arrive at a scheme which is both expressive enough for the intended tasks and explicitly defined in away which makes it amenable to high-accuracy automatic processing.", "labels": [], "entities": [{"text": "argument analysis", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.9017052352428436}, {"text": "argumentation mining", "start_pos": 136, "end_pos": 156, "type": "TASK", "confidence": 0.8514936566352844}]}, {"text": "Automatic linguistic annotation often requires the use of a ground-truth data set -a gold standard -for evaluating -and often training -different kinds of algorithms and software.", "labels": [], "entities": [{"text": "Automatic linguistic annotation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7328778704007467}]}, {"text": "Since the gold standard annotations will invariably need to be introduced by humans, we require an annotation scheme which human annotators can learn (in a reasonable amount of time) to apply with high accuracy and high inter-annotator agreement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9976977705955505}]}, {"text": "One of the most elaborate and extensive efforts to devise a comprehensive set of argumentation schemes is that by, which builds on along line of works in philosophy and law studies.", "labels": [], "entities": []}, {"text": "further explicitly intend their schemes to be usable \"in AI\".", "labels": [], "entities": []}, {"text": "The 60 schemes (with additional sub-schemes in many cases) presented in the book are given detailed, formalized descriptions, and in the present paper we describe and discuss the initial stage in an effort intended to evaluate the suitability and usefulness of this set of schemes for argumentation mining.", "labels": [], "entities": [{"text": "argumentation mining", "start_pos": 285, "end_pos": 305, "type": "TASK", "confidence": 0.9692964255809784}]}, {"text": "As indicated above, a prerequisite for this is that a sufficient amount of suitable text can be manually annotated with high inter-annotator agreement.", "labels": [], "entities": []}, {"text": "Consequently, we have initiated an annotation effort (the first of several), where a small set of Swedish political texts (newspaper editorials) have been annotated using the schemes of.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first annotation study which applies Walton's schemes directly to text, without any preannotated structure step beforehand.", "labels": [], "entities": []}, {"text": "In the present paper, we present and discuss the results of this exercise, and outline what the next steps of this effort should be, based on these results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to measure inter-annotator agreement (IA) we use the measure in Equation 1 based on the S\u00f8rensen-Dice coefficient, where a 1 and a 2 are the sets of annotations from each annotator, and m is the set of pairs of annotations from a 1 and a 2 that are matching (i.e. they are considered equivalent).", "labels": [], "entities": [{"text": "inter-annotator agreement (IA)", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.6193201184272766}]}, {"text": "Annotations can be either units (spans of text representing premises or conclusions) or arguments (a conclusion with one or more spans).", "labels": [], "entities": []}, {"text": "We don't use measures such as Fleiss' kappa or Krippendorff's alpha because these measures calculate agreement over annotation tasks that consist of assigning a discrete label or score to each element in a set, which is different to annotating spans over continuous text.", "labels": [], "entities": [{"text": "Fleiss'", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9189605116844177}]}, {"text": "Previous work on argumentation annotation such as as in Stab and Gurevych (2017) uses them because their annotation task is defined as marking whether predefined spans of texts door do not contain annotations or units, but in our annotation task the annotators themselves create the spans.", "labels": [], "entities": []}, {"text": "To determine if two units are matching, the amount of overlap between the strings representing the units is compared to a given threshold \u03b1.", "labels": [], "entities": []}, {"text": "The strings are defined as ranges of character indices within the text.", "labels": [], "entities": []}, {"text": "The amount of overlap is measured as the ratio between the length of the longest common continuous substring to both strings and the length of the longest of both strings.", "labels": [], "entities": [{"text": "overlap", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.8528139591217041}]}, {"text": "For example, the units below have an overlap of 0.68.", "labels": [], "entities": [{"text": "overlap", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9898703694343567}]}, {"text": "Two values of \u03b1 are used in the experiments.", "labels": [], "entities": []}, {"text": "A strict one of 0.9, which can still account for small differences in whitespace, and a more lenient threshold of 0.5.", "labels": [], "entities": []}, {"text": "In order to compare how well the annotators agree, the arguments are compared unit by unit.", "labels": [], "entities": []}, {"text": "First, the conclusions of the arguments are compared, and if the conclusion matches, the premises are compared.", "labels": [], "entities": []}, {"text": "Given both a matching conclusion and premise, the schemes of the two matching arguments are compared.", "labels": [], "entities": []}, {"text": "If a unit occurs more than once, it will belong to different arguments.", "labels": [], "entities": []}, {"text": "Each occurrence is thus treated as a unique occurrence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Occurrences of units", "labels": [], "entities": [{"text": "Occurrences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.924929141998291}]}, {"text": " Table 3: Usage of schemes for Annotator 1 and 2", "labels": [], "entities": []}, {"text": " Table 4: IA and m for conclusions.", "labels": [], "entities": [{"text": "IA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9967531561851501}, {"text": "m", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.9817453026771545}]}, {"text": " Table 5. With the full overlap  \u03b1, used for both premises and conclusions, the IA  is 0.56 for at least one matching premise. With the  same \u03b1, only 6 of the matching conclusions have  all premises matching. Using the 0.5 \u03b1, the IA is  0.71 for at least one matching premise, and 0.20 all  premises matching. The IA within all arguments is  low for both \u03b1.", "labels": [], "entities": [{"text": "IA", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9980618357658386}, {"text": "IA", "start_pos": 230, "end_pos": 232, "type": "METRIC", "confidence": 0.9861998558044434}, {"text": "IA", "start_pos": 314, "end_pos": 316, "type": "METRIC", "confidence": 0.9906560778617859}]}, {"text": " Table 5: IA and m for premises, given a matching con- clusion.", "labels": [], "entities": [{"text": "IA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9956343770027161}]}, {"text": " Table 6: IA and m for only premises.", "labels": [], "entities": [{"text": "IA", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9986918568611145}, {"text": "m", "start_pos": 17, "end_pos": 18, "type": "METRIC", "confidence": 0.5917608141899109}]}, {"text": " Table 8: Matching schemes with the new groups of  schemes, given a matching conclusion and at least one  premise.", "labels": [], "entities": []}, {"text": " Table 9: Matching schemes with the new groups of  schemes, only conclusions.", "labels": [], "entities": []}]}