{"title": [{"text": "A Wide-Coverage Symbolic Natural Language Inference System", "labels": [], "entities": [{"text": "Symbolic Natural Language Inference", "start_pos": 16, "end_pos": 51, "type": "TASK", "confidence": 0.6917027235031128}]}], "abstractContent": [{"text": "We present a system for Natural Language Inference which uses a dynamic semantics converter from abstract syntax trees to Coq types.", "labels": [], "entities": [{"text": "Natural Language Inference", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.6528041958808899}]}, {"text": "It combines the fine-grainedness of a dynamic semantics system with the pow-erfulness of a state-of-the-art proof assistant.", "labels": [], "entities": []}, {"text": "We evaluate the system on all sections of the FraCaS test suite, excluding section 6.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9805744091669718}]}, {"text": "This is the first system that does a complete run on the anaphora and ellip-sis sections of the FraCaS.", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.984262228012085}]}, {"text": "It has a better overall accuracy than any previous system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9979825019836426}]}], "introductionContent": [{"text": "Natural Language Inference (NLI) is the task of determining of whether an NL hypothesis H follows from an NL premise(s) P.", "labels": [], "entities": [{"text": "Natural Language Inference (NLI)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.79605200390021}]}, {"text": "NLI has received a lot of attention in the Computational Semantics literature and has been approached using a variety of techniques, ranging from logical approaches, all the way to the recent Deep Learning (DL) models for NLI.", "labels": [], "entities": []}, {"text": "The latter approaches, following a general trend in NLP, have been dominating NLI and a number of impressive results have been produced.", "labels": [], "entities": [{"text": "NLI", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8559540510177612}]}, {"text": "State-of-the-art DL systems achieve an accuracy of around 0.9 when tested on suitable datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9995267391204834}]}, {"text": "However, the datasets that are used are assuming a definition of inference that can bethought to be 'looser' or less precise compared to the definition assumed in platforms based in logical approaches.", "labels": [], "entities": []}, {"text": "For example, consider the following example from the SNLI dataset, predominatly used to test DL approaches: These are the three systems with the best results on SNLI in increasing order at the time of writing.", "labels": [], "entities": [{"text": "SNLI dataset", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.8916780948638916}]}, {"text": "(1) PA man selling donuts to a customer during a world exhibition event held in the city of Angeles.", "labels": [], "entities": []}, {"text": "H A woman drinks her coffee in a small cafe.", "labels": [], "entities": []}, {"text": "Label: Contradiction In (1), a number of non-trivial assumptions have to be made in order to arrive at a contradiction: a) the two situations described have to betaken to refer to the same situation in order to judge that the latter contradicts the former, b) the indefinite article in the premise has to be identified with the indefinite article in the hypothesis.", "labels": [], "entities": []}, {"text": "(Additionally considering that a person cannot be a man selling donuts and a woman drinking coffee at the same time.)", "labels": [], "entities": []}, {"text": "While this can be part of the reasoning humans perform, it is not the only possibility.", "labels": [], "entities": []}, {"text": "More precise, logical reasoning is also a possibility, and will render the above label as unknown.", "labels": [], "entities": []}, {"text": "Furthermore, reasoning can get very fine-grained as the Contained Deletion ellipsis example (2) below shows: For this reason, and despite the dominance of DL approaches in pretty much all NLP tasks, logical approaches continue to be developed and evaluated on datasets like the FraCaS test suite and the SICK dataset ().", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 278, "end_pos": 295, "type": "DATASET", "confidence": 0.9631178776423136}, {"text": "SICK dataset", "start_pos": 304, "end_pos": 316, "type": "DATASET", "confidence": 0.8011240065097809}]}, {"text": "Bernardy and Chatzikyriakidis (2017) define a correspondence between abstract syntax parse trees of the FraCas examples, parsed using the Grammatical Framework (GF, Ranta (2011)), and modern typetheoretic semantics that are output in the Coq proof assistant (the FraCoq system).", "labels": [], "entities": [{"text": "FraCas examples", "start_pos": 104, "end_pos": 119, "type": "DATASET", "confidence": 0.9134667813777924}, {"text": "FraCoq system", "start_pos": 263, "end_pos": 276, "type": "DATASET", "confidence": 0.9371291100978851}]}, {"text": "The accuracy is 0.85 for 5 sections of the FraCaS test suite.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997571110725403}, {"text": "FraCaS test suite", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9787522157033285}]}, {"text": "The LANGPRO system presented by is based on a Natural Logic tableau theorem prover.", "labels": [], "entities": []}, {"text": "It achieves an accuracy of .82 on the SICK dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995773434638977}, {"text": "SICK dataset", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9114031195640564}]}, {"text": "In this paper, we concentrate on this sort of fine-grained, logical reasoning.", "labels": [], "entities": []}, {"text": "In particular, we present a logic-based system that deals with many linguistic phenomena at the same time.", "labels": [], "entities": []}, {"text": "It is the first system covering the sections on ellipsis and anaphora in the FraCaS test suite and has the best coverage and accuracy on the overall test suite.", "labels": [], "entities": [{"text": "FraCaS test suite", "start_pos": 77, "end_pos": 94, "type": "DATASET", "confidence": 0.9836273193359375}, {"text": "coverage", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.985139787197113}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9987450838088989}]}], "datasetContent": [{"text": "We evaluated FraCoq against 8 sections of the FraCaS test suite, fora total of 259 cases.", "labels": [], "entities": [{"text": "FraCoq", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.8738463521003723}, {"text": "FraCaS test suite", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9665937821070353}]}, {"text": "We excluded only section 7, \"temporal reference\".", "labels": [], "entities": []}, {"text": "The reason for doing so is that, in our view, it contains too many examples which require ad-hoc treatment, and thus makes little sense to include without complementing it with a more thorough suite which captures a more complete landscape of the phenomena that section 7 touches.", "labels": [], "entities": []}, {"text": "FraCaS classifies each problem as either entailment (YES), entailment of the opposite (NO) or no entailment (UNK).", "labels": [], "entities": [{"text": "FraCaS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9521562457084656}, {"text": "YES", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.8583265542984009}, {"text": "NO) or no entailment (UNK)", "start_pos": 87, "end_pos": 113, "type": "METRIC", "confidence": 0.7022874355316162}]}, {"text": "In this work, we have amended the FraCaS suite to correct a few problems.", "labels": [], "entities": [{"text": "FraCaS suite", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9460528194904327}]}, {"text": "First, mentation, however, are.", "labels": [], "entities": []}, {"text": "as such (using an \"undef\" labelling), and we removed those.", "labels": [], "entities": []}, {"text": "Second, a few test cases occur twice in the suite, but with two different labellings (one YES and one UNK), with an annotation that those labellings correspond to different readings.", "labels": [], "entities": [{"text": "YES", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9927571415901184}, {"text": "UNK", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.8370530009269714}]}, {"text": "However, elsewhere in the suite, if a problem has several readings but only one has entailment, it occurs only once and is marked as YES.", "labels": [], "entities": [{"text": "YES", "start_pos": 133, "end_pos": 136, "type": "METRIC", "confidence": 0.989937424659729}]}, {"text": "To make the test suite consistent, if one reading yields entailment we have always considered it as YES.", "labels": [], "entities": [{"text": "YES", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9957296252250671}]}, {"text": "We have also removed case 199 (which appears to be vacuous).", "labels": [], "entities": [{"text": "case 199", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9548583030700684}]}, {"text": "Finally we changed the labelling of 6 cases which appeared to have been misclassified.", "labels": [], "entities": []}, {"text": "We note that the majority of the mistaken classifications occur in sections 3 and 4, which have not been previously attempted and thus, we propose, have not been properly scrutinized.", "labels": [], "entities": [{"text": "mistaken classifications", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.5058874487876892}]}, {"text": "In terms of comparison, this only has a minor effect, since our system is the first system to run sections 3 and 4.", "labels": [], "entities": []}, {"text": "Our system classifies a case as YES if a proof can be constructed from the premises to the hypothesis, NO if a proof of the negated hypothesis can be constructed and UNK otherwise.", "labels": [], "entities": [{"text": "YES", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9931634664535522}, {"text": "NO", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.9921581745147705}, {"text": "UNK", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.7934722304344177}]}, {"text": "Because we work with a non-decidable logic, one cannot in general conclude decisively that no proof exists.", "labels": [], "entities": []}, {"text": "Thus, we consider here that no proof exists if it cannot be constructed with reasonable effort.", "labels": [], "entities": []}, {"text": "In particular, we test at the minimum that the automatic proof search builtin Coq does not succeed before classifying a problem as UNK.", "labels": [], "entities": [{"text": "UNK", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.8812773823738098}]}, {"text": "3 shows a considerable improvement over earlier approaches in terms of coverage, with three more sections covered over previous approaches.", "labels": [], "entities": [{"text": "coverage", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9862581491470337}]}, {"text": "We thus cover 259 out of 337 cases (77%), compared to at most 174 cases (52%) in previous work.", "labels": [], "entities": []}, {"text": "Additionally, our system performs generally the: Accuracy of our system compared to others.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9993062019348145}]}, {"text": "\"Ours\" refers to the approach presented in this paper.", "labels": [], "entities": []}, {"text": "When a system does not handle the nominal number of test cases (shown in the second column), the actual number of test cases attempted is shown below the accuracy figure, in smaller font.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9995883107185364}]}, {"text": "\"FraCoq\" refers to the work of.", "labels": [], "entities": [{"text": "FraCoq", "start_pos": 1, "end_pos": 7, "type": "DATASET", "confidence": 0.9545471668243408}]}, {"text": "\"MINE\" refers to the approach of, \"NUT\" to the CCG system that utilizes the first-order automated theorem prover nutcracker, and \"Langpro\" to the system presented by.", "labels": [], "entities": [{"text": "MINE", "start_pos": 1, "end_pos": 5, "type": "METRIC", "confidence": 0.8575838208198547}]}, {"text": "A dash indicates that no attempt was made for the section.", "labels": [], "entities": [{"text": "dash", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9981266856193542}, {"text": "section", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.8233828544616699}]}, {"text": "best in terms of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9995461106300354}]}, {"text": "In particular, section 6 largely improves inaccuracy, which we attribute to our dynamic semantics analysis of comparatives.", "labels": [], "entities": []}, {"text": "error analysis Our system fails to correctly classify 28 cases out of 259.", "labels": [], "entities": []}, {"text": "We give here a summary of the missing features which are responsible for the failures.", "labels": [], "entities": []}, {"text": "The biggest source of error is incomplete handling of group readings.).", "labels": [], "entities": []}, {"text": "These are cases where a syntactic conjunction of individuals is treated as a semantic group, or where precise counting of the members of a group is necessary.", "labels": [], "entities": []}, {"text": "Other problematic cases include definite plurals with no universal readings (091, 094, 095).", "labels": [], "entities": []}, {"text": "Additionally, neither measure phrases (242) nor attributive comparatives (244, 245) are handled.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overruled FraCaS cases", "labels": [], "entities": [{"text": "Overruled", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8647841811180115}, {"text": "FraCaS", "start_pos": 20, "end_pos": 26, "type": "DATASET", "confidence": 0.7258747220039368}]}, {"text": " Table 2: Accuracy of our system compared to oth- ers. \"Ours\" refers to the approach presented in this  paper. When a system does not handle the nomi- nal number of test cases (shown in the second col- umn), the actual number of test cases attempted  is shown below the accuracy figure, in smaller  font. \"FraCoq\" refers to the work of", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9595192074775696}, {"text": "accuracy", "start_pos": 270, "end_pos": 278, "type": "METRIC", "confidence": 0.9992625117301941}, {"text": "FraCoq", "start_pos": 306, "end_pos": 312, "type": "DATASET", "confidence": 0.90120530128479}]}]}