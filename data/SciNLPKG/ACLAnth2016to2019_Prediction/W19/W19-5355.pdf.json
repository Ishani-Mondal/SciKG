{"title": [{"text": "SAO WMT19 Test Suite: Machine Translation of Audit Reports", "labels": [], "entities": [{"text": "SAO WMT19 Test Suite", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.8069807440042496}, {"text": "Machine Translation of Audit Reports", "start_pos": 22, "end_pos": 58, "type": "TASK", "confidence": 0.8394688725471496}]}], "abstractContent": [{"text": "This paper describes a machine translation test set of documents from the auditing domain and its use as one of the \"test suites\" in the WMT19 News Translation Task for translation directions involving Czech, English and Ger-man.", "labels": [], "entities": [{"text": "WMT19 News Translation Task", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.6696528196334839}, {"text": "translation directions", "start_pos": 169, "end_pos": 191, "type": "TASK", "confidence": 0.9018979370594025}]}, {"text": "Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9905441999435425}]}, {"text": "The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary.", "labels": [], "entities": []}, {"text": "For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.994532585144043}]}, {"text": "Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain mismatch is often the main sources of machine translation errors.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.734228253364563}]}, {"text": "At the same time, it has been suggested in the speech recognition area that models trained on extremely large data can perform well across domains, i.e. without any particular domain adaptation (.", "labels": [], "entities": [{"text": "speech recognition area", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8692511518796285}]}, {"text": "We believe that for some of the language pairs annually tested in the WMT shared translation task, the best machine translation systems may have grown to sizes where the domain dependence maybe less critical.", "labels": [], "entities": [{"text": "WMT shared translation task", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.9367720484733582}, {"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7379548251628876}]}, {"text": "At the same time, we know that most of current MT systems still operate at the level of individual sentences and therefore have no control over document-level coherence e.g. in terms of lexical choice.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9842958450317383}]}, {"text": "To investigate the two questions, domain independence and document-level coherence, we cleaned and prepared a dedicated set of documents from the auditing domain and submitted it as one of the \"test suites\" to this year's WMT News Translation Task.", "labels": [], "entities": [{"text": "WMT News Translation Task", "start_pos": 222, "end_pos": 247, "type": "TASK", "confidence": 0.78373022377491}]}, {"text": "The collection is called \"SAO WMT19 Test Suite\" after the Supreme Audit Office of the Czech Republic (SAO) who provided the original audit reports created in cooperation with other national supreme audit institutions (SAIs).", "labels": [], "entities": [{"text": "SAO WMT19 Test Suite", "start_pos": 26, "end_pos": 46, "type": "DATASET", "confidence": 0.6119013130664825}]}, {"text": "This paper is organized as follows: In Section 2 we describe the source and our processing of the test documents.", "labels": [], "entities": []}, {"text": "Section 3 provides automatic scores of WMT19 MT systems on the test suite and Section 4 presents the manual evaluation.", "labels": [], "entities": [{"text": "WMT19 MT", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.45927803218364716}]}, {"text": "One more document type, namely a sublease agreement, was evaluated separately, see Section 5.", "labels": [], "entities": []}, {"text": "We release the test suite for public use, see Section 6, and we conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "For automatic evaluation, we use several of common MT evaluation metrics ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.8912301361560822}]}, {"text": "Metrics listed with the prefix \"n\" are reversed (1 \u2212 score) so that higher numbers indicate a better translation in all the figures we report.", "labels": [], "entities": [{"text": "Metrics", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9458854794502258}, {"text": "translation", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9146454930305481}]}, {"text": "We calculate the score for each of the documents in our test suite separately and report the average score and the standard deviation.", "labels": [], "entities": []}, {"text": "The scores are detailed in The main observation across the tables is that all the scores heavily vary across individual documents.", "labels": [], "entities": []}, {"text": "The typical standard deviation is 3-5 for BLEU and similarly for other metrics.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 12, "end_pos": 30, "type": "METRIC", "confidence": 0.931636780500412}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.996812641620636}]}, {"text": "The metrics do not always agree on the overall ranking of the systems, as indicated by \"\" in the tables, but these differences are much smaller that the variance due to the particular documents.", "labels": [], "entities": []}, {"text": "A big caveat should betaken when interpreting all automatic scores as an estimate of real translation quality, because they are all based on the single reference translation.", "labels": [], "entities": []}, {"text": "See also the discussion in Section 4.2 below.", "labels": [], "entities": []}, {"text": "The evaluation of this small set containing one source document, one human translation and 11 machine translated documents was done manually.", "labels": [], "entities": []}, {"text": "The evaluation was partially blind.", "labels": [], "entities": []}, {"text": "Technically, the candidate translations were not labelled with the system name, but the main annotator could     guess some of the systems.", "labels": [], "entities": []}, {"text": "Only the systems online-X, Y and G are truly blind, we do not know their identity even from past evaluations.", "labels": [], "entities": []}, {"text": "We are confident that even the knowledge of the MT system did not affect our evaluation because we fully focused on the hard criteria such as named entity preservation or term consistence throughout the document.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.974371612071991}, {"text": "named entity preservation", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.6302713056405386}, {"text": "term consistence", "start_pos": 171, "end_pos": 187, "type": "TASK", "confidence": 0.6369012892246246}]}, {"text": "The only soft criterion included was the \"fluency\" one.", "labels": [], "entities": []}, {"text": "We have also included the reference document in the evaluation.", "labels": [], "entities": []}, {"text": "By inspecting several of the MT outputs, we first defined the assessment criteria.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9636490345001221}]}, {"text": "They generally fall into two categories: (1) target-only, and source-based.", "labels": [], "entities": []}, {"text": "Whereas in the former category, we consider only quality of the target texts on their own, regardless the source, in the latter we validate if the selected bits of information were preserved or corrupted during the translation process.", "labels": [], "entities": []}, {"text": "In the target-only category, we focused on the following: \u2022 fluency; \u2022 grammatical correctness (this is very strict and well defined in Czech; most errors were in morphological agreement and sometimes verb tense); \u2022 casing errors (esp.", "labels": [], "entities": []}, {"text": "in named entities); \u2022 incomprehensibility of the segment; \u2022 \"spasm\", i.e. the situation when the MT system gets stuck in repeating some tokens; \u2022 superfluous words; \u2022 missing words or a whole sentence.", "labels": [], "entities": [{"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9318071603775024}]}, {"text": "As for the source-based category, we have focused on the errors, which were formed either by wrong translation of a very domain-specific term or an inconsistence of used terms throughout the whole document.", "labels": [], "entities": []}, {"text": "\u2022 Document-specific terms: -Tenant; -Lessee; -Supplement (of the agreement); -Sublease agreement; -Contracting parties; -Apartment in question; -Equipment (e.g. the kitchen); -Amenities (e.g. a cellar or a segment of the garden); -Housing cooperative; -Team of owners; -Term of the lease; -The specification of the supplement (\"no. 1\").", "labels": [], "entities": [{"text": "Supplement", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.960378110408783}]}, {"text": "In the category of \"Document-specific terms\", we focused on evaluation whether: \u2022 the term is translated correctly, incorrectly (incl.", "labels": [], "entities": []}, {"text": "not translated at all), or missing altogether; \u2022 the target term is preserved in the document.", "labels": [], "entities": []}, {"text": "It should be noted that the MT system was often free to choose from several translation options of a term.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9703965187072754}]}, {"text": "At the same time, a very important criterion was whether the translation of each of the terms was consistent throughout the document and also whether it did not clash with other choices.", "labels": [], "entities": []}, {"text": "For example, each of the terms \"tenant\" and \"lessee\" could be-depending on the particular situation-correctly translated as \"pronaj\u00edmatelka\", \"n\u00e1jemkyn\u011b\" or \"podn\u00e1jemkyn\u011b\" (all are feminine variants of the words, because incidentally, it was women who were entering this sample agreement).", "labels": [], "entities": []}, {"text": "If the two different parties however happened to have been referred to in anyway that could lead to confusion, we marked this as a (serious) error.", "labels": [], "entities": []}, {"text": "In some cases, we had a strict expectation.", "labels": [], "entities": []}, {"text": "For instance the term \"sublease\" could be translated into Czech in principle either as \"pron\u00e1jem\" (which corresponds to the relationship between a landlord and a tenant) or as \"podn\u00e1jem\" (which corresponds to the relationship between a tenant and a lessee).", "labels": [], "entities": []}, {"text": "Based on the text of the agreement, it was however clear that the correct term is \"podn\u00e1jem\" (the tenant is not the actual owner of the property), so we demanded the this particular choice.", "labels": [], "entities": []}, {"text": "Because of the relatively small amount of data, the evaluation was done on paper, see.", "labels": [], "entities": []}, {"text": "The annotations of \"source-based\" error types were done with respect to the source text using a fixed set of \"markables\", i.e. the set of occurrences of words and expressions to annotate for correctness.", "labels": [], "entities": []}, {"text": "The set of markables was identical for all the candidate translations.", "labels": [], "entities": []}, {"text": "Each markable in each translation candidate received a label indicating if it was translated correctly, with an error, or if was fully missing.", "labels": [], "entities": []}, {"text": "The \"target-only\" error types were marked independently for each system, with no number of markable positions given apriori.", "labels": [], "entities": []}, {"text": "The question was how to deal with inconsistency in used terms.", "labels": [], "entities": []}, {"text": "At the beginning it was not clear whether we should assume that the first occurrence of term \"defines\" it for the rest of the Target-Only Source-Based Total System Errs (Miss) Errs (Miss) Errs (Miss: Total number of errors \"Errs\", and of those the cases when the output was completely missing \"(Miss)\", by English-Czech WMT19 news translation systems applied to the sublease agreement.", "labels": [], "entities": [{"text": "WMT19 news translation", "start_pos": 320, "end_pos": 342, "type": "TASK", "confidence": 0.7827634811401367}]}, {"text": "document or whether we should take the most frequent one as the \"intended one\" by the MT system and treat other translations as errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.7320342659950256}]}, {"text": "After the first round of corrections, we chose the first option.", "labels": [], "entities": []}, {"text": "Some terms, e.g. \"tenant\", \"lessee\" or \"agreement\" had always only one correct translation, but some, e.g. \"sublease\" could have had multiple possible translations.", "labels": [], "entities": []}, {"text": "In these latter cases, we always marked the first occurrence as correct.", "labels": [], "entities": [{"text": "occurrence", "start_pos": 50, "end_pos": 60, "type": "METRIC", "confidence": 0.8008146286010742}]}, {"text": "The summary of manual evaluation is presented in.", "labels": [], "entities": []}, {"text": "Errors in the source-based categories are more frequent than in target-only.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9715405106544495}]}, {"text": "This is mainly due to the incorrect translation of the term \"lessee\" (see Section 5.4.2 below).", "labels": [], "entities": []}, {"text": "One thing worth mentioning is the 9 errors and 3 omissions in the reference translation.", "labels": [], "entities": []}, {"text": "This can be partly attributed to Czech being in fact the original and English (i.e. the source for MT systems) its translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9610095024108887}]}, {"text": "What is a good Czech\u2192English manual translation is not always literal enough when observed from the English side.", "labels": [], "entities": [{"text": "Czech\u2192English manual translation", "start_pos": 15, "end_pos": 47, "type": "TASK", "confidence": 0.5043818175792694}]}, {"text": "Three errors were for instance incurred from one single case where the Czech text referred to the agreement itself onetime less than the English text, but this \"missing reference\" (fully acceptable in the Czech\u2192English direction) counted as several missing expressions.", "labels": [], "entities": []}, {"text": "As for the true errors, there was one incorrect translation of term \"lessee\" and one mistake in the number of the Supplement.", "labels": [], "entities": [{"text": "number", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9690161347389221}, {"text": "Supplement", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.46804970502853394}]}, {"text": "The number of errors considerably varies across the systems.", "labels": [], "entities": [{"text": "errors", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.7641300559043884}]}, {"text": "The best system) in our evaluation is also the winner on news in the evaluation last year.", "labels": [], "entities": []}, {"text": "As report, this system significantly outperformed humans at the level of individual sentences in that evaluation.", "labels": [], "entities": []}, {"text": "In our setting, the number of errors by CUNI-Transformer-T2T-2018 is twice the number of errors in the reference, but aside from term choice discussed in Section 5.4.2, one could say that the translation is very good.", "labels": [], "entities": [{"text": "CUNI-Transformer-T2T-2018", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.9408741593360901}]}, {"text": "In the target-only category, we did not have any pre-defined items that could be corrector incorrect.", "labels": [], "entities": []}, {"text": "Therefore the number of errors varies greatly across the systems.", "labels": [], "entities": []}, {"text": "From the lowest number of errors in the CUNI-Transformer-T2T-2019 (5 errors) and in CUNI-Transformer-T2T-2018 (6 errors) to the very high numbers in online-X and online-G (48 and 34 errors, respectively).", "labels": [], "entities": [{"text": "CUNI-Transformer-T2T-2019", "start_pos": 40, "end_pos": 65, "type": "DATASET", "confidence": 0.9513561725616455}, {"text": "CUNI-Transformer-T2T-2018", "start_pos": 84, "end_pos": 109, "type": "DATASET", "confidence": 0.952014148235321}]}, {"text": "As for the \"(Miss)\" counts, there were two types of situations: (1) only a single word was missing in the output and (2) the whole sentence or a half of a paragraph was not there.", "labels": [], "entities": []}, {"text": "The second case often lead to a large increase in the \"(Miss)\" count because several markables from the source were supposed to appear in the lost part.", "labels": [], "entities": [{"text": "the \"(Miss)\" count", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.8873157501220703}]}, {"text": "The systems uedin and online-X were most affected by this.", "labels": [], "entities": []}, {"text": "Another interesting fact worth mentioning is that even though the system online-Y had a relatively low number of mistakes, those errors made the readability and the comprehensibility of the message substantially more difficult than e.g. the translation by online-B with a higher error count.   is important but their type can be critical, too.", "labels": [], "entities": []}, {"text": "We already mentioned the missing sentences or \"spasm\", which accounted for the 14 missing term translations in the output of uedin.", "labels": [], "entities": []}, {"text": "Another interesting case is a \"misunderstanding\" of the MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9842917919158936}]}, {"text": "For instance, uedin system misunderstood \"I.\"", "labels": [], "entities": []}, {"text": "(the Roman numeral) for the pronoun \"I\" or mistranslated the \"ZIP CODE\" as \"ob\u010danka\" (personal ID card).", "labels": [], "entities": []}, {"text": "It is exactly these types of errors, which are the most serious from the reader's point of view.", "labels": [], "entities": []}, {"text": "provides further details on error types observed in the outputs of individual MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9771949648857117}]}, {"text": "The table is again sorted by the total number of errors as in.", "labels": [], "entities": []}, {"text": "We see that the best system (CUNI-Transformer-T2T-2018) fully failed in the translation of the terms \"lessee\", \"amenities\" and \"term of the lease\".", "labels": [], "entities": [{"text": "CUNI-Transformer-T2T-2018", "start_pos": 29, "end_pos": 54, "type": "DATASET", "confidence": 0.876776397228241}]}, {"text": "This system was also the only one which dealt well with abbreviations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of languages in SAO Test Suite. Lan- guages in bold were used in WMT19 Shared Transla- tion Task.", "labels": [], "entities": [{"text": "SAO Test Suite", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.7746926546096802}, {"text": "WMT19 Shared Transla- tion Task", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.7090251247088114}]}, {"text": " Table 2: Evaluated language pairs, documents and MT  systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9847840070724487}]}, {"text": " Table 5: Automatic scores for German\u2192English. \"\" marks scores out of sequence.", "labels": [], "entities": []}, {"text": " Table 6: Automatic scores for Czech\u2192German. \"\" marks scores out of sequence. Note that online systems use  parallel data while the others use only monolingual data.", "labels": [], "entities": []}, {"text": " Table 7: Automatic scores for German\u2192Czech. \"\" marks scores out of sequence. Note that online systems use  parallel data while the others use only monolingual data.", "labels": [], "entities": []}, {"text": " Table 8: Summary of manual annotations.", "labels": [], "entities": []}, {"text": " Table 9: Mean scores of English\u2192Czech translation obtained in manual evaluation. The systems are sorted by the  \"coherence and overall understanding\" criterion. Higher scores are better. \"\" marks scores out of sequence.", "labels": [], "entities": []}, {"text": " Table 10: Mean ordinal numbers of English\u2192Czech systems sorted by manual evaluation scores for each annotator.  Lower numbers are better.", "labels": [], "entities": []}, {"text": " Table 11: Mean scores of English\u2192German translation obtained in manual evaluation. The systems are sorted by  the \"coherence and overall understanding\" criterion. Higher scores are better.", "labels": [], "entities": []}, {"text": " Table 12: Mean scores of German\u2192English translation obtained in manual evaluation. The systems are sorted by  the \"coherence and overall understanding\" criterion. Higher scores are better.", "labels": [], "entities": []}, {"text": " Table 13: Summary of errors found by SAO annotators.", "labels": [], "entities": [{"text": "SAO", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.6670348048210144}]}, {"text": " Table 14: Total number of errors \"Errs\", and of those the cases when the output was completely missing \"(Miss)\",  by English-Czech WMT19 news translation systems applied to the sublease agreement.", "labels": [], "entities": [{"text": "Total number of errors \"", "start_pos": 11, "end_pos": 35, "type": "METRIC", "confidence": 0.8781901240348816}, {"text": "Errs", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.6229256391525269}, {"text": "WMT19 news translation", "start_pos": 132, "end_pos": 154, "type": "TASK", "confidence": 0.7522531946500143}]}]}