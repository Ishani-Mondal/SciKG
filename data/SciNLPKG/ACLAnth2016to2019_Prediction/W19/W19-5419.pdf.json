{"title": [{"text": "Exploring Transfer Learning and Domain Data Selection for the Bio-medical translation", "labels": [], "entities": [{"text": "Domain Data Selection", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.6827480792999268}, {"text": "Bio-medical translation", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.8618163168430328}]}], "abstractContent": [{"text": "Transfer Learning and Selective data training are two of the many approaches being extensively investigated to improve the quality of Neural Machine Translation systems.", "labels": [], "entities": [{"text": "Transfer Learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9600327908992767}, {"text": "Selective data training", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.7581067681312561}, {"text": "Neural Machine Translation", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.8571187853813171}]}, {"text": "This paper presents a series of experiments by applying transfer learning and selective data training for participation in the Bio-medical shared task of WMT19.", "labels": [], "entities": [{"text": "Bio-medical shared task", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.6351459622383118}, {"text": "WMT19", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.4132944345474243}]}, {"text": "We have used Information Retrieval to selectively choose related sentences from out-of-domain data and used them as additional training data using transfer learning.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.7576252222061157}]}, {"text": "We also report the effect of tokenization on translation model performance.", "labels": [], "entities": [{"text": "translation model", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8905253112316132}]}], "introductionContent": [{"text": "This paper describes the first system submission by Fatima Jinnah Women University under the NRPU project (NRPU-FJ) for the Biomedical task.", "labels": [], "entities": [{"text": "NRPU project (NRPU-FJ)", "start_pos": 93, "end_pos": 115, "type": "DATASET", "confidence": 0.9305891394615173}, {"text": "Biomedical task", "start_pos": 124, "end_pos": 139, "type": "TASK", "confidence": 0.9261385202407837}]}, {"text": "We have built our systems using the paradigm of Neural Machine Translation.", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7787555456161499}]}, {"text": "We worked on translation between French and English (in both directions) and incorporated domain adaption by using selective data training utilizing information retrieval to retrieve domain related sentences from out-of-domain corpus.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9692565202713013}, {"text": "domain adaption", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.718169704079628}]}, {"text": "Neural Machine Translation (NMT)), is the current state-of-theart in Machine Translation.", "labels": [], "entities": [{"text": "Neural Machine Translation (NMT))", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7703980306784312}, {"text": "Machine Translation", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.8693466782569885}]}, {"text": "Since its arrival, active research is being done to investigate the field and exploit its benefits to produce quality translations.", "labels": [], "entities": []}, {"text": "These efforts have resulted instate of the art translation architectures (.", "labels": [], "entities": []}, {"text": "Despite the winning results of NMT over it's counterpart Statistical Machine Translation (SMT) for large training corpora; the quality of NMT systems for low resource languages and smaller corpora is still a challenge (.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7709596951802572}]}, {"text": "To overcome this challenge various studies explore numerous techniques to improve NMT quality especially in low resource settings.", "labels": [], "entities": [{"text": "NMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9762008190155029}]}, {"text": "Domain adaptation, transfer learning (, fine tuning ( and data selective training (van der; are few terms being interchangeably used for such techniques as reported in the literature.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8338981866836548}, {"text": "transfer learning", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.9623099267482758}]}, {"text": "As is common in machine learning approaches, the quality of the system being built depends on the data used to train the system.", "labels": [], "entities": []}, {"text": "This was true for SMT systems and still holds significance for NMT based systems (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.993713915348053}]}, {"text": "The domain of the training data is crucial to get quality translations.", "labels": [], "entities": []}, {"text": "MT performance quickly degrades when the testing domain is different from the training domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9855308532714844}]}, {"text": "The reason for this degradation is that the learning models closely approximate the empirical distributions of the training data).", "labels": [], "entities": []}, {"text": "An MT system trained on parallel data from the news domain may not give appropriate translations when used to translate articles from the medical domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9574422240257263}]}, {"text": "The availability of language resources has increased over the last decade, previously this was mainly true only for monolingual corpora, whereas parallel corpora were a limited resource for most domains.", "labels": [], "entities": []}, {"text": "Most of the parallel data available to the research community was limited to texts produced by international organizations, parliamentary debates or legal texts (proceedings of the Canadian or European Parliament (), or of the United Nations, 1 MultiUN.", "labels": [], "entities": [{"text": "MultiUN", "start_pos": 245, "end_pos": 252, "type": "DATASET", "confidence": 0.5544039607048035}]}, {"text": "These only covered specific languages and domains which posed a challenge for the porta-bility of MT systems across different application domains and also its adaptability with respect to language within the same application domain.", "labels": [], "entities": [{"text": "MT", "start_pos": 98, "end_pos": 100, "type": "TASK", "confidence": 0.9868934154510498}]}, {"text": "Translation quality of medical texts also suffers due to fewer resources available to train a quality NMT system.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9397246241569519}]}, {"text": "Though, medical domain is a growing domain with respect to availability of parallel corpora like scielo (, EMEA (Tiedemann, 2012), Medline () and others in making are being made available to the research community.", "labels": [], "entities": [{"text": "EMEA (Tiedemann, 2012)", "start_pos": 107, "end_pos": 129, "type": "DATASET", "confidence": 0.7472962538401285}]}, {"text": "In this paper we present an approach which aims at increasing the training corpus by mining similar in domain (Bio Med) sentences from out of domain data.", "labels": [], "entities": []}, {"text": "We have developed NMT system for English-French language pair, for translation in both directions.", "labels": [], "entities": [{"text": "translation", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9627351760864258}]}, {"text": "Data selective training over cascaded transfer learning, approach has been used to train the model for English to French translation direction; whereas for French to English translation, data selective training approach was used over the whole corpus.", "labels": [], "entities": [{"text": "English to French translation direction", "start_pos": 103, "end_pos": 142, "type": "TASK", "confidence": 0.6419630527496338}, {"text": "French to English translation", "start_pos": 156, "end_pos": 185, "type": "TASK", "confidence": 0.703726127743721}]}, {"text": "The systems were built with tokenized and untokenized data to study the affect of tokenization in NMT.", "labels": [], "entities": []}, {"text": "Tokenization is an important prepossessing step to build MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9787463545799255}]}, {"text": "It benefits the MT system by splitting the words into sub-word units, removing punctuations and any other unnecessary tags from the corpus; thus decreasing the vocabulary and helping to translate the unknown words.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9848869442939758}]}, {"text": "Tokenization, where, improves the MT system quality it also raises a challenge of developing good quality tokenizers for each language.", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9860013723373413}]}, {"text": "Studies are performed to investigate tokenization for SMT systems, the question arises how important tokenization is for NMT?", "labels": [], "entities": [{"text": "tokenization", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.9743089079856873}, {"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9816911816596985}]}, {"text": "Could tokenization be ignored in NMT?", "labels": [], "entities": [{"text": "tokenization", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.9864538311958313}]}, {"text": "() investigate tokenization in NMT, to explore the impact of tokenization scheme selected for building NMT, but do not report that, if tokenization is not done, how much will it affect the quality of the NMT system.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.967994213104248}]}, {"text": "We present an answer to this question along with other explorations.", "labels": [], "entities": []}, {"text": "The rest of the paper structured as follows; Section 2 provides a brief overview of the related work and background.", "labels": [], "entities": []}, {"text": "Section 3 discusses the experimental setup.", "labels": [], "entities": []}, {"text": "Results for the different systems are presented in section 3.3.", "labels": [], "entities": []}, {"text": "The paper concludes with a brief conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have studied two approaches being used to improve NMT in low resource settings.", "labels": [], "entities": [{"text": "NMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9764255881309509}]}, {"text": "A detailed description of our experiments is provided in this section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU scores for English to French Models and English to French. \u21d2 shows the direction of transfer  learning while building the models. The best model from IR was chosen which was top2 for French IR and top1  for English IR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9995456337928772}]}]}