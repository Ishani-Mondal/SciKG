{"title": [{"text": "What do phone embeddings learn about Phonology?", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent work has looked at evaluation of phone embeddings using sound analogies and correlations between distinctive feature space and embedding space.", "labels": [], "entities": []}, {"text": "It has not been clear what aspects of natural language phonology are learnt by neural network inspired distributed representational models such as word2vec.", "labels": [], "entities": []}, {"text": "To study the kinds of phonological relationships learnt by phone embeddings, we present artificial phonology experiments that show that phone embeddings learn paradigmatic relationships such as phonemic and allophonic distribution quite well.", "labels": [], "entities": []}, {"text": "They are also able to capture co-occurrence restrictions among vowels such as those observed in languages with vowel harmony.", "labels": [], "entities": []}, {"text": "However, they are unable to learn co-occurrence restrictions among the class of consonants.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last few years, distributed representation models based on neural networks such as word2vec () and GloVe () have been of much importance in speech and natural language processing (NLP).", "labels": [], "entities": [{"text": "speech and natural language processing (NLP)", "start_pos": 149, "end_pos": 193, "type": "TASK", "confidence": 0.7597812488675117}]}, {"text": "The word2vec technique is a shallow neural network that takes a text corpus as input and outputs a vector space containing all unique words in the text.", "labels": [], "entities": []}, {"text": "The dense vector representations of words induced using word2vec have been shown to capture multiple degrees of similarities between words.", "labels": [], "entities": []}, {"text": "show that word embeddings can solve word analogy questions and sentence completion tasks.", "labels": [], "entities": [{"text": "word analogy questions", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8241478204727173}, {"text": "sentence completion tasks", "start_pos": 63, "end_pos": 88, "type": "TASK", "confidence": 0.7789650857448578}]}, {"text": "show that word embeddings represent words in continuous space, making it possible to perform algebraic operations, such as vector(King) \u2212 vector(Man) + vector(Woman) = vector(Queen).", "labels": [], "entities": []}, {"text": "Considerable attention has been paid to evaluating these vector representations using human judgement datasets.", "labels": [], "entities": []}, {"text": "use artificial language experiments to study the difference between similarity and relatedness in evaluating distributed semantic models.", "labels": [], "entities": []}, {"text": "Phone embeddings induced from phonetic corpora have been used in tasks such as word inflection) and sound sequence alignment (Sofroniev and C \u00b8 \u00a8 oltekin, 2018).", "labels": [], "entities": [{"text": "sound sequence alignment", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.649560958147049}]}, {"text": "show that dense vector representations of phones learnt using various techniques are able to solve analogies such asp is to b as t is to X, where X = d.", "labels": [], "entities": []}, {"text": "They also show that there is a significant correlation between distinctive feature space and the phone embedding space.", "labels": [], "entities": []}, {"text": "Our goal in this paper is to understand better the evaluation of phone embeddings.", "labels": [], "entities": []}, {"text": "We argue that significant correlation between distinctive feature space and phone embedding space cannot be automatically interpreted as the model's ability to capture facts about the phonology of natural language.", "labels": [], "entities": []}, {"text": "Since many distinctive features tend to be phonetically based, natural classes denoted by these features capture phonetic facts as well as phonological facts.", "labels": [], "entities": []}, {"text": "For example, the feature denotes the distinction between long and short vowels, which is a language-independent phonetic fact.", "labels": [], "entities": []}, {"text": "But, whether this distinction is a phonological fact varies from language to language.", "labels": [], "entities": []}, {"text": "It is important to make this distinction between phonetic facts and phonological facts when evaluating phone embeddings for their learning of phonology.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative methodology to evaluate word2vec's ability to learn phonological facts.", "labels": [], "entities": []}, {"text": "We define artificial languages with different kinds of phoneme-allophone distinctions and co-occurrence restrictions and study how well phone embeddings capture these relationships.", "labels": [], "entities": []}, {"text": "Several interesting insights regarding the relationship between phonetics and phonol-ogy, the role of distinctive features and the task of distinctive feature/phoneme induction accrue from our experiments.", "labels": [], "entities": [{"text": "distinctive feature/phoneme induction", "start_pos": 139, "end_pos": 176, "type": "TASK", "confidence": 0.654112184047699}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlation between distinctive feature space  and embedding space, all values significant (p < 0.01)", "labels": [], "entities": []}]}