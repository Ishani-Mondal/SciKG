{"title": [{"text": "JHU System Description for the MADAR Arabic Dialect Identification Shared Task", "labels": [], "entities": [{"text": "JHU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8487037420272827}, {"text": "MADAR Arabic Dialect Identification Shared", "start_pos": 31, "end_pos": 73, "type": "TASK", "confidence": 0.7083127319812774}]}], "abstractContent": [{"text": "Our submission to the MADAR shared task on Arabic dialect identification (Bouamor et al., 2019) employed a language modeling technique called Prediction by Partial Matching , an ensemble of neural architectures, and sources of additional data for training word embeddings and auxiliary language models.", "labels": [], "entities": [{"text": "MADAR shared task on Arabic dialect identification", "start_pos": 22, "end_pos": 72, "type": "TASK", "confidence": 0.6704463916165488}]}, {"text": "1 We found several of these techniques provided small boosts in performance, though a simple character-level language model was a strong baseline, and a lower-order LM achieved best performance on Subtask 2.", "labels": [], "entities": []}, {"text": "Interestingly, word embeddings provided no consistent benefit, and ensembling struggled to outperform the best component submodel.", "labels": [], "entities": []}, {"text": "This suggests the variety of architectures are learning redundant information, and future work may focus on encouraging decorrelated learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "While Modern Standard Arabic (MSA) is used across many countries for formal written communication, regional Arabic dialects vary substantially.", "labels": [], "entities": []}, {"text": "Dialect identification has traditionally been performed at the level of broad families of dialects-for instance grouping many dialects across the Arabian Peninsula together.", "labels": [], "entities": [{"text": "Dialect identification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9427194595336914}]}, {"text": "However, even within a single country there is often noticeable variation from one city to another.", "labels": [], "entities": []}, {"text": "The MADAR dataset and corresponding shared task aim to perform dialect identification at a finergrained level.", "labels": [], "entities": [{"text": "MADAR dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8248788416385651}, {"text": "dialect identification", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7762627899646759}]}, {"text": "Subtask 1 aims to distinguish travel phrases produced between Arabic dialect speakers from 25 different cities, as well as MSA.", "labels": [], "entities": [{"text": "MSA", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9126172661781311}]}, {"text": "Sub-1 Code available at https://bit.ly/2Kouo5X task 2 aims to distinguish Twitter users from different Arabic-speaking countries.", "labels": [], "entities": []}, {"text": "Along with the inherent difficulty of classifying short documents, highly-correlated modalities like topic and proper names can lead to overfitting, particularly for userdirected content like Twitter.", "labels": [], "entities": []}, {"text": "Our method attempts to address the former by using a language modeling technique that has empirically been found to perform well on extremely short documents.", "labels": [], "entities": []}, {"text": "For the latter, we employ ensembles of heterogeneous neural architectures and aggressive dropout, with the goal of finding abroad range of features that support the task without overfitting.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Performance of PPM models on the subtask  dev sets using different values of N .", "labels": [], "entities": []}, {"text": " Table 3. We see that utilizing additional data pro- vided marginal performance gains, helping more  in Subtask 2 where much of our additional data  was also Twitter data, making it in-domain.", "labels": [], "entities": []}, {"text": " Table 3: Effect of different word embeddings,  Macro-Average F1 for final ensemble models on dev  data.", "labels": [], "entities": [{"text": "F1", "start_pos": 62, "end_pos": 64, "type": "METRIC", "confidence": 0.532697856426239}]}]}