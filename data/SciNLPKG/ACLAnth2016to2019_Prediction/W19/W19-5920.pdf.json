{"title": [{"text": "Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain.", "labels": [], "entities": [{"text": "Domain adaptation in natural language generation (NLG)", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.7704047891828749}]}, {"text": "This is particularly the case for dialogue systems , where we want to be able to seamlessly include new domains into the conversation.", "labels": [], "entities": []}, {"text": "Therefore, it is crucial for generation models to share knowledge across domains for the effective adaptation from one domain to another.", "labels": [], "entities": []}, {"text": "In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains.", "labels": [], "entities": []}, {"text": "In addition, a layer-wise attention mechanism between the tree encoder and the decoder is adopted to further improve the model's capability.", "labels": [], "entities": []}, {"text": "The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the adaptation data is limited.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 98, "end_pos": 108, "type": "METRIC", "confidence": 0.981477826833725}, {"text": "slot error rate", "start_pos": 117, "end_pos": 132, "type": "METRIC", "confidence": 0.9321116010348002}]}, {"text": "In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on in-formativeness and naturalness than other systems .", "labels": [], "entities": []}], "introductionContent": [{"text": "Building open-domain Spoken Dialogue Systems (SDS) remains challenging.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7464562604824702}]}, {"text": "This is partially because of the difficulty of collecting sufficient data for all domains and the high complexity of natural language.", "labels": [], "entities": []}, {"text": "Typical SDSs are designed based on a pre-defined ontology) which might cover knowledge spanning over multiple domains and topics.", "labels": [], "entities": []}, {"text": "A crucial component of a Spoken Dialogue System is the Natural Language Generation (NLG) module, which generates the text that is finally presented to the user.", "labels": [], "entities": [{"text": "Spoken Dialogue", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.9028503000736237}, {"text": "Natural Language Generation (NLG)", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7749881048997244}]}, {"text": "NLG is especially challeng- ing when building a multi-domain dialogue systems.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7218502759933472}]}, {"text": "Given a semantic representation (SR), the task for NLG is to generate natural language conveying the information encoded in the SR.", "labels": [], "entities": []}, {"text": "Typically, an SR is composed of a set of slot-value pairs and a dialogue act consistent with an ontology.", "labels": [], "entities": []}, {"text": "A dialogue act represents the intention of the system output and the slots provide domaindependent information.", "labels": [], "entities": []}, {"text": "presents examples of SRs with their corresponding natural language representations in various datasets.", "labels": [], "entities": []}, {"text": "The input semantics has its own hierarchical structure in which there are different sets of slotvalue pairs under different dialogue acts across various domains.", "labels": [], "entities": []}, {"text": "Modelling the semantic structure might be helpful for sharing information across domains and achieve better performance for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7300939559936523}]}, {"text": "However, prior work encodes semantic representation in a flat way such as using a binary vector or using a sequential model such as an LSTM.", "labels": [], "entities": []}, {"text": "In that case, the structure of semantics is not fully captured by these encoding methods.", "labels": [], "entities": []}, {"text": "This might limit models' performance especially when adapting to anew domain.", "labels": [], "entities": []}, {"text": "This paper investigates the possibility of lever- aging the semantic structure for NLG domain adaptation in dialogue systems.", "labels": [], "entities": [{"text": "NLG domain adaptation", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.816805919011434}]}, {"text": "We present a generation model with a tree-structured semantic encoder that models the internal structure of the semantic representation to facilitate knowledge sharing across domains.", "labels": [], "entities": []}, {"text": "Moreover, we propose a layer-wise attention mechanism to improve the generation performance.", "labels": [], "entities": []}, {"text": "We perform experiments on the multi-domain Wizard-of-Oz corpus (MultiWOZ) ( ) and with human subjects.", "labels": [], "entities": [{"text": "Wizard-of-Oz corpus (MultiWOZ)", "start_pos": 43, "end_pos": 73, "type": "DATASET", "confidence": 0.8447209119796752}]}, {"text": "The results show that the proposed model outperforms previous methods on both automatic metrics and with human evaluation, suggesting that modelling the semantic structure can facilitate domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 187, "end_pos": 204, "type": "TASK", "confidence": 0.712179571390152}]}, {"text": "To the best of our knowledge, this work is the first study exploiting the tree LSTM ( to model the input semantics of NLG in spoken dialogue systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "The generators are implemented using the Pytorch library (  in a generated sentence, and N is the number of total slots that a generated sentence should contain.", "labels": [], "entities": [{"text": "Pytorch library", "start_pos": 41, "end_pos": 56, "type": "DATASET", "confidence": 0.9052333235740662}]}, {"text": "The results are averaged over 10 samples and 5 random initialised seeds.", "labels": [], "entities": []}, {"text": "As explained above each delexicalised slot token in an utterance is in the format of @domain-act-slot.", "labels": [], "entities": []}, {"text": "When calculating the SER, the predicted slot token is correct only if its domain, dialogue act and slot information are all correct.", "labels": [], "entities": [{"text": "SER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8826305270195007}]}, {"text": "For example, if there is a desired slot area under dialogue act inform within restaurant domain in SR, the model needs to generate the token @restaurant-inform-area.", "labels": [], "entities": []}, {"text": "decoding way as Tree+Att to only predict @ with three additional classifiers for domain, act and slot prediction.", "labels": [], "entities": [{"text": "slot prediction", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7233764678239822}]}, {"text": "However, baseline models obtains better performance by the original decoding method so we keep that in the following experiments.", "labels": [], "entities": []}, {"text": "All the models are optimized by selecting the best one based on the validation set result.", "labels": [], "entities": []}, {"text": "In order to examine the models' ability to share knowledge between domains, we performed experiments in three domain adaptation scenarios: (a) adapting to hotel from restaurant domain; (b) adapting to attraction from restaurant domain and (c) adapting to taxi from train domain.", "labels": [], "entities": []}, {"text": "The adaptation models were fine-tuned with adaptation data based on the models trained on source domain 8 . The SER results are presented in the first row of.", "labels": [], "entities": [{"text": "SER", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.8975092768669128}]}, {"text": "Generally, our model without attention (Tree) performs similarly with RALSTM but better than TGen and SCLSTM.", "labels": [], "entities": []}, {"text": "With the layerwise attention mechanism, our model (Tree+Att) improves significantly and performs better than baselines at all different levels of adaptation data amount.", "labels": [], "entities": []}, {"text": "Especially when the adaptation data used is only 1.25%, the SER is reduced from above 75% to around 25%.", "labels": [], "entities": [{"text": "SER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9964925646781921}]}, {"text": "We found that this is because baseline models tend to predict the slots with the wrong dialogue actor in the wrong domain as the limited adaptation data makes it difficult to learn the sentence pattern in the target domain.", "labels": [], "entities": []}, {"text": "However, with the layer-wise attention mechanism, our model is able to pay attention on the information at different levels in the tree to make the correct predictions.", "labels": [], "entities": []}, {"text": "(See more details in section 5 with error analysis and visualisation of attention distributions.)", "labels": [], "entities": []}, {"text": "A similar trend can be observed in the BLEU results in the second row of.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9949695467948914}]}, {"text": "Because automatic evaluation such as BLEU may not consistently agree with human perception), we performed human testing via the Amazon Mechanical Turk service.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.998480498790741}, {"text": "Amazon Mechanical Turk service", "start_pos": 128, "end_pos": 158, "type": "DATASET", "confidence": 0.9364084005355835}]}, {"text": "We showed MTurk workers the generated sentences in adaptation experiments with adaptation data from 1.25% to 10% as we focus on the models' performance with limited adaptation data.", "labels": [], "entities": []}, {"text": "Five models were compared together by showing, for each model, the 2 sentences with the highest probabilities out of the 10 generated sentences by beam search.", "labels": [], "entities": []}, {"text": "The workers were asked to score each sentence from 1 (bad) to 5 (good) in terms of its informativeness and naturalness.", "labels": [], "entities": []}, {"text": "The informativeness is defined as the degree to which the generated sentence contains all the information specified in the given semantic representation (SR) without conveying extra information and the naturalness is defined as whether the sentence is natural like human language.", "labels": [], "entities": []}, {"text": "pointed out that malicious workers might take advantage of the difficulty of verifying the results and therefore submit answers with low quality.", "labels": [], "entities": []}, {"text": "In order to filter out submissions with bad quality, we also asked them to score the ground truth sentence and an artificial sentence containing irrelevant information to the SR.", "labels": [], "entities": []}, {"text": "If the worker gave ground truth sentence a low score (< 3) or gave the artificial sentence a high score (> 3) in terms of informativeness, the submission was discarded.", "labels": [], "entities": []}, {"text": "The results pertaining to informativeness and naturalness are reported in in three adaptation settings: Restaurant (Rest.) to Hotel domain; Restaurant to Attraction (Attr.) domain and Train to Taxi domain.", "labels": [], "entities": []}, {"text": "For informativeness, our models (both Tree+Att & Tree) outperform all baseline models in the different settings.", "labels": [], "entities": []}, {"text": "This result is consistent with the slot error rate of the automatic evaluation reported in and indicates that the tree-structured semantic encoder does help the model to produce utterances with the correct information.", "labels": [], "entities": [{"text": "slot error rate", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.9482362469037374}]}, {"text": "For naturalness, Tree+Att performs the best in two settings, while SCLSTM performs better when adapting to taxi domain.", "labels": [], "entities": []}, {"text": "This might be because SCLSTM is good at generating utterances with simple patterns and the taxi domain is relatively easy due to its low number of combinations of SR 9 . When adapting to more complex domains such as hotel or attraction, our models provide both informative and natural utterances.", "labels": [], "entities": []}, {"text": "presents example semantic representations with corresponding ground truth sentence and the top-1 utterance generated by each model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The data statistics for each domain.", "labels": [], "entities": []}, {"text": " Table 1. The data split for train, dev  and test is 3:1:1. The details of the ontology is  presented in", "labels": [], "entities": []}, {"text": " Table 2: Human evaluation for utterance quality in  three adaptation settings: Restaurant (Rest.) to Hotel  domain; Restaurant to Attraction (Attr.) domain and  Train to Taxi domain. Informativeness (Info.) and Nat- uralness (Nat.) are reported (rating out of 5).", "labels": [], "entities": [{"text": "Nat- uralness (Nat.)", "start_pos": 212, "end_pos": 232, "type": "METRIC", "confidence": 0.6860616902510325}]}, {"text": " Table 4: Error analysis -number of examples in the  testing set and the number of wrong generated utter- ances (at least 1 missing or redundant slot) by each  model in different adaptation data scenarios. The test- ing example is defined as seen if its semantics appears  in the training set.", "labels": [], "entities": []}]}