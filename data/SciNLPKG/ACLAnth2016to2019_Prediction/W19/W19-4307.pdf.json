{"title": [{"text": "MoRTy: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding", "labels": [], "entities": [{"text": "MoRTy", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8346439003944397}]}], "abstractContent": [{"text": "Word embeddings have undoubtedly revolutionized NLP.", "labels": [], "entities": []}, {"text": "However, pre-trained embed-dings do not always work fora specific task (or set of tasks), particularly in limited resource setups.", "labels": [], "entities": []}, {"text": "We introduce a simple yet effective, self-supervised post-processing method that constructs task-specialized word representations by picking from a menu of reconstructing transformations to yield improved end-task performance (MORTY).", "labels": [], "entities": [{"text": "MORTY", "start_pos": 227, "end_pos": 232, "type": "METRIC", "confidence": 0.6346172094345093}]}, {"text": "The method is complementary to recent state-of-the-art approaches to inductive transfer via fine-tuning, and forgoes costly model archi-tectures and annotation.", "labels": [], "entities": [{"text": "inductive transfer", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7225644290447235}]}, {"text": "We evaluate MORTY on abroad range of setups, including different word embedding methods, corpus sizes and end-task semantics.", "labels": [], "entities": [{"text": "MORTY", "start_pos": 12, "end_pos": 17, "type": "TASK", "confidence": 0.8982981443405151}]}, {"text": "Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word embeddings are ubiquitous in Natural Language Processing.", "labels": [], "entities": []}, {"text": "They provide a low-effort, high pay-off way to improve the performance of a specific supervised end-task by transferring knowledge.", "labels": [], "entities": []}, {"text": "However, recent works indicate that universally best embeddings are not yet possible, and that they instead need to be tuned to fit specific end-tasks using inductive bias -i.e., semantic supervision for the unsupervised embedding learning process (.", "labels": [], "entities": []}, {"text": "This way, embeddings can be tuned to fit a specific single-task (ST) or multi-task (MT: set of tasks) semantic (.", "labels": [], "entities": []}, {"text": "Fine-tuning requires labeled data, which is often either too small, not available or of low quality and creating or extending labeled data is costly and slow.", "labels": [], "entities": []}, {"text": "Word embeddings are typically induced from huge unlabeled corpora with billions of tokens, but for limited-resource domains like biology or medicine, it becomes less clear whether there is still transfer.", "labels": [], "entities": []}, {"text": "We set out to create task-specified embeddings cheaply, with selfsupervision, that are able to provide consistent improvements, even in limited resource settings.", "labels": [], "entities": []}, {"text": "We evaluate the impact of our method, named MORTY, on 18 publicly available benchmark tasks developed by 1 using two ways to induce embeddings, Fasttext and GloVe.", "labels": [], "entities": [{"text": "MORTY", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.85304194688797}]}, {"text": "We test them in two setups corresponding to two different overall aims: (a) to specialize embeddings to better fit a single supervised task or, (b) to generalize embeddings for multiple supervised end-tasks, i.e., to optimize MORTYs for single or multi-task settings.", "labels": [], "entities": []}, {"text": "Since most embeddings are pre-trained on large corpora, we also investigate whether our method further improves embeddings trained on small corpus setups.", "labels": [], "entities": []}, {"text": "Hence, we demonstrate the method's application for single-task, multi-task, small, medium and web-scale (common crawl) corpus-size settings (Section 4).", "labels": [], "entities": []}, {"text": "Learning to scale-up by pretraining on more (un-)labeled data is both: (a) not always possible in low-resource domains due to lack of such data, and (b) heavily increases the compute requirements of comparatively small supervised down-stream task.", "labels": [], "entities": []}, {"text": "This not only leads to high per model-instance costs but also limits learning to scale-out, i.e., when combining many smaller models into a larger dynamic model as is desirable in continual learning settings, where models, inputs and objectives may emerge or disappear overtime.", "labels": [], "entities": []}, {"text": "To provide an alternative in such settings we design MORTY as a learning-toscale-down approach, that uses less data and compute to achieve a performance improvement despite forgoing (un-)supervised fine tuning on target domain data.", "labels": [], "entities": []}, {"text": "Consequently, MORTY uses very little resources, 2 producing a low carbon footprint, especially regarding recent, compute intensive, scale-up approaches like ELMo or BERT () which have high hardware and training time requirements and a large carbon footprint as recently demonstrated by.", "labels": [], "entities": [{"text": "MORTY", "start_pos": 14, "end_pos": 19, "type": "TASK", "confidence": 0.6218777894973755}, {"text": "BERT", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9844921827316284}]}, {"text": "As a result, we demonstrate a simple, unsupervised scale-down method, that allows further pretraining exploitation, while requiring minimum extra effort, time and compute resources.", "labels": [], "entities": []}, {"text": "As in standard methodology, optimal post-processed embeddings can be selected according to multiple proxy-tasks for overall improvement or using a single end-task's development split-e.g., on a fast baseline model for further time reduction.", "labels": [], "entities": []}], "datasetContent": [{"text": "With the aim of deriving a simple yet effective 'best practice' usage recipe, we evaluate MORTY as follows: a) using two word embedding methods f ; b) corpora of different sizes to induce E org , i.e., small, medium and web-scale; c) evaluation across 18 semantic benchmark tasks spanning three semantic categories to broadly examine MORTY's impact, while assessing both single and multi-task end goals; and finally e) evaluate 1-epoch setups in relation to different corpus sizes.", "labels": [], "entities": []}, {"text": "Embeddings and Corpus Size: We evaluate embeddings trained on small, medium (millions of tokens) and large (billions of tokens) corpus sizes.", "labels": [], "entities": [{"text": "Corpus Size", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.8111083507537842}]}, {"text": "In particular, we train 100-dimensional embeddings with Fasttext ( and GloVe () on the 2M and 103M WikiText created by.", "labels": [], "entities": []}, {"text": "We complement them with off-the-shelf webscale Fasttext and GloVe embeddings (trained on 600B and 840B tokens, respectively).", "labels": [], "entities": []}, {"text": "This results in the following vocabulary sizes for Fasttext and GloVe embeddings, respectively: on 2M 25,249 and 33,237 word types.", "labels": [], "entities": [{"text": "2M 25,249", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.9114740490913391}]}, {"text": "For 103M we get 197,256 and 267,633 vocabulary words.", "labels": [], "entities": []}, {"text": "Public, off-the-shelf -common-crawl trained -Fasttext and GloVe embeddings have very large vocabularies of 1,999,995 and 2,196,008 words.", "labels": [], "entities": []}, {"text": "To account for variation in results, we train both embedding methods five times each 6 on the two WikiText corpus sizes.", "labels": [], "entities": [{"text": "WikiText corpus sizes", "start_pos": 98, "end_pos": 119, "type": "DATASET", "confidence": 0.9171261588732401}]}, {"text": "We observed only minor variations, < 0.5% between runs for both Fasttext and GloVe, in overall performance \u03a3 -i.e., when summing the scores of all benchmark tasks.", "labels": [], "entities": []}, {"text": "Semantic benchmark tasks: We use a publicly available word embedding benchmark implementation developed by -chosen for reproducibility and breadth.", "labels": [], "entities": [{"text": "breadth", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9751851558685303}]}, {"text": "The 18 tasks span three semantic categories: (a) word similarity (6 tasks), (b) word analogy (3 tasks), and (c) word and sentence categorization (9 tasks).", "labels": [], "entities": [{"text": "word analogy", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.7408589124679565}, {"text": "word and sentence categorization", "start_pos": 112, "end_pos": 144, "type": "TASK", "confidence": 0.58341433852911}]}, {"text": "7 Evaluation and Experimental Details For the single-task setup we show MORTY'S relative, percentual performance change (ST % change) produced by choosing the best MORTY embedding per task -18 MORTYs.", "labels": [], "entities": [{"text": "MORTY'S relative, percentual performance change (ST % change)", "start_pos": 72, "end_pos": 133, "type": "METRIC", "confidence": 0.7572217014702883}]}, {"text": "Correspondingly, for multi-task results we show MT % change obtained by choosing the MORTY embedding with the best score overall tasks \u03a3 -i.e., one MORTY for all tasks.", "labels": [], "entities": [{"text": "MT % change", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.942301869392395}, {"text": "MORTY", "start_pos": 85, "end_pos": 90, "type": "METRIC", "confidence": 0.8625883460044861}]}, {"text": "Performances in are averaged over 5 runs each of Fasttext and GloVe per corpus size.", "labels": [], "entities": []}, {"text": "To maximize MORTY'S usability we evaluate a 1-epoch training scheme.", "labels": [], "entities": [{"text": "MORTY'S usability", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7495405673980713}]}, {"text": "We test its robustness -particularly for limited resource use -by training 1 epoch on three corpus sizes (small to web-scale), using the best multi-task (MT/ \u03a3) base embedder -see Fasttext.", "labels": [], "entities": []}, {"text": "We again account for variation by using 3 randomly initialized MORTY runs, each over the 5 respective runs per corpus size.", "labels": [], "entities": []}, {"text": "In this experiment, a single epoch yielded very stable boosts, that are comparable to multi-epoch training.", "labels": [], "entities": []}], "tableCaptions": []}