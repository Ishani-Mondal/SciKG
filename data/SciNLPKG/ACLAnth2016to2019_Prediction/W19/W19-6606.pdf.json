{"title": [{"text": "Domain Adaptation for MT: A Study with Unknown and Out-of-Domain Tasks", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7380133867263794}, {"text": "MT", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.887988805770874}]}], "abstractContent": [{"text": "Translation quality could degrade non-gracefully outside the desired domain for MT.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9514204859733582}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9900859594345093}]}, {"text": "Meanwhile, translation requests are often unknown and potentially out-of-domain in practice.", "labels": [], "entities": [{"text": "translation requests", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9123934507369995}]}, {"text": "This paper shows that having an ecosystem with a range of pre-trained domain-specific MT systems can reduce the effect: a translation task can be out of scope of most pre-trained MT systems, but a few others can be capable of handling the task.", "labels": [], "entities": []}, {"text": "But how to obtain the best translation from an ecosystem for such translation requests?", "labels": [], "entities": []}, {"text": "We contribute two frameworks to address the problem.", "labels": [], "entities": []}, {"text": "Experiments show that our frameworks give the performance in the middle between top rank MT systems with reasonably large-scale ecosystems.", "labels": [], "entities": [{"text": "MT", "start_pos": 89, "end_pos": 91, "type": "TASK", "confidence": 0.9732945561408997}]}], "introductionContent": [{"text": "Translation models have been developed under the assumption that we know the domain attest time in advance, and the domain is strictly relevant to our training data.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9586242437362671}]}, {"text": "However, we inevitably will come across test data that is sampled from a different distribution to our training data when using the models in the wild.", "labels": [], "entities": []}, {"text": "Another critical thing is that the domain of test data is often unknown in practice (e.g. Google Translate and Microsoft Translators receive translation requests from their users without knowing in advance their interests).", "labels": [], "entities": []}, {"text": "We have not had a solution for this well-known problem yet.", "labels": [], "entities": []}, {"text": "Machine Translation (MT) has been advanced by new models, including using Neural Machine Translation (NMT) instead of Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9217302560806274}, {"text": "Neural Machine Translation", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.750831127166748}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 118, "end_pos": 155, "type": "TASK", "confidence": 0.7855469485123953}]}, {"text": "The hope is that a better translation model would improve the translation in all settings/situations.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9756306409835815}, {"text": "translation", "start_pos": 62, "end_pos": 73, "type": "TASK", "confidence": 0.9709314703941345}]}, {"text": "This, however, is not true.", "labels": [], "entities": []}, {"text": "Translation quality could degrade nongracefully outside the desired domain for both NMT and SMT.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9405162334442139}, {"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9606976509094238}]}, {"text": "In fact, it has been known that NMT suffers even harder than SMT when the test data is out-of-domain (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.654781699180603}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9873260855674744}]}, {"text": "We also improve MT by using domain adaptation methods (i.e. improving translation system from having a small seed indomain data such as system interpolation, instance weighting and data selection).", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9960566759109497}]}, {"text": "In practice, this is not a thorough solution because we do not know the domain of user translation requests in advance.", "labels": [], "entities": [{"text": "user translation requests", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.7333754301071167}]}, {"text": "The contribution of this work is to provide a simple, easy-and-fast-to-deploy, translation model-agnostic 1 solution to the challenging problem.", "labels": [], "entities": [{"text": "translation model-agnostic 1", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.8683063785235087}]}, {"text": "Our approach is to construct an \"ecosystem\" with a range of pre-trained domain-specific MT systems, each specialized in a certain domain (e.g. Speech, Financial, Food, etc.).", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9428452253341675}]}, {"text": "Our intuition is that having such an ecosystem could reduce the decrease in translation quality for an outside domain.", "labels": [], "entities": []}, {"text": "That is, an out-of-domain translation task can be out of scope of most pre-trained MT systems in the ecosystem.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9877869486808777}]}, {"text": "However, with the diversity of domains in a reasonably large ecosystem, we hope there is a chance to have certain pre-trained systems in the ecosystem that can be capable of handling the task well.", "labels": [], "entities": []}, {"text": "The larger our ecosystem is, the more likely we have more capable pre-trained MT systems to an out-of-domain task.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9813976287841797}]}, {"text": "The next step is to work on an unsupervised method that automatically finds the best translations from an ecosystem for every translation request from an unknown and out-of-domain translation task.", "labels": [], "entities": [{"text": "translation request from an unknown and out-of-domain translation task", "start_pos": 126, "end_pos": 196, "type": "TASK", "confidence": 0.6081777281231351}]}, {"text": "Creating a domain classifier for translation requests provides suboptimal performance, because the target domain is unknown and out-of-domain.", "labels": [], "entities": []}, {"text": "System combination could degrade translation quality substantially, as the majority of pre-trained MT systems in the ecosystem are incapable of handling the task.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9767175316810608}, {"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.9784786701202393}]}, {"text": "We propose two frameworks to address the problem.", "labels": [], "entities": []}, {"text": "VOTING I involves two separate steps for handling each translation request: First, the request is translated by all pre-trained MT systems.", "labels": [], "entities": [{"text": "VOTING I", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.6903966069221497}, {"text": "translation request", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.8814288675785065}, {"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9402691125869751}]}, {"text": "Second, the translation output that is most similar to others is returned to the user.", "labels": [], "entities": []}, {"text": "An agreement measure is proposed to calculate how similar translation outputs are.", "labels": [], "entities": []}, {"text": "The intuition behind VOTING I is that good translations maybe similar to the others.", "labels": [], "entities": []}, {"text": "That is, because they are good translations, they must be similar to translation references, and therefore it is likely that they are similar to the others as well.", "labels": [], "entities": []}, {"text": "VOTING II selects only a limited number of MT systems for decoding.", "labels": [], "entities": [{"text": "VOTING II", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8564892411231995}, {"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.951470673084259}]}, {"text": "Decoding cost is thus substantially cheaper in VOTING II.", "labels": [], "entities": [{"text": "VOTING II", "start_pos": 47, "end_pos": 56, "type": "TASK", "confidence": 0.488159716129303}]}, {"text": "The intuition behind VOTING II is that MT systems that are good in a domain tend to agree with each other.", "labels": [], "entities": [{"text": "VOTING II", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.646569699048996}, {"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9780685305595398}]}, {"text": "However, the expertise parameters of MT systems regarding to an unknown domain are hidden and we thus do not know which MT systems we should select.", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9797206521034241}, {"text": "MT", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9459608793258667}]}, {"text": "In VOTING II expertise parameters are initialized randomly and our heuristic learning algorithm consequently updates the parameters during translation.", "labels": [], "entities": []}, {"text": "We note that VOTING II works with the assumption that the translation requests would be handled in sequential (not parallel).", "labels": [], "entities": []}, {"text": "While this is not true for all cases, it is true when we translate request translations of large documents as one task.", "labels": [], "entities": [{"text": "translate request translations of large documents", "start_pos": 57, "end_pos": 106, "type": "TASK", "confidence": 0.8510229090849558}]}, {"text": "We conduct extensive experiments with Spanish-English, French-English and GermanEnglish to support our intuition.", "labels": [], "entities": []}, {"text": "Experiments show that VOTING I gives the performance in between the top two systems for medium-scale ecosystem, and in between the top three systems fora large-scale ecosystem.", "labels": [], "entities": [{"text": "VOTING I", "start_pos": 22, "end_pos": 30, "type": "TASK", "confidence": 0.5756288766860962}]}, {"text": "VOTING II performs substantially better than VOTING I and occasionally reaches close to the top Rank 1 MT system for medium-scale ecosystems.", "labels": [], "entities": [{"text": "VOTING", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8225343823432922}]}, {"text": "Our framework is scalable and has promising applications to large-scale online translation services.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct experiments with three language pairs: Spanish-English, French-English and GermanEnglish.", "labels": [], "entities": []}, {"text": "We create different translation ecosystems with a large number (from 6 to 10) of domainspecific MT systems for experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9359272122383118}]}, {"text": "Our experiments are extensive with 23 translation tasks in total, which are unknown and out-of-domain.", "labels": [], "entities": []}, {"text": "Note that we use NMT for one language pair and SMT for the rest, and the motivation behind this decision is simply that training SMT is somewhat easier than NMT for us.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9676363468170166}, {"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.94835364818573}]}], "tableCaptions": [{"text": " Table 1: Positive example with VOTING I: Good translations (e.g. Book, Wikipedia) tend to be similar to the others.", "labels": [], "entities": [{"text": "VOTING I", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.8792964220046997}, {"text": "Book", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9506576657295227}]}, {"text": " Table 2: Two negative examples with VOTING I. On the left: bad translations (e.g. IT, Wikipedia, Speech) are also similar to  the others by chance. On the right: a case of \"black sheep\": a very good translation (Book) is too different from the others.", "labels": [], "entities": [{"text": "VOTING I", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8798969089984894}]}, {"text": " Table 3: Results for Spanish-English experiments.", "labels": [], "entities": []}, {"text": " Table 4: Results for French-English experiments.", "labels": [], "entities": []}, {"text": " Table 5: Results for German-English experiments.", "labels": [], "entities": []}, {"text": " Table 6: A detailed comparison for other baselines (SC:  System Combination, DC: Domain Classification, Avg. TR:  Average baseline between top rank MT systems (Rank 1 and  Rank 2) for Spanish-English.", "labels": [], "entities": [{"text": "Avg. TR:  Average baseline", "start_pos": 105, "end_pos": 131, "type": "METRIC", "confidence": 0.8992294371128082}]}]}