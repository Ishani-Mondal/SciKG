{"title": [{"text": "NJU Submissions for the WMT19 Quality Estimation Shared Task", "labels": [], "entities": [{"text": "NJU", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9158045053482056}, {"text": "WMT19 Quality Estimation Shared", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.8218144774436951}]}], "abstractContent": [{"text": "In this paper, we describe the submissions of the team from Nanjing University for the WMT19 sentence-level Quality Estimation (QE) shared task on English-German language pair.", "labels": [], "entities": [{"text": "Nanjing University", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9173942804336548}, {"text": "WMT19 sentence-level Quality Estimation (QE) shared task", "start_pos": 87, "end_pos": 143, "type": "TASK", "confidence": 0.7386603156725565}]}, {"text": "We develop two approaches based on a two-stage neural QE model consisting of a feature extractor and a quality estima-tor.", "labels": [], "entities": []}, {"text": "More specifically, one of the proposed approaches employs the translation knowledge between the two languages from two different translation directions; while the other one employs extra monolingual knowledge from both source and target sides, obtained by pre-training deep self-attention networks.", "labels": [], "entities": []}, {"text": "To efficiently train these two-stage models, a joint learning training method is applied.", "labels": [], "entities": []}, {"text": "Experiments show that the ensemble model of the above two models achieves the best results on the benchmark dataset of the WMT17 sentence-level QE shared task and obtains competitive results in WMT19, ranking 3rd out of 10 submissions.", "labels": [], "entities": [{"text": "WMT17 sentence-level QE shared task", "start_pos": 123, "end_pos": 158, "type": "DATASET", "confidence": 0.7852630138397216}, {"text": "WMT19", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.8932511210441589}]}], "introductionContent": [{"text": "Sentence-level Quality Estimation (QE) of Machine Translation (MT) is a task to predict the quality scores for unseen machine translation outputs at run-time, without relying on reference translations.", "labels": [], "entities": [{"text": "Sentence-level Quality Estimation (QE) of Machine Translation (MT)", "start_pos": 0, "end_pos": 66, "type": "TASK", "confidence": 0.7937494764725367}]}, {"text": "There are some interesting applications of sentence-level QE, such as deciding whether a given translation is good enough for publishing, informing readers of the target language only whether or not they can rely on a translation, filtering out sentences that are not good enough for post-editing by professional translators, selecting the best translation among multiple MT systems and soon.", "labels": [], "entities": [{"text": "sentence-level QE", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.5047173202037811}]}, {"text": "The common methods formalize the sentencelevel QE as a supervised regression task.", "labels": [], "entities": []}, {"text": "Traditional QE models () have * Corresponding author.", "labels": [], "entities": []}, {"text": "two independent modules: feature extractor module and machine learning module.", "labels": [], "entities": []}, {"text": "The feature extractor module is used to extract human-crafted features, which describe the translation quality, such as source fluency indicators, translation complexity indicators, and adequacy indicators.", "labels": [], "entities": []}, {"text": "And the machine learning module serves for predicting how much effort is needed to post-edit translations to acceptable results as measured by the Humantargeted Translation Edit Rate (HTER)) based on extracted features above.", "labels": [], "entities": [{"text": "Humantargeted Translation Edit Rate (HTER))", "start_pos": 147, "end_pos": 190, "type": "METRIC", "confidence": 0.7193633658545358}]}, {"text": "With the great success of deep neural networks in a number of tasks in natural language processing (NLP), some researches have begun to apply neural networks to QE task and these neural approaches have shown promising results.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7778647641340891}]}, {"text": "combine neural features, such as word embedding features and neural network language model (NNLM) features with other features produced by.; apply modified recurrent neural network (RNN) based neural machine translation (NMT) model () to the sentence-level QE task, which does not require manual effort for finding the best relevant features.", "labels": [], "entities": [{"text": "recurrent neural network (RNN) based neural machine translation (NMT)", "start_pos": 156, "end_pos": 225, "type": "TASK", "confidence": 0.8047685439770038}]}, {"text": "replace the above NMT model with modified self-attention mechanism based transformer model (.", "labels": [], "entities": []}, {"text": "This approach achieves the best result we know so far in the WMT17 sentence-level QE task on English-German language pair.", "labels": [], "entities": [{"text": "WMT17 sentence-level QE task", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.5656661689281464}]}, {"text": "In this paper, we present two different approaches for the sentence-level QE task, which employ bi-directional translation knowledge and large-scale monolingual knowledge to the QE task, respectively.", "labels": [], "entities": []}, {"text": "Also, a simple ensemble of them can help to achieve better quality estimation performance in the sentence-level QE task.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 and Section 3, we separately describe the two proposed QE models above.", "labels": [], "entities": []}, {"text": "In Section 4, we report experimental results and conclude our paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The bilingual parallel corpus that we used for training word predictors is officially released by the WMT17 Shared Task:  as development dataset.", "labels": [], "entities": [{"text": "training word predictors", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.6062313715616862}, {"text": "WMT17 Shared Task", "start_pos": 102, "end_pos": 119, "type": "DATASET", "confidence": 0.8437075813611349}]}, {"text": "Pre-processing script can be found at github 2 . To test the performance of the proposed QE models, we conducted experiments on the WMT17 and WMT19 sentence-level QE task for English-to-German (en-de) direction.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.9467813968658447}, {"text": "WMT19 sentence-level QE task", "start_pos": 142, "end_pos": 170, "type": "DATASET", "confidence": 0.7944842427968979}, {"text": "English-to-German (en-de) direction", "start_pos": 175, "end_pos": 210, "type": "TASK", "confidence": 0.5481492340564728}]}, {"text": "Because the gold standard labels of testing data on the WMT18 sentence-level QE task are unobtainable.", "labels": [], "entities": [{"text": "WMT18 sentence-level QE task", "start_pos": 56, "end_pos": 84, "type": "DATASET", "confidence": 0.818137526512146}]}, {"text": "The statistics of the dataset are shown in Pearson's correlation coefficient (Pearson) (as primary metric), Mean Average Error (MAE) and Root Mean Squared Error (RMSE) are used to evaluate the correlation between the predicted quality scores and the true HTER scores.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient (Pearson)", "start_pos": 43, "end_pos": 86, "type": "METRIC", "confidence": 0.8857932601656232}, {"text": "Mean Average Error (MAE)", "start_pos": 108, "end_pos": 132, "type": "METRIC", "confidence": 0.9813009401162466}, {"text": "Root Mean Squared Error (RMSE)", "start_pos": 137, "end_pos": 167, "type": "METRIC", "confidence": 0.886995690209525}]}, {"text": "Both of the word predictors of Bi-directional QE Model hold the same parameters.", "labels": [], "entities": [{"text": "Bi-directional QE Model", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.5504947304725647}]}, {"text": "The number of layers for the self-attention encoder and forward/backward self-attention decoder are all set as 6, where we use 8-head self-attention in practice.", "labels": [], "entities": []}, {"text": "The dimensionality of word embedding and selfattention layers are all 512 except the feed-forward sub-layer is 2048.", "labels": [], "entities": []}, {"text": "The dropout rate is set as 0.1.", "labels": [], "entities": []}, {"text": "Worth mentioning, the normal transformer model introduced in BERT-based QE model is trained using the same parallel corpus and parameter settings as word predictors.", "labels": [], "entities": [{"text": "BERT-based QE model", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.7775323589642843}]}, {"text": "For quality estimator module, the number of hidden units for forward and backward LSTM is 512.", "labels": [], "entities": []}, {"text": "And we uniformly use a minibatch stochastic gradient descent (SGD) algorithm together with Adam () to train all models described.", "labels": [], "entities": [{"text": "stochastic gradient descent (SGD)", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.7100968360900879}]}, {"text": "These proposed models were compared with the traditional QE framework QuEst++ ( , the neural network features based 2 https://github.com/zhaocq-nlp/MT-data-processing QE model SHEF/QUEST-EMB () and the QE model combined with NMT model, including POSTECH (), QEBrain ( , and UNQE ( ).", "labels": [], "entities": [{"text": "QEBrain", "start_pos": 258, "end_pos": 265, "type": "DATASET", "confidence": 0.8630146980285645}]}, {"text": "In this section, we will report the experimental results of our approaches for WMT17 and WMT19 sentence-level QE task in English-German direction.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.6923757791519165}, {"text": "WMT19 sentence-level QE task", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.5726519823074341}]}, {"text": "For WMT17 QE task, we tried to verify our proposed models and chose the best two models to participate in WMT19 QE task.", "labels": [], "entities": [{"text": "WMT17 QE task", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7734274864196777}, {"text": "WMT19 QE task", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.6287241180737814}]}, {"text": "In and, results of WMT17 and WMT19 QE tasks are listed respectively.", "labels": [], "entities": [{"text": "WMT17", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7737287282943726}, {"text": "WMT19 QE tasks", "start_pos": 29, "end_pos": 43, "type": "DATASET", "confidence": 0.6876137653986613}]}, {"text": "From the results listed in, our proposed single models, Bi-directional QE and BERT-based QE (+NMT) can outperform all the other compared single models for the primary metric.", "labels": [], "entities": [{"text": "Bi-directional QE", "start_pos": 56, "end_pos": 73, "type": "METRIC", "confidence": 0.9381402730941772}, {"text": "BERT-based QE", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9360973238945007}]}, {"text": "Then, we ensemble the two best single models above, where corresponding weights are tuned according to Pearson's correlation coefficient on the development dataset.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 103, "end_pos": 136, "type": "METRIC", "confidence": 0.7160728350281715}]}, {"text": "The ensemble model can be comparable or better than the state-of-the-art (SOTA) ensemble models of WMT17 sentence-level QE task.", "labels": [], "entities": [{"text": "WMT17 sentence-level QE task", "start_pos": 99, "end_pos": 127, "type": "TASK", "confidence": 0.5682265162467957}]}, {"text": "Considering the experimental results obtained from WMT17 QE task, we submitted the ensemble model and Bi-directional QE model to WMT19 sentence-level QE task, and ranked 3rd and 4th respectively according to WMT19 QE website.", "labels": [], "entities": [{"text": "WMT17 QE task", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8252370953559875}, {"text": "WMT19 sentence-level QE task", "start_pos": 129, "end_pos": 157, "type": "TASK", "confidence": 0.6529262661933899}, {"text": "WMT19 QE website", "start_pos": 208, "end_pos": 224, "type": "DATASET", "confidence": 0.9482246438662211}]}], "tableCaptions": [{"text": " Table 1: Statistics of the en-de dataset of the WMT17  sentence-level QE task.", "labels": [], "entities": [{"text": "WMT17  sentence-level QE task", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6230263113975525}]}, {"text": " Table 2: Statistics of the en-de dataset of the WMT19  sentence-level QE task.", "labels": [], "entities": [{"text": "WMT19  sentence-level QE task", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.561054065823555}]}, {"text": " Table 3: Results of the models on the WMT17  sentence-level QE. \"BERT-based QE model\" repre- sents the original model with the sentence pair of  source sentence and target sentence as inputs. \"+NMT\"  represents that we use the sentence pair of pseudo- reference and target sentence as inputs of BERT-based  QE model. And the rest of these two models remain  the same.", "labels": [], "entities": [{"text": "WMT17  sentence-level QE", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8625923792521158}, {"text": "BERT-based", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9863325953483582}]}, {"text": " Table 4: Results of submitted models on the WMT19  sentence-level QE.", "labels": [], "entities": [{"text": "WMT19  sentence-level QE", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.8406232198079427}]}]}