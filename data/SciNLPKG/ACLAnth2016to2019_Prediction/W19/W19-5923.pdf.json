{"title": [{"text": "FriendsQA: Open-Domain Question Answering on TV Show Transcripts", "labels": [], "entities": [{"text": "FriendsQA", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9086869359016418}, {"text": "Open-Domain Question Answering on TV Show Transcripts", "start_pos": 11, "end_pos": 64, "type": "TASK", "confidence": 0.7045287362166813}]}], "abstractContent": [{"text": "This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations.", "labels": [], "entities": [{"text": "FriendsQA", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.96844881772995}, {"text": "question answering", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7428255379199982}]}, {"text": "Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue.", "labels": [], "entities": []}, {"text": "A series of crowdsourcing tasks are conducted to ensure good annotation quality , resulting a high inter-annotator agreement of 81.82%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.6642681360244751}]}, {"text": "A comprehensive annotation ana-lytics is provided fora deeper understanding in this dataset.", "labels": [], "entities": []}, {"text": "Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset.", "labels": [], "entities": [{"text": "BERT", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9979425072669983}]}, {"text": "BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8991335034370422}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9995355606079102}, {"text": "answer utterance selection", "start_pos": 71, "end_pos": 97, "type": "TASK", "confidence": 0.8735718131065369}, {"text": "F1-score", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9991821646690369}, {"text": "answer span selection", "start_pos": 127, "end_pos": 148, "type": "TASK", "confidence": 0.6428393522898356}, {"text": "FriendsQA", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.9164606332778931}]}], "introductionContent": [{"text": "Question answering (QA) has received lots of hype over the recent years as deep learning models have progressively pushed the limit of machine comprehension to the level of human intelligence.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9513607025146484}]}, {"text": "Several systems have demonstrated their superiority over human for answering quizbowl questions.", "labels": [], "entities": []}, {"text": "Strong evidences have been found that advance neural network models will likely surpass human performance for answering open-domain questions in a foreseeable future.", "labels": [], "entities": []}, {"text": "Nonetheless, no system has reached such high intelligence for understanding contexts in dialogue, although it is the most natural means of human communication.", "labels": [], "entities": [{"text": "understanding contexts in dialogue", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7883123904466629}]}, {"text": "Moreover, the amount of data in this form has increased at a faster rate than any other type of textual data).", "labels": [], "entities": []}, {"text": "Many datasets have been presented for various QA tasks (Section 2.1).", "labels": [], "entities": [{"text": "QA tasks", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8516588807106018}]}, {"text": "While numerous models have shown remarkable results with these datasets (Section 2.2), the evidence passages, where the contexts of questions are derived from, mostly reside within wiki articles, newswire, (non-)fictional stories, or children's books, but not from multiparty dialogue.", "labels": [], "entities": []}, {"text": "Contextual understanding in dialogue is challenging because it needs to interpret contents composed by multiple speakers, and anticipate colloquial language filled with sarcasms, metaphors, humors, etc.", "labels": [], "entities": [{"text": "Contextual understanding", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9116942286491394}]}, {"text": "This inspires us to create anew dataset, FriendsQA, that aims to enhance machine comprehension on this domain.", "labels": [], "entities": [{"text": "FriendsQA", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.9673448801040649}]}, {"text": "Dialogues in this dataset are excerpted from transcripts of the TV show Friends, that is the world-wide and also go-to show for English learners to get familiarized with everyday conversations.", "labels": [], "entities": []}, {"text": "Section 3 describes the FriendsQA dataset with annotation details.", "labels": [], "entities": [{"text": "FriendsQA dataset", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9942447245121002}]}, {"text": "Section 4 describes the architectures of QA systems experimented on this dataset.", "labels": [], "entities": []}, {"text": "Finally, Section 5 shows the experimental results with an in-depth error analysis.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, FriendsQA is the first dataset that is publicly available and challenges span-based QA on multiparty dialogue with everyday topics.", "labels": [], "entities": [{"text": "FriendsQA", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.9606277942657471}]}, {"text": "The contributions of this work include: \u2022 An open-domain question answering dataset on multiparty dialogue comprising 1,222 dialogues, 10,610 questions, and 21,262 answer spans.", "labels": [], "entities": [{"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.6755024045705795}]}, {"text": "\u2022 A comprehensive corpus analytics to ensure its validity as a deep learning resource and explain the diverse nature of this dataset for QA.", "labels": [], "entities": []}, {"text": "\u2022 Model comparisons between three state-of-theart QA systems trained on this dataset to project its practicality in real applications.", "labels": [], "entities": []}, {"text": "\u2022 A thorough error analysis to illustrate major challenges found in this task and make suggestions to future research on the dialogue domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "The NLP community has been dedicated to produce three types of question answering (QA) datasets.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.8286572217941284}]}, {"text": "The first is for reading comprehension QA, where the model picks answers for multiple choice questions regarding the evidence passages.", "labels": [], "entities": [{"text": "reading comprehension QA", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.7838226556777954}]}, {"text": "MCTest is an open-domain dataset comprising short fictional stories (.", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9127277731895447}]}, {"text": "RACE is a large dataset compiled from English assessments for 12-18 years old students (.", "labels": [], "entities": [{"text": "RACE", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.609408974647522}]}, {"text": "TQA gives passages from middle school science lessons and textbooks (.", "labels": [], "entities": [{"text": "TQA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9495450854301453}]}, {"text": "SciQ gives passages from science exams collected via crowdsourcing (.", "labels": [], "entities": []}, {"text": "DREAM gives multiparty dialogue passages from English-as-a-foreignlanguage exams).", "labels": [], "entities": [{"text": "DREAM", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.914050281047821}]}, {"text": "The second is for cloze-style QA, for which the model fills in the blanks that obliterate certain contents in sentences describing the evidence passages.", "labels": [], "entities": [{"text": "cloze-style QA", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.40000300109386444}]}, {"text": "CNN/Daily Mail targets on entities in bullet points summarizing articles from CNN and Daily News ().", "labels": [], "entities": [{"text": "CNN/Daily Mail", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9322451800107956}]}, {"text": "Children's Book Test focuses on named entities, nouns, verbs, and prepositions in passages from children's books (.", "labels": [], "entities": [{"text": "Children's Book Test", "start_pos": 0, "end_pos": 20, "type": "DATASET", "confidence": 0.9049022793769836}]}, {"text": "Who-did-What gives description sentences and evidence passages extracted from news articles in English Gigaword (.", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.7767637372016907}]}, {"text": "BookTest is similar to Children's Book Test but 60 times larger (.", "labels": [], "entities": [{"text": "BookTest", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9440297484397888}, {"text": "Children's Book Test", "start_pos": 23, "end_pos": 43, "type": "DATASET", "confidence": 0.8758496344089508}]}, {"text": "The third is for span-based QA, where the model finds the answer contents as spans in the evidence passages.", "labels": [], "entities": [{"text": "QA", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.7500360012054443}]}, {"text": "bAbI aims to reinforce learning on event types and infer a sequence of event descriptions ).", "labels": [], "entities": [{"text": "bAbI", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8593467473983765}]}, {"text": "WikiQA ( and) use Wikipedia, whereas NewsQA () use CNN articles as evidence passages.", "labels": [], "entities": [{"text": "NewsQA", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.9477782249450684}]}, {"text": "MS MARCO gives questions involving zero to multiple answer contents from web documents).", "labels": [], "entities": [{"text": "MS MARCO", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7997532486915588}]}, {"text": "TriviaQA is compiled by trivia enthusiasts to challenge machine comprehension (.", "labels": [], "entities": []}, {"text": "CoQA focuses on conversational flows between a questioner and an answerer ().", "labels": [], "entities": []}, {"text": "presented R-Net that used gated attention-based recurrent networks and refined QA representation with self-matching attention.", "labels": [], "entities": []}, {"text": "presented ReasoNet that took multiple turns to reason over the relationships between query, documents, and answers.", "labels": [], "entities": [{"text": "ReasoNet", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.882355272769928}]}, {"text": "presented the Attention Over Attention Reader to better capture similarities between questions and answer contents.", "labels": [], "entities": []}, {"text": "presented the Reinforced Mnemonic Reader to combine the memorized attention with new attention.", "labels": [], "entities": [{"text": "Reinforced Mnemonic Reader", "start_pos": 14, "end_pos": 40, "type": "DATASET", "confidence": 0.5808214942614237}]}, {"text": "applied self-attention to QA, which became known as the Transformer.", "labels": [], "entities": []}, {"text": "presented FusionNet that kept the history of word representations and used multi-level attention.", "labels": [], "entities": [{"text": "FusionNet", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.919845461845398}]}, {"text": "presented a standard neural architecture with rich contextualized word representations.", "labels": [], "entities": []}, {"text": "presented Stochastic Answer Network (SAN) with a stochastic prediction dropout layer as the final layer.", "labels": [], "entities": [{"text": "Stochastic Answer Network (SAN)", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7569696406523386}]}, {"text": "presented QANet with CNN and self-attention to combine local and global interactions.", "labels": [], "entities": [{"text": "QANet", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.9008079171180725}, {"text": "CNN", "start_pos": 21, "end_pos": 24, "type": "DATASET", "confidence": 0.9162536263465881}]}, {"text": "presented the Embeddings from Language Models (ELMo) that used bi-directional presented the Bidirectional Encoder Representations (BERT) that used deep-layered transformers to generate contextualized word embeddings.", "labels": [], "entities": []}, {"text": "For the generation of the FriendsQA dataset, 1,222 scenes from the first four seasons of the Character Mining dataset are selected (Section 2.3).", "labels": [], "entities": [{"text": "FriendsQA dataset", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9953767657279968}, {"text": "Character Mining dataset", "start_pos": 93, "end_pos": 117, "type": "DATASET", "confidence": 0.6787026723225912}]}, {"text": "Scenes with fewer than five utterances are discarded (83 of them), and each scene is considered an independent dialogue.", "labels": [], "entities": []}, {"text": "FriendQA can be viewed as answer span selection, where questions are asked for some contexts in a dialogue and the model is expected to find certain spans in the dialogue containing answer contents.", "labels": [], "entities": [{"text": "FriendQA", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.890745222568512}, {"text": "answer span selection", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.6908381779988607}]}, {"text": "The dialogue aspects of this dataset, however, make it more challenging than other datasets comprising passages informal languages (Section 2.1).", "labels": [], "entities": []}, {"text": "Three challenging aspects that are commonly found in dialogue QA are illustrated in.", "labels": [], "entities": [{"text": "dialogue QA", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.6076675355434418}]}, {"text": "For our experiments, all dialogues from are randomly shuffled and redistributed as the training (80%), development (10%), and test (10%) as shown in.", "labels": [], "entities": []}, {"text": "Two tasks are experimented, answer utterance selection and answer span selection, with the FriendsQA dataset.", "labels": [], "entities": [{"text": "answer utterance selection", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.9254184166590372}, {"text": "answer span selection", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7675491372744242}, {"text": "FriendsQA dataset", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.9931244850158691}]}, {"text": "shows results from 9 models trained by the three state-of-the-art systems in Section 5.2 using the three answer selection strategies in Section 5.1.", "labels": [], "entities": []}, {"text": "All experiments are run three times and their average scores with standard deviations are reported.", "labels": [], "entities": []}, {"text": "BERT and QANet perform better with the multipleanswer strategy, that gives more training instances per question, whereas R-Net performs better with the other strategies.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9805989265441895}]}, {"text": "This could be due to R-Net's self-matching mechanism that gets confused when multiple answers are provided for training the same question.", "labels": [], "entities": []}, {"text": "BERT models significantly outperform ones from the other two systems in all evaluations.", "labels": [], "entities": [{"text": "BERT", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9702085256576538}]}, {"text": "Since our hyper-parameters are tuned around grids provided by the original papers, it is possible that these results are still suboptimal, which points out another important property of BERT that it is not as sensitive to different QA datasets.: Results with respect to question types using BERT and the multiple-answer strategy.", "labels": [], "entities": [{"text": "BERT", "start_pos": 186, "end_pos": 190, "type": "METRIC", "confidence": 0.9122365713119507}, {"text": "BERT", "start_pos": 291, "end_pos": 295, "type": "METRIC", "confidence": 0.5605495572090149}]}, {"text": "shows results from BERT's multiple answer models by question types.", "labels": [], "entities": [{"text": "BERT", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.4580780863761902}]}, {"text": "Answers to where and when questions are mostly factoid, which show the highest performance.", "labels": [], "entities": []}, {"text": "On the other hand, answers to why and how usually span out to longer sequences, leading to worse performance.", "labels": [], "entities": []}, {"text": "Answers to who and what questions give a good mixture of proper and common nouns and show moderate performance.", "labels": [], "entities": []}, {"text": "when k = 14 and 20, respectively.", "labels": [], "entities": []}, {"text": "More importantly, the gap between UM and SM gets smaller ask increases, which implies that FriendsQA is not only learnable by deep learning but also can be enhanced by re-ranking the answer predictions.", "labels": [], "entities": [{"text": "FriendsQA", "start_pos": 91, "end_pos": 100, "type": "DATASET", "confidence": 0.8150900602340698}]}], "tableCaptions": [{"text": " Table 5: Data split for our experiments.", "labels": [], "entities": []}, {"text": " Table 6: Results from the three state-of-the-art QA systems. All models are experimented three times and their  average scores with standard deviations are reported. UM: Utterance Match, SM: Span Match, EM: Exact Match.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 208, "end_pos": 219, "type": "METRIC", "confidence": 0.814659833908081}]}, {"text": " Table 7: Results with respect to question types using  BERT and the multiple-answer strategy.", "labels": [], "entities": [{"text": "BERT", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9880254864692688}]}]}