{"title": [{"text": "AI Blues at FinSBD Shared Task: CRF-based Sentence Boundary Detection in PDF Noisy Text in the Financial Domain", "labels": [], "entities": [{"text": "FinSBD", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8844630718231201}, {"text": "CRF-based Sentence Boundary Detection", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.5728681087493896}]}], "abstractContent": [{"text": "This paper reports the team AI Blues's participation in the FinSBD 2019 shared Task on 'Sentence Boundary Detection in PDF Noisy Text in the Financial Domain'.", "labels": [], "entities": [{"text": "FinSBD 2019 shared Task", "start_pos": 60, "end_pos": 83, "type": "DATASET", "confidence": 0.9214769899845123}, {"text": "Sentence Boundary Detection in PDF Noisy Text in the Financial Domain", "start_pos": 88, "end_pos": 157, "type": "TASK", "confidence": 0.8511791229248047}]}, {"text": "Sentence detection from noisy text is a challenging task.", "labels": [], "entities": [{"text": "Sentence detection from noisy text", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9340502858161926}]}, {"text": "We modeled the sentence boundary detection problem as a sequence labeling problem using Conditional Random Field (CRF) approach for English and French language financial texts.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.7545411189397176}, {"text": "sequence labeling", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.6836526393890381}]}, {"text": "We proposed to use punctuation embeddings as an additional feature along with the basic language specific features and obtained 84.5%(F1) and 86.5%(F1) accuracies in the English and French language shared task datasets respectively.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.9874144196510315}, {"text": "F1) accuracies", "start_pos": 148, "end_pos": 162, "type": "METRIC", "confidence": 0.9253942171732584}, {"text": "French language shared task datasets", "start_pos": 182, "end_pos": 218, "type": "DATASET", "confidence": 0.5479151248931885}]}], "introductionContent": [{"text": "The task of Sentence Boundary Detection (SBD) is to identify the sentence segments within a text.", "labels": [], "entities": [{"text": "Sentence Boundary Detection (SBD)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.9330984155337015}]}, {"text": "In Natural Language Processing (NLP), the sentence is the foundational unit and extracting sentences or detecting the boundary of sentences from a noisy text is a challenging task.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7562815845012665}, {"text": "extracting sentences or detecting the boundary of sentences from a noisy text", "start_pos": 80, "end_pos": 157, "type": "TASK", "confidence": 0.6966670230031013}]}, {"text": "Any imperfect sentence boundary detection system can affect the morphologic, syntactic, semantic and discourse analysis in text processing.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.7182417710622152}]}, {"text": "The punctuations such as '.', '?' and '!' are commonly used as sentence boundaries.", "labels": [], "entities": []}, {"text": "However, the usage of punctuation '.' is ambiguous.", "labels": [], "entities": []}, {"text": "It can be used along with decimals, email addresses, abbreviations, initials in names, etc.", "labels": [], "entities": []}, {"text": "Despite the important role of sentence boundary detection in NLP, this area has not received enough attention so far.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.7128299176692963}, {"text": "NLP", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9063424468040466}]}, {"text": "The existing approaches for this task are confined to formal texts and to the best of our knowledge no studies have been conducted in noisy texts for this task.", "labels": [], "entities": []}, {"text": "In FinSBD shared task, the focus is to detect the beginning and ending boundaries for extracting well segmented sentences from financial texts.", "labels": [], "entities": []}, {"text": "These financial texts are PDF documents in which investment funds precisely describe their characteristics and investment modalities.", "labels": [], "entities": []}, {"text": "The noisy unstructured text from these PDF files was parsed by the shared task organizers and the task is to transform them into semi-structured text by tagging the sentence boundaries in two languages -English and French.", "labels": [], "entities": []}, {"text": "For example: consider the English sentence \"Subscriptions may only be received on the basis of this Prospectus.\".", "labels": [], "entities": []}, {"text": "Here the word Subscriptions is tagged as the beginning and the period '.' is tagged as the ending of the sentence in the given corpus.", "labels": [], "entities": []}, {"text": "We have modeled the sentence boundary detection problem as a sequence labeling problem.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.7285914421081543}, {"text": "sequence labeling", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7052749395370483}]}, {"text": "The tokenized text is the input and the output is the corresponding labels.", "labels": [], "entities": []}, {"text": "The labels assigned to the tokens are 'BS', 'ES' and 'O' to mark the beginning of the boundary, ending of the boundary and non-boundary token respectively.", "labels": [], "entities": [{"text": "BS', 'ES' and 'O'", "start_pos": 39, "end_pos": 56, "type": "METRIC", "confidence": 0.7366775065660477}]}, {"text": "We propose a Conditional Random Field (CRF)] model to predict the label sequence of the input text.", "labels": [], "entities": []}, {"text": "The rules for detecting sentence boundaries can be captured as features of CRF and learns the conditional probability of the label sequence given the observation sequence of features.", "labels": [], "entities": []}, {"text": "We report the related work in Section 2 and briefly discussed the idea of conditional random field in Section 3.", "labels": [], "entities": []}, {"text": "In Section 4, we explain the proposed part of speech and punctuation embeddings-based clustering features for the CRF model in this task.", "labels": [], "entities": []}, {"text": "Section 5 presents the data sets, experiments, evaluation and its results.", "labels": [], "entities": []}, {"text": "Section 6 summarizes the error analysis and discussion which is followed by conclusion and future work in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the data sets, features used in the CRF method and evaluation of results.", "labels": [], "entities": [{"text": "CRF", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9532822370529175}]}, {"text": "In FinSBD shared task, data sets are provided for English and French language.", "labels": [], "entities": []}, {"text": "As the corpus is related to finance domain, the text contains various data elements such as formatting indicators, titles, subtitles, sections.", "labels": [], "entities": []}, {"text": "Each of these elements, in turn, contains various types of vocabulary including special symbols, numerals, currencies, named entities.", "labels": [], "entities": []}, {"text": "The data sets are in JSON format which contains i) the text to detect sentence boundaries and this text is already tokenized using NLTK, ii) begin sentence which contains the indexes of tokens in the text that mark the beginning of well-formed sentences in the text, iii) end sentence which contains the indexes of tokens in the text that mark the end of well-formed sentences in the text.", "labels": [], "entities": []}, {"text": "The dataset statistics for English and French languages are given in  begin with uppercase letter, iii) some tokens are fully given in uppercase, iv) the punctuation '-' is used as the beginning token of bullet points more frequently.", "labels": [], "entities": []}, {"text": "We extract the basic features and punctuation cluster features of tokens as discussed in Section 4.", "labels": [], "entities": []}, {"text": "The parts of speech tag is tagged using python NLTK 2 package for English and stanford pos tagger 3 is used for French.", "labels": [], "entities": []}, {"text": "To obtain the punctuation embeddings for English text, we use pre-trained glove embeddings of 100 dimension and clustered them using k-means clustering.", "labels": [], "entities": []}, {"text": "We experimented with different values of k ranges from 2 to 10 over the development data.", "labels": [], "entities": []}, {"text": "We observed that the punctuations are clustered based on its contextual behaviour and the clusters resulted when k=7 are given in.", "labels": [], "entities": []}, {"text": "For French, we use pretrained.", "labels": [], "entities": []}, {"text": "Our CRF model is compared with the baselines such as punkt sentence tokenizer and sentence boundary detector proposed by Tomanek et al.", "labels": [], "entities": [{"text": "punkt sentence tokenizer", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.5663989782333374}]}, {"text": "The punkt tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.", "labels": [], "entities": []}, {"text": "We use the punkt tokenizer model implemented in NLTK for English and French languages to report the results.", "labels": [], "entities": [{"text": "NLTK", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.9001686573028564}]}, {"text": "The sentence boundary detector by Tomanek et al. is a conditional random field model with the following set of features.", "labels": [], "entities": []}, {"text": "\u2022 The token and its length.", "labels": [], "entities": [{"text": "length", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9874927997589111}]}, {"text": "\u2022 A binary feature which checks whether the token is a sentence boundary symbols such as full stop, question mark, and exclamation mark.", "labels": [], "entities": []}, {"text": "\u2022 Canonical word form which is constructed by applying the transformation rules such as i) replace capital letters by 'A', ii) replace lowercase letters by 'a', iii) replace digits by '0' and, iv) replace all other characters by '-'.", "labels": [], "entities": [{"text": "Canonical word form", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7223002314567566}]}, {"text": "\u2022 Features such as HasDash, AllCaps, InitalCap, hasParenthesis.", "labels": [], "entities": [{"text": "HasDash", "start_pos": 19, "end_pos": 26, "type": "DATASET", "confidence": 0.8153268694877625}]}, {"text": "\u2022 A binary feature which is set to 1 if the token is contained in a list of abbreviations.", "labels": [], "entities": []}, {"text": "\u2022 Local context features of neighboring tokens in the window [-1,1]  We use python CRFsuite to model a linear chain CRF and it is trained using the gradient descent algorithm.", "labels": [], "entities": []}, {"text": "The parameters such as \"all possible transitions\" and \"all possible states\" are set as True.", "labels": [], "entities": []}, {"text": "We tuned the regularization parameters using separate development set for each model.", "labels": [], "entities": []}, {"text": "The context window k to fetch the features of surrounding prefix and suffix tokens is selected based on the F1-score over the development data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9976600408554077}]}, {"text": "The value k is chosen for English and French is 6.", "labels": [], "entities": []}, {"text": "We use a paragraph as a sequence for training and the entire text in the test data as a single sequence for testing.", "labels": [], "entities": []}, {"text": "As the prediction of boundary tokens require the characteristics of preceding and following tokens, the paragraph is considered over sentence as a sequence in the training phase.", "labels": [], "entities": []}, {"text": "The text in the test data is input as a sequence as the paragraph information of test data is not available.", "labels": [], "entities": []}, {"text": "During training, the sentences are constructed using the beginning and ending information given in the train data and a paragraph is considered as five consecutive sentences.", "labels": [], "entities": []}, {"text": "The number of sentences in the paragraph is chosen based on the F1-score over the development data.: Results submitted to FinSBD shared task for the test data  F1-score is used as the evaluation measure in the FinSBD shared task and we also report the precision and recall for evaluation.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9980582594871521}, {"text": "FinSBD shared task", "start_pos": 122, "end_pos": 140, "type": "DATASET", "confidence": 0.8996394673983256}, {"text": "F1-score", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.956588864326477}, {"text": "FinSBD shared task", "start_pos": 210, "end_pos": 228, "type": "DATASET", "confidence": 0.895409365495046}, {"text": "precision", "start_pos": 252, "end_pos": 261, "type": "METRIC", "confidence": 0.9996126294136047}, {"text": "recall", "start_pos": 266, "end_pos": 272, "type": "METRIC", "confidence": 0.9989996552467346}]}, {"text": "The precision, recall and F1-score are averaged for 'BS' and 'ES' labels and this average score is used for reporting the best model.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996578693389893}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9993390440940857}, {"text": "F1-score", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9993670582771301}, {"text": "BS' and 'ES' labels", "start_pos": 53, "end_pos": 72, "type": "METRIC", "confidence": 0.7590928205421993}]}, {"text": "The precision, recall, and F1-score obtained for English and French text over the development data are given in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996507167816162}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9989283680915833}, {"text": "F1-score", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9998108744621277}]}, {"text": "The average F1-score obtained for English text is 89.5% and that of French text is 88% over the development data.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996371269226074}]}, {"text": "The predicted results on the given test set are reported in.", "labels": [], "entities": []}, {"text": "The submitted results are predicted using the CRF model which is trained using all the features discussed in Section 4.", "labels": [], "entities": []}, {"text": "However, we later figured out that we used only a subset of all the features in the feature prepossessing step of the prediction module that we used for generating  results on given test data.", "labels": [], "entities": []}, {"text": "We fixed this mistake later and reported the corrected results in.", "labels": [], "entities": [{"text": "corrected", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9859930276870728}]}, {"text": "The highest accuracy values for precision, recall, and averaged F1 measures are specified in bold font as shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9993519186973572}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996190071105957}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9992893934249878}, {"text": "F1 measures", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9423134624958038}]}, {"text": "In, we report the evaluation scores of baselines such as punkt and CRF model, and our proposed CRF models using basic features and basic + punctuation-based cluster features (See Section 4) for the English gold standard test set given in FinSBD shared task.", "labels": [], "entities": [{"text": "English gold standard test set", "start_pos": 198, "end_pos": 228, "type": "DATASET", "confidence": 0.8048782885074616}, {"text": "FinSBD shared task", "start_pos": 238, "end_pos": 256, "type": "DATASET", "confidence": 0.8247858683268229}]}, {"text": "We can observe that the CRF model using basic and punctuation-based cluster features are performing better than all other methods and, this model scores the highest F1-score for both 'BS' and 'ES' labels.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9991362690925598}]}, {"text": "The CRF model which uses basic features performs better than other baselines and scores the highest F1-score for 'BS'.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9995366334915161}, {"text": "'BS'", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.7367898424466451}]}, {"text": "While the Punkt unsupervised model scores an F1-score of 81% for 'ES' label, the sentence beginning performing very poorly.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995412826538086}, {"text": "ES", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.7666264772415161}]}, {"text": "All CRF based models report greater than 80% F1-score for the 'BS' labels and it indicates that the sequential labeling of tokens gains more information on sentence beginning.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9995549321174622}]}, {"text": "We performed a paired t-test to check the statistical significance of the improvements of proposed CRF models over the baselines and observed that the improvements are statistically significant with p-value less than 0.05..", "labels": [], "entities": []}, {"text": "The proposed CRF models are compared with the baselines and the CRF model which uses basic features.", "labels": [], "entities": []}, {"text": "The CRF model which uses basic and punctuation cluster features is performing better than all other models in the gold standard test data.", "labels": [], "entities": [{"text": "gold standard test data", "start_pos": 114, "end_pos": 137, "type": "DATASET", "confidence": 0.7118500024080276}]}, {"text": "This model identifies beginning and ending of the sentence with F1-scores 89% and 91% respectively.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9993233680725098}]}, {"text": "The CRF model which uses basic features also performs much better than baselines.", "labels": [], "entities": []}, {"text": "The CRF model by identifies the 'BS' and 'ES' labels with F1-scores 80% and 84% respectively.", "labels": [], "entities": [{"text": "BS' and 'ES' labels", "start_pos": 33, "end_pos": 52, "type": "METRIC", "confidence": 0.7359239629336766}, {"text": "F1-scores", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9943479895591736}]}, {"text": "The punkt model performs very poorly for French text in identifying both beginning and ending of the sentences.", "labels": [], "entities": []}, {"text": "The improvements of the proposed models over the baselines] are statistically significant with pvalue less than 0.05 in paired t-test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Punctuation Clusters ob- tained from English text", "labels": [], "entities": []}, {"text": " Table 5: Punctuation Clusters  obtained from French text", "labels": [], "entities": []}, {"text": " Table 5.  Our CRF model is compared with the baselines such as  punkt sentence tokenizer", "labels": [], "entities": [{"text": "punkt sentence tokenizer", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.5953659415245056}]}, {"text": " Table 6: Results obtained for the development data for the submitted  model", "labels": [], "entities": []}, {"text": " Table 7: Results submitted to FinSBD shared task for the test data", "labels": [], "entities": [{"text": "FinSBD", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9161525368690491}]}, {"text": " Table 8: Evaluation of English gold standard test set with all the  features (corrected results post the shared task submission)", "labels": [], "entities": [{"text": "English gold standard test set", "start_pos": 24, "end_pos": 54, "type": "DATASET", "confidence": 0.9601327300071716}]}]}