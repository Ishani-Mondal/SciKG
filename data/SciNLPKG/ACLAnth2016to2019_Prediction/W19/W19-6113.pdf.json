{"title": [{"text": "Annotating evaluative sentences for sentiment analysis: a dataset for Norwegian", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.9134891629219055}]}], "abstractContent": [{"text": "This paper documents the creation of a large-scale dataset of evaluative sentences-i.e. both subjective and objective sentences that are found to be sentiment-bearing-based on mixed-domain professional reviews from various news-sources.", "labels": [], "entities": []}, {"text": "We present both the annotation scheme and first results for classification experiments.", "labels": [], "entities": []}, {"text": "The effort represents a step toward creating a Norwegian dataset for fine-grained sentiment analysis.", "labels": [], "entities": [{"text": "Norwegian dataset", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.8324584364891052}, {"text": "fine-grained sentiment analysis", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6663940250873566}]}], "introductionContent": [{"text": "Sentiment analysis is often approached by first locating the relevant, sentiment-bearing sentences.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9819371402263641}]}, {"text": "Traditionally, one has distinguished between subjective and objective sentences, where only the former were linked to sentiment.", "labels": [], "entities": []}, {"text": "Objective sentences typically present facts about the world, whereas subjective sentences express personal feelings, views, or beliefs.", "labels": [], "entities": []}, {"text": "More recently, however, it has become widely recognized in the literature that subjectivity should not be equated with opinion (Liu, 2015): On the one hand, there are many subjective sentences that do not express sentiment, e.g., I think that he went home, and on the other hand there are many objective sentences that do, e.g., The earphone broke in two days, to quote some examples from.", "labels": [], "entities": []}, {"text": "Additionally, sentences often contain several polarities in a single sentence, which complicates the labeling of a full sentence as positive or negative.", "labels": [], "entities": []}, {"text": "This paper documents both the annotation effort and first experimental results for sentencelevel evaluative labels added to a subset of the data in the Norwegian Review Corpus (NoReC) (, a corpus of full-text reviews from a range of different domains, collected from several of the major Norwegian news sources.", "labels": [], "entities": [{"text": "Norwegian Review Corpus (NoReC)", "start_pos": 152, "end_pos": 183, "type": "DATASET", "confidence": 0.9463950196901957}]}, {"text": "The annotated subset, dubbed NoReC eval , covers roughly 8000 sentences across 300 reviews and 10 different thematic categories (literature, products, restaurants, etc.).", "labels": [], "entities": [{"text": "NoReC eval", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.9248074889183044}]}, {"text": "Sentences are labeled to indicate whether they are evaluative, i.e. where they are intended by the author (or some other opinion holder) to serve as an evaluation or judgment.", "labels": [], "entities": [{"text": "Sentences are labeled to indicate whether they are evaluative, i.e. where they are intended by the author (or some other opinion holder) to serve as an evaluation or judgment", "start_pos": 0, "end_pos": 174, "type": "Description", "confidence": 0.7521429508924484}]}, {"text": "They are not, however, annotated with respect to positive/negative polarity.", "labels": [], "entities": []}, {"text": "The reason for this is that polarity is often mixed at the sentence-level.", "labels": [], "entities": []}, {"text": "Hence, we defer annotating polarity to a later round of phrase-level annotation.", "labels": [], "entities": []}, {"text": "Although most of the sentences labeled as evaluative will be subjective and personal, they can also include objective sentences.", "labels": [], "entities": []}, {"text": "Moreover, our annotation scheme singles out a particular category of evaluative sentences called factimplied non-personal, following the terminology of.", "labels": [], "entities": []}, {"text": "Evaluative sentences are also further sub-categorized as to whether they are considered on-topic with respect to the object being reviewed, and whether they express the first-person view of the author.", "labels": [], "entities": []}, {"text": "The annotation scheme is described in further detail in Sections 3 and 4.", "labels": [], "entities": []}, {"text": "We start, however, by briefly outlining relevant previous work and background in Section 2.", "labels": [], "entities": []}, {"text": "In Section 5 we describe more practical aspects of the annotation procedure and goon to analyze inter-annotator agreement in Section 6, before Section 7 summarizes the resulting dataset.", "labels": [], "entities": []}, {"text": "In Section 8, we analyze the corpus experimentally and present a series of preliminary classification experiments using a wide range of state-of-the-art sentiment models including CNNs, BiLSTMs and self-attention networks, before we in Section 9 conclude and outline some remaining avenues for future work.", "labels": [], "entities": []}, {"text": "The dataset and the annotation guidelines are made available, along with code for replicating the experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we apply a range of different architectures to provide first baseline results for predicting the various labels in the new corpus.", "labels": [], "entities": []}, {"text": "Data splits for training, validation and testing are inherited from NoReC.", "labels": [], "entities": [{"text": "NoReC", "start_pos": 68, "end_pos": 73, "type": "DATASET", "confidence": 0.9412822723388672}]}, {"text": "We apply the models to five experimental setups.", "labels": [], "entities": []}, {"text": "The main task is to classify each sentence as evaluative (EVAL), fact-implied non-personal (FACT-NP), or non-evaluative (NONE).", "labels": [], "entities": [{"text": "FACT-NP", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8795225620269775}]}, {"text": "In order to provide a view of how difficult it is to model the secondary properties mentioned in Section 3,  two additional binary classification tasks are performed; determining if the sentence is on topic (OT) and if the opinion expressed is from a firstperson perspective (FP).", "labels": [], "entities": []}, {"text": "Only the best performing model from the main experiment above is applied for these subtask, and the model is trained and tested separately on the two subsets of sentences annotated as EVAL and FACT-NP, leading to four binary classification experiments in total.", "labels": [], "entities": [{"text": "FACT-NP", "start_pos": 193, "end_pos": 200, "type": "DATASET", "confidence": 0.4888581931591034}]}, {"text": "For all models, we choose the optimal hyperparameters by performing a random search on the development data.", "labels": [], "entities": []}, {"text": "Given that neural models are sensitive to random initialization parameters, we run each neural experiment five times with different random seeds and report means for both perclass and micro F 1 in addition to their standard deviation.", "labels": [], "entities": []}, {"text": "shows the results for all models on the main three-way classification task.", "labels": [], "entities": [{"text": "three-way classification task", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6849881708621979}]}, {"text": "All classifiers perform better than the majority baseline (at 49.5 F 1 overall).", "labels": [], "entities": [{"text": "49.5 F 1", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.7638354301452637}]}, {"text": "Of the two logistic regression classifiers, the AVE model based on averaged embeddings as input performs much better than the standard discrete bag-of-words variant (65.8 vs. 71.6 overall).", "labels": [], "entities": [{"text": "AVE", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9248158931732178}]}, {"text": "While the AVE model proves to be a strong baseline, the three neural models have the strongest performance.", "labels": [], "entities": [{"text": "AVE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.7972420454025269}]}, {"text": "The CNN achieves the best results on the EVAL class (76.3) and improves 1.8 ppt over AVE on NONE.", "labels": [], "entities": [{"text": "CNN", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.7863379120826721}, {"text": "EVAL class", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.7096741497516632}, {"text": "AVE", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9069371223449707}, {"text": "NONE", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9684103727340698}]}, {"text": "While overall results are quite even, the strongest model is SAN -the selfattention network -which achieves an overall F 1 of 73.7.", "labels": [], "entities": [{"text": "F 1", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9932138621807098}]}, {"text": "This model also proves more stable in the sense of having slightly lower variance across the multiple runs, at least compared to the CNN.", "labels": [], "entities": [{"text": "CNN", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.9518903493881226}]}], "tableCaptions": [{"text": " Table 1: F 1 inter-annotator agreement for each  top-level label.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9872273206710815}]}, {"text": " Table 2: Distribution of documents, sentences and labels across the thematic categories of reviews. Note  that the percentages for \u00acOT and \u00acFP are relative to evaluative (EVAL or FACT-NP) sentences.", "labels": [], "entities": [{"text": "\u00acOT and \u00acFP", "start_pos": 132, "end_pos": 143, "type": "METRIC", "confidence": 0.6542018175125122}]}, {"text": " Table 3: Per class F 1 score and overall micro F 1  of baseline models on the main classification task.  For the neural models mean micro F 1 and standard  deviation across five runs are shown.", "labels": [], "entities": [{"text": "micro F 1", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.839149276415507}, {"text": "micro F 1", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.7488646308581034}]}, {"text": " Table 5: Per-class and micro F 1 for the self-attention network trained to predict whether an example is  on topic (OT) or not (\u00acOT) or whether the opinion is expressed by the first person (FP) or not (\u00acFP). The  models are trained and tested on the subset of sentences annotated as evaluative (EVAL) and fact-implied  (FACT-NP).", "labels": [], "entities": [{"text": "micro F 1", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.8602039217948914}]}]}