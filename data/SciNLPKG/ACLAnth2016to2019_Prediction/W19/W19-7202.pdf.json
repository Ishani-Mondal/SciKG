{"title": [{"text": "Transductive Data-Selection Algorithms for Fine-Tuning Neural Machine Translation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 55, "end_pos": 81, "type": "TASK", "confidence": 0.7313041289647421}]}], "abstractContent": [{"text": "Machine Translation models are trained to translate a variety of documents from one language into another.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7445296943187714}]}, {"text": "However, models specifically trained fora particular characteristics of the documents tend to perform better.", "labels": [], "entities": []}, {"text": "Fine-tuning is a technique for adapting an NMT model to some domain.", "labels": [], "entities": []}, {"text": "In this work, we want to use this technique to adapt the model to a given test set.", "labels": [], "entities": []}, {"text": "In particular, we are using transductive data selection algorithms which take advantage the information of the test set to retrieve sentences from a larger parallel set.", "labels": [], "entities": []}, {"text": "In cases where the model is available at translation time (when the test set is provided), it can be adapted with a small subset of data, thereby achieving better performance than a generic model or a domain-adapted model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine Translation (MT) models aim to generate a text in the target language which corresponds to the translation of a text in the source language, the test set.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8759368419647217}]}, {"text": "These models are trained with a set of parallel sentences so they can learn how to generalize and infer a translation when anew document is seen.", "labels": [], "entities": [{"text": "generalize and infer a translation", "start_pos": 83, "end_pos": 117, "type": "TASK", "confidence": 0.6023990452289582}]}, {"text": "In the field of MT, Neural Machine Translation (NMT) models tend to achieve the best performances when large amounts of parallel sentences are used.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9943927526473999}, {"text": "Neural Machine Translation (NMT)", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.8231922686100006}]}, {"text": "However, relevant data is more useful than having more data.", "labels": [], "entities": []}, {"text": "Previous studies (Silva et showed that models trained with in-domain sentences perform better than generaldomain models.", "labels": [], "entities": []}, {"text": "However, training models for domains that are distant from general domains, such as scientific documents, is not always a simple task as parallel sentences are not always available.", "labels": [], "entities": []}, {"text": "In addition, identifying the domain adds complexity if the domain of the document to be translated is too specific.", "labels": [], "entities": []}, {"text": "The alternative explored in this work is to build models adapted to a given test set.", "labels": [], "entities": []}, {"text": "In order to build task-specific models, data selection algorithms play an important role as they retrieve sentences from the training data.", "labels": [], "entities": []}, {"text": "Data selection methods can be classified ( according to the criteria considered to select sentences (e.g. select sentences of a particular domain, good quality sentences, etc.).", "labels": [], "entities": []}, {"text": "In this work, we use the transductive) data selection methods which use the document to be translated to select sentences that are the most relevant for translating such text.", "labels": [], "entities": []}, {"text": "In some cases, the organizations in charge of translating a document are also the owner of the translation model and training data.", "labels": [], "entities": []}, {"text": "Therefore, knowing the test set is an advantage that can be helpful for adapting the generic MT model towards the test set ().", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.939617931842804}]}, {"text": "The approaches presented here consist of building a single NMT model and delay part of the process of training data for adapting the model when the test set is available.", "labels": [], "entities": []}, {"text": "Although this implies increasing the time involved in translating a document, it also has some benefits.", "labels": [], "entities": []}, {"text": "First, using a single model causes storing multiple task-adapted models not to be necessary.", "labels": [], "entities": []}, {"text": "Moreover, identifying the domain of the document (and so, the most appropriate model) before the", "labels": [], "entities": []}], "datasetContent": [{"text": "The data sets used in the experiments are based on the ones used in the work of: We build German-to-English NMT model using the data provided in the WMT 2015) (4.5M sentence pairs).", "labels": [], "entities": [{"text": "WMT 2015)", "start_pos": 149, "end_pos": 158, "type": "DATASET", "confidence": 0.9498748183250427}]}, {"text": "We consider this data set as the general-domain training data to build the non-adapted NMT (BASE).", "labels": [], "entities": [{"text": "BASE", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8162554502487183}]}, {"text": "As development data, we use 5K randomly sampled sentences from development sets of previous years.", "labels": [], "entities": []}, {"text": "The BASE model is adapted to two domains: news and health.", "labels": [], "entities": [{"text": "BASE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.6705991625785828}]}, {"text": "Therefore we also use two test sets and two in-domain training set (for the research question 2 and 3 explained in Section 2): \u2022 News Domain: We use the test set provided in WMT 2015 News Translation Task, and the in-domain rapid2016 1 data set (1.3M sentence pairs) provided in WMT 2017 News Translation ().", "labels": [], "entities": [{"text": "WMT 2015 News Translation Task", "start_pos": 174, "end_pos": 204, "type": "TASK", "confidence": 0.8005071163177491}, {"text": "WMT 2017 News Translation", "start_pos": 279, "end_pos": 304, "type": "TASK", "confidence": 0.8730582296848297}]}, {"text": "\u2022 Note that the general-domain set contains sentences from a corpus such as Europarl () which causes the domain to be closer to the news domain.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9800844192504883}]}, {"text": "All data sets are tokenized, truecased and Byte Pair Encoding (BPE) () is applied with 89500 merge operations (the number of operations used in the work of).", "labels": [], "entities": [{"text": "Byte Pair Encoding (BPE)", "start_pos": 43, "end_pos": 67, "type": "METRIC", "confidence": 0.9154957433541616}]}, {"text": "The models have been built using OpenNMT-py ().", "labels": [], "entities": []}, {"text": "We keep the default settings of OpenNMT-py: 2-layer LSTM with 500 hidden units, vocabulary size of 50000 words for each language.", "labels": [], "entities": []}, {"text": "We use different evaluation metrics to evaluate the performance of the models builtin the experiments.", "labels": [], "entities": []}, {"text": "These models are evaluated on the test sets using several evaluation metrics: BLEU (), TER () and ME-TEOR ( presents the results evaluated with the news test set evaluated in the 12th epoch of the base model (BASE12) and the 13th epoch (BASE13).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9988884329795837}, {"text": "TER", "start_pos": 87, "end_pos": 90, "type": "METRIC", "confidence": 0.9966885447502136}, {"text": "ME-TEOR", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.9916136264801025}, {"text": "news test set", "start_pos": 148, "end_pos": 161, "type": "DATASET", "confidence": 0.7365802029768626}, {"text": "BASE12", "start_pos": 209, "end_pos": 215, "type": "METRIC", "confidence": 0.5061971545219421}, {"text": "BASE13", "start_pos": 237, "end_pos": 243, "type": "DATASET", "confidence": 0.6376079320907593}]}, {"text": "Similarly, presents the results evaluated with the test set in the health domain.", "labels": [], "entities": []}, {"text": "These results help to confirm that the models trained for 12 epochs are close to convergence: In the increment in performance from the 12th to the 13th epoch is just of 0.0018 BLEU points and in the performance is worse in the 13th epoch.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.998529314994812}]}, {"text": "In order to answer the questions in Section 2, we perform three set of experiments: fine-tune the BASE12 model with a subset of the general domain data (Section 7.1), with a subset of in-domain data, and with a subset of data retrieved from both general domain data and indomain data (Section 7.3).", "labels": [], "entities": [{"text": "BASE12", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.6931273937225342}]}, {"text": "We use the default configuration of the data selection methods.", "labels": [], "entities": []}, {"text": "We used = 0.5, c = 0 and 3-grams as features in FDA (Equation).", "labels": [], "entities": [{"text": "FDA", "start_pos": 48, "end_pos": 51, "type": "DATASET", "confidence": 0.7748889327049255}]}, {"text": "In the INR method we also use 3-grams as ngr (in Equation).", "labels": [], "entities": [{"text": "INR", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.5035465955734253}, {"text": "Equation", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9329444169998169}]}, {"text": "In order to find a value of the threshold for the experiments, in this paper we execute several runs of INR using different values oft, multiplying by two in each execution (we try 10, 20, 40, 80 ...).", "labels": [], "entities": []}, {"text": "In the experiments we use the highest value oft that fulfills one of the following criteria: (i) the execution time should be under 48 hours or (ii) the number of sentences retrieved at least 500K.", "labels": [], "entities": []}, {"text": "Accordingly, the value oft in news domain is 80 (230K sentences retrieved) and in health domain 640 (275K sentences retrieved).", "labels": [], "entities": []}, {"text": "In order to investigate the first question mentioned in Section 2 we select a subset of sentences of the general-domain data (the data set used to build BASE12).", "labels": [], "entities": [{"text": "BASE12", "start_pos": 153, "end_pos": 159, "type": "DATASET", "confidence": 0.7236511707305908}]}, {"text": "We extract subsets of three different sizes: 100K, 200K, and 500K lines.", "labels": [], "entities": []}, {"text": "The only exception is the INR method which, with the established configuration, retrieves at most 230K sentences and 275K sentences using the news and health test, respectively.", "labels": [], "entities": [{"text": "INR", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.5342615246772766}]}, {"text": "The BASE12 model is fine-tuned fora 13th epoch using the subset of data extracted.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.5738499760627747}]}], "tableCaptions": [{"text": " Table 1: Results of the model BASE12 and BASE13 evalu- ated on the news test set.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.5530893802642822}, {"text": "BASE13", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.7896125912666321}, {"text": "news test set", "start_pos": 68, "end_pos": 81, "type": "DATASET", "confidence": 0.920428196589152}]}, {"text": " Table 2: Results of the model BASE12 and BASE13 evalu- ated on the health test set.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.975188672542572}, {"text": "BASE13", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.5929217338562012}, {"text": "health test set", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.7734626134236654}]}, {"text": " Table 3: Results of the model BASE12 fine-tuned with the  in-domain news set.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.4315604567527771}]}, {"text": " Table 4: Results of the model BASE12 fine-tuned with the  in-domain health set.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.5183762311935425}]}, {"text": " Table 5: Performance on the news test for the BASE12  model, fine-tuned with subsets of the training data.", "labels": [], "entities": [{"text": "BASE12", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.7589586973190308}]}, {"text": " Table 6: Performance on the health test for the BASE12  model, fine-tuned with subsets of the training data.", "labels": [], "entities": [{"text": "BASE12  model", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8183684647083282}]}, {"text": " Table 7: Performance on the news test for the BASE12  model, fine-tuned with subsets of the rapid2016 data set.", "labels": [], "entities": [{"text": "BASE12  model", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.8912861049175262}, {"text": "rapid2016 data set", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.966995894908905}]}, {"text": " Table 8: Performance on the health test for the BASE12  model, fine-tuned with subsets of the EMEA data set.", "labels": [], "entities": [{"text": "BASE12  model", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.821765810251236}, {"text": "EMEA data set", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9797638654708862}]}, {"text": " Table 9: Percentage of base training data lines retrieved.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9629635810852051}, {"text": "base training data lines", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.6709777638316154}]}, {"text": " Table 10: Performance on the news test for the BASE12  model, fine-tuned with subsets of a combination of the BASE  and rapid2016 data sets.", "labels": [], "entities": [{"text": "news test", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8657327592372894}, {"text": "BASE12  model", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9181284308433533}, {"text": "BASE  and rapid2016 data sets", "start_pos": 111, "end_pos": 140, "type": "DATASET", "confidence": 0.6938871681690216}]}, {"text": " Table 11: Performance on the health test for the BASE12  model, fine-tuned with subsets of a combination of the BASE  and EMEA data sets.", "labels": [], "entities": [{"text": "BASE12  model", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9195794463157654}, {"text": "BASE", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.5569919347763062}, {"text": "EMEA data sets", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.8638735810915629}]}]}