{"title": [{"text": "Kingsoft's Neural Machine Translation System for WMT19", "labels": [], "entities": [{"text": "Kingsoft", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9151453971862793}, {"text": "Neural Machine Translation", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6139519214630127}, {"text": "WMT19", "start_pos": 49, "end_pos": 54, "type": "TASK", "confidence": 0.5078809261322021}]}], "abstractContent": [{"text": "This paper describes the Kingsoft AI Lab's submission to the WMT2019 news translation shared task.", "labels": [], "entities": [{"text": "Kingsoft AI Lab", "start_pos": 25, "end_pos": 40, "type": "DATASET", "confidence": 0.8404115637143453}, {"text": "WMT2019 news translation shared task", "start_pos": 61, "end_pos": 97, "type": "TASK", "confidence": 0.8170543432235717}]}, {"text": "We participated in two language directions: English\u2192Chinese and Chinese\u2192English.", "labels": [], "entities": []}, {"text": "For both language directions , we trained several variants of Transformer models using the provided parallel data enlarged with a large quantity of back-translated monolingual data.", "labels": [], "entities": []}, {"text": "The best translation result was obtained with ensemble and reranking techniques.", "labels": [], "entities": [{"text": "translation", "start_pos": 9, "end_pos": 20, "type": "TASK", "confidence": 0.9684157371520996}]}, {"text": "According to automatic metrics (BLEU) our Chinese\u2192English system reached the second highest score, and our English\u2192Chinese system reached the second highest score for this subtask.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9064866900444031}]}], "introductionContent": [{"text": "In recent years, the development of sequenceto-sequence (seq2seq) models have changed the field of machine translation a lot.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7873463332653046}]}, {"text": "This kind of models replaced traditional statistical approaches with neural machine translation (NMT) systems which is based on the encoder-decoder framework.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 69, "end_pos": 101, "type": "TASK", "confidence": 0.8316002488136292}]}, {"text": "Two years ago, the Transformer model, which is based on the multi-head attention mechanism and feedforward networks, has further advanced the field of NMT by improving the translation quality and speed of convergence (.", "labels": [], "entities": []}, {"text": "Until now, a variety of NMT models and advanced techniques have been proposed, leading to better performance of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8175546228885651}]}, {"text": "We participated in the WMT19 shared task: the machine translation of news on English\u2194Chinese language pairs.", "labels": [], "entities": [{"text": "WMT19 shared task", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.609209418296814}, {"text": "machine translation of news on English\u2194Chinese language pairs", "start_pos": 46, "end_pos": 107, "type": "TASK", "confidence": 0.8136517167091369}]}, {"text": "This paper describes the NMT systems we submitted for the WMT19 Chinese\u2192English and English\u2192Chinese machine translation tasks.", "labels": [], "entities": [{"text": "WMT19 Chinese\u2192English and English\u2192Chinese machine translation", "start_pos": 58, "end_pos": 119, "type": "TASK", "confidence": 0.5383693128824234}]}, {"text": "For data augmentation, we selected a subset of * Corresponding author monolingual corpus as additional datasets and applied back translation to augment our training corpus.", "labels": [], "entities": []}, {"text": "The baseline model in our system was based on the Transformer architecture.", "labels": [], "entities": []}, {"text": "In order to improve the single system's performance, we experimented with some research findings such as Transformer with Relative Position Attention ( and Dynamic Convolution Networks (.", "labels": [], "entities": []}, {"text": "We also proposed our own model architectures and applied them in the tasks.", "labels": [], "entities": []}, {"text": "These architectures improve translation quality a lot and will be described in the next section.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9715859293937683}]}, {"text": "For further improvement, we tried different multi-system based techniques, such as model ensembling and model reranking.", "labels": [], "entities": [{"text": "model ensembling", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.6989420354366302}]}, {"text": "These techniques can improve translation performance on the basis of a very strong single system.", "labels": [], "entities": [{"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9788162708282471}]}, {"text": "At the same time, we also designed some specific strategies to deal with problems during ensembling, such as the overflow of memory space and the slow decoding speed.", "labels": [], "entities": []}, {"text": "As a result, our Chinese\u2192English system achieved the second highest cased BLEU score among all 15 submitted constrained systems, and our English\u2192Chinese system ranked the second out of 12 submitted systems.", "labels": [], "entities": [{"text": "cased", "start_pos": 68, "end_pos": 73, "type": "METRIC", "confidence": 0.9791232347488403}, {"text": "BLEU score", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9053318202495575}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: English\u2192Chinese Systems BLEU results on  newsdev2017 and newstest2018. As for newsdev2017  ensemble step, we only mannually selected two models  for ensembling test but for newstest2018, we applied  our ensemble algorithm on all models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9986514449119568}]}, {"text": " Table 2: Chinese\u2192English Systems BLEU results on  newstest2017.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9943496584892273}, {"text": "newstest2017", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9768614172935486}]}, {"text": " Table 3: BLEU results for different model architec- tures. For EN-ZH, It represents the results on news- dev2017 and for ZH-EN, it represents the results on  newstest2017. All models are trained with synthetic  data after back translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9990097284317017}, {"text": "newstest2017", "start_pos": 159, "end_pos": 171, "type": "DATASET", "confidence": 0.9516379833221436}]}]}