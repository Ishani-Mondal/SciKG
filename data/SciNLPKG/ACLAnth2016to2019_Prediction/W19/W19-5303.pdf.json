{"title": [{"text": "Findings of the First Shared Task on Machine Translation Robustness", "labels": [], "entities": [{"text": "Machine Translation Robustness", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8438350160916647}]}], "abstractContent": [{"text": "We share the findings of the first shared task on improving robustness of Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.8597671747207641}]}, {"text": "The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models' robustness to noisy input and domain mismatch.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.9875481128692627}]}, {"text": "We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit 1 and professionally sourced translations.", "labels": [], "entities": []}, {"text": "As anew task, we received 23 submissions by 11 participating teams from universities , companies, national labs, etc.", "labels": [], "entities": []}, {"text": "All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9980461597442627}]}, {"text": "We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson's r = 0.94 and 0.95).", "labels": [], "entities": [{"text": "automatic evaluation (BLEU)", "start_pos": 52, "end_pos": 79, "type": "METRIC", "confidence": 0.9004721879959107}, {"text": "Pearson's r", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.8813968499501547}]}, {"text": "Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt 2 , which revealed their salient differences in handling challenges in this task.", "labels": [], "entities": []}, {"text": "Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9961937665939331}]}], "introductionContent": [{"text": "In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8455577731132508}]}, {"text": "However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial 1 www.reddit.com 2 https://github.com/neulab/compare-mt examples (.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8486639857292175}]}, {"text": "From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.9923831820487976}]}, {"text": "The goal of this shared task is to provide a testbed for improving MT models' robustness to orthographic variations, grammatical errors, and other linguistic phenomena common in usergenerated content, via better modelling, training, adaptation techniques, or leveraging monolingual training data.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9901651740074158}]}, {"text": "Specifically, the shared task aims to bring improvements on the following challenges: \u2022 To improve NMT's robustness to orthographic variations, grammatical errors, informal language, and other linguistic phenomena or noise common on social media.", "labels": [], "entities": []}, {"text": "\u2022 To explore effective approaches to leverage abundant out-of-domain parallel data.", "labels": [], "entities": []}, {"text": "\u2022 To explore novel approaches to leverage abundant monolingual data on the Web (e.g., tweets, Reddit comments, commoncrawl, etc.).", "labels": [], "entities": []}, {"text": "\u2022 To thoroughly investigate and understand the overall challenges in translating social media text and identify major themes of efforts which needs more research from the community.", "labels": [], "entities": [{"text": "translating social media text", "start_pos": 69, "end_pos": 98, "type": "TASK", "confidence": 0.9195045977830887}]}, {"text": "In this first iteration, the shared-task used the MTNT dataset) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng\u2192Fra, Fra\u2192Eng, Eng\u2192Jpn, and Jpn\u2192Eng.", "labels": [], "entities": [{"text": "MTNT dataset", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.9403790533542633}]}, {"text": "We describe the dataset and the task setup in Section 3.", "labels": [], "entities": []}, {"text": "The shared-task attracted a total of 23 submissions from 11 teams.", "labels": [], "entities": []}, {"text": "The teams employed a variety of methods to improve robustness.", "labels": [], "entities": []}, {"text": "A specific challenge was the small size of the in-domain noisy parallel dataset.", "labels": [], "entities": []}, {"text": "We summarize the participating systems in Section 4 and the notable methods in Section 5.", "labels": [], "entities": []}, {"text": "The contributions were evaluated both automatically and via a human evaluation.", "labels": [], "entities": []}, {"text": "The results demonstrate a significant progress of the state-of-the-art in MT robustness, with multiple teams surpassing the sharedtask baseline by a large margin.", "labels": [], "entities": [{"text": "MT robustness", "start_pos": 74, "end_pos": 87, "type": "TASK", "confidence": 0.8995784521102905}]}, {"text": "These results are discussed in Section 6.", "labels": [], "entities": []}, {"text": "We hope that this task leads to more efforts from the community in building robust MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.9955490827560425}]}], "datasetContent": [{"text": "The system outputs were evaluated by professional translators.", "labels": [], "entities": []}, {"text": "The translators were presented the original source sentence, the reference and the system output side by side.", "labels": [], "entities": []}, {"text": "The order between the reference and the system output was randomized by the user interface.", "labels": [], "entities": []}, {"text": "The translators rated both the reference and the translation on a scale from 1 to 100.", "labels": [], "entities": []}, {"text": "For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems' outputs.", "labels": [], "entities": []}, {"text": "The user interface for annotation is illustrated in.", "labels": [], "entities": []}, {"text": "We also evaluated BLEU () for each system using SacreBLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9991795420646667}, {"text": "SacreBLEU", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.8983832001686096}]}, {"text": "For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options.", "labels": [], "entities": []}, {"text": "In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none.", "labels": [], "entities": []}, {"text": "The results of human evaluation following the evaluation protocol described in Section 3.4 are outlined in.", "labels": [], "entities": []}, {"text": "Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in.", "labels": [], "entities": [{"text": "automatic evaluation (BLEU)", "start_pos": 25, "end_pos": 52, "type": "METRIC", "confidence": 0.6873098373413086}]}], "tableCaptions": [{"text": " Table 1: Statistics of the test sets.", "labels": [], "entities": []}, {"text": " Table 2: Average human judgments over all submitted systems (the higher the better). The systems' rank for each  translation direction is shown in parentheses. The best system is highlighted.", "labels": [], "entities": []}, {"text": " Table 3: Automatic evaluation (BLEU, cased) over all submitted systems, with the system's rank in parentheses.  The best system is highlighted.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9719439148902893}]}, {"text": " Table 4: An example of handling of casing in two Fra\u2192Eng systems", "labels": [], "entities": []}, {"text": " Table 5: Examples of translation results on special  characters.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9783912301063538}]}, {"text": " Table 6: An example of translation results on as sentence with an unusual number of special symbols.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.983737587928772}]}, {"text": " Table 7: Examples of n-grams where one the NTT  Eng\u2192Jpn system was more accurate than the NLE sys- tem", "labels": [], "entities": [{"text": "NTT  Eng\u2192Jpn system", "start_pos": 44, "end_pos": 63, "type": "DATASET", "confidence": 0.8917382717132568}, {"text": "NLE sys- tem", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.8841147869825363}]}]}