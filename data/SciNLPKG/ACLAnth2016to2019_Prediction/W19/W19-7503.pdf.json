{"title": [{"text": "Revisiting the Role of Feature Engineering for Compound Type Identification in Sanskrit", "labels": [], "entities": [{"text": "Compound Type Identification", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6753836770852407}]}], "abstractContent": [{"text": "We propose an automated approach for semantic class identification of compounds in San-skrit.", "labels": [], "entities": [{"text": "semantic class identification of compounds", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.8404764175415039}]}, {"text": "It is essential to extract semantic information hidden in compounds for improving overall downstream Natural Language Processing (NLP) applications such as information extraction, question answering, machine translation, and many more.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.7973371148109436}, {"text": "question answering", "start_pos": 180, "end_pos": 198, "type": "TASK", "confidence": 0.9018150866031647}, {"text": "machine translation", "start_pos": 200, "end_pos": 219, "type": "TASK", "confidence": 0.8029292821884155}]}, {"text": "In this work, we systematically investigate the following research question: Can recent advances in neural network outperform traditional hand engineered feature based methods on the semantic level multi-class compound classification task for Sanskrit?", "labels": [], "entities": [{"text": "semantic level multi-class compound classification task", "start_pos": 183, "end_pos": 238, "type": "TASK", "confidence": 0.6354184150695801}]}, {"text": "Contrary to the previous methods, our method does not require feature engineering.", "labels": [], "entities": []}, {"text": "For well-organized analysis, we categorize neural systems based on Multi-Layer Perceptron (MLP), Convolution Neu-ral Network (CNN) and Long Short Term Memory (LSTM) architecture and feed input to the system from one of the possible levels, namely, word level, sub-word level, and character level.", "labels": [], "entities": []}, {"text": "Our best system with LSTM architecture and FastText embedding with end-to-end training has shown promising results in terms of F-score (0.73) compared to the state of the art method based on feature engineering (0.74) and outperformed in terms of accuracy (77.68%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9994383454322815}, {"text": "accuracy", "start_pos": 247, "end_pos": 255, "type": "METRIC", "confidence": 0.9995620846748352}]}], "introductionContent": [{"text": "The landscape of Natural Language Processing has significantly shifted towards the realm of Deep Learning and Artificial Neural Networks.", "labels": [], "entities": []}, {"text": "With the benefit of hindsight, the title for the seminal work on a neural pipeline for NLP from, \"Natural Language Processing (Almost) from Scratch\", seems prophetic.", "labels": [], "entities": []}, {"text": "Neural networks have demonstrated promising results in a wide variety of problems like sentiment analysis, information extraction (), text classification, machine translation () among others.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9534343183040619}, {"text": "information extraction", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.7973835170269012}, {"text": "text classification", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.8160311281681061}, {"text": "machine translation", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7961852848529816}]}, {"text": "Many of such models in fact have become part and parcel of a standard NLP pipeline for data processing, especially for the resource-rich languages such as English (.", "labels": [], "entities": []}, {"text": "There have been academic debates over the philosophical implications of the use of such statistical black box approaches in Computational Linguistics, especially towards the trade-off between performance and interpretability as also summarised in.", "labels": [], "entities": []}, {"text": "However, in this work, we focus more on the pragmatic side of using such approaches for low resource languages like Sanskrit.", "labels": [], "entities": []}, {"text": "Deep Learning models demand a humongous amount of data to train a model effectively.", "labels": [], "entities": []}, {"text": "Additionally, it is challenging and often tricky to incorporate available linguistic knowledge into these neural architectures ().", "labels": [], "entities": []}, {"text": "Summarily, we can say that a standard off the shelf neural model relies mostly on its capacity to learn distributional information from the large datasets provided as input during training.", "labels": [], "entities": []}, {"text": "In this pretext, we revisit the problem of compound type identification in Sanskrit ( and experiment with various neural architectures for solving the task.", "labels": [], "entities": [{"text": "compound type identification", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.6814712981383005}]}, {"text": "The process of compounding and the nature of compositionality of the compounds are well studied in the field of NLP.", "labels": [], "entities": []}, {"text": "Given that compounding is a productive process of word-formation in languages, this is of much interest in the area of word-level semantics in NLP.", "labels": [], "entities": []}, {"text": "There are various aspects involved in the compound analysis.", "labels": [], "entities": [{"text": "compound analysis", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7623624205589294}]}, {"text": "These include productivity and recursiveness of the words involved in the process, presence of implicit relations between the components, and finally, the analysis of a compound relies on its pragmatic or contextual features.", "labels": [], "entities": []}, {"text": "Recently, there has been a concerted effort in studying the nature of compositionality in compounds by leveraging on distributional word-representations or word embeddings and then learning function approximators to predict the nature of compositionality of such words.", "labels": [], "entities": []}, {"text": "In Sanskrit, have proposed a framework for semantic type classification of compounds in Sanskrit.", "labels": [], "entities": [{"text": "semantic type classification of compounds", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.8151364505290986}]}, {"text": "They proposed a multi-class classifier using Random Forests (), where they classified a given compound into one of the four coarse level compound classes, namely, Avyay\u00af \u0131bh\u00af ava, Tatpurus . a, Bahuvr\u00af \u0131hi and Dvandva.", "labels": [], "entities": []}, {"text": "They have used an elaborate feature set, which summarily consists of rules from the grammar treatise As . t . \u00af adhy\u00af ay\u00af \u0131 pertaining to compounding, semantic relations between the compound components from a lexical database Amarakos . a and distributional subword patterns from the data using Adaptor.", "labels": [], "entities": []}, {"text": "Inspired from the recent advances in using neural models for compound analysis in NLP, we revisit the task of compound class identification and validate the efficacy of such models under the low-resource setting like that of Sanskrit.", "labels": [], "entities": [{"text": "compound analysis", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.8242524266242981}, {"text": "compound class identification", "start_pos": 110, "end_pos": 139, "type": "TASK", "confidence": 0.71934974193573}]}, {"text": "In this work, we experiment with multiple deep learning models for compound type classification.", "labels": [], "entities": [{"text": "compound type classification", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.763430674870809}]}, {"text": "Our extensive experiments include standard neural models comprising of Multi-Layer Perceptrons (MLP), Convolution Neural Networks (CNN) () and Recurrent models such as Long Short Term Memory (LSTM) configurations.", "labels": [], "entities": []}, {"text": "Unlike the feature-rich representation of, we rely on various word embedding approaches, which include character level, sub-word level, and word-level embedding approaches.", "labels": [], "entities": []}, {"text": "Using end-to-end training, the pretrained embeddings are fine tuned for making them task specific embeddings.", "labels": [], "entities": []}, {"text": "So all the architectures are integrated with end-to-end training).", "labels": [], "entities": []}, {"text": "The best system of ours, an end-to-end LSTM architecture initialised with fasttext embeddings has shown promising results in terms of F-score (0.73) compared to the state of the art classifier from and outperformed it in terms of accuracy (77.68%).", "labels": [], "entities": [{"text": "F-score", "start_pos": 134, "end_pos": 141, "type": "METRIC", "confidence": 0.9990487694740295}, {"text": "accuracy", "start_pos": 230, "end_pos": 238, "type": "METRIC", "confidence": 0.9995303153991699}]}, {"text": "Summarily, we find that the models we experimented with, report competitive results with the current state of the art model for compound type identification.", "labels": [], "entities": [{"text": "compound type identification", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.7917345960934957}]}, {"text": "We achieve the same without making use of any feature engineering or domain expertise.", "labels": [], "entities": []}, {"text": "We release the codebase for all our models experimented with at https://github.com/Jivnesh/ISCLS-19.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our text corpus contains data from the Digital Corpus of Sanskrit (DCS) 1 , as well as scraped data from Wikipedia and Vedabase corpus.", "labels": [], "entities": [{"text": "Digital Corpus of Sanskrit (DCS) 1", "start_pos": 39, "end_pos": 73, "type": "DATASET", "confidence": 0.8804965987801552}, {"text": "Vedabase corpus", "start_pos": 119, "end_pos": 134, "type": "DATASET", "confidence": 0.8360651135444641}]}, {"text": "The number of words in each corpus are 3.8 M, 0.7 M, and 0.2 M, respectively.", "labels": [], "entities": []}, {"text": "DCS and Vedabase are segmented, but the Wikipedia data is unsegmented.", "labels": [], "entities": [{"text": "DCS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.942101776599884}, {"text": "Vedabase", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.9276672005653381}, {"text": "Wikipedia data", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9743945002555847}]}, {"text": "We have used this corpus to learn word embedding features.", "labels": [], "entities": []}, {"text": "Most of the data in our corpus is in the form of poetry.", "labels": [], "entities": []}, {"text": "presents a few statistics regarding the corpus utilized.", "labels": [], "entities": []}, {"text": "The labelled dataset for the compound classification task with a segmented pair of components is obtained from the department of Sanskrit studies, UoHyd 2 . These compounds are part of ancient texts, namely, Bhagavadg\u00af \u0131t\u00af a, Carakasam . h\u00af \u0131ta, etc.", "labels": [], "entities": [{"text": "compound classification task", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7862937847773234}, {"text": "UoHyd", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.9176486730575562}]}, {"text": "We have used the same experimental setting as for the classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.9343966543674469}]}, {"text": "The dataset for the compound classification task has more than 32,000 sandhi splitted compounds with labels.", "labels": [], "entities": [{"text": "compound classification task", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.8576729893684387}]}, {"text": "There are four broad classes, namely, Avyay\u00af \u0131bh\u00af ava, Tatpurus . a, Bahuvr\u00af \u0131hi and Dvandva.", "labels": [], "entities": []}, {"text": "More than 75% data points were from Tatpurus . a class, down-sampled it to 4,000, which takes it close to the count of the second most highly populated class Bahuvr\u00af \u0131hi.", "labels": [], "entities": [{"text": "Tatpurus . a class", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.9375584870576859}]}, {"text": "Avyay\u00af \u0131bh\u00af ava class is highly skewed, 5% of the Bahuvr\u00af \u0131hi class.", "labels": [], "entities": []}, {"text": "After down-sampling, number of compounds are 239 in Avyay\u00af \u0131bh\u00af ava, 4,271 in Bahuvr\u00af \u0131hi, 1,176 in Dvandva, and 4,266 in Tatpurus . a. Out of 9,952 datapoints, 7,957 were kept for training and remaining for testing.", "labels": [], "entities": []}, {"text": "We have created development (dev) dataset for hyperparameter tuning, from 20 % stratified sampling of the training data.", "labels": [], "entities": [{"text": "hyperparameter tuning", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.8447865545749664}]}, {"text": "We have not used test dataset in any part of training or hyperparameter tuning.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Next to the embedding layer, a drop-out layer with drop-out rate 0.2 is used to avoid  over-fitting (", "labels": [], "entities": []}, {"text": " Table 1: MLP architecture used for different embeddings. [a x b] indicates that there are total  'a' segments of compound and dimension of each segment is 'b'. For instance, for", "labels": [], "entities": []}, {"text": " Table 2: CNN architecture used for different embeddings. For embedding layer, same conven- tion is used. For charCNN, 25 segments correspond to the max number of characters in the  compound, and 1014 dimensional embedding is used for each of these.", "labels": [], "entities": []}, {"text": " Table 3: LSTM architecture used for different embeddings.", "labels": [], "entities": []}, {"text": " Table 4: Evaluation measures are accuracy (A), macro precision (P), macro recall (R) and  macro F-score (F). Results reported on the test data are averaged over 5 runs. '+' sign indicates  end-to-end training integrated with classifier.", "labels": [], "entities": [{"text": "accuracy (A)", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9329688847064972}, {"text": "precision (P)", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.9182815551757812}, {"text": "recall (R)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9543780088424683}, {"text": "F-score (F)", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9454818964004517}]}, {"text": " Table 5: Evaluation measures are accuracy (A), macro precision (P), macro recall (R) and macro  F-score (F). Results reported on test data in table are averaged over 5 runs. '+' sign indicates  end-to-end training integrated with classifier.", "labels": [], "entities": [{"text": "accuracy (A)", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9328950345516205}, {"text": "precision (P)", "start_pos": 54, "end_pos": 67, "type": "METRIC", "confidence": 0.918490931391716}, {"text": "recall (R)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9544253796339035}, {"text": "F-score (F)", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9452270716428757}]}]}