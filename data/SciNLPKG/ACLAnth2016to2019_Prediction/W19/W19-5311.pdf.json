{"title": [{"text": "The TALP-UPC Machine Translation Systems for WMT19 News Translation Task: Pivoting Techniques for Low Resource MT", "labels": [], "entities": [{"text": "TALP-UPC Machine Translation", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.619980255762736}, {"text": "WMT19 News Translation", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.7771701614061991}]}], "abstractContent": [{"text": "In this article, we describe the TALP-UPC research group participation in the WMT19 news translation shared task for Kazakh-English.", "labels": [], "entities": [{"text": "WMT19 news translation shared task", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7877818465232849}]}, {"text": "Given the low amount of parallel training data, we resort to using Russian as pivot language, training subword-based statistical translation systems for Russian-Kazakh and Russian-English that were then used to create two synthetic pseudo-parallel corpora for Kazakh-English and English-Kazakh respectively.", "labels": [], "entities": []}, {"text": "Finally, a self-attention model based on the decoder part of the Transformer architecture was trained on the two pseudo-parallel corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "Attention-based models like the Transformer architecture ( or the Dynamic Convolution architecture ( are currently the dominant approaches for Machine Translation (MT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 143, "end_pos": 167, "type": "TASK", "confidence": 0.8536973714828491}]}, {"text": "Nevertheless, these architectures offer best results when trained on large training corpora.", "labels": [], "entities": []}, {"text": "When faced with a low-resource scenario, other supporting techniques are needed in order to obtain good translation results.", "labels": [], "entities": [{"text": "translation", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.9593786001205444}]}, {"text": "In the WMT19 news translation shared task, two lowresourced language pairs where proposed, namely Gujarati-English and In this report, we describe the participation of the TALP Research Group at Universitat Polit\u00e8cnica de Catalunya (UPC) at the WMT19 news translation shared task () in Kazakh\u2192English and English\u2192Kazakh translation directions.", "labels": [], "entities": [{"text": "WMT19 news translation shared task", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.7092973828315735}, {"text": "WMT19 news translation shared task", "start_pos": 245, "end_pos": 279, "type": "TASK", "confidence": 0.7666260600090027}]}, {"text": "The amount of available parallel KazakhEnglish data is very low.", "labels": [], "entities": []}, {"text": "In order to overcome this problem in the frame of the shared task, we made use of Russian as an pivot language.", "labels": [], "entities": []}, {"text": "This way, we used English-Russian and Kazakh-Russian data to train intermediate translation systems that we then used to create synthetic pseudo-parallel KazakhEnglish data.", "labels": [], "entities": []}, {"text": "This data enabled us to train the final Kazakh-English translation systems.", "labels": [], "entities": [{"text": "Kazakh-English translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.8007272779941559}]}, {"text": "The source code used for the data download, data preparation and training of the pivot and final systems is available at https://github.", "labels": [], "entities": [{"text": "data preparation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.7125681042671204}]}, {"text": "com/noe/wmt19-news-lowres.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to assess the translation quality of the systems, we computed the BLEU score () over the respective held out test sets.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.983675628900528}]}, {"text": "As there is not much literature of current NMT approaches being applied to English-Kazakh, we prepared different baselines to gauge the range of BLEU values to expect: \u2022 Rule-based machine translation system (RBMT): we used the Apertium system, which is based on transfer rules distilled from linguistic knowledge.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9988870024681091}, {"text": "Rule-based machine translation system (RBMT)", "start_pos": 170, "end_pos": 214, "type": "TASK", "confidence": 0.6381217028413501}]}, {"text": "Using the BLEU score to compare an RBMT system with data-driven systems is not fair (see \u00a78.2.7) but we included it to have a broader picture.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9707659482955933}]}, {"text": "We used the fairseq () implementation with the same hyperparameters as the IWSLT model, namely an embedding dimensionality of 512, 6 layers of attention, 4 attention heads and 1024 for the feedwordward expansion dimensionality.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 75, "end_pos": 80, "type": "DATASET", "confidence": 0.8531999588012695}]}, {"text": "The translation quality BLEU scores of the aforedescribed baselines were very low, as shown in table 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.979563295841217}]}, {"text": "In order to evaluate the pivot translation systems described in section 5.1, we also measured the BLEU scores in the respective held out test sets, obtaining 36.05 BLEU for the Russian\u2192English system and 21.06 for the Russian\u2192Kazakh system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9996793270111084}, {"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9989821314811707}]}, {"text": "With these pivot systems, we created two pseudo-parallel synthetic corpora, merged them with the parallel data and trained a self-attention NMT model that obtained BLEU scores one order of magnitude above the chosen baselines, as shown in table 4.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9987037181854248}]}, {"text": "When we tested the final Kazakh\u2192English system on the shared task test set, we identified several sentences that remained completely in Cyrillic script.", "labels": [], "entities": [{"text": "shared task test set", "start_pos": 54, "end_pos": 74, "type": "DATASET", "confidence": 0.7267205715179443}]}, {"text": "In order to mitigate this problem, we trained a SMT system on the augmented KazakhEnglish data and used it for the sentences that had a large percentage of Cyrillic characters.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.990571141242981}]}, {"text": "This lead to a mere 0.1 increase in the case-insensitive BLEU score and no change for the uncased one.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.976576954126358}]}], "tableCaptions": [{"text": " Table 1: Summary statistics of the Kazakh-English  training data.", "labels": [], "entities": [{"text": "Kazakh-English  training data", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.7069079081217448}]}, {"text": " Table 2: Summary statistics of the English-Russian  training data.", "labels": [], "entities": [{"text": "English-Russian  training data", "start_pos": 36, "end_pos": 66, "type": "DATASET", "confidence": 0.6487545569737753}]}, {"text": " Table 3: Summary statistics of the Russian-Kazakh  training data.", "labels": [], "entities": [{"text": "Russian-Kazakh  training data", "start_pos": 36, "end_pos": 65, "type": "DATASET", "confidence": 0.5608396828174591}]}, {"text": " Table 4: BLEU scores (cased) of the Rule-based baseline (RBMT), the Moses system trained on the parallel  Kazakh-English data with word-level tokenization (SMT(w)), the Moses system trained on the parallel Kazakh- English data with subword-level tokenization (SMT(sw)), the NMT system trained on the parallel Kazakh-English  data, and the final systems trained on the augmented pseudo-parallel corpus data (NMT pseudo-p.)", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994332194328308}]}]}