{"title": [], "abstractContent": [{"text": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (De-vlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months.", "labels": [], "entities": [{"text": "BERT", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9958284497261047}]}, {"text": "However , these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist.", "labels": [], "entities": [{"text": "BERT", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.7771230936050415}]}, {"text": "In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically.", "labels": [], "entities": [{"text": "BERT", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9826059341430664}]}, {"text": "We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings.", "labels": [], "entities": []}, {"text": "These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing (NLP) has been shaken in recent months with the dramatic successes enabled by transfer learning and contextual word embedding models, such as ELMo (,, and BERT ().", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7951234777768453}, {"text": "BERT", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9886749982833862}]}, {"text": "These models have been primarily explored for general domain text, and, recently, biomedical text with BioBERT (.", "labels": [], "entities": []}, {"text": "However, clinical narratives (e.g., physician notes) have known differences in linguistic characteristics from both general text and non-clinical biomedical text, motivating the need for specialized clinical BERT models.", "labels": [], "entities": []}, {"text": "In this work, we build and publicly release exactly such an embedding model.", "labels": [], "entities": []}, {"text": "Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike.", "labels": [], "entities": [{"text": "BERT", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.886390209197998}]}, {"text": "In particular, we make the following contributions:", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Task dataset evaluation metrics, output dimen- sionality, and train/dev/test dataset sizes (in number of  sentences). Exact F1 requires that the text span and la- bel be an exact match to be considered correct.", "labels": [], "entities": [{"text": "F1", "start_pos": 134, "end_pos": 136, "type": "METRIC", "confidence": 0.5041481852531433}]}, {"text": " Table 2: Accuracy (MedNLI) and Exact F1 score (i2b2) across various clinical NLP tasks.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993802309036255}, {"text": "Exact F1 score (i2b2)", "start_pos": 32, "end_pos": 53, "type": "METRIC", "confidence": 0.8927513062953949}]}]}