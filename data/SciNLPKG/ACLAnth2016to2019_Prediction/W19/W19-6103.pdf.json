{"title": [{"text": "Lexical Resources for Low-Resource PoS Tagging in Neural Times", "labels": [], "entities": [{"text": "PoS Tagging in Neural Times", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.687321525812149}]}], "abstractContent": [{"text": "More and more evidence is appearing that integrating symbolic lexical knowledge into neural models aids learning.", "labels": [], "entities": []}, {"text": "This contrasts the widely-held belief that neural networks largely learn their own feature representations.", "labels": [], "entities": []}, {"text": "For example, recent work has shown benefits of integrating lexicons to aid cross-lingual part-of-speech (PoS).", "labels": [], "entities": [{"text": "cross-lingual part-of-speech (PoS)", "start_pos": 75, "end_pos": 109, "type": "TASK", "confidence": 0.5868283748626709}]}, {"text": "However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.", "labels": [], "entities": [{"text": "coverage", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9462197422981262}]}, {"text": "This paper seeks to fill this gap by providing a thorough analysis on the contributions of lexical resources for cross-lingual PoS tagging in neural times.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.850790798664093}]}], "introductionContent": [{"text": "In natural language processing, the deep learning revolution has shifted the focus from conventional hand-crafted symbolic representations to dense inputs, which are adequate representations learned automatically from corpora.", "labels": [], "entities": []}, {"text": "However, particularly when working with low-resource languages, small amounts of symbolic lexical resources such as user-generated lexicons are often available even when gold-standard corpora are not.", "labels": [], "entities": []}, {"text": "Recent work has shown benefits of combining conventional lexical information into neural cross-lingual part-ofspeech (PoS) tagging.", "labels": [], "entities": [{"text": "neural cross-lingual part-ofspeech (PoS) tagging", "start_pos": 82, "end_pos": 130, "type": "TASK", "confidence": 0.6361791491508484}]}, {"text": "However, little is known on how complementary such additional information is, and to what extent improvements depend on the coverage and quality of these external resources.", "labels": [], "entities": [{"text": "coverage", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9462197422981262}]}, {"text": "The contribution of this paper is in the analysis of the contributions of models' components (tagger transfer through annotation projection vs. the contribution of encoding lexical and morphosyntactic resources).", "labels": [], "entities": [{"text": "tagger transfer through annotation projection", "start_pos": 94, "end_pos": 139, "type": "TASK", "confidence": 0.7910850048065186}]}, {"text": "We seek to understand under which conditions a low-resource neural tagger benefits from external lexical knowledge.", "labels": [], "entities": []}, {"text": "In particular: a) we evaluate the neural tagger across a total of 20+ languages, proposing a novel baseline which uses retrofitting; b) we investigate the reliance on dictionary size and properties; c) we analyze model-internal representations via a probing task to investigate to what extent model-internal representations capture morphosyntactic information.", "labels": [], "entities": []}, {"text": "Our experiments confirm the synergetic effect between a neural tagger and symbolic linguistic knowledge.", "labels": [], "entities": []}, {"text": "Moreover, our analysis shows that the composition of the dictionary plays a more important role than its coverage.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the baselines, the data and the tagger hyperparameters.", "labels": [], "entities": []}, {"text": "Data We use the 12 Universal PoS tags ().", "labels": [], "entities": [{"text": "Universal PoS tags", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.5444774975379308}]}, {"text": "The set of languages is motivated by accessibility to embeddings and dictionaries.", "labels": [], "entities": []}, {"text": "We here focus on 21 dev sets of the Universal Dependencies 2.1, test set results are reported by showing that DSDS provides a viable alternative.", "labels": [], "entities": [{"text": "Universal Dependencies 2.1", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.8038965662320455}, {"text": "DSDS", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.7652797698974609}]}, {"text": "Annotation projection To build the taggers for new languages, we resort to annotation projection following.", "labels": [], "entities": []}, {"text": "In particular, they employ the approach by Agi\u00b4c, where labels are projected from multiple sources to multiple targets and then decoded through weighted majority voting with word alignment probabilities and source PoS tagger confidences.", "labels": [], "entities": []}, {"text": "The wide-coverage Watchtower corpus (WTC) by Agi\u00b4c is used, where 5k instances are selected via data selection by alignment coverage following.", "labels": [], "entities": [{"text": "wide-coverage Watchtower corpus (WTC)", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.7796252171198527}]}, {"text": "Baselines We compare to the following alternatives: type-constraint Wiktionary supervision () and retrofitting initialization.", "labels": [], "entities": []}, {"text": "Hyperparameters We use the same setup as Plank and Agi\u00b4cAgi\u00b4c (2018), i.e., 10 epochs, word dropout rate (p=.25) and l=40-dimensional lexicon embeddings for DSDS, except for downscaling the hidden dimensionality of the character representations from 100 to 32 dimensions.", "labels": [], "entities": [{"text": "word dropout rate", "start_pos": 87, "end_pos": 104, "type": "METRIC", "confidence": 0.6872387131055196}]}, {"text": "This ensures that our probing tasks always get the same input dimensionality: 64 (2x32) dimensions for cw, which is the same dimension as the off-theshelf word embeddings.", "labels": [], "entities": []}, {"text": "Language-specific hyperparameters could lead to optimized models for each language.", "labels": [], "entities": []}, {"text": "However, we use identical settings for each language which worked well and is less expensive, following.", "labels": [], "entities": []}, {"text": "For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.", "labels": [], "entities": [{"text": "mean", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9588707685470581}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.8715325593948364}]}, {"text": "We use the off-the-shelf Polyglot word embeddings (Al-Rfou et al., 2013).", "labels": [], "entities": []}, {"text": "Word embedding initialization provides a consistent and considerable boost in this cross-lingual setup, up to 10% absolute improvements across 21 languages when only 500 projected training instances are available.", "labels": [], "entities": [{"text": "Word embedding initialization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5951348145802816}, {"text": "absolute", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9589938521385193}]}, {"text": "Note that we em- pirically find it to be best to not update the word embeddings in this noisy training setup, as that results in better performance, see Section 4.4.", "labels": [], "entities": []}, {"text": "presents our replication results, i.e., tagging accuracy for the 21 individual languages, with means overall languages and language families (for which at least two languages are available).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.971493124961853}]}], "tableCaptions": [{"text": " Table 1: Replication of results on the dev sets. 5k:  model trained on only projected data; TC W : type  constraints; Retro: retrofitted initialization.", "labels": [], "entities": [{"text": "TC W", "start_pos": 93, "end_pos": 97, "type": "METRIC", "confidence": 0.889555811882019}]}]}