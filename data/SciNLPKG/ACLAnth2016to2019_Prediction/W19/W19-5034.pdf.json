{"title": [{"text": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing", "labels": [], "entities": [{"text": "Biomedical Natural Language Processing", "start_pos": 37, "end_pos": 75, "type": "TASK", "confidence": 0.5933630019426346}]}], "abstractContent": [{"text": "Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift.", "labels": [], "entities": []}, {"text": "Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6751271883646647}]}, {"text": "This paper describes scis-paCy, anew Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library.", "labels": [], "entities": [{"text": "biomedical/scientific text processing", "start_pos": 77, "end_pos": 114, "type": "TASK", "confidence": 0.64114910364151}]}, {"text": "We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets.", "labels": [], "entities": []}, {"text": "Models and code are available at https:// allenai.github.io/scispacy/.", "labels": [], "entities": []}], "introductionContent": [{"text": "The publication rate in the medical and biomedical sciences is growing at an exponential rate).", "labels": [], "entities": []}, {"text": "The information overload problem is widespread across academia, but is particularly apparent in the biomedical sciences, where individual papers may contain specific discoveries relating to a dizzying variety of genes, drugs, and proteins.", "labels": [], "entities": []}, {"text": "In order to cope with the sheer volume of new scientific knowledge, there have been many attempts to automate the process of extracting entities, relations, protein interactions and other structured knowledge from scientific papers (.", "labels": [], "entities": []}, {"text": "Although there exists a wealth of tools for processing biomedical text, many focus primarily on named entity recognition and disambiguation., the two most widely used and supported tools for biomedical text processing, support entity linking with negation detection and acronym resolution.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7010600368181864}, {"text": "biomedical text processing", "start_pos": 191, "end_pos": 217, "type": "TASK", "confidence": 0.6427537500858307}, {"text": "negation detection", "start_pos": 247, "end_pos": 265, "type": "TASK", "confidence": 0.9462706446647644}, {"text": "acronym resolution", "start_pos": 270, "end_pos": 288, "type": "TASK", "confidence": 0.845007061958313}]}, {"text": "However, tools which cover more classical natural language processing (NLP) tasks such as the GENIA tagger ( ), or phrase structure parsers such as those presented in typically do not make use of new research innovations such as word representations or neural networks.", "labels": [], "entities": [{"text": "GENIA tagger", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.6682402193546295}, {"text": "phrase structure parsers", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.6858223378658295}]}, {"text": "In this paper, we introduce scispaCy, a specialized NLP library for processing biomedical texts which builds on the robust spaCy library, and document its performance relative to state of the art models for part of speech (POS) tagging, dependency parsing, named entity recognition (NER) and sentence segmentation.", "labels": [], "entities": [{"text": "part of speech (POS) tagging", "start_pos": 207, "end_pos": 235, "type": "TASK", "confidence": 0.6277761076177869}, {"text": "dependency parsing", "start_pos": 237, "end_pos": 255, "type": "TASK", "confidence": 0.7836464941501617}, {"text": "named entity recognition (NER)", "start_pos": 257, "end_pos": 287, "type": "TASK", "confidence": 0.7842058936754862}, {"text": "sentence segmentation", "start_pos": 292, "end_pos": 313, "type": "TASK", "confidence": 0.7373107820749283}]}, {"text": "Specifically, we: \u2022 Release a reformatted version of the GENIA 1.0 () corpus converted into Universal Dependencies v1.0 and aligned with the original text from the PubMed abstracts.: Vocabulary statistics for the two core packages in scispaCy.", "labels": [], "entities": [{"text": "GENIA 1.0 () corpus", "start_pos": 57, "end_pos": 76, "type": "DATASET", "confidence": 0.8715585768222809}, {"text": "PubMed abstracts.", "start_pos": 164, "end_pos": 181, "type": "DATASET", "confidence": 0.9356513917446136}]}, {"text": "\u2022 Benchmark 9 named entity recognition models for more specific entity extraction applications demonstrating competitive performance when compared to strong baselines.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6837229579687119}, {"text": "entity extraction", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7834982573986053}]}, {"text": "\u2022 Release and evaluate two fast and convenient pipelines for biomedical text, which include tokenization, part of speech tagging, dependency parsing and named entity recognition.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 92, "end_pos": 104, "type": "TASK", "confidence": 0.9654611349105835}, {"text": "part of speech tagging", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.6135390102863312}, {"text": "dependency parsing", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.8013177812099457}, {"text": "named entity recognition", "start_pos": 153, "end_pos": 177, "type": "TASK", "confidence": 0.6416963338851929}]}], "datasetContent": [{"text": "The main NER model in both released packages in scispaCy is trained on the mention spans in the MedMentions dataset (.", "labels": [], "entities": [{"text": "MedMentions dataset", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.9809777438640594}]}, {"text": "Since the MedMentions dataset was originally designed for entity linking, this model recognizes a wide variety of entity types, as well as non-standard syntactic phrases such as verbs and modifiers, but the model does not predict the entity type.", "labels": [], "entities": [{"text": "MedMentions dataset", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.942082941532135}, {"text": "entity linking", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.7334810793399811}]}, {"text": "In order to provide for users with more specific requirements around entity types, we release four additional packages en ner {bc5cdr|craft |jnlpba|bionlp13cg} md with finer-grained NER models trained on BC5CDR (for chemicals and diseases; , CRAFT (for cell types, chemicals, proteins, genes; Bada et al., 2011), JNLPBA (for cell lines, cell types, DNAs, RNAs, proteins;) and BioNLP13CG (for cancer genetics;, respectively.", "labels": [], "entities": [{"text": "BioNLP13CG", "start_pos": 376, "end_pos": 386, "type": "METRIC", "confidence": 0.7014129757881165}]}, {"text": "As NER is a key task for other biomedical text processing tasks, we conduct a through evaluation of the suitability of scispaCy to provide baseline performance across a wide variety of datasets.", "labels": [], "entities": [{"text": "NER", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9450039267539978}]}, {"text": "In particular, we retrain the spaCy NER model on each of the four datasets mentioned earlier (BC5CDR, CRAFT, JNLPBA, BioNLP13CG) as well as five more datasets in: AnatEM, BC2GM, BC4CHEMD, Linnaeus, NCBI-Disease.", "labels": [], "entities": [{"text": "BioNLP13CG", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.8501030802726746}, {"text": "Linnaeus", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9465370774269104}, {"text": "NCBI-Disease", "start_pos": 198, "end_pos": 210, "type": "DATASET", "confidence": 0.8925381302833557}]}, {"text": "These datasets cover a wide variety of entity types required by different biomedical domains, including cancer genetics, disease-drug interactions, pathway analysis and trial population extraction.", "labels": [], "entities": [{"text": "pathway analysis", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.7694243788719177}, {"text": "trial population extraction", "start_pos": 169, "end_pos": 196, "type": "TASK", "confidence": 0.7300531466801962}]}, {"text": "Additionally, they vary considerably in size and number of entities.", "labels": [], "entities": []}, {"text": "For example, BC4CHEMD ( provides a thorough comparison of the scispaCy NER models compared to a variety of models.", "labels": [], "entities": [{"text": "BC4CHEMD", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8297605514526367}]}, {"text": "In particular, we compare the models to strong baselines which do not consider the use of 1) multi-task learning across multiple datasets and 2) semi-supervised learning via large pretrained language models.", "labels": [], "entities": []}, {"text": "Overall, we find that the scispaCy models are competitive baselines for 5 of the 9 datasets.", "labels": [], "entities": []}, {"text": "Additionally, in we evaluate the recall of the pipeline mention detector available in both scispaCy models (trained on the MedMentions dataset) against all 9 specialised NER datasets.", "labels": [], "entities": [{"text": "recall", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9995816349983215}, {"text": "MedMentions dataset", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.991702139377594}, {"text": "NER datasets", "start_pos": 170, "end_pos": 182, "type": "DATASET", "confidence": 0.7968065738677979}]}, {"text": "Overall, we observe a modest drop in average recall when compared directly to the MedMentions results in", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9688441157341003}]}], "tableCaptions": [{"text": " Table 1: Vocabulary statistics for the two core packages  in scispaCy.", "labels": [], "entities": []}, {"text": " Table 2: Wall clock comparison of different publicly  available biomedical NLP pipelines. All experiments  run on a single machine with 12 Intel(R) Core(TM)  i7-6850K CPU @ 3.60GHz and 62GB RAM. For the  Biaffine Parser, a pre-compiled Tensorflow binary with  support for AVX2 instructions was used in a good faith  attempt to optimize the implementation. Dynet does  support the Intel MKL, but requires compilation from  scratch and as such, does not represent an \"off the  shelf\" system. TF is short for Tensorflow.", "labels": [], "entities": []}, {"text": " Table 3: Part of Speech tagging results on the GENIA  Test set.", "labels": [], "entities": [{"text": "Speech tagging", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7999618649482727}, {"text": "GENIA  Test set", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.9819960594177246}]}, {"text": " Table 4: Dependency Parsing results on the GENIA 1.0  corpus converted to dependencies using the Stanford  Universal Dependency Converter. We additionally pro- vide evaluations using Stanford Dependencies(SD) in  order for comparison relative to the results reported in  (Nguyen and Verspoor, 2018).", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8098248839378357}, {"text": "GENIA 1.0  corpus", "start_pos": 44, "end_pos": 61, "type": "DATASET", "confidence": 0.9461516539255778}, {"text": "Stanford  Universal Dependency Converter", "start_pos": 98, "end_pos": 138, "type": "DATASET", "confidence": 0.8815634101629257}]}, {"text": " Table 6: Recall on the test sets of 9 specialist NER  datasets, when the base mention detector is trained on  MedMentions. The base mention detector is available  in both en core sci sm and en core sci md models.", "labels": [], "entities": [{"text": "NER  datasets", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.7709591388702393}, {"text": "MedMentions", "start_pos": 111, "end_pos": 122, "type": "DATASET", "confidence": 0.9595926403999329}]}, {"text": " Table 7: Performance of the base mention detector on  the MedMentions Corpus.", "labels": [], "entities": [{"text": "MedMentions Corpus", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9742883741855621}]}, {"text": " Table 5: Test F1 Measure on NER for the small and medium scispaCy models compared to a variety of  strong baselines and state of the art models. The Baseline and SOTA (State of the Art) columns include  only single models which do not use additional resources, such as language models, or additional sources  of supervision, such as multi-task learning. + Resources allows any type of supervision or pretraining. All  scispaCy results are the mean of 5 random seeds.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9895658493041992}, {"text": "NER", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.8817878365516663}]}, {"text": " Table 8: Sentence segmentation performance for the  core spaCy and scispaCy models. cs = custom rule  based sentence segmenter and ct = custom rule based  tokenizer, both designed explicitly to handle citations  and common patterns in biomedical text.", "labels": [], "entities": [{"text": "Sentence segmentation", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.9336643218994141}]}]}