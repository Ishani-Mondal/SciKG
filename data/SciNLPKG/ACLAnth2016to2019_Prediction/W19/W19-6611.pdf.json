{"title": [{"text": "An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation Architectures", "labels": [], "entities": [{"text": "Intrinsic Nearest Neighbor Analysis of Neural Machine Translation Architectures", "start_pos": 3, "end_pos": 82, "type": "TASK", "confidence": 0.6261949770980411}]}], "abstractContent": [{"text": "Earlier approaches indirectly studied the information captured by the hidden states of recurrent and non-recurrent neural machine translation models by feeding them into different classifiers.", "labels": [], "entities": []}, {"text": "In this paper, we look at the encoder hidden states of both transformer and recurrent machine translation models from the nearest neighbors perspective.", "labels": [], "entities": []}, {"text": "We investigate to what extent the nearest neighbors share information with the underlying word embeddings as well as related WordNet entries.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.9262965321540833}]}, {"text": "Additionally , we study the underlying syntactic structure of the nearest neighbors to shed light on the role of syntactic similarities in bringing the neighbors together.", "labels": [], "entities": []}, {"text": "We compare transformer and recurrent models in a more intrinsic way in terms of capturing lexical semantics and syntactic structures, in contrast to extrinsic approaches used by previous works.", "labels": [], "entities": []}, {"text": "In agreement with the ex-trinsic evaluations in the earlier works, our experimental results show that transformers are superior in capturing lexical semantics , but not necessarily better in capturing the underlying syntax.", "labels": [], "entities": []}, {"text": "Additionally, we show that the backward recurrent layer in a recurrent model learns more about the semantics of words, whereas the forward recurrent layer encodes more context.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) has achieved state-of-the-art performance for many language pairs (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.825665283203125}]}, {"text": "Additionally, it is straightforward to train an NMT system in an end-to-end fashion.", "labels": [], "entities": []}, {"text": "This has been made possible with an encoderdecoder architecture that encodes the source sentence into a distributed representation and then decodes this representation into a sentence in the target language.", "labels": [], "entities": []}, {"text": "While earlier work has investigated what information is captured by the attention mechanism of an NMT system, it is not exactly clear what linguistic information from the source sentence is encoded in the hidden distributed representation themselves.", "labels": [], "entities": []}, {"text": "Recently, some attempts have been made to shed some light on the information that is being encoded in the intermediate distributed representations.", "labels": [], "entities": []}, {"text": "Feeding the hidden states of the encoder of different seq2seq systems, including multiple NMT systems, as the input to different classifiers, aim to show what syntactic information is encoded in the hidden states.", "labels": [], "entities": []}, {"text": "They provide evidence that syntactic information such as the voice and tense of a sentence and the part-ofspeech (POS) tags of words are being learned with reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9830673336982727}]}, {"text": "They also provide evidence that more complex syntactic information such as the parse tree of a sentence is also learned, but with lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9957183003425598}]}, {"text": "follow the same approach as to conduct more analyses about how syntactic and morphological information are encoded in the hidden states of the encoder.", "labels": [], "entities": []}, {"text": "They carryout experiments for POS tagging and morphological tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9385599493980408}, {"text": "morphological tagging", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7719709873199463}]}, {"text": "They study the effect of different word representations, different layers of the encoder and target languages on the accuracy of their classifiers to reveal the impact of these variables on the amount of the syntactic information captured in the hidden states.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9976098537445068}]}, {"text": "Additionally, there are recent approaches that compare different state-of-the-art encoder-decoder architectures in terms of their capabilities to capture syntactic structures ( and lexical semantics (.", "labels": [], "entities": []}, {"text": "These works also use some extrinsic tasks to do the comparison.", "labels": [], "entities": []}, {"text": "use subject-verb agreement and logical inference tasks to compare recurrent models with transformers.", "labels": [], "entities": []}, {"text": "On the other hand, use subject-verb agreement and word sense disambiguation for comparing those architectures in terms of capturing syntax and lexical semantics respectively.", "labels": [], "entities": []}, {"text": "In addition to these tasks, compare recurrent models with transformers on a multilingual machine translation task.", "labels": [], "entities": [{"text": "multilingual machine translation task", "start_pos": 76, "end_pos": 113, "type": "TASK", "confidence": 0.759541779756546}]}, {"text": "Despite the approaches discussed above, attempts to study the hidden states more intrinsically are still missing.", "labels": [], "entities": []}, {"text": "For example, to the best of our knowledge, there is no work that studies the encoder hidden states from a nearest neighbor perspective to compare these distributed word representations with the underlying word embeddings.", "labels": [], "entities": []}, {"text": "It seems intuitive to assume that the hidden state of the encoder corresponding to an input word conveys more contextual information compared to the embedding of the input word itself.", "labels": [], "entities": []}, {"text": "But what type of information is captured and how does it differ from the word embeddings?", "labels": [], "entities": []}, {"text": "Furthermore, how different is the information captured by different architectures, especially recurrent vs selfattention architectures which use entirely different approaches to capture context?", "labels": [], "entities": []}, {"text": "In this paper, we choose to investigate the hidden states from a nearest neighbors perspective and try to show the similarities and differences between the hidden states and the word embeddings.", "labels": [], "entities": []}, {"text": "We collect statistics showing how much information from embeddings of the input words is preserved by the corresponding hidden states.", "labels": [], "entities": []}, {"text": "We also try to shed some light on the information encoded in the hidden states that goes beyond what is transferred from the word embeddings.", "labels": [], "entities": []}, {"text": "To this end, we analyze how much the nearest neighbors of words based on their hidden state representations are covered by direct relations in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9651556611061096}]}, {"text": "For our German side experiments, we use GermaNet (.", "labels": [], "entities": [{"text": "GermaNet", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8234035968780518}]}, {"text": "From now on, we use WordNet to refer to either WordNet or GermaNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 20, "end_pos": 27, "type": "DATASET", "confidence": 0.9689311981201172}, {"text": "WordNet", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.9699518084526062}]}, {"text": "This paper does not directly seek improvements to neural translation models, but to further our understanding of the inside behaviour of these models.", "labels": [], "entities": [{"text": "neural translation", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7247930020093918}]}, {"text": "It explains what information is learned in addition to what is already captured by embeddings.", "labels": [], "entities": []}, {"text": "This paper makes the following contributions: 1.", "labels": [], "entities": []}, {"text": "We provide interpretable representations of hidden states in NMT systems highlighting the differences between hidden state representations and word embeddings.", "labels": [], "entities": []}, {"text": "2. We compare transformer and recurrent models in a more intrinsic way in terms of capturing lexical semantics and syntactic structures.", "labels": [], "entities": []}, {"text": "3. We provide analyses of the behaviour of the hidden states for each direction layer and the concatenation of the states from the direction layers.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct our analysis using recurrent and transformer machine translation models.", "labels": [], "entities": [{"text": "transformer machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6553133328755697}]}, {"text": "Our recurrent model is a two-layer bidirectional recurrent model with Long Short-Term Memory (LSTM) units) and global attention ().", "labels": [], "entities": [{"text": "Long Short-Term Memory (LSTM)", "start_pos": 70, "end_pos": 99, "type": "METRIC", "confidence": 0.8049898793299993}]}, {"text": "The encoder consists of a two-layer unidirectional forward and a two-layer unidirectional backward pass.", "labels": [], "entities": []}, {"text": "The corresponding output representations from each direction are concatenated to form the encoder hidden state representation for each word.", "labels": [], "entities": []}, {"text": "A concatenation and down-projection of the last states of the encoder is used to initialize the first state of the decoder.", "labels": [], "entities": []}, {"text": "The decoder uses a two-layer unidirectional (forward) LSTM.", "labels": [], "entities": []}, {"text": "We use no residual connection in our recurrent model as they have been shown to result in performance drop if used on the encoder side of recurrent model ().", "labels": [], "entities": []}, {"text": "Our transformer model is a 6-layer transformer with multi-headed attention of 8 heads (.", "labels": [], "entities": []}, {"text": "We choose these settings to obtain competitive models with the relevant core components from each architecture.", "labels": [], "entities": []}, {"text": "We train our models for two directions, namely English-German and German-English, both of which use the WMT15 parallel training data.", "labels": [], "entities": [{"text": "WMT15 parallel training data", "start_pos": 104, "end_pos": 132, "type": "DATASET", "confidence": 0.8180765211582184}]}, {"text": "We exclude 100k randomly chosen sentence pairs which are used as our held-out data.", "labels": [], "entities": []}, {"text": "Our recurrent system has hidden state dimensions of the size of 1,024 (512 for each direction) and is trained using a batch size of 64 sentences.", "labels": [], "entities": []}, {"text": "The learning rate is set to 0.001 for the Adam optimizer () with a maximum gradient norm of 5.", "labels": [], "entities": []}, {"text": "A dropout rate of 0.3 has been used to avoid overfitting.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9402811229228973}]}, {"text": "Our transformer model has hidden state dimensions of 512 and a batch size of 4096 tokens and uses layer normalization (.", "labels": [], "entities": []}, {"text": "A learning rate of 2 changed under warmup strategy with 8000 warm-up steps is used for Adam optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.998 and = 10 \u22129 ().", "labels": [], "entities": []}, {"text": "The dropout rate is set to 0.1, and no gradient clipping is used.", "labels": [], "entities": [{"text": "dropout rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9246571958065033}]}, {"text": "The word embedding size of both models is 512.", "labels": [], "entities": []}, {"text": "We apply Byte-Pair Encoding (BPE) ( ) with 32K merge operation.", "labels": [], "entities": [{"text": "Byte-Pair Encoding (BPE)", "start_pos": 9, "end_pos": 33, "type": "METRIC", "confidence": 0.6691702544689179}]}, {"text": "We train our models until convergence and then use the trained models to translate 100K sentences from a held-out dataset and log the hidden states for later use in our analyses.", "labels": [], "entities": []}, {"text": "The 100K held-out data is randomly chosen from the WMT15 parallel training data.", "labels": [], "entities": [{"text": "WMT15 parallel training data", "start_pos": 51, "end_pos": 79, "type": "DATASET", "confidence": 0.8647358864545822}]}, {"text": "The remaining of the WMT15 parallel training data is used as our training data.", "labels": [], "entities": [{"text": "WMT15 parallel training data", "start_pos": 21, "end_pos": 49, "type": "DATASET", "confidence": 0.8592422753572464}]}, {"text": "summarizes the performance of our experimental models in BLEU () on different standard test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9886661767959595}]}, {"text": "This is to make sure that the models are trustable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of our experimental systems in BLEU on WMT (Bojar et al., 2017) German-English and English-German  standard test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9379732608795166}, {"text": "WMT (Bojar et al., 2017) German-English and English-German  standard test sets", "start_pos": 61, "end_pos": 139, "type": "DATASET", "confidence": 0.732424259185791}]}, {"text": " Table 2: Percentage of the nearest neighbors of hidden states covered by the list of the nearest neighbors of embeddings.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9604655504226685}]}, {"text": " Table 3: Percentage of the nearest neighbors of hidden states covered by the list of the directly related words to the correspond- ing word of the hidden states in WordNet.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9501979947090149}, {"text": "WordNet", "start_pos": 165, "end_pos": 172, "type": "DATASET", "confidence": 0.9712226986885071}]}, {"text": " Table 4: Average parse tree similarity (PARSEVAL scores) between word occurrences and their nearest neighbors. Note  that the apparent identity of precision and recall values is due to rounding and the very close number of constituents in the  corresponding parse tree of words (gold parse trees) and the corresponding parse trees of their nearest neighbors (candidate  parse trees).", "labels": [], "entities": [{"text": "Average parse tree similarity (PARSEVAL scores)", "start_pos": 10, "end_pos": 57, "type": "METRIC", "confidence": 0.8470134511590004}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9979686141014099}, {"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9817982316017151}]}, {"text": " Table 5: Percentage of the nearest neighbors of hidden states,  from the forward and backward layers, that are covered by the  list of the nearest neighbors of embeddings and the list of the  directly related words in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 219, "end_pos": 226, "type": "DATASET", "confidence": 0.9729578495025635}]}]}