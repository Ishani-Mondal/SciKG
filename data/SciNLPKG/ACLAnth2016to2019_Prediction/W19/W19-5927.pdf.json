{"title": [{"text": "Zero-shot transfer for implicit discourse relation classification", "labels": [], "entities": []}], "abstractContent": [{"text": "Automatically classifying the relation between sentences in a discourse is a challenging task, in particular when there is no overt expression of the relation.", "labels": [], "entities": [{"text": "classifying the relation between sentences in a discourse", "start_pos": 14, "end_pos": 71, "type": "TASK", "confidence": 0.8293486461043358}]}, {"text": "It becomes even more challenging by the fact that annotated training data exists only fora small number of languages, such as English and Chinese.", "labels": [], "entities": []}, {"text": "We present anew system using zero-shot transfer learning for implicit discourse relation classification, where the only resource used for the target language is unannotated parallel text.", "labels": [], "entities": [{"text": "implicit discourse relation classification", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.6435190588235855}]}, {"text": "This system is evaluated on the discourse-annotated TED-MDB parallel corpus, where it obtains good results for all seven languages using only En-glish training data.", "labels": [], "entities": [{"text": "TED-MDB parallel corpus", "start_pos": 52, "end_pos": 75, "type": "DATASET", "confidence": 0.8357125520706177}]}], "introductionContent": [{"text": "The difference between a set of randomly selected sentences and a discourse lies in coherence.", "labels": [], "entities": []}, {"text": "Among other attempts at defining the elusive nature of coherence, one way is to look at the meaning conveyed between the adjacent pair of sentences.", "labels": [], "entities": []}, {"text": "In the current study, we follow the Penn Discourse Treebank (PDTB) framework which regards abstract objects as the units of discourse and views the text as a collection of discourse level predicates, each taking two arguments.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB) framework", "start_pos": 36, "end_pos": 76, "type": "DATASET", "confidence": 0.9455827219145638}]}, {"text": "Such predicates, called discourse connectives, may (Ex. 1) or may not (Ex. 2) be represented in the surface form: 1.", "labels": [], "entities": []}, {"text": "Because the drought reduced U.S. stockpiles, they have more than enough storage space for their new crop, and that permits them to wait for prices to rise.", "labels": [], "entities": []}, {"text": "2. But a few funds have taken other defensive steps.", "labels": [], "entities": []}, {"text": "Some have raised their cash positions to record levels.", "labels": [], "entities": []}, {"text": "Implicit = BECAUSE High cash positions help buffer a fund when the market falls.", "labels": [], "entities": [{"text": "Implicit", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9773474335670471}, {"text": "BECAUSE", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9874810576438904}]}, {"text": "where italics represents the first and boldface the second argument to the underlined discourse connective.", "labels": [], "entities": []}, {"text": "The discourse relations which lack an overt discourse connective (Ex.", "labels": [], "entities": []}, {"text": "2) are referred as implicit discourse relations and are shown to be the most challenging part of the discourse parsing (e.g..", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7243896126747131}]}, {"text": "In this paper, we perform implicit discourse relation classification using three recent data sets annotated according to the same guidelines: Penn Discourse Treebank (PDTB) 3.0, the Turkish Discourse Bank (TDB), and the multilingual TED-MDB.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6792573432127634}, {"text": "Penn Discourse Treebank (PDTB) 3.0", "start_pos": 142, "end_pos": 176, "type": "DATASET", "confidence": 0.948756320135934}, {"text": "Turkish Discourse Bank (TDB)", "start_pos": 182, "end_pos": 210, "type": "DATASET", "confidence": 0.9083253343900045}]}, {"text": "To the best of our knowledge, multilingual training and zero-shot transfer has not previously been investigated for this problem.", "labels": [], "entities": [{"text": "zero-shot transfer", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8570228219032288}]}, {"text": "The results suggest that an implicit discourse relation classifier can transfer well across dissimilar languages, and that pooling training data from unrelated languages (English and Turkish) leads to significantly better performance for all languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We formulate the implicit relation classification as four \"one vs other\" binary classification task.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.6811791658401489}]}, {"text": "We follow the conventional setting of the first study () and split the PDTB 2.0 into training (sections 2-20), development (sections 0-1 and 23-24) and test sets (sections to have directly comparable results with the previous work.", "labels": [], "entities": [{"text": "PDTB 2.0", "start_pos": 71, "end_pos": 79, "type": "DATASET", "confidence": 0.8729066550731659}]}, {"text": "However, following the PDTB's original distinction but unlike some previous work, we distinguish Entity-based relations from implicit relations.", "labels": [], "entities": []}, {"text": "Each classifier is trained on an equal number of positive and negative instances where the negative instances are randomly selected in each epoch to have a better representation of the data during the training.", "labels": [], "entities": []}, {"text": "This model is evaluated on the PDTB 2.0 test set to confirm whether our model performs adequatly on same-language, same-domain data.", "labels": [], "entities": [{"text": "PDTB 2.0 test set", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.9216281324625015}]}, {"text": "These results are directly comparable to previous work.", "labels": [], "entities": []}, {"text": "We use d=100 in the experiments As TED-MDB is annotated according to the PDTB 3.0 framework, we train separate classifiers on PDTB 3.0 following the same convention as above.", "labels": [], "entities": [{"text": "PDTB 3.0 framework", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.8934443791707357}]}, {"text": "We test the trained models on all the implicit discourse relations in the TED-MDB corpus.", "labels": [], "entities": [{"text": "TED-MDB corpus", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.8285364210605621}]}, {"text": "The PDTB framework allows annotations to be labelled with more than one label.", "labels": [], "entities": [{"text": "PDTB framework", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8648625016212463}]}, {"text": "In such cases we only keep the first label, inline with previous studies (among others.", "labels": [], "entities": []}, {"text": "The argument vectors are kept fixed during the training, and we do not update the parameters of the LASER model.", "labels": [], "entities": [{"text": "LASER", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.7632962465286255}]}, {"text": "We use cross-entropy loss, and AdaGrad as the optimizer.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.8604482412338257}]}, {"text": "We evaluate using the model which achieved the highest F-score on the development set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 55, "end_pos": 62, "type": "METRIC", "confidence": 0.9988797307014465}]}, {"text": "As for the regularization, we use a dropout layer between the input and the hidden layer with a dropout probability of 0.3.", "labels": [], "entities": []}, {"text": "All models are run 100 times to estimate the variance due to random initialization and stochastic training.", "labels": [], "entities": []}, {"text": "All the models are implemented in PyTorch 3 . shows the same-language, same-domain performance of our system, in comparison to previous work.", "labels": [], "entities": []}, {"text": "All figures refer to PDTB 2.0 test set F-score, when trained on the PDBT 2.0 training set, and are directly comparable.", "labels": [], "entities": [{"text": "PDTB 2.0 test set", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9079926460981369}, {"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9117003679275513}, {"text": "PDBT 2.0 training set", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.9692480713129044}]}, {"text": "While our model does not achieve state-of-the-art performance in this setting, this experiment shows that it performs adequately for English, and provides a reasonable baseline for the zero-shot experiments presented in.", "labels": [], "entities": []}, {"text": "We also include a naive baseline system which always predicts TRUE and is evaluated on the respective (PDTB 2.0 or PDTB 3.0)   test set in our comparisons.", "labels": [], "entities": [{"text": "TRUE", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9824009537696838}, {"text": "PDTB 3.0)   test set", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.8638293623924256}]}, {"text": "In all zero-shot experiments, evaluation is performed on the available test data with PDTB 3.0 annotations: TED-MDB, and the PDTB 3.0 test set itself.", "labels": [], "entities": [{"text": "TED-MDB", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9079448580741882}, {"text": "PDTB 3.0 test set", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.876269057393074}]}, {"text": "Results in use PDTB 3.0 only for training, whereas presents the effect of having additional training data from Turkish (a language unrelated to English).", "labels": [], "entities": []}, {"text": "Pooling training data from different languages is possible since our model is language-agnostic.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of top level senses of the im- plicit discourse relations in PDTB 2.0 and PDTB 3.0  training, development and test sets: comp(arison),  cont(ingency), exp(ansion), temp(oral).", "labels": [], "entities": [{"text": "PDTB", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9094406962394714}]}, {"text": " Table 3: Comparison of the F scores (%) of binary classifiers on PDTB 2.0 test set. Left out scores refer to the  results where EntRel relations are also considered to be Expansion.", "labels": [], "entities": [{"text": "F scores", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.972429096698761}, {"text": "PDTB 2.0 test set", "start_pos": 66, "end_pos": 83, "type": "DATASET", "confidence": 0.9628182351589203}]}, {"text": " Table 4: F scores (%) when the model is trained only on PDTB 3.0. In the table, PDTB 3.0 refers to the test set of  the PDTB 3.0 corpus. The remaining rows refer to evaluations using TED-MDB.", "labels": [], "entities": [{"text": "F scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9800688326358795}, {"text": "PDTB 3.0 corpus", "start_pos": 121, "end_pos": 136, "type": "DATASET", "confidence": 0.7887757420539856}, {"text": "TED-MDB", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.8328094482421875}]}, {"text": " Table 5: Comparison of average F-scores (%) when the  model is trained on different training sets. Bold means  significantly higher F-score than the second highest  column (p < 0.001, Mann-Whitney U test).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9930688142776489}, {"text": "F-score", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9966289401054382}]}]}