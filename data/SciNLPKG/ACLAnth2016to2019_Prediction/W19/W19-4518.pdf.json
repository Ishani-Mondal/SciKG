{"title": [{"text": "Gradual Argumentation Evaluation for Stance Aggregation in Automated Fake News Detection", "labels": [], "entities": [{"text": "Stance Aggregation in Automated Fake News Detection", "start_pos": 37, "end_pos": 88, "type": "TASK", "confidence": 0.7261869992528643}]}], "abstractContent": [{"text": "Stance detection plays a pivot role in fake news detection.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9287350475788116}, {"text": "fake news detection", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7584757208824158}]}, {"text": "The task involves determining the point of view or stance-for or against-a text takes towards a claim.", "labels": [], "entities": []}, {"text": "One very important stage in employing stance detection for fake news detection is the aggregation of multiple stance labels from different text sources in order to compute a prediction for the veracity of a claim.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8953946530818939}, {"text": "fake news detection", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6627533137798309}]}, {"text": "Typically, aggregation is treated as a credibility-weighted average of stance predictions.", "labels": [], "entities": []}, {"text": "In this work, we take the novel approach of applying, for aggregation, a gradual argu-mentation semantics to bipolar argumentation frameworks mined using stance detection.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 154, "end_pos": 170, "type": "TASK", "confidence": 0.8734540343284607}]}, {"text": "Our empirical evaluation shows that our method results in more accurate veracity predictions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of fake news has existed from time immemorial.", "labels": [], "entities": []}, {"text": "But in recent times, both the rise of social media as the go-to platform for receiving news updates and a series of significant politic elections events, the results of which are speculated to have been influenced by misinformation, has culminated in the phrase being pushed to the forefront of our consciousness.", "labels": [], "entities": []}, {"text": "It is widely acknowledged (e.g., see) that fake news is an important problem, and that attention should be directed to tackle it.", "labels": [], "entities": []}, {"text": "Fake news is a particularly challenging problem, one that consists of a number of subproblems, and one for which many approaches have been proposed (e.g., see ().", "labels": [], "entities": [{"text": "Fake news", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9642207324504852}]}, {"text": "Generally fake news detection amounts to collating evidence and counter-evidence from various sources in order to make an assessment regarding the veracity of a given claim, e.g., as in the Fact Extraction and Verification (FEVER) shared task (  Veracity assessment is typically formulated as a 3-class problem where we aim to arrive at a value for the factuality of a claim, which is based on the stances of Texts 1, . .", "labels": [], "entities": [{"text": "fake news detection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6765726804733276}, {"text": "Fact Extraction and Verification (FEVER) shared task", "start_pos": 190, "end_pos": 242, "type": "TASK", "confidence": 0.7543523410956064}]}, {"text": ", N (see).", "labels": [], "entities": []}, {"text": "These texts could be headlines, articles, and even other claims.", "labels": [], "entities": []}, {"text": "One of the tasks underpinning the prediction of factuality is stance detection.", "labels": [], "entities": [{"text": "prediction of factuality", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.856318473815918}, {"text": "stance detection", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9261621534824371}]}, {"text": "It involves examining agreement expressed by a text in relation to a claim.", "labels": [], "entities": []}, {"text": "The text could be a headline, a topic () or a lengthier text fragment (.", "labels": [], "entities": []}, {"text": "Stance detection can bethought of as a two-part task: we first aim to determine if the text and claim are sufficiently close with respect to their subject matter, and then, once relatedness of the text and claim is established, we want to know whether the text takes a favourable or unfavourable view of the claim.", "labels": [], "entities": [{"text": "Stance detection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9689308106899261}]}, {"text": "The intuition behind the use of stance detection for fake news analysis is that the trustworthiness of a claim is strongly tied to the level of agreement expressed either for or against it in other texts, particularly the agreement or disagreement expressed by sources with high credibility.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7667041718959808}, {"text": "fake news analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6362385153770447}]}, {"text": "For that reason, we should be able to aggregate these disjoint stance valuations in order to arrive at a prediction for the veracity of the claim, as described by.", "labels": [], "entities": [{"text": "veracity", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9871916770935059}]}, {"text": "In this paper we draw inspiration from uses of relation-based argument mining) to generate and evaluate bipolar argumentation frameworks (BAFs)) in order to perform classification tasks (e.g., in, for deception detection).", "labels": [], "entities": [{"text": "relation-based argument mining", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6937333146731058}, {"text": "deception detection", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.937442272901535}]}, {"text": "In the same spirit, we propose and use a stance detection classifier to generate BAFs and evaluate arguments therein with the existing DF-QuAD gradual semantics () in order to assess veracity of news against evidence.", "labels": [], "entities": [{"text": "stance detection classifier", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8097255229949951}, {"text": "BAFs", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.6825283765792847}]}, {"text": "We show empirically, using a stance detection classifier built from the Fake News Challenge dataset ( and tested on the RumourEval dataset, that DF-QuAD performs competitively in comparison with a standard stance aggregation method using a credibility-weighted average of stance predictions.", "labels": [], "entities": [{"text": "stance detection classifier", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.863580067952474}, {"text": "Fake News Challenge dataset", "start_pos": 72, "end_pos": 99, "type": "DATASET", "confidence": 0.9722607284784317}, {"text": "RumourEval dataset", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9504193961620331}]}, {"text": "The aggregation method resulting from deploying DF-QuAD, unlike the standard aggregation method, considers also the dialectical relationships between different evidence and counterevidence texts in order to gauge the veracity of target claims.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two datasets are employed as part of this study: the Fake News Challenge dataset 2 , used to train the stance detection classifiers, and the RumourEval dataset 3 , which we adapt for the problem of fake news detection to evaluate our argumentation-based stance aggregation methods.", "labels": [], "entities": [{"text": "Fake News Challenge dataset", "start_pos": 53, "end_pos": 80, "type": "DATASET", "confidence": 0.8756022900342941}, {"text": "stance detection classifiers", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.8090746402740479}, {"text": "RumourEval dataset", "start_pos": 141, "end_pos": 159, "type": "DATASET", "confidence": 0.695055678486824}, {"text": "fake news detection", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.6704466342926025}]}, {"text": "Our methodology is shown in.", "labels": [], "entities": []}, {"text": "We train a number of stance detection classifiers on the FNC-1 dataset, the best of which we use to predict the labels for the RumourEval Task A dataset.", "labels": [], "entities": [{"text": "stance detection classifiers", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.8980301022529602}, {"text": "FNC-1 dataset", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.9838734567165375}, {"text": "RumourEval Task A dataset", "start_pos": 127, "end_pos": 152, "type": "DATASET", "confidence": 0.7388650327920914}]}, {"text": "We then perform stance aggregation on the predicted labels, in order to arrive at a veracity prediction.", "labels": [], "entities": []}, {"text": "We compare their veracity assessment performance against the gold standard labels from the RumourEval task B dataset.", "labels": [], "entities": [{"text": "veracity", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9741831421852112}, {"text": "RumourEval task B dataset", "start_pos": 91, "end_pos": 116, "type": "DATASET", "confidence": 0.6605890542268753}]}, {"text": "This allowed us to compare and evaluate the usefulness of the stance detection predictions.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9456652700901031}]}, {"text": "The reliability of these labels also enabled us to gauge the effectiveness of stance detection as a tool for veracity assessment.", "labels": [], "entities": [{"text": "stance detection", "start_pos": 78, "end_pos": 94, "type": "TASK", "confidence": 0.9275307059288025}, {"text": "veracity assessment", "start_pos": 109, "end_pos": 128, "type": "TASK", "confidence": 0.8095525205135345}]}, {"text": "In the remainder of this section, first we describe the methods we employ for stance classification and then our stance aggregation methods.", "labels": [], "entities": [{"text": "stance classification", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.9268264770507812}, {"text": "stance aggregation", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8130109012126923}]}, {"text": "We developed our own stance detection classifiers using gradient boosting as well as (three forms of) neural networks, of which we selected two (LSTM and BiLSTM) as best performing instance prediction, to generate BAFs.", "labels": [], "entities": [{"text": "stance detection classifiers", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.9062721331914266}, {"text": "BiLSTM", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.8977194428443909}]}, {"text": "For stance aggregation, a credibility-weighted average, DF-QuAD with only direct replies, and DF-QuAD with both direct and nested replies, applied to appropriately constructed BAFs using the stance detection classifiers.", "labels": [], "entities": [{"text": "stance aggregation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8821526169776917}, {"text": "stance detection classifiers", "start_pos": 191, "end_pos": 219, "type": "TASK", "confidence": 0.7572704156239828}]}], "tableCaptions": [{"text": " Table 1: Summary of FNC-1, RumourEval Task A and  RumourEval Task B datasets.", "labels": [], "entities": [{"text": "FNC-1", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.716526210308075}, {"text": "RumourEval Task B datasets", "start_pos": 51, "end_pos": 77, "type": "DATASET", "confidence": 0.6091331914067268}]}, {"text": " Table 2: Hyper-parameters for training RNN models.", "labels": [], "entities": []}, {"text": " Table 3: Precision (P), recall (R), and F1-score (F1) of stance detection classifiers on FNC-1 test set and Ru- mourEval dataset (see Section 5).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9450497925281525}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9515733122825623}, {"text": "F1-score (F1)", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9592465460300446}, {"text": "stance detection classifiers", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8129583994547526}, {"text": "FNC-1 test set", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9475383957227071}, {"text": "Ru- mourEval dataset", "start_pos": 109, "end_pos": 129, "type": "DATASET", "confidence": 0.5626868903636932}]}, {"text": " Table 4: Precision (P), recall (R), and F1-score (F1) of the stance aggregation methods when applied to both  gold standard stance labels and the stance labels predicted by the LSTM and bidirectional LSTM trained stance  detection classifiers.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9438640773296356}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9540242254734039}, {"text": "F1-score (F1)", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9486744105815887}]}]}