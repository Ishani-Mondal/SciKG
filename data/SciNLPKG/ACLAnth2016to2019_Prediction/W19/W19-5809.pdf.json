{"title": [{"text": "A Sequence Modeling Approach for Structured Data Extraction from Unstructured Text", "labels": [], "entities": [{"text": "Sequence Modeling Approach", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.8613686958948771}, {"text": "Structured Data Extraction from Unstructured Text", "start_pos": 33, "end_pos": 82, "type": "TASK", "confidence": 0.7758082002401352}]}], "abstractContent": [{"text": "Extraction of structured information from un-structured text has always been a problem of interest for NLP community.", "labels": [], "entities": [{"text": "Extraction of structured information from un-structured text", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8519295454025269}]}, {"text": "Structured data is concise to store, search and retrieve; and it facilitates easier human & machine consumption.", "labels": [], "entities": []}, {"text": "Traditionally, structured data extraction from text has been done by using various parsing methodologies, applying domain specific rules and heuristics.", "labels": [], "entities": [{"text": "structured data extraction from text", "start_pos": 15, "end_pos": 51, "type": "TASK", "confidence": 0.7813819885253906}]}, {"text": "In this work, we leverage the developments in the space of sequence modeling for the problem of structured data extraction.", "labels": [], "entities": [{"text": "structured data extraction", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6434306104977926}]}, {"text": "Initially, we posed the problem as a machine translation problem and used the state-of-the-art machine translation model.", "labels": [], "entities": [{"text": "machine translation problem", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.8158658544222513}, {"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7301890254020691}]}, {"text": "Based on these initial results, we changed the approach to a sequence tagging one.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.7319077849388123}]}, {"text": "We propose an extension of one of the attractive models for sequence tagging tailored and effective to our problem.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 60, "end_pos": 76, "type": "TASK", "confidence": 0.8011361956596375}]}, {"text": "This gave 4.4% improvement over the vanilla sequence tagging model.", "labels": [], "entities": [{"text": "vanilla sequence tagging", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6507899363835653}]}, {"text": "We also propose another variant of the sequence tagging model which can handle multiple labels of words.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.6992161273956299}]}, {"text": "Experiments have been performed on Wikipedia Infobox Dataset of bi-ographies and results are presented for both single and multi-label models.", "labels": [], "entities": [{"text": "Wikipedia Infobox Dataset", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.9196521639823914}]}, {"text": "These models indicate an effective alternate deep learning technique based methods to extract structured data from raw text.", "labels": [], "entities": []}], "introductionContent": [{"text": "A humongous volume of data in the form of text, images, audio and video is being generated daily.", "labels": [], "entities": []}, {"text": "It has been reported that 90% of all the data available today has been generated in the last two years.", "labels": [], "entities": []}, {"text": "The pace of data generation is growing exponentially.", "labels": [], "entities": [{"text": "data generation", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.7619326114654541}]}, {"text": "The generation of data is not only restricted to open domains and social media; even in closed groups like private organizations and corporations, textual data is being produced in abundance.", "labels": [], "entities": []}, {"text": "Unstructured data is * Work done at Accenture Technology Labs present in a variety of forms like documents, reports and surveys, logs etc.", "labels": [], "entities": [{"text": "Accenture Technology Labs", "start_pos": 36, "end_pos": 61, "type": "DATASET", "confidence": 0.9528133273124695}]}, {"text": "Restricting this data to be captured directly in structured form prohibits the natural capturing of the data, leaving out essential pieces.", "labels": [], "entities": []}, {"text": "But structured data presents the data in a concise and well-defined manner which is easier to understand than a corresponding document.", "labels": [], "entities": []}, {"text": "Structured data can be transformed into tables which can be easily stored in databases.", "labels": [], "entities": []}, {"text": "It can be indexed, queried for and searched to retrieve relevant results of a query.", "labels": [], "entities": []}, {"text": "Thus structured representation is quintessential to facilitate machine consumption of data.", "labels": [], "entities": []}, {"text": "Moreover in the world of data abundance, such structured representation is essential for human consumption as well.", "labels": [], "entities": []}, {"text": "In many business processes, like Finance and Healthcare, the transformation of the unstructured data into structured form is done manually or semi-automatically through domain specific rules and heuristics.", "labels": [], "entities": []}, {"text": "Let's take the example of Pharmacovigilance (, where adverse effects of prescribed drugs are reported by patients or medical practitioners.", "labels": [], "entities": []}, {"text": "This information is used to detect signals of adverse effects.", "labels": [], "entities": []}, {"text": "Collection, analysis and reporting of these adverse effects by the drug companies is mandated bylaw.", "labels": [], "entities": [{"text": "Collection", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9541183710098267}]}, {"text": "In most cases, it is easy for patients or medical practitioners to describe the side-effects of their drugs in a common, day today language, in free form text.", "labels": [], "entities": []}, {"text": "Then it has to be transformed into a structured format which is analyzed with clinical knowledge for signals of adverse effects.", "labels": [], "entities": []}, {"text": "Currently this is done by human analysts or through very rigid text processing heuristics for certain kinds of text.", "labels": [], "entities": []}, {"text": "Another domain is extraction and management of legal contracts in domains such as real estate.", "labels": [], "entities": [{"text": "extraction and management of legal contracts", "start_pos": 18, "end_pos": 62, "type": "TASK", "confidence": 0.7025786191225052}]}, {"text": "Specifically Lease Abstraction involves manual inspection and validation of commercial rental lease documents.", "labels": [], "entities": [{"text": "Lease Abstraction", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7939005196094513}]}, {"text": "It is done by offshore experts who extract relevant information from the documents into a structured form.", "labels": [], "entities": []}, {"text": "This structured information is further used for aggregate analytics and decision making by large real estate firms (.", "labels": [], "entities": [{"text": "aggregate analytics", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8126069009304047}, {"text": "decision making", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.7760444581508636}]}, {"text": "shows a sample unstructured text and its corresponding structured output.", "labels": [], "entities": []}, {"text": "In case the structured data is to be stored in a database, the labels like name, birth date etc can be the column names of the database and the corresponding values like charles and 12 march 1970 can be the actual values stored.", "labels": [], "entities": [{"text": "charles and 12 march 1970", "start_pos": 170, "end_pos": 195, "type": "DATASET", "confidence": 0.6538131058216095}]}, {"text": "As mentioned earlier, previous work in this space involved parsing the natural language sentences, and writing rules and heuristics on the parse tree or structure to extract the information required (.", "labels": [], "entities": [{"text": "parsing the natural language sentences", "start_pos": 59, "end_pos": 97, "type": "TASK", "confidence": 0.8071892142295838}]}, {"text": "In this work, we approach the problem from sequence modeling perspective and weave together state of the art models in the space to extract structured information from raw unstructured data.", "labels": [], "entities": []}, {"text": "The task of information extraction to build structured data can be described as generating or matching appropriate tags or labels to corresponding parts of raw data.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7241427451372147}]}, {"text": "For each token in the raw data, a corresponding tag is attached marking what kind of data it stands for.", "labels": [], "entities": []}, {"text": "OTHER tag gets attached if the data in the raw text is not relevant.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9623056650161743}]}, {"text": "We have approached the problem both from machine translation and sequence tagging perspectives.", "labels": [], "entities": [{"text": "machine translation and sequence tagging", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.6625300109386444}]}, {"text": "In machine translation, typically there are two sequences, one in source language (say English) and the other in target language (say French).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7492603659629822}]}, {"text": "Machine learning models try to convert the sequence of tokens in source language to sequence of tokens in target language.", "labels": [], "entities": []}, {"text": "In case of translation problem, the core idea being expressed in the input and the output is the same, however it is in a different language.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9850960373878479}]}, {"text": "Similarly, for our problem both the input and output have same content although it is represented in an unstructured or structured format.", "labels": [], "entities": []}, {"text": "Soto start with, we approach the problem from translation perspective and treat the source text as word sequence of unstructured text and the corresponding tag sequence as target sequence.", "labels": [], "entities": []}, {"text": "State of the art machine translation models (sequence to sequence model ()) can then be attempted for the same.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.6946449279785156}]}, {"text": "We have experimented with this approach and treat it as our baseline.", "labels": [], "entities": []}, {"text": "We didn't find any previous work on this dataset for structured data extraction task.", "labels": [], "entities": [{"text": "structured data extraction", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6589549680550894}]}, {"text": "However we realized that this problem cannot be directly mapped to a translation problem.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.9189057052135468}]}, {"text": "There is significantly more word or phrase level information in the input which cannot be appropriately represented by translation models.", "labels": [], "entities": []}, {"text": "We realized that sequence tagging models (like for POS tagging) are more suitable for this problem.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.7229240536689758}, {"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7702139616012573}]}, {"text": "We have experimented with state of the art sequence tagging model for the problem and propose some problem specific variants to improve the performance.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7022935748100281}]}, {"text": "Main contributions of this work are as follows: 1.", "labels": [], "entities": []}, {"text": "We approach the problem from sequence modeling perspective, which is perhaps the first attempt in literature to the best of our knowledge.", "labels": [], "entities": [{"text": "sequence modeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8765961229801178}]}, {"text": "Modeled this way, we can eliminate usage of traditional ways for parsing or writing domain specific rules.", "labels": [], "entities": [{"text": "parsing or writing domain specific rules", "start_pos": 65, "end_pos": 105, "type": "TASK", "confidence": 0.7295768558979034}]}, {"text": "A parallel corpus of unstructured and structured data is sufficient to train the models.", "labels": [], "entities": []}, {"text": "2. We have designed a modified version of the state of the art sequence tagging model along with PoS tags and attention which further im- 3.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 63, "end_pos": 79, "type": "TASK", "confidence": 0.5800855755805969}]}, {"text": "We have also designed a multi-label sequence tagging model which can generate multiple labels of words using a customized learning loss based on Set similarity.", "labels": [], "entities": [{"text": "multi-label sequence tagging", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.6463648974895477}]}, {"text": "The paper is organized as follows: We present the details of seq2seq and sequence tagging models in Section 2 along with some interesting work which has been done previously using these models.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 73, "end_pos": 89, "type": "TASK", "confidence": 0.6338787823915482}]}, {"text": "In Section 3 we give details of our approach along with details of vanilla model and modified models for single and multi-label problems.", "labels": [], "entities": []}, {"text": "The details of our experiments are in Section 4.", "labels": [], "entities": []}, {"text": "We present some related work in Section 5.", "labels": [], "entities": []}, {"text": "We conclude in Section 6 by giving some future work directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used the Wikipedia Infobox dataset created by) which is available in the public domain . It consists of total 728, 321 biographies, each having the first Wikipedia paragraph and the corresponding infobox, both of which have been tokenized.", "labels": [], "entities": [{"text": "Wikipedia Infobox dataset", "start_pos": 17, "end_pos": 42, "type": "DATASET", "confidence": 0.9348401625951132}]}, {"text": "Originally this dataset  was created to build models to generate text based on the the infobox.", "labels": [], "entities": []}, {"text": "In our case, the problem is reversed.", "labels": [], "entities": []}, {"text": "Given a paragraph of unstructured data, we try to generate the corresponding infobox or structured data.", "labels": [], "entities": []}, {"text": "In the dataset, some information might be present in the paragraph but not in infobox and vice versa.", "labels": [], "entities": []}, {"text": "We have pruned the infoboxes so that it contains only that information which is present in the paragraph.", "labels": [], "entities": []}, {"text": "The information which is not present in the paragraph cannot be generated by any model by itself without external knowledge.", "labels": [], "entities": []}, {"text": "We have split the dataset into three parts in the ratio 8:1:1 for train, validation and test.", "labels": [], "entities": [{"text": "validation", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.9637412428855896}]}, {"text": "We have done basic pre-processing on both paragraphs and infoboxes.", "labels": [], "entities": []}, {"text": "Extra information and labels tagged as none have been removed from infoboxes.", "labels": [], "entities": []}, {"text": "The words have been initialized to GloVe) embeddings and character embeddings () have been randomly initialized.", "labels": [], "entities": []}, {"text": "Words are 300 dimensional and characters are 100 dimensional.", "labels": [], "entities": []}, {"text": "The models have been trained for 15 epochs or until it showed no improvement.", "labels": [], "entities": []}, {"text": "Single label model has been trained using Adam Optimizer ( and multi-label model using Adagrad Optimizer (.", "labels": [], "entities": []}, {"text": "Adaptive learning rate has been used.", "labels": [], "entities": [{"text": "Adaptive learning", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8558534681797028}]}, {"text": "have been used as regularizer.", "labels": [], "entities": []}, {"text": "shows some sample results of single and multi-label sequence tagging models.", "labels": [], "entities": [{"text": "multi-label sequence tagging", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6401120821634928}]}, {"text": "shows the Accuracy and Perplexity scores of the baseline approach using seq2seq model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994333386421204}, {"text": "Perplexity", "start_pos": 23, "end_pos": 33, "type": "METRIC", "confidence": 0.97262042760849}]}, {"text": "Here, accuracy is calculated as total number of correctly predicted words by total number of words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9995631575584412}]}, {"text": "Perplexity metric is from NLP models and it represents probability distribution of a language model over the text 2 . Lower perplexity represents better generalization and thus better performance.", "labels": [], "entities": []}, {"text": "We observed that LSTM Encoder-Decoder performs better than CNN Encoder-Decoder as it is able to take the temporal order or words into account and also because it handles short / medium length text well.", "labels": [], "entities": []}, {"text": "We also gave sequence of words and corresponding PoS tags as input and the results of this were the best among all the seq2seq models.", "labels": [], "entities": []}, {"text": "Despite these enhancements, this model does not perform well and has a low accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9987194538116455}]}, {"text": "shows the Sequence Tagging results on the same data using vanilla model and other model variants described earlier.", "labels": [], "entities": [{"text": "Sequence Tagging", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9128232598304749}]}, {"text": "In this case, accuracy metric is computed as number of labels correctly predicted by total number of words and F1 score is calculated as usual as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9991546869277954}, {"text": "F1 score", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.988323450088501}, {"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9992071986198425}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9962911605834961}]}, {"text": "We present the results of vanilla model and sequence tagging model with improvements like PoS tags and attention.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6750656515359879}]}, {"text": "We notice that the results of sequence tagging models are significantly better than the seq2seq models.", "labels": [], "entities": [{"text": "sequence tagging", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.6518497169017792}]}, {"text": "In multilabel sequence tagging model, the hamming loss on the test dataset was 0.1927.", "labels": [], "entities": [{"text": "multilabel sequence tagging", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6130626499652863}, {"text": "hamming loss", "start_pos": 42, "end_pos": 54, "type": "METRIC", "confidence": 0.9780112206935883}]}], "tableCaptions": [{"text": " Table 4: Baseline Results -Seq2Seq Model", "labels": [], "entities": []}, {"text": " Table 5: Sequence Tagging Results", "labels": [], "entities": [{"text": "Sequence Tagging", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9737769067287445}]}]}