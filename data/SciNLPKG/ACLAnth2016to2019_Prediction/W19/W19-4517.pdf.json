{"title": [], "abstractContent": [{"text": "In data ranking applications, pairwise annotation is often more consistent than cardinal annotation for learning ranking models.", "labels": [], "entities": []}, {"text": "We examine this in a case study on ranking text passages for argument convincing-ness.", "labels": [], "entities": []}, {"text": "Our task is to choose text passages that provide the highest-quality, most-convincing arguments for opposing sides of a topic.", "labels": [], "entities": []}, {"text": "Using data from a deployed system within the Bing search engine, we construct a pairwise-labeled dataset for argument convincingness that is substantially more comprehensive in topical coverage compared to existing public resources.", "labels": [], "entities": [{"text": "argument convincingness", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.7255087792873383}]}, {"text": "We detail the process of extracting topical passages for queries submitted to a search engine, creating annotated sets of passages aligned to different stances on a topic, and assessing argument convincingness of passages using pairwise annotation.", "labels": [], "entities": []}, {"text": "Using a state-of-the-art convincingness model, we evaluate several methods for using pairwise-annotated data examples to train models for ranking passages.", "labels": [], "entities": []}, {"text": "Our results show pairwise training outperforms training that regresses to a target score for each passage.", "labels": [], "entities": []}, {"text": "Our results also show a simple 'win-rate' score is a better regression target than the previously proposed page-rank target.", "labels": [], "entities": []}, {"text": "Lastly, addressing the need to filter noisy crowd-sourced annotations when constructing a dataset, we show that filtering for transitivity within pairwise annotations is more effective than filtering based on annotation confidence measures for individual examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "In online searches, results are typically presented to users ranked only by the relevancy of the results to the query.", "labels": [], "entities": []}, {"text": "Search engines typically learn such relevancy through the positive reinforcement of user clicks.", "labels": [], "entities": []}, {"text": "However, when queries address topics with multiple perspectives, some of which maybe polarizing and divisive, search result clickthrough may reinforce biases of users contributing to the digital filter bubble or echo chamber phenomena (.", "labels": [], "entities": []}, {"text": "To counter the filter bubble effect, search engines may seek to actively provide diverse results to topical queries), or even explicitly present arguments on different sides of an issue.", "labels": [], "entities": []}, {"text": "In such scenarios, it is desirable to not only consider the relevancy of the diverse search results, but also their quality and convincingness.", "labels": [], "entities": []}, {"text": "In our work, we seek to rank a collection of text passages by their argument convincingness, for use in Bing's multi-perspective search feature that presents arguments on different sides of a topical issue requested by a search query.", "labels": [], "entities": []}, {"text": "An example of our use case and the goal of the model we aim to construct are presented in. formally introduced the task of predicting argument convincingness to the language processing community by providing the first annotated corpus 1 (the UKP dataset), as well as providing initial experimental results on the dataset.", "labels": [], "entities": [{"text": "predicting argument convincingness", "start_pos": 123, "end_pos": 157, "type": "TASK", "confidence": 0.8936130801836649}, {"text": "UKP dataset)", "start_pos": 242, "end_pos": 254, "type": "DATASET", "confidence": 0.9766530593236288}]}, {"text": "The UKP dataset is annotated in a pairwise fashion: given two arguments with the same stance toward an issue, label which argument is more convincing.", "labels": [], "entities": [{"text": "UKP dataset", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9938768744468689}]}, {"text": "The implementation of pairwise annotation for this dataset is theoretically and practically grounded.", "labels": [], "entities": []}, {"text": "Motivated by the pioneering work of, pairwise labeling is a popular method for annotating items for attribute value (Heldsinger Query: reasons why nafta is good Passages with a \"Pro\" stance Passages with a \"Con\" stance Candidate 1: NAFTA has six advantages.", "labels": [], "entities": [{"text": "NAFTA", "start_pos": 232, "end_pos": 237, "type": "DATASET", "confidence": 0.8809302449226379}]}, {"text": "First, it quadrupled trade between Canada, Mexico, and the United States.", "labels": [], "entities": []}, {"text": "That's because the agreement eliminated tariffs.", "labels": [], "entities": []}, {"text": "Trade increased to $1.14 trillion in 2015.", "labels": [], "entities": []}, {"text": "Second, it lowered prices.", "labels": [], "entities": []}, {"text": "The United States imports Mexican oil for less than before the agreement.", "labels": [], "entities": []}, {"text": "Candidate 1: Is NAFTA a Bad Deal?", "labels": [], "entities": [{"text": "Is NAFTA a Bad Deal", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7479440450668335}]}, {"text": "The North American Free Trade Agreement (NAFTA) has come under fire recently, with some labeling it a disaster and claiming that it is the driving force behind the relocation of American firms like Ford Motor Company to Mexico.", "labels": [], "entities": [{"text": "North American Free Trade Agreement (NAFTA)", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.5550934635102749}]}, {"text": "Candidate 2: Because it helps in political interests.", "labels": [], "entities": []}, {"text": "NAFTA is meant to lower tariffs and therefore create pro business alliances between the three signing nations.", "labels": [], "entities": [{"text": "NAFTA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9549890756607056}]}, {"text": "This allows for the U.S. to buy products cheaper from Canada and tears down the barriers to trade such as tariffs fees etc.", "labels": [], "entities": []}, {"text": "Candidate 2: Best Answer: see...", "labels": [], "entities": [{"text": "Answer", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9157482981681824}]}, {"text": "people who support NAFTA only compare it to either all out free trade... or no trade.", "labels": [], "entities": [{"text": "NAFTA", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.7618809342384338}]}, {"text": "trade is good and needed...", "labels": [], "entities": []}, {"text": "but that doesn't mean it has to be, or should be FREE trade...", "labels": [], "entities": [{"text": "FREE", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.9971016049385071}]}, {"text": "so stop with these false comparisons of we have to trade...", "labels": [], "entities": []}, {"text": "shows the use-case fora ranking model for convincingness.", "labels": [], "entities": []}, {"text": "Suppose a user has typed the query 'reasons why nafta is good'.", "labels": [], "entities": []}, {"text": "Normally, this query will elicit links to texts that reflect only a positive stance toward the 'nafta' issue.", "labels": [], "entities": []}, {"text": "Alternatively, a system can be designed to show arguments from both sides of the issue.", "labels": [], "entities": []}, {"text": "In our system, we seek to select and present one passage to show for each side of the issue.", "labels": [], "entities": []}, {"text": "Given passages that have been mapped to the pro and con sides of the issue, we will use our model to choose the best passage to show for each side of the issue.", "labels": [], "entities": []}, {"text": "The above example illustrates a situation with two passage candidates for each of the pro/con sides, and our model needs to choose the most convincing one to display for each side.).", "labels": [], "entities": []}, {"text": "Recently, have conducted a suite of annotation experiments in order to empirically validate the belief that pairwise annotation is faster and more accurate than cardinal annotation for comparative tasks 2 . This paper presents a practical case study of a scenario where we have annotated data in a pairwise fashion and wish to train a model for ranking purposes.", "labels": [], "entities": []}, {"text": "The base model we use for predicting argument convincingness is an extension of the sumof-embeddings model proposed by.", "labels": [], "entities": [{"text": "predicting argument convincingness", "start_pos": 26, "end_pos": 60, "type": "TASK", "confidence": 0.9070382912953695}]}, {"text": "Our base model records state-of-the-art performance on the ranking subtask from the UKP dataset.", "labels": [], "entities": [{"text": "UKP dataset", "start_pos": 84, "end_pos": 95, "type": "DATASET", "confidence": 0.994535356760025}]}, {"text": "Building on the base model, we explain two primary methods for going from pairwise data to a general ranking model: 1) Train a model that independently produces scores for each passage using a pairwise training paradigm to minimize across entropy objective function; 2) Assign real-valued scores to each passage, and train a model with a regression objective function to minimize the model's error against these scores.", "labels": [], "entities": []}, {"text": "The second approach requires a method to pregenerate the real-valued passage scores used as the regression targets using only pairwise annotations.", "labels": [], "entities": []}, {"text": "Towards this secondary goal, we test two approaches: 1) Following, we generate PageRank (PR) () scores using directed graphs derived from the labeled pairs; 2) We compute a simple 'WinRate' (WR) percentage based on how often a passage is rated more convincing against its competitor passages.", "labels": [], "entities": [{"text": "WinRate' (WR) percentage", "start_pos": 181, "end_pos": 205, "type": "METRIC", "confidence": 0.9324568013350168}]}, {"text": "In order to test the robustness of the proposed techniques for using pairwise-labeled data to create a ranking model, we construct anew dataset for convincingness with a superior coverage of topics compared to the UKP dataset, which only has passages for 16 topics and roughly 1k total passages.", "labels": [], "entities": [{"text": "UKP dataset", "start_pos": 214, "end_pos": 225, "type": "DATASET", "confidence": 0.9928716123104095}]}, {"text": "In comparison, our dataset covers 3,234 topics, with roughly 30k total passages.", "labels": [], "entities": []}, {"text": "The results of experiments on the large-scale dataset show that the best method for training a ranking model is to use the pairwise labels directly.", "labels": [], "entities": []}, {"text": "Secondly, regarding the regression-based models, regressing to WR is better than PR, and even competitive with pairwise training.", "labels": [], "entities": [{"text": "WR", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9919448494911194}, {"text": "PR", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.7942608594894409}]}, {"text": "Finally, filtering data based on label confidence can actually hurt performance, although it can be beneficial to weight a pairwise model based on label confidence.", "labels": [], "entities": []}, {"text": "Alternatively, removing query-passage sets where cycles appear in the directed graphs induced by the labels of passage pairs is a preferred method for data-filtering in our case study.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the utility of a convincingness model over a large variety of topics we created a dataset with larger topical coverage compared to the UKP data.", "labels": [], "entities": [{"text": "UKP data", "start_pos": 152, "end_pos": 160, "type": "DATASET", "confidence": 0.988237589597702}]}, {"text": "We seeded the process with data collected for Bing's multi-perspective search feature, which was designed to show two short passages arguing for opposing stances of an issue expressed by a user query submitted to the system (e.g., \"is coffee good for you\").", "labels": [], "entities": []}, {"text": "The dataset consists of topic, query, passage triples.", "labels": [], "entities": []}, {"text": "Each query conveys a pro or con sentiment for the expressed topic.", "labels": [], "entities": []}, {"text": "Multiple potential passages are matched with each topic based on the Bing search engine's relevancy rankings with each passage assigned to the pro or con side of the topic based on a sentiment analysis classifier trained for the task.", "labels": [], "entities": [{"text": "Bing search engine", "start_pos": 69, "end_pos": 87, "type": "DATASET", "confidence": 0.8819361130396525}]}, {"text": "The passages themselves are snippets of text that have been scraped from the Web.", "labels": [], "entities": []}, {"text": "For each query in a triplet, we have also automatically determined a paired query expressing the opposing stance (e.g., \"is coffee bad for you\") which we use to help validate the stance of passages as detailed below.", "labels": [], "entities": []}, {"text": "The initial seed set contained 95,318 triples across 18,864 unique queries covering 3,439 topics.", "labels": [], "entities": []}, {"text": "The initial annotations of the pro/con stances of queries and passages of the data available from the pre-existing system were created using automatic means (e.g., a sentiment analysis model) and were hence errorful.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.9034199118614197}]}, {"text": "Additionally, no assessment of the convincingness of the passages had been conducted.", "labels": [], "entities": []}, {"text": "Thus, we performed a two-stage manual annotation process on the dataset to (1) generate ground truth stance labels for query/passage pairs, and (2) generate pairwise convincingness assessments of passages associated with the same topic and stance.", "labels": [], "entities": []}, {"text": "In this section we describe the details for evaluating the methods we propose in Section 4.2, namely the approaches for filtering the fully annotated dataset, as well as creating a properly curated train/test split.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results of ranking experiments on our newly-annotated dataset. Bold indicates the best performance for  a given model on a given evaluation metric, and * indicates the best result across all models.", "labels": [], "entities": []}]}