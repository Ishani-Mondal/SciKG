{"title": [{"text": "Memory-Augmented Neural Networks for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7338794767856598}]}], "abstractContent": [{"text": "Memory-augmented neural networks (MANNs) have been shown to outper-form other recurrent neural network architectures on a series of artificial sequence learning tasks, yet they have had limited application to real-world tasks.", "labels": [], "entities": []}, {"text": "We evaluate direct application of Neural Turing Machines (NTM) and Differentiable Neural Computers (DNC) to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7788103818893433}]}, {"text": "We further propose and evaluate two models which extend the attentional encoder-decoder with capabilities inspired by memory augmented neural networks.", "labels": [], "entities": []}, {"text": "We evaluate our proposed models on IWSLT Vietnamese\u2192English and ACL Romanian\u2192English datasets.", "labels": [], "entities": [{"text": "IWSLT Vietnamese\u2192English", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.8498023152351379}, {"text": "ACL Romanian\u2192English datasets", "start_pos": 64, "end_pos": 93, "type": "DATASET", "confidence": 0.7345412373542786}]}, {"text": "Our proposed models and the memory augmented neural networks perform similarly to the attentional encoder-decoder on the Vietnamese\u2192English translation task while have a 0.3-1.9 lower BLEU score for the Romanian\u2192English task.", "labels": [], "entities": [{"text": "Vietnamese\u2192English translation task", "start_pos": 121, "end_pos": 156, "type": "TASK", "confidence": 0.6562716841697693}, {"text": "BLEU", "start_pos": 184, "end_pos": 188, "type": "METRIC", "confidence": 0.9993707537651062}]}, {"text": "Interestingly, our analysis shows that despite being equipped with additional flexibility and being randomly initialized memory augmented neural networks learn an algorithm for machine translation almost identical to the attentional encoder-decoder.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 177, "end_pos": 196, "type": "TASK", "confidence": 0.7255360335111618}]}], "introductionContent": [{"text": "Attentional encoder-decoders (, where the encoder and decoder are often LSTMs or other gated RNNs such as the Gated Recurrent Unit (, area class of neural network models that have achieved state-of-the-art performance on many language pairs for machine translation).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 245, "end_pos": 264, "type": "TASK", "confidence": 0.7333258092403412}]}, {"text": "An encoder RNN reads the source sentence one token at a time.", "labels": [], "entities": []}, {"text": "The encoder both maintains an internal vector representing the full source sentence and it encodes each token in the source sentence into a vector often assumed to represent the meaning of that token in its surrounding context.", "labels": [], "entities": []}, {"text": "The decoder receives the internal vector from the encoder and can read from the encoded source sentence when producing the target sentence.", "labels": [], "entities": []}, {"text": "Attentional encoder-decoders can be seen as a basic form of MANN.", "labels": [], "entities": []}, {"text": "The collection of vectors representing the encoded source sentence can be viewed as external memory which is written to by the encoder and read from by the decoder.", "labels": [], "entities": []}, {"text": "But attentional encoder-decoders do not have the same range of capabilities as MANNs such as the Neural Turing Machine (NTM) ( ) or Differentiable Neural Computer (DNC) (.", "labels": [], "entities": []}, {"text": "The encoder RNN in attentional encoder-decoders must write a vector at each timestep and this write must be to a single memory location.", "labels": [], "entities": []}, {"text": "The encoder is notable to update previously written vectors and has only one write head.", "labels": [], "entities": []}, {"text": "The decoder has read only access to the encoded source sentence and typically just a single read head.", "labels": [], "entities": []}, {"text": "Widely used attention mechanisms () do not have the ability to iterate through the source sentence from a previously attended location.", "labels": [], "entities": []}, {"text": "All of these capabilities are present in In this paper we propose two extensions to the attentional encoder-decoder which add several capabilities present in other MANNs.", "labels": [], "entities": []}, {"text": "We are also the first that we are aware of to evaluate the performance of MANNs applied directly to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7544444799423218}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Vietnamese\u2192English translation results  (BLEU) on dev (TED tst2012) and test (TED  tst2013) sets. M.A.D \u2194 Memory-Augmented De- coder. 1 R/W head means the MANN had 1 read  and 1 write head into external memory.", "labels": [], "entities": [{"text": "BLEU)", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9720946252346039}]}, {"text": " Table 2: Romanian\u2192English translation results  (BLEU) on dev (newsdev2016) and test (new- stest2016) sets. M.A.D \u2194 Memory-Augmented  Decoder. 1 R/W head means the MANN had 1  read and 1 write head into external memory.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.995250940322876}]}]}