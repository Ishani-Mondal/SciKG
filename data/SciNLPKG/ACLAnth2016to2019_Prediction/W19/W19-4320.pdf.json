{"title": [{"text": "Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose Multilingual Meta-Embeddings (MME), an effective method to learn multilingual representations by lever-aging monolingual pre-trained embeddings.", "labels": [], "entities": []}, {"text": "MME learns to utilize information from these embeddings via a self-attention mechanism without explicit language identification.", "labels": [], "entities": [{"text": "MME", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9422473907470703}]}, {"text": "We evaluate the proposed embedding method on the code-switching English-Spanish Named Entity Recognition dataset in a multilingual and cross-lingual setting.", "labels": [], "entities": [{"text": "English-Spanish Named Entity Recognition dataset", "start_pos": 64, "end_pos": 112, "type": "DATASET", "confidence": 0.5486529290676116}]}, {"text": "The experimental results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning a representation through embedding is a fundamental technique to capture latent word semantics.", "labels": [], "entities": []}, {"text": "Practically, word-level representation has been extensively explored to improve many downstream natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "word-level representation", "start_pos": 13, "end_pos": 38, "type": "TASK", "confidence": 0.749793529510498}]}, {"text": "A new wave of \"meta-embeddings\" research aims to learn how to effectively combine pre-trained word embeddings in supervised training into a single dense representation (.", "labels": [], "entities": []}, {"text": "This method is known to be effective to overcome domain and modality limitations.", "labels": [], "entities": []}, {"text": "However, the generalization ability of previous works has been limited to monolingual tasks, so we aim to extend the method to multilingual contexts which benefits the processing of code-switching text.", "labels": [], "entities": []}, {"text": "In multilingual societies, speakers tend to move back and forth from one language to another during the same conversation, which is commonly  called \"code-switching\".", "labels": [], "entities": []}, {"text": "Code-Switching is produced in both written text and speech in a discourse.", "labels": [], "entities": []}, {"text": "Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (, named entity recognition (, and language identification ().", "labels": [], "entities": [{"text": "language modeling", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.7291380912065506}, {"text": "named entity recognition", "start_pos": 113, "end_pos": 137, "type": "TASK", "confidence": 0.6032557189464569}, {"text": "language identification", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.7714937627315521}]}, {"text": "Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences.", "labels": [], "entities": []}, {"text": "Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) ().", "labels": [], "entities": []}, {"text": "Language identifiers were commonly used to solve the word ambiguity issue in mixed-language sentences.", "labels": [], "entities": []}, {"text": "However, it may not reliably coverall code-switching cases, and it creates a bottleneck that would require large-scale crowdsourcing to annotate language identifiers in code-switching data correctly.", "labels": [], "entities": []}, {"text": "To overcome the code-switching problem, we introduce a multilingual meta-embedding model learned from different languages.", "labels": [], "entities": []}, {"text": "Our approach can be seen as a method to create a universal mul-tilingual meta-embedding learned in a supervised way with code-switching contexts by gathering information from monolingual sources.", "labels": [], "entities": []}, {"text": "Concurrently, this is a language-agnostic approach where it does not require any language information of each word.", "labels": [], "entities": []}, {"text": "We show the possibility of transferring information from multiple languages to unseen languages, and this approach can also be useful fora low-resource setting.", "labels": [], "entities": []}, {"text": "To effectively leverage the embeddings, we use FastText subwords information to solve out-of-vocabulary (OOV) issues.", "labels": [], "entities": []}, {"text": "By applying this method, our model can align the words with the corresponding languages.", "labels": [], "entities": []}, {"text": "Our contributions are two-fold: \u2022 We propose to generate multilingual metarepresentations from pre-trained monolingual word embeddings.", "labels": [], "entities": []}, {"text": "The model can learn how to construct the best word representation by mixing multiple sources without explicit language identification.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our multilingual metaembedding on English-Spanish codeswitching Named Entity Recognition (NER).", "labels": [], "entities": [{"text": "codeswitching Named Entity Recognition (NER)", "start_pos": 64, "end_pos": 108, "type": "TASK", "confidence": 0.7000618662152972}]}, {"text": "The result shows the effectiveness of the method on multilingual setting and demonstrates that our meta-embedding can generalize to unseen languages in a cross-lingual setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiment, we use English-Spanish tweets data provided by.", "labels": [], "entities": []}, {"text": "There are nine entity labels.", "labels": [], "entities": []}, {"text": "The labels use IOB format, where every token is labeled as a B-label in the beginning and then an I-label if it is a named entity, or O otherwise.", "labels": [], "entities": []}, {"text": "We use pre- Implementation Details Our model is trained using a Noam optimizer with a dropout of 0.1 for multilingual setting and 0.3 for the crosslingual setting.", "labels": [], "entities": []}, {"text": "Our model contains four layers of transformer blocks with a hidden size of 200 and four heads.", "labels": [], "entities": []}, {"text": "We start the training with a learning rate of 0.1.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 29, "end_pos": 42, "type": "METRIC", "confidence": 0.9650667011737823}]}, {"text": "We replace user hashtags (#user) and mentions (@user) with <USR>, and URL (https://domain.com) with <URL>, similarly to.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Multilingual results (mean and standard devia- tion from five experiments). EN: both English FastText  and GloVe word embeddings.", "labels": [], "entities": [{"text": "EN", "start_pos": 86, "end_pos": 88, "type": "METRIC", "confidence": 0.6463508009910583}]}]}