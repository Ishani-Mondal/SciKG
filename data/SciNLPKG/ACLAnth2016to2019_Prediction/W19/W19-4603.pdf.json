{"title": [{"text": "POS Tagging for Improving Code-Switching Identification in Arabic", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7486733794212341}, {"text": "Improving Code-Switching Identification", "start_pos": 16, "end_pos": 55, "type": "TASK", "confidence": 0.679882804552714}]}], "abstractContent": [{"text": "When speakers code-switch between their native language and a second language or language variant, they follow a syntactic pattern where words and phrases from the embedded language are inserted into the matrix language.", "labels": [], "entities": []}, {"text": "This paper explores the possibility of utilizing this pattern in improving code-switching identification between Modern Standard Ara-bic (MSA) and Egyptian Arabic (EA).", "labels": [], "entities": [{"text": "code-switching identification", "start_pos": 75, "end_pos": 104, "type": "TASK", "confidence": 0.704914391040802}, {"text": "Modern Standard Ara-bic (MSA) and Egyptian Arabic (EA)", "start_pos": 113, "end_pos": 167, "type": "DATASET", "confidence": 0.8514034400383631}]}, {"text": "We try to answer the question of how strong is the POS signal in word-level code-switching identification.", "labels": [], "entities": [{"text": "word-level code-switching identification", "start_pos": 65, "end_pos": 105, "type": "TASK", "confidence": 0.6878486772378286}]}, {"text": "We build a deep learning model enriched with linguistic features (including POS tags) that outperforms the state-of-the-art results by 1.9% on the development set and 1.0% on the test set.", "labels": [], "entities": []}, {"text": "We also show that in intra-sentential code-switching, the selection of lexical items is constrained by POS categories, where function words tend to come more often from the dialectal language while the majority of content words come from the standard language .", "labels": [], "entities": []}], "introductionContent": [{"text": "Code-switching (CS) is common in multilingual communities as well as diglossic ones, where the language of information and education is different from the language of speaking and daily interaction.", "labels": [], "entities": [{"text": "Code-switching (CS)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6046947464346886}]}, {"text": "With the increased level of education, mobility, globalization, multiculturalism, and multilingualism in modern societies, combined with the rise of social media, where people write in the way they speak, CS has become a pervasive phenomenon, particularly in user-generated data, and a major challenge for NLP systems dealing with that data.", "labels": [], "entities": []}, {"text": "CS is interesting for two reasons: first, there is a large population of bilingual and diglossic speakers, or at least speakers with some exposure to a foreign language, who tend to mix and blend two languages for various pragmatic, psycholinguistic and sociolinguistic reasons.", "labels": [], "entities": []}, {"text": "Second, existing theoretical and computational linguistic models are based on monolingual data and cannot adequately explain ordeal with the influx of CS data whether spoken or written.", "labels": [], "entities": []}, {"text": "CS has been studied for over half a century from different perspectives, including theoretical linguistics, applied linguistics), socio-linguistics, psycho-linguistics), and more recently computational linguistics (.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the possibility of using POS tagging to improve word-level language identification for diglossic Arabic in a deep-learning system.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7425126731395721}, {"text": "word-level language identification", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.6205808222293854}]}, {"text": "We present some syntactic characterization of intra-sentential codeswitching, and show that POS can be a powerful signal for code-switching identification.", "labels": [], "entities": [{"text": "code-switching identification", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.7204351127147675}]}, {"text": "We also pay special attention to intra-sentential codeswitching and examine the distribution of POS categories involved in this type of data.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: in the remainder of this introduction we present challenges, definitions, and types of CS, and the particular aspects involved in Arabic CS.", "labels": [], "entities": [{"text": "Arabic CS", "start_pos": 165, "end_pos": 174, "type": "TASK", "confidence": 0.6116182506084442}]}, {"text": "Section 2 gives an overview of related works.", "labels": [], "entities": []}, {"text": "In Section 3, we describe and record our observations on the data used in our experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents a description of our system and the features used.", "labels": [], "entities": []}, {"text": "Section gives the details of our experiments and discusses the results, and finally we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a number experiments with different layers in the neural network model stacked on top of each other, making use of word and character representation, POS, FastText pre-trained embeddings, and other features.", "labels": [], "entities": []}, {"text": "This allows us to seethe significance of each feature and how it contributes to the overall performance of the system.", "labels": [], "entities": []}, {"text": "The experiments are shown in.", "labels": [], "entities": []}, {"text": "The results in are reported for the f-score measure on the validation set, except for the last row which gives the best model results on the test set.", "labels": [], "entities": []}, {"text": "The results generally show that the DNN model is incrementally improving by adding more features and external resources.", "labels": [], "entities": []}, {"text": "The best result is obtained with the aggregation of all features, excluding the SP (spell checking word list).", "labels": [], "entities": []}, {"text": "In the training data, lang1 (MSA) is the majority class representing 68.7% of the labels.", "labels": [], "entities": [{"text": "lang1 (MSA)", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.8296313434839249}]}, {"text": "We use majority voting as the baseline in order to detect if  POS tags alone do send any positive signal to the model at all.", "labels": [], "entities": []}, {"text": "We note that the baseline is very low which is due to the fact that the tag distribution in the training set is disproportionate with both the validation and the test set, where lang1 represents only 30.96% and 28.10% of the data respectively.", "labels": [], "entities": []}, {"text": "It is to be noted that we apply post-processing (PP) to the output of the prediction.", "labels": [], "entities": []}, {"text": "The idea is that foreign words (words written in Latin script), punctuation marks, user names (words starting with the '@' sign), and hashtags (words starting with the '#' sign) should all be assigned the other tag.", "labels": [], "entities": []}, {"text": "As these are deterministic cases, we develop a post-process procedure to correct errors in the predictions of the probabilistic model, and to make sure that they are assigned the right tag.", "labels": [], "entities": []}, {"text": "Our experiments show that POS tags do give a strong signal to the network that leads to a significant improvement over the baseline, from 30.97% to 66.19% using coarse-grained POS features and 72.99% using the fine-grained tags.", "labels": [], "entities": []}, {"text": "We also no-   tice that using the predicted fine-grained POS is significantly more helpful than using the predicted coarse-grained one (although the prediction accuracy for fine-grained tags is lower).", "labels": [], "entities": []}, {"text": "This is probably because the fine-grained POS tags encode more lexical information (related to clitics and affixes) that can have distinctive combinations.", "labels": [], "entities": []}, {"text": "claimed that part-ofspeech (POS) tags can predict CS points more reliably than words themselves, but our results show that words still give a stronger signal than POS tags alone.", "labels": [], "entities": []}, {"text": "We also notice that Brown Clusters, named entity gazetteers and FastText pre-trained embeddings contribute to incrementally improve the performance of the system.", "labels": [], "entities": []}, {"text": "Unfortunately adding information from the spelling word list did not show any improvement on the system, and this is why it is removed from the final system architecture.", "labels": [], "entities": []}, {"text": "Now we compare our best model to the state-ofthe-art system of, which won the 2016 Second Shared Task on Language Identification in Code-Switched Data ( ) on the MSA-EA dataset.", "labels": [], "entities": [{"text": "Language Identification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.6932390630245209}, {"text": "MSA-EA dataset", "start_pos": 162, "end_pos": 176, "type": "DATASET", "confidence": 0.9736014008522034}]}, {"text": "We compare the performance of the two systems in terms of f-score accuracy on both the development and test set, in respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.5910536646842957}]}, {"text": "We also include the number of instances and the ratio percentage for each label.", "labels": [], "entities": []}, {"text": "As the tables show, the category lang2 constitutes the majority class for both amb ne mixed other L1: Confusion matrix for the development dataset.", "labels": [], "entities": []}, {"text": "the validation and test sets (45.22% and 46.62% respectively), contrary to the training set where lang1 makes up 68.70% of the labels.", "labels": [], "entities": []}, {"text": "For the development set our system outperforms that of by 1.9% absolute with significant gains for lang1 (3% absolute) and ne (2% absolute).", "labels": [], "entities": []}, {"text": "For the test set our system again outperforms that of by 1.0% absolute with the gain spread almost evenly across all labels.", "labels": [], "entities": []}, {"text": "presents the confusion matrix for the validation set, which shows that ne suffers the largest confusion as it gets mixed up as either lang2 (EA) or lang1 (MSA).", "labels": [], "entities": []}, {"text": "This is due to the fact that many named entities in Arabic can also be used as ordinary words, and, unlike English, there is no case marking or other orthographic features that can superficially distinguish the two.", "labels": [], "entities": []}, {"text": "For example, the word krym, can mean either \"Kareem\" as an ne or \"generous\" as an adjective, and jmAl can mean \"Jamal\" as an ne or \"beauty\" as a noun.", "labels": [], "entities": []}, {"text": "The second largest confusion is between lang1 and lang2, where we find that a considerable amount of the mix-up coming from function words, such as wa \"and\", >aw \"or\" and <ilY \"to\", which can equally be used as either lang1 or lang2, depending on the context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Tag count and ratio in the training set, where  lang1 is MSA, lang2 is EA, and ne is a named entity.", "labels": [], "entities": [{"text": "EA", "start_pos": 81, "end_pos": 83, "type": "METRIC", "confidence": 0.9952155351638794}]}, {"text": " Table 4: DNN experiments and Results. Abbreviations:  BC: Brown Clusters, GZ: named entity gazetteer, SP:  Spelling word list, PP: post-processing", "labels": [], "entities": []}, {"text": " Table 5: F1 score token level comparison between  Samih et al. (2016) and the current system on the de- velopment dataset.", "labels": [], "entities": [{"text": "F1 score token level", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.925410658121109}, {"text": "de- velopment dataset", "start_pos": 101, "end_pos": 122, "type": "DATASET", "confidence": 0.5627735108137131}]}, {"text": " Table 6: F1 score token level comparison between  Samih et al. (2016) and the current system on the test  dataset.", "labels": [], "entities": [{"text": "F1 score token level", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9301587790250778}]}, {"text": " Table 7: Confusion matrix for the development dataset.", "labels": [], "entities": []}]}