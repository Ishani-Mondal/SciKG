{"title": [], "abstractContent": [{"text": "This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task.", "labels": [], "entities": []}, {"text": "The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.8520126342773438}]}, {"text": "Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias.", "labels": [], "entities": []}, {"text": "The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems.", "labels": [], "entities": []}, {"text": "Evaluation on GAP test data results in a state-of-the-art performance at 92.5% F1 (gen-der bias of 0.97), edging closer to the human performance of 96.6%.", "labels": [], "entities": [{"text": "GAP test data", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.8311192790667216}, {"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9996639490127563}, {"text": "gen-der bias", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9636353552341461}]}, {"text": "The end-to-end solution 1 presented here placed 1st in the Kaggle competition, winning by a significant lead.", "labels": [], "entities": [{"text": "Kaggle competition", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.7133859992027283}]}], "introductionContent": [{"text": "The Gendered Ambiguous Pronouns (GAP) shared task aims to mitigate bias observed in the performance of coreference resolution systems when dealing with gendered pronouns.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 103, "end_pos": 125, "type": "TASK", "confidence": 0.9175308048725128}]}, {"text": "State-ofthe-art coreference models suffer from a systematic bias in resolving masculine entities more confidently compared to feminine entities.", "labels": [], "entities": []}, {"text": "To this end, published anew GAP dataset 2 to encourage research into building models and systems that are robust to gender bias.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.9114197790622711}]}, {"text": "The arrival of modern language models like ELMo (), BERT, and GPT (, have significantly advanced the state-of-the art in a wide The code is available at https://github.com/ sattree/gap 2 https://github.com/ google-research-datasets/gap-coreference range of NLP problems.", "labels": [], "entities": [{"text": "BERT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9781144857406616}, {"text": "GPT", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.8337772488594055}]}, {"text": "All of them have a common theme in that a generative language model is pretrained on a large amount of data, and is subsequently fine-tuned on the target task data.", "labels": [], "entities": [{"text": "generative language", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.8861742913722992}]}, {"text": "This approach of transfer learning has been very successful.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.9840407967567444}]}, {"text": "The current work applies the same philosophy and uses BERT as the base model to encode lowlevel features, followed by a task-specific module that is trained from scratch (fine-tuning BERT in the process).", "labels": [], "entities": [{"text": "BERT", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9922653436660767}, {"text": "BERT", "start_pos": 183, "end_pos": 187, "type": "METRIC", "confidence": 0.9952552318572998}]}, {"text": "GAP shared task presents the general GAP problem in gold-two-mention) format and formulates it as a classification problem, where the model must resolve a given pronoun to either of the two given candidates or neither . Neither instances are particularly difficult to resolve since they require understanding a wider context and perhaps a knowledge of the world.", "labels": [], "entities": [{"text": "GAP shared task", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5138546427090963}]}, {"text": "A parallel for this case can be drawn from Question-Answering systems where identifying unanswerable questions confidently remains an active research area.", "labels": [], "entities": []}, {"text": "Recent work shows that it is possible to determine lack of evidence with greater confidence by explicitly modeling for it.", "labels": [], "entities": []}, {"text": "Works of and demonstrate model designs with specialized deep learning architectures that encode evidence in the input and show significant improvement in identifying unanswerable questions.", "labels": [], "entities": []}, {"text": "This paper first introduces a baseline that is based on a language model.", "labels": [], "entities": []}, {"text": "Then, a novel architecture for pooling evidence from off-the-shelf coreference models is presented, that further boosts the confidence of the base classifier and specifically helps in resolving Neither instances.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: \u2022 Demonstrate the effectiveness of pretrained language models and their transferability to establish a strong baseline (ProBERT) for the gold-two-mention shared task.", "labels": [], "entities": [{"text": "ProBERT", "start_pos": 162, "end_pos": 169, "type": "METRIC", "confidence": 0.7215551733970642}]}, {"text": "\u2022 Introduce an Evidence Pooling based neural architecture (GREP) to draw upon the strengths of off-the-shelf coreference systems.", "labels": [], "entities": []}, {"text": "\u2022 Present the model results that placed 1st in the GAP shared task Kaggle competition.", "labels": [], "entities": [{"text": "GAP shared task Kaggle competition", "start_pos": 51, "end_pos": 85, "type": "TASK", "confidence": 0.8035884261131286}]}, {"text": "All datasets are approximately gender balanced, other than stage 2 test set.", "labels": [], "entities": []}, {"text": "The datasets, gap-development, gap-validation, and gap-test, are part of the publicly available GAP corpus.", "labels": [], "entities": [{"text": "GAP corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.7457660734653473}]}, {"text": "The preprocessing and sanitization steps are described next.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9077284336090088}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus statistics. Masculine (M) and Femi- nine (F) instances were identified based on the gender  of the pronoun mention labeled in the sample.  \u2020 Only a  subset of these may have been used for final evaluation.", "labels": [], "entities": [{"text": "Femi- nine (F)", "start_pos": 47, "end_pos": 61, "type": "METRIC", "confidence": 0.812385673324267}]}, {"text": " Table 2: GAP dataset label distribution before and after sanitization. (-x) indicates the number of samples that  were moved out of a given class and (+x) indicates the number of samples that were added post-sanitization.", "labels": [], "entities": [{"text": "GAP dataset", "start_pos": 10, "end_pos": 21, "type": "DATASET", "confidence": 0.8350154757499695}]}, {"text": " Table 3: Single model performance on gap-test set by gender. M: masculine, F: feminine, B: (bias) ratio of feminine  to masculine performance, O: overall. Log loss is not available for systems that only produce labels.  \u2020 As reported  by", "labels": [], "entities": [{"text": "B: (bias) ratio", "start_pos": 89, "end_pos": 104, "type": "METRIC", "confidence": 0.8661427398522695}, {"text": "Log loss", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.7571919858455658}]}, {"text": " Table 5: Class-wise comparison of model accuracy for  ProBERT and GREP. Off-diagonal terms show cases  where GREP fixes errors made by ProBERT and vice- versa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.984587550163269}, {"text": "ProBERT", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8831186294555664}, {"text": "GREP", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.8495796322822571}, {"text": "ProBERT", "start_pos": 136, "end_pos": 143, "type": "DATASET", "confidence": 0.9162229895591736}]}, {"text": " Table 8: Example 1 -A comparison of probabilities assigned by ProBERT and GREP", "labels": [], "entities": [{"text": "ProBERT", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9475150108337402}, {"text": "GREP", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.6665903925895691}]}]}