{"title": [{"text": "DFKI-NMT Submission to the WMT19 News Translation Task", "labels": [], "entities": [{"text": "DFKI-NMT", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8328016400337219}, {"text": "WMT19 News Translation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7837479710578918}]}], "abstractContent": [{"text": "This paper describes the DFKI-NMT submission to the WMT19 News translation task.", "labels": [], "entities": [{"text": "WMT19 News translation task", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7470501288771629}]}, {"text": "We participated in both English-to-German and German-to-English directions.", "labels": [], "entities": []}, {"text": "We trained standard Transformer models and adopted various techniques for effectively training our models, including data selection, back-translation, in-domain fine-tuning and model ensemble.", "labels": [], "entities": [{"text": "data selection", "start_pos": 117, "end_pos": 131, "type": "TASK", "confidence": 0.745564877986908}]}, {"text": "We show that these training techniques improved the performance of our Transformer models up to 5 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9993299245834351}]}, {"text": "We give a detailed analysis of the performance of our system.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the DFKI-NMT submission to the WMT19 News translation task.", "labels": [], "entities": [{"text": "WMT19 News translation task", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.7470501288771629}]}, {"text": "We participated in both English-to-German and German-toEnglish directions.", "labels": [], "entities": []}, {"text": "We trained Transformer models () using Sockeye).", "labels": [], "entities": [{"text": "Sockeye", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.888763427734375}]}, {"text": "Compared to RNN-based translation models (), Transformer models can be trained very fast due to parallelizable self-attention networks.", "labels": [], "entities": [{"text": "RNN-based translation", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7072964310646057}]}, {"text": "We applied several very useful techniques for effectively training our models.", "labels": [], "entities": []}, {"text": "Data Selection The parallel training data provided for German-English is quite large (38M sentence pairs).", "labels": [], "entities": []}, {"text": "Most of the parallel data is crawled from the Internet and is not in News domain.", "labels": [], "entities": []}, {"text": "Out-ofdomain training data can hurt the translation performance on News test sets () and also significantly increase training time.", "labels": [], "entities": [{"text": "translation", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.9684296250343323}, {"text": "News test sets", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.9515750805536906}]}, {"text": "Therefore, we trained neural language models on a large monolingual News corpus to perform data selection ().", "labels": [], "entities": [{"text": "data selection", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.6979726701974869}]}, {"text": "Back-translation Large monolingual data in the News domain is provided for both German and 1 https://github.com/awslabs/sockeye English, which can be back-translated as additional parallel training data for our system.", "labels": [], "entities": [{"text": "News domain", "start_pos": 47, "end_pos": 58, "type": "DATASET", "confidence": 0.9815191030502319}]}, {"text": "The back-translated parallel data is in the News domain, which is a big advantage compared to outof-domain parallel training data provided for the News task.", "labels": [], "entities": []}, {"text": "In-domain Fine-tuning The Transformer models were finally fine-tuned using the small in-domain parallel data provided for the News task (.", "labels": [], "entities": []}, {"text": "Note that the large back-translated parallel data is also in-domain, but it has relatively low quality due to translation errors.", "labels": [], "entities": []}, {"text": "Model Ensemble We trained two Transformer models with different sizes, Transformer-base and Transformer-big.", "labels": [], "entities": []}, {"text": "Our final submission is an ensemble of both models (.", "labels": [], "entities": []}, {"text": "The ensemble of both models outperformed a single base or big model most likely because the two models can capture somewhat different features for the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 151, "end_pos": 167, "type": "TASK", "confidence": 0.9219954609870911}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Case-insensitive BLEU scores on new- stest2018.  \"Ensemble\" means ensemble both  Transformer-base and Transformer-big after Stage 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9369713664054871}]}]}