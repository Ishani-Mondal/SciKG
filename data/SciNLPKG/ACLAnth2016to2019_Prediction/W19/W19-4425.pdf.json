{"title": [{"text": "Improving Precision of Grammatical Error Correction with a Cheat Sheet", "labels": [], "entities": [{"text": "Improving Precision of Grammatical Error", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.9290550708770752}]}], "abstractContent": [{"text": "In this paper, we explore two approaches of generating error-focused phrases and examine whether these phrases can lead to better performance in grammatical error correction for the restricted track of BEA 2019 Shared Task on GEC.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.6322835187117258}, {"text": "BEA 2019 Shared Task on GEC", "start_pos": 202, "end_pos": 229, "type": "DATASET", "confidence": 0.8329001863797506}]}, {"text": "Our results show that phrases directly extracted from GEC corpora outperform phrases from a statistical machine translation phrase table by a large margin.", "labels": [], "entities": []}, {"text": "Appending er-ror+context phrases to the original GEC corpora yields comparably higher precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9969605803489685}]}, {"text": "We also explore the generation of artificial syntactic error sentences using error+context phrases for the unrestricted track.", "labels": [], "entities": []}, {"text": "The additional training data greatly facilitates syntactic error correction (e.g., verb form) and contributes to better overall performance.", "labels": [], "entities": [{"text": "syntactic error correction", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.6070315639177958}]}], "introductionContent": [{"text": "Grammatical Error Correction (GEC) is a natural language processing (NLP) task of automatically detecting and correcting grammatical errors in the text.", "labels": [], "entities": [{"text": "Grammatical Error Correction (GEC)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8454360713561376}, {"text": "automatically detecting and correcting grammatical errors in the text", "start_pos": 82, "end_pos": 151, "type": "TASK", "confidence": 0.6722235083580017}]}, {"text": "With the ever-growing number of second language learners of English and demand to facilitate their learning with timely feedback, GEC has become increasingly popular and attracted much attention in both academia and industry in recent years.", "labels": [], "entities": [{"text": "GEC", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.5042806267738342}]}, {"text": "Ina typical GEC task, for example, Travel and bored.", "labels": [], "entities": [{"text": "GEC task", "start_pos": 12, "end_pos": 20, "type": "TASK", "confidence": 0.5228749513626099}]}, {"text": "in the sentence Travel by bus is expensive and bored needs to be first detected as incorrect and then be modified to their correct forms (Travelling and boring).", "labels": [], "entities": []}, {"text": "Various approaches have been proposed to solve this problem including language modeling, rule-based classifiers, machine-learning based classifiers, machine translation (MT), and etc.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7332232594490051}, {"text": "machine translation (MT)", "start_pos": 149, "end_pos": 173, "type": "TASK", "confidence": 0.8321841359138489}]}, {"text": "In the past few years, both GEC-tuned statistical machine translation (SMT) and neural machine translation (NMT) using sequence-to- * Equally contributed authors sequence (seq2seq) learning have demonstrated to be more effective in grammatical error correction than other approaches.", "labels": [], "entities": [{"text": "GEC-tuned statistical machine translation (SMT)", "start_pos": 28, "end_pos": 75, "type": "TASK", "confidence": 0.7155349765505109}, {"text": "neural machine translation", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.7557655970255533}, {"text": "grammatical error correction", "start_pos": 232, "end_pos": 260, "type": "TASK", "confidence": 0.7052826484044393}]}, {"text": "Just as in other machine translation tasks, the quantity anfd quality of data play an important role in the MT approach to grammatical error correction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7058136612176895}, {"text": "MT", "start_pos": 108, "end_pos": 110, "type": "TASK", "confidence": 0.9949775338172913}, {"text": "grammatical error correction", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.5824094911416372}]}, {"text": "While several recent studies have focused on generating artificial grammatical error sentences (e.g., the current study explores how error-focused phrases influence the performance of grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 184, "end_pos": 212, "type": "TASK", "confidence": 0.6364576121171316}]}, {"text": "There are slightly over half million errorcontained sentences in the training data provided by the BEA 2019 Shared Task, and the total number of errors is over 1.3 million, which means there are on average 2 or 3 errors in each error sentence.", "labels": [], "entities": [{"text": "BEA 2019 Shared Task", "start_pos": 99, "end_pos": 119, "type": "DATASET", "confidence": 0.9150220155715942}]}, {"text": "Our intuition is that multiple errors in one sentence can be challenging for MT models to learn and generalize, especially when the amount of training data is limited.", "labels": [], "entities": [{"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9881548881530762}]}, {"text": "Thus, by augmenting the training data with error-focused phrases, which we term \"cheat sheet\", MT models can directly \"see\" the errors and their corrections.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9739497900009155}]}, {"text": "We predict that this will lead to better overall performance and precision in particular.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.999464213848114}]}, {"text": "We examine two ways of creating a cheat sheet-one extracting errors and surrounding context and the other one extracting from a SMT phrase table ( \u00a72).", "labels": [], "entities": [{"text": "SMT phrase", "start_pos": 128, "end_pos": 138, "type": "TASK", "confidence": 0.8404261469841003}]}, {"text": "Phrases extracted from the first method are also used to generate artificial syntactic error sentences for the unrestricted track of the shared task ( \u00a73).", "labels": [], "entities": []}, {"text": "We run both SMT using Moses () and multi-layer CNN seq2seq NMT (Chollampatt and Ng, 2018) for our training data in restricted (original training + cheat sheet) and unrestricted (original training + cheat sheet + syntactic pseudo corpus) settings ( \u00a74).", "labels": [], "entities": [{"text": "SMT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9871330261230469}]}, {"text": "In general, our results show that a cheat sheet created with errors and surrounding context does lead to an improvement in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9989196062088013}]}, {"text": "However, compared to current state-of-the-art results, the recall of our models is considerably lower.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9996092915534973}]}, {"text": "These results and future work are discussed in the last section.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used all the four datasets -FCE, NUCLE, W&I+LOCNESS and Lang-8 -provided in the BEA 2019 Shared Task 2 as our baseline data (1,171,078 sentence pairs).", "labels": [], "entities": [{"text": "FCE", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.7943950891494751}, {"text": "NUCLE", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.7497577667236328}, {"text": "BEA 2019 Shared Task 2", "start_pos": 83, "end_pos": 105, "type": "DATASET", "confidence": 0.8939698219299317}]}, {"text": "For the restricted track, we appended the baseline data with our cheat sheet (in total over 2M sentence / phrase pairs), and for the unrestricted track, the additional syntactic pseudo corpus was supplemented on top of the training data in the restricted track (over 4M sentences in total).", "labels": [], "entities": []}, {"text": "The official W&I+LOCNESS development set and test set were used as development and evaluation 3 . We did not use any spellcheck to pre-or post-process our data, which could affect our results negatively.", "labels": [], "entities": [{"text": "W&I+LOCNESS development set and test set", "start_pos": 13, "end_pos": 53, "type": "DATASET", "confidence": 0.8864612519741059}]}, {"text": "For the SMT approach to GEC, we used the same Moses () setup as in \u00a72.2, except for the sentence length, which we changed to the default value (1 -80, we set the word embedding dimensions in both encoders and decoders to 300 rather than 500, and we trained the word embeddings separately using the error and correct side training data instead of external corpora.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9955123066902161}, {"text": "GEC", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.7543455362319946}]}, {"text": "During inference, we used abeam size of 10.", "labels": [], "entities": []}, {"text": "shows the baseline result and the results we submitted to the BEA 2019 Shared Task.", "labels": [], "entities": [{"text": "BEA 2019 Shared Task", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.901902437210083}]}, {"text": "The submitted results were all from the versions with an error+context cheat sheet because our phrase table cheat sheet yielded much worse results.", "labels": [], "entities": []}, {"text": "Overall, our models with an error+context cheat sheet achieved higher precision and F 0.5 in both restricted and unrestricted tracks than the baseline model.", "labels": [], "entities": [{"text": "error+context cheat sheet", "start_pos": 28, "end_pos": 53, "type": "METRIC", "confidence": 0.7492579102516175}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9996896982192993}, {"text": "F 0.5", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9918689429759979}]}, {"text": "Within our own models, GEC-tuned NMT, as expected, consistently outperformed the generic SMT models.", "labels": [], "entities": [{"text": "GEC-tuned NMT", "start_pos": 23, "end_pos": 36, "type": "DATASET", "confidence": 0.7399183809757233}, {"text": "SMT", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.966723620891571}]}, {"text": "In the unrestricted setting, for example, the gap in F 0.5 was over 5%.", "labels": [], "entities": [{"text": "F 0.5", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9556165933609009}]}, {"text": "When comparing the two NMT models across the two tracks, our results clearly show that the additional pseudo corpus contributed to better performance in precision, recall and F 0.5 .", "labels": [], "entities": [{"text": "precision", "start_pos": 153, "end_pos": 162, "type": "METRIC", "confidence": 0.9996931552886963}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.999563992023468}, {"text": "F 0.5", "start_pos": 175, "end_pos": 180, "type": "METRIC", "confidence": 0.984275221824646}]}], "tableCaptions": [{"text": " Table 1: Examples of a syntactic error and a semantic error", "labels": [], "entities": []}]}