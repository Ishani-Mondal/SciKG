{"title": [], "abstractContent": [{"text": "This paper discusses an empirical refoundation of selected Greenbergian word order universals based on a data analysis of the Universal Dependencies project.", "labels": [], "entities": []}, {"text": "The nature of the data we work on allows us to extract rich details for testing well-known typological universals and constitutes therefore a valuable basis for validating Greenberg's universals.", "labels": [], "entities": []}, {"text": "Our results show that we can refine some Greenbergian universals in a more empirical and accurate way by means of a data-driven typological analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern research in the field of language typology, mostly based on, focuses lesson lexical similarity and relies rather on various structural linguistic indices for language classification and generally puts much emphasis on the syntactic word order of some grammatical relations in a sentence ().", "labels": [], "entities": [{"text": "language classification", "start_pos": 165, "end_pos": 188, "type": "TASK", "confidence": 0.7545153498649597}]}, {"text": "Considered as the founder of word order typology, proposed 45 linguistic universals and 28 of them refer to the relative position of syntactic units, such as the linear relative order of subject, object, and verb in a sentence.", "labels": [], "entities": [{"text": "word order typology", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7612004081408182}]}, {"text": "A more empirical way of examining word order typologies, testing correlations between two binary grammatical relations such as OV vs. VO and SV vs. VS, can be found in Dryer (1992) (following, in which, some detailed word order correlations based on a sample of 625 languages are reported.", "labels": [], "entities": []}, {"text": "It is noteworthy that the field of word order typology has a strong empirical tradition, working with data and trying to describe the data with great precision.", "labels": [], "entities": [{"text": "word order typology", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7326838672161102}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9748177528381348}]}, {"text": "From a perspective of data analysis, new language data is emerging everyday in this so-called era of 'big data'.", "labels": [], "entities": [{"text": "data analysis", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.7442065179347992}]}, {"text": "It has never been a better moment than today to challenge, test, and corroborate existing ideas based on better and bigger data.", "labels": [], "entities": []}, {"text": "With the appearance of larger sets of treebanks, research has begun to test existing word order typology claims or hypothesis based on treebank data.", "labels": [], "entities": []}, {"text": "Investigating treebanks of 20 languages, Liu (2010) tested the 'traditional' typological claims with the subject-verb, object-verb and adjective-noun data extracted from the treebanks, with coherent results, also showing that these 20 languages can be arranged on a continuum with absolute head-initial and head-final patterns at the two ends.", "labels": [], "entities": []}, {"text": "Liu further states that treebank based methods will be able to provide more complete and fine-grained typological analyses, while previous methods usually had to settle fora focus on basic word order phenomena.", "labels": [], "entities": []}, {"text": "These new resources allow reviewing and verifying well-known typological claims based on annotations of authentic texts ().", "labels": [], "entities": []}, {"text": "The Universal Dependencies project (UD,, the basis of the present study, has seen a rapid growth into its present ample size with more than 140 treebanks of about 85 different lan-guages.", "labels": [], "entities": []}, {"text": "UD has been developed with the goal of facilitating multilingual parser development, crosslingual learning, and parsing research from a perspective of language typology ).", "labels": [], "entities": [{"text": "multilingual parser development", "start_pos": 52, "end_pos": 83, "type": "TASK", "confidence": 0.7596853574117025}, {"text": "parsing research", "start_pos": 112, "end_pos": 128, "type": "TASK", "confidence": 0.904394119977951}]}, {"text": "The annotation scheme is an attempt to unify previous dependency treebank developments based on an evolution of (universal) Stanford dependencies (de ), Google universal part-ofspeech tags, and the Interset interlingua for morphosyntactic tagsets.", "labels": [], "entities": []}, {"text": "The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing language-specific extensions when necessary.", "labels": [], "entities": []}, {"text": "UD expects the schema, as well as the treebank data, to be \"satisfactory on linguistic analysis grounds for individual languages\", and at the same time, to be appropriate for linguistic typology, i.e., to provide \"a suitable basis for bringing out cross-linguistic parallelism across languages and language families\".", "labels": [], "entities": [{"text": "UD", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9589623212814331}]}, {"text": "One outstanding advantage of using this data set for language typology studies is the sheer size of the data set: we worked on UD 2.2, which includes 110 treebanks in over 70 languages.", "labels": [], "entities": [{"text": "UD 2.2", "start_pos": 127, "end_pos": 133, "type": "DATASET", "confidence": 0.7691485583782196}]}, {"text": "As all UD treebanks use the same annotation scheme, the database provides rich informative evidence that can be easily compared and interpreted across authentic texts of various languages.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.7050141245126724}]}, {"text": "Following, this paper aims to test well-known existing word-order universals based on the data analysis of a set of uniformly annotated texts of diverse languages.", "labels": [], "entities": []}, {"text": "Even though the set of languages of UD is currently not well-balanced in terms of language diversity (half of the languages of the database are Indo-European languages and non-Indoeuropean treebanks are often too small to betaken into account for some measures; cf.,,,,, Dik (2010) on language sampling) and the results will have to be confirmed in the future on an even wider collection of languages, this resource allows us to have anew take on the question of language universals.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce dependency treebanks and explain amendments of the current annotation scheme that were necessary to obtain typologically relevant data.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss and compare some of Greenberg's (1963) Universals with our results.", "labels": [], "entities": []}, {"text": "In the conclusion, we discuss the potential of using UD treebanks for future typological studies.", "labels": [], "entities": [{"text": "UD treebanks", "start_pos": 53, "end_pos": 65, "type": "DATASET", "confidence": 0.7508121132850647}]}], "datasetContent": [{"text": "In order to evaluate the suitability of the delexicalized models for the processing of our target language, we created an evaluation sample in Occitan.", "labels": [], "entities": []}, {"text": "This sample contains around 1000 tokens from 4 newspaper texts, 3 of which are in Lengadocian and 1 in Gascon (cf.).", "labels": [], "entities": []}, {"text": "The sample is tagged with UD POS tags, obtained by a conversion from an existing Occitan corpus which was tagged manually using EAGLES and GRACE tagging standards ( At the moment, the syntactic annotation is limited to first-level dependency labels (no complex syntactic labels).", "labels": [], "entities": [{"text": "EAGLES", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.788636326789856}]}, {"text": "This is due to the fact that the annotation of this evaluation sample was in fact the first round of syntactic annotation in the project.", "labels": [], "entities": []}, {"text": "It was therefore used to test and refine the general UD guidelines, but also to gather information as to which two-level labels maybe necessary.", "labels": [], "entities": [{"text": "UD", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.7690893411636353}]}, {"text": "The result of this analysis will be included in the next round of annotation.", "labels": [], "entities": []}, {"text": "The syntactic annotation of the sample was done manually using the brat annotation tool ( . Each text was processed by one annotator who had extensive experience with dependency syntax, UD guidelines and the annotation interface (although not on Occitan), and one novice.", "labels": [], "entities": []}, {"text": "The inter-annotator agreement on the sample in terms of Cohen's kappa (excluding punctuation marks) is 88.1.", "labels": [], "entities": []}, {"text": "This can be considered as a solid result given that this was the very first cycle of annotation.", "labels": [], "entities": []}, {"text": "All disagreements were resolved in an adjudication process, resulting in a gold-standard annotated sample.", "labels": [], "entities": []}, {"text": "The goal of this first evaluation was to establish the baseline results for each model.", "labels": [], "entities": []}, {"text": "This baseline was to be used to assess the stability of the models when transferred to Occitan.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The corpus names contain the language code and the name of the corpus in lowercase.", "labels": [], "entities": []}, {"text": "Parsing results are given as LAS and UAS . The top 5 models in terms of the LAS are highlighted in bold.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.774975597858429}, {"text": "LAS", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9747666716575623}, {"text": "UAS", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.49247878789901733}, {"text": "LAS", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9497296810150146}]}, {"text": "For the following step, we selected the best performing model for each of the languages in the top 5 (it_isdt, fr_partut+gsd+sequoia, pt_bosque) and used them to pre-annotate new Occitan samples.", "labels": [], "entities": []}, {"text": "It is important to note that the difference in scores between it_isdt and it_idst+partut is explained by different annotation of 3 tokens when it comes to LAS, and 1 token when it comes to UAS, whereas the difference between it_isdt+partut and e.g. pt_bosque is much more important.", "labels": [], "entities": [{"text": "UAS", "start_pos": 189, "end_pos": 192, "type": "DATASET", "confidence": 0.7755507230758667}]}, {"text": "However, we preferred having models based on different languages and comparing their performances rather than adhering strictly to the quantitative results.", "labels": [], "entities": []}, {"text": "The experiments in this paper are organised in two parts: the experiments with an extended training corpus on the level of morphosyntax and lemma and the experiments on adding an inflectional lexicon to the lemmatization process.", "labels": [], "entities": []}, {"text": "While we perform experiments on the levels of morphosyntax, lemma and dependency syntax, we use gold segmentation to simplify our experiments as different tokenisers and sentence splitters are available for the two languages in question.", "labels": [], "entities": [{"text": "sentence splitters", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7574671804904938}]}, {"text": "Performing different preprocessing on the two languages would blur our experiments.", "labels": [], "entities": []}, {"text": "On the other hand, applying the out-of-the box segmentation of stanfordnlp would produce results that are detrimental to those of our rule-based tokenizers and sentence splitters.", "labels": [], "entities": [{"text": "sentence splitters", "start_pos": 160, "end_pos": 178, "type": "TASK", "confidence": 0.7246882021427155}]}, {"text": "Overall, our previous experiments show that true segmentation deteriorates the results slightly on all levels of annotation, but that relations between results of different systems or setups hold regardless of whether gold or true segmentation is used.", "labels": [], "entities": []}, {"text": "When performing training and evaluation on levels of lemmatization and dependency syntax, we preannotate all the three data portions (train, dev and test) with the models from the upstream levels.", "labels": [], "entities": []}, {"text": "We therefore apply morphosyntactic models on the data to be used for training and evaluating lemmatization, and we apply morphosyntactic tagging and lemmatization before training and evaluating dependency parsing models.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.7661750316619873}]}, {"text": "While it is to be expected that training and applying the models on the training data will give an https://github.com/clarinsi/babushka-bench 8 For both languages, the babushka split of data with full UD annotations differs from the official UD data releases, which are advised not to change across UD releases.", "labels": [], "entities": [{"text": "UD data releases", "start_pos": 242, "end_pos": 258, "type": "DATASET", "confidence": 0.8442945877710978}]}, {"text": "However, baseline experiment results for both data split versions remain comparable (see Section 4).", "labels": [], "entities": []}, {"text": "We do not consider improving morphosyntactic annotation via the inflectional lexicon in this paper as initial experiments have shown that various approaches to simple application of the inflectional lexicon (via lookup) do not yield any improvements.", "labels": [], "entities": []}, {"text": "Exploiting the inflectional lexicon, probably while training the morphosyntactic annotation model, is left for future work.", "labels": [], "entities": []}, {"text": "Readers interested in the comparison between the various segmenters should investigate the results published at https: //github.com/clarinsi/babushka-bench unrealistically good automatic annotation of the training data, our intuition is that, given that development data can be considered realistically annotated, the final impact of this simplifying solution (jack-knifing, i.e. annotating the training data via cross-validation would bean alternative) on the quality of annotation of the test (or any other) data will be minimal, if any.", "labels": [], "entities": []}, {"text": "Simply preannotating training data with the model trained on that same data was also the approach taken by the developers of stanfordnlp during the CoNLL 2018 shared task (.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 148, "end_pos": 170, "type": "DATASET", "confidence": 0.8410200923681259}]}, {"text": "The experiments on using a larger dataset for training the morphosyntactic tagging and lemmatization models, for which we expect to have a positive impact on the parsing quality, are split into two main parts: (1) training and evaluating morphosyntactic tagging and lemmatization models on the UD data and on all the available data, and (2) applying both models as pre-processing for training and evaluating models for dependency parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 162, "end_pos": 169, "type": "TASK", "confidence": 0.9711830019950867}, {"text": "UD data", "start_pos": 294, "end_pos": 301, "type": "DATASET", "confidence": 0.8129760921001434}, {"text": "dependency parsing", "start_pos": 419, "end_pos": 437, "type": "TASK", "confidence": 0.8236706554889679}]}, {"text": "The experiments on using the inflectional lexicon for improving lemmatization by extending the lookup method on the external lexicon, consist, similarly, of the experiments on training and evaluating the lemmatization models based on UD and all the available data, both with and without the lexicon, and inspecting the impact of the improved lemmatization on the parsing quality.", "labels": [], "entities": []}, {"text": "We evaluate all approaches with the evaluation script of the CoNLL 2018 shared task (, reporting F1 on all relevant levels, these being LEMMA, UPOS, XPOS, FEATS scores for morphology.", "labels": [], "entities": [{"text": "CoNLL 2018 shared task", "start_pos": 61, "end_pos": 83, "type": "DATASET", "confidence": 0.8871262669563293}, {"text": "F1", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9864115715026855}, {"text": "LEMMA", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.954168975353241}, {"text": "XPOS", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.7241992354393005}, {"text": "FEATS", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9931661486625671}]}, {"text": "For dependency syntax, the standard unlabelled (UAS) and labelled (LAS) attachment scores are complemented with the recently proposed morphology-aware labelled attachment score (MLAS), which also takes part-of-speech tags and morphological features into account and treats function words as features of content words, and bi-lexical dependency score (BLEX), which is similar to MLAS, but also incorporates lemmatization.", "labels": [], "entities": [{"text": "morphology-aware labelled attachment score (MLAS)", "start_pos": 134, "end_pos": 183, "type": "METRIC", "confidence": 0.7458892379488263}, {"text": "bi-lexical dependency score (BLEX)", "start_pos": 322, "end_pos": 356, "type": "METRIC", "confidence": 0.7656851063172022}]}, {"text": "For evaluation, we use only the UD portions of the test datasets to keep the numbers obtained on the UD data and the extended data as comparable as possible.", "labels": [], "entities": [{"text": "UD data", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.8146853148937225}]}, {"text": "We manually annotated a set of 50 sentences held out from the rule generation process to evaluate the quality of the converter.", "labels": [], "entities": [{"text": "rule generation", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7338665723800659}]}, {"text": "The sentences were chosen by randomly sampling the part B of the HDT and also used as validation in.", "labels": [], "entities": [{"text": "HDT", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9383121132850647}]}, {"text": "Our manual annotations differ slightly because of changes to the current UD standards.", "labels": [], "entities": [{"text": "UD standards", "start_pos": 73, "end_pos": 85, "type": "DATASET", "confidence": 0.8416943550109863}]}, {"text": "Punctuation marks (84 out of the 782 tokens) were ignored during this comparison.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9767712354660034}]}, {"text": "Looking at the 698 remaining converted words, 558 matched the hand-annotated dependency relations, leaving 127 words not matching the gold standard.", "labels": [], "entities": []}, {"text": "Further analysis showed that the dependency label was actually correct in 88 of these cases but the transducer gave the dependency relation an additional (correct) subtype which was not given in the set of hand-annotated sentences.", "labels": [], "entities": []}, {"text": "Some of the hand-annotated relations turned out to be wrong when comparing them to the result from the transducer.", "labels": [], "entities": []}, {"text": "In the end, neglecting the punctuation marks, for 679 out of 698 words the automatically converted annotations matched the corrected hand-annotations, yielding a labeled accuracy of 97.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.7955402731895447}]}, {"text": "We further evaluated the conversion as performed by: by critically looking through 100 randomly selected sentences and checking for annotation and conversion errors.", "labels": [], "entities": [{"text": "conversion", "start_pos": 25, "end_pos": 35, "type": "TASK", "confidence": 0.9731964468955994}]}, {"text": "The resulting accuracy confirms the previous evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999626874923706}]}, {"text": "Overall, 71% of the evaluated sentences where converted without any errors and 1506 of the 1548 non-punctuation dependencies where converted correctly, again yielding an accuracy of 97.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9993317723274231}]}, {"text": "This accuracy is significantly higher than other reported conversion accuracies; Seddah et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9994363188743591}]}, {"text": "(2018) e. g. report a labeled conversion accuracy of 94.75% and 93.27% on their held-out sets, which is twice the amount of labeled errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.5809608101844788}]}, {"text": "For our initial set of experiments we trained models that used the top 4 RLTs on the stack, and the front 4 on the buffer as input features to the feed forward hidden layer.", "labels": [], "entities": []}, {"text": "We compare our results initially to those of , who used Stack-LSTMs, and, who used Hierarchical Tree-LSTMs, since they are the closest in the literature to our approach.", "labels": [], "entities": []}, {"text": "We make a more complete comparison with state of the art Transition-based parsers in table 3.", "labels": [], "entities": []}, {"text": "Recursive representation was used by  to represent elements on the stack, similar to our approach.", "labels": [], "entities": []}, {"text": "However, their representation is computed through the recursive application of a feedforward composition function that encodes a (head, relation, dependent) tuple, encoding children in the order in which they are reduced.", "labels": [], "entities": []}, {"text": "uses a bottom up recursive approach to build a tree representation as well, but separates the sequence of children into a left and aright sequence, with the head itself being the start of both sequences, and the final representation of the subtree being a concatenation of the output of both sequences.", "labels": [], "entities": []}, {"text": "As in our work, use bi-LSTM vectors to represent words being input to the encoding LSTM.", "labels": [], "entities": []}, {"text": "When setting h v to be the concatenation of the word and pos vectors, the resulting accuracy score, shown in table 1, approaches the performance of  and.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9994971752166748}]}, {"text": "Using bi-lstm contextualized representation ash v , however, significantly improves accuracy to 94.26/92.01 on the development set and beating both of our baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9992942810058594}]}, {"text": "Our second set of experiments were to investigate whether or not RLTs retain the properties of the bilstm representation in addition to its own, i.e., produce an h \u03c4 that can represent a token's special position in a sentence in addition to representing it as the head of its own subtree.", "labels": [], "entities": []}, {"text": "The results shown thus far are the results of a wide feature set, the first 4 items on both structures {s 0\u22123 , b 0\u22123 }, which is comparable to earlier feature sets used by and, but without the need for structural features, such as left-most and right-most dependents which are already encoded in the way a tree vector is produced.", "labels": [], "entities": []}, {"text": "The results in show the performance of our RLT models on increasingly small feature sets.", "labels": [], "entities": []}, {"text": "This second set of experiments show that RLTs are also able to represent contextual information about the node from the bi-lstm layer in addition to its own structural information.", "labels": [], "entities": []}, {"text": "Interestingly the drop in the accuracy of RLTs with the complete removal of buffer features is limited.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9994576573371887}, {"text": "RLTs", "start_pos": 42, "end_pos": 46, "type": "TASK", "confidence": 0.4575888216495514}]}, {"text": "Our minimal feature set here consists of only the top 2 items on the stack {s 0,1 }.", "labels": [], "entities": []}, {"text": "These 2 elements represent the fundamental task of an Arc-Standard parser, which is to decide whether or not these 2 words are related, and so are not themselves contextual features.", "labels": [], "entities": []}, {"text": "In order to observe the effect of the changes on the parsing performance of the IMST-UD Treebank, a transition-based LSTM dependency parser (, which is a morphologically enhanced version of  and a state-of-the-art graph-based neural parser () are trained on the previous and updated versions of the treebank separately.", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.9660611152648926}, {"text": "IMST-UD Treebank", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.9624192118644714}, {"text": "LSTM dependency parser", "start_pos": 117, "end_pos": 139, "type": "TASK", "confidence": 0.611599733432134}]}, {"text": "Both projective and nonprojective dependencies are included in the training and test phases, in contrast to many past studies on the IMST-UD Treebank as well as on its previous versions that used only the projective dependencies.", "labels": [], "entities": [{"text": "IMST-UD Treebank", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.9623901546001434}]}, {"text": "51,022 advcl , 351 iobj , and a total of 79 for xcomp, dislocated, orphan, clf, goeswith, and dep.", "labels": [], "entities": [{"text": "xcomp", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.893581211566925}]}, {"text": "The training part of both versions of the treebank includes 3,685 annotated sentences and the development and test parts include 975 annotated sentences each.", "labels": [], "entities": []}, {"text": "That is, we used the original training/development/test partition of the treebank in all of our experiments.", "labels": [], "entities": []}, {"text": "As the pre-trained word vectors, we used the Turkish word embeddings of the CoNLL-17 pre-trained word embeddings from . In the evaluation of the dependency parser, we used word-based unlabeled attachment score (UAS) and labeled attachment score (LAS) metrics, where the UAS is measured as the percentage of words that are attached to the correct head, and the LAS is defined as the percentage of words that are attached to the correct head with the correct dependency type.", "labels": [], "entities": [{"text": "CoNLL-17", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9213414192199707}, {"text": "word-based unlabeled attachment score (UAS)", "start_pos": 172, "end_pos": 215, "type": "METRIC", "confidence": 0.7243333416325706}, {"text": "labeled attachment score (LAS)", "start_pos": 220, "end_pos": 250, "type": "METRIC", "confidence": 0.8171151429414749}, {"text": "UAS", "start_pos": 270, "end_pos": 273, "type": "METRIC", "confidence": 0.9668768644332886}, {"text": "LAS", "start_pos": 360, "end_pos": 363, "type": "METRIC", "confidence": 0.9900201559066772}]}, {"text": "From the experiment results, we observe that the performances of the parsers in finding correct headdependent relations increase on the updated version of the IMST-UD Treebank.", "labels": [], "entities": [{"text": "IMST-UD Treebank", "start_pos": 159, "end_pos": 175, "type": "DATASET", "confidence": 0.972958892583847}]}, {"text": "Although the number of unique dependency tags increased from 33 to 41 with the newly introduced 8 dependency types mentioned in the previous section, the labeled attachment score remains almost the same on the updated version for the transition-based parser and increases for the graph-based parser.", "labels": [], "entities": [{"text": "labeled attachment score", "start_pos": 154, "end_pos": 178, "type": "METRIC", "confidence": 0.7196901043256124}]}, {"text": "Sentence (9) shows an example sub-sentence from the previous version of the treebank and its correct annotation in the updated version.", "labels": [], "entities": []}, {"text": "Previously, the dependency tag between sa\u011flanan and finansman\u0131 was nmod although the appropriate tag would be acl.", "labels": [], "entities": [{"text": "nmod", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9451560974121094}, {"text": "acl", "start_pos": 110, "end_pos": 113, "type": "METRIC", "confidence": 0.9759714007377625}]}, {"text": "The trained parsers predict this dependency tag as acl.", "labels": [], "entities": []}, {"text": "However, this prediction is counted as false when the previous treebank version is used.", "labels": [], "entities": []}, {"text": "In the updated version, such errors were corrected, leading to a better accuracy in terms of parsing of the treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9994852542877197}, {"text": "parsing", "start_pos": 93, "end_pos": 100, "type": "TASK", "confidence": 0.9725605249404907}]}, {"text": "Karelian languages are languages closely related to Finnish spoken mainly in the republic of Karelia in Russia and surroundings.", "labels": [], "entities": []}, {"text": "The languages are split in the ISO 639-3 standard between a few language codes: Karelian (krl) and Livvi or Olonets karelian (olo) for the two main branches of the language.", "labels": [], "entities": []}, {"text": "The fact that 'krl' is commonly refered to as just Karelian can be confusing because 'olo' is also Karelian but I try to make the distinction clear throughout the article by using the ISO codes when necessary.", "labels": [], "entities": []}, {"text": "The division is not totally unproblematic but I have followed it in the treebank for ease of development and use.", "labels": [], "entities": []}, {"text": "There are some 35,000 native speakers of Karelian (krl) 1 and 31,000 for Livvi (olo) 2 according to Ethnologue, and both are classified as \"Developing\".", "labels": [], "entities": [{"text": "Ethnologue", "start_pos": 100, "end_pos": 110, "type": "DATASET", "confidence": 0.7706016302108765}]}, {"text": "The languages are developed enough to have some grammars), dictionaries and books written, as well as some regular newspapers and broadcasts, but very few digital or computational resources so far.", "labels": [], "entities": []}, {"text": "For unannotated corpora I have found a source with freely usable texts classified according to ISO language codes.", "labels": [], "entities": []}, {"text": "This paper discusses creation and ongoing work for two Karelian treebanks and compatible morphological parsers.", "labels": [], "entities": []}, {"text": "The first part of the Karelian data will be included in the 2.4 release of the Universal Dependencies and I hope to enlarge and verify the data with native informants as well as include the Livvi data by the next release.", "labels": [], "entities": [{"text": "Karelian data", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.7616392374038696}, {"text": "2.4 release of the Universal Dependencies", "start_pos": 60, "end_pos": 101, "type": "DATASET", "confidence": 0.6656825840473175}, {"text": "Livvi data", "start_pos": 190, "end_pos": 200, "type": "DATASET", "confidence": 0.97967728972435}]}, {"text": "The treebanks were named under the abbreviation of KKPP or Karjalan kielten puupankit which is Finnish for Karelian treebanks.", "labels": [], "entities": []}, {"text": "The rest of the article is organised as follows: in Section 2 I describe the languages and our goals for the treebanking, in Section 3 I describe the tools and methods for building treebanks, in Section 4 I describe the corpus selection and finally in Section 5 I summarise the article and talk about future work and ideas.", "labels": [], "entities": [{"text": "summarise", "start_pos": 264, "end_pos": 273, "type": "TASK", "confidence": 0.9634414911270142}]}], "tableCaptions": [{"text": " Table 1: Occitan evaluation sample", "labels": [], "entities": [{"text": "Occitan evaluation", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8237310647964478}]}, {"text": " Table 2. The  corpus names contain the language code and the name of the corpus in lowercase. Parsing results are given  as LAS", "labels": [], "entities": [{"text": "LAS", "start_pos": 125, "end_pos": 128, "type": "METRIC", "confidence": 0.9877147674560547}]}, {"text": " Table 2: Baseline evaluation of models trained on UD Romance corpora", "labels": [], "entities": []}, {"text": " Table 3: Evaluation on the manually annotated Occitan sample. (Bold: models selected for further experi- ments.)", "labels": [], "entities": [{"text": "Occitan sample", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9082627296447754}]}, {"text": " Table 4: Results of the manual annotation of new Occitan samples", "labels": [], "entities": []}, {"text": " Table 1: Subject, verb and complement focus in Wolof.", "labels": [], "entities": []}, {"text": " Table 2: Texts and genres in WTB.", "labels": [], "entities": [{"text": "WTB", "start_pos": 30, "end_pos": 33, "type": "DATASET", "confidence": 0.9028218388557434}]}, {"text": " Table 6: Noun class numbering for Wolof", "labels": [], "entities": []}, {"text": " Table 8: Universal dependency relations in WTB", "labels": [], "entities": [{"text": "WTB", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.5512471795082092}]}, {"text": " Table 1: Size of newly available corpora for UD morphology.", "labels": [], "entities": [{"text": "UD morphology", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9669970571994781}]}, {"text": " Table 2: Size of newly available lexica for UD morphology.", "labels": [], "entities": [{"text": "UD morphology", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.9639920592308044}]}, {"text": " Table 4: Improvements in baseline stanfordnlp lemmatization, tagging and parsing performance for  Croatian and Slovenian through a larger training set for UD morphology.", "labels": [], "entities": [{"text": "tagging", "start_pos": 62, "end_pos": 69, "type": "TASK", "confidence": 0.9541037678718567}, {"text": "UD morphology", "start_pos": 156, "end_pos": 169, "type": "TASK", "confidence": 0.9272572696208954}]}, {"text": " Table 5: Improvements in baseline stanfordnlp lemmatization, tagging and parsing performance for  Croatian and Slovenian through a simple lexicon lookup for lemmatization.", "labels": [], "entities": []}, {"text": " Table 1: SO and OS frequencies in Yodish and their entropy.", "labels": [], "entities": []}, {"text": " Table 2: Mean dependency length in Yodish (original and standardized).", "labels": [], "entities": [{"text": "Mean dependency length", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9153937697410583}]}, {"text": " Table 3: Position of lexical predicate in original and standardized Yodish.", "labels": [], "entities": []}, {"text": " Table 1: The parataxis relation in English PUD distributed on subtypes.", "labels": [], "entities": []}, {"text": " Table 2: Absolute frequencies for the parataxis relation in fourteen PUD treebanks: English, Arabic, Czech, German, Spanish,", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9755639433860779}, {"text": "PUD treebanks", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8112208843231201}]}, {"text": " Table 3: How English instances of parataxis are distributed compared to instances in other PUD treebanks. Overlaps have", "labels": [], "entities": [{"text": "PUD treebanks", "start_pos": 92, "end_pos": 105, "type": "DATASET", "confidence": 0.7754550874233246}]}, {"text": " Table 1: Development set scores on WSJ (SD)  comparing between h v being a concatenation of  the tokens word/pos vectors and h v being a con- catenation of contextualized vectors.", "labels": [], "entities": [{"text": "WSJ (SD)", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.745061993598938}]}, {"text": " Table 2: Development set scores for different fea- ture sets, using a bi-lstm contextualized vector as  h v , for Forward and Bi-directional encoding.", "labels": [], "entities": []}, {"text": " Table 3: Test set scores on WSJ (SD) for some of the highest scoring Transition-based Dependency  Parsers in current literature. Contextualized vectors refer to the bi-lstm vector representation used for h v ,  and word/pos embeddings refers to the concatenation of these vectors to represent h v . 8 feats. refers to  the use of the top 4 items on the stack and buffer, 2 feats. refers to the use of the top 2 items on the stack.", "labels": [], "entities": [{"text": "WSJ (SD)", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.7477087080478668}]}, {"text": " Table 4: Test set scores on 7 corpuses from the CONLL'18 shared task. These sets use Universal  Dependencies, and use an F1 score calculation for UAS and LAS that includes punctuation.", "labels": [], "entities": [{"text": "CONLL'18 shared task", "start_pos": 49, "end_pos": 69, "type": "DATASET", "confidence": 0.8939688404401144}, {"text": "F1 score calculation", "start_pos": 122, "end_pos": 142, "type": "METRIC", "confidence": 0.9702723423639933}]}, {"text": " Table 1: The number of alterations that we made for the most frequent changes.", "labels": [], "entities": []}, {"text": " Table 2: UAS and LAS scores of the two parsers on the previous and updated versions of the IMST-UD  treebank.", "labels": [], "entities": [{"text": "UAS", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9268554449081421}, {"text": "LAS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9789074063301086}, {"text": "IMST-UD  treebank", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.9480251967906952}]}, {"text": " Table 1: Ellipsis Types in the original BTB and the number of their occurrences.", "labels": [], "entities": [{"text": "BTB", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7770742177963257}]}, {"text": " Table 1: The sizes of analysers of Uralic languages.", "labels": [], "entities": []}, {"text": " Table 2: The sizes of treebanks of Uralic languages. Dependency trees is number of annotated sentences  and syntactic words as defined in UD guidelines.", "labels": [], "entities": []}]}