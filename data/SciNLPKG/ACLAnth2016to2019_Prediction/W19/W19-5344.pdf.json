{"title": [{"text": "The LMU Munich Unsupervised Machine Translation System for WMT19", "labels": [], "entities": [{"text": "LMU", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.9050739407539368}, {"text": "Machine Translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.7554628252983093}, {"text": "WMT19", "start_pos": 59, "end_pos": 64, "type": "TASK", "confidence": 0.8205744624137878}]}], "abstractContent": [{"text": "We describe LMU Munich's machine translation system for German\u2192Czech translation which was used to participate in the WMT19 shared task on unsupervised news translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.6444506198167801}, {"text": "German\u2192Czech translation", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.6398112624883652}, {"text": "WMT19 shared task on unsupervised news translation", "start_pos": 118, "end_pos": 168, "type": "TASK", "confidence": 0.5640195650713784}]}, {"text": "We train our model using monolingual data only from both languages.", "labels": [], "entities": []}, {"text": "The final model is an unsupervised neural model using established techniques for unsupervised translation such as denoising autoencoding and online back-translation.", "labels": [], "entities": []}, {"text": "We bootstrap the model with masked language model pretraining and enhance it with back-translations from an un-supervised phrase-based system which is itself bootstrapped using unsupervised bilingual word embeddings.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe the system we developed at the LMU Munich Center for Information and Language Processing, which we used to participate in the unsupervised track of the news translation task at WMT19.", "labels": [], "entities": [{"text": "news translation task", "start_pos": 178, "end_pos": 199, "type": "TASK", "confidence": 0.7638498644034067}, {"text": "WMT19", "start_pos": 203, "end_pos": 208, "type": "DATASET", "confidence": 0.6793653964996338}]}, {"text": "The system builds on our last year's submission to the unsupervised shared task ( and previous work on unsupervised machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7443272471427917}]}, {"text": "We submitted system runs for the German\u2192Czech translation direction.", "labels": [], "entities": []}, {"text": "The goal of the unsupervised track is to train machine translation models without access to any bilingual or comparable monolingual data.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7156880497932434}]}, {"text": "Supervised neural machine translation (NMT) has achieved state-of-the-art results (.", "labels": [], "entities": [{"text": "neural machine translation (NMT", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.8057234287261963}]}, {"text": "With the introduction of the Transformer () the quality of automatic translations has been significantly improved.", "labels": [], "entities": []}, {"text": "However, a prerequisite for high performance has been access to large scale bilingual data.", "labels": [], "entities": []}, {"text": "Naturally, this is not available for many language pairs and specific domains.", "labels": [], "entities": []}, {"text": "Moreover, also show that in low-resource setups neural models fail to match traditional phrasebased systems in terms of quality.", "labels": [], "entities": []}, {"text": "This is the motivation for the unsupervised track at WMT19.", "labels": [], "entities": [{"text": "WMT19", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9457507729530334}]}, {"text": "The system we use to participate in the shared task is multipart and borrows on existing techniques for unsupervised learning.", "labels": [], "entities": []}, {"text": "We make use of bilingual word embeddings (BWE), phrase-based translation (PBT), cross-lingual masked language models (MLM) and NMT models, all trained in an unsupervised way. and showed that, given proper bootstrapping, it is possible to train unsupervised NMT models by making use of two general techniques, denoising autoencoding and online backtranslation. and further showed that this is also possible for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based translation (PBT)", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8059764564037323}, {"text": "phrase-based statistical machine translation", "start_pos": 410, "end_pos": 454, "type": "TASK", "confidence": 0.577382043004036}]}, {"text": "A key technique that enables this is obtaining word-by-word translations by utilizing unsupervised bilingual word embeddings.", "labels": [], "entities": []}, {"text": "further simplified the bootstrapping step by showing that jointly trained BPE-level embeddings area better alternative, assuming closely related languages that potentially share surface forms.", "labels": [], "entities": []}, {"text": "also showed that a single shared encoder and decoder are sufficient for learning both translation directions.", "labels": [], "entities": []}, {"text": "A general trend in NLP recently has been unsupervised masked language model pretraining.", "labels": [], "entities": [{"text": "masked language model pretraining", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.6360442712903023}]}, {"text": "showed that a wide range of NLP tasks are significantly improved by fine-tuning large MLM.", "labels": [], "entities": []}, {"text": "They propose away to train a Transformer language model which has access to left and right context as opposed to traditional LM which only have left context access.", "labels": [], "entities": []}, {"text": "extended the approach to a multilingual setting and showed that this vastly outperforms the previous approaches for bootstrapping NMT models.", "labels": [], "entities": []}, {"text": "The model we used to participate in the shared task makes use of several of the aforementioned techniques.", "labels": [], "entities": []}, {"text": "We train unsupervised BWEs and use them to bootstrap an unsupervised PBT model.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.8137154579162598}]}, {"text": "We use large scale German and Czech monolingual NewsCrawl data to train a cross-lingual masked language model in order to bootstrap our unsupervised NMT model which itself is trained using denoising autoencoding and online backtranslation.", "labels": [], "entities": [{"text": "NewsCrawl data", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.8459234833717346}]}, {"text": "We combine all of these techniques and obtain competitive results in the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned earlier we initialize our PBT system with BWEs trained on compound split data.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8303194642066956}]}, {"text": "In we show baseline word-by-word (wbw) results, i.e., we greedily translate each source word independently of the others using the most similar target word, according to the BWE-based dictionary, without any reordering.", "labels": [], "entities": [{"text": "BWE-based dictionary", "start_pos": 174, "end_pos": 194, "type": "DATASET", "confidence": 0.9639192223548889}]}, {"text": "We compare BWEs trained with and without compound split data.", "labels": [], "entities": []}, {"text": "The results of both approaches are low, which is due to the morphological richness of the target lan-  guage.", "labels": [], "entities": []}, {"text": "On one hand, based on manual investigation 5 of the BWE-based dictionary and the sentence translations, we conclude that the various inflected forms of the correct Czech stems are often the most similar translations of given German words.", "labels": [], "entities": [{"text": "BWE-based dictionary", "start_pos": 52, "end_pos": 72, "type": "DATASET", "confidence": 0.976816713809967}]}, {"text": "On the other hand, without the context it is much harder to pick the right form as opposed to some other language pairs such as German and English.", "labels": [], "entities": []}, {"text": "Compound splitting resulted in performance increase of the system which is due to the translation of German compounds to multiple Czech words.", "labels": [], "entities": [{"text": "Compound splitting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.665784940123558}]}, {"text": "In addition, it also helped lowering the number of Out-Of-Vocabulary (OOV) words which is partly due to limiting the size of the vocabulary.", "labels": [], "entities": []}, {"text": "also presents the results from our PBT system.", "labels": [], "entities": [{"text": "PBT system", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.8618541061878204}]}, {"text": "At iteration 0 the model obtains 6.0 BLEU on newstest2013.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9979385733604431}, {"text": "newstest2013", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.9704787731170654}]}, {"text": "The score increased to 7.9 BLEU at iteration 1 and to 8.4 at iteration 2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9990063309669495}]}, {"text": "We show the results from our unsupervised neural model and the combination with synthetic data from the phrase-based system.", "labels": [], "entities": []}, {"text": "Our primary submission at WMT19 has achieved competitive results despite using a single model with no ensembling.", "labels": [], "entities": [{"text": "WMT19", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.8717527389526367}]}, {"text": "The model for the primary submission was trained for \u223c12h due to time constraints.", "labels": [], "entities": []}, {"text": "For the contrastive experiments we present in we further trained this model for \u223c62h overall.", "labels": [], "entities": []}, {"text": "We train the models on 8 Nvidia GTX 1080 Ti with 12 GB RAM.", "labels": [], "entities": [{"text": "Nvidia GTX 1080 Ti", "start_pos": 25, "end_pos": 43, "type": "DATASET", "confidence": 0.9305442124605179}, {"text": "RAM", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9417319297790527}]}, {"text": "We present results on newstest2013.", "labels": [], "entities": [{"text": "newstest2013", "start_pos": 22, "end_pos": 34, "type": "DATASET", "confidence": 0.9824052453041077}]}, {"text": "For model selection we used newstest2009.", "labels": [], "entities": [{"text": "model selection", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.651929646730423}, {"text": "newstest2009", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9530221819877625}]}, {"text": "The first row in shows our baseline unsupervised neural Note that none of the authors speak the target language.   system.", "labels": [], "entities": []}, {"text": "This model achieves significant improvements over the word-by-word approach and PBT system.", "labels": [], "entities": []}, {"text": "All results except for the unsup.", "labels": [], "entities": [{"text": "unsup", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9841405153274536}]}, {"text": "NMT baseline are obtained by applying compound splitting to the German input from newstest2013.", "labels": [], "entities": [{"text": "German input from newstest2013", "start_pos": 64, "end_pos": 94, "type": "DATASET", "confidence": 0.8732779920101166}]}, {"text": "We present the result for the baseline without compound splitting because the initial cross-lingual MLM and unsupervised NMT system were trained with German monolingual data which was not compound split.", "labels": [], "entities": []}, {"text": "However, the BWEs and PBT system were trained with compound split German monolingual data and as a result the German back-translations we obtain from the PBT system were compound split.", "labels": [], "entities": [{"text": "BWEs", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.9462815523147583}]}, {"text": "Consequently, all contrastive models where we fine-tune the original unsupervised NMT system are trained with compound split German monolingual data.", "labels": [], "entities": []}, {"text": "However, we do not observe any adverse effects on translation quality.", "labels": [], "entities": []}, {"text": "Furthermore, the results from the fine-tuned models show that very similar results are obtained with both versions of the test set.", "labels": [], "entities": []}, {"text": "When fine-tuning our model with PBT synthetic data, we disable denoising autoencoding, but continue to do online back-translation.", "labels": [], "entities": [{"text": "PBT synthetic data", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.7538498242696127}]}, {"text": "Even though we used PBT synthetic data from iteration 0, we observe significant improvements.", "labels": [], "entities": [{"text": "PBT synthetic data", "start_pos": 20, "end_pos": 38, "type": "DATASET", "confidence": 0.7891730070114136}]}, {"text": "We fine-tune the model for \u223c62h and BLEU score was improved from 17.0 to 18.5.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9575443267822266}]}, {"text": "We use this model for the primary submission, but aversion which was trained for \u223c12h only.", "labels": [], "entities": []}, {"text": "We intuitively assumed that removing this data and continuing training with online back-translation only would further improve performance.", "labels": [], "entities": []}, {"text": "However, we observe that BLEU score decreased to 18.3.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9253422319889069}]}, {"text": "We also experimented with adding PBT synthetic data from iteration 1.", "labels": [], "entities": []}, {"text": "We tried adding this data as we did with the back-translations from iteration 0.", "labels": [], "entities": []}, {"text": "Furthermore, we also tried fine-tuning the model trained on iteration 0 data with data from iteration 1.", "labels": [], "entities": []}, {"text": "For this setup, the data from iteration 0 was removed.", "labels": [], "entities": []}, {"text": "It is interesting that finetuning the initial unsupervised NMT obtains better performance than fine-tuning the model trained with iteration 0 data.", "labels": [], "entities": []}, {"text": "The best score we managed to obtain was 19.1 by fine-tuning the initial unsupervised NMT with iteration 1 data and translating a compound split version of newstest2013.", "labels": [], "entities": [{"text": "NMT", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.8826461434364319}, {"text": "newstest2013", "start_pos": 155, "end_pos": 167, "type": "DATASET", "confidence": 0.866288423538208}]}, {"text": "In we show the results on newstest2019.", "labels": [], "entities": [{"text": "newstest2019", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.973018229007721}]}, {"text": "Our primary submission obtained 17.0 BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9986938834190369}]}, {"text": "Further training and including synthetic data from iteration 1 increased the score to 17.8 BLEU.", "labels": [], "entities": [{"text": "score", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.7920119762420654}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9987554550170898}]}], "tableCaptions": [{"text": " Table 2: Baseline results (BLEU) with word-by- word translations (wbw) and unsupervised phrase- based translations (PBT) on newstest2013. We com- pare wbw results with and without compound splitting  on the German language side. For the unsupervised  PBT experiments, German is compound-split.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9977719187736511}, {"text": "newstest2013", "start_pos": 125, "end_pos": 137, "type": "DATASET", "confidence": 0.9510285258293152}]}, {"text": " Table 3: BLEU scores with the unsupervised NMT sys- tems on newstest2013.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983415603637695}, {"text": "NMT sys- tems", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8394442647695541}, {"text": "newstest2013", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.548585057258606}]}, {"text": " Table 4: BLEU scores with the unsupervised NMT sys- tems on newstest2019. * -primary submission, trained  for \u223c12h.  \u2021-trained for \u223c62h.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986296892166138}, {"text": "NMT sys- tems", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.824346125125885}, {"text": "newstest2019", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.5410552024841309}]}]}