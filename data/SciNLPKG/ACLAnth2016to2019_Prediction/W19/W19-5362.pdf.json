{"title": [], "abstractContent": [{"text": "In this paper we describe our neural machine translation (NMT) systems for Japanese\u2194English translation which we submitted to the translation robustness task.", "labels": [], "entities": [{"text": "Japanese\u2194English translation", "start_pos": 75, "end_pos": 103, "type": "TASK", "confidence": 0.5356365069746971}]}, {"text": "We focused on leveraging transfer learning via fine tuning to improve translation quality.", "labels": [], "entities": [{"text": "transfer learning", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9022243618965149}, {"text": "translation", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.9576109647750854}]}, {"text": "We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et al., 2017) to improve translation quality for Japanese\u2194English.", "labels": [], "entities": []}, {"text": "We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios.", "labels": [], "entities": []}, {"text": "However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.", "labels": [], "entities": []}], "introductionContent": [{"text": "Neural machine translation (NMT) ( has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PB-SMT) (.", "labels": [], "entities": [{"text": "Neural machine translation (NMT)", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7656561980644861}, {"text": "word alignments", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.7408780455589294}, {"text": "phrase-based statistical machine translation (PB-SMT)", "start_pos": 226, "end_pos": 279, "type": "TASK", "confidence": 0.684483528137207}]}, {"text": "NMT performs well in resource-rich scenarios but badly in resource-poor ones (.", "labels": [], "entities": [{"text": "NMT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6213566660881042}]}, {"text": "One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc.", "labels": [], "entities": [{"text": "translation of noisy sentences", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.8840620666742325}]}, {"text": "There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena.", "labels": [], "entities": []}, {"text": "One of the first works on dealing with noisy translation led to the development of the MTNT ( test suite for testing MT models that are robust to noisy text.", "labels": [], "entities": [{"text": "dealing with noisy translation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.6827095821499825}]}, {"text": "Fortunately, the problem of noisy text translation can be treated as a domain adaptation problem and there is an abundant amount of Japanese-English text that be leveraged for this purpose.", "labels": [], "entities": [{"text": "noisy text translation", "start_pos": 28, "end_pos": 50, "type": "TASK", "confidence": 0.7814573645591736}]}, {"text": "In this paper, we describe the systems for Japanese\u2194English translation, that we developed and submitted for WMT 2019 under the team name \"NICT\".", "labels": [], "entities": [{"text": "Japanese\u2194English translation", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.5830162912607193}, {"text": "WMT 2019", "start_pos": 109, "end_pos": 117, "type": "TASK", "confidence": 0.5413719415664673}, {"text": "NICT", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8990256786346436}]}, {"text": "In particular our observations can be summarized as follows: Japanese\u2194English translation dramatically fails given the limited amount of noisy training data.", "labels": [], "entities": [{"text": "Japanese\u2194English translation", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.5406520217657089}]}, {"text": "Fine-Tuning is simple but has over-fitting risks.", "labels": [], "entities": []}, {"text": "Mixed-Fine-Tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7222025245428085}]}, {"text": "Kindly refer to the task overview paper ( for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the official Japanese\u2192English and English\u2192Japanese datasets provided by WMT.", "labels": [], "entities": [{"text": "WMT", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.9551666975021362}]}, {"text": "The out-of-domain (non noisy) datasets are KFTT, JESC and TED Talks, all of which are adequately described in the original MTNT paper.", "labels": [], "entities": [{"text": "KFTT", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9578558206558228}, {"text": "JESC", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8664619326591492}, {"text": "MTNT paper", "start_pos": 123, "end_pos": 133, "type": "DATASET", "confidence": 0.7779983878135681}]}, {"text": "The total number of out-ofdomain sentence pairs is 3,900,772.", "labels": [], "entities": []}, {"text": "As for the in-domain corpus, the number of training sentence pairs for Japanese\u2192English translation is 6,506 pairs and for English\u2192Japanese translation there are 5,775 pairs.", "labels": [], "entities": []}, {"text": "Upon inspection of the English\u2192Japanese data, we noted that many sentences were actually paragraphs which are almost useless for NMT training as they are trimmed to avoid out-of-memory errors.", "labels": [], "entities": [{"text": "English\u2192Japanese data", "start_pos": 23, "end_pos": 44, "type": "DATASET", "confidence": 0.5675615519285202}, {"text": "NMT training", "start_pos": 129, "end_pos": 141, "type": "TASK", "confidence": 0.8886153995990753}]}, {"text": "We tried a naive paragraph splitting method where we split a paragraphs into sentences and keep the splits if there are an equal number of sentences.", "labels": [], "entities": [{"text": "paragraph splitting", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7969914674758911}]}, {"text": "Upon manual investigation we found out that this splitting leads to correct splits most of the times.", "labels": [], "entities": []}, {"text": "As a result, the number of training sentences for English\u2192Japanese translation increases to 10,060 pairs.", "labels": [], "entities": [{"text": "English\u2192Japanese translation", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.6270313858985901}]}, {"text": "We pre-processed the Japanese text using).", "labels": [], "entities": []}, {"text": "Other than this, we do not perform any pre-processing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for Japanese\u2194English translation for the robustness task.", "labels": [], "entities": [{"text": "Japanese\u2194English translation", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.5542380213737488}]}]}