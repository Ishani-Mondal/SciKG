{"title": [{"text": "Interactive-Predictive Neural Machine Translation through Reinforcement and Imitation", "labels": [], "entities": [{"text": "Neural Machine Translation", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.6203756332397461}, {"text": "Imitation", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.47027328610420227}]}], "abstractContent": [{"text": "We propose an interactive-predictive neu-ral machine translation framework for easier model personalization using reinforcement and imitation learning.", "labels": [], "entities": [{"text": "neu-ral machine translation", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.5982239147027334}]}, {"text": "During the interactive translation process, the user is asked for feedback on uncertain locations identified by the system.", "labels": [], "entities": []}, {"text": "Responses are weak feedback in the form of \"keep\" and \"delete\" edits, and expert demonstrations in the form of \"substitute\" edits.", "labels": [], "entities": []}, {"text": "Conditioning on the collected feedback, the system creates alternative translations via constrained beam search.", "labels": [], "entities": []}, {"text": "In simulation experiments on two language pairs our systems get close to the performance of supervised training with much less human effort.", "labels": [], "entities": []}], "introductionContent": [{"text": "Despite recent success reports on neural machine translation (NMT) reaching human parity (, professional use cases of NMT require model personalization where the NMT system is adapted to user feedback provided for suggested NMT outputs.", "labels": [], "entities": [{"text": "neural machine translation (NMT)", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.8195811212062836}]}, {"text": "In this paper, we will focus on the paradigm of interactivepredictive machine translation () which has been shown to fit easily into the sequence-to-sequence model of NMT ().", "labels": [], "entities": [{"text": "interactivepredictive machine translation", "start_pos": 48, "end_pos": 89, "type": "TASK", "confidence": 0.5937547783056895}]}, {"text": "The standard interactive-predictive protocol takes a human-corrected prefix as conditioning context in predicting a sentence completion, which is again corrected or accepted by the human user.", "labels": [], "entities": [{"text": "predicting a sentence completion", "start_pos": 103, "end_pos": 135, "type": "TASK", "confidence": 0.7393883615732193}]}, {"text": "Recent work showed in simulation experiments that human effort can be reduced by asking humans for reward signals or validations of partial system outputs instead of for corrections.", "labels": [], "entities": []}, {"text": "Our goal is to combine both feedback modes -corrections and rewards -by treating them as expert demonstrations and reward values in an interactive protocol that combines imitation learning (IL)) and reinforcement learning (RL), respectively, using only limited human edits.", "labels": [], "entities": []}, {"text": "A further difference of our framework to standard interactive-predictive NMT is our use of an uncertainty criterion that reduces the amount of feedback requests to the tokens where the entropy of the policy distribution is highest.", "labels": [], "entities": []}, {"text": "This idea has been used successfully before in and and connects our work to the area of active learning.", "labels": [], "entities": []}, {"text": "Lastly, our framework differs from prior work by allowing model updates based on partial translations.", "labels": [], "entities": []}, {"text": "Our experiments show that weak feedback inform of keep/delete rewards on translation outputs yields consistent improvements of between 2.6 and 4.3 BLEU points over the pre-trained baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.999163031578064}]}, {"text": "On one language pair, it even matches the improvements gained by forcing word substitutions from reference translations into the re-decoded output.", "labels": [], "entities": []}, {"text": "Furthermore, both feedback scenarios considerably reduce human effort.", "labels": [], "entities": []}], "datasetContent": [{"text": "To demonstrate the effectiveness of our reinforcement and imitation strategies, we simulate the interactive-predictive workflow described in Section 4 in a domain adaptation setup.", "labels": [], "entities": []}, {"text": "A human translator is simulated by comparing partial translations with corresponding gold translation to extend the set of feedback rules in every round.", "labels": [], "entities": []}, {"text": "In the RL setting, the simulated human translator provides only weak feedback (KEEP and DELETE edits) on tokens generated by the system, while in the IL setting the simulated translator addition-: Data used in pre-and interactive training for French-English (fr-en) and German-English (de-en).", "labels": [], "entities": [{"text": "KEEP", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9900692105293274}, {"text": "DELETE", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9713758826255798}]}, {"text": "ally injects expert feedback (SUBSTITUTE edit) by demonstrating how the system should act at a specific time step.", "labels": [], "entities": [{"text": "SUBSTITUTE edit)", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.7139317790667216}]}, {"text": "In our simulation experiments, we focus on the uncertain tokens of the partial translation.", "labels": [], "entities": []}, {"text": "An exact match between the uncertain token and the reference generates a KEEP edit, while differing tokens generate either a DELETE or SUB-STITUTE edit depending on the type of system.", "labels": [], "entities": [{"text": "KEEP", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9161903858184814}, {"text": "DELETE", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.9585810303688049}]}, {"text": "Tokens exceeding the sentence length of the reference always receive a DELETE feedback.", "labels": [], "entities": [{"text": "DELETE", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9943405389785767}]}, {"text": "We refer to the first system as KEEP+DELETE, and the second system as +SUBSTITUTE.", "labels": [], "entities": [{"text": "KEEP", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.8864997625350952}, {"text": "DELETE", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.8765217065811157}, {"text": "SUBSTITUTE", "start_pos": 71, "end_pos": 81, "type": "METRIC", "confidence": 0.9478840827941895}]}, {"text": "While the system parameters are updated online after every such simulated interaction, system evaluation is done by a standard offline translation of an unseen test set.", "labels": [], "entities": []}, {"text": "For pre-training, we use the Europarl (EP) corpus version 5 for the French-English system, and version 7 for German-English.", "labels": [], "entities": [{"text": "Europarl (EP) corpus version 5", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.9529790111950466}]}, {"text": "For interactive training, we use the News Commentary (NC) 2006 corpus.", "labels": [], "entities": [{"text": "News Commentary (NC) 2006 corpus", "start_pos": 37, "end_pos": 69, "type": "DATASET", "confidence": 0.8731947115489415}]}, {"text": "Both corpora are publicly available on the WMT13's homepage.", "labels": [], "entities": [{"text": "WMT13's homepage", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9782010515530905}]}, {"text": "All experiments are conducted on two language pairs, i.e., GermanEnglish (de-en) and French-English (fr-en).", "labels": [], "entities": []}, {"text": "Data sets were tokenized and lowercased using MOSES preprocessing scripts ().", "labels": [], "entities": []}, {"text": "We applied compound splitting on the German source sentences using CDEC's tool (.", "labels": [], "entities": [{"text": "compound splitting", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.740297332406044}]}, {"text": "Our data sets for interactive training differ from the original News Commentary data splits as follows: (1) we sample a subset of the original training set to reduce the number of parallel sentences to 18,432 for French-English and 18,927 for GermanEnglish, and (2) we increase both validation and test set for French-English to 3,001 and 5,014 parallel sentences by moving data from the original training set excluding sentences that were sampled for training.", "labels": [], "entities": [{"text": "News Commentary data splits", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.8954961150884628}]}, {"text": "Note that a training set size of less than 19,000 parallel sentences is very small even 3 https://www.statmt.org/wmt13/ in a domain adaptation setup.", "labels": [], "entities": []}, {"text": "summarizes the statistics of our datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Character-F (ChrF), and BLEU test results on the French-English (fr-en) and German-English  (de-en) translation tasks. Highest scores on RL and IL systems are printed in bold. The \u2206 columns  indicate the score differences to the pre-trained baseline system. All scores are averaged over three runs  with standard deviation \u03c3 in parentheses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9993280172348022}, {"text": "German-English  (de-en) translation tasks", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.7457800507545471}]}]}