{"title": [{"text": "The BEA-2019 Shared Task on Grammatical Error Correction", "labels": [], "entities": [{"text": "BEA-2019", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.5171888470649719}, {"text": "Grammatical Error Correction", "start_pos": 28, "end_pos": 56, "type": "TASK", "confidence": 0.5924299458662668}]}], "abstractContent": [{"text": "This paper reports on the BEA", "labels": [], "entities": [{"text": "BEA", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.547264575958252}]}], "introductionContent": [{"text": "The Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC) continues the tradition of the previous Helping Our Own (HOO) and Conference on Natural Language Learning (CoNLL) shared tasks ( and was motivated by the need to re-evaluate the field after a five year hiatus.", "labels": [], "entities": [{"text": "Building Educational Applications (BEA) 2019 Shared Task on Grammatical Error Correction (GEC)", "start_pos": 4, "end_pos": 98, "type": "TASK", "confidence": 0.7688313331454992}]}, {"text": "Although significant progress has been made since the end of the last CoNLL-2014 shared task, recent systems have been trained, tuned and tested on different combinations of metrics and corpora ().", "labels": [], "entities": []}, {"text": "Thus one of the main aims of the BEA-2019 shared task is to once again provide a platform where systems can be reevaluated under more controlled conditions.", "labels": [], "entities": [{"text": "BEA-2019 shared task", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.7666014830271403}]}, {"text": "With this in mind, another significant contribution of the BEA-2019 shared task is the introduction of anew annotated dataset, the Cambridge English Write & Improve (W&I) and LOCNESS corpus, which is designed to represent a much wider range of English levels and abilities than previous corpora.", "labels": [], "entities": [{"text": "BEA-2019 shared task", "start_pos": 59, "end_pos": 79, "type": "DATASET", "confidence": 0.7829056779543558}, {"text": "Cambridge English Write & Improve (W&I)", "start_pos": 131, "end_pos": 170, "type": "TASK", "confidence": 0.7153574466705322}]}, {"text": "This is significant because systems have traditionally only been tested on the CoNLL-2014 test set, which only contains 50 essays (1,312 sentences) on 2 different topics written by 25 South-East Asian undergraduates ().", "labels": [], "entities": [{"text": "CoNLL-2014 test set", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.979223628838857}]}, {"text": "In contrast, the W&I+LOCNESS test set contains 350 essays (4,477 sentences) on approximately 50 topics written by 334 authors from around the world (including native English speakers).", "labels": [], "entities": [{"text": "W&I+LOCNESS test set", "start_pos": 17, "end_pos": 37, "type": "DATASET", "confidence": 0.8392024636268616}]}, {"text": "We hope that this diversity will encourage the development of systems that can generalise better to unseen data.", "labels": [], "entities": []}, {"text": "Another difference to the previous shared tasks is the introduction of tracks; namely the Restricted, Unrestricted and Low Resource track.", "labels": [], "entities": []}, {"text": "While annotated data was comparatively scarce five years ago, it has since become more available, so we can now control what resources participants have access to.", "labels": [], "entities": []}, {"text": "The Restricted track is closest to the original shared tasks, in that we specify precisely which annotated learner datasets participants should use, while the Unrestricted track allows use of any and all available datasets.", "labels": [], "entities": []}, {"text": "The Low Resource track, in contrast, significantly limits the amount of annotated data available to participants and encourages development of systems that do not rely on large quantities of humanannotated sentences.", "labels": [], "entities": []}, {"text": "A goal of the Low Resource track is thus to facilitate research into GEC for languages where annotated training corpora do not exist.", "labels": [], "entities": []}, {"text": "Like CoNLL-2014, the main evaluation metric was F 0.5 , which weights precision twice as much as recall.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 5, "end_pos": 15, "type": "DATASET", "confidence": 0.815396785736084}, {"text": "F 0.5", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9842257499694824}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.99931800365448}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9981029033660889}]}], "datasetContent": [{"text": "Systems are evaluated on the W&I+LOCNESS test set using the ERRANT scorer, an improved version of the MaxMatch scorer) that was previously used in the CoNLL shared tasks.", "labels": [], "entities": [{"text": "W&I+LOCNESS test set", "start_pos": 29, "end_pos": 49, "type": "DATASET", "confidence": 0.8888380016599383}, {"text": "ERRANT scorer", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9830056428909302}, {"text": "CoNLL shared tasks", "start_pos": 151, "end_pos": 169, "type": "TASK", "confidence": 0.6459082961082458}]}, {"text": "As in the previous shared tasks, this means that system performance is primarily measured in terms of span-based correction using the F 0.5 metric, which weights precision twice as much as recall.", "labels": [], "entities": [{"text": "F 0.5 metric", "start_pos": 134, "end_pos": 146, "type": "METRIC", "confidence": 0.9744052886962891}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9982615113258362}, {"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.9952390193939209}]}, {"text": "In span-based correction, a system is only rewarded if a system edit exactly matches a reference edit in terms of both its token offsets and correction string.", "labels": [], "entities": [{"text": "span-based correction", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7479397058486938}]}, {"text": "If more than one set of reference edits are available (there were 2 in CoNLL-2014 and 5 in BEA-2019), ERRANT chooses the reference that maximises the global F 0.5 score, or else maximises true positives and minimises false positives and false negatives.", "labels": [], "entities": [{"text": "CoNLL-2014", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.9513124227523804}, {"text": "BEA-2019", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8427286744117737}, {"text": "ERRANT", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9399939179420471}, {"text": "F 0.5 score", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.9720536271731058}]}, {"text": "ERRANT is also able to report performance in terms of span-based detection and token-based detection (.", "labels": [], "entities": [{"text": "token-based detection", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.800459623336792}]}, {"text": "Although the W&I+LOCNESS training and development sets are released as separate files for each CEFR level, the test set texts are combined and shuffled such that the sentence order in each essay is preserved, but the order of the CEFR levels is random.", "labels": [], "entities": [{"text": "W&I+LOCNESS training and development sets", "start_pos": 13, "end_pos": 54, "type": "DATASET", "confidence": 0.8010146617889404}]}, {"text": "This is done because systems should not expect to know the CEFR level of an input text in advance and should hence be prepared to handle all levels and abilities.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.6882467865943909}]}, {"text": "In Section 7, we nevertheless also report system performance in terms of different CEFR and native levels, as well as in terms of detection and error types.", "labels": [], "entities": [{"text": "CEFR", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.7763615846633911}]}], "tableCaptions": [{"text": " Table 2: W&I (A, B, C) and LOCNESS (N) corpus  statistics.", "labels": [], "entities": [{"text": "LOCNESS (N) corpus  statistics", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.6689088741938273}]}, {"text": " Table 4: The ERRANT error type distributions of the FCE, Lang-8, NUCLE and W&I+LOCNESS corpora. See  Bryant et al. (2017) for more information about each error type. The distribution of the W&I+LOCNESS test data  is averaged across all 5 annotators.", "labels": [], "entities": [{"text": "ERRANT error type", "start_pos": 14, "end_pos": 31, "type": "METRIC", "confidence": 0.9195483922958374}, {"text": "FCE", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.9640178680419922}, {"text": "Lang-8", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.8428939580917358}, {"text": "NUCLE", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8399287462234497}, {"text": "W&I+LOCNESS test data", "start_pos": 191, "end_pos": 212, "type": "DATASET", "confidence": 0.555026718548366}]}, {"text": " Table 6: Correlation between various evaluation met- rics and human judgements.", "labels": [], "entities": []}, {"text": " Table 7: Official BEA-2019 results for all teams in all tracks using the main overall span-based correction ER- RANT F 0.5 . The highest values (lowest for False Positives and False Negatives) are shown in bold.", "labels": [], "entities": [{"text": "BEA-2019", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8295580744743347}, {"text": "span-based correction ER- RANT F 0.5", "start_pos": 87, "end_pos": 123, "type": "METRIC", "confidence": 0.8454660773277283}]}, {"text": " Table 9: This table shows the performance of each team in each track in terms of Missing, Replacement and  Unnecessary token edits. In terms of frequency, approximately 25% of all edits are M, 65% are R, and 10% are U  (cf. Table 4). The highest scores for each column are shown in bold.", "labels": [], "entities": [{"text": "Missing", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9427391290664673}]}, {"text": " Table 10: Main error type ERRANT F 0.5 scores for each team in the Restricted Track. Darker red indicates a lower  score. The percent frequency of each type in the test set is also shown.", "labels": [], "entities": [{"text": "Main error type ERRANT F 0.5", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.8748255868752798}]}, {"text": " Table 11: Main error type ERRANT F 0.5 scores for each team in the Unrestricted and Low Resource Track. Darker  red indicates a lower score. The percent frequency of each type in the test set is also shown.", "labels": [], "entities": [{"text": "Main error type ERRANT F 0.5", "start_pos": 11, "end_pos": 39, "type": "METRIC", "confidence": 0.8694641788800558}]}, {"text": " Table 12: ERRANT F 0.5 scores on the official gold references are compared against automatic references and other  popular metrics. The differences in how these metrics would rank each team are also shown, where a darker red  indicates a lower rank.", "labels": [], "entities": [{"text": "ERRANT F 0.5 scores", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.9185444861650467}]}]}