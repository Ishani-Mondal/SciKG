{"title": [{"text": "The Seemingly (Un)systematic Linking Element in Danish", "labels": [], "entities": [{"text": "Seemingly (Un)systematic Linking Element", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.5335434590067182}, {"text": "Danish", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.39925745129585266}]}], "abstractContent": [{"text": "The use of a linking element between compound members is a common phenomenon in Germanic languages.", "labels": [], "entities": []}, {"text": "Still, the exact use and conditioning of such elements is a disputed topic in linguistics.", "labels": [], "entities": []}, {"text": "In this paper we address the issue of predicting the use of linking elements in Danish.", "labels": [], "entities": []}, {"text": "Following previous research that shows how the choice of linking element might be conditioned by phonology , we frame the problem as a language modeling task: Considering the linking elements -s/-\u2205 the problem becomes predicting what is most probable to encounter next, a syllable boundary or the joining element , s.", "labels": [], "entities": []}, {"text": "We show that training a language model on this task reaches an accuracy of 94 %, and in the case of an unsupervised model, the accuracy reaches 80 %.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9995044469833374}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9994257688522339}]}], "introductionContent": [{"text": "In Danish, Norwegian and Swedish, as well as in other Germanic languages, a common way of forming new words is by compounding.", "labels": [], "entities": []}, {"text": "Here, novel words can be formed by combining already known words with an addition of a linking element between the components.", "labels": [], "entities": []}, {"text": "Within linguistic research this linking element is somewhat of a puzzle: First of all, several languages within the Germanic family seem to share similar linking elements).", "labels": [], "entities": []}, {"text": "The origin of these elements are however disputed.", "labels": [], "entities": []}, {"text": "Even if we assume a common origin, the use and distribution of single elements have changed among daughter languages, and we often find contradicting examples e.g., when comparing Ge.", "labels": [], "entities": []}, {"text": "Volk-s-musik and Da. folk-e-musik 'folk music').", "labels": [], "entities": [{"text": "Volk-s-musik", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9604899287223816}, {"text": "Da. folk-e-musik 'folk music", "start_pos": 17, "end_pos": 45, "type": "DATASET", "confidence": 0.830486512184143}]}, {"text": "Secondly, even though the choice of a linking element maybe clear to the individual speaker, linguists still struggle to establish rules for when the individual elements occur.", "labels": [], "entities": []}, {"text": "In Danish, the linking element is decided from the first member of a compound.", "labels": [], "entities": []}, {"text": "But when looking for rules that systematize what words take which element, only few guidelines are given (.", "labels": [], "entities": []}, {"text": "Interestingly, recent studies on linking elements in German suggest that the choice of linking element is at least partially phonological, determined by features such as stress.", "labels": [], "entities": []}, {"text": "Compounding has received attention in language technology as well, since it is the essence of one of the main challenges within this field: that language is productive.", "labels": [], "entities": []}, {"text": "Within the area of statistical machine translation, segmentation of compounds into units is an important task, e.g., when translating compound words from German to English where compounding is not productive ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6952865322430929}, {"text": "segmentation of compounds into units", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.8557360053062439}]}, {"text": "Similarly, when translating English multiword expressions (MWE) into German, methods for synthesis or generation of compounds are called for (.", "labels": [], "entities": [{"text": "translating English multiword expressions (MWE)", "start_pos": 16, "end_pos": 63, "type": "TASK", "confidence": 0.7161060571670532}]}, {"text": "Here the choice of a correct linking element becomes an issue.", "labels": [], "entities": []}, {"text": "In the work by they rely on a rule-based morphological analyzer for German to generate the correct compounding form.", "labels": [], "entities": []}, {"text": "Here they report that on a reference set of 283 correctly identified compounds 44 had an incorrect linking element.", "labels": [], "entities": []}, {"text": "Recent work by proposes a translation model from English MWE to German compounds that allows for modeling linking elements.", "labels": [], "entities": []}, {"text": "In their work, they report a high recall score when generating novel compounds.", "labels": [], "entities": [{"text": "recall score", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9839635193347931}]}, {"text": "Their error analysis show that their model has issues when choosing linking elements (e.g., when generating Kirchent\u00fcrme instead of the correct Kircht\u00fcrme 'church towers'), but they do not further provide any metrics on this subtask.", "labels": [], "entities": []}, {"text": "In this paper we wish to see how well a simple character-based language model is able to predict the usage of linking elements in Danish.", "labels": [], "entities": []}, {"text": "More specifically we will look at the case of predicting occurrence of two elements -s and -\u2205 (traditionally referred to as a nulfuge 'zero link') in Danish noun-noun compounds.", "labels": [], "entities": []}], "datasetContent": [{"text": "We introduce two models to approach the problem and two baselines from which we make our conclusions.", "labels": [], "entities": []}, {"text": "First, we investigated whether characterbased language models would be able to estimate the correct linking element of a word.", "labels": [], "entities": []}, {"text": "To this end, we trained a language model on syllabified words together with their linking element (s/+).", "labels": [], "entities": []}, {"text": "Second, we approached the problem in an unsupervised manner, training a general language model on syllabified words, without providing any specific information on linking elements.", "labels": [], "entities": []}, {"text": "In both experiments the models are evaluated as a prediction task on how well they are able to predict the correct linking element of a word by weighing the estimated probabilities of observing an s or a syllable boundary (+) in the end of: Examples of input, training signals and prediction task for the supervised and unsupervised approaches.", "labels": [], "entities": []}, {"text": "In order to validate the performance of our models, we employ 5-Fold Cross-Validation on the set of -s/-\u2205 nouns from RO.", "labels": [], "entities": []}, {"text": "For each iteration, we train a model with four folds and we divide the remaining fold equally for development and test.", "labels": [], "entities": []}, {"text": "In the first experiment, we train a character-based Recurrent Neural Network (RNN) language model on the entire set of -s/-\u2205 nouns from RO, including the linking element at the end of each instance.", "labels": [], "entities": []}, {"text": "We use a two-layer RNN with LSTM that receives an embedded representation of the characters with 128 dimensions, which are learned while training.", "labels": [], "entities": []}, {"text": "Each LSTM layer has 64 dimensions and predictions are made using a softmax over the vocabulary of characters.", "labels": [], "entities": []}, {"text": "We train the model using Stochastic Gradient Descent with cyclical learning rate (Smith, 2015) using the DyNet framework (.", "labels": [], "entities": []}, {"text": "As shows, besides a beginning-of-word symbol (\u02c6), we also add an end-of-word symbol (EOW) ($) to the input.", "labels": [], "entities": [{"text": "end-of-word symbol (EOW)", "start_pos": 65, "end_pos": 89, "type": "METRIC", "confidence": 0.7863927006721496}]}, {"text": "We expect that this addition will improve the performance of the model, as it helps to supervise the training signal more clearly by restricting the distribution of sand + as compounding elements to occur only after the EOW symbol.", "labels": [], "entities": [{"text": "EOW", "start_pos": 220, "end_pos": 223, "type": "METRIC", "confidence": 0.8543583750724792}]}, {"text": "However, this approach also adds noise to the signal as the original sequence of characters is altered.", "labels": [], "entities": []}, {"text": "In the second experiment, we train an RNN identical to that in the first experiment, but without including any specific information on linking elements.", "labels": [], "entities": []}, {"text": "Thus, attest time, this model has not been trained on nouns and linking elements, but would estimate the probabilities, P (s) and P (+), from the distribution of sand + word-internally.", "labels": [], "entities": []}, {"text": "The model is trained on the words from RO that are not included within the set of -s/-\u2205 nouns.", "labels": [], "entities": []}, {"text": "The difference between the two models is summarized in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for the supervised and unsupervised approaches and their baselines.", "labels": [], "entities": []}]}