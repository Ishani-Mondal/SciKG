{"title": [{"text": "Neural Word Decomposition Models for Abusive Language Detection", "labels": [], "entities": [{"text": "Neural Word Decomposition", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7063671946525574}, {"text": "Abusive Language Detection", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.7828189531962076}]}], "abstractContent": [{"text": "User generated text on social media often suffers from a lot of undesired characteristics including hatespeech, abusive language, insults etc. that are targeted to attack or abuse a specific group of people.", "labels": [], "entities": []}, {"text": "Often such text is written differently compared to traditional text such as news involving either explicit mention of abusive words, obfuscated words and typo-logical errors or implicit abuse i.e., indicating or targeting negative stereotypes.", "labels": [], "entities": []}, {"text": "Thus, processing this text poses several robustness challenges when we apply natural language processing techniques developed for traditional text.", "labels": [], "entities": []}, {"text": "For example, using word or token based models to process such text can treat two spelling variants of a word as two different words.", "labels": [], "entities": []}, {"text": "Following recent work, we analyze how character, subword and byte pair encoding (BPE) models can be aid some of the challenges posed by user generated text.", "labels": [], "entities": [{"text": "character, subword and byte pair encoding (BPE)", "start_pos": 38, "end_pos": 85, "type": "TASK", "confidence": 0.7055504858493805}]}, {"text": "In our work, we analyze the effectiveness of each of the above techniques, compare and contrast various word decomposition techniques when used in combination with others.", "labels": [], "entities": [{"text": "word decomposition", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.756677508354187}]}, {"text": "We experiment with finetuning large pretrained language models, and demonstrate their robust-ness to domain shift by studying Wikipedia attack , toxicity and Twitter hatespeech datasets.", "labels": [], "entities": [{"text": "Twitter hatespeech datasets", "start_pos": 158, "end_pos": 185, "type": "DATASET", "confidence": 0.748201827208201}]}], "introductionContent": [{"text": "In recent years, with the growing popularity of social media applications, there has been an exponential increase in the amount of user-generated text including microblog posts, status updates and comments posted on the web.", "labels": [], "entities": []}, {"text": "The power of communicating freely with large number of users has resulted in not only sharing news and exchanging content but has also led to a problem of large number of harmful, offensive and aggressive interactions online.", "labels": [], "entities": []}, {"text": "Previous work on identifying abusive language has tackled this problem by training computational methods that are capable of automatically recognizing offensive content for text on MySpace (), Twitter (, Wikipedia comments ( and Facebook posts ().", "labels": [], "entities": [{"text": "MySpace", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.9701877236366272}]}, {"text": "Most of these models are based on features extracted from words or word n-grams or the recurrent neural networks that operate on word embeddings () with few exceptions of models that utilize character n-grams that can model noisy text and out-of-vocabulary words (.", "labels": [], "entities": []}, {"text": "However, these models are not very effective at modeling obfuscated words such as w0m3n, nlgg3r which are prominent in user generated text that are intended at evading hate speech detection (.", "labels": [], "entities": [{"text": "hate speech detection", "start_pos": 168, "end_pos": 189, "type": "TASK", "confidence": 0.6460293432076772}]}, {"text": "In this work, we aim to address this by investigating word, subword and character-based models for abusive language detection.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 99, "end_pos": 125, "type": "TASK", "confidence": 0.6655926207701365}]}, {"text": "Recent advances in unsupervised pre-training of language models have led to strong improvements on various general natural language processing and understanding tasks such as question answering, sentiment and natural language inference (.", "labels": [], "entities": [{"text": "general natural language processing and understanding", "start_pos": 107, "end_pos": 160, "type": "TASK", "confidence": 0.7139330705006918}, {"text": "question answering", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.8376013338565826}]}, {"text": "However, it is unclear how such models trained on standard text would transfer information when fine-tuned on noisy user generated text.", "labels": [], "entities": []}, {"text": "In additional to studying word, subword and character-based model performances on abusive language detection we also combine these with pre-trained embeddings and fine-tuning these pre-trained language models and understand their efficiency and robustness in identifying abusive text.", "labels": [], "entities": [{"text": "abusive language detection", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.6785827477773031}]}, {"text": "Specifically, in this work, we address the effectiveness of character-based models, subword or Byte Pair Encoding (BPE) based models and word features based models along with pre-trained word embeddings and fine tuning pretrained language models for detecting abusive language in text.", "labels": [], "entities": [{"text": "detecting abusive language in text", "start_pos": 250, "end_pos": 284, "type": "TASK", "confidence": 0.808501148223877}]}, {"text": "Precisely we make following contributions: \u2022 We compare the effectiveness of end-to-end character based models, with word + character embedding models, byte pair encoding and subword models, to show which of the techniques perform better than pure word based models.", "labels": [], "entities": [{"text": "byte pair encoding", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.6744033495585123}]}, {"text": "\u2022 We demonstrate how fine-tuning large pretrained language models, the latest breakthrough in NLP, enhance state of the art on few of the abusive language datasets, and show that the domain shift isn't considerable when applied to abusive language datasets.", "labels": [], "entities": []}, {"text": "\u2022 We also examine how preprocessing documents with byte pair encoding model pretrained on a large corpus, boost the performance of several word embedding based models massively.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with three datasets: Twitter dataset (, Personal Attack and Toxicity datasets from Wikipedia Talk dataset) that covers sexism/racism, personal attack and toxicity aspects of abuse in user generated text online.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8917514681816101}, {"text": "Wikipedia Talk dataset", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8996439178784689}]}, {"text": "We use the hatespeech Twitter dataset (Hatespeech) provided by.", "labels": [], "entities": [{"text": "hatespeech Twitter dataset (Hatespeech)", "start_pos": 11, "end_pos": 50, "type": "DATASET", "confidence": 0.9180952111879984}]}, {"text": "This dataset was created from a corpus of 136k tweets collected from Twitter by searching for commonly used racist and sexist slurs on various ethnic, gender and religious minorities over a two-month period.", "labels": [], "entities": []}, {"text": "The original data had 16,907 tweets corresponding to sexist, racist and neither labels.", "labels": [], "entities": []}, {"text": "However, we could only retrieve 11170 of the tweets (2914: sexism, 17: racism and 8239: neither) with python's Tweepy library.", "labels": [], "entities": [{"text": "python's Tweepy library", "start_pos": 102, "end_pos": 125, "type": "DATASET", "confidence": 0.6744004115462303}]}, {"text": "Similar issue of missing tweets has been reported by.", "labels": [], "entities": []}, {"text": "However, the percent of tweets we lost are much higher than theirs and most of the tweets lost are for the racism label.", "labels": [], "entities": []}, {"text": "We have lost majority of the tweets corresponding to sexism label.", "labels": [], "entities": []}, {"text": "Since we lost large chunk of tweets we conduct our experiments on cross validation of 5 splits and report scores on all of the 5 splits.", "labels": [], "entities": [{"text": "cross validation of 5 splits", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.7616421222686768}]}, {"text": "In this section, we present different variants of the models described in Section 4 presented in.", "labels": [], "entities": []}, {"text": "fastText: We use multiple variants of fastText model.", "labels": [], "entities": []}, {"text": "Our fastText ngrams=1 uses embeddings learned for each unigram.", "labels": [], "entities": []}, {"text": "We treat this as our baseline model without any preprocessing of the text.", "labels": [], "entities": []}, {"text": "Our fastText ngrams=2 model also uses bigrams along with unigrams as independent tokens to learn embeddings.", "labels": [], "entities": []}, {"text": "All pairs of bigrams are chosen wtihout ant frequency threshold.", "labels": [], "entities": [{"text": "wtihout ant frequency threshold", "start_pos": 32, "end_pos": 63, "type": "METRIC", "confidence": 0.8389477282762527}]}, {"text": "Our fastText ngrams=2 + subword (2 \u2212 6) also uses all subwords within a word boundary within the range of 2 \u2212 6.", "labels": [], "entities": []}, {"text": "All our models are trained with learning rate of 0.5 and for 5 epochs.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9827306568622589}]}, {"text": "TextCNN): We run the TextCNN for classification in non-static mode, with learning rate of 0.0001, dropout of 0.5 for 50 epochs.", "labels": [], "entities": [{"text": "TextCNN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8934416174888611}, {"text": "learning rate", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9431396722793579}]}, {"text": "We have used default kernel window sizes N f = (3, 4, 5) with m = 100 filters.", "labels": [], "entities": []}, {"text": "We initialize the embeddings layer with word2vec pretrained embeddings 1 publicly available from google.", "labels": [], "entities": []}, {"text": "We used the non-static variant of TextCNN, with pretrained embedding initialization for word embedding layer.", "labels": [], "entities": [{"text": "TextCNN", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9055884480476379}]}, {"text": "TextCNN + char n-grams: The word embedding layer is constructed for this approach as mentioned in 1(b we report in our experiments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: We report Weighted F1-scores for the different models on the Hatespeech, W-TOX and W-ATT datasets.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9117489457130432}, {"text": "Hatespeech", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.9809522032737732}, {"text": "W-TOX", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.7931435108184814}, {"text": "W-ATT datasets", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8942819237709045}]}, {"text": " Table 2: Sample document split created by BERT BPE tokenizer, Custom BPE tokenizer", "labels": [], "entities": [{"text": "Sample document split", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8331997394561768}, {"text": "BERT", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9769246578216553}]}, {"text": " Table 3: Macro F1 average on the W-TOX and W-ATT  datasets.", "labels": [], "entities": [{"text": "F1 average", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9292178750038147}, {"text": "W-TOX", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.927010715007782}, {"text": "W-ATT  datasets", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9197141528129578}]}]}