{"title": [{"text": "Automatic error classification with multiple error labels", "labels": [], "entities": [{"text": "Automatic error classification", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6156198183695475}]}], "abstractContent": [{"text": "Although automatic classification of machine translation errors still cannot provide the same detailed granularity as manual error classification, it is an important task which enables estimation of translation errors and better understanding of the analyzed MT system, in a short time and on a large scale.", "labels": [], "entities": [{"text": "automatic classification of machine translation errors", "start_pos": 9, "end_pos": 63, "type": "TASK", "confidence": 0.7245748241742452}, {"text": "manual error classification", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.73704065879186}, {"text": "MT", "start_pos": 259, "end_pos": 261, "type": "TASK", "confidence": 0.9584396481513977}]}, {"text": "State-of-the-art methods use hard decisions to assign single error labels to each word.", "labels": [], "entities": []}, {"text": "This work presents first results of anew error classification method, which assigns multiple error labels to each word.", "labels": [], "entities": [{"text": "error classification", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.6829483807086945}]}, {"text": "We assign fractional counts for each label, which can be interpreted as a confidence for the label.", "labels": [], "entities": []}, {"text": "Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations or by automatic metrics (;; Popovi\u00b4cPopovi\u00b4c, 2015;.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8166785836219788}]}, {"text": "All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information.", "labels": [], "entities": []}, {"text": "Translation error analysis, both manual (  2014b) as well as automatic), as away to identify weaknesses of the systems and define priorities for their improvement, has received a fair amount of attention in the MT community.", "labels": [], "entities": [{"text": "Translation error analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8428884545962015}, {"text": "MT", "start_pos": 211, "end_pos": 213, "type": "TASK", "confidence": 0.9835671186447144}]}, {"text": "Although automatic error classification still cannot deal with fine-grained error taxonomies, it represents a valuable tool for fast and large scale translation error analysis.", "labels": [], "entities": [{"text": "automatic error classification", "start_pos": 9, "end_pos": 39, "type": "TASK", "confidence": 0.5898237427075704}, {"text": "translation error analysis", "start_pos": 149, "end_pos": 175, "type": "TASK", "confidence": 0.8883565862973531}]}, {"text": "With the emergence of neural MT systems, first insights about the differences between the neural approach and the then stateof-the-art statistical phrase-based approach were obtained by using automatic error classification.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.893084704875946}]}, {"text": "analyzed four MT systems for English into German by comparing different TER () scores and sub-scores, and applied the WER-based approach proposed by Popovi\u00b4cPopovi\u00b4c and Ney (2011) fora multilingual and multifaceted evaluation of eighteen MT systems for nine translation directions including six languages from four different families.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9625259637832642}, {"text": "TER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9833216667175293}]}, {"text": "So far, automatic error classification is based on hard decisions about the error class fora given word.", "labels": [], "entities": [{"text": "automatic error classification", "start_pos": 8, "end_pos": 38, "type": "TASK", "confidence": 0.6084970037142435}]}, {"text": "Addicter) uses a first-order Markov model for aligning reference words with hypothesis words, and Popovi\u00b4cPopovi\u00b4c and Ney (2011) use WER alignments; both methods assign only one single error label for each word.", "labels": [], "entities": []}, {"text": "However, the assumption that each word can be tagged with only one error category can be somewhat restrictive.", "labels": [], "entities": []}, {"text": "Human annotators' feedback) have pointed out that sometimes it is not completely clear what error category should be assigned to a word (e.g. it is difficult to differentiate a lexical error from a missing or extra word, or to decide which word reference: in some places rents will even rise hypothesis: in some places even grow rents Possible ambiguties: \u2022 which words should be tagged as reordering errors, \"rents\" or \"even\"?", "labels": [], "entities": []}, {"text": "\u2022 \"rise\"/\"grow\" can be reordering errors too, and lexical errors at the same time \u2022 are \"will\", \"rise\" and \"grow\" lexical errors, or \"will\" and \"rise\" are missing words and \"grow\" is an extra word?", "labels": [], "entities": []}, {"text": "span should be tagged as a reordering issue), or it maybe the case that a generated word should be assigned more than one error (e.g. a lexical and a reordering error).", "labels": [], "entities": []}, {"text": "Examples of such cases can be seen in.", "labels": [], "entities": []}, {"text": "In this work we propose to expand the automatic error classification approach by suggesting multiple error categories for each word.", "labels": [], "entities": [{"text": "automatic error classification", "start_pos": 38, "end_pos": 68, "type": "TASK", "confidence": 0.6163922846317291}]}, {"text": "Additionally, with each error category we are able to assign a (fractional) count which intuitively can be interpreted as a confidence for each error category.", "labels": [], "entities": []}, {"text": "Since, to the best of our knowledge, this represents the first attempt of multi-label automatic classification, we first explore what kind of multi-error suggestions are generated by our method.", "labels": [], "entities": [{"text": "multi-label automatic classification", "start_pos": 74, "end_pos": 110, "type": "TASK", "confidence": 0.657754252354304}]}, {"text": "We then compare our results with manual error annotations and with the method based on a single WER alignment.", "labels": [], "entities": [{"text": "WER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.639804482460022}]}, {"text": "As translation corpora with manual error analysis allowing multiple labels are not yet available, we evaluate our method by computing the correlation of the global distribution of errors with human assigned labels.", "labels": [], "entities": []}, {"text": "We also try to gain insights about the behaviour of the system and find out that the system makes sensible multi-error suggestions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied the new method as well as the single WER path method described in) to the publicly available test sets from the TERRA corpus () and PE2RR corpus) designed for evaluating automatic error classification.", "labels": [], "entities": [{"text": "TERRA corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.8892170190811157}, {"text": "PE2RR corpus", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.9088974893093109}, {"text": "automatic error classification", "start_pos": 181, "end_pos": 211, "type": "TASK", "confidence": 0.6071599622567495}]}, {"text": "In addition to translation hypotheses and post-edits (PE2RR) or references (TERRA), manual error annotations are also available.", "labels": [], "entities": [{"text": "PE2RR) or references (TERRA)", "start_pos": 54, "end_pos": 82, "type": "METRIC", "confidence": 0.7184501332896096}]}, {"text": "The statistics of the test corpora are shown in.", "labels": [], "entities": []}, {"text": "The main differences between the two data sets are (i) post-edited MT hypotheses are available in PE2RR (and standard reference translations in TERRA), (ii) manual error annotation in PE2RR is based on correcting automatically assigned labels whereas in TERRA it is performed from scratch.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9800229072570801}, {"text": "PE2RR", "start_pos": 98, "end_pos": 103, "type": "DATASET", "confidence": 0.9138854742050171}, {"text": "TERRA", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.6096249222755432}, {"text": "PE2RR", "start_pos": 184, "end_pos": 189, "type": "DATASET", "confidence": 0.8902109265327454}, {"text": "TERRA", "start_pos": 254, "end_pos": 259, "type": "DATASET", "confidence": 0.6452621221542358}]}, {"text": "All results are reported separately for each of the data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Example from Figure 1 with single error labels and with multiple error labels together with their fractional counts.", "labels": [], "entities": []}, {"text": " Table 3: Statistics of the used error annotated corpora: num- ber of different translation hypotheses, number of sentences  in all hypotheses, number of running words in all hypotheses,  and number of different language pairs.", "labels": [], "entities": []}, {"text": " Table 4: Relative frequencies of multiple error labels for PE2RR and TERRA.", "labels": [], "entities": [{"text": "Relative", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9687480330467224}, {"text": "TERRA", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9904367327690125}]}, {"text": " Table 5: Most frequent multiple error labels and the relation between their fractional counts.", "labels": [], "entities": []}, {"text": " Table 6: Pearson correlations comparison between error classes (interClass) and between translation hypotheses (interHyp)", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8830483257770538}]}]}